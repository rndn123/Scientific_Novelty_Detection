title
TAT - QA : A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance
abstract
Hybrid data combining both tabular and textual content ( e.g. , financial reports ) are quite pervasive in the real world .
However , Question Answering ( QA ) over such hybrid data is largely neglected in existing research .
In this work , we extract samples from real financial reports to build a new large-scale QA dataset containing both Tabular And Textual data , named TAT - QA , where numerical reasoning is usually required to infer the answer , such as addition , subtraction , multiplication , division , counting , comparison / sorting , and their compositions .
We further propose a novel QA model termed TAGOP , which is capable of reasoning over both tables and text .
It adopts sequence tagging to extract relevant cells from the table along with relevant spans from the text to infer their semantics , and then applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer .
TAGOP achieves 58.0 % in F 1 , which is an 11.1 % absolute increase over the previous best baseline model , according to our experiments on TAT - QA .
But this result still lags far behind the performance of human expert , i.e. 90.8 % in F 1 .
It demonstrates that our TAT - QA is very challenging and can serve as a benchmark for training and testing powerful QA models that address hybrid data .
Our dataset is publicly available for noncommercial use at https://nextplusplus.
github.io/TAT -QA / .
Introduction Existing QA systems largely focus on only unstructured text ( Hermann et al. , 2015 ; Rajpurkar et al. , 2016 ; Dua et al. , 2019 ; Li et al. , 2020 ; Nie et al. , 2020 ) , structured knowledge base ( KB ) ( Berant et al. , 2013 ; Yih et al. , 2015 ; Talmor and Berant , 2018 ) , or semi-structured tables ( Pasupat and Liang , 2015 ; Zhong et al. , 2017 ; , * * Corresponding author 2018 ; Zhang and Balog , 2019 ; . Though receiving growing interests ( Das et al. , 2017 ; Sun et al. , 2019 ; Chen et al. , 2020 b
Chen et al. , , 2021 , works on hybrid data comprising of unstructured text and structured or semi-structured KB / tables are rare .
Recently , Chen et al. ( 2020 b ) attempt to simulate a type of hybrid data through manually linking table cells to Wiki pages via hyperlinks .
However , such connection between table and text is relatively loose .
In the real world , a more common hybrid data form is , the table ( that usually contains numbers ) is more comprehensively linked to text , e.g. , semantically related or complementary .
Such hybrid data are very pervasive in various scenarios like scientific research papers , medical reports , financial reports , etc .
The left box of Figure 1 shows a real example from some financial report , where there is a table containing row / column header and numbers inside , and also some paragraphs describing it .
We call the hybrid data like this example hybrid context in QA problems , as it contains both tabular and textual content , and call the paragraphs associated paragraphs to the table .
To comprehend and answer a question from such hybrid context relies on the close relation between table and paragraphs , and usually requires numerical reasoning .
For example , one needs to identify " revenue from the external customers " in the describing text so as to understand the content of the table .
As for " How much does the commercial cloud revenue account for the total revenue in 2019 ? " , one needs to get the total revenue in 2019 , i.e. " 125 , 843 million " from the table and commercial cloud revenue , i.e. " 38.1 billion " , from the text to infer the answer .
To stimulate progress of QA research over such hybrid data , we propose a new dataset , named TAT - QA ( Tabular And Textual dataset for Question Answering ) .
The hybrid contexts in TAT - QA are extracted from real-world financial reports , each composed of a table with row / col header and numbers , as well as at least two paragraphs that describe , analyse or complement the content of this table .
Given hybrid contexts , we invite annotators with financial knowledge to generate questions that are useful in real-world financial analyses and provide answers accordingly .
It is worth mentioning that a large portion of questions in TAT - QA demand numerical reasoning , for which derivation of the answer is also labeled to facilitate developing explainable models .
In total , TAT - QA contains 16 , 552 questions associated with 2 , 757 hybrid contexts from 182 reports .
We further propose a novel TAGOP model based on TAT - QA .
Taking as input the given question , table and associated paragraphs , TAGOP applies sequence tagging to extract relevant cells from the table and relevant spans from text as the evidences .
Then it applies symbolic reasoning over them with a set of aggregation operators to arrive at the final answer .
Predicting the magnitude of a number is an important aspect when tackling hybrid data in TAT - QA , including thousand , million , billion , etc. that are often omitted or shown only in headers or associated paragraphs of the table for brevity .
We term such magnitude of a number as its scale .
Take Question 6 in Figure 1 as an example : " How much of the total revenue in 2018 did not come from devices ? "
The numerical value in the answer is obtained by subtraction : " 110 , 360 - 5 , 134 " , while the scale " million " is identified from the first-row header of the table .
In TAGOP , we incorporate a multi-class classifier for scale prediction .
We test three types of QA models on TAT - QA , specially addressing tabular , textual , and hybrid data .
Our TAGOP achieves 58.0 % in terms of F 1 , which is a 11.1 % absolute increase over the best baseline model , according to our experiments on TAT - QA .
It is worth noting that the results still lag far behind performance of human experts , i.e. 90.8 % in F 1 .
We can see that to tackle the QA task over the hybrid data as in TAT - QA is challenging and more effort is demanded .
We expect our TAT - QA dataset and TAGOP model to serve as a benchmark and baseline respectively to contribute to the development of QA models for hybrid data , especially those requiring numerical reasoning .
Dataset Construction and Analysis
We here explain how we construct TAT - QA and analyze its statistics to better reveal its proprieties .
Data Collection and Preprocessing In TAT - QA there are two forms of data : tables and their relevant text , which are extracted from real-world financial reports .
In particular , we first download about 500 financial reports released in the past two years from an online website 1 .
We adopt the table detection model in to detect tables in these reports , and apply Apache PDFBox 2 library to extract the table contents to be processed with our annotation tool .
We only keep those tables with 3 ? 30 rows and 3 ? 6 columns .
Finally , about 20 , 000 candidate tables are retained , which have no standard schema and lots of numbers inside .
The corresponding reports with selected tables are also kept .
Note that these candidate tables may still contain errors , such as containing too few or many rows / cols , mis-detected numbers , which will be manually picked out and deleted or fixed during the annotation process .
Dataset Annotation
The annotation is done with our self-developed tool .
All the annotators are with financial background knowledge .
Adding Relevant Paragraphs to Tables
We build valid hybrid contexts based on the original reports kept in the previous step .
A valid hybrid context in TAT - QA consists of a table and at least two associated paragraphs surrounding it , as shown in the left box in Figure 1 .
To associate enough relevant paragraphs to a candidate table , the annotators first check whether there are ?
2 paragraphs around this table , and then check whether they are relevant , meaning the paragraphs should be describing , analysing or complementing the content in the table .
If yes , then all the surrounding paragraphs will be associated to this table .
Otherwise , the table will be skipped ( discarded ) .
3 Question - Answer Pair Creation Based on the valid hybrid contexts , the annotators are then asked to create question - answer pairs , where the questions need to be useful in real-world financial analyses .
In addition , we encourage them to create questions that can be answered by people without much finance knowledge and use common words instead of the same words appeared in the hybrid context ( Rajpurkar et al. , 2016 ) .
Given one hybrid context , at least 6 questions are generated , including extracted and calculated questions .
For extracted questions , the answers can be a single span or multiple spans from either the table or the associated paragraphs .
For calculated questions , numerical reasoning is required to produce the answers , including addition , subtraction , multiplication , division , counting , comparison / sorting and their compositions .
Furthermore , we particularly ask the annotators to annotate the right scale for the numerical answer when necessary .
Answer Type and Derivation Annotation
The answers in TAT - QA have three types : a single span or multiple spans extracted from the table or text , as well as a generated answer ( usually obtained through numerical reasoning ) .
The annotators will 3 About two thirds of candidate tables were discarded .
also need to label its type after they generate an answer .
For generated answers , the corresponding derivations are provided to facilitate the development of explainable QA models , including two types : 1 ) an arithmetic expression , like ( 11 , 386 - 10 , 353 ) /10 , 353 ) for Question 8 in Figure 1 , which can be executed to arrive at the final answer ; and 2 ) a set of items separated with " # # " , like " device ## enterprise services " for Question 4 in Figure 1 where the count of items equals the answer .
We further divide questions in TAT - QA into four kinds : Span , Spans , Arithmetic and Counting , where the latter two kinds correspond to the above two types of deviations , to help us better investigate the numerical reasoning capability of a QA model .
Answer Source Annotation
For each answer , annotators are required to specify the source ( s ) it is derived from , including Table , Text , and Table-text ( both ) .
This is to force the model to learn to aggregate information from hybrid sources to infer the answer , thus lift its generalizability .
For example , to answer Question 7 in Figure 1 : " How much does the commercial cloud revenue account for the total revenue in 2019 ? " , we can observe from the derivation that " 125 , 843 million " comes from the table while " 38.1 billion " from text .
Quality Control
To ensure the quality of annotation in TAT - QA , we apply strict quality control procedures .
Competent Annotators
To build TAT - QA , financial domain knowledge is necessary .
Hence , we employ about 30 university students majored in finance or similar disciplines as annotators .
We give all candidate annotators a minor test and only those with 95 % correct rate are hired .
Before starting the annotation work , we give a training session to the annotators to help them fully understand our annotation requirements and also learn the usage of our annotation system .
Two -round Validation
For each annotation , we ask two different verifiers to perform a two -round validation after it is submitted , including checking and approval , to ensure its quality .
We have five verifiers in total , including two annotators who have good performance on this project and three graduate students with financial background .
In the checking phase , a verifier checks the submitted annotation and asks the annotator to fix it if any mistake or problem is found .
In the approval phase , a different verifier inspects the annotation again that has been confirmed by the first verifier , and then approves it if no problem is found .
Dataset Analysis
Averagely , an annotator can label two hybrid contexts per hour ; the whole annotation work lasts about three months .
Finally , we attain a total of 2 , 757 hybrid contexts and 16 , 552 corresponding question - answer pairs from 182 financial reports .
The hybrid contexts are randomly split into training set ( 80 % ) , development set ( 10 % ) and test set ( 10 % ) ; hence all questions about a particular hybrid context belong to only one of the splits .
We show the basic statistics of each split in Table 1 , and the question distribution regarding answer source and answer type in Table 2 . In Figure 1
TAGOP Model
We introduce a novel QA model , named TAGOP , which first applies sequence TAGging to extract relevant cells from the table and text spans from the paragraphs inspired by ( Li et al. , 2016 ; Sun et al. , 2016 ; Segal et al. , 2020 ) .
This step is analogy to slot filling or schema linking , whose effectiveness has been demonstrated in dialogue systems and semantic parsing .
And then TAGOP performs symbolic reasoning over them with a set of aggregation OPerators to arrive at the final answer .
The overall architecture is illustrated in Figure 2 .
Sequence Tagging Given a question , TAGOP first extracts supporting evidences from its hybrid context ( i.e. the table and associated paragraphs ) via sequence tagging with the Inside - Outside tagging ( IO ) approach ( Ramshaw and Marcus , 1995 ) .
In particular , it assigns each token either I or O label and takes those tagged with I as the supporting evidences for producing the answer .
The given question , flattened table by row ( Herzig et al. , 2020 ) and associated paragraphs are input sequentially to a transformerbased encoder like RoBERTa , as shown in the bottom part of Figure 2 , to obtain corresponding representations .
Each sub-token is tagged independently , and the corresponding cell in the table or word in the paragraph would be regarded as positive if any of its sub-tokens is tagged with I .
For the paragraphs , the continuous words that are predicted as positive are combined as a span .
During testing , all positive cells and spans are taken as the supporting evidences .
Formally , for each sub-token t in the paragraph , the probability of the tag is computed as p tag t = softmax ( FFN ( h t ) ) ( 1 ) where FFN is a two -layer feed -forward network with GELU ( Hendrycks and Gimpel , 2016 ) activation and h t is the representation of sub-token t.
Aggregation Operator Next , we perform symbolic reasoning over obtained evidences to infer the final answer , for which we apply an aggregation operator .
In our TAGOP , there are ten types of aggregation operators .
For each input question , an operator classifier is applied to decide which operator the evidences would go through ; for some operators sensitive to the order of input numbers , an auxiliary number order classifier is used .
The aggregation operators are explained as below , covering most reasoning types as listed in Figure 1 . ? Span-in- text :
To select the span with the highest probability from predicted candidate spans .
The probability of a span is the highest probability of all its sub-tokens tagged I. 1 where the hybrid context is also shown , TAGOP supports 10 operators , which are described in Section 3.2 . ?
Spans :
To select all the predicted cell and span candidates ; ?
Sum :
To sum all predicted cells and spans purely consisting of numbers ; ?
Count :
To count all predicted cells and spans ; ?
Average :
To average over all the predicted cells and spans purely consisting of numbers ; ? Multiplication :
To multiply all predicted cells and spans purely consisting of numbers ; ? Division :
To first rank all the predicted cells and spans purely consisting of numbers based on their probabilities , and then apply division calculation to top-two ; ?
Difference :
To first rank all predicted numerical cells and spans based on their probabilities , and then apply subtraction calculation to top-two . ?
Change ratio :
For the top-two values after ranking all predicted numerical cells and spans based on their probabilities , compute the change ratio of the first value compared to the second one .
Operator Classifier
To predict the right aggregation operator , a multi-class classifier is developed .
In particular , we take the vector of [ CLS ] as input to compute the probability : p op = softmax ( FFN ( [ CLS ] ) ( 2 ) where FFN denotes a two -layer feed -forward network with the GELU activation .
Number Order Classifier
For operators of Difference , Division and Change ratio , the order of the input two numbers matters in the final result .
Hence we additionally append a number order classifier after them , formulated as p order = softmax ( FFN ( avg ( h t1 , h t2 ) ) ( 3 ) where FFN denotes a two -layer feed - forward network with the GELU activation , h t1 , h t2 are representations of the top two tokens according to probability , and " avg " means average .
For a token , its probability is the highest probability of all its sub-tokens tagged I , and its representation is the average over those of its sub-tokens .
Scale Prediction
Till now we have attained the string or numerical value to be contained in the final answer .
However , a right prediction of a numerical answer should not only include the right number but also the correct scale .
This is a unique challenge over TAT - QA and very pervasive in the context of finance .
We develop a multi-class classifier to predict the scale .
Generally , the scale in TAT - QA may be None , Thousand , Million , Billion , and Percent .
Taking as input the concatenated representation of [ CLS ] , the table and paragraphs sequentially , the multi-class classifier computes the probability of the scale as p scale = softmax ( FFN ( [ [ CLS ] ; h tab ; h p ] ) ( 4 ) where h tab and h p are the representations of the table and the paragraphs respectively , which are obtained by applying an average pooling over the representations of their corresponding tokens , " ; " denotes concatenation , and FFN denotes a two -layer feed -forward network with the GELU activation .
After obtaining the scale , the numerical or string prediction is multiplied or concatenated with the 3282 corresponding scale as the final prediction to compare with the ground - truth answer respectively .
Training
To optimize TAGOP , the overall loss is the sum of the loss of the above four classification tasks : L = NLL ( log ( P tag ) , G tag ) + NLL ( log ( P op ) , G op ) + NLL ( log ( P scale ) , G scale ) + NLL ( log ( P order ) , G order ) ( 5 ) where NLL ( ? ) is the negative log-likelihood loss , G tag and G op come from the supporting evidences which are extracted from the annotated answer and derivation .
We locate the evidence in the table first if it is among the answer sources , and otherwise in its associated paragraphs .
Note we only keep the first found if an evidence appears multiple times in the hybrid context .
G scale uses the annotated scale of the answer ; G order is needed when the groundtruth operator is one of Difference , Division and Change ratio , which is obtained by mapping the two operands extracted from their corresponding ground -truth deviation in the input sequence .
If their order is the same as that in the input sequence , G order = 0 ; otherwise it is 1 .
Experiments and Results
Baselines Textual QA Models
We adopt two reading comprehension ( RC ) models as baselines over textual data : BERT - RC ( Devlin et al. , 2018 ) , which is a SQuAD - style RC model ; and NumNet + V2 4 ( Ran et al. , 2019 ) , which achieves promising performance on DROP that requires numerical reasoning over textual data .
We adapt them to our TAT - QA as follows .
We convert the table to a sequence by row , also as input to the models , followed by tokens from the paragraphs .
Besides , we add a multi-class classifier , exactly as in our TAGOP , to enable the two models to predict the scale based on Eq. ( 4 ) .
Tabular QA Model
We employ TaPas for Wik-iTableQuestion ( WTQ ) ( Herzig et al. , 2020 ) as a baseline over tabular data .
TaPas is pretrained over large-scale tables and associated text from Wikipedia jointly for table parsing .
To train it , we heuristically locate the evidence in the table with the annotated answer or derivation , which is the 4 https://github.com/llamazing/numnet plus first matched one if a same value appears multiple times .
In addition , we remove the " numerical rank id " feature in its embedding layer , which ranks all values per numerical column in the table but does not make sense in TAT - QA .
Similar to above textual QA setting , we add an additional multi-class classifier to predict the scale as in Eq. ( 4 ) .
Hybrid QA Model
We adopt HyBrider ( Chen et al. , 2020 b ) as our baseline over hybrid data , which tackles tabular and textual data from Wikipedia .
We use the code released in the original paper 5 , but adapt it to TAT - QA .
Concretely , each cell in the table of TAT - QA is regarded as " linked " with associated paragraphs of this table , like hyperlinks in the original paper , and we only use its cell matching mechanism to link the question with the table cells in its linking stage .
The selected cells and paragraphs are fed into the RC model in the last stage to infer the answer .
For ease of training on TAT - QA , we also omit the prediction of the scale , i.e. we regard the predicted scale by this model as always correct .
Evaluation Metrics
We adopt the popular Exact Match ( EM ) and numeracy - focused F 1 score ( Dua et al. , 2019 ) to measure model performance on TAT - QA .
However , the original implementation of both metrics is insensitive to whether a value is positive or negative in the answer as the minus is omitted in evaluation .
Since this issue is crucial for correctly interpreting numerical values , especially in the finance domain , we keep the plus-minus of a value when calculating them .
In addition , the numeracy - focused F 1 score is set to 0 unless the predicted number multiplied by predicted scale equals exactly the ground truth .
Results and Analysis
In the following , we report our experimental results on dev and test sets of TAT - QA .
Comparison with Baselines
We first compare our TAGOP with three types of previous QA models as described in Section 4.1 .
The results are summarized in Table 3 .
It can be seen that our model is always superior to other baselines in terms of both metrics , with very large margins over the second best , namely 50.1/58.0 vs. 37.0/46.9 in EM / F 1 on test set of TAT - QA respectively .
This well reveals the effectiveness of our method that reasons over both tabular and textual data involving lots 3283 of numerical contents .
For two textual QA baselines , NumNet + V2 performs better than BERT - RC , which is possibly attributed to the stronger capability of numerical reasoning of the latter , but it is still worse than our method .
The tabular QA baseline Tapas for WTQ is trained with only tabular data in TAT - QA , showing very limited capability to process hybrid data , as can be seen from its performance .
The HyBrider is the worst among all baseline models , because it is designed for Hy-bridQA ( Chen et al. , 2020 b ) which does not focus on the comprehensive interdependence of table and paragraphs , nor numerical reasoning .
However , all the models perform significantly worse than human performance 6 , indicating TAT - QA is challenging to current QA models and more efforts on hybrid QA are demanded .
Answer Type and Source Analysis Furthermore , we analyze detailed performance of TAGOP w.r.t answer type and source in Table 4 .
It can be seen that TAGOP performs better on the questions whose answers rely on the tables compared to those from the text .
This is probably because table cells have clearer boundaries than text spans to the model , thus it is relatively easy for the model to extract supporting evidences from the tables leveraging sequence tagging techniques .
In addition , TAGOP performs relatively worse on arithmetic questions compared with other types .
This may be because the calculations for arithmetic questions are diverse and harder than other types , indicating the challenge of TAT - QA , especially for the requirement of numerical reasoning .
Results of TAGOP with Different Operators
We here investigate the contributions of the ten aggregation operators to the final performance of TAGOP .
As shown in Table 5 , we devise nine variants of the full model of TAGOP ; based on the variant of TAGOP with only one operator ( e.g. Span-in-text ) , for each of other variants , we add one more operator back .
As can be seen from the table , all added operators can benefit the model performance .
Furthermore , we find that some operators like Spanin-text , Cell-in-table , Difference and Average make 6
The human performance is evaluated by asking annotators to answer 50 randomly sampled hybrid contexts ( containing 301 questions ) from our test set .
Note the human performance is still not 100 % correct because our questions require relatively heavy cognitive load like tedious numerical calculations .
Comparing human performance of F1 in SQUAD ( Rajpurkar et al. , 2016 ) ( 86.8 % ) and DROP ( Dua et al. , 2019 ) ) ( 96.4 % ) , the score ( 90.8 % ) in our dataset already indicates a good quality and annotation consistency in our dataset .
Evidence ( 29 % ) , meaning the model failed to extract the supporting evidence for the answer ; ( 3 ) Wrong Calculation ( 9 % ) , meaning the model failed to compute the answer with the correct supporting evidence ; ( 4 ) Unsupported Calculation ( 4 % ) , meaning the ten operators defined cannot support this calculation ; ( 5 ) Scale Error ( 3 % ) , meaning the model failed to predict the scale of the numerical value in an answer .
Method
We can then observe about 84 % error is caused by the failure to extract the supporting evidence from the table and paragraphs given a question .
This demonstrates more efforts are needed to strengthen the model 's capability of precisely aggregating information from hybrid contexts .
After instance - level analysis , we find another interesting error resource is the dependence on domain knowledge .
While we encourage annotators to create questions answerable by humans without much finance knowledge , we still find domain knowledge is required for some questions .
For example , given the question " What is the gross profit margin of the company in 2015 ? " , the model needs to extract the gross profit and revenue from the hybrid context and compute the answer according to the finance formula ( " gross profit margin = gross profit / revenue " ) .
How to integrate such finance knowledge into QA models to answer questions in TAT - QA still needs further exploration .
( Hermann et al. , 2015 ) , SQuAD ( Rajpurkar et al. , 2016 ) , etc .
Recently deep reasoning over textual data has gained increasing attention ( Zhu et al. , 2021 ) , e.g. multihop reasoning Welbl et al. , 2018 ) . DROP ( Dua et al. , 2019 ) is built to develop numerical reasoning capability of QA models , which in this sense is similar to TAT - QA , but only focuses on textual data .
KB / Tabular QA aims to automatically answer questions via wellstructured KB ( Berant et al. , 2013 ; Talmor and Berant , 2018 ; Yih et al. , 2015 ) or semi-structured tables ( Pasupat and Liang , 2015 ; Zhong et al. , 2017 ; Yu et al. , 2018 ) .
Comparably , QA over hybrid data receives limited efforts , focusing on mixture of KB / tables and text .
HybridQA ( Chen et al. , 2020 b ) is one existing hybrid dataset for QA tasks , where the context is a table connected with Wiki pages via hyperlinks .
Numerical Reasoning Numerical reasoning is key to many NLP tasks like question answering ( Dua et al. , 2019 ; Ran et al. , 2019 ; Andor et al. , 2019 ; Chen et al. , 2020a ; Pasupat and Liang , 2015 ; Herzig et al. , 2020 ; Yin et al. , 2020 ; and arithmetic word problems ( Kushman et al. , 2014 ; Mitra and Baral , 2016 ; Huang et al. , 2017 ; Ling et al. , 2017 ) .
To our best knowledge , no prior work attempts to develop models able to perform numerical reasoning over hybrid contexts .
Conclusion
We propose a new challenging QA dataset TAT - QA , comprising real- word hybrid contexts where the table contains numbers and has comprehensive dependencies on text in finance domain .
To answer questions in TAT - QA , the close relation between table and paragraphs and numerical reasoning are required .
We also propose a baseline model TAGOP based on TAT - QA , aggregating information from hybrid context and performing numerical reasoning over it with pre-defined operators to compute the final answer .
Experiments show TAT - QA dataset is very challenging and more effort is demanded for tackling QA tasks over hybrid data .
We expect our TAT - QA dataset and TAGOP model would serve as a benchmark and baseline respectively to help build more advanced QA models , facilitating the development of QA technologies to address more complex and realistic hybrid data , especially those requiring numerical reasoning .
Figure 1 : 1 Figure 1 : An example of TAT -QA .
The left dashed line box shows a hybrid context .
The rows with blue background are row header while the column with grey is column header .
The right solid line box shows corresponding question , answer with its scale , and derivation to arrive at the answer .
