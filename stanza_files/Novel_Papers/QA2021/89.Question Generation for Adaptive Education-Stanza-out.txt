title
Question Generation for Adaptive Education
abstract
Intelligent and adaptive online education systems aim to make high-quality education available for a diverse range of students .
However , existing systems usually depend on a pool of hand -made questions , limiting how finegrained and open-ended they can be in adapting to individual students .
We explore targeted question generation as a controllable sequence generation task .
We first show how to fine- tune pre-trained language models for deep knowledge tracing ( LM - KT ) .
This model accurately predicts the probability of a student answering a question correctly , and generalizes to questions not seen in training .
We then use LM - KT to specify the objective and data for training a model to generate questions conditioned on the student and target difficulty .
Our results show we succeed at generating novel , wellcalibrated language translation questions for second language learners from a real online education platform .
Introduction Online education platforms can increase the accessibility of educational resources around the world .
However , achieving equitable outcomes across diverse learning needs benefits from systems that are adaptive and individualized to each student ( Doroudi and Brunskill , 2019 ) .
Traditionally , adaptive education methods involve planning over a pool of pre-made questions ( Atkinson , 1972 ; Hunziker et al. , 2018 ) .
These are naturally limited by the diversity and coverage of the pool , as well as the scaling capacity of curriculum planning algorithms .
Recent approaches , such as procedural generation for personalized programming games ( Valls - Vargas et al. , 2017 ) , are limited to well -specified small domains .
We address these limitations by leveraging recent success in deep generative models , in particular language models ( LMs ) .
Many educational activities involve sequential data , such as language translation , reading compre -
< Y> Figure 1 : Example input and outputs for our LM - based knowledge tracing model ( middle ) and question generation model ( bottom ) for an online reverse language translation task ( top ) .
A question in this task consists of a target phrase for the student , in this case a Spanish learner , to translate ( e.g. " the woman " ) .
hension , algebra , and deductive logic .
Meanwhile , pre-trained LMs can effectively handle sequences from a wide range of modalities ( Madani et al. , 2020 ; Polu and Sutskever , 2020 ) .
In this work , we focus on natural language sequences , where recent progress in language modeling has shown great success at capturing abstract properties of language ( Hewitt and Manning , 2019 ; Liu et al. , 2019 ) .
Specifically , we show how pre-trained LMs can be easily leveraged to adaptively generate questions for a given student and target difficulty in a reverse translation task , using difficulty at answering questions as a proxy for more complex future learning objectives .
We introduce an LM - based knowledge tracing model ( LM - KT ) to predict students ' difficulty on novel questions ( e.g. target phrases to translate ) .
We show that LM - KT is well - calibrated , allowing us to pose the learning problem for the question generator : given a student state , generate a question that will achieve a target difficulty , according to LM -KT .
We evaluate both LM - KT and question generation models on real users and responses from Duolingo 1 , a popular online second - language learning platform .
Background & Related Works
There exists a rich body of work on precisely modeling student " ability " and learning .
For example , Item Response Theory ( IRT ) seeks to model individual student ability based on their responses to different questions , creating a strong factorization between students and test items ( Lord , 1980 ; Hambelton and Jodoin , 2003 ) .
Meanwhile , Computer Adaptive Testing ( CAT ) techniques are used to determine a fixed student ability as quickly as possible by selecting test items based on information utility ( Weiss and Kingsbury , 1984 ; Thissen and Mislevy , 2000 ; Settles et al. , 2020 ) .
However , these methods , which have been used to develop efficient standardized tests , do not necessarily optimize a student 's learning experience ( Mu et al. , 2018 ) .
We instead focus on tracking each student 's evolving knowledge , choosing questions to target difficulty .
Knowledge Tracing ( KT ) seeks to model a student 's knowledge state from their answer history in order to help individualize exercise sequences ( Corbett and Anderson , 1995 ) .
This draws inspiration from traditional education curriculum practices , such as distributed spacing of vocabulary ( Bloom and Shuell , 1981 ) and mixed review in mathematics ( Rohrer , 2009 ) .
To address simplifying assumptions in earlier KT approaches , such as discrete knowledge representations , Piech et al . ( 2015 ) introduced Deep Knowledge Tracing ( DKT ) , which uses RNNs to enable more complex knowledge representations for students .
Recently , SAINT +
( Shin et al. , 2020 ) showed state - of- the - art performance on the popular EdNet KT task using a Transformer model to capture temporal information across activities , motivating our use of Transformer LMs .
Controllable Text Generation aims to steer LMs towards desired attributes .
Examples include using reinforcement learning to control quality metrics ( Ranzato et al. , 2016 ) , adjusting sampling weights to control for poetry style ( Ghazvininejad et al. , 2017 ) , and learning to condition on valence or domain-specific codes ( Keskar et al. , 2019 ; Peng et al. , 2018 ) .
To the best of our knowledge , we are
Method Given any autoregressive language model ( e.g. GPT - 2 ( Radford et al. , 2019 ) , we can fine- tune a LM - KT model ( p ? KT ) to predict whether an individual student will correctly answer the next question .
If this model has well - calibrated uncertainty , we can use its predicted probability of a correct answer as a proxy for the difficulty of a question to a student .
We then train a question generation model ( p ? QG ) to generate a new question conditioned on a student and desired target difficulty .
Question Representation Unlike standard DKT , which treats questions as IDs or simple handcrafted features , we represent questions fully in text ( e.g. " she eats " in Figure 1 ) .
This is a key contribution of our work , required by our eventual goal of generating questions in text , and allows the model to leverage similarity across linguistic features .
We thus represent a question q as a sequence of words , with prefix and suffix tokens : q i = < Q> w i 1 w i 2 w i 3 ... w i n < A> Student State
We represent a student as a temporally - evolving sequence of questions and their responses .
As in much previous KT work , we represent the student response as simply correct / incorrect , with special tokens < Y> and < N >.
A student 's current state is thus represented as a sequence of all past question and response pairs : s j = q j 1 a j 1 q j 2 a j 2 ... q j m a j m , a i ? {< Y > , <N >} LM -KT
Given the sequential nature of student learning over time , we can easily frame knowledge tracing as an autoregressive language modeling task .
Given a dataset D of students s 1 , s 2 , ... , s | D | , we employ the standard training objective of finding the parameters ?
KT that minimizes L KT = ? | D| i=1 |x ( i ) | t=1 logp ? KT ( x ( i ) t |x ( i ) <t ) ( 1 ) where x ( j ) = ( x ( j ) 1 , .... , x ( j ) | x | ) is the entire sequence tokens corresponding to student s j , consisting of all their past questions and answers .
Using the softmax output of the LM - KT model ( p ? KT ) , we estimate a student 's ( inverse ) difficulty in answering a specific question as d qs = p ? KT ( < Y >|s , q ) .
We find that p ?
KT is well - calibrated ( Section 4.2 ) , yielding a good proxy for the true question difficulty .
Question Generation
We frame question generation as finetuning a new autoregressive LM .
Given random samples of students and questions from a held - out set not used to train LM - KT , we can construct a new dataset D consisting of s i d i < G > q i sequences , where < G> is a special generation token and d i = p ? KT ( < Y>|s i , q i ) is the continuous difficulty value assigned by LM -KT .
We learn a linear layer to map the continuous input difficulty into a difficulty control vector c d of dimension matching the LM word-embeddings , which we append to the token embeddings .
Unlike LM - KT , we train our question generation model p ?
QG to minimize the loss only on the question text , which only appears after the < G > token .
If t g is the token index of < G > , then our modified loss is : L QG = ?
| D | i=1 |x ( i ) | t=tg + 1 logp ? QG ( x ( i ) t |x ( i ) <t ) ( 2 ) where sequence x ( j ) contains the full s j d j < G >q j sequence .
At test time , we generate tokens w 1 ...w n conditioned on the s j d j < G> prefix .
Experiments
Our method generalizes to any education activity that can be represented with text sequences .
Due to the availability of real student learning data , we focus on a reverse language translation task , where a student translates phrases from their native language ( e.g. English , " she eats " ) to the second language they are learning ( e.g. Spanish , " ella come " ) .
Experimental Details
We use the 2018 Duolingo Shared Task on Second Language Acquisition Modeling ( Settles et al. , 2018 ) dataset , which contains questions and responses for Duolingo users over the first 30 days of learning a second language .
While the original task 's goal was to identify token - level mistakes , we collapse these errors into binary ( correct / incorrect ) per-question labels .
We use the provided train / dev/ test splits for users learning Spanish and French .
We create separate held - out sets from the test set to evaluate the LM - KT and question generation models .
For both models , we finetune separate GPT - 2 ( Radford et al. , 2019 )
Results : Student Modeling
We evaluate LM - KT two ways : first , its ability to predict if an individual student will answer a novel question correctly on a held - out test set of real Duolingo student responses .
Second , how wellcalibrated these predictions are , which is crucial to our later use of LM - KT for question generation .
Table 1 compares AUC - ROC on a held - out test set for our LM - KT model with standard DKT , which uses question IDs instead of text , and a baseline that ignores the student state , only using the question text representation .
This question only baseline would perform well if the Duolingo dataset largely consisted of universally " easy " and " difficult " questions , independent of individual student .
Our results show that incorporating the student state is crucial for accurately predicting Duolingo user responses , and including question text also leads to a significant improvement .
LM - KT outperforms Standard DKT especially on novel questions -a necessary generalization ability for generation .
Finally , we measure the calibration of our LM - KT models for both Spanish and French ( from En-glish ) learners , which is the crucial property for our downstream generation task .
We bin our test data by predicted question difficulty , and plot the fraction of true correct answers in each bin .
Figure 2 shows that LM - KT is well - calibrated , for both Spanish and French , meaning the predicted difficulty matches the empirically observed proportion of correct answers .
Results : Question Generation
We evaluate four different aspects of our question generation model : ( i ) successful control for difficulty , ( ii ) novelty , ( iii ) fluency , and ( iv ) latency .
Difficulty Control
To explore whether our question generation model indeed depends on target difficulty and the individual student , we first measure the model 's perplexity on a held - out test set of Duolingo questions , compared to permutation baselines .
Table 2 ( top ) shows that perplexity is lower for true student / target difficulty inputs than when either or both of these are permuted .
The target difficulty values in this analysis were defined by the LM - DKT model .
We can remove this dependence by using the actual student responses from Duolingo : we set the target difficulty to 1 if the student was correct and 0 otherwise .
Table 2 ( bottom ) shows our model prefers questions paired with these " true correctness " targets than paired with random ones .
To evaluate how well our generation model achieves target difficulties , we take 15 unseen students and generate 30 questions for each of 9 input difficulties ( 0.1-0.9 ) .
We then use LM - KT ( a wellcalibrated proxy for true difficulty ) to measure the difficulty of these generated questions for each student .
Figure 3 shows that we are able to achieve fine- grained control over target difficulty for both Spanish and French students , with an average Root - Mean Squared Error ( RMSE ) of .052 across all students and target difficulties .
Adding a sampling penalty ( Keskar et al. , 2019 ) increases the variance in difficulty ( RMSE .062 ) in exchange for more novel and diverse questions , as discussed next .
Novelty and Fluency
By leveraging a pretrained language model 's ability to manipulate structure , we can generate novel questions not present in the entire Duolingo question set ( See Table 3 ) .
Across 4,050 questions generated for Spanish learners , we found that with a repetition penalty ( Keskar et al. , 2019 ) , around 43 % of all questions , and 66 % of high difficulty ( d = 0.1 ) required to rank all questions in the pool , varying its size ( Figure 4 ) .
On one NVIDIA Titan XP GPU , we find that , averaged across all target difficulties , our question generation model takes half the time to achieve the same quality as pool selection .
The gap increases when trying to sample harder questions ( d < 0.5 ) - even a pool size of 1000 does not have sufficient difficult questions , likely due to a skew in the Duolingo question set .
Additional controls , such as for style or topic , can easily be combined with our generation method , but would make pool selection exponentially more complex .
Pool Sampling ( all targets ) Pool Sampling ( difficult targets only ) Generation ( all targets ) Generation ( difficult targets only )
Figure 4 : Pool selection ( for one student ) suffers worse question quality vs. latency trade - off than question generation , especially for sampling difficult questions .
Conclusion
Our work is a first step toward showing that sequence - based models combined with domain knowledge , such as pre-trained LMs , can be leveraged for adaptive learning tasks .
We show how to use modern LMs to generate novel reversetranslation questions that achieve a target difficulty , allowing adaptive education methods to expand beyond limited question pools .
Limitations of our approach include the compute constraints of large LMs and training data availability .
More detailed student data will be crucial to future model development .
For instance , while most publicly available education datasets do not include the full student responses ( e.g. full translation response in Duolingo ) , such information could significantly improve the performance of our LM - KT model .
Other future directions include exploring non-language domains , such as math or logic exercises , and controlling for auxiliary objectives such as question topic .
Finally , designing appropriate user studies to evaluate our method is a complex yet critical next step to determine its suitability in a real-world education setting .
Our techniques allows control for individual student difficulty , but it leaves open the question of optimal curriculum design using difficulty - directed question generation .
