title
Multi-Scale Progressive Attention Network for Video Question Answering
abstract
Understanding the multi-scale visual information in a video is essential for Video Question Answering ( VideoQA ) .
Therefore , we propose a novel Multi-Scale Progressive Attention Network ( MSPAN ) to achieve relational reasoning between cross-scale video information .
We construct clips of different lengths to represent different scales of the video .
Then , the cliplevel features are aggregated into node features by using max -pool , and a graph is generated for each scale of clips .
For cross-scale feature interaction , we design a message passing strategy between adjacent scale graphs , i.e. , topdown scale interaction and bottom - up scale interaction .
Under the question 's guidance of progressive attention , we realize the fusion of all-scale video features .
Experimental evaluations on three benchmarks : TGIF - QA , MSVD - QA and MSRVTT - QA show our method has achieved state - of - the - art performance .
Introduction Video Question Answering ( VideoQA ) is a popular vision - language task , which focuses on predicting the correct answer to a given natural language question based on the corresponding video .
VideoQA task entails representing video features in both spatial and temporal dimensions .
Compared with the visual features of a picture in Visual Question Answering , it requires a more complex attention .
Therefore , ( Jang et al. , 2017 ) employed appearance features and motion features as video representation , and designed a dual - LSTM network based on spatio-temporal attention to fuse visual and text information .
Next , memory networks are widely used to capture long-term dependencies .
For example , ( Cai et al. , 2020 ) applied feature augmented memory to strengthen the information augmentation of video and text .
Complex relational reasoning is important for VideoQA task .
Consequently , a conditional relationship network ( Le et al. , 2020 ) Question : who is cleaning in a kitchen while wearing gloves ?
Answer : woman time Video : two - frame clip one - frame clip three - frame clip was designed in previous work , which can support high-order relationships and multi-step reasoning .
Many methods complete this task from a certain aspect , however , none of them have a fine- grained understanding of video information .
When looking for the answer in a question - based video , the video frames corresponding to different objects in the question are of different lengths .
As shown in Fig. 1 , when asked " who is cleaning in a kitchen while wearing gloves ? " , we need to find the keywords " cleaning " , " a kitchen " and " wearing gloves " from different levels of clips .
Previous methods searched for the answer on the same level of clips in a video , leading to insufficient or redundant information .
Firstly , we construct clips of different lengths from the frame sequence , and regard the length of a clip as its scale information .
Then , multi-scale graphs are generated separately for clips of different scales .
The nodes in the multi-scale graphs indicate video features corresponding to different clips .
For implementing relational reasoning , the nodes in each scale graph are first updated by using graph convolution .
Most importantly , under the guidance of the question , progressive attention has been utilized to enable the fusion of multi-scale features during cross-scale graph interaction .
In detail , each graph is gradually updated in top-down scale order , followed by updating each graph in bottom - up scale order .
Finally , node features of a graph are fused with question embedding , and a classifier is employed to find the answer .
2 Method
An overview of the proposed MSPAN is shown in Fig. 2 .
The input is a short video and a question sentence , while the output is the produced answer .
Video and Question Representation Video representation N frames are uniformly sampled to represent the video .
Then we use the pre-trained ResNet - 152 ( He et al. , 2016 ) to extract video appearance features for each frame .
And , we apply the 3D ResNet - 152 ( Hara et al. , 2018 ) pre-trained on Kinetics - 700 ( Carreira et al. , 2019 ) dataset to extract video motion features .
Specifically , 16 frames around each frame are placed into the 3D ResNet - 152 to obtain the motion features around this frame .
Finally , we get a joint video representation by concatenating appearance features and motion features .
By using a fully - connected layer to reduce feature dimension , we obtain video representation as V = { v i : i ?
N , v i ?
R 2048 }. Question representation
All words in question are represented as 300 - dimensional embeddings initialized with pre-trained GloVe vectors ( Pennington et al. , 2014 ) .
And a 512 - dimensional question embedding is generated from the last hidden state of a three - layer BiLSTM , i.e. , q ? R 512 .
Multi-Scale Graphs Generation
Each object in the video corresponds to a different number of frames , but previous methods ( Seo et al. , 2020 ; Lei et al. , 2021 ) cannot treat various levels of visual information separately .
Therefore , we construct clips of different lengths to express the visual information in the video delicately , and regard the length attribute as a scale .
We use max-pools of different kernel-sizes to aggregate frame- level visual features , and kernelsize is the scale attribute of these clips .
In this way , clip-level visual features are obtained , as follows : P = { pool i | 1 ? i ?
K , kernel size i = i} ( 1 ) V i = P i ( v 1 , v 2 , ? ? ? , v N ) ( 2 ) Where K is the range of scales , and K ? N .
Thus , we construct M i = N ? i + 1 clips at scale i : V i = {v i j : 1 ? j ?
M i , v i j ? R 2048 } ( 3 ) In order to reason the relationships between different objects in a video , we separately build a graph for each scale .
Each node in a graph represents the clip-level visual features .
Only when two nodes contain overlapping or adjacent frames , an edge will be connected between them .
Frame interval of the j-th clip at scale i is [ j , j + i ? 1 ] , so all edges in the K graphs can be expressed as : E i = {( x , y ) |x ? i ? y ? x + i} ( 4 ) Finally , these multi-scale graphs constructed in this paper can be denoted as G i = { V i , E i }.
Cross - Scale Feature Interaction
Before cross-scale feature interaction , the original node features of K graphs are copied as V o i = V i .
Interaction at the same scale .
For all nodes with the same scale , we apply a two -layer graph convolutional network ( GCN ) ( Kipf and Welling , 2017 ) to perform relational reasoning over the K graphs .
The process of graph convolution is represented as : X l+1 = D? 1 2 ? D? 1 2 X l W l ( 5 ) Where ? is the input adjacency matrix , X l is the node feature matrix of layer l , and W l is the learnable weight matrix .
The diagonal node degree matrix D is used to normalize ?.
Due to the small number of nodes in each graph , we decide to share the weight matrix W l when K graphs are updated .
Interaction at top-down scale .
We realize the interaction of adjacent scale graphs from small scale to large scale .
Therefore , visual information is understood step by step from details to the whole through the interaction of top-down scale .
Guided by the question , the nodes in graph G i are used to update the nodes in graph G i+ 1 .
Visual features at different scales show hierarchical attention to the question , so we call it progressive attention .
If the clip corresponding to node x in graph G i has the same frames as the clip corresponding to node y in graph G i + 1 , there will exist a directed edge from x to y .
Therefore , we can use the edge to fuse the cross-scale features of these same frames .
Firstly , visual features and question embedding are fused to capture the joint features of each node in graph G i .
Then , the process of message passing from graph G i to graph G i+1 can be expressed as : m xy = ( W 1 v i+ 1 y ) ?
( ( W 2 v i x ) ( W 3 q ) ) T ( 6 ) Where ? is the outer product , is the hadamard product .
After receiving the delivery messages , the attention weights of these messages are calculated : w xy = sof t max x?Ny ( m xy ) ( 7 )
Where N y is the set of all neighbor nodes in graph G i through cross-scale edges .
Consequently , all the messages passed into node y are summed to derive the update of node y , as follows : ? i+1 y = x?Ny w xy ? ( ( W 4 v i x ) ( W 5 q ) ) ( 8 ) V u i+1 = {?
i+ 1 y : y ? M i+1 , ?i+1 y ? R 2048 } ( 9 ) When updating all nodes in graph G i + 1 , we consider the new features V u i+1 and the original features V o i + 1 .
Therefore , we use the residual connection to preserve original information of the video : V i+1 = W 6 [ V i+1 ; V u i+1 ] + V o i + 1 ( 10 ) Where [ ; ] is the concatenation operator .
Above W 1 ? W 6 are learnable weights , and they are shared in the update of graphs G 2 ? G K .
To summarize , the update of K ?
1 graphs is a progressive process from small scale to large scale , hence it is referred to as top-down scale interaction .
Interaction at bottom - up scale .
After an overall understanding of the video , people can accurately find all details related to the question at the second time they watch the video .
Therefore , we achieve an understanding of the video from global to local through bottom - up scale interaction .
After the previous interaction , we realize the interaction of adjacent graphs from large scale to small scale .
Following the same method as top-down scale interaction from Eq. 6 to Eq. 10 , we apply graph G i to update graph G i?1 in this interaction .
But the weights W 1 ? W 6 are another group in the update of graphs G K?1 ? G 1 . After this interaction , graph G 1 can grasp the all-scale video features related to the question by progressive attention .
Multimodal Fusion and Answer Decoder After T iterations of cross-scale feature interaction , we read out all the nodes in graph G 1 .
Then , a simple attention is used to aggregate the N nodes .
And , final multi-modal representation is given as : wj = sof t max ( W 7 ( W 8 v 1 j ) ( W 9 q ) ) ( 11 ) F = N j=1 wj ? v 1 j ( 12 ) F = ELU ( W 10 F W 11 q + b ) ( 13 )
Where ELU is activation function , above W 7 ? W 11 are learnable weights and b is learnable bias .
We can find the answer by applying a classifier ( two fully - connected layers ) on multi-modal representation F . Multi-label classifier is applied to open-ended tasks , and cross-entropy loss function is used to train the model .
Due to repetition count is a regression task , we use the MSE loss function .
For the multi-choice task , each question corresponds to R answer sentences .
We first get the embedding of each answer in the same way as the question embedding .
Then we use the multi-modal fusion method in Eq. 11?13 to fuse the answer embedding with node features .
After using two fully - connected layers , the answer scores {s i } R 3 Experiments 3.1 Datasets TGIF - QA ( Jang et al. , 2017 ) is a widely used largescale benchmark dataset for VideoQA .
And four task types are covered in this dataset : repeating action ( Action ) , repetition count ( Count ) , video frame QA ( FrameQA ) and state transition ( Trans . ) .
MSVD -QA ( Xu et al. , 2017 ) and MSRVTT - QA ( Xu et al. , 2016 ) are open-ended tasks which are generated from video descriptions .
In both datasets , questions can be divided into 5 types according to question words : what , who , how , when and where .
Implementation Details
We evenly sample N = 16 frames for each video in the three datasets .
The hyperparameters we set in experiments are as follows : T = 3 , K = 8 .
When training the network , Adam is used with an initial learning rate of 10 ?4 . For TGIF - QA dataset , the batch size is 64 .
While the batch size is set to 128 for both MSVD - QA and MSRVTT - QA datasets .
Results
We compare our MSPAN with the state - of- the - art methods : PSAC ( Li et al. , 2019 ) , HME ( Fan et al. , 2019 ) , FAM ( Cai et al. , 2020 ) , LGCN ( Huang et al. , 2020 ) , HGA ( Jiang and Han , 2020 ) , QueST and HCRN ( Le et al. , 2020 ) . 1 , our method outperforms the state - of - the - art methods by 2.5 % and 1.9 % of accuracy on Action and Transition tasks .
For the Count task , our method also achieves the best Mean Square Error ( MSE ) of 3.57 among all methods .
Due to QueST used multidimension visual features containing more appearance information , our method can only get the same accuracy 59.7 % as QueST on the FrameQA task .
All in all , our method makes sense of the multiscale information of the video , so that the effect on tasks related to action recognition , temporal relationship and object count are very noticeable .
Results on MSVD -QA .
As shown in Table 2 , our method improves the overall accuracy by 4.2 % compared to recent methods .
We have achieved the best accuracy on questions whose question words are " What " , " Who " , " When " and " Where " .
Due to a small proportion , the accuracy on the question word " How " is lower than other methods .
Results on MSRVTT -QA .
As shown in Table 3 , our method achieves the best overall accuracy of 37.8 % .
What 's more , Our method could obtain excellent accuracy on different question words .
Ablation Studies
To explore the potential of our network , ablation experiments are performed on TGIF - QA dataset .
Default hyperparameters are : T = 3 and K = 8 .
We study the effectiveness of our network in the next two aspects , as shown in Table 4 and Fig. 4 .
Different Structures
Considering the interaction of cross-scale graphs , three structures are designed , as shown in Fig.
3 . For the dense scale in Fig. 3 ( a ) , we apply graphs G 1 ?
G K to update each graph G i .
The other two structures have been introduced in Sec 2.3 , and we will not use a graph to update itself for the three 977 structures .
The readout of top-down scale interaction is graph G K , and the readout of bottom - up scale interaction is G 1 .
However , the readout of dense scale interaction is all K graphs .
Our network is a combination of top-down scale interaction and bottom - up scale interaction , but we will use these two structures separately for comparison .
Network structure
When choosing the pooling function to aggregate these frames in a clip , we find that max -pool is more effective than avg-pool .
In reverse gradient propagation of max-pool , only the maximum of features in the previous layer receive the gradient .
So , max -pool facilitates the fusion of appearance features and motion features in the previous layer .
Our experiments show that GCN is beneficial to the stable training of models .
If there is no GCN , the gradient will gradually disappear as the number of interactions between the graphs increases .
The role of GCN is to re-recover the features of these nodes which have lost their visual features .
As shown in Table 4 , the performances of the three structures in Fig. 3 are poorer than that of our entire network .
Due to dense connections between all scale graphs , the dense scale interaction will add much unnecessary computation , and make it difficult to accurately find the visual information related to the question .
Although both the top-down scale interaction and the bottom - up scale interaction can achieve good performance .
However , the combination of these two structures will obtain a more detailed understanding of the video .
Hyperparameters
T and K As the number of iterations T increases , the model will achieve better performance .
But when T = 4 , the effect of the model decreases , as shown in Table 4 . Because too many modules will produce noise for answer generation .
The improvement of models with the increase of K is very obvious , and best performance is obtained when K = 8 , as shown in Fig.
4 .
However , the larger K also means that many multi-scale graphs , which will lead to network instability .
Conclusion
We introduce a multi-scale learning method to achieve a fine-grained understanding of the video .
Compared with existing spatio-temporal attention , we use progressive attention to realize cross-scale feature interaction .
The top-down and bottom - up structures we have designed are conducive to learning all-scale visual information of the video .
For longer videos , we plan to use dilated max-pools with different strides to reduce the size of graphs .
In general , we consider the VideoQA task from the perspective of multi-scale information interaction , and the proposed network is instructive .
Figure 1 : 1 Figure 1 : Understanding the video and answering the question require different levels of clips .
