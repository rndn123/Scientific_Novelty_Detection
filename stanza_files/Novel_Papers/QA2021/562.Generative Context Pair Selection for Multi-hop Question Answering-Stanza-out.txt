title
Generative Context Pair Selection for Multi-hop Question Answering
abstract
Compositional reasoning tasks such as multihop question answering require models to learn how to make latent decisions using only weak supervision from the final answer .
Crowdsourced datasets gathered for these tasks , however , often contain only a slice of the underlying task distribution , which can induce unanticipated biases such as shallow word overlap between the question and context .
Recent works have shown that discriminative training results in models that exploit these underlying biases to achieve a better held - out performance , without learning the right way to reason .
We propose a generative context selection model for multi-hop QA that reasons about how the given question could have been generated given a context pair and not just independent contexts .
We show that on HotpotQA , our proposed generative passage selection model , while being comparable to the state - of - the - art answering performance , has a better performance ( 4.9 % higher than baseline ) on an adversarial held - out set that tests the robustness of model 's multi-hop reasoning capabilities .
Introduction Recently many reading comprehension datasets like HotpotQA ( Yang et al. , 2018 ) and Wiki-Hop ( Welbl et al. , 2018 ) that require compositional reasoning over several disjoint passages have been introduced .
This style of compositional reasoning , also referred to as multi-hop reasoning , first requires finding the correct set of passages relevant to the question and then finding the answer span in the selected set of passages .
These dataset are often collected via crowdsourcing , which makes the training and evaluation of such models heavily reliant on the quality of the collected held - out sets .
Crowdsourced datasets , however , often present only a partial picture of the underlying data distribution .
Learning complex latent sequential decisions , like in multi-hop reasoning , to answer a given question under such circumstances is marred by numerous biases , such as annotator bias ( Geva et al. , 2019 ) , label bias ( Dua et al. , 2020 ; Gururangan et al. , 2018 ) , survivorship bias ( Min et al. , 2019 b ; Jiang and Bansal , 2019 ) , and ascertainment bias ( Jia and Liang , 2017 ) .
As a result , testing model performance on such biased held - out sets becomes unreliable as the models exploit these biases and learn shortcuts to get the right answer but without learning the right way to reason .
Consider an example from HotpotQA in Figure 1 , where the latent entity " Virgina Commonwealth University " can be used by the model ( Jiang and Bansal , 2019 ) to bridge the two relevant passages ( highlighted in green ) from the original dev set and correctly predict the answer " 1838 " .
However , upon adding an adversarial context ( highlighted in pink ) to the pool of contexts , the model prediction changes to " 1938 " implying that the model did not learn the right way to reason .
This is because the discriminatively trained passage selector exploits lexical cues like " founded " in the second passage and does not pay attention to the complete question .
The absence of such adversarial contexts at training allows the model to find incorrect reasoning paths .
In this work , we propose a generative context pair selection model that reasons through the data generation process of how a specific question could have been generated given pair of passages .
We show that our proposed passage selection module has a better performance on the original ( + 2.2 % ) and the adversarial dev set ( + 4.9 % ) that tests the model 's reasoning abilities ( unlike the original dev set which is marred by bias ) .
We use a generic answering model and show that while being comparable in end-to - end performance with close to state - of - the - art systems on the original dev set , our model provides a better performance on the adversarial set .
Any advances in the answering model can be applied in a straightforward manner to a generative passage selector to further improve performance .
Generative Passage Selection Given a set of contexts C = {c 0 , ... , c N } , the goal of multi-hop question answering is to combine information from C to an identify answer span a for a given question q. Let ? = {( c i , c j ) = c i j : c i ?
C , c j ? C ) } be the set of all possible context pairs that can be formed from C.
Existing models for multi-hop question answering ( Tu et al. , 2020 ; consist of two components : a discriminative passage selection and an answering model .
Passage selection identifies which pairs of contexts are relevant for answering the given question , i.e. , it estimates p( c i j | q , ? ) .
This is followed by the answering model to extract the answer span given a context pair and the question ( p( a | q , c i j ) ) .
These are combined as follows : p( a | q , ? ) = c i j p( a | q , c i j ) p( c i j | q , ? ) ( 1 )
The discriminative passage selector learns to select a set of contexts by conditioning on the question representation .
This learning process does not encourage the model to pay attention to the entire question , which can result in ignoring parts of the question , and thus , learning spurious correlations .
For prediction , best context pair c * i j is used by the answering module to get the answer , a * = argmax p( a | q , c * i j ) .
As shown by Min et al . ( 2019a ) , using the top scoring reasoning chain to answer the question is often sufficient and does not require marginalization over multiple chains .
1
Proposed Model
We propose a joint question - answering model that learns p( a , q | ? ) instead of p( a | q , ? ) .
This model can be factorized into a generative passage selector and a standard answering model as : p( a , q | ? ) = c i j p( a | q , c i j ) p ( q| c i j ) p( c i j | ? ) ( 2 ) A prior p( c i j |? ) over the context pairs establishes a measure of compatibility between passages in a particular dataset .
The conditional generation model p( q|c i j ) estimates the likelihood of generating the given question from a selected pair of passages .
Finally , a standard answering model p( a | q , c i j ) learns a likely answer distribution given a question and context pair .
The first two terms ( prior and conditional generation ) can be seen as a generative model that selects a pair of passages from which the question could have been constructed .
The answering model can be instantiated with any existing SOTA model , like graph neural network ( Tu et al. , 2020 ; Shao et al. , 2020 ) and entity - based chain reasoning .
The process at prediction is identical to that with discriminative passage selection , except that the context pairs are scored by taking the entire question into account , c * i j = argmax c i j p( q| c i j ) p( c i j |? ) .
Model Learning
For learning the generative model , we train the prior , p( c i j |? ) and the conditional generation model p( q | c i j , ? ) jointly .
First , the prior network projects the concatenated contextualized representation , r i j , of starting and ending token of concatenated contexts ( c i ; c j ) , from the encoder to obtain un-normalized scores , which are then normalized across all context-pairs via softmax operator .
The loss function tries to increases the likelihood of gold context pair over all possible context pairs .
r i j = encoder ( c i ; c j ) ( 3 ) s i j = W 1?d ( r i j [ start ] ; r i j [ end ] ) ( 4 )
The conditional question generation network gets contextual representations for context - pair candidates from the encoder and uses them to generate the question , via the decoder .
The objective function increases the likelihood of the question for gold context pairs and the unlikelihood ( Welleck et al. , 2020 ) for a set of negative context pairs ( Eq. 5 ) .
The negative context pairs are randomly sampled from all possible non-oracle context pairs .
L ( ? ) = | question | t=1 log p( q t | q <t , c gold ) + n?|neg.pairs | | question | t=1 log ( 1 ? p( q t | q <t , c n ) ) ( 5 )
Experiments and Results
We experiment with two popular multi-hop datasets : HotpotQA ( Yang et al. , 2018 ) and Wik-iHop ( Welbl et al. , 2018 ) .
We use a pre-trained T5 ( Raffel et al. , 2019 ) ( Tu et al. , 2020 ) .
Table 1 compares end-to - end original dev set performance of answering model when combined with a standard ( 79.5 F 1 ) and a generative passage selector ( 81.9 F 1 ) for HotpotQA .
Table 2 shows minor improvements in passage accuracy on using generative selector in WikiHop .
This shows that generative passage selecetor is able to find ( latent ) entity connections between context pairs that are consistent with the complete question and not just parts of it .
Adversarial Evaluation
We use an existing adversarial set ( Jiang and Bansal , 2019 ) for HotpotQA to test the robustness of model 's multi-hop reasoning capabilities given a confusing passage .
This helps measure , quantitatively , the degree of biased correlations learned by the model .
In Table 1 , we show that the standard discriminative passage selector has a much higher performance drop ( ? 4 % ) as compared to the generative selector ( ? 1 % ) on adversarial dev set ( Jiang and Bansal , 2019 ) , showing that generative selector is less biased and less affected by conservative changes ( Ben - David et al. , 2010 ) to the data distribution .
While the end to end QA performance of our model is comparable ( ? 0.3 F1 ) to Fang et al. ( 2020 ) on the original dev-set , on the adversarial set our method is better than Fang et al . ( 2020 ) ( ? 1.2 F1 ) .
Table 3 shows that the decoder of generative passage selector was able to generate multi-hop style questions from a pair of contexts .
Context pairs vs. Sentences
Some context selection models for HotpotQA use a multi-label classifier that chooses top-k sentences ( Fang et al. , 2020 ; Clark and Gardner , 2018 ) which result in limited inter-document interaction than context pairs .
To compare these two input types , we construct a multi-label sentence classifier p( s|q , C ) that selects relevant sentences .
This classifier projects a concatenated sentence and question representation , followed by a sigmoid , to predict if the sentence should be selected .
This model has a better performance over the context - pair selector but is more biased ( Table 4 ) .
We performed similar experiments with the generative model , where we train a generative sentence selection model by first selecting a set of sentences with a gumbel softmax ( prior ) and then generating The Vermont Catamounts men's soccer team represents the University of Vermont in all NCAA Division I men's college soccer competitions .
The team competes in the America East Conference .
Original Question , q : the vermont catamounts men's soccer team currently competes in a conference that was formerly known as what from 1988 to 1996 ?
Generated Questions : p( q | c i j , ? ) the vermont catamounts men's soccer team competes in what collegiate athletic conference affiliated with the ncaa division i , whose members are located mainly in the northeastern united states ?
the vermont catamounts men's soccer team competes in a conference that was known as what from 1979 to 1988 ?
the vermont catamounts men's soccer team competes in a conference that was known as what from 1988 to 1996 ?
the question given the set of sentences .
Given that the space of set of sentences is much larger than context pairs , the generative sentence selector does not have good performance ( Table 4 ) .
Since sentence selection helped improve performance of the discriminative passage selector , we add an auxiliary loss term to our generative passage selector that also predicts the relevant sentences in the context pair when generating the question ( p( q , s|c i j , ? ) ) , in a multi-task manner .
We see slight performance improvements by using relevant sentences as an additional supervision signal .
Related work Many recent passage selection models for Hot-potQA and Wikihop 's distractor style setup employ discriminative context selectors given the question ( Tu et al. , 2020 ; Fang et al. , 2020 ; Shao et al. , 2020 ) .
The high performance of such passage selectors can be attributed to existing bias in Hot-potQA ( Jiang and Bansal , 2019 ; Min et al. , 2019 b ) , which allows shallow lexical overlap of question with a single context to result in the correct answer .
Another more general line of work dynamically updates the working memory to re-rank the set of passage at each hop ( Das et al. , 2019 ) .
With the release of datasets like SearchQA ( Dunn et al. , 2017 ) , TriviaQA ( Joshi et al. , 2017 ) , and NaturalQuestions ( Kwiatkowski et al. , 2019 ) , a lot of work has been done in open-domain passage retrieval , especially in the full Wikipedia setting .
However , these questions do not necessarily require multi-hop reasoning .
A series of work has tried to match a document- level summarized embedding to the question ( Seo et al. , 2018 ; for obtaining the relevant answers .
In generative question answering , a few works ( Lewis and Fan , 2019 ; Nogueira dos Santos et al. , 2020 ) have used a joint question answering approach on a single context .
A large body of work has employed simple question generation for factoid answers in numerous cases , like answer verification ( Duan et al. , 2017 ) , fact checking ( Fan et al. , 2020 ) , data augmentation Serban et al. , 2016 ) , pedagogical systems ( Lindberg et al. , 2013 ) , and dialog systems ( Yanmeng et al. , 2020 ) etc .
Conclusion
We proposed a generative formulation of context pair selection for multi-hop question answering .
By encouraging this selection model to explain the entire question , it is less susceptible to bias , performing substantially better on adversarial data than existing discriminative methods .
Our proposed model is simple to implement and can be used with any existing ( or future ) answering model ; we will release code to support this integration .
Since context pair selection scales quadratically with the number of contexts , it is not ideal for scenarios that involve a large number of possible contexts .
However , it allows for deeper inter-document interaction as compared to other approaches that use summarized document representations .
With more reasoning steps , selecting relevant documents given only the question becomes challenging , increasing the need for inter-document interaction .
An easy way to reduce the computation cost is to consider only a set of top-k contexts and perform a two -stage coarse- to -fine passage selection .
The generative story presented in the paper may not work for question types beyond the datasets considered , for eg. , in case of multiple ( > 1 ) bridge entities or more than two contexts .
However , we demonstrate that this idea works for most common reasoning types that are central in current multihop reasoning datasets .
The code is available at https://github.com/dDua/JointQA
