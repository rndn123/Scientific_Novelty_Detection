title
Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering
abstract
Coupled with the availability of large scale datasets , deep learning architectures have enabled rapid progress on Question Answering tasks .
However , most of those datasets are in English , and the performances of state - of - theart multilingual models are significantly lower when evaluated on non-English data .
Due to high data collection costs , it is not realistic to obtain annotated data for each language one desires to support .
We propose a method to improve Crosslingual Question Answering performance without requiring additional annotated data , leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion .
We show that the proposed method allows to significantly outperform the baselines trained on English data only , establishing thus a new state - of - the - art on four multilingual datasets : MLQA , XQuAD , SQuAD - it and PIAF ( fr ) .
* * : equal contribution .
The work of Arij Riabi was partly carried out while she was working at reciTAL .
Introduction Question
Answering is a fast- growing research field , aiming to improve the capabilities of machines to read and understand documents .
Significant progress has recently been enabled by the use of large pre-trained language models Raffel et al. , 2020 ) , which reach human-level performances on several publicly available benchmarks , such as SQuAD ( Rajpurkar et al. , 2016 ) and NewsQA ( Trischler et al. , 2017 ) .
Given that the majority of large scale Question Answering ( QA ) datasets are in English ( Hermann et al. , 2015 ; Rajpurkar et al. , 2016 ; Choi et al. , 2018 ) , the development of QA systems targeting other languages is currently addressed via two cross-lingual QA datasets : XQuAD ( Artetxe et al. , 2020 ) and MLQA ( Lewis et al. , 2020a ) , covering respectively 10 and 7 languages .
Due to the cost of annotation , both are limited only to an evaluation set .
They are comparable to the validation set of the original SQuAD ( see more details in Section 3.3 ) .
In both datasets , each paragraph is paired with questions in various languages , allowing to evaluate models in a cross-lingual experimental scenario : the input context and the question can be in two different languages .
This scenario has important practical applications , such as querying a set of documents in various languages .
Performing this cross-lingual task is complex and remains challenging for current models , assuming only English training data : transfer results are shown to rank behind training - language performance ( Artetxe et al. , 2020 ; Lewis et al. , 2020a ) .
In other words , multilingual models fine-tuned only on English data are found to perform significantly better on English than on other languages .
Besides the almost simultaneous work of Shakeri et al . ( 2020 ) , very few alternatives to such a simple zeroshot transfer method have been proposed so far .
In this paper , we propose to generate synthetic data in a cross-lingual fashion , borrowing the idea from monolingual QA research efforts ( Duan et al. , 2017 ) .
On English corpora , generating synthetic questions has shown to significantly improve the performance of QA models ( Du et al. , 2017 ; Golub et al. , 2017 ; Du and Cardie , 2018 ; .
However , the adaptation of this technique to cross-lingual QA is not straightforward : crosslingual text generation is a challenging task per se which has not been yet extensively explored , in particular when no multilingual training data is available .
We explore two Question Generation scenarios : ( i ) requiring only SQuAD data ; and ( ii ) using a translator tool to obtain translated versions of SQuAD .
As expected , the method leveraging on a translator has shown to perform the best .
Leveraging on such synthetic data , our best model obtains significant improvements on XQuAD and MLQA over the state - of - the - art for both Exact Match and F1 scores .
In addition , we evaluate the QA models on languages not seen during training ( even for the synthetic data ) - using SQuAD - it ( for Italian ) , PIAF ( for French ) , and KorQUaD ( for Korean ) - reporting a new state - of - the - art for Italian and French , and observing significant improvements on Korean compared to zero-shot without augmentation .
This indicates that the proposed method allows to capture better multilingual representations beyond the training languages .
Our method paves the way toward multilingual QA domain adaptation , especially for under-resourced languages .
Our contributions can be summarized as follows : ?
We present a data augmentation approach for Cross-Lingual Question Answering based on synthetic Question Generation ; Simmons ( 1965 ) reported fifteen implemented English language question - answering systems .
More recently , with the rise of large scale datasets ( Hermann et al. , 2015 ) , and large pre-trained models , the performance drastically increased , approaching human-level performance on standard benchmarks - see for instance the SQuAD leader board .
1 More challenging evaluation benchmarks have recently been proposed : Dua et al. ( 2019 ) released the DROP dataset , for which the annotators were encouraged to provide adversarial questions ; Burchell et al. ( 2020 ) released the MSQ dataset , consisting of multi-sentence questions .
However , all these works are focused on English .
Another popular research direction focuses on the development of multilingual QA models .
For this purpose , the first step has been to provide the community with multilingual evaluation sets : Artetxe et al. ( 2020 ) and Lewis et al . ( 2020a ) concurrently proposed two different evaluation sets which are comparable to the SQuAD development set .
Both reach the same conclusion : due to the lack of non-English training data , models do not achieve the same performance in Non-English languages than they do in English .
To the best of our knowledge , no method has been proposed to fill this gap .
Question Generation ( QG ) QG can be seen as the dual task of QA : the input is composed of the answer and the paragraph containing it , and the model is trained to generate the question .
Proposed by Rus et al . ( 2010 ) , it has leveraged on the development of new QA datasets Scialom et al. , 2019 ) .
Similar to QA , significant performance improvements have been obtained using pre-trained language models ( Dong et al. , 2019 ) .
Still , due to the lack of multilingual datasets , most previous works have been limited to monolingual text generation .
We note the exceptions of Kumar et al . ( 2019 ) and , who resorted to multilingual pre-training before fine-tuning on monolingual downstream NLG tasks .
However , the quality of the generated questions is still found inferior to the corresponding English ones .
Question Generation for Question Answering Data augmentation via synthetic data generation is a well - known technique to improve models ' accuracy and generalisation .
It has found successful application in several areas , such as time series analysis ( Forestier et al. , 2017 ) and computer vision ( Buslaev et al. , 2020 ) .
In the context of QA , generating synthetic questions to complete a dataset has shown to improve QA performances ( Duan et al. , 2017 ; .
So far , all these works have focused on English QA given the difficulty to generate questions in other languages without available data .
This lack of data , and the difficulty to obtain some , constitutes the main motivation of our work and justifies exploring cost-effective approaches such as data augmentation via the generation of questions .
In a very recent work , almost simultaneous to our previously submitted version , Shakeri et al . ( 2020 ) address multilingual QA with a similar approach .
However , we argue that their experimental protocol does not allow to totally answer the research question .
We detail the differences in our discussion , Section 5.3 .
Data
English Training Data SQuAD en
The original SQuAD ( Rajpurkar et al. , 2016 ) , which we refer as SQuAD en for clarity in this paper .
It is one of the first , and among the most popular , large scale QA datasets .
It contains about 100K question / paragraph / answer triplets in English , annotated via Mechanical Turk .
2 QG datasets
Any QA dataset can be reversed into a QG dataset , by switching the generation targets from the answers to the questions .
In this paper , we use the qg subscript to specify when the dataset is used for QG ( e.g. SQuAD en;qg indicates the English SQuAD data in QG format ) .
Synthetic Training Sets SQuAD trans is a machine translated version of the SQuAD train set in the seven languages of MLQA , released by the authors together with their paper .
WikiScrap
We collected 500 Wikipedia articles for all the languages present in MLQA .
They are not paired with any question or answer .
We use them as contexts to generate synthetic multilingual questions , as detailed in Section 4.2 .
Following the SQuAD en protocol , we used project Nayuki 's code 3 to parse the top 10 K Wikipedia pages according to the PageRank algorithm ( Page et al. , 1999 ) .
We then filtered out paragraphs with character length outside of a [ 500 , 1500 ] interval .
Articles with less than 5 paragraphs are discarded , since they tend to be less developed , in a lower quality or being only redirection pages .
Out of the filtered articles , we randomly selected 500 per language .
Multilingual Evaluation Sets XQuAD ( Artetxe et al. , 2020 ) is a human translation of the SQuAD en development set in 10 languages ( Arabic , Chinese , German , Greek , Hindi , Russian , Spanish , Thai , Turkish , and Vietnamese ) , providing 1 k QA pairs for each language .
MLQA ( Lewis et al. , 2020a ) is an evaluation dataset in 7 languages ( English , Arabic , Chinese , German , Hindi , and Spanish ) .
The dataset is built from aligned Wikipedia sentences across at least two languages ( full alignment between all languages being impossible ) , with the goal of providing natural rather than translated paragraphs .
The QA pairs are manually annotated on the English sentences and then human translated on the aligned sentences .
The dataset contains about 46 k aligned QA pairs in total .
Language-specific benchmarks
In addition to the two aforementioned multilingual evaluation corpora , we benchmark our models on three languagespecific datasets for French , Italian and Korean , as detailed below .
We choose these datasets since none of these languages are present in XQuAD or MLQA .
Hence , they allow us to evaluate our models in a scenario where the target language is not available during training , even for the synthetic questions .
PIAF Keraron et al. ( 2020 ) provided an evaluation set in French following the SQuAD protocol , containing 3835 examples .
KorQuAD 1.0 the Korean Question Answering Dataset ( Lim et al. , 2019 ) , a Korean dataset also built following the SQuAD protocol .
SQuAD - it Derived from SQuAD en , it was obtained via semi-automatic translation to Italian ( Croce et al. , 2018 ) .
Models Recent works ( Raffel et al. , 2020 ; Lewis et al. , 2019 ) have shown that classification tasks can be framed as a text - to - text problem , achieving stateof - the - art results on established benchmarks , such as GLUE ( Wang et al. , 2018 ) .
Accordingly , we employ the same architecture for both Question Answering and Generation tasks .
This also allows fairer comparisons for our purposes , by removing differences between QA and QG architectures and their potential impact on the results obtained .
In particular , we use a distilled version of XLM -R ( Conneau et al. , 2020 ) : MiniLM -M ( see Section 4.3 for further details ) .
Baselines QA No-synth Following previous works , we finetuned the multilingual models on SQuAD en , and consider them as our baselines .
English as Pivot Leveraging on translation models , we consider a second baseline method , which uses English as a pivot .
First , both the question in language L q and the paragraph in language L p are translated into English .
We then invoke the baseline model described above , QA No-synth , to predict the answer .
Finally , the predicted answer is translated back into the target language L p .
We used the google translate API .
4 QA + SQuAD - trans the translated data SQuAD trans are used as additional training data to SQuAD en , to train the QA model .
Question Generation Data Augmentation
In this work we consider data augmentation via generating synthetic questions , to improve the QA performance .
Different training schemes for the question generator are possible , resulting in different quality of the synthetic data .
Before this work , its impact on the final QA system remained unexplored in a multilingual context .
For all the following experiments , only the synthetic data changes .
Given a specific set of synthetic data , we always follow the same two -stages protocol , similar to : we first train the QA model on the synthetic QA data , then on SQuAD en .
We also tried to train the QA model in one stage , with all the synthetic and human data shuffled together , but observed no improvements over the baseline .
We explored two different synthetic generation modes :
Synth the QG model is trained on SQuAD en , qg ( i.e. , English data only ) and the synthetic data are generated on WikiScrap .
Under this setup , the only annotated samples this model has access to are those from SQuAD -en .
Synth + trans the QG model is trained on SQuAD trans , qg in addition to SQuAD en , qg .
The questions can thus be in a different languages than the context .
Hence , the model needs an indication about the language it is expected to generate the question in .
To control the target language , we use a specific prompt per language , defining a special token < LANG > , which corresponds to the desired target language Y .
Thus , the input is structured as < LANG > < SEP >
Answer < SEP >
Context , where < LANG > indicates to the model in what language the question should be generated , and < SEP > is a special token acting as a separator .
These attributes offer flexibility on the target language .
Similar techniques are used in the literature to control the style of the output ( Keskar et al. , 2019 ; .
Implementation details
For all our experiments we use Multilingual MiniLM v1 ( MiniLM -m ) , a 12 - layer with 384 hidden size architecture distilled from XLM -R Base multilingual ( Conneau et al. , 2020 ) .
With only 66 M parameters , it is an order of magnitude smaller than state - of - the - art architectures such as BERT - large or XLM - large .
We used the official Microsoft implementation .
5 For all the experiments - both QG and QA - we trained the model for 5 epochs , using the default hyperparameters .
We used a single nVidia gtx2080 ti with 11G RAM , and the training times amount to circa 4 and 2 hours for Question Generation and for Question Answering , respectively .
To evaluate our models , we used the official MLQA evaluation scripts .
6
For reproducibility purposes , we make the code available .
7 5 Results
Question Generation
We report examples of generated questions in Table 1 .
Controlling the Target Language
In the context of multilingual text generation , controlling the target language is not trivial .
When a QA model is trained only on English data , at inference , given a non-English paragraph , it predicts the answer in the input language , as one would expect , since it is an extractive process .
Ideally , we would like to observe the same behavior for a Question Generation model trained only on English data ( such as Synth ) , leveraging on the multilingual pre-training .
Conversely to QA , QG is a language generation task .
Multilingual generation is much more challenging , as the model 's decoding ability plays a major role .
When a QG model is fine-tuned only on English data ( i.e SQuAD -en ) , its controllability of the target language suffers from catastrophic forgetting : the input language does not Paragraph ( EN ) Peyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls .
He is also the oldest quarterback ever to play in a Super Bowl at age 39 .
The past record was held by John Elway , who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denver 's Executive Vice President of Football Operations and General Manager .
Answer Broncos QG synth
What team did John Elway lead to victory at age 38 ?
QG synth+ trans ( target language = en )
What team did John Elway lead to win in the Super Bowl ?
Paragraph ( ES ) Peyton Manning se convirti ?
en el primer mariscal de campo de la historia en llevar a dos equipos diferentes a participar en m?ltiples Super Bowls .
Ademas , es con 39 a?os , el mariscal de campo m?s longevo de la historia en jugar ese partido .
El r?cord anterior estaba en manos de John Elway - m?nager general y actual vicepresidente ejecutivo para operaciones futbol ?
sticas de Denver-que condujo a los Broncos a la victoria en la Super Bowl XXXIII a los 38 a? os de edad .
Answer Broncos QG synth
Where did Peyton Manning condujo ?
QG synth+ trans ( target language = es ) Qu? equipo gan ?
el r?cord anterior ?
( Which team won the previous record ? )
QG synth+ trans ( target language = en )
What team did Menning win in the Super Bowl ?
Paragraph ( ZH ) ?39 ? ?38?33 ? ?
Answer ?
QG synth
What is the name for the name that the name is used ?
QG synth+ trans ( target language = zh ) ?13?33 ??
( Which team did John Elvey lead to win the 33rd Super Bowl at the age of 13 ? ) QG synth+ trans ( target language = en )
What team won the 33th Super Bowl ?
Table 1 : Example of questions generated by the different models on an XQuAD 's paragraph in different languages .
For QG synth + trans , we report the outputs given two target languages , the one of the context and English .
propagate to the generated text .
While still relevant to the context , the synthetic questions are generated in English : for instance , in Table 1 we observe that the QG synth model outputs English questions for the paragraphs in Chinese and Spanish .
The same phenomenon was reported by .
Cross-Lingual Training
To overcome the aforementioned limitation on target language controllability ( i.e. to enable the generation in other languages than English ) , multilingual data is needed .
We can leverage on the translated versions of the dataset to add the required non-English examples .
As detailed in Section 4.2 , we simply use a specific prompt that corresponds to the target language ( with N different prompts corresponding to the N languages present in the dataset ) .
In Table 1 , we show how QG synth + trans can generate questions in the same language as the input .
These synthetic questions seem much more relevant , coherent and fluent , if compared to those produced by QG synth : for the Spanish paragraph , the question is well formed and focused on the input answer ; for Chinese ( see bottom row of Table 1 for QG synth + trans ) is perfectly written .
In Table 2 we report the BLEU4 scores for QG synth + trans grouped by the language of the question .
As expected , the score is maximized on the q/c en es de ar hi vi zh en 14.5 8.9 7.2 5.9 6.5 8.4 6.0 es 9.0 10 . 6.6 4.2 5.9 diagonal ( same languages for the context and the question ) .
Still , most of these scores are lower on non-English languages .
It is interesting to note BLEU4 correlates with the QA scores : 0.51 Pearson coefficient .
The reasons are two folds : 1 ) QA and QG share the same Language Model , which might struggle for the same languages ; 2 ) the better the QG , the better the synthetic data , therefore the better the QA performs .
We discuss further in Section 5.3 how this impacts the QA performance .
In addition to BLEU , we also report the QA F1 scores for different QA models when applied on the generated questions in the supplementary material .
Yet , we warn the reader that these results should be taken with caution : evaluating NLG is known to be an open research problem ; BLEU is known to suffer from important limitations ( Novikova et al. , 2017 ) , which might be accentuated in a multilingual context .
For this reason , we conducted a manual qualitative analysis on a small number of samples .
Note that the annotators need to have a professional level in the language of the generated question to evaluate its fluency , and to be bilingual , when evaluating its relevance w.r.t. input context in our cross language scenario .
This is a significant challenge to conduct a large scale evaluation .
So far , our results ( see at the end of Supplementary Material ) for Arabic and German show an overall good quality in the questions : only one question for Arabic was genuinely missing the point while for German there were 2 lexical questionable choices that invalidate the question ( out of 10 samples for both languages so far ) .
This indicates that Arabic questions could actually be better than what their low BLEU score shows .
Arabic has a very different morphological structure that could explain such low BLEU ( Bouamor et al. , 2014 ) .
This emphasizes the limitation of the current automatic metrics in a multilingual context .
Question Answering
We report the main results of our experiments on XQuAD and MLQA in Table 3 .
The scores correspond to the average over all the different possible combination of languages ( de-de , de-ar , etc. ) .
English as Pivot Using English as a pivot does not lead to good results .
This may be due to the evaluation metrics , which are based on n-grams similarity .
For extractive QA , F1 and EM metrics measure the overlap between the predicted answer and the ground truth .
Therefore , meaningful answers worded differently are penalized , a situation that is likely to occur because of the backtranslation mechanism .
This makes automatic evaluation challenging for this setup , as metrics suffer from similar difficulties as those observed for text generation ( Sulem et al. , 2018 ) .
As an additional downside , this model requires multiple translations at inference time .
For these reasons , we decided not to explore this approach further .
Synthetic without translation ( + synth ) Compared to the MiniLM baseline , we observe a small performance increase for MiniLM +synth ( Exact Match increases from 29.5 to 33.1 on XQuAD and from 26.0 to 27.5 on MLQA ) .
During the self-supervised pre-training stage , the model was exposed to multilingual inputs .
Yet , for a given input , the target language was always consistent , preventing the model to be exposed to such a cross-lingual scenario .
The synthetic inputs are composed of questions in English ( see examples in Table 1 ) while the contexts can be in any languages .
Therefore , the QA model is exposed for the first time to a cross-lingual scenario .
We hypothesise that such a cross-lingual ability is not innate for a default multilingual model : exposing a model to this scenario allows to develop this ability and contributes to improve its performance .
Synthetic with translation ( + synth-trans ) : For MiniLM + synth-trans , we obtain a much larger improvement over its baselines , MiniLM , compared to MiniLM +synth , on both MLQA and XQuAD .
Also , it outperforms MiniLM + SQuADtrans , indicating the benefit of our proposed approach .
This supports the intuition developed in the previous paragraph : independently of the multilingual capacity of the model , a cross-lingual ability is developed when the two inputs components are not exclusively written in the same language .
In Section 5.3 , we discuss this phenomenon more in depth .
Discussion Cross Lingual Generalisation
To explore the models ' effectiveness in dealing with cross-lingual inputs , we report in Figure 1 the performance for our MiniLM +synth- trans setup , varying the number of samples and the languages present in the synthetic data .
The abscissa x corresponds to the progressively increasing number of synthetic samples used ; at x = 0 , it corresponds to the MiniLM + trans baseline , where the model has access only to the original English data from SQuAD en .
We explore two sampling strategies for the synthetic examples : 0 . All Languages corresponds to sampling the examples from any of the different languages .
0 . Conversely , for Not All Languages , we progressively added the different languages : for x = 50K , all the 50 K synthetic data are on a unique language input , L1 .
Then for x = 100K , the synthetic data are from either L1 , or an additional language L2 ; finally , for x = 250K , all MLQA languages are present .
In Figure 1 , we observe that the performance for All Languages increases largely at the beginning , then remains mostly stable .
Conversely , we note a gradual improvement for Not All Languages , as This shows that when all the languages are present in the synthetic data , the model immediately develops cross-lingual abilities .
However , it appears that even with only one language pair present , the model is able to develop a cross-lingual ability that brings benefits on other languages : of Figure 2 , we can see that most of the improvement is happening given only one crosslingual language pair ( i.e. English and Spanish ) .
Unseen Languages
To measure the benefit of our approach on unseen languages ( i.e. not present in the synthetic data from MLQA / XQuAD ) , we test our models on three QA evaluation sets : PIAF ( fr ) , KorQuAD and SQuAD - it ( see Section 3.3 ) .
The results are consistent with the previous experiments on MLQA and XQuAD .
Our MiniLM + synth-trans model outperforms its baseline by more than 4 Exact Match points , while XLM -R + synth-trans obtains a new state - of- the- art .
Notably , our multilingual XLM -R +synth-trans outperforms CamemBERT on PIAF , even if the latter is a pure monolingual , in - domain language model .
On the correlation between BLEU4 and QA scores
To measure the impact of the quality of the generated questions on the QA performance , we computed the Pearson correlation between the BLEU4 and the QA scores .
The coefficient is equal to 0.65 ( p < .001 ) .
When we observe the correlations grouping the samples w.r.t. their language question ( i.e. the rows in Table 2 ) , we obtain : en 0.94 ; es 0.84 ; de 0.46 ; ar 0.36 ; hi 0.33 ; vi 0.73 ; zh 0.92 .
We observe stronger correlation for languages with higher BLEU scores ( i.e en & zh ) , and lower for the Arab that had the lowest BLEU , indicating an impact on the final QA score in par to the quality of the synthetic questions .
Differences with Shakeri et al . ( 2020 )
A very recent work has addressed multilingual QA with a very similar approach .
However , we note a major difference in our respective experiments regarding the choice for the QA and QG models .
Shakeri et al. ( 2020 ) choose mBert for QA and T5 -m for QG .
We would like to emphasize that because T5 m significantly outperforms
mBert it is not clear where the improvement comes from : is it due to the proposed approach , or simply from a distillation effect from T5 -m to mBert ?
In our case , we Figure 1 : Left : F1 score on MLQA , for models with different number of synthetic data in two setups : for All Languages , the synthetic questions are sampled among all the five languages in MLQA ; for Not All Languages , the synthetic questions are sampled progressively from only one language , two , . . . , to all five for the last point , which corresponds to All Languages .
We report the standard deviation over five different permutations of the language ordering .
Note that , as expected , the more the synthetic data , the lower the variance in the results .
Right : same as on the left , but evaluated on XQuAD .
deliberately used MiniLM for both QA and QG : this allows a fairer investigation about the benefits of the proposed approach .
Hidden distillation effect
The relative improvement for our best synthetic configuration + synthtrans , over the baseline , is above 60 % EM for MiniLM ( from 29.5 to 49.5 on XQuAD and from 26.0 to 41.4 on MLQA ) .
Significantly higher than that observed for XLM -R ( + 11.7 % on XQuAD and + 2.71 % on MLQA ) , it indicates that XLM -R provides superior cross-lingual transfer abilities than MiniLM , a fact that we hypothesize due to distillation .
Such loss of generalisation can be difficult to identify , and opens questions for future work .
QA , an unsolved task for lower resource languages Factoid QA tasks have been criticized for being a too easy task : the answer can often be identified given simple heuristics : e.g. a " When " question is answered by one of the " date " spans in the context ( Ko?isk ?
et al. , 2018 ; Kwiatkowski et al. , 2019 ) . SQuAD - v2 was for instance introduced to increase the difficulty of the task by adding unanswerable questions .
The research community is now moving towards the construction of long context questions and non-factoid QA datasets ( Dulceanu et al. , 2018 ; Hashemi et al. , 2019 ; Fan et al. , 2019 ; Lewis et al. , 2020 b ) .
In any case , the motivation of this work was to cope for the lack of training data for under-served languages in the QA domain which was severely impacting models performance .
Therefore , potential criticisms regarding the simplicity of the task do not apply if seen from a lowerresource language scenario : our work deals with alleviating the lack of native training data , allowing us to focus our future work on further important issues such as domain adaptation , robustness and explainability in low-resource contexts .
Conclusion
In this work , we presented a method to generate synthetic QA dataset in a multilingual fashion , showing how QA models can benefit from it and reporting large improvements over the baselines .
The proposed approach contributes to fill the gap between English and other languages , and is shown to generalize for languages not present in the synthetic corpus ( e.g. French , Italian , Korean ) .
In future work , we plan to investigate whether the proposed data augmentation method could be applied to other multilingual tasks , such as classification .
We will also experiment more in depth with different strategies to control the target language of a model , and extrapolate on unseen ones .
q/c en es de ar hi vi zh en 14.5 8.9 7.2 5.9 6.5 8.4 6.0 es 9.0 10 . 6.6 4.2 5.9
In addition , we report the F1 scores for XLM -R finetuned on SQuAD en and XLM -R +synth-trans on all the language pairs , on both MLQA and XQuAD in Tables 7 , 8 , 9 and 10 .
