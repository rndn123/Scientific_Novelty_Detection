title
NB - MLM : Efficient Domain Adaptation of Masked Language Models for Sentiment Analysis
abstract
While Masked Language Models ( MLM ) are pre-trained on massive datasets , the additional training with the MLM objective on domain or task -specific data before fine-tuning for the final task is known to improve the final performance .
This is usually referred to as the domain or task adaptation step .
However , unlike the initial pre-training , this step is performed for each domain or task individually and is still rather slow , requiring several GPU days compared to several GPU hours required for the final task fine-tuning .
We argue that the standard MLM objective leads to inefficiency when it is used for the adaptation step because it mostly learns to predict the most frequent words , which are not necessarily related to a final task .
We propose a technique for more efficient adaptation that focuses on predicting words with large weights of the Naive Bayes classifier trained for the task at hand , which are likely more relevant than the most frequent words .
The proposed method provides faster adaptation and better final performance for sentiment analysis compared to the standard approach .
Introduction
Pre-training of neural networks with a language model ( LM ) or masked language model ( MLM ) objective on large amounts of non-domain-specific texts has given a significant boost of performance in almost all natural language processing tasks .
While 16 GB of texts were shown to BERT ( Devlin et al. , 2019 ) and ten times more to RoBERTa ( Liu et al. , 2019 ) during pre-training , the further training of these models with the MLM objective on domainspecific texts before fine-tuning to the target task was shown to further improve the final results ( Sun et al. , 2019 ; Gururangan et al. , 2020 ) .
This technique is called the domain or task adaptation , depending on the degree of similarity of the data for adaptation to the target dataset .
While initial pretraining is extremely expensive , it does not depend on the final task and can be performed only once .
However , domain or task adaptation is done for each domain or task individually and is still quite resource - demanding , requiring hundreds of thousands of training steps or several GPU days , unlike final fine-tuning , which can often be done in a few GPU hours ( Sun et al. , 2019 ) .
In this work , we propose a method for more efficient MLM adaptation .
We have noticed that the standard MLM spends most of the training time on learning to restore the most frequent words like determiners or auxiliary verbs hidden ( masked ) from its input .
While such training examples may be useful for learning English grammar , their domination during the adaptation phase seems to be wasteful for many final tasks .
Since the final task and the dataset are already known in this phase , we propose to undersample such examples in favor of examples with targets related to the final task .
This relatedness is estimated using a Naive Bayes classifier .
Hence , we call our modified objective Naive Bayes Masked Language Model ( NB - MLM ) .
We hypothesize that hiding from the model and asking it to restore mostly features that are important for the final task will likely result in faster adaptation .
Additionally , the absence of simple features and the requirement to restore them may teach the model to exploit more sophisticated and implicit features relevant to the final task .
We evaluate the proposed method on two datasets for sentiment analysis .
It is one of the most popular tasks in natural language processing ( Feldman , 2013 ) and an excellent playground for the comparison of adaptation methods due to the large amount of labeled and unlabeled user reviews of different products available .
In particular , we consider the task of classifying the binary sentiment polarity of a given review .
Our experiments show that the NB - MLM objective can significantly reduce adaptation time while achieving the same final performance or help to improve performance given the same amount of time for adaptation .
1
Related Work Pre-training Transformer networks with the MLM objective is proposed in ( Devlin et al. , 2019 ) for the BERT model and is shown to outperform the more traditional LM objective , though the similar task of predicting a word from its left and right context was used with different architectures earlier ( Mikolov et al. , 2013 ; Melamud et al. , 2016 ) .
RoBERTa enhances BERT by pre-training longer on ten times larger corpora , getting rid of the next sentence prediction ( NSP ) task during pre-training , and selecting different target words to be masked and predicted in each epoch ( dynamic masking ) .
Various approaches to further pre-training of BERT on domain or task -specific data are compared in ( Sun et al. , 2019 ) , while Gururangan et al . ( 2020 ) carry out a similar investigation with RoBERTa .
They try various options of data sources for adaptation : texts only from the target dataset ( called task adaptation or within- task pre-training ) , larger datasets from the same domain ( called domain adaptation or in- domain pretraining ) , and datasets from different domains ( called cross-domain pre-training ) .
They find the task adaptation , which is a computationally cheapest option , to be a surprisingly good solution .
In their experiments , it often outperforms the domain adaptation and is only marginally worse than com-bining both methods .
However , due to the large amount of data used in domain adaptation , Gururangan et al . ( 2020 ) train the MLM only for one or very few epochs .
We find that our method leveraging large data more efficiently makes the domain adaptation comparable to the task adaptation , and their combination is significantly better than each of them .
Our idea of employing Naive Bayes weights is inspired by the NB - SVM model ( Wang and Manning , 2012 ; Mesnil et al. , 2014 ) , which scales bag-ofngrams vectors with Naive Bayes classifier weights and then trains linear SVM or logistic regression classifiers on them .
It proved to be a very strong baseline , often outperforming both linear and more sophisticated models from that time .
MLM Objectives for Adaptation Uniform MLM .
For each input example , the standard MLM objective , as proposed by Devlin et al . ( 2019 ) , samples 15 % of the input positions ( subwords ) for calculating the loss .
The positions are sampled from the uniform distribution without replacement : P ( pos ) ?
1 . Then 80 % of the tokens on sampled positions are masked ( replaced with a [ MASK ] token ) , 10 % are replaced with some random tokens from the uniform distribution over the vocabulary , and 10 % are left intact .
NB - MLM .
As an alternative , we propose sampling 15 % of positions from a non-uniform distribution that gives higher probabilities to positions that contain subwords with high feature importance f i ( w ) : P ( pos ) ? exp( f i( w pos ) /T ) , where the temperature T is the hyperparameter allowing to balance between uniform sampling and determin- istic selection of positions that contain the most important features .
For binary classification , the feature importance is estimated using the Naive Bayes classifier weights as follows : f i( w ) = | logP ( w| 1 ) ? logP ( w|0 ) | .
Thus , those features that are much more probable in one class than in another receive the highest scores .
Similar to the method proposed by Wang and Manning ( 2012 ) , the probabilities are estimated by the multinomial Naive Bayes model with additive smoothing ( alpha = 0.1 ) .
Additionally , the scores are set to zero for those features that occurred in less than m examples to avoid the overrepresentation of unreliable features .
As an example , Figure 1 shows the words that the model is most frequently asked to predict during the task adaptation on the IMDB movie reviews dataset ( T = 0.1 , m = 5 for NB - MLM ) .
Evidently , NB - MLM learns to predict words relevant to sentiment analysis more often than the standard MLM .
Along with the uniform and NB - based distributions , during the preliminary experiments , we tried other options , which are described and compared in Appendix D. However , only NB - MLM outperformed the uniform baseline .
Experiments and Results During the preliminary experiments described in Appendix A , we found that our method helps for both BERT and RoBERTa models .
However , the latter model achieved significantly better performance .
Therefore , we describe the results for RoBERTa in the rest of the paper .
For domain adaptation ( denoted as DAPT ) , we employed the Amazon Reviews dataset ( McAuley et al. , 2015 ) with duplicates removed .
We removed reviews shorter than 500 characters and split the rest into the training and validation sets of 21 M and 10 K reviews correspondingly .
The valida- tion set was used to calculate perplexity during MLM training .
For task adaptation ( denoted as all - TAPT ) , we used all texts ( without labels ) from the target dataset , i.e. IMDB ( Maas et al. , 2011 ) or Yelp ( Zhang et al. , 2015 ) 2 . For IMDB , we employed the split of Gururangan et al . ( 2020 ) to make the results of our experiments directly comparable with their results .
We used the binary classification version of Yelp ( Zhang et al. , 2015 ) .
For validation , we randomly selected 5 K positive and 5 K negative examples .
For domain and task adaptation , we used the batch size of 1024 , while classifiers were fine-tuned with the batch size of 32 .
Based on our preliminary experiments , we set the learning rate of 2e - 4 for the domain adaptation , 1e - 4 for the task adaptation , and 1e - 5 for final fine-tuning .
Following Gururangan et al. ( 2020 ) , we performed domain adaptation for one epoch on the Amazon dataset ( 20 K steps , 38h on one V100 GPU ) and task adapta -
Figure 2 shows how the final classification accuracy improves during the task and domain adaptation .
Our NB - MLM model significantly helps for domain adaptation on IMDB .
For task adaptation , the difference is much smaller and fits into two standard deviations .
Still , on average , the NB - MLM seems to provide a consistent improvement throughout the adaptation .
For Yelp , the improvements from NB - MLM are also small but consistent .
Table 1 compares our models and the previously published results on the test sets .
In order to apply McNemar 's test for statistical significance , instead of averaging across all runs of each model with different random seeds , we have to take predictions of a particular run .
Thus , for each of our models , we selected the run with the median performance ( for the even number of runs , the one just above the median ) and reported its performance in the table .
For IMDB , the domain adaptation with NB - MLM obtains results similar to the Uniform MLM in 5x fewer training steps and data ( only 20 % of the data is seen during the first 4 K steps ) .
When trained for one epoch , it improves the results by more than 0.3 % , which is also statistically significant .
For task adaptation , the NB - MLM gives a much smaller improvement .
Similarly to the results of Gururangan et al . ( 2020 ) , in our experiments , the task adaptation with the Uniform MLM outperforms the domain adaptation that employs much more data by almost 0.5 % .
We suppose that this is due to the small proportion of relevant examples sampled by the Uniform MLM , which require many repetitions to learn from .
Probably , training domain adaptation for hundreds of epochs , similarly to task adaptation , can fix this problem , but this is not feasible for large datasets and moderate computation resources .
More efficient domain adaptation with NB - MLM , which focuses on targets that are likely relevant for the final task , reduces this difference to 0.2 % .
Finally , using the domain adaptation followed by the task adaptation results in the best final performance .
In this scenario , NB - MLM gives 0.2 % improvement for short adaptation and 0.1 % for long adaptation .
For Yelp , the metrics are higher , and the differences are smaller but still consistent .
Conclusion
We proposed a technique for the more efficient domain and task adaptation of MLMs .
It is especially helpful for leveraging large data efficiently during the domain adaptation , resulting in significantly shorter adaptation time or better performance .
To verify our hypothesis , in the preliminary experiments , we tried improving the results of the ITPT ( withIn - Task Pre-Training ) method ( Sun et al. , 2019 ) .
Since no code for this paper was available at that time , we implemented this method using the Transformers library ( Wolf et al. , 2020 ) 4 , which closely followed the details and hyperparameters specified in the paper but adopted recommendations from more recent models by not using NSP prediction and exploiting dynamic masking .
Since no official development set is available for the IMDB dataset ( Maas et al. , 2011 ) and the split is not specified in the paper , for early stopping during classifier fine-tuning and NB - MLM hyperparameters selection , we employed our own split 5 .
Note that this split was used only for preliminary experiments ; later , we switched to the split of Gururangan et al . ( 2020 ) .
For adaptation , we used the whole dataset , excluding half of the development set to measure the validation perplexity .
