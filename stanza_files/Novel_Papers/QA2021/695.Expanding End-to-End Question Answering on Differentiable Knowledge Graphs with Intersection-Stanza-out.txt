title
Expanding End-to- End Question Answering on Differentiable Knowledge Graphs with Intersection
abstract
End - to- end question answering using a differentiable knowledge graph is a promising technique that requires only weak supervision , produces interpretable results , and is fully differentiable .
Previous implementations of this technique have focused on single-entity questions using a relation following operation .
In this paper , we propose a model that explicitly handles multiple-entity questions by implementing a new intersection operation , which identifies the shared elements between two sets of entities .
We find that introducing intersection improves performance over a baseline model on two datasets , WebQuestionsSP ( 69.6 % to 73.3 % Hits@1 ) and ComplexWebQuestions ( 39.8 % to 48.7 % Hits@1 ) , and in particular , improves performance on questions with multiple entities by over 14 % on WebQuestionsSP and by 19 % on ComplexWebQuestions .
Introduction Knowledge graphs ( KGs ) are data structures that store facts in the form of relations between entities .
Knowledge Graph - based Question Answering ( KGQA ) is the task of learning to answer questions by traversing facts in a knowledge graph .
Traditional approaches to KGQA use semantic parsing to parse natural language to a logical query , such as SQL .
Annotating these queries , however , can be expensive and require experts familiar with the query language and KG ontology .
End -to - end question answering ( E2EQA ) models overcome this annotation bottleneck by requiring only weak supervision from question - answer pairs .
These models learn to predict paths in a knowledge graph using only the answer as the training signal .
In order to train an E2EQA model in a fully differentiable way , proposed differentiable knowledge graphs as a way to represent KGs as tensors and queries as differentiable mathematical operations .
Previous implementations of E2EQA models using differentiable knowledge graphs have focused on single-entity questions using a relation following operation .
For example , to answer " Where was Natalie Portman born ? " , the model could predict a path starting at the Natalie Portman entity and following a place of birth relation to the correct answer .
While this follow operation handles many questions , it often struggles on questions with multiple entities .
For example , to answer " Who did Natalie Portman play in Star Wars Episode II ? " , it is not enough to identify all the characters Natalie Portman has played , nor all the characters in Star Wars Episode II .
Instead , the model needs to find what character Natalie Portman has played that is also a character in Star Wars .
This can be solved through intersection .
An intersection of two sets A and B returns all elements in A that also appear in B .
This example is illustrated in Figure 1 .
In this paper , we propose to explicitly handle multiple-entity questions in E2EQA by learning intersection in a dynamic multi-hop setting .
Our intersection models learn to both follow relations and intersect sets of resulting entities in order to arrive at the correct answer .
We find that our mod-els score 73.3 % on WebQuestionsSP and 48.7 % on ComplexWebQuestions , and in particular , improve upon a baseline on questions with multiple entities from 56.3 % to 70.6 % on WebQuestionsSP and 36.8 % to 55.8 % on ComplexWebQuestions .
Related Works Traditional approaches to KGQA have used semantic parsing ( Zelle and Mooney , 1996 ; Zettlemoyer and Collins , 2005 ) to parse natural language into a logical form .
Collecting semantic parsing training data can be expensive and is done either manually ( Dahl et al. , 1994 ; Finegan - Dollak et al. , 2018 ) or using automatic generation ( Wang et al. , 2015 ) which is not always representative of natural questions ( Herzig and Berant , 2019 ) .
Another line of work in KGQA uses embedding techniques to implicitly infer answers from knowledge graphs .
These methods include GRAFT - Net ( Sun et al. , 2018 ) , which uses a graph convolutional network to infer answers from subgraphs , PullNet ( Sun et al. , 2019 ) , which improves GRAFT - Net by learning to retrieve subgraphs , and EmbedKGQA ( Saxena et al. , 2020 ) , which incorporates knowledge graph embeddings .
EmQL is a query embedding method using set operators , however these operators need to be pretrained for each KB .
TransferNet ( Shi et al. , 2021 ) is a recent model that trains KGQA in a differentiable way , however it stores facts as an N x N matrix , where N is the number of entities , so it runs into scaling issues with larger knowledge graphs .
Our approach to KGQA based on has three main advantages : ?
Interpretability : Models based on graph convolutional networks ( PullNet , GRAFT - Net ) get good performance but have weak interpretability because they do not output intermediate reasoning paths .
Our approach outputs intermediate paths as well as probabilities .
?
Scaling : show that differentiable KGs can be distributed across multiple GPUs and scaled horizontally , so that different triple IDs are stored on different GPUs , allowing for scaling to tens of millions of facts .
Other methods using embedding techniques ( EmbedKGQA , EmQL ) or less efficient representations ( TransferNet ) are more memory intensive and not easily distributed .
?
No retraining for new entities :
Models based on latent representations of entities ( Em- bedKGQA , EmQL ) get state - of - the - art performance , however they need to be retrained whenever a new entity is added to the KG ( e.g. , a new movie ) to learn updated embeddings .
Our approach can incorporate new entities easily without affecting trained models .
3 Models
The Baseline Model
Our baseline model , which we call Rigel - Baseline , is based on differentiable knowledge graphs and the ReifiedKB model .
We provide an overview here but full details can be found in the original paper .
Differentiable Knowledge Graphs
Assume we have a graph : G = ( s , p , o ) | s ?
E , o ?
E , p ? R , ( 1 ) where E is the set of entities , R is the set of relations , and ( s , p , o ) is a triple showing that the relation p holds between a subject entity s and an object entity o .
To create a differentiable knowledge graph , we represent the set of all triples T = {t i } N T i=1 , t i = ( s s i , p p i , o o i ) in three matrices : a subject matrix ( M s ) , relation matrix ( M p ) , and object matrix ( M o ) .
A triple ( s , p , o ) is represented across all three matrices at a given index .
M s ? { 0 , 1 } N T ?N E , M s ( i , j ) = I e j = s s i M p ? { 0 , 1 } N T ?N R , M p ( i , j ) = I p j = p p i M o ? { 0 , 1 } N T ?N E , M o ( i , j ) = I e j = o o i Since the knowledge graph is represented as matrices , interacting with the knowledge graph is done with matrix operations .
ReifiedKB was implemented with a follow operation : Given an entity vector x t?1 ? R N E at t ?
1 - th time step and a relation vector r t ?
R N R , the resulting entity vector x t is computed by Equation 2 where is elementwise multiplication .
x t = follow ( x t?1 , r t ) = M T o ( M s x t?1 M p r t ) ( 2 )
Model
The Rigel - Baseline model is composed of an encoder , which encodes the question , and a decoder , which returns a probability distribution over KG relations .
The question entities ( which , in our experiments , are provided from the datasets ) and predicted relations are followed in the differentiable knowledge graph to return predicted answers .
Predicted answers are compared to labeled answers , and the loss is used to update the model .
Rigel - Baseline is illustrated in Figure 2 .
We make the following key improvements to Rei-fiedKB .
First , we use RoBERTa ( Liu et al. , 2019 ) as our encoder instead of word2vec .
Second , Rei-fiedKB used different methods to determine the correct number of hops in multi-hop questions .
We implement an attention mechanism using a hierarchical decoder W dec t , which is learned and a unified approach across datasets .
Given a question embedding h q and relation vector r t , the resulting entity vector x t is computed as : r t =softmax W dec t h q |r t?1 | ? ? ? |r 1 T ( 3 ) x t = follow ( x t?1 , r t ) ( 4 )
We compute an attention score across all hops with : c t =W att t h q |r t?1 | ? ? ? |r 1 T ( 5 ) a = softmax ( [ c 1 , ? ? ? , c T h ] ) ( 6 ) and compute the final estimate ?
as : ? = T h t=1 a t x t . ( 7 ) Finally , while ReifiedKB used cross-entropy as its loss function , we instead use a multi-label loss function across all entities .
This is because the output space in many samples contains multiple entities , so cross-entropy loss is inadequate .
L(y , ? ) = 1 N E N E i=1 y i log ? i + ( 1 ? y i ) log ( 1 ? ?i ) ( 8 )
The Intersection Model
In order to build a model that can handle multiple entities , we expand Rigel - Baseline with a differentiable intersection operation to create Rigel -Intersect .
We define intersection as the elementwise minimum of two vectors .
While differentiable intersection has previously been implemented as element-wise multiplication value , and elements that appears in one or neither vector return a 0 . min elem ? ? ? ? ? ? ? ? a 1 a 2 ... a n ? ? ? ? , ? ? ? ? b 1 b 2 ... b n ? ? ? ? ? ? ? ? = ? ? ? ? min( a 1 , b 1 ) min( a 2 , b 2 ) ... min( a n , b n ) ? ? ? ? ( 9 ) Next , we modify the encoder to allow entityspecific question embeddings .
The Rigel - Baseline encoder creates one generic question embedding per question , but we may want to follow different relations for each entity .
To calculate the question embedding h q using an encoder f q , for each question entity , we concatenate the question text q with the entity 's mention or canonical name m separated by a separator token [ SEP ] .
We use the embedding at the separator token index i SEP as the entity -specific representation of the question .
h q = f q ( q [ SEP ] m ) [ : , i SEP , :] ( 10 )
In the decoder , we predict inference chains in parallel for each entity , and follow the entities and relations in the differentiable KG to return intermediate answers .
We intersect the two intermediate answers to return the final answer .
In multi-hop settings , we weight entities in each vector before intersection based on the attention score .
We train using the hyperparameters in Appendix A. Rigel -Intersect is illustrated in Figure 3 .
Our implementation of intersection takes a maximum of two entities per question .
We use the first two labeled entities per question and ignore subsequent ones .
This works well for our datasets where 94 % of questions contain a maximum of two entities .
Given a dataset with more complex questions , we can extend our implementation in the future to an arbitrary number of entities .
Datasets
We use two datasets in our experiments : WebQuestionsSP
( Yih et al. , 2016 ) is an English question - answering dataset of 4,737 questions ( 2,792 train , 306 dev , 1,639 test ) that are answerable using Freebase ( Bollacker et al. , 2008 ) .
During training , we exclude 30 training set questions with no answer , and during evaluation , we exclude 13 test set questions with no answer and count them as failures in our Hits@1 score .
All questions are answerable by 1 or 2 hop chains of inference , so we set our models to a maximum of 2 hops .
To create a subset of Freebase , we identify all question entities and relations in WebQuestionsSP and build a subgraph containing all facts reachable from 2 hops of the question entities , as done in previous works .
This creates a sub-graph of 17.8 million facts , 9.9 million entities , and 670 relations .
We create inverse relations for each relation ( e.g. , place of birth returns a person 's birthplace ; inv-place of birth returns people born in that location ) , for a total of 1,340 relations .
ComplexWebQuestions ( Talmor and Berant , 2018 ) is an extended version of WebQuestionsSP with 34,689 questions ( 27,649 train , 3,509 dev , 3,531 test ) in English requiring complex reasoning .
During training , we exclude 163 questions that are missing question or answer entities , and during evaluation , we exclude 21 examples from the test set for the same reasons and count them as failures in the Hits@1 score .
We limit our model to 2 hops for training efficiency .
To create a subset of Freebase , we identify all question entities and relations in the dataset and build a subgraph containing all facts reachable within 2 hops of the question entities .
This results in a subgraph of 43.2 million facts , 17.5 million entities , and 848 relations ( 1,696 including inverse relations ) .
Results
Model WQSP CWQ KVMem ( Miller et al. , 2016 ) 46.7 21.1 GRAFT - Net ( Sun et al. , 2018 ) 67.8 32.8 PullNet ( Sun et al. , 2019 ) 68.1 47.2 ReifiedKB 52.7 - EmQL 75.5 - TransferNet ( Shi et al. , 2021 ) 71
Our results are in Tables 1 and 2 . Scores are reported as Hits@1 , which is the accuracy of the top predicted answer from the model .
Rigel - Baseline is not incapable of handling multiple-entity questions because not all questions require intersection .
For example , in " Who played Jacob Black in Twilight ? " , the model can follow Jacob Black to the actor , Taylor Lautner , without intersecting with Twilight because only one actor has played Jacob Black .
This is not possible for characters such as James Bond or Batman , who are portrayed by different actors in different movies .
Although Rigel - Baseline can spuriously handle multiple-entity questions , Rigel - Intersect uses more accurate inference chains .
Conclusions
In this paper , we expand an end-to - end question answering model using differentiable knowledge graphs to learn an intersection operation .
We show that introducing intersection improves performance on WebQuestionsSP and ComplexWebQuestions .
This improvement comes primarily from better handling of questions with multiple entities , which improves by over 14 % on WebQuestionsSP , and by 19 % on ComplexWebQuestions .
In future work , we plan to expand our model to more operations , such as union or difference , to continue improving model performance on complex questions .
Figure 1 : 1 Figure 1 : To answer " Who did Natalie Portman play in Star Wars Episode II ? " , we identify all the characters Natalie Portman has played , all the characters in Star Wars Episode II , and intersect the two resulting sets to get to the answer , Padm ?
Amidala .
Figure 2 : 2 Figure2 : A detailed illustration of the Rigel- Baseline model .
The encoder encodes the question , the decoder predicts relations , and attention selects the final hop .
The entities and relations are followed in the KG to return predicted answers .
The loss between the predicted and actual answers is the training signal for the whole model .
Figure 3 : 3 Figure 3 : An illustration of the Rigel-Intersect model .
Given a question with two entities , we run each question entity in parallel to return intermediate answers .
These answers are intersected to return the final answer .
The loss between the final answer and the actual answer is the training signal for the whole model .
Table 1 1 The breakdown of results in Table2 shows that the improved performance of Rigel - Intersect comes from better handling of questions with multiple entities .
While Rigel-Baseline and Rigel-Intersect are comparable on questions with one entity , Rigel-Intersect surpasses Rigel - Baseline on questions with more than 1 entity by over 14 % on and by 19 % on Com-plexWebQuestions ( 36.8 % vs. 55.8 % ) .
Example model outputs are in Appendix C. com -
