title
Learn to Resolve Conversational Dependency : A Consistency Training Framework for Conversational Question Answering
abstract
One of the main challenges in conversational question answering ( CQA ) is to resolve the conversational dependency , such as anaphora and ellipsis .
However , existing approaches do not explicitly train QA models on how to resolve the dependency , and thus these models are limited in understanding human dialogues .
In this paper , we propose a novel framework , EXCORD ( Explicit guidance on how to resolve Conversational Dependency ) to enhance the abilities of QA models in comprehending conversational context .
EXCORD first generates self-contained questions that can be understood without the conversation history , then trains a QA model with the pairs of original and self-contained questions using a consistency - based regularizer .
In our experiments , we demonstrate that EXCORD significantly improves the QA models ' performance by up to 1.2 F1 on QuAC 5.2 F1 on CANARD ( Elgohary et al. , 2019 ) , while addressing the limitations of the existing approaches .
1
Introduction Conversational question answering ( CQA ) involves modeling the information - seeking process of humans in a dialogue .
Unlike single-turn question answering ( QA ) tasks ( Rajpurkar et al. , 2016 ; Kwiatkowski et al. , 2019 ) , CQA is a multi-turn QA task , where questions in a dialogue are contextdependent ; 2 hence they need to be understood with the conversation history Reddy et al. , 2019 ) .
As illustrated in Figure 1 , to answer ?
Corresponding author 1 Our models and code are available at : https://github.com/dmis-lab/excord
2
While the term " context " usually refers to the evidence document from which the answer is extracted , in CQA , it refers to conversational context .
.
Owing to linguistic phenomena in human conversations , such as anaphora and ellipsis , the current question q 3 should be understood based on the conversation history : q 1 , a 1 , q 2 , and a 2 . Question q 3 can be reformulated as a self-contained question q3 via a question rewriting ( QR ) process .
the current question " Was he close with anyone else ? , " a model should resolve the conversational dependency , such as anaphora and ellipsis , based on the conversation history .
A line of research in CQA proposes the end-toend approach , where a single QA model jointly encodes the evidence document , the current question , and the whole conversation history Yeh and Chen , 2019 ; Qu et al. , 2019a ) .
In this approach , models are required to automatically learn to resolve conversational dependencies .
However , existing models have limitations to do so without explicit guidance on how to resolve these dependencies .
In the example presented in Figure 1 , models are trained without explicit signals that " he " refers to " Leonardo da Vinci , " and " anyone else " can be more elaborated with " other than his pupils , Salai and Melzi . "
Another line of research proposes a pipeline approach that decomposes the CQA task into question rewriting ( QR ) and QA , to reduce the complexity of the task .
Based on the conversation history , QR models first generate selfcontained questions by rewriting the original questions , such that the self-contained questions can be understood without the conversation history .
For instance , the current question q 3 is reformulated as the self-contained question q3 by a QR model in Figure 1 .
After rewriting the question , QA models are asked to answer the self-contained questions rather than the original questions .
In this approach , QA models are trained to answer relatively simple questions whose dependencies have been resolved by QR models .
Thus , this limits reasoning abilities of QA models for the CQA task , and causes QA models to rely on QR models .
In this paper , we emphasize that QA models can be enhanced by using both types of questions with explicit guidance on how to resolve the conversational dependency .
Accordingly , we propose EXCORD ( Explicit guidance on how to Resolve Conversational Dependency ) , a novel training framework for the CQA task .
In this framework , we first generate self-contained questions using QR models .
We then pair the self-contained questions with the original questions , and jointly encode them to train QA models with consistency regularization ( Laine and Aila , 2016 ; Xie et al. , 2019 ) .
Specifically , when original questions are given , we encourage QA models to yield similar answers to those when self -contained questions are given .
This training strategy helps QA models to better understand the conversational context , while circumventing the limitations of previous approaches .
To demonstrate the effectiveness of EXCORD , we conduct extensive experiments on the three CQA benchmarks .
In the experiments , our framework significantly outperforms the existing approaches by up to 1.2 F1 on QuAC and by 5.2 F1 on CANARD ( Elgohary et al. , 2019 ) .
In addition , we find that our framework is also effective on a dataset CoQA ( Reddy et al. , 2019 ) that does not have the self-contained questions generated by human annotators .
This indicates that the proposed framework can be adopted on various CQA datasets in future work .
We summarize the contributions of this work as follows : ?
We identify the limitations of previous approaches and propose a unified framework to address these .
Our novel framework improves QA models by incorporating QR models , while reducing the reliance on them . ?
Our framework encourages QA models to learn how to resolve the conversational dependency via consistency regularization .
To the best of our knowledge , our work is the first to apply the consistency training framework to the CQA task . ?
We demonstrate the effectiveness of our framework on three CQA benchmarks .
Our framework is model- agnostic and systematically improves the performance of QA models .
Background
Task Formulation
In CQA , a single instance is a dialogue , which consists of an evidence document d , a list of questions q = [ q 1 , ... , q T ] , and a list of answers for the questions a = [ a 1 , ... , a T ] , where T represents the number of turns in the dialogue .
For the t-th turn , the question q t and the conversation history H t = [ ( q 1 , a 1 ) , ... , ( q t?1 , a t?1 ) ] are given , and a model should extract the answer from the evidence document as : ?t = arg max at P( a t |d , q t , H t ) ( 1 ) where P ( ? ) represents a likelihood function over all the spans in the evidence document , and ?t is the predicted answer .
Unlike single- turn QA , since the current question q t is dependent on the conversation history H t , it is important to effectively encode the conversation history and resolve the conversational dependency in CQA .
End-to-end Approach
A naive approach in solving CQA is to train a model in an end-to - end manner ( Figure 2a ) .
Since standard QA models generally are ineffective in the CQA task , most studies attempt to develop a QA model structure or mechanism for encoding the conversation history effectively Yeh and Chen , 2019 ; Qu et al. , 2019a , b) . Although Figure 2 : Overview of the end-to- end approach , the pipeline approach , and ours .
In the end-to- end approach , QA models are asked to answer the original questions based on the conversation history .
In the pipeline approach , the self-contained questions are generated by a QR model , and then QA models answer them .
Standard QA models are commonly used in this approach ; however conversational QA models that encode the history can be adopted ( the dotted line in Figure ( b ) ) .
In ours , the original and self-contained question are jointly encoded to train QA models with the consistency loss .
these efforts improved performance on the CQA benchmarks , existing models remain limited in understanding conversational context .
In this paper , we emphasize that QA models can be further improved with explicit guidance using self-contained questions effectively .
Pipeline Approach Recent studies decompose the task into two subtasks to reduce the complexity of the CQA task .
The first sub-task , question rewriting , involves generating self-contained questions by reformulating the original questions .
Neural- net - based QR models are commonly used to obtain selfcontained questions ( Lin et al. , 2020 ; . The QR models are trained on the CANARD dataset ( Elgohary et al. , 2019 ) , which consists of 40 K pairs of original QuAC questions and their self-contained versions that are generated by human annotators .
After generating the self-contained questions , the next sub-task , question answering , is carried out .
Since it is assumed that the dependencies in the questions have already been resolved by QR models , existing works usually use standard QA models ( not specialized to CQA ) ; however conversational QA models can also be used ( the dotted line in Figure 2 b ) .
We formulate the process of predicting the answer in the pipeline approach as : P( a t |d , q t , H t ) ?
P rewr ( q t |q t , H t ) ?
P read ( a t |d , qt ) ( where P rewr ( ? ) and P read ( ? ) are the likelihood functions of QR and QA models , respectively .
qt is a self-contained question rewritten by the QR model .
The main limitation of the pipeline approach is that QA models are never trained on the original questions , which limits their abilities to understand the conversational context .
Moreover , this approach makes QA models dependent on QR models ; hence QA models suffer from the error propagation from QR models .
3
On the other hand , our framework enhances QA models ' reasoning abilities for CQA by jointly utilizing original and self-contained questions .
In addition , QA models in our framework do not rely on QR models at inference time and thus do not suffer from error propagation .
EXCORD : Explicit Guidance on Resolving Conversational Dependency
We introduce a unified framework that jointly encodes the original and self-contained questions as illustrated in Figure 2 c .
Our framework consists of two stages : ( 1 ) generating self -contained questions using a QR model ( ?3.1 ) and ( 2 ) training a QA model with the original and self-contained questions via consistency regularization ( ?3.2 ) .
Question Rewriting Similar to the pipeline approach , we utilize a QR model to obtain self-contained questions .
We use the obtained questions for explicit guidance in the next stage .
As shown in Equation 2 , the QR task is to generate a self-contained question given an original question and a conversation history .
Following Lin et al. ( 2020 ) , we adopt a T5 - based sequence generator ( Raffel et al. , 2020 ) as our QR model , which achieves comparable performance with that of humans in QR .
4 For training and evaluating the QR model , we use the CANARD dataset following previous works on QR ( Lin et al. , 2020 ; . During inference , we utilize the top-k random sampling decoding based on beam search with the adjustment of the softmax temperature ( Fan et al. , 2018 ; Xie et al. , 2019 ) .
Consistency Regularization
Our goal is to enhance the QA model 's ability to understand conversational context .
Accordingly , we use consistency regularization ( Laine and Aila , 2016 ; Xie et al. , 2019 ) , which enforces a model to make consistent predictions in response to transformations to the inputs .
We encourage the model 's predicted answers from the original questions to be similar to those from the self-contained questions ( ?3.1 ) .
Our consistency loss is defined as : L cons t = KL ( P read ?
( a t |d , q t , H t ) | | P read ?
( a t |d , qt , Ht ) ) ( 3 ) where KL ( ? ) represents the Kullback - Leibler divergence function between two probability distributions .
? is the model 's parameters , and ? depicts a fixed copy of ?.
With the consistency loss , QA models are regularized to make consistent predictions , regardless of whether the given question is self-contained or not .
In order to output an answer distribution that is closer to P read ?
( a t |d , qt , Ht ) , QA models should treat original questions as if they were rewritten into self-contained questions by referring to the conversation history .
Through this process , our consistency regularization method serves as explicit guidance that encourages QA models to resolve the conversational dependency .
In our framework , P read ? ( a t | ? ) is the answer span distribution over all evidence document tokens .
In contrast to Asai and Hajishirzi ( 2020 ) , by using all probability values in the answer distributions , the signals of selfcontained questions can be effectively propagated to the QA model .
In addition to using all probability values , we also sharpened the target distribution P read ?
( a t |d , qt , Ht ) by adjusting the temperature ( Xie et al. , 2019 ) to strengthen the QA model 's training signal .
Finally , we calculate the final loss as : L = L orig + ?
1 L self + ? 2 L cons ( 4 ) where ?
1 and ?
2 are hyperparameters .
L orig and L self are calculated by the negative log-likelihood between the predicted answers and gold standards given the original and self-contained questions , respectively .
Comparison with previous works Consistency training has mainly been studied as a method for regularizing model predictions to be invariant to small noises that are injected into the input samples ( Sajjadi et al. , 2016 ; Laine and Aila , 2016 ; Miyato et al. , 2016 ; Xie et al. , 2019 ) .
The intuition behind consistency training is to push noisy inputs closer towards their original versions .
Therefore , only the original parameters ( i.e. , ? ) are updated , while the copied model parameters ( i.e. , ? ) are fixed .
In contrast to the original concept of consistency training , our goal is to go in the opposite direction and update the original parameters .
Thus , we fix the parameters ? with self-contained questions , and soley update ? for each training step as shown in Equation 3 .
Experiments
In this section , we describe our experimental setup and compare our framework to baseline approaches ( i.e. , the end-to- end and pipeline approaches ) .
Datasets QuAC QuAC comprises 100k QA pairs in information - seeking dialogues , where a student asks questions based on a topic with background information provided , and a teacher provides the answers in the form of text spans in Wikipedia documents .
Since the test set is only available in the QuAC challenge , we evaluate models on the development set .
5 For validation , we use a subset of the original training set of QuAC , which consists of questions that correspond to the self-contained questions in CANARD 's development set .
The remaining data is used for training .
CANARD CANARD ( Elgohary et al. , 2019 ) consists of 31K , 3K , and 5 K QA pairs for training , development , and test sets , respectively .
The questions in CANARD are generated by rewriting a subset of the original questions in QuAC .
We use the training and development sets for training and validating QR models , and the test set for evaluating QA models .
CoQA CoQA ( Reddy et al. , 2019 ) consists of 127 K QA pairs and evidence documents in seven domains .
In terms of the question distribution , CoQA significantly differs from QuAC ( see ?5.3 ) .
We use CoQA to test the transferability of EX - CORD , where a QR model trained on CANARD generates the self-contained questions in a zeroshot manner .
Subsequently , we train a QA model by using the original and synthetic questions .
Similar to QuAC , the test set of CoQA is soley available in the CoQA challenge .
6
Therefore , we randomly sample 5 % of the QA dialogues in the training set and adopt them as our development set .
Metrics Following , we use the F1 , HEQ -Q , and HEQ -D for QuAC and CANARD .
HEQ - Q measures whether a model finds more accurate answers than humans ( or the same answers ) in a given question .
HEQ -D measures the same thing , but in a given dialog instead of a question .
For CoQA , we report the F1 scores for each domain ( children 's story , literature from Project Gutenberg , middle and high school English exams , news articles from CNN , Wikipedia ) and the overall F1 score , as suggested by Reddy et al . ( 2019 ) .
QA models
Note that the baseline approaches and our framework do not limit the structure of QA models .
For a fair comparison of the baseline approaches and EXCORD , we test the same QA models in all approaches .
The selected QA models are commonly used and have been proven to be effective in CQA .
BERT BERT ) is a contextualized word representation model that is pretrained on large corpora .
BERT also works well on CQA datasets , although it is not designed for CQA .
It receives the evidence document , current question , and conversation history of the previous turn as input .
BERT + HAE BERT + HAE is a BERT - based QA model with a CQA - specific module .
Following Qu et al. ( 2019a ) , we add the history answer embedding ( HAE ) to BERT 's word embeddings .
HAE encodes the information of the answer spans from the previous questions .
RoBERTa RoBERTa improves BERT by using pretraining techniques to obtain the robustly optimized weights on larger corpora .
In our experiments , we found that RoBERTa performs well in CQA , achieving comparable performance with the previous SOTA model , HAM ( Qu et al. , 2019 b ) , on QuAC .
Thus , we adopt RoBERTa as our main baseline model owing to its simplicity and effectiveness .
It receives the same input as BERT , otherwise specified .
Implementation Details
The CANARD training set provides 31,527 selfcontained questions from the original QuAC questions .
Therefore , we can obtain 31,527 pairs of original and self-contained questions without question rewriting .
For the rest of the original questions , we automatically generate self-contained questions by using our QR model .
Finally , we obtain 83,568 question pairs and use them in our consistency training .
We denote the original questions , selfcontained questions generated by humans , and selfcontained questions generated by a QR model as Q , Qhuman , and Qsyn , respectively .
Additional implementation details are described in Appendix B
Results
Table 1 presents the performance comparison of the baseline approaches to our framework on QuAC and CANARD .
Compared to the end-to- end approach , EXCORD consistently improves the performance of QA models on both datasets .
Also , these improvements are significant : EXCORD improves the performance of the RoBERTa by absolutely 1.2 and 2.3 F1 scores and BERT by 1.2 and 5.2 F1 scores on QuAC and CANARD , respectively .
From these results , we conclude that the consistency training with original and self-contained 6135 questions enhances ability of QA models to understand the conversational context .
On QuAC , the pipeline approach underperforms the end-to - end approach in all baseline models .
This indicates that training a QA model soley with self-contained questions is ineffective when human rewrites are not given at the inference phase .
On the other hand , EXCORD improves QA models by using both types of questions .
As presented in Table 1 , our framework significantly outperforms the baseline approaches on QuAC .
On CANARD , the pipeline approach is significantly more effective than the end-to - end approach .
Since QA models are trained with self-contained questions in the pipeline approach , they perform well on CANARD questions .
Nevertheless , EX - CORD still outperforms the pipeline approach in most cases .
Compared to the pipeline approach , our framework improves the performance of RoBERTa by absolutely 1.2 F1 score .
Analysis and Discussion
We elaborate on analyses regarding component ablation and transferability .
We also describe a case study carried out to highlight such differences between our and baseline approaches .
Ablation Study
In this section , we comprehensively explore the factors contributing to this improvement in detail : ( 1 ) using self -contained questions that are rewritten by humans ( Qhuman ) as additional data , ( 2 ) using self -contained questions that are synthetically generated by the QR model ( Qsyn ) , and ( 3 ) training a QA model with our consistency framework .
In Table 2 , we present the performance gaps when each component is removed from our framework .
We use RoBERTa on QuAC in this experiment .
We first explore the effects of Qhuman and Qsyn .
As shown in Table 2 , excluding Qhuman degrades the performance of RoBERTa in our framework .
Although automatically generated , Qsyn contributes to the performance improvement .
Therefore , both types of self-contained questions are useful in our framework .
To investigate the effect of our framework , we simply augment Qhuman and Qsyn to Q orig , which is called Question Augment ( question data augmentation ) .
We find that Question Augment slightly improves the performance of RoBERTa on CA - NARD , whereas it degrades the performance on QuAC .
This shows that simply augmenting the questions is ineffective and does not guarantee improvement .
On the other hand , our consistency training approach significantly improves performance , showing that EXCORD is a more optimal way to utilizing self-contained questions .
Case Study
We analyze several cases that the baseline approaches answered incorrectly , but our framework answered correctly .
We also explore how our framework improves the reasoning ability of QA models , compared to the baseline approaches .
These cases Table 3 : Error analysis for predictions of RoBERTa that are trained with the baseline approaches and EXCORD .
In the first case , the QA model trained with the end-to - end approach fails to resolve the conversational dependency .
The QR model in the second case misunderstands the " my , " and generates an unnatural question , triggering an incorrect prediction .
are obtained from the development set of QuAC .
The first case in Table 3 shows the predictions of the two RoBERTa models trained in the end-to - end approach and our framework , respectively .
Note that " the film " in the current question does not refer to " The Search " ( red box ) in the document d , but " Red River " ( blue box ) in a 1 .
When trained in the end-to - end approach , the model failed to comprehend the conversational context and misunderstood what " the film " refers to , resulting in an incorrect prediction .
On the other hand , when trained in EXCORD , the model predicted the correct answer because it enhances the ability to resolve conversational dependency .
In the second case , we compare the pipeline approach to EXCORD .
In this case , the QR model misunderstood " my " in the current question as a pronoun and replaced it with the band 's name , " Train's . "
Consequently , the QA model received the erroneous self-contained question , resulting in an incorrect prediction .
On the other hand , the QA model trained in our framework predicted the correct answer based on the original question q 6 .
Transferability
We train a QR model to rewrite QuAC questions into CANARD questions .
Then , self- contained questions can be generated for the samples that do not have human rewrites .
This results in the improvement of QA models ' performance on QuAC and CANARD ( ?4.5 ) .
However , it is questionable whether the QR model can successfully rewrite questions when the original questions significantly differ from those in QuAC .
To answer this , we test our framework on another CQA dataset , CoQA .
We first analyze how the question distributions of QuAC and CoQA differ .
We found that question types in QuAC and CoQA are significantly different , such that QR models could suffer from the gap of question distributions between two datasets .
( See details in Appendix A ) .
To test the transferability of EXCORD , we compare the end-to - end approach to our framework on the CoQA dataset .
Using a QR model trained on CANARD , we generate the self-contained questions for CoQA and train QA models with our framework .
As presented in Table 4 , our framework performs well on CoQA .
The improvement in BERT is 0.5 based on the overall F1 , and the performance of RoBERTa is also improved by an overall F1 of 0.6 .
Improvements are also consistent in most of the documents ' domains .
Therefore , we conclude that our framework can be simply extended to other datasets and improve QA performance even when question distributions are significantly different .
We plan to improve the transferability of our framework by fine-tuning QR models on target datasets in future work .
Related Work Conversational Question Answering Recently , several works introduced CQA datasets such as QUAC and COQA ( Reddy et al. , 2019 ) .
We classified proposed methods to solve the datasets into two approaches : ( 1 ) end-to-end and ( 2 ) pipeline .
Most works based on the endto-end approach focused on developing a model structure ( Zhu et al. , 2018 ; Ohsugi et al. , 2019 ; Qu et al. , 2019 a , b ) or training strategy such as multitask with rationale tagging ( Ju et al. , 2019 ) that are specialized in the CQA task or datasets .
Several works demonstrated the effectiveness of the flow mechanism in CQA Yeh and Chen , 2019 ) .
With the advent of a dataset consisting of selfcontained questions rewritten by human annotators ( Elgohary et al. , 2019 ) , the pipeline approach has drawn attention as a promising method for CQA in recent days .
The approach is particularly useful for the open-domain CQA or passage -retrieval ( PR ) tasks ( Dalton et al. , 2019 ; Ren et al. , 2020 ; Qu et al. , 2020 ) since self-contained questions can be fed into existing non-conversational search engines such as BM25 .
Note that our framework can be used jointly with the pipeline approach in the opendomain setting because our framework can improve QA models ' ability to find the answers from the retrieved documents .
We will test our framework in the open-domain setting in future work .
Question Rewriting QR has been studied for augmenting training data ( Buck et al. , 2018 ; Sun et al. , 2018 ; Zhu et al. , 2019 ; or clarifying ambiguous questions ( Min et al. , 2020 ) .
In CQA , QR can be viewed as a task of simplifying difficult questions that include anaphora and ellipsis in a conversation .
Elgohary et al. ( 2019 ) first proposed the question rewriting task as a sub-task of CQA and the CANARD dataset for the task , which consists of pairs of original and self-contained questions that are generated by human annotators .
used a coreference - based model ( Lee et al. , 2018 ) and GPT - 2 ( Radford et al. , 2019 ) as QR models and tested the models in the QR and PR tasks .
Lin et al. ( 2020 ) conducted the QR task using T5 ( Raffel et al. , 2020 ) and achieved on performance comparable to humans on CANARD .
Following Lin et al. ( 2020 ) , we use T5 in our experiments to generate high-quality questions for enhancing QA models .
Consistency Training Consistency regularization ( Laine and Aila , 2016 ; Sajjadi et al. , 2016 ) has been mainly explored in the context of semisupervised learning ( SSL ) ( Chapelle et al. , 2009 ; Oliver et al. , 2018 ) , which has been adopted in the textual domain as well ( Miyato et al. , 2016 ; Clark et al. , 2018 ; Xie et al. , 2020 ) .
However , the consistency training framework is also applicable when only the labeled samples are available ( Miyato et al. , 2018 ; Jiang et al. , 2019 ; Asai and Hajishirzi , 2020 ) .
The consistency regularization requires adding noise to the sample , which can be either discrete ( Xie et al. , 2020 ; Asai and Hajishirzi , 2020 ) or continuous ( Miyato et al. , 2016 ; Jiang et al. , 2019 ) .
Existing works regularize the predictions of the perturbed samples to be equivalent to be that of the originals ' .
On the other hand , our method encourages the models ' predictions for the original asnwers to be similar to those from the rewritten questions , i.e. , synthetic ones .
Conclusion
We propose a consistency training framework for conversational question answering , which enhances QA models ' abilities to understand conversational context .
Our framework leverages both the original and self-contained questions for explicit guidance on how to resolve conversational dependency .
In our experiments , we demonstrate that our framework significantly improves the QA model 's performance on QuAC and CANARD , compared to the existing approaches .
In addition , we verified that our framework can be extended to CoQA .
In future work , the transferability of our framework can be further improved by fine-tuning the QR model on target datasets .
Furthermore , future work would include applying our framework to the open-domain setting .
