title
More is Better : Enhancing Open-Domain Dialogue Generation via Multi-Source Heterogeneous Knowledge
abstract
Despite achieving remarkable performance , previous knowledge - enhanced works usually only use a single-source homogeneous knowledge base of limited knowledge coverage .
Thus , they often degenerate into traditional methods because not all dialogues can be linked with knowledge entries .
This paper proposes a novel dialogue generation model , MSKE - Dialog , to solve this issue with three unique advantages : ( 1 ) Rather than only one , MSKE - Dialog can simultaneously leverage multiple heterogeneous knowledge sources ( it includes but is not limited to commonsense knowledge facts , text knowledge , infobox knowledge ) to improve the knowledge coverage ; ( 2 ) To avoid the topic conflict among the context and different knowledge sources , we propose a Multi-Reference Selection to better select context / knowledge ; ( 3 ) We propose a Multi-Reference Generation to generate informative responses by referring to multiple generation references at the same time .
Extensive evaluations on a Chinese dataset show the superior performance of this work against various state - of - the - art approaches .
To our best knowledge , this work is the first to use the multi-source heterogeneous knowledge in the open-domain knowledge - enhanced dialogue generation .
Introduction
The rapid developments of knowledge- enhanced techniques have enabled machines to understand the instinct semantics of human conversations further and generate informative responses ( Yu et al. , 2020 ) .
External knowledge , such as commonsense bases ( Speer et al. , 2017 ) , documents , and tables ( Wu et al. , 2021 ) , can bridge the gap between machines and humans in conversation by generously providing knowledge that is Commonsense ( iPhone , related to , smartphone ) ( smartphone , has context , mobile phones ) ( Android , has context , mobile phones ) ?.
Text The iPhone is a line of smartphones designed and marketed by Apple Inc. that use Apple 's iOS mobile operating system .?
Infobox Table Entity Name : i Phone ( Type : Smartphone ) ( Developer : Apple Inc. ) ( Services : iTunes , i Cloud , Apple Music ) ?.
Dialogue Examples Query :
What is your phone ?
Response : I am using an iPhone , because I love iOS and Apple Music .
hard to be learned from a conversational corpus ( Ghazvininejad et al. , 2018 ) .
However , previous knowledge- enhanced works are still far from satisfactory because they usually solely rely on a single-source homogeneous knowledge base : ( 1 ) Conversations are diverse because humans are free to talk about whatever topics they like Hu et al. , 2020 ) , but the knowledge coverage of a single knowledge base is limited .
Thus , only a finite portion of dialogues could benefit from the external knowledge ; the remaining can only rely on the given query because no knowledge can be matched .
Suffering from the long-tail issue and the cost of a massive workforce , it is not wise to improve the coverage by expanding the number of entries in a single-source knowledge base .
( 2 ) Each knowledge source has its advantages and disadvantages , for example , plain text has richer information than knowledge graph , but it performs worse in logically modeling .
No knowledge type can always perform best ; the most suitable knowledge only depends on the case .
Human beings can use various kinds of knowledge learned from different sources .
Therefore , as shown in Figure 1 , we believe using multiple knowledge sources can improve knowledge coverage and have more room to select appropriate knowledge .
However , every coin has two sides ; using multi-source heterogeneous is more challenging because of the following two conflicts : ( 1 ) Topic Conflict : given a dialogue , knowledge entries are usually retrieved by the entity name matching technique ( Wu et al. , 2020a ) .
Thus , knowledge entries retrieved from one source may be irrelevant to the dialogue context and have different topics compared to entries retrieved from other sources .
Blindly using such irrelevant / conflicting knowledge entries can confuse the model ; ( 2 ) Generation Conflict :
Although dialogue utterances and different knowledge bases are made of words , the word distributions vary among them .
It can affect the generation if a model tries to improve the informativeness by copying words from knowledge entries .
For example , if a word ' apple ' appears in both the dialogue context and the commonsense knowledge , there exist two tokens of ' apple ' in the dialogue vocab and the commonsense vocab , respectively .
Then , two ' apple ' will have two different tokens / probabilities when predicting the next word , making it difficult for a model to judge which one should be the objective .
This issue is severe when using multi-source heterogeneous knowledge .
With more knowledge sources , there are more chances for conflicts ; then , the more conflicts , the lower the response quality .
This paper proposes a novel multi-source heterogeneous knowledge - enhanced dialogue generation model , MSKE - Dialog .
MSKE - Dialog can improve knowledge coverage by integrating more knowledge sources .
In this paper , we use commonsense knowledge , text knowledge , and infobox knowledge at the same time .
Compared to only use one of them , simultaneously using such three knowledge sources can improve the coverage by 63 ? 200 % in our dataset .
To alleviate the impact of topic conflict , we propose a Multi-Reference Selection mechanism .
It uses a global relevance gate and a dynamic selection gate to select relevant knowledge from different sources .
We also propose a Multi-Reference Generation mechanism , which will construct a unified dynamic vocab and comprehensively refer to all inputs ( i.e. , the context and the multi-source knowledge ) during the decoding .
As a result , MSKE - Dialog can avoid the impact of generation conflict as possible and generate an informative response .
Our experiments are conducted on a Chinese Weibo dataset .
In both automatic and human evaluations , MSKE - Dialog can outperform various stateof - the- art knowledge - enhanced methods by notable margins , as well as can surpass the fine-tuned pretraining system CDial - GPT ( GPT & GPT2 ) even with fewer parameters and training corpus .
Extensive deep analyses also demonstrate : ( 1 ) Compared to simply integrating multiple knowledge bases , MSKE - Dialog has better performance because it can alleviate two mentioned challenging conflicts ; ( 2 ) Even if MSKE - Dialog only uses a single-source knowledge , our model can also achieve promising results .
It demonstrates the performance gain comes from not only the multisource knowledge but also the approach itself .
Approach
Problem Statement and Overview
The goal is to generate the dialogue response Y = (y 1 , . . . , y l Y ) conditioned on R , where R = ( R X , { R K i } ) is a set of given references to guide the generation .
R X = ( r X ,1 , . . . , r X ,l X ) represents the dialogue context ( history ) , { R K i } represents a set of multi-source heterogeneous knowledge , where the i-th R K i = ( r K i ,1 , . . . , r K i , l K i ) represents the relevant entries retrieved from the i-th knowledge source .
Considering both R X and { R K i } serve as a type of reference in the response generation stage , we call R X and { R K i } as dialogue reference and knowledge references , respectively .
Thus , R is called as the reference set .
As shown in Figure 2 , MSKE - Dialog employs three heterogeneous knowledge sources ; in other words , { R K i } contains the commonsense knowledge R K C , the text knowledge R K T , and the infobox knowledge R K I .
The high- level architecture of MSKE - Dialog consists of three parts .
( 1 ) Reference Encoding :
We propose four different encoders to encode the given references R X , R K C , R K T , R K I into intermediate hidden rep- resentations R X , R K C , R K T , R K I , respectively .
( 2 ) Reference Selection :
In the decoding stage , we update the decoder with not only the last predicted token , but also the context - aware readouts gathered from the encoded reference set R. R .
To avoid the conflicts during the generation , we propose a Multi-Reference Generation mechanism and a Dynamic Copy mechanism .
Reference Encoding Dialogue Reference : Each word r X , t ?
R X is first embedded as r w X , t , with the fixed vocab V R X .
Then , a bi-directional GRU network ( denoted as g ) ( Cho et al. , 2014 ) is adopted to encode R X into hidden states R X = ( r X ,1 , ? ? ? , r X , l X ) , r X , t = [ r ? X , t ; r ? X , t ] , [ ? ; ?] indicates the concatenation operation : r ? X , t = g ? ( r w X , t , r ? X ,t?1 ) r ?
X , t = g ? ( r w X , t , r ? X , t +1 ) ( 1 ) Commonsense Reference : Text Reference : Each text reference is a word sequence R K T = ( r K T ,1 , . . . , r K T ,l K T ) .
Thus , each token r K T ,n is first embedded as r w K T ,n with the 1 TransE is not the STOA method .
However , this paper does not focus on embedding learning .
For comparing models accurately , we use TransE as previous works do .
vocab V R K T . Considering R K T is a long text paragraph , we use a 2 - layer Transformer ( Vaswani et al. , 2017 ) to encode the sequence efficiently : ) is a short text .
Thus , R K I can be subsequently decomposed to a set of key -word pairs { a kw n , m } , where each key - word pair a kw n , m includes the n-th key a k n and the m-th word in the n-th value a w n , m .
Then , a kw n , m is embedded as : Each entry r K C ,n ?
R K C is a fact triplet r K C , n = ( e h , R K T = {r K T , n } = T ransf ormer T 2 ( { r w K T , n } ) ( 3 ) a kw n , m = [ a k n ; a w n , m ; pos n , m ] ( 4 ) where the attribute key embedding a k n uses the vocab V R K I , K , the attribute word embedding a w n , m uses the vocab V R K I , the positional embedding pos n , m is appended to indicate the position ( i.e , n , m ) .
After decomposing key-value pairs to keyword pairs , the number of pairs will significantly increase .
Therefore , for the efficiency , we use a 2 - layer Transformer to encode key -word pairs : R K I = {r K I , n , m } = T ransf ormer I2 ( { a kw n , m } ) ( 5 ) 2.2.1 Scalability
Now , the reference set R has been encoded to R = ( R X , { R K i } ) 2 .
Reference Selection State Updating :
At each decoding step t , the decoder state z t is firstly updated with a GRU unit g d , the embedding of the last generated token y t?1 , and the context - aware reference readout c t : z t = g d ( z t?1 , c t , y t?1 ) ( 6 ) Multi-Reference Selection :
The reference readout c t is obtained by fusing local reference readouts {r c R j , t } = {r c X , t , r c K C , t , r c K T , t , r c K I , t } with relevance gates {?
rel R j } and selection gates {? sel R j , t } : c t = R j ?R ? rel R j ? ? sel R j , t Rm?R ? rel Rm ? ? sel Rm , t r c R j , t ( 7 ) where the dialogue reference readout r c X ,t gathers from the encoded R X = ( r X ,1 , ? ? ? , r X , l X ) 3 : n exp ( ( W R k X r X , n ) ( W Rq X z t?1 ) ) W Rv X r X , n m exp ( ( W R k X r X , m ) ( W Rq X z t?1 ) ) ( 8 ) and each knowledge reference readout r c R K i , t ? {r c K C , t , r c K T , t , r c K I , t } gathers from the encoded R K C/T/ I , respectively : r c K i , t = n exp ( r K i ,n W Ra K i z t?1 ) W Rv K i r K i , n m exp ( r K i , m W Ra K i z t?1 ) ( 9 ) Relevance Gate : Each reference R j ?
R may have various importance , and may have conflicts with other references .
Thus , we employ a global Relevance Gate ? rel R j ? ( 0 , 1 ) to control the participation of each reference .
Each relevance gate ?
rel R j is given before the decoding : ? rel R j = ?( W G 2 R j ELU ( W G 1 R j [ W G X r X , l X ; s R j ] ) ) ( 10 ) where ? is the sigmoid activation function , ELU is another activation function ( Clevert et al. , 2016 ) , s R j is the reference summary of R j .
3 the shape of vectors / matrices is defined as R n?1 / R n?m
Each reference summary s R j is given by taking the last dialogue reference state r X , lx as attention query , and the encoded reference R j = {r j , n } as keys / values : n exp ( ( W S k R j r j, n ) ( W Sq R j r X , l X ) ) m exp ( ( W S k R j r j , m ) ( W Sq R j r X , l X ) ) W Sv R j r j,n ( 11 ) Selection Gate : During each decoding step , we employ a dynamic context - aware Selection Gate ? sel R j , t to control the fine- grained usage of R j : a sel t = ( W D [ z t?1 ; {r c R j , t } ; y t?1 ] ) ( 12 ) where is the sof tmax operation , a sel t ?
R | R | ; thus , each local selection gate is ? sel R j , t = a sel t [ j ] .
Multi-Reference Guided Generation
Copying words besides the fixed vocabulary has shown great potential in promoting OOV - free , informative and diverse responses ( Lin et al. , 2020 ) .
However , token distributions are various among multiple references R . It poses a great challenge to avoid conflicts .
We propose a Multi-Reference Generation mechanism to address this issue .
Word Prediction :
To predict the next token y t , we first compute a generation probability over the fixed vocab V R X by a two - layer M LP f gen : p gen t = sof tmax ( f gen ( z t , c t , y t?1 ) ) ( 13 ) then , for each reference R j ?
R , we compute a probability distribution to estimate the probability to copy a token from the corresponding reference : p copy R j , t = f R j copy ( [ z t ; c t ; y t?1 ] , R j ) ( 14 ) where each f R j copy is a General attention function ( Luong et al. , 2015 ) , [ z t ; c t ; y t?1 ] is the attention query , the encoded R j serves as the attention key .
Dynamic Vocab :
To eliminate conflicts brought by different word distributions of given references , a dynamic vocab V d is built , which consists of all words that appear in both the reference set R and the fixed vocab V R X 4 : V d = ?( R ) ? V R X ( 15 ) Then , a projection matrix M V R X ? R | V d |?|V R X | to map the computed generation distribution p gen t to the dynamic vocab space .
Similarly , for each copy distribution P copy R j ,t of reference R j , we construct a projection matrix M R j ? R | V d |?|R j | , which maps the copying distribution of R j to the dynamic vocab space .
Multi-Reference Generation :
The probability of the next word y t is given by infusing all distributions with a generation gate ? gen t and several copy gates ?
copy R j , t : P t ( y t ) = ? gen t M V R X p gen t + R j ? copy R j , t M R j p copy R j , t ? gen t + R j ? copy R j , t ( 16 ) we not only use a mode weight ? mode * , t to control the participation of each distribution , but also adopt the previous relevance gate ?
rel R j to help the infusing of copy distributions : ? gen t = ? mode gen , t , ? copy R j , t = ? rel R j ? ? mode R j , t ( 17 ) where mode gates ? mode gen , t = a m t [ 0 ] and ? mode R j , t = a m t [ j + 1 ] are given by : a m t ? R 1 + | R | = sof tmax ( W M [ z t ; c t ; y t?1 ] ) ( 18 ) Training : Finally , P t ?
R | V d | can be used to predict the next token .
The model can subsequently be optimized by minimizing the following negative log-likelihood : L = ? t log P t ( y t |y 1:t?1 , R ) ( 19 ) 3 Experiment
Experiment Methodology Dataset :
It is built upon three open-released Chinese Weibo corpora ( Shang et al. , 2015 ; Ke et al. , 2018 ; Cai et al. , 2019 ) .
We adopt a ConceptNet ( Speer et al. , 2017 ) base released by as the commonsense knowledge .
It contains about 696 K triples , 27 K entities , and 26 relations .
For the text knowledge , we collect introduction paragraphs of 1,663 K entities from Chinese Wikipedia .
Besides , we also collect infobox tables of 1,581 K entities from Chinese Wikipedia .
All texts are tokenized by Jieba 5 .
Following , entity words ?
R X are used as queries to retrieve knowledge queries from knowledge bases .
For each dialogue , we retrieve up to 200 most relevant 6 commonsense triplets , up to 1 relevant text pineni et al. , 2002 ) , ROUGE -L ( Lin , 2004 ) , and to evaluate the relevance to the ground -truth responses .
Following , we use DIST - Uni / Bi ( the ratio of distinct 1/2 - grams among all generated tokens ) , and the 4 - gram entropy Ent4 to evaluate the diversity .
In addition , we use the entity score ( i.e. , the number of the generated entity / knowledge words per sentence ) to measure the knowledge utilization .
We count the entity score on each type of knowledge ( CSK , TXT , IBT : commonsense , text , infobox ) , and compute the averaged entity score ( AVG ) .
Finally , to fairly compare the overall performance , we report the overall geometric mean scores relative to Seq2Seq , Appendix B has elaborated the detail .
When comparing different approaches , we do not use the perplexity , because the definitions and computations vary among approaches .
We will report the perplexity in the ablation study , because all model variants share the same computation .
Experimental Results Automatic Evaluation :
As reported in logues .
Moving to the aspect of knowledge , MSKE - Dialog undoubtedly beats other baselines in the overall score with the cooperation of three heterogeneous knowledge sources .
MSKE - Dialog loses to RefNet / CCM in terms of text / commonsense entity score .
The reason is such two baselines only use one knowledge source , but our approach uses three sources ; thus , our approach would not only focus on using one source .
Human Evaluation :
We conduct the pair-wise evaluation .
Baselines include ConKADI , TransInfo , RefNet , GPT base ( the best in the corresponding group ) , and the naive Seq2Seq .
We employ 3 welleducated native speakers to annotate the sampled 200 test cases .
There are two criteria : ( 1 ) Appropriateness evaluates the fluency , and the relevance to the context ; ( 2 ) Informativeness evaluates how much new knowledge is provided .
As reported in Table 3 , MSKE - Dialog can also outperform baselines in human evaluation .
Compared with the automatic results , Seq2Seq has better performance in human evaluation .
This is due to humans always have a high tolerance for boring / generic but fluent responses .
The remaining results are roughly in line with the automatic results .
pre-training + 700 K fine-tune ) .
It verifies the advantage of using multi-source heterogeneous knowledge and the effectiveness of our model .
Ablation Analysis
To investigate what makes the most contribution to MSKE - Dialog , we conduct extensive studies .
Knowledge Contribution :
We design a set of single-source variants of MSKE - Dialog to explore which knowledge brings the most improvement .
As reported in Table 4 , to Base , which neither uses external knowledge nor copies word , all three single -source variants have improvements in both the overall performance and the perplexity .
Previous works ( Gu et al. , 2016 ; Vinyals et al. , 2015 ) have shown that copying words from the context R X can significantly improve the performance .
Our models can also benefit from this factor , the Context +* outperforms Base +* by notable margins .
Evidently , among the three knowledge sources , commonsense knowledge and text knowledge bring more contributions .
The perplexity of Context / Base +IBT have notable improvements , but the improvement of the overall score ( i.e. , the quality of the generated responses ) is not notable 7 .
We guess the employed beam-search decoding may be a bottleneck , we leave it as future work .
It is worth noting that our approach can also beat the best knowledge - enhanced baselines without using more source knowledge sources .
The best com -
Full does not achieve the best in each aspect but has the best overall performance and perplexity , which indicates using multi-source knowledge is quite challenging .
It is crucial to fuse the knowledge sources into the context without the impact of the possible conflicts .
Table 6 : Some cases generated by models .
entity / [ ? ] means the new information ( new knowledge word ) .
Figure 3 : Evaluations on different data sizes .
We test the full model and a variant that does not use multisource knowledge .
We report the overall score and the geomean relative score in the aspect of relevance .
Compared to the diversity / knowledge , relevance is a more representative aspect in the low-resource evaluation .
More Studies Low Resource Evaluation :
We train MSKE - Dialog and a non-knowledge - enhanced variant on only a part of the dataset .
As illustrated in Figure 3 , with the incorporation of multi-source knowledge , MSKE - Dialog with only 1 2 ? 1 4 conversational data can archive comparable performance with the nonknowledge - enhanced variant .
It indicates the multisource knowledge can indeed help the dialogue generation if the conversational data is not enough .
This can be quite useful when constructing a system in a low-resource language / scenario .
Case Study :
We show three cases generated by our MSKE - Dialog and two better baselines in the human evaluation in Table 6 . In Case # 1 , only MSKE - Dialog provided the new information , demonstrating our multi-source heterogeneous knowledge - enhanced approach is able to generate more informative responses with the improved knowledge coverage .
In the next Case # 2 , although ConKADI also provided new information , it failed to generate a fluent response .
It indicates it is crucial to alleviate the conflict between knowledge and context .
In the last Case # 3 , although all three models have generated fluent and informative responses , GPT base generated a more natural response and brought more information , which can be attributed to GPT base was trained by more training data .
It tells us the potential to investigate the pre-training methods ; we leave it as future work .
This work only focuses on the non-pre-training method because pre-training models have expensive costs in training / using .
Related Work
The vanilla Seq2Seq tends to generate generic responses , such as ' I do n't know ' ( Chen et al. , 2017 ) .
Many efforts have been devoted to diversifying the generations Gao et al. , 2019 ) , etc .
One crucial factor leading to this issue is the lack of sufficient knowledge .
During the conversation , the vanilla Seq2Seq model can only access the given query , which only contains limited knowledge ( Ghazvininejad et al. , 2018 ) .
The insufficient knowledge makes it hard for a model to understand the context and generate an informative response .
To this end , knowledge - enhanced approaches have been proposed and demonstrated promising performances ( Yu et al. , 2020 ) .
The knowledge can be texts ( Ren et al. , 2020 ; Kim et al. , 2020 ; Tam , 2020 ) , the structured graphs / tables / bases ( Zhou et al. , 2018 ; Qin et al. , 2019 ; Wu et al. , 2020 b , c ; , the semi-structured infobox ( Wu et al. , 2021 ) , the pre-trained models ( Devlin et al. , 2019 ; Radford et al. , 2019 ; Moghe et al. , 2020 ) , and many other external knowledge components Xu et al. , 2019 ) .
However , most previous works can only use single-source homogeneous knowledge .
Solely re-lying on only one type of knowledge greatly limits the performance in the real scenario .
Some previous works have also noticed this issue .
For example , augmenting the knowledge graph with an external text comprehension module or a KBQA module ( Wang et al. , 2020a ) , introducing multi-modal visual features ( Liang et al. , 2020 ) for emotional conversation or visual conversation ( Meng et al. , 2020 b ) .
Our work is different from them because we focus on the open-domain knowledge - enhanced dialogue response generation , rather than the emotional / visual conversation , etc .
To our best knowledge , few works have studied this topic in this area .
In addition , MSKE - Dialog has a framework .
A new knowledge source can easily be integrated by simply adding a knowledge encoder .
Conclusion & Future Work
This paper proposes a novel multi-source heterogeneous knowledge- enhanced dialogue generation approach , MSKE - Dialog , which outperforms competitive knowledge - enhanced baselines and pretraining models .
It verifies the advantages of using multi-source heterogeneous knowledge and the advantages of our approach .
We will continue to investigate the advantages of knowledge - enhanced dialogue generation .
We notice the current decoding strategy may be a bottleneck of knowledge - enhanced works and the potential of multi-source knowledge + pre-training .
We will also pay more attention to such topics .
In this work , the dataset involves both conversational data and knowledge data .
All three involved Chinese Weibo ( weibo.com , an open SNS in China ) conversational datasets are open-released by previous works for research ( Shang et al. , 2015 ; Ke et al. , 2018 ; Cai et al. , 2019 ) .
Including but not limited to the involved three datasets , conversational data crawled from Weibo are widely used in training / evaluating in the research of Chinese dialogue generation research and other NLP researches , for example , Su et al. , 2020 ) .
All data crawled from Weibo are open-accessed posts / responses that everyone can see ; no privacyrelated data ( such as gender , nickname , birthday , etc. ) are used .
But if it needs commercial use , it may need to ask for additional permission from the original author / copyright owner .
We use the commonsense knowledge from ConceptNet ( Speer et al. , 2017 ) ; according to its description , it is allowed to reuse them in research ( see conceptnet .io ) .
We also collect text knowledge and infobox knowledge from Wikipedia ( under the license CC BY - SA 3.0 ) ; it is allowed to reuse them in both research and commercial .
To summary , as research work , this work has no concern on the dataset and other aspects .
