title
A Comparative Study on Schema-Guided Dialogue State Tracking
abstract
Frame - based state representation is widely used in modern task - oriented dialog systems to model user intentions and slot values .
However , a fixed design of domain ontology makes it difficult to extend to new services and APIs .
Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot , thus offering a dynamic set of schema .
In this paper , we conduct in - depth comparative studies to understand the use of natural language description for schema in dialog state tracking .
Our discussion mainly covers three aspects : encoder architectures , impact of supplementary training , and effective schema description styles .
We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation .
Introduction
From early frame-driven dialog system GUS ( Bobrow et al. , 1977 ) to virtual assistants ( Alexa , Siri , and Google Assistant et al. ) , frame - based dialog state tracking has long been studied to meet various challenges .
In particular , how to support an everincreasing number of services and APIs spanning multiple domains has been a focal point in recent years , evidenced by multi-domain dialog modeling ( Budzianowski et al. , 2018 ; Byrne et al. , 2019 ; Shah et al. , 2018a ) and transferable dialog state tracking to unseen intent / slots ( Mrk ?i? et al. , 2017 ; Hosseini - Asl et al. , 2020 ) . Recently , Rastogi et al. ( 2019 ) proposed a new paradigm called schema-guided dialog for transferable dialog state tracking by using natural language description to define a dynamic set of service schemata .
As shown in Figure 1 , the primary motivation is that these descriptions can offer effective * * Work done when Jie Cao was an intern at Amazon knowledge sharing across different services , e.g. , connecting semantically similar concepts across heterogeneous APIs , thus allowing a unified model to handle unseen services and APIs .
With the publicly available schema-guided dialog dataset ( SG - DST henceforward ) as a testbed , they organized a state tracking shared task composed of four subtasks : intent classfication ( Intent ) , requested slot identification ( Req ) , categorical slot labeling ( Cat ) , and noncategorical slot labeling ( NonCat ) .
Many participants achieved promising performance by exploiting the schema description for dialog modeling , especially on unseen services .
Despite the novel approach and promising results , current schema-guided dialog state tracking task only evaluates on a single dataset with limited variation in schema definition .
It is unknown how this paradigm generalizes to other datasets and other different styles of descriptions .
In this paper , we focus our investigation on the study of three aspects in schema-guided dialog state tracking : ( 1 ) schema encoding model architectures ( 2 ) supplementary training on intermediate tasks ( 3 ) various styles for schema description .
To make a more general discussion on the schema-guided dialog state tracking , we perform extensive empirical studies on both SG - DST and MULTIWOZ 2.2 datasets .
In summary , our contributions include : ?
A comparative study on schema encoding architectures , suggesting a partial -attention encoder for good balance between inference speed and accuracy .
?
An experimental study of supplementary training on schema-guided dialog state tracking , via intermediate tasks including natural language inference and question answering .
?
An in- depth analysis of different schema description styles on a new suite of benchmarking datasets with variations in schema description for both SG - DST and MULTIWOZ 2.2 .
Schema-Guided Dialog State Tracking
A classic dialog state tracker predicts a dialog state frame at each user turn given the dialog history and predefined domain ontology .
As shown in Figure 1 , the key difference between schema-guided dialog state tracking and the classic paradigm is the newly added natural language descriptions .
In this section , we first introduce the four subtasks and schema components in schema-guided dialog state tracking , then we outline the research questions in our paper .
Subtasks .
As shown in Figure 1 , the dialog state for each service consists of 3 parts : active intent , requested slots , user goals ( slot values ) .
Without loss of generality , for both SG - DST and MULTIWOZ 2.2 datasets , we divide their slots into categorical and non-categorical slots by following previous study on dual-strategies ( Zhang et al. , 2019 ) .
Thus to fill the dialog state frame for each user turn , we solve four subtasks : intent classification ( Intent ) , requested slot identification ( Req ) , categorical slot labeling ( Cat ) , and non-categorical slot labeling ( NonCat ) .
All subtasks require matching the current dialog history with candidate schema descriptions for multiple times .
Schema Components .
Figure 1 shows three main schema components : service , intent , slot .
For each intent , the schema also describes optional or required slots for it .
For each slot , there are flags indicating whether it is categorical or not .
Categorical means there is a set of predefined candidate values ( Boolean , numeric or text ) .
For instance , has_live_music in Figure 1 is a categorical slot with Boolean values .
Non-categorical , on the other hand , means the slot values are filled from the string spans in the dialog history .
New Questions .
These added schema descriptions pose the following three new questions .
We discuss each of them in the following sections .
we also conduct our comparative study on the two typical architectures Cross-Encoder ( Bordes et al. , 2014 ; Lowe et al. , 2015 ) and Dual-Encoder ( Wu et al. , 2017 ; Yang et al. , 2018 ) .
However , they only focus on sentence - level matching tasks .
All subtasks in our case require sentence - level matching between dialog context and each schema , while the non-categorical slot filling task also needs to produce a sequence of token - level representation for span detection .
Hence , we study multi-sentence encoding for both sentence - level and token - level tasks .
Moreover , to share the schema encoding across subtasks and turns , we also introduce a simple Fusion - Encoder by caching schema token embeddings in ?5.1 , which improves efficiency without sacrificing much accuracy .
Multi-domain Dialog State Tracking .
Recent research on multi-domain dialog system have been largely driven by the release of large-scale multi-domain dialog datasets , such as Multi-WOZ ( Budzianowski et al. , 2018 ) , M2M
( Shah et al. , 2018a ) , accompanied by studies on key issues such as in / cross-domain carry - over .
In this paper , our goal is to understanding the design choice for schema descriptions in dialog state tracking .
Thus we simply follow the indomain cross-over strategies used in TRADE .
Additionally , explicit cross-domain carryover ( Naik et al. , 2018 ) is difficult to generalize to new services and unknown carryover links .
We use longer dialog history to inform the model on the dialog in the previous service .
This simplified strategy does impact our model performance negatively in comparison to a well - designed dialog state tracking model on seen domains .
However , it helps reduce the complexity of matching extra slot descriptions for cross-service carryover .
We leave the further discussion for future work .
Transferable Dialog State Tracking .
Another line of research focuses on how to build a transferable dialog system that is easily scalable to newly added intents and slots .
This covers diverse topics including e.g. , resolving lexical / morphological variabilities by symbolic de-lexicalization - based methods ( Henderson et al. , 2014 ; Williams et al. , 2016 ) , neural belief tracking ( Mrk ?i? et al. , 2017 ) , generative dialog state tracking ( Peng et al. , 2020 ; Hosseini - Asl et al. , 2020 ) , modeling DST as a question answering task ( Zhang et al. , 2019 ; Gao et al. , , 2019 .
Our work is similar with the last class .
However , we further investigate whether the DST can benefit from NLP tasks other than question answering .
Furthermore , without rich description for the service / intent / slot in the schema , previous works mainly focus on simple format on question answering scenarios , such as domain-slottype compounded names ( e.g. , " restaurant-food " ) , or simple question template " What is the value for slot i ? " .
We incorporate different description styles into a comparative discussion on ?7.1 .
Datasets
To the best of our knowledge , at the time of our study , SG - DST and MULTIWOZ 2.2 are the only two publicly available corpus for schema-guided dialog study .
We choose both of them for our study .
In this section , we first introduce these two representative datasets , then we discuss the generalizibility in domain diversity , function overlapping , data collecting methods .
Schema- Guided Dialog Dataset .
SG - DST dataset 1 is especially designed as a test-bed for schema-guided dialog , which contains welldesigned heterogeneous APIs with overlapping functionalities between services ( Rastogi et al. , 2019 ) .
In DSTC8 , SG - DST was introduced as the standard benchmark dataset for schema-guided dialog research .
SG - DST covers 20 domains , 88 intents , 365 slots .
2
However , previous research are mainly conducted based on this single dataset and the provided single description style .
In this paper , we further extended this dataset with other benchmarking description styles as shown in ?7 , and then we perform both homogenous and hetergenous evalution on it .
Remixed MultiWOZ 2.2 Dataset .
To eliminate potential bias from the above single SG - DST dataset , we further add MULTIWOZ 2.2 to our study .
Among various extended versions for MultiWOZ dataset ( 2.0 - 2.3 , Budzianowski et al. , 2018 ; Eric et al. , 2020 ; Han et al. , 2020 ) , besides rectifying the annotation errors , MULTIWOZ 2.2 also introduced the schema-guided annotations , which covers 8 domains , 19 intents , 36 slots .
To evaluate performance on seen / unseen services with Multi-WOZ , we remix the MULTIWOZ 2.2 dataset to include as seen services dialogs related to restaurant , attraction and train during training , and eliminate slots from other domains / services from training split .
For dev , we add two new domains hotel and taxi as unseen services .
For test , we add all remaining domains as unseen , including those that have minimum overlap with seen services , such as hospital , police , bus .
The statistics of data splits are shown in Appendix A.2 .
Note that this data split is different from the previous work on zero-shot MultiWOZ DST which takes a leave- one - out approach in .
By remixing the data in the way described above , we can evaluate the zeroshot performance on MultiWOZ in a way largely compatible with SG - DST .
Discussion .
First , the two datasets cover diverse domains .
MULTIWOZ 2.2 covers various possible dialogue scenarios ranging from requesting basic information about attractions through booking a hotel room or travelling between cities .
While SG - DST covers more domains , such as ' Payments ' , ' Calender ' , ' DoctorServices ' and so on .
Third , they are collected by two different approaches which are commonly used in dialog collecting .
SG - DST is firstly collected by machine - tomachine self -play ( M2 M , Shah et al. , 2018 b ) with dialog flows as seeds , then paraphrased by crowdworkers .
While MULTIWOZ 2.2 are human-tohuman dialogs ( H2H , Kelley , 1984 ) , which are collected with the Wizard - of - Oz approach .
We summarize the above discussion in Table 1 .
We believe that results derived from these two representative datasets can guide future research in schema guided dialog .
Dialog & Schema Representation and Inference ( Q1 )
In this section , we focus on the model architecture for matching dialog history with schema descriptions using pretrained BERT ( Devlin et al. , 2019 ) 3 .
To support four subtasks , we first extend Dual-Encoder and Cross- Encoder to support both sentence - level matching and token - level prediction .
Then we propose an additional Fusion - Encoder strategy to get faster inference without sacrificing much accuracy .
We summarize different architectures in Figure 2 .
Then we show the classification head and results for each subtask .
Cross-Encoder .
Another popular architecture as Figure 2 ( b ) is Cross - Encoder , which concatenates the dialog and schema as a single input , and encodes jointly with a single self-attentive encoder spanning over the two segments .
When using BERT to encode the concatenated sentence pair , it performs full ( cross ) self-attention in every transformer layers , thus offer rich interaction between the dialog and schema .
BERT naturally produces a summarized representation with [ CLS ] embedding CLS CE and each schema-attended dialog token embeddings TOK CE .
Since the dialog and schema encoding always depend on each other , it requires recomputing dialog and schema encoding for multiple times , thus much slower in inference .
Fusion-Encoder .
In Figure 2 ( c ) , similar to Dual-Encoder , Fusion - Encoder also encodes the schema independently with a fixed BERT and finetuning another BERT for dialog encoding .
However , instead of caching a single [ CLS ] vector for schema representation , it caches all token representation for the schema including the [ CLS ] token .
What 's more , to integrate the sequences dialog token representation with schema token representation , an extra stack of transformer layers are added on top to allow token - level fusion via self-attention , similar to Cross-Encoder .
The top transformer layers will produce embeddings for each token TOK F E including a schema-attended CLS F E of the input [ CLS ] from the dialog history .
With cached schema token - level representations , it can efficiently produce schema- aware sentence - and token - level representation for each dialog-schema pairs .
Model Overview
All the above 3 encoders will produce both sentence - and token - level representations for a given sentence pair .
In this section , we abstract them as two representations CLS and TOK , and present the universal classification heads to make decisions for each subtask .
Active Intent .
To decide the intent for current dialog turn , we match current dialog history D with each intent descriptions I 0 ... I k .
For each dialog-intent pair ( D , I k ) , we project the final sentence - level CLS representation to a single number P active I k with a linear layer follows a sigmoid function .
We predict " NONE " if the P active I k of all intents are less than a threshold 0.5 , which means no intent is active .
Otherwise , we predict the intent with largest P active I k .
We predict the intent for each turn independently without considering the prediction on previous turns .
Requested Slot .
As in Figure 1 , mulitple requested slots can exist in a single turn .
We use the same strategy as in active intent prediction to predict a number P active req .
However , to support the multiple requested slots prediction .
We predict all the requested slots with P active req > 0.5 . Categorical Slot .
Categorical slots have a set of candidate values .
We cannot predict unseen values via n-way classification .
Instead , we do binary classification on each candidate value .
Besides , rather than directly matching with values , we also need to check that whether the corresponding slot has been activated .
For Cross-Encoder and Fusion- Encoder , we use typical two -stage state tracking to incrementally build the state : Step 1 .
Using CLS to predict the slot status as none , dontcare or active .
When the status is active , we use the predicted slot value ;
Otherwise , it will be assigned to dontcare meaning no user preference for this slot , or none meaning no value update for the slot in current turn ; Step 2 .
If Step 1 is active , we match the dialog history with each value and select the most related value by ranking .
We train on cross entropy loss .
Two -stage strategy is efficient for Dual-Encoder and Fusion - Encoder , where cached schema can be reused , and get efficiently ranked globally in a single batch .
However , it is not scalable for Cross - Encoder , especially for large number of candidate values in MultiWOZ dataset .
Hence , during training , we only use a binary cross-entropy for each single value and postpone the ranking only to the inference time .
Noncategorical Slot .
The slot status prediction for noncategorical slot use the same two -stage strategy .
Besides that , we use the token representation of dialog history TOK to compute two softmax scores f i start and f i end for each token i , to represent the score of predicting the token as start and end position respectively .
Finally , we find the valid span with maximum sum of the start and end scores .
Experiments on Encoder Comparison
To fairly compare all three models , we follow the same schema input setting as in Table 2 .
We trained separate models for SG - DST and the remixed Mul-tiWOZ datasets for all the experiments in our pa- Rastogi et al . ( 2019 ) .
Other models are trained with the architecture described in our paper .
pers 5 .
Because there are very few intent and requested slots in MULTIWOZ 2.2 dataset , we ignore the intent and requested slots tasks for MUL - TIWOZ 2.2 in our paper .
Results .
As shown in Table 3 , Cross - Encoder performs the best over all subtasks .
Our Fusion- Encoder with partial attention outperforms the Dual- Encoder by a large margin , epsecially on categorical and noncategorical slots predictions .
Additionally , on seen services , we found that Dual-Encoder and Fusion - Encoder can perform as good as Cross-Encoder on Intent and Req tasks .
However , they cannot generalize well on unseen services as Cross-Encoder .
Inference Speed .
To test the inference speed , we conduct all the experiments with a maximum affordable batch size to fully exploit 2 V100 GPUs ( with 16GB GPU RAM each ) .
During training , we log the inference time of each evaluation on dev set .
Both Dual-Encoder and Fusion- Encoder can do joint inference across 4 subtasks to obtain an integral dialog state for a dialog turn example .
Dual - Encoder achieves the highest inference speed of 603.35 examples per GPU second , because the 5 Appendix A.1 shows the detailed experiment setup encoding for dialog and schema are fully separated .
A dialog only needed to be encoded for once during the inference of a dialog state example while the schema are precomputed once .
However , for Cross - Encoder , to predict a dialog state for a single turn , it need to encode more than 300 sentence pairs in a batch , thus only processes 4.75 examples per GPU second .
Fusion - Encoder performs one time encoding on dialog history , but it needs to jointly encode the same amount of dialog-schema pair ws Cross - Encoder , instead , however , with a two -layer transformer encoder .
Overall it achieves 10.54 examples per GPU second , which is 2.2x faster than Cross-Encoder .
With regarding to the accuracy in Table 3 , Fusion - Encoder performs much better than Dual- Encoder , especially on unseen services .
Supplementary Training ( Q2 ) Besides the pretrain-fintune framework used in ?5 , Phang et al. ( 2018 ) propose to add a supplementary training phase on an intermediate task after the pretraining , but before finetuning on target task .
It shows significant improvement on the target tasks .
Moreover , large amount pretrained and finetuned transformer - based models are publicly accessible , and well - organized in model hubs for sharing , training and testing 6 .
Given the new task of schemaguided dialog state tracking , in this section , we study our four subtasks with different intermediate tasks for supplementary training .
Intermediate Tasks
As described in ? 5.2 , all our 4 subtasks take a pair of dialog and schema description as input , and predict with the summerized sentence - pair CLS representation .
While NonCat also requires span-based detection such as question answering .
Hence , they share the similar problem structure with the following sentence - pair encoding tasks .
Natural Language Inference .
Given a hypothesis / premise sentence pair , natural language inference is a task to determine whether a hypothesis is entailed , contradicted or neutral given that premise .
Question Answering .
Given a passage / question pairs , the task is to extract the span-based answer in the passage .
Hence , when finetuning BERT on our subtaks , instead of directly using the originally pretrained BERT , we use the BERT finetuned on the above SG - DST MULTIWOZ 2.2 intent req cat noncat cat noncat all seen unseen all seen unseen all seen unseen all seen unseen all seen unseen all seen unseen ?
SNLI +0.51 +0.02 +0.68 -0.19 +0.38 -0.38 - 1.63 - 2.87 - 1.23 - 4.7 -0.1 -6.25 +2.05 +0.6 -0.7 +3.64 + 1.05 +4.84 ? SQuAD -1.81 -0.17 -1.32 -0.25 -0.01 -0.33 - 2.87 -3.02 -5.17 +1.99 - 1.79 +3.25 +0.04 -0.71 +0.41 +1.93 -2.21 +4.27
Results on Supplementary Training Table 4 shows the performances gain when finetuning 4 subtasks based on models with the above SNLI and SQuAD2.0 supplementary training .
We mainly find that SNLI helps on Intent task , SQuAD2 mainly helps on NonCat task , while neither of them helps much on Cat task .
Recently , Namazifar et al. ( 2020 ) also found that when modeling dialog understanding as question answering task , it can benefit from a supplementary training on SQuAD2 dataset , especially on few-shot scenarios , which is a similar findings as our NonCat task .
Result difference on Req task is minor , because it is a relatively easy task , adding any supplementary training did n't help much .
Moreover , for Cat task , the sequence 2 of the input pair is the slot description with a categorical slot value , thus the meaning overlapping between the full dialog history and the slot / value is much smaller than SNLI tasks .
On the other side , CLS token in SQuAD BERT is finetuned for null predictions via start and end token classifers , which is different from the the single CLS classifer in Cat task .
Impact of Description Styles ( Q3 ) Previous work on schema-guided dialog are only based on the provided descriptions in SG - DST dataset .
Recent work on modeling dialog state tracking as reading comprehension ( Gao et al. , 2019 ) only formulate the descriptions as simple question format with existing intent / slot names , it is unknown how it performs when compared to other description styles .
Moreover , they only conduct homogeneous evaluation where training and test data share the same descrip-tion style .
In this section , We also investigate how a model trained on one description style will perform on other different styles , especially in a scenario where chat - bot developers may design their own descriptions .
We first introduce different styles of descriptions in our study , and then we train models on each description style and evaluate on tests with corresponding homogeneous and heterogeneous styles of descriptions .
Given the best performance of Cross- Encoder shown in the previous section and its popularity in DSTC8 challenges , we adopt it as our model architecture in this section .
Benchmarking Styles
For each intent / slot , we describe their functionalities by the following different descriptions styles : Identifer .
This is the least informative case of name - based description : we only use meaningless intent / slot identifiers , e.g. Intent_1 , Slot_2 .
It means we do n't use description from any schema component .
We want to investigate how a simple identifier - based description performs in schemaguided dialog modeling , and the performance lower - bound on transferring to unseen services .
NameOnly .
Using the original intent / slot names in SG - DST and MULTIWOZ 2.2 dataset as descriptions , to show whether name is enough for schema-guided dialog modeling .
Q- Name .
This is corresponding to previous work by Gao et al . ( 2019 ) .
For each intent / slot , it generate a question to inquiry about the intent and slot value of the dialog .
For each slot , it simply follows the template ' What is the value for slot i ? '.
Besides that , our work also extend the intent description by following the template " Is the user intending to intent j " .
Orig .
The original descriptions in SG - DST and MULTIWOZ 2.2 dataset .
Q- Orig .
Different from the Q-Name , firstly it is based on the original descriptions ; secondly , rather than always use the " what is " template to inquiry the intent / slot value , We add " what " , " which " , " how many " or " when " depending on the entity type required for the slot .
Same as Q-Name , we just add prefixes as " Is the user intending to .
. . " in front of the original description .
In a sum , this description is just adding question format to original description .
The motivation of this description is to see whether the question format is helpful or not for schema-guided dialog modeling .
To test the model robustness , we also create two paraphrased versions Name -Para and Orig -Para for NameOnly and Orig respectively .
We first use nematus ( Sennrich et al. , 2017 ) to automatically paraphrase the description with back translation , from English to Chinese and then translate back , then we manually check the paraphrase to retain the main meaning .
Appendix A.5.1 shows examples for different styles of schema descriptions .
Results on Description Styles
Unlike the composition used in Table 2 , we do n't use the service description to avoid its impact .
For each style , we train separate models on 4 subtasks , then we evaluate them on different target styles .
First ,
Table 5 summarizes the performance for homogeneous evaluation , while Table 6 shows how the question style description can benefit from SQuAD2 finetuning .
Then we also conduct heterogeneous evaluation on the other styles 7 as shown in Table 7
Homogeneous Evaluation
Is name - based description enough ?
As shown in Table 5 , Identifer is the worst case of using name description , its extremely bad performance indicates name - based description can be very unstable .
However , we found that simple meaningful name - based description actually can perform the best in Intent and Req task , and they perform 7 We do n't consider the meaningless Identifer style due to its bad performance worse on Cat and NonCat tasks comparing to the bottom two rich descriptions .
8 After careful analysis on the intents in SG - DST datasets , we found that most services only contains two kinds of intents , an information retrieval intent with a name prefix " Find - " , " Get - " , " Search - " ; another transaction intent like " Add - " , " Reserve - " or " Buy - " .
Interestingly , we found that all the intent names in the original schema-guided dataset strictly follows an action-object template with a composition of words without abbreviation , such as " FindEvents " , " BuyEventTickets " .
This simple name template is good enough to describe the core functionality of an intent in SG - DST dataset .
9 Additionally , Req is a relaitively simper task , requesting information are related to specifial attributes , such as " has_live_music " , " has_wifi " , where keywords cooccured in the slot name and in the user utterance , hence rich explanation cannot help further .
On the other side , rich descriptions are more necessary for Cat and NonCat task .
Because in many cases , slot names are too simple to represent the functionalities behind it , for example , slot name " passengers " cannot fully represent the meaning " number of passengers in the ticket booking " .
Does question format help ?
As shown in Table 5 , when comparing row Q-Orig v.s.
Orig , we found extra question format can improve the performance on Cat and NonCat task on both SG - DST and MULTIWOZ 2.2 datasets , but not for Intent and Req tasks .
We believe that question format helps the model to focus more on specific entities in the dialog history .
However , when adding a simple question pattern to NameOnly , comparing row Q-Name and NameOnly , there is no consistent improvement on both of the two datasets .
Further more , we are curious about whether BERT finetuned on SQuAD2 ( SQuAD2 - BERT ) can further help on the question format .
Because Non - Cat are similar with span-based question answering , we focus on NonCat here .
Table 6 shows that , after applying the supplementary training on SQuAD2 ( ?6 ) , almost all models get improved on unseen splits however slightly dropped on seen services .
Moreover , comparing to Q-Name , Q- Orig is more similar to the natural questions in the SQuAD2 , we obverse that Q-Orig gains more than Q-Name from pretrained model on SQuAD2 .
Heterogeneous
In this subsection , we first simulate a scenario when there is no recommended description style for the future unseen services .
Hence , unseen services can follow any description style in our case .
We average the evaluation performance on three other descriptions and summarized in Table 7 .
The ? column shows the performance change compared to the homogeneous performance .
It is not surprising that almost all models perform worse on heterogeneous styles than on homogeneous styles due to different distribution between training and evaluation .
The bold number shows the best average performance on heterogeneous evaluation for each subtask .
The trends are similar with the analysis in homogeneous evaluation 7.2.1 , the name - based descriptions perform better than other rich descriptions on intent classification tasks .
While on other tasks , the Orig description performs more robust , especially on NonCat task .
Furthermore , we consider another scenario where fixed description convention such as Name-Only and Orig are suggested to developers , they must obey the basic style convention but still can freely use their own words , such as abbreviation , synonyms , adding extra modifiers .
We train each model on NameOnly and Orig , then evaluate on the corresponding paraphrased version respectively .
In the last two rows of Table 7 , the column ' para ' shows performance on paraphrased schema , while ? shows the performance change compared to the homogeneous evaluation .
Orig still performs more robust than NameOnly when schema descriptions get paraphrased on unseen services .
Table 7 : Results on unseen service with heterogeneous description styles on SG - DST dataset .
More results and qualitative analysis are in the appendix A.5
Conclusion
In this paper , we studied three questions on schemaguided dialog state tracking : encoder architectures , impact of supplementary training , and effective schema description styles .
The main findings are as follows :
By caching the token embedding instead of the single CLS embedding , a simple partial -attention Fusion - Encoder can achieve much better performance than Dual- Encoder , while still infers two times faster than Cross-Encoder .
We quantified the gain via supplementary training on two intermediate tasks .
By carefully choosing representative description styles according to recent works , we are the first of doing both homogeneous / heterogeneous evaluations for different description style in schema-guided dialog .
The results show that simple name - based description performs well on Intent and Req tasks , while NonCat tasks benefits from richer styles of descriptions .
All tasks suffer from inconsistencies in description style between training and test , though to varying degrees .
Our study are mainly conducted on two datasets : SG - DST and MULTIWOZ 2.2 , while the speedaccuracy balance of encoder architectures and the findings in supplementary training are expected to be dataset - agnostic , because they depend more on the nature of the subtasks than the datasets .
Based on our proposed benchmarking descriptions suite , the homogeneous and heterogeneous evaluation has shed the light on the robustness of cross-style schema-guided dialog modeling , we believe our study will provide useful insights for future research .
