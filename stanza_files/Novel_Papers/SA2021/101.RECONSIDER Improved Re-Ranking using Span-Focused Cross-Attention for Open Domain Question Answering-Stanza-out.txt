title
RECONSIDER : Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering
abstract
State- of- the-art Machine Reading Comprehension ( MRC ) models for Open-domain Question Answering ( QA ) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples .
This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions , but a low overall accuracy , motivating the need for answer re-ranking .
We develop a successful re-ranking approach ( RECONSIDER ) for span-extraction tasks that improves upon the performance of MRC models , even beyond large-scale pre-training .
RE -CONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions , and uses in - passage span annotations to perform span-focused reranking over a smaller candidate set .
As a result , RECONSIDER learns to eliminate close false positives , achieving a new extractive state of the art on four QA tasks , with 45.5 % Exact Match accuracy on Natural Questions with real user questions , and 61.7 % on TriviaQA .
We will release all related data , models , and code 1 .
Introduction Open-domain Question Answering ( Voorhees et al. , 1999 ) ( QA ) involves answering questions by extracting correct answer spans from a large corpus of passages , and is typically accomplished by a light - weight passage retrieval model followed by a heavier Machine Reading Comprehension ( MRC ) model ( Chen et al. , 2017 ) .
The span selection components of MRC models are trained on distantly supervised positive examples ( containing the answer string ) together with heuristically chosen negative examples , typically from upstream retrieval models .
This training scheme possibly explains empirical findings ( Wang et al. , 2018 b , c ) that while MRC models can confidently identify top - K answer candidates ( high recall ) , they cannot effectively discriminate between top semantically similar false positive candidates ( low accuracy ) .
In this paper , we develop a general approach to make answer reranking successful for span-extraction tasks , even over large pretrained models , and improve the state of the art on four QA datasets .
Earlier work ( Wang et al. , 2018 c , b ) on opendomain QA have recognized the potential of answer re-ranking , which we continue to observe despite recent advances using large pre-trained models like BERT .
Figure 1 shows the top - 3 predictions of a BERT - based SOTA model on a question from Natural Questions ( NQ ) ( Kwiatkowski et al. , 2019 ) , " Who was the head of the Soviet Union when it collapsed ? "
While all predictions are very relevant and refer to Soviet Union heads , Mikhail Gorbachev is correct and the rest are close false positives .
any of the top-k predictions for k = 1 , 5 , 10 and 25 .
We observe that an additional 10 % and 20 % of correct answers exist amongst the top - 5 and top - 25 candidates respectively , presenting an enormous opportunity for span reranking models .
Our re-ranking model is trained using positive and negative examples extracted from high confidence MRC model predictions , and thus , learns to eliminate hard false positives .
This can be viewed as a coarse- to-fine approach of training span selectors , with the base MRC model trained on heuristically chosen negatives and the re-ranker trained on finer , more subtle negatives .
This contrasts with multi-task training approaches ( Wang et al. , 2018 c ) , whose re-scoring gains are limited by training on the same data , especially when coupled with large pre-trained models .
Our approach also scales to any number of ranked candidates , unlike previous concatenation based cross - passage re-ranking methods ( Wang et al. , 2018 b ) that do not transfer well to current length - bounded large pre-trained models .
Similar to MRC models , our re-ranking approach uses cross-attention between the question and a candidate passage ( Seo et al. , 2016 ) .
However , we now demarcate a specific candidate answer span in each passage , to assist the model to perform span-focused reasoning , in contrast to MRC models , which must reason across all spans .
Therefore , the re-ranker performs span ranking of carefully chosen candidates , rather than span selection like the MRC model .
Similar focused cross-attention methods have recently proved to be effective for Entity Linking tasks , although they annotate the query rather than the passage .
We use our broadly applicable span-focused reranking approach on models from and achieve a new extractive state of the art on four QA datasets , including 45.5 % on the opendomain setting of NQ ( real user queries , + 1.6 % on small models ) and 61.1 % on TriviaQA ( Joshi et al. , 2017 ) ( + 2.5 % on small models ) .
To our knowledge , we are the first to successfully leverage re-ranking to improve over large pre-trained models on opendomain QA .
Background Open-domain Question Answering ( QA ) aims to answer factoid questions from a large corpus of passages ( Voorhees et al. , 1999 ) ( such as Wikipedia ) in contrast with single passage MRC tasks ( Rajpurkar et al. , 2016 ) .
Prior works use pipelined approaches , that first retrieve candidate passages and subsequently use a neural MRC model to extract answer spans ( Chen et al. , 2017 ) , with further improvements using joint learning ( Wang et al. , 2018a ; Tan et al. , 2018 ) .
Recent successes involve improving retrieval , thereby increasing the coverage of passages fed into the MRC model ( Guu et al. , 2020 ; .
In this paper , we significantly improve MRC model performance by making re-ranking successful using span-focused re-ranking of its highly confident predictions .
For Open-domain QA , it is crucial to train MRC models to distinguish passage -span pairs containing the answer ( positives ) from those that do not ( negatives ) .
Using negatives that appear as close false positives can produce more robust MRC models .
However , prior work relies on upstream retrieval models to supply distantly supervised positives ( contain answer string ) and negatives ( Asai et al. , 2020 ) , that are in- turn trained using heuristically chosen positives and negatives .
Our approach leverages positives and negatives from highly confident MRC predictions which are hard to classify , and thus , improve upon MRC model performance .
Jia and Liang ( 2017 ) motivate recent work on answer verification for QA by showing that MRC models are easily confused by similar passages .
Wang et al. ( 2018 b ) use a weighted combination of three re-rankers and rescore a concatenation of all passages with a particular answer using a sequential model , while , Wang et al . ( 2018 c ) develop a multi-task end-to - end answer scoring approach .
Although the main idea is to consider multiple passage -span candidates collectively , such approaches either used concatenation , which is prohibitively expensive to couple with lengthrestricted models like BERT , or are trained on the same data without variations only to realize marginal gains .
Hu et al . ( 2019 ) use answer verification to predict the unanswerability of a question - passage pair for traditional MRC tasks .
To our knowledge , our work is the first to ( i ) successfully demonstrate a re-ranking approach that significantly improves over large pre-trained models in an open domain setting , and ( ii ) use annotated top model predictions as harder negatives to train more robust models for QA .
Model
We assume an extractive MRC model M coupled with a passage retrieval model , that given a question q and a passage corpus P , produces a list of N passage and span pairs , {( p j , s j ) }
N j=1 , p j ?
P and s j is a span within p j , ranked by the likelihood of s j answering q.
Note that { p j } N j=1 is not disjoint as a passage can have multiple answer spans .
In this section , we develop a span-focused re-ranking model R , that learns a distribution p , over top - K ( p j , s j ) pairs 1 ? j ? K , given question q.
Essentially , model R first scores every ( q , p j , s j ) triple using scoring function r , and then normalizes over these scores to produce p : p( q , p j , s j ) = e r( q , p j , s j ) 1 ?k? K e r( q , p k , s k ) .
( 1 ) Specifically , if E(q , p j , s j ) ?
R H is a dense representation of ( q , p j , s j ) , r is defined as : r( q , p j , s j ) = w T E(q , p j , s j ) , ( 2 ) where w ?
R H is a learnable vector .
Span-focused tuple encoding
We compute E using the representation of the [ CLS ] token of a BERT model applied to a spanfocused encoding of ( q , p j , s j ) .
This encoding is generated by first marking the tokens of s j within passage p j with special start and end symbols [ A ] and [ /A ] , to form pj , followed by concatenating the [ CLS ] and question tokens , with the annotated passage tokens pj , using separator token [ SEP ] .
We find span marking to be a crucial ingredient for answer re-ranking , without which , performance deteriorates ( Section 5 ) .
Training
We obtain top K predictions ( p j , s j ) of model M for each question q i in its training set , which we divide into positives , where s j is exactly the groundtruth answer , and remaining negatives .
We train R using mini-batch gradient descent , where in each iteration , for question q , we include 1 randomly chosen positive and M ?
1 randomly chosen negatives , and maximize the likelihood of the positive .
Unlike the heuristically chosen negatives used to train M , R is trained using negatives from high confidence predictions of M , which are harder to classify .
Thus , this can be viewed as an effective coarse- to -fine negative selection strategy for span extraction models ( Section 5 ) .
Baseline Model M
We use the state- of - the - art models of which consists of 1 ) a dense passage retriever , and 2 ) a span extractive BERT reader , as our model M .
The retriever uses a passage encoder f p and a question encoder f q to represent all passages and questions as dense vectors in the same space .
During inference , it retrieves top - 100 passages similar to question q based on their inner product , and passes them on to the MRC reader .
The MRC reader is an extension of model R of Section 3 , to perform span extraction .
We briefly describe it but has complete details .
Its input is a question q together with positive and negative passages p j from its retrieval model .
( q , p j ) tuples are encoded as before ( enc( q , p j ) = q [ SEP ] p j ) , but without spans being marked ( as spans are unavailable ) .
A distribution over passages p s is computed as before using scoring function r and context encoder E .
In addition , a start-span probability , p st ( t i |q , p j ) and an end-span probability , p e ( t i |q , p j ) is computed for every token t i in enc(q , p j ) .
The model is trained to maximize the likelihood of p s ( p j ) ?
p st ( s|q , p j ) ? p e ( t|q , p j ) for each correct answer span ( s , t ) in p j , and outputs the top - K scoring passage - span pairs during inference .
Experiments Datasets
We use four benchmark open-domain QA datasets following : Natural Questions ( NQ ) contains real user questions asked on Google searches ; we consider questions with short answers up to 5 tokens .
TRIVIAQA ( Joshi et al. , 2017 ) consists of questions collected from trivia and quiz-league websites ; we take questions in an unfiltered setting and discard the provided web snippets .
WebQuestions ( WEBQ ) ( Berant et al. , 2013 ) ( Min et al. , 2019a ) 28.1 50.9 -- GraphRetriever ( Min et al. , 2019 b ) 34.5 56.0 36.4 - PathRetriever ( Asai et al. , 2020 ) 32.6 ---REALM ( Guu et al. , 2020 ) 39.2 -40.2 46.8 REALM News ( Guu et al. , 2020 ) 40.4 - 40.7 42.9
Models that use DPR multi DPR - BERT base ( Karpukhin et Implementation details
For all datasets , we use the retrieval model ( without retraining ) and setup from , retrieving 100 - token passages from a Wikipedia corpus ( from 2018 - 12- 20 ) .
We also use their MRC model with their best performing hyperparameters as model M. For model R , we experiment with both BERT base and BERT large , use top - 100 predictions from model M during training ( top - 5 for testing ) , and use M = 30 .
We use a batch size of 16 on NQ and TRIVI - AQA and 4 otherwise .
For WEBQ and TREC , we start training from our trained NQ model .
Results
Table 2 presents end-to- end test-set exact match accuracies for these datasets , compared with previous models .
The BERT base version of RECONSIDER outperforms the previous state- ofthe - art DPR model of ( our model M ) by 1.6 % on NQ and ? 2 % on TRIV -IAQA and WEBQ .
For training on the smaller WEBQ and TREC datasets , we initialize models using the corresponding NQ model .
Table 2 demonstrates the effectiveness of a coarse- to-fine approach for selecting negative passages , with dense retrieval based negatives ( DPR ) outperforming BM25 , and in turn , improved upon by our reranking approach .
We obtain gains despite R being not only very similar in architecture to the MRC reader M , but also trained on the same QA pairs , owing to ( i ) training using harder false - positive style negatives , and ( ii ) answer-span annotations that allow a re-allocation of modeling capacity from modeling all spans to reasoning about specific spans with respect to the question and the passage .
Re-ranking performance suffers without these crucial methods .
For example , replacing answer-span annotations with answer concatenation reduces accuracy by ?1 % on the dev set of NQ .
We train a large variant of RECONSIDER using BERT large for model R , trained on predictions from a BERT large model M.
For a fair comparison , we re-evaluate DPR using BERT large .
RECONSIDER large outperforms it by ?1 % on all datasets ( + ? 2 % on TREC ) .
This model is also comparable in size to RAG ( which uses BART large ) but outperforms it on all tasks ( + 1 on NQ , + 5.5 on TRIVIAQA , +3 on TREC ) , demonstrating that retrieve-extract architectures can perform better than answer generation models .
We find K=5 ( testing ) to be best for all datasets , and increasing K has little effect on accuracy , despite training on top - 100 predictions .
Although in contrast with our expectations based on Table 1 , this is anticipated since very low-ranked predictions are less likely to be reranked highly , but this also presents an opportunity for future work .
In Table 3 , we present examples from the validation set of NQ , of cases where 1 ) DPR - BERT base
Model Prediction
Question
Where are zebra mussels found in the United States ?
DPR -BERTbase ... on the genetic algorithm for rule - set production ( garp ) , a group of researchers predicted that the southeastern united states is moderately to highly likely to be inhabited by zebra mussels ... + RECONSIDER ... of zebra mussel in the great lakes alone exceeds $ 500 million a year .
Question
Where do you find neurons in the brain ?
DPR -BERTbase ... there is strong evidence for generation of substantial numbers of new neurons in two brain areas , the hippocampus and olfactory bulb .
A neuron is a specialized type of cell found in the bodies ... + RECONSIDER
The brain is the most complex organ in a vertebrate 's body .
In a human , the cerebral cortex contains approximately 14 - 16 billion neurons .
C Validation Performance
Table 8 presents validation set performance for the experiments that we ran for this paper .
D Hyperparameters
For training RECONSIDER , we use top - 100 predictions of the baseline MRC model .
This was chosen based on validation set accuracy , and other values that were experimented with were 50 and 75 .
For training RECONSIDER we use 1 positive and M ?1 negatives during each iteration .
We tried values of M between 5 and 40 in increments of 5 and chose M = 30 based on validation set accuracy ( see Table 6 ) .
Similarly , we re-rank K = 5 candidates K Accuracy ( % ) 3
E Dataset Statistics Figure 1 : 1 Figure 1 : Top - 3 passage-spans predicted by a BERT - MRC model on a question from NQ ( answer spans are underlined ) .
RECONSIDER re-evaluates the passages with marked candidate answers , eliminates close false positives and ranks Mikhail Gorbachev as correct .
Table 1 presents accuracies obtained by the same model on four QA datasets , if the answer exactly matches Dataset Top - 1 Top -5 Top -10 Top -25 NQ 40.3 49.5 50.9 62.4 TRIVIAQA 57.2 64.6 65.7 73.1 WEBQ 42.6 49.0 50.7 60.4 TREC 49.6 58.7 60.9 71.4 Table 1 : Top-k EM accuracies using a state - of - the - art model ( Karpukhin et al. , 2020 ) on four open-domain QA tasks ( dev set ) .
Improvements of up to 22 % can po- tentially be achieved by re-ranking top - 25 candidates .
is a collection of questions extracted from the Google Suggest API , with answers being Freebase entities .
Model NQ TRIVIAQA WEBQ TREC BM25 + BERT ( Lee et al. , 2019 ) 26.5 47.1 17.7 21.3 ORQA ( Lee et al. , 2019 ) 33.3 45.0 36.4 30.1 HardEM CuratedTREC ( Baudi ? and ?ediv? , 2015 ) con- tains curated questions from TREC QA track .
Table 2 : 2 End-to- end QA test-set ( Exact Match ) accuracy .
Models in the lower half use dense passage retrieval from .
RECONSIDER outperforms previous methods under both base and large versions .
al. , 2020 ) 41.5 56.8 42.4 49.4 RECONSIDER base ( Ours ) 43.1 59.3 44.4 49.3 DPR - BERT ? large ( Karpukhin et al. , 2020 ) 44.6 60.9 44.8 53.5 RAG large ( Lewis et al. , 2020 ) 44.5 56.1 45.5 52.2 RECONSIDER large ( Ours ) 45.5 61.7 45.9 55.3
Table layout and baselines are borrowed from Karpukhin et al . ( 2020 ) ( published extractive SOTA ) .
Dataset stats and dev set results are in Appendix , and baseline descriptions can be found in .
? : numbers from our own experiments .
QuestionWho said if I have seen further it is by standing on the shoulders of giants ?
DPR -BERTbase Standing on the shoulders of giants ... this concept has been traced to the 12th century , attributed to Bernard of Chartres .
Its most familiar expression in English is by Isaac Newton in 1675 : " If I have seen further it is by standing on the shoulders of giants ... + RECONSIDER
Standing on the shoulders of giants ... this concept has been traced to the 12th century , attributed to Bernard of Chartres .
Its most familiar expression in English is by Isaac Newton in 1675 : " If I have seen further it is by standing on the shoulders of giants ...
Table 3 : 3 Top passage with answer span ( in bold ) for example questions from the validation set of NQ , both with and without re-ranking using RECONSIDER .
For the first two examples , RECONSIDER re-ranks to obtain the correct answer , while in the last example , re-ranking eliminates the already correct top answer .
produces an incorrect top answer , which is cor- rected after re-ranking with RECONSIDER ( top 2 examples ) , and 2 ) DPR - BERT base 's answer is correct but is ranked lower after re-ranking .
Of the 15.4 % validation examples that were amenable for correction by re-ranking the top - 5 candidates from DPR - BERT base , RECONSIDER was able to fix 6.1 % .
However , in this process , 4.3 % of answers that were originally correct ( top- ranked ) , lost their top-rank after RECONSIDER , and this presents an opportunity for further improving re-ranking .
6 Conclusion
We use a synergistic combination of two techniques viz .
retraining with harder negatives , and , span - focused cross attention , to make re-ranking success - ful for span-extractive tasks over large pretrained models .
This method achieves SOTA extractive re- sults on four open domain QA datasets , also outper - forming recent generative pre-training approaches .
Table 4 : 4 Runtime for training and inference , and number of parameters of the models that we executed , on NQ .
Runtimes for RECONSIDER do not include the time required to train and obtain predictions from DPR .
Table 5 : 5 Hyper-parameter tuning for K at inference time on the validation set of NQ .
Train -M Accuracy ( % ) 25 42.34 30 42.48 40 41.25
Table 6 : 6 Hyper-parameter tuning for the number of negative passages i.e. train - M , on the validation set of NQ.during inference , and this value was chosen by experimenting with values 2 , 3 , 4 and values between 5 and 20 , in increments of 5 ( see Table5 ) .
Dataset Train Dev Test NQ 67,098 8,757 3,610 TRIVIAQA 67,975 8,837 11,313 WEBQ 2,898 361 2,032 TREC 1,240 133 694
Table 7 : 7 Training , validation and testing set sizes for the four open-domain QA tasks evaluated in our paper .
Table 7 presents the number of examples in the training , validation and testing splits of the four open-domain QA datasets that we use , based on the dataset prepared by .
