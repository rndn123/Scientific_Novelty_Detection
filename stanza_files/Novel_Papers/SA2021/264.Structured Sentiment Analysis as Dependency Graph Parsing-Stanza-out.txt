title
Structured Sentiment Analysis as Dependency Graph Parsing
abstract
Structured sentiment analysis attempts to extract full opinion tuples from a text , but over time this task has been subdivided into smaller and smaller sub-tasks , e.g. , target extraction or targeted polarity classification .
We argue that this division has become counterproductive and propose a new unified framework to remedy the situation .
We cast the structured sentiment problem as dependency graph parsing , where the nodes are spans of sentiment holders , targets and expressions , and the arcs are the relations between them .
We perform experiments on five datasets in four languages ( English , Norwegian , Basque , and Catalan ) and show that this approach leads to strong improvements over state - of - the - art baselines .
Our analysis shows that refining the sentiment graphs with syntactic dependency information further improves results .
Introduction Structured 1 sentiment analysis , i.e. , the task of predicting a structured sentiment graph like the ones in Figure 1 , can be theoretically cast as an information extraction problem in which one attempts to find all of the opinion tuples O = O i , . . . , O n in a text .
Each opinion O i is a tuple ( h , t , e , p ) where h is a holder who expresses a polarity p towards a target t through a sentiment expression e , implicitly defining pairwise relationships between elements of the same tuple .
Liu ( 2012 ) argues that all of these elements 2 are essential to fully resolve the sentiment analysis problem .
However , most research on sentiment analysis focuses either on a variety of sub-tasks , which avoids performing the full task , or on simplified and idealized tasks , e.g. , sentence - level binary polarity classification .
We argue that the division of structured sentiment into these sub-tasks has become counterproductive , as reported experiments are often not sensitive to whether a given addition to the pipeline improves the overall resolution of sentiment , or do not take into account the inter-dependencies of the various sub-tasks .
As such , we propose a unified approach to structured sentiment which jointly predicts all elements of an opinion tuple and their relations .
Moreover , we cast sentiment analysis as a dependency graph parsing problem , where the sentiment expression is the root node , and the other elements have arcs which model the relationships between them .
This methodology also enables us to take advantage of recent improvements in semantic dependency parsing ( Dozat and Manning , 2018 ; Kurtz et al. , 2020 ) to efficiently learn a sentiment graph parser .
This perspective also allows us to unify a number of approaches , including targeted , and opinion tuple mining .
We aim to answer RQ1 : whether graph- based approaches to structured sentiment outperform state - of - the - art sequence labeling approaches , and RQ2 : how to best encode structured sentiment as parsing graphs .
We perform experiments on five standard datasets in four languages ( English , Norwegian , Basque , Catalan ) and show that graph - based approaches outperform state- ofthe - art baselines on all datasets on several standard metrics , as well as our proposed novel ( unlabeled and labeled ) sentiment graph metrics .
We further propose methods to inject linguistic structure into the sentiment graphs using syntactic dependencies .
Our main contributions are therefore 1 ) proposing a holistic approach to structured sentiment through 3388 Some others give the new UMUC 5 stars - do n't believe them . positive negative holder target expression target expression Figure 1 : A structured sentiment graph is composed of a holder , target , sentiment expression , their relationships and a polarity attribute .
Holders and targets can be null .
sentiment graph parsing , 2 ) introducing new evaluation metrics for measuring model performance , and 3 ) extensive experimental results that outperform state - of - the - art baselines .
Finally , we release the code and datasets 3 to enable future work on this problem .
Related Work Structured sentiment analysis can be broken down into five sub-tasks : i ) sentiment expression extraction , ii ) sentiment target extraction , iii ) sentiment holder extraction , iv ) defining the relationship between these elements , and v ) assigning polarity .
Previous work on information extraction has used pipeline methods which first extract the holders , targets , and expressions ( tasks iiii ) and subsequently predict their relations ( task iv ) , mostly on the MPQA dataset ( Wiebe et al. , 2005 ) . CRFs and a number of external resources ( sentiment lexicons , dependency parsers , named-entity taggers ) ( Choi et al. , 2006 ; Yang and Cardie , 2012 ) are strong baselines .
Given the small size of the training data and the complicated task , these techniques often still outperform neural models , such as BiLSTMs ( Katiyar and Cardie , 2016 ) .
Transition - based end-toend approaches have shown some potential ( Zhang et al. , 2019 ) .
However , all of this work ignores the polarity classification subtask .
Targeted sentiment analysis only concentrates on extracting sentiment targets ( task ii ) and classifying the polarity directed towards them ( task iv ) ( Jiang et al. , 2011 ; Mitchell et al. , 2013 ) .
Recent shared tasks on Aspect- Based Sentiment Analysis ( ABSA ) ( Pontiki et al. , 2014 ( Pontiki et al. , , 2015 ( Pontiki et al. , , 2016 ) also include target extraction and polarity classification subtasks .
Joint approaches perform on par with pipeline methods ( Li et al. , 2019a ) and multitask models can perform even better ( He et al. , 2019 ) .
Finally , pretrained language models ( Devlin et al. , 2019 ) can also lead to improvements on the ABSA data ( Li et al. , 2019 b ) .
End2End sentiment analysis is a recently proposed subtask which combines targeted sentiment ( tasks ii and v ) and sentiment expression extraction ( task i ) , without requiring the resolution of relationships between targets and expressions .
Wang et al. ( 2016 ) augment the ABSA datasets with sentiment expressions , but provide no details on the annotation process or any inter-annotator agreement .
He et al . ( 2019 ) make use of this data and propose a multi-layer CNN ( IMN ) to create hidden representations h which are then fed to a target and opinion extraction module ( AE ) , which is also a multi-layer CNN .
This module predicts ? ae , a sequence of BIO tags 4 that predict the presence or absence of targets and expressions .
After jointly predicting the targets and expressions , a second multi-layer CNN with a final self-attention network is used to classify the polarity , again as sequence labeling task ( AS ) .
This second module combines the information from h and ?ae by incorporating the predicted probability of a token to be a target in the formulation of self-attention .
Finally , an iterative message - passing algorithm updates h using the predictions from all the modules at the previous timestep .
Chen and Qian ( 2020 ) instead propose Relation - Aware Collaborative Learning ( RACL ) .
This model creates task specific representations by first embedding a sentence , passing through a shared feed -forward network and finally a task-specific CNN .
This approach then models interactions between each pair of sub-tasks ( target extraction , expression extraction , sentiment classification ) by creating pairwise weighted attention representations .
These are then concatenated and used to create the task -specific predictions .
The authors finally stack several RACL layers , using the output from the previous layer as input for the next .
3389
Both models perform well on the augmented Se- mEval data , but it is unlikely that these annotations are adequate for full structured sentiment , as Wang et al . ( 2016 ) only provide expression annotations for sentences that have targets , generally only include sentiment - bearing words ( not phrases ) , and do not specify the relationship between target and expression .
Finally , the recently proposed aspect sentiment triplet extraction ( Peng et al. , 2019 ; ? ) attempts to extract targets , expressions and their polarity .
However , the datasets used are unlikely to be adequate , as they augment available targeted datasets , but do not report annotation guidelines , procedure , or inter-annotator agreement .
Graph parsing : Syntactic dependency graphs are regularly used in applications , supplying them with necessary grammatical information ( Mintz et al. , 2009 ; Cui et al. , 2005 ; Bj?rne et al. , 2009 ; Johansson and Moschitti , 2012 ; Lapponi et al. , 2012 ) .
The dependency graph structures used in these systems are predominantly restricted to trees .
While trees are sufficient to encode syntactic dependencies , they are not expressive enough to handle meaning representations , that require nodes to have multiple incoming arcs , or having no incoming arcs at all ( Kuhlmann and Oepen , 2016 ) .
While much of the early research on parsing these new structures ( Oepen et al. , 2014 ( Oepen et al. , , 2015 focused on specialized decoding algorithms , Dozat and Manning ( 2018 ) presented a neural dependency parser that essentially relies only on its neural network structure to predict any type of dependency graph without restrictions to certain structures .
Using the parser 's ability to learn arbitrary dependency graphs , Kurtz et al . ( 2020 ) phrased the task of negation resolution ( Morante and Blanco , 2012 ; Morante and Daelemans , 2012 ) as a graph parsing task .
This transformed the otherwise flat representations to dependency structures that directly encode the often overlapping relations between the building blocks of multiple negation instances at the same time .
In a simpler fashion , Yu et al . ( 2020 ) exploit the parser of Dozat and Manning ( 2018 ) to predict spans of named entities .
Datasets
We here focus on datasets that annotate the full task of structured sentiment as described initially .
We perform experiments on five structured sentiment datasets in four languages , the statistics of which are shown in Table 1 .
The largest available structured sentiment dataset is the NoReC Fine dataset ( ? vrelid et al. , 2020 ) , a multi-domain dataset of professional reviews in Norwegian , annotated for structured sentiment .
MultiB EU and MultiB CA ( Barnes et al. , 2018 ) are hotel reviews in Basque and Catalan , respectively .
MPQA ( Wiebe et al. , 2005 ) annotates news wire text in English .
Finally , DS Unis ( Toprak et al. , 2010 ) annotate English reviews of online universities and e-commerce .
In our experiments , we use only the university reviews , as the e-commerce reviews have a large number of ' polar targets ' , i.e. , targets with a polarity , but no accompanying sentiment expression .
While all the datasets annotate holders , targets , and expressions , the frequency and distribution of these vary .
Regarding holders , MPQA has the most ( 2,054 ) and DS Unis has the fewest ( 94 ) , whereas NoReC Fine has the largest proportion of targets ( 8,923 ) and expressions ( 11 , 115 ) .
The average length of holders ( 2.6 tokens ) and targets ( 6.1 tokens ) in MPQA is also considerably higher than the others .
It is also worth pointing out that MPQA and DS Unis additionally include neutral polarity .
In the case of MPQA the neutral class refers to verbs which are subjective but do not convey polarity , e.g. , ' say ' , ' opt for ' .
In DS Unis , however , the neutral label tends to indicate expressions that could entail mixed polarity or are polar under the right conditions , e.g. , ' the classes were not easy ' is considered neutral , as it is possible for difficult classes to be desirable at a university .
MultiB EU , and MultiB CA also have labels for strong positive and strong negative , which we map to positive and negative , respectively .
Finally , NoReC
Fine includes intensity annotations ( strong , normal , slight ) , which we disregard for the purposes of these experiments .
Modeling
This section describes how we define and encode sentiment graphs , detail the neural dependency graph models , as well as two state - of - the - art baselines for end-to - end sentiment analysis ( target and expression extraction , plus polarity classification ) .
Graph Representations
Structured sentiment graphs as in Figure 1 can span over multiple tokens and may have multiple incoming edges .
The resulting graphs can have multiple entry points ( roots ) , are not necessarily connected , and not every token is a node in the graph .
The sentence 's sentiment expressions correspond to the roots of the graphs , connecting explicitly to their respective holders and targets .
In order to apply the algorithm of Dozat and Manning ( 2018 ) , we simplify these structures into bi-lexical dependency graphs visualized in Figure 2 .
Here , nodes correspond one - to -one to the tokens of the sequence and follow the same linear order .
The edges are drawn as arcs in the half - plane above the sentence , connecting heads to dependents .
Similarly to the source structures , the graphs can have multiple roots and nodes can have multiple or no incoming arcs .
For some rare instances of structured sentiment graphs , the reduction to dependency graphs is lossy , as they do not allow multiple arcs to share the same head and dependent .
This results in a slight mismatch of the learned and aimed - for representations .
The choice of how to encode the sentiment graphs as parsing graphs opens for several alternate representations depending on the choice of head / dependent status of individual tokens in the target / holder / expression spans of the sentiment graph .
We here propose two simple parsing graph representations : head - first and head-final , which are shown in Figure 2 .
For head - first , we set the first token of the sentiment expression as a root node , and similarly set the first token in each holder and token span as the head of the span with all other tokens within that span as dependents .
The labels simply denote the type of relation ( target / holder ) and for sentiment expressions , additionally encode the polarity .
Head-final is similar , but instead sets the final token of spans as the heads , and the final token of the sentiment expression as the root node .
Some others give the new UMUC 5 stars - do n't believe them .
Proposed model
The neural graph parsing model used in this work is a reimplementation of the neural parser by Dozat and Manning ( 2018 ) which was used by Kurtz et al . ( 2020 ) for negation resolution .
The parser learns to score each possible arc to then finally predict the output structure simply as a collection of all positively scored arcs .
The base of the network structure is a bidirectional LSTM ( BiLSTM ) , that processes the input sentence both from left-toright and right - to - left , to create contextualized representations c 1 , . . . , c n = BiLSTM ( w 1 , . . . , w n ) where w i is the concatenation of a word embedding , POS tag embedding , lemma embedding , and character embedding created by a character - based LSTM for the ith token .
In our experiments , we further augment the token representations with pretrained contextualized embeddings from multilingual BERT .
We use multilingual BERT as several languages did not have available monolingual BERT models at the time of the experiments ( Catalan , Norwegian ) .
The contextualized embeddings are then processed by two feedforward neural networks ( FNN ) , creating specialized representations for potential heads and dependents , h i = FNN head ( c i ) and d i = FNN dep ( c i ) .
The scores for each possible arclabel combination are computed by a final bilinear transformation using the tensor U .
Its inner dimension corresponds to the number of sentiment graph labels plus a special NONE label , indicating the ab-sence of an arc , which allows the model to predict arcs and labels jointly , score ( h i , d j ) = h i U d j .
Baselines
We compare our proposed graph prediction approach with three state - of - the - art baselines 5 for extracting targets and expressions and predicting the polarity : IMN 6 , RACL 7 , as well as RACL - BERT , which also incorporates contextualized embeddings .
Instead of using BERT Large , we use the cased BERT - multilingual - base in order to fairly compare with our own models .
Note , however , that our model does not update the mBERT representations , putting it at a disadvantage to RACL - BERT .
We also compare with previously reported extraction results from Barnes et al . ( 2018 ) and ? vrelid et al . ( 2020 ) .
Evaluation
As we are interested not only in extraction or classification , but rather in the full structured sentiment task , we propose metrics that capture the relations between all predicted elements , while enabling comparison with previous state - of - the - art models on different subtasks .
The main metrics we use to rank models are Targeted F 1 and Sentiment Graph F 1 . Token - level F 1 for Holders , Targets , and Expressions
To easily compare our models to pipeline models , we evaluate how well these models are able to identify the elements of a sentiment graph with token - level F 1 . Targeted F 1 This is a common metric in targeted sentiment analysis ( also referred to as F 1 - i ( He et al. , 2019 ) or ABSA F 1 ( Chen and Qian , 2020 ) ) .
A true positive requires the combination of exact extraction of the sentiment target , and the correct polarity .
Parsing graph metrics
We additionally compute graph- level metrics to determine how well the models predict the unlabeled and labeled arcs of the parsing graphs : Unlabeled F 1 ( UF 1 ) , Labeled F 1 ( LF 1 ) .
These measure the amount of ( in ) correctly predicted arcs and labels , as the harmonic mean of precision and recall ( Oepen et al. , 2014 ) .
These metrics inform us of the local properties of the graph , and do not overly penalize a model if a few edges of a graph are incorrect .
Sentiment graph metrics
The two metrics that measure how well a model is able to capture the full sentiment graph ( see Figure 1 ) are Non-polar Sentiment Graph F 1 ( NSF 1 ) and Sentiment Graph F 1 ( SF 1 ) .
For NSF 1 , each sentiment graph is a tuple of ( holder , target , expression ) , while for SF 1 we include polarity ( holder , target , expression , polarity ) .
A true positive is defined as an exact match at graph-level , weighting the overlap in predicted and gold spans for each element , averaged across all three spans .
For precision we weight the number of correctly predicted tokens divided by the total number of predicted tokens ( for recall , we divide instead by the number of gold tokens ) .
We allow for empty holders and targets .
Experiments
All sentiment graph models use token - level mBERT representations in addition to word2vec skip-gram embeddings openly available from the NLPL vector repository 8 ( Fares et al. , 2017 ) .
We train all models for 100 epochs and keep the model that performs best regarding LF 1 on the dev set ( Targeted F 1 for the baselines ) .
We use default hyperparameters from Kurtz et al . ( 2020 ) ( see Appendix ) and run all of our models five times with different random seeds and report the mean ( standard deviation shown as well in Table 8 in the Appendix ) .
We calculate statistical difference between the best and second best models through a bootstrap with replacement test ( Berg- Kirkpatrick et al. , 2012 ) .
As there are 5 runs , we require that 3 of 5 be statistically significant at p < 0.05 .
Table 3 shows the results for all datasets .
On NoReC Fine , the baselines IMN , RACL , and RACL - BERT perform well at extracting targets ( 35.9 , 45.6 , and 47.2 F 1 , respectively ) and expressions ( 48.7/55.4/56.3 ) , but struggle with the full targeted sentiment task ( 18.0/20.1/30.3 ) .
The graphbased models extract targets better ( 50.1/54.8 ) and have comparable scores for expressions ( 54.4/55.5 ) .
The holder extraction scores have a similar range ( 51.1/60.4 ) .
These patterns hold throughout the other datasets , where the proposed graph models nearly always perform best on extracting spans , although RACL - BERT achieves the best score on extracting targets on DS Unis ( 44.6 vs. 42.1 ) .
The graph models also outperform the strongest baseline ( RACL - BERT ) on targeted sentiment on all 5 datasets , although this difference is often not statistically significant ( NoReC Fine Head-first , MultiB EU Head-final ) and RACL - BERT is better than Head-first on DS Unis .
Regarding the Graph metrics , the results depend highly on the dataset , with UF 1 and LF 1 ranging from 35.3/31.4 ( DS Unis Head-first ) to 66.8/62.1 ( MultiB CA Head-first ) .
Sentiment Graph metrics NSF 1 and SF 1 have a similar , though slightly lower range ( 24.5/17.7 - 62.0/56.8 ) .
The graph and sentiment graph metrics do not correlate perfectly , however , as UF 1 and LF ( 40.0/36.9 and 41.4/38.0 for Head-first and Headfinal , respectively ) , but the NSF 1 and SF 1 are poor ( 24.5/17.4 and 26.1/18.8 ) .
On average IMN is the weakest baseline , followed by RACL and then RACL - BERT .
The main improvement that RACL - BERT gives over RACL on these datasets is seen in the Targeted metric , i.e. , the contextualized representations improve the polarity classification more than the extraction task .
The proposed graph- based models are consistently the best models across the metrics and datasets .
Regarding graph representations , the differences between Head-first and Head-final are generally quite small .
Head-first performs better on MultiB CA and slightly better on MultiB EU , while for the others ( NoReC Fine , MPQA , and DS Unis ) Head-final is better .
This suggests that the main benefit is the joint prediction of all spans and relationships , and that the specific graph representation matters less .
Analysis
In this section we perform a deeper analysis of the models in order to answer the research questions .
Do syntactically informed sentiment graphs improve results ?
Our two baseline graph representations , Head-first and Head-final , are crude approximations of linguistic structure .
In syntactic and semantic dependency graphs , heads are often neither the first or last word , but rather the most salient word according to various linguistic criteria .
First , we enrich the dependency labels to distinguish edges that are internal to a holder / target / expression span from those that are external and perform experiments by adding an ' in label ' to non-head nodes within the graph , which we call + inlabel .
graphs , where we compute the dependency graph for each sentence 9 and set the head of each span to be the node that has an outgoing edge in the corresponding syntactic graph .
As there can be more than one such edge , we default to the first .
A manual inspection showed that this approach sometimes set unlikely dependency label types as heads , e.g. , punct , obl .
Therefore , we suggest a final approach , Dep. labels , which filters out these unlikely heads .
The full results are shown in Table 8 in the Appendix .
The implementation of the graph structure has a large effect on all metrics , although the specific results depend on the dataset .
We plot the average effect of each implementation across all datasets in Figure 3 , as well as each individual dataset ( Figures 4 - 8 in the Appendix ) .
+ inlabel tends to improve results on the non-English datasets , consistently increasing target and expression extraction and targeted sentiment .
It also generally improves the graph scores UF 1 and LF 1 on the non-English datasets .
Dep. edges has the strongest positive effect on the NSF 1 and SF 1 ( an avg .
2.52 and 2.22 percentage point ( pp ) over Head-final , respectively ) .
However , this average is pulled down by poorer performance on the English datasets .
Removing these two , the average benefit is 5.2 and 4.2 for NSF 1 and SF 1 , respectively .
On span extraction and targeted sentiment , however , Dep. edges leads to poorer scores overall .
Dep. labels does not lead to any consistent improvements .
These results indicate that incorporating syntactic dependency information is particularly helpful for the full structured sentiment task , but that these benefits do not always show at a more local level , i.e. , span extraction .
Do graph models perform better on sentences with multiple targets ?
We hypothesize that predicting the full sentiment graph may have a larger effect on sentences with multiple targets .
Therefore , we create a subset of the test data containing sentences with multiple targets and reevaluate Head-first , Head-final , and RACL - BERT on the target extraction task .
Table 4 shows the number of sentences with multiple targets and the Target span extraction score for each model .
On this subset , Head-first and Head-final outperform RACL - BERT on 9 of 10 experiments , confirming the hypothesis that the graph models improve on examples with multiple targets .
How much does mBERT contribute ?
We also perform experiments without mBERT ( shown in Table 7 in the Appendix ) and show the average gains ( over all 6 graph setups ) of including it in Table 5 .
Adding the mBERT features leads to average improvements in all experiments : for extracting spans an average gain of 4.1 pp for holders , 3.4 for targets , and 3.1 for expressions .
For targeted sentiment there is a larger gain of 4.2 pp , while for the parsing graph metrics UF 1 and lF 1 the gains are more limited ( 3.3 pp / 3.8 pp ) and similarly for NSF 1 and SF 1 ( 3.6 pp / 3.9 pp ) .
Table 6 : Polarity F 1 scores ( unweighted and weighted ) of models augmented with mBERT on the head-final setup .
We report average and standard deviation over 5 runs .
largest for the English datasets ( MPQA , DS Unis ) followed by NoReC Fine , and finally MultiB CA and MultiB EU .
This corroborates the bias towards English and similar languages that has been found in multilingual language models ( Artetxe et al. , 2020 ; Conneau et al. , 2020 ) and motivates the need for language -specific contextualized embeddings .
Analysis of polarity predictions
In this section we zoom in on polarity , in order to quantify how well models perform at predicting only polarity .
As the polarity annotations are bound to the expressions , we consider true positives to be any expression that overlaps the gold expression and has the same polarity .
Table 6 shows that the polarity predictions are best on and MultiB CA , followed by NoReC Fine and DS Unis , and finally MPQA .
This is likely due to the number of domains and characteristics of the data .
NoReC
Fine contains many domains and has longer expressions , while MPQA contains many highly ambiguous polar expressions , e.g. , ' said ' , ' asked ' , which have different polarity depending on the context .
Conclusion
In this paper , we have proposed a dependency graph parsing approach to structured sentiment analysis and shown that these models outperform state - of - the - art sequence labeling models on five benchmark datasets .
Using parse trees as input has shown promise for sentiment analysis in the past , either to guide a tree-based algorithm ( Socher et al. , 2013 ; Tai et al. , 2015 ) or to create features for sentiment models ( Nakagawa et al. , 2010 ; Almeida et al. , 2015 ) .
However , to the authors ' knowledge , this is the first attempt to directly predict dependencybased sentiment graphs .
In the future , we would like to better exploit the similarities between dependency parsing and sentiment graph parsing , either by augmenting the token - level representations with contextualized vectors from their heads in a dependency tree ( Kurtz et al. , 2020 ) or by multi-task learning to dependency parse .
We would also like to explore different graph parsing approaches , e.g. , PERIN ( Samuel and Straka , 2020 ) .
