title
Unsupervised Multi-hop Question Answering by Question Generation
abstract
Obtaining training data for multi-hop question answering ( QA ) is time - consuming and resource-intensive .
We explore the possibility to train a well - performed multi-hop QA model without referencing any human-labeled multihop question - answer pairs , i.e. , unsupervised multi-hop QA .
We propose MQA - QG , an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources .
MQA - QG generates questions by first selecting / generating relevant information from each data source and then integrating the multiple information to form a multi-hop question .
Using only generated training data , we can train a competent multi-hop QA which achieves 61 % and 83 % of the supervised learning performance for the HybridQA and the HotpotQA dataset , respectively .
We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data .
Our codes are publicly available at https : //github.com/teacherpeterpan / Unsupervised -Multi-hop-QA .
Introduction Extractive Question Answering ( EQA ) is the task of answering questions by selecting a span from the given context document .
Works on EQA can be divided into the single - hop ( Rajpurkar et al. , 2016 ( Rajpurkar et al. , , 2018 Kwiatkowski et al. , 2019 ) and multihop cases ( Yang et al. , 2018 ; Welbl et al. , 2018 ; Perez et al. , 2020 ) .
Unlike single- hop QA , which assumes the question can be answered with a single sentence or document , multi-hop QA requires combining disjoint pieces of evidence to answer a question .
Though different well - designed neural models ( Qiu et al. , 2019 ; Fang et al. , 2020 ) have achieved near-human performance on the multi-hop QA datasets ( Welbl et al. , 2018 ; Figure 1 : An overview of our approach for generating bridge-type multi-hop questions from table and text .
The full set of supported input types and question types are described in Section 3.2 .
. availability of large-scale human annotation .
Compared with single - hop QA datasets ( Rajpurkar et al. , 2016 ) , annotating multi-hop QA datasets is significantly more costly and time - consuming because a human worker needs to read multiple data sources in order to propose a reasonable question .
To address the above problem , we pursue a more realistic setting , i.e. , unsupervised multi-hop QA , in which we assume no human-labeled multi-hop question is available for training , and we explore the possibility of generating human-like multi-hop question - answer pairs to train the QA model .
We study multi-hop QA for both the homogeneous case where relevant evidence is in the textual forms ( Yang et al. , 2018 ) and the heterogeneous case where evidence is manifest in both tabular and textual forms ( Chen et al. , 2020 b ) .
Though successful attempts have been made to generate single - hop question - answer pairs by style transfer ( Lewis et al. , 2019 ) or linguistic rules ( Li et al. , 2020 ) , these methods are not directly applicable to the multi-hop setting as : 1 ) they cannot integrate information from multiple data sources , and 2 ) they only handle free-form text but not heterogeneous sources as input contexts .
We propose Multi-Hop Question Generator ( MQA - QG ) , a simple yet general framework that decomposes the generation of a multi-hop question into two steps : 1 ) selecting relevant information from each data source , 2 ) integrating the multiple information to form a question .
Specifically , the model first defines a set of basic operators to retrieve / generate relevant information from each input source or to aggregate different information .
Afterwards , we define six reasoning graphs .
Each corresponds to one type of multi-hop question and is formulated as a computation graph built upon the operators .
We generate multi-hop question - answer pairs by executing the reasoning graph .
Figure 1 shows an example of generating a table - to - text question : a) Given the inputs of ( table , text ) , the F indBridge operator locates a bridge entity that connects the contents between table and text . b)
We generate a simple , single - hop question for the bridge entity from the text ( QGwithEnt operator ) and generate a sentence describing the bridge entity from the table ( DescribeEnt operator ) .
c)
The BridgeBlend operator blends the two generated contents to obtain the multi-hop question .
We evaluate our method on two multi-hop QA datasets : HotpotQA ( Yang et al. , 2018 ) and Hy-bridQA ( Chen et al. , 2020 b ) . Questions in Hot-potQA reason over multiple texts ( homogeneous data ) , while questions in HybridQA reason over both table and text ( heterogeneous data ) .
The experiments show that MQA - QG can generate highquality multi-hop questions for both datasets .
Without using any human-labeled examples , the generated questions alone can be used to train a surprisingly well QA model , reaching 61 % and 83 % of the F1 score achieved by the fully - supervised setting on the HybridQA and HotpotQA dataset , respectively .
We also find that our method can be used in a few-shot learning setting .
For example , after pretraining the QA model with our generated data , we can obtain 64.6 F1 with only 50 labeled examples in HotpotQA , compared with 21.6 F1 without the warm - up training .
In summary , our contributions are : ?
To the best of our knowledge , this is the first work to investigate unsupervised multi-hop QA . ?
We propose MQA - QG , a novel framework to generate high-quality training data without the need to see any human-annotated multi-hop question .
?
We show that the generated training data can greatly benefit the multi-hop QA system in both unsupervised and few-shot learning settings .
Related Work Unsupervised Question Answering .
To reduce the reliance on expensive data annotation , Unsupervised / Zero - Shot QA has been proposed to train question answering models without any humanlabeled training data .
Lewis et al. ( 2019 ) proposed the first unsupervised QA model which generates synthetic ( context , question , answer ) triples to train the QA model using unsupervised machine translation .
However , the generated questions are unlike human-written questions and tend to have a lot of lexical overlaps with the context .
To address this , followup works utilized the Wikipedia cited documents ( Li et al. , 2020 ) , predefined templates ( Fabbri et al. , 2020 ) , or pretrained language model ( Puri et al. , 2020 ) to produce more natural questions resembling the human-annotated ones .
However , all the existing studies are focused on the SQuAD ( Rajpurkar et al. , 2016 ) dataset to answer single - hop and text-only questions .
These methods do not generalize to multi-hop QA because they lack integrating and reasoning over disjoint pieces of evidence .
Furthermore , they are restricted to text - based QA without considering structured or semi-structured data sources such as KB and Table .
In contrast , we propose the first framework for unsupervised multi-hop QA , which can reason over disjoint structured or unstructured data to answer complex questions .
Multi-hop Question Generation .
Question Generation ( QG ) aims to automatically generate questions from textual inputs ( Pan et al. , 2019 ) .
Early work of Question Generation ( QG ) relied on syntax rules or templates to transform a piece of given text to questions ( Heilman , 2011 ; Chali and Hasan , 2012 ) .
With the proliferation of deep learning , QG evolved to use supervised neural models , where most systems were trained to generate questions from ( passage , answer ) pairs in the SQuAD dataset ( Du et al. , 2017 ; Zhao et al. , 2018 ; Kim et al. , 2019 ) .
With the advent of pretraining language models ( Dong et al. , 2019 ) , the challenge of generating single - hop questions similar to SQuAD have largely been addressed .
QG research has started to generate more complex questions that require deep comprehension and multi-hop reasoning ( Tuan et al. , 2020 ; Yu et al. , 2020 ) .
For example , Tuan et al . ( 2020 ) proposed a multi-state attention mechanism to mimic the multi-hop reasoning process .
parsed the input passage as a semantic graph to facilitate the reasoning over different entities .
However , these supervised methods require large amounts of human-written multi-hop questions as training data .
Instead , we propose the first unsupervised QG system to generate multi-hop questions without the need to access those annotated data .
Methodology
The setup of Multi-hop QA is as follows .
Given a question q and a set of input contexts , C ) predicts the answer a for the question q by integrating and reasoning over information from C. C = { C 1 , ? ? ? , C n } , In this paper , we consider two -hop questions and denote the required contexts as C i and C j .
Formally , each time our model takes as inputs C i , C j to generate a set of ( q , a ) pairs .
We focus on two modalities : the heterogeneous case where C i , C j are table and text and the homogeneous case where C i , C j are both texts .
However , the design of our framework is flexible enough to generalize to multihop QA for other modalities .
Our model MQA - QG consists of three compo-nents : operators , reasoning graphs , and question filtration .
Operators are atomic operations implemented by rules or off- the-shelf pretrained models to retrieve , generate , or fuse relevant information from input contexts ( C i , C j ) .
Different reasoning graphs define different types of reasoning chains for multi-hop QA with the operators as building blocks .
Training ( q , a ) pairs are generated by executing the reasoning graphs .
Question filtration removes irrelevant and unnatural ( q , a ) pairs to give the final training set D for multi-hop QA .
Operators In Table 1 , we define eight basic operators and divide them into three types : 1 ) selection : retrieve relevant information from a single context , 2 ) generation : generate information from a single context , and 3 ) fusion : fuse multiple retrieved / generated information to construct multi-hop questions .
?
FindBridge :
Most multi-hop questions rely on the entities that connect different input contexts , i.e. , bridge entities , to integrate multiple pieces of information ( Xiong et al. , 2019 ) .
FindBridge takes two contexts ( C i , C j ) as inputs , and extracts the entities that appear in both C i and C j as bridge entities .
For example , in Figure 1 , we extract " Jenson Button " as the bridge entity .
?
FindComEnt : from the input text .
We extract entities with NER types N ationality , Location , DateT ime , and N umber from the input text as comparative properties ( cf , " Comparison " in Figure 4 ) . ? QGwithAns , QGwithEnt :
These two operators generate simple , single - hop questions from a single context , which are subsequently used to compose multi-hop questions .
We use the pretrained Google T5 model ( Raffel et al. , 2019 ) fine -tuned on SQuAD to implement these two operators .
Given the SQuAD training set of context-question - answer triples D = { ( c , q , a ) } , we jointly fine - tune the model on two tasks .
1 ) QGwithAns aims to generate a question q with a as the answer , given ( c , a ) as inputs .
2 ) QGwithEnt aims to generate a question q that contains a specific entity e , given ( c , e ) as inputs .
The evaluation of this T5 - based model can be found in Appendix A.1 . ?
DescribeEnt : Given a table T and a target entity e in the table , the DescribeEnt operator generates a sentence that describes the entity e based on the information in the table T .
We implement this using the GPT - TabGen model ( Chen et al. , 2020a ) shown in Figure 2 .
The model first uses template to flatten the table T into a document P T and then feed P T to the pre-trained GPT - 2 model ( Radford et al. , 2019 ) to generate the output sentence Y .
To avoid irrelevant information in P T , we apply a template that only describes the row where the target entity locates .
We then finetune the model on the ToTTo dataset ( Parikh et al. , 2020 ) , a large-scale dataset of controlled table - to - text generation , by maximizing the likelihood of p( Y | P T ; ? ) , with ? denoting the model parameters .
The implementation details and the model evaluation are in Appendix A.1 . ? QuesToSent :
This operator convert a question q into its declarative form s by applying the linguistic rules defined in Demszky et al . ( 2018 ) . ?
BridgeBlend :
The operator composes a bridgetype multi-hop question based on : 1 ) a bridge entity e , 2 ) a single - hop question q that contains e , and 3 ) a sentence s that describes e.
As exemplified in Figure 3 , we implement this by applying a simple yet effective rule that replaces the bridge entity e in q with " the [ MASK ] that s " and employ the pretrained BERT - Large to fill in the [ MASK ] word .
?
CompBlend :
This operator composes a comparison-type multi-hop question based on two single - hop questions q 1 and q 2 .
The two questions ask about the same comparative property p for two different entities e 1 and e 2 .
We form the multi-hop question by filling p , e 1 , and e 2 into pre-defined templates ( Further details in Appendix A.2 ) .
Reasoning Graphs
Based on the basic operators , we define six types of reasoning graphs to generate questions with different types .
Each reasoning graph is represented as a directed acyclic graph ( DAG ) G , where each node in G corresponds to an operator .
A node s i is connected by an incoming edge s j , s i if the output of s j is given as an input to s i .
As shown in Figure 4 , Table - Only and Text - Only represent single - hop questions from table and text , respectively .
The remaining reasoning graphs define four types of multi-hop questions .
1 )
Text - Only
Table-Only Text-to- Table
Question Filtration Finally , we employ two methods to refine the quality of generated QA pairs .
1 ) Filtration .
We use a pretrained GPT - 2 model to filter out those questions that are disfluent or unnatural .
The top N samples with the lowest perplexity scores are selected as the generated dataset to train the multi-hop QA model .
2 ) Paraphrasing .
We train a question paraphrasing model based on the BART model to paraphrase each generated question .
Our experiments show that filtration brings noticeable improvements to the QA model .
However , we show in Section 4.5 that paraphrasing produces more human-like questions but introduces the semantic drift problem that harms the QA performance .
Experiments
We evaluate our framework on two multi-hop QA datasets : HotpotQA ( Yang et al. , 2018 ) and Hy-bridQA ( Chen et al. , 2020b are designed to aggregate both tabular information and text information , i.e. , lack of either form renders the question unanswerable .
Table 2 shows the statistics of these two datasets and Appendix B.1 gives their data examples .
There are two types of multi-hop questions in HotpotQA : bridge-type ( 81 % ) and comparison-type ( 19 % ) .
For HybridQA , questions are divided by whether their answers come from the table ( In - Table question , 56 % ) or from the passage ( In - Passage question , 44 % ) .
Around 80 % HybridQA questions requires bridge-type reasoning .
Unsupervised QA Results Question Generation .
In HybridQA , we extract its table - text corpus consisting of ( T , D ) input pairs , where T denotes the table and set of its linked passages D . We generate two multi-hop QA datasets Q tbl ? txt and Q txt ? tbl with MQA - QG by executing the " Table - to- Text " and " Text - to - Table " reasoning graphs for each ( T , D ) , resulting in a total of 170K QA pairs .
We then apply question filtration to obtain the training set Q hybrid with 100K QA pairs .
Similarly , for HotpotQA , we first generate Q bge and Q com , which contains only the bridgetype questions and only the comparison -type questions , respectively .
Afterward , we merge them and filter the questions to obtain the final training set Q hotpot with 100K QA pairs .
In Appendix B.2 , we gives the statistics of all the generated datasets .
Question Answering For HybridQA , we use the HYBRIDER ( Chen et al. , 2020 b ) as the QA model , which breaks the QA into linking and reasoning to cope with heterogeneous information , achieving the best result in HybridQA .
For HotpotQA , we use the SpanBERT ( Joshi et al. , 2020 ) since it achieved promising results on HotpotQA with reproducible codes .
We use the standard Exact Match ( EM ) and F 1 metrics to measure the QA performance .
Baselines .
We compare MQA - QG with both supervised and unsupervised baselines .
For Hy-bridQA , we first include the two supervised baselines Table-Only and Passage - Only in Chen et al . ( 2020 b ) , which only rely on the tabular information or the textual information to find the answer .
As we are the first to target unsupervised QA on Hy-bridQA , there is no existing unsupervised baseline for direct comparison .
Therefore , we construct a strong baseline QDMR - to - Question that generate questions from Question Decomposition Meaning Representation ( QDMR ) ( Wolfson et al. , 2020 ) , a logical representation specially designed for multihop questions .
We first generate QDMR expressions from the input ( table , text ) using pre-defined templates and then train a Seq2Seq model ( Bahdanau et al. , 2014 ) 3 and Table 4 summarizes the QA performance on HybridQA and HotpotQA , respectively .
For HybridQA , we use the reported performance of HYBRIDER as the supervised benchmark ( S3 ) and apply the same model setting of HYBRIDER to train the unsupervised version , i.e. , using our generated QA pairs as the training data ( U2 and U3 ) .
For HotpotQA , the original paper of SpanBERT only reported the results for the MRQA - 2019 shared task ( Fisch et al. , 2019 ) , which only includes the bridge-type questions in HotpotQA .
Therefore , we retrain the SpanBERT on the full HotpotQA dataset to get the supervised benchmark ( S4 ) and using the same model setting to train the unsupervised versions ( U7 and U8 ) .
Our unsupervised model MQA - QG attains 30.5 F 1 on the HybridQA test set and 68.6 F 1 on the HotpotQA dev set , outperforming all the unsupervised baselines ( U1 , U4 , U5 , U6 ) by large margins .
Without using their human-annotated training data , the F 1 gap to the fully - supervised version is only 19.5 and 14.2 for HybridQA and HotpotQA , respectively .
In particular , the results of U2 and U3 even outperform the two weak supervised baselines ( S1 and S2 ) in HybridQA .
This demonstrates the effectiveness of MQA - QG in generating good multi-hop questions for training the QA model .
Model In -Table In
Ablation Study
To understand the impact of different components in MQA - QG , we perform an ablation study on the HybridQA development set .
In Table 5 , we compare our full model ( A7 ) with six ablation settings by removing certain the model components ( A1 - A4 ) or by restricting the reasoning types ( A5 and A6 ) .
We make three key observations .
Single -hop questions vs. multi-hop questions .
A1 to A3 generates single - hop questions using the reasoning graph of Text-Only ( A1 ) , Table-Only ( A2 ) , or a union of them ( A3 ) .
Afterwards , we use them to train the HYBRIDER model and test the multi-hop QA performance .
In these cases , the model is trained to answer questions based on either table or text but lacking the ability to rea-son between table and text .
As shown in Table 5 , A1 - A3 achieves a low performance of EM and F1 , especially for In - Passage questions , showing that single - hop questions alone are insufficient to train a good multi-hop QA system .
This reveals that learning to reason between different contexts is essential for multi-hop QA and justifies the necessity of generating multi-hop questions .
However , for HotpotQA , we observe that the benefit of multihop questions is not as evident as in HybridQA : the SQuAD - Transfer ( U6 ) achieves a relatively good F1 of 62.8 .
A potential reason is that the examples of HotpotQA contain reasoning shortcuts through which models can directly locate the answer by word-matching , without the need of multi-hop reasoning , as observed by Jiang and Bansal ( 2019 ) . ) .
We believe the reason is that the information in the text can also answer some In - Table questions .
Using both reasoning types ( A6 ) , the model improves on average by 8.6 F1 compared with the models using a single reasoning type ( A4 , A5 ) .
This shows that it is beneficial to train the multi-hop QA model with diverse reasoning chains .
5873 Effect of question filtration .
Question filtration also helps to train a better QA model , leading to a + 2.3 F1 for HybridQA and + 1.1 F1 for Hot-potQA .
We find that the GPT - 2 based model can filter out most ungrammatical questions but would keep valid yet unnatural questions such as " Where was the event that is held in 2016 held ? " .
Few-shot Multi-hop QA
We then explore MQA - QG 's effectiveness in the few-shot learning setting where only a few humanlabeled ( q , a ) pairs are available .
We first train the unsupervised QA model based on the training data generated by our best model .
Then we finetune the model with limited human-labeled data .
The blue line in Figure 5 ( a ) and Figure 5 ( b ) shows the F1 scores with different numbers of labeled training data for HybridQA and HotpotQA , respectively .
We compare this with training the QA model directly on the human-labeled data without unsupervised QA pretraining ( grey lines in Figure 5 ) .
With progressively larger training dataset sizes , our model performs consistently better than the model without unsupervised pretraining for both two datasets .
The performance improvement is especially prominent in very data-poor regimes ; for example , our approach achieves 69.3 F1 with only 100 labeled examples in HotpotQA , compared with 21.4 F1 without unsupervised pretraining ( 47.9 absolute gain ) .
The results show pretraining QA with MQA - QG greatly reduce the demand for humanannotated data .
It can be used to provide a " warm start " for online learning QA system in which training data are quite limited for a new domain .
Analysis of Generated Questions
Although the generated questions are used to optimize for downstream QA performance , it is still instructive to examine the output QA pairs to better understand our system 's advantages and limitations .
In Figure 6 , we plot the question type distribution for both the human-labeled dataset and the generated data for HybridQA .
We find that the two datasets have a similar question type distribution , where " What " questions constitute the major type .
However , our model generates more " When " and " Where " questions but fewer " Which " questions .
This is because the two reasoning graphs we apply for HybridQA are bridge-type questions while " Which " questions mostly compare .
Table 6 shows representative examples generated by our model .
Most questions are fluent and exhibit encouraging language variety , such as Examples 2 , 3 , 5 .
Our model also shows almost no sign of semantic drift , meaning most of the questions are valid despite sometimes being unnatural .
The two major deficiencies are inaccurate references ( in red ) and redundancies ( in blue ) , shown in Examples 1 , 4 , 6 .
This can be addressed by incorporating minimal supervision to guide the fusion process ; i.e. , more flexible paraphrasing in fusion .
Effects of Question Paraphrasing
As discussed in Section 3.3 , to generated more natural - looking questions , we attempted to train a BART - based question paraphrasing model to paraphrase each generated question .
We finetune the pretrained BART model on the Quora Question Paraphrasing dataset 1 , which contains over 100,000 question pairs with equivalent semantic meaning .
The evaluation results are shown in Table 7 .
Surprisingly , we observe a performance drop for both the HybridQA and the HotpotQA dataset , with a 4.3 and 4.8 decrease in F1 , respectively .
We observe that paraphrasing indeed produces more fluent questions by rewriting the redundancy parts of the original questions into more concise expression .
However , paraphrasing introduces the " semantic drift " problem , i.e. , the paraphrased question changes the semantic meaning of the original question .
We believe this severally hurts the QA performance because it produces noisy samples with inconsistent question and answer .
Therefore , we argue that in unsupervised multi-hop QA , semantic faithfulness is more important than fluency for the generated questions .
This explains why we design hand -crafted reasoning graphs to ensure the semantic faithfulness .
However , how to generate fluent human-like questions while keeping semantic faithfulness is an important future direction .
Conclusion and Future Works
In this work , we study unsupervised multi-hop QA and propose a novel framework MQA - QG to generate multi-hop questions via composing reasoning graphs built upon basic operators .
The experiments show that our model can generate human-like questions that help to train a well - performing multi-hop QA model in both the unsupervised and the fewshot learning setting .
Further work is required to include more flexible paraphrasing at the fusion stage .
We can also design more reasoning graphs and operators to generate more complex questions and support more input modalities .
