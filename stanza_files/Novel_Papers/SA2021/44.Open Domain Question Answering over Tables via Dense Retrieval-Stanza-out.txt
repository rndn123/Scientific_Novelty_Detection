title
Open Domain Question Answering over Tables via Dense Retrieval
abstract
Recent advances in open-domain QA have led to strong models based on dense retrieval , but only focused on retrieving textual passages .
In this work , we tackle open-domain QA over tables for the first time , and show that retrieval can be improved by a retriever designed to handle tabular context .
We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives .
As relevant datasets are missing , we extract a subset of NATURAL QUESTIONS ( Kwiatkowski et al. , 2019 ) into a Table QA dataset .
We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to - end QA results from 33.8 to 37.7 exact match , over a BERT based retriever .
*
Work completed while interning at Google .
S read ( " Chlorine " ) S read ( ... ) S read ( " Latin " ) which element is named for the greek word for green ?
TAPAS q ( q ) List of chemical element name etymologies Element Origin Meaning ... Fluorine Latin a flowing ...
Introduction Models for question answering ( QA ) over tables usually assume that the relevant table is given during test time .
This applies for semantic parsing ( e.g. , for models trained on SPIDER ( Yu et al. , 2018 ) ) and for end-to- end QA ( Neelakantan et al. , 2016 ; Herzig et al. , 2020 ) .
While this assumption simplifies the QA model , it is not realistic for many use-cases where the question is asked through some open-domain natural language interface , such as web search or a virtual assistant .
In these open-domain settings , the user has some information need , and the corresponding answer resides in some table in a large corpus of tables .
The QA model then needs to utilize the corpus as an information source , efficiently search for the relevant table within , parse it , and extract the answer .
Recently , much work has explored open-domain QA over a corpus of textual passages ( Chen et al. , 2017 ; Sun et al. , 2018 ; Yang et al. , 2019 ; Lee et al. , 2019 , inter alia ) .
These approaches usually follow a two -stage framework : ( 1 ) a retriever first selects a small subset of candidate passages relevant to the
TAPAS r ( q , h 1 , T 1 ) S ret ( q , T 2 ) Top K
Which element is named for the greek word for green ?
S ret ( q , ... )
Figure 1 : An overview of our approach .
A dense table retriever scores the question against all tables and outputs the top K tables ( K = 1 in this example ) , and a reader selects the answer out of the top K tables .
question , and then ( 2 ) a machine reader examines the retrieved passages and selects the correct answer .
While these approaches work well on free text , it is not clear whether they can be directly applied to tables , as tables are semi-structured , and thus different than free text .
In this paper we describe the first study to tackle open-domain QA over tables , and focus on modifying the retriever .
We follow the two-step approach of a retriever model that retrieves a small set of candidate tables from a corpus , followed by a QA model ( Figure 1 ) .
Specifically , we utilize dense retrieval approaches targeted for retrieving passages Guu et al. , 2020 ; , and modify the retriever to better handle tabular contexts .
We present a simple and effective pre-training procedure for our retriever , and further improve its performance by mining hard negatives using the retriever model .
Finally , as relevant open domain datasets are missing , we process NATU - RAL QUESTIONS ( Kwiatkowski et al. , 2019 )
Setup
We formally define open domain extractive QA over tables as follows .
We are given a training set of N examples D train = {( q i , T i , a i ) }
N i=1 , where q i is a question , T i is a table where the answer a i resides , and a corpus of M tables C = { T i } M i=1 .
The answer a i is comprised of one or more spans of tokens in T i .
Our goal is to learn a model that given a new question q and the corpus C returns the correct answer a .
Our task shares similarities with open domain QA over documents ( Chen et al. , 2017 ; Yang et al. , 2019 ; , where the corpus C consists of textual passages extracted from documents instead of tables , and the answer is a span that appears in some passage in the corpus .
As in these works , dealing with a large corpus ( of tables in our setting ) , requires relevant context retrieval .
Naively applying a QA model , for example TAPAS ( Herzig et al. , 2020 ) , over each table in the large corpus is not practical because inference is too expensive .
To this end we break our system into two independent steps .
First , an efficient table retriever component selects a small set of candidate tables C R from a large corpus of tables C. Second , we apply a QA model to extract the answer a given the question q and the candidate tables C R .
Dense Table Retrieval
In this section we describe our dense table retriever ( DTR ) , which retrieves a small set of K candidate tables C R given a question q and a corpus C .
In this work we set K = 10 and take C to be the set of all tables in the dataset we experiment with ( see ?6 ) .
As in recent work for open domain QA on passages Guu et al. , 2020 ; Chen et al. , 2021 ; , we also follow a dense retrieval architecture .
As tables that contain the answer to q do not necessarily include tokens from q , a dense encoding can better capture similarities between table contents and a question .
For training DTR , we leverage both in - domain training data D train , and automatically constructed pre-training data D pt of text - table pairs ( see below ) .
Retrieval Model
In this work we focus on learning a retriever that can represent tables in a mean-ingful way , by capturing their specific structure .
Traditional information retrieval methods such as BM25 are targeted to capture token overlaps between a query and a textual document , and other dense encoders are pre-trained language models ( such as BERT ) targeted for text representations .
Recently , Herzig et al. ( 2020 ) proposed TAPAS , an encoder based on BERT , designed to contextually represent text and a table jointly .
TAPAS includes table specific embeddings that capture its structure , such as row and column ids .
In DTR , we use TAPAS to represent both the query q and the table T .
For efficient retrieval during inference we use two different TAPAS instances ( for q and for T ) , and learn a similarity metric between them as ; .
More concretely , the TAPAS encoder TAPAS ( x 1 , [ x 2 ] ) takes one or two inputs as arguments , where x 1 is a string and x 2 is a flattened table .
We then define the retrieval score as the inner product of dense vector representations of the question q and the table T : h q = W q TAPAS q ( q ) [ CLS ] h T = W T TAPAS T ( title ( T ) , T ) [ CLS ]
S ret ( q , T ) = h T q h T , where TAPAS ( ? ) [ CLS ] returns the hidden state for the CLS token , W q and W T are matrices that project the TAPAS output into d = 256 dimensional vectors , and title ( T ) is the page title for table T .
We found the table 's page title to assist in retrieving relevant tables , which is also useful for Wikipedia passage retrieval .
Training
The goal of the retriever is to create a vector space such that relevant pairs of questions and tables will have smaller distance ( which results in a large dot product ) than the irrelevant pairs , by learning an embedding .
To increase the likelihood of gold ( q , T ) pairs , we train the retriever with in - batch negatives ( Gillick et al. , 2019 ; Henderson et al. , 2017 ; . Let {( q i , T i ) }
B i=1 be a batch of B examples from D train , where for each q i , T i is the gold table to retrieve , and for each j = i we treat T j as a negative .
We now define the likelihood of the gold table T i as : p( T i |q i ) = exp [ S ret ( q i , T i ) ]
B j=1 exp [ S ret ( q i , T j ) ] .
To train the model efficiently , we define Q and T to be a B?d matrix that hold the representations for questions and tables respectively .
Then , S = QT T gives an B ?
B matrix where the logits from the gold table are on the diagonal .
We then train using a row-wise cross entropy loss where the labels are a B ?
B identity matrix .
Pre-training
One could train our retriever from scratch , solely relying on a sufficiently large indomain training dataset D train .
However , we find performance to improve after using a simple pretraining method for our retriever .
suggest to pre-train a textual dense retriever using an Inverse Cloze Task ( ICT ) .
In ICT , the goal is to predict a context given a sentence s.
The context is a passage that originally contains s , but with s masked .
The motivation is that the relevant context should be semantically similar to s , and should contain information missing from s.
Similarly , we posit that a table T that appears in close proximity to some text span s is more relevant to s than a random table .
To construct a set 2020 ) uses extracted ( s , T ) pairs for pretraining TAPAS with a masked language modeling objective , we pre-train DTR from these pairs , with the same objective used for in- domain data .
D pt = {( s i , T i ) }
M i=1 that consists of M pre- training pairs ( s , T ) , Hard Negatives Following similar work ( Gillick et al. , 2019 ; Xiong et al. , 2021 ) , we use an initial retrieval model to extract the most similar tables from C for each question in the training set .
From this list we discard each table that does contain the reference answer to remove false negatives .
We use the highest scoring remaining table as a particular hard negative .
Given the new triplets of question , reference table and mined negative table , we train a new model using a modified version of the in- batch negative training discussed above .
Given Q and S as defined above and a new matrix N ( B ? d ) that holds the representations of the negative tables , S = QN T gives another B ?
B matrix that we want to be small in value ( possibly negative ) .
If we concatenate S and S row-wise we get a new matrix for which we can perform the same cross entropy train-ing as before .
The label matrix is now obtained by concatenating an identity matrix row-wise with a zero matrix .
Inference During inference time , we apply the table encoder TAPAS T to all the tables T ? C offline .
Given a test question q , we derive its representation h q and retrieve the top K tables with representations closest to h q .
In our experiments , we use exhaustive search to find the top K tables , but to scale to large corpora , fast maximum inner product search using existing tools such as FAISS ( Johnson et al. , 2019 ) and SCANN ( Guo et al. , 2020 ) could be used , instead .
Question Answering over Tables
A reader model is used to extract the answer a given the question q and K candidate tables .
The model scores each candidate and at the same time extracts a suitable answer span from the table .
Each table and question are jointly encoded using a TAPAS model .
The score is a simple logistic loss based on the CLS token , as in .
The answer span extraction is modeled as a softmax over all possible spans up to a certain length .
Spans that are located outside of a table cell or that cross a cell are masked .
Following Lee et al. ( 2017 , the span representation is the concatenation of the contextual representation of the first and last token in the span s : h start = TAPAS r ( q , title ( T ) , T ) [ START ( s ) ] h end = TAPAS r ( q , title ( T ) , T ) [ END ( s ) ]
S read ( q , T ) = MLP ( [ h start , h end ] ) .
The training and test data are created by running a retrieval model .
We extract the K = 10 highest scoring candidate tables for each question .
At training time we add the reference table if it is missing from the candidates .
At inference time all table candidates are processed and the answer of the candidate with the highest score is returned as the predicted answer .
Dataset
We create a new English dataset called NQ - TABLES from NATURAL QUESTIONS ( Kwiatkowski et al. , 2019 ) from real Google search queries and the answers are spans in Wikipedia articles identified by annotators .
Although the answers for most questions appear in textual passages , we identified 12 K examples where the answer resides in a table , and can be used as a QA over tables example .
To this end , we form NQ - TABLES that consists of ( q , T , a ) triplets from these examples .
Tables are extracted from the article 's HTML , and are normalized by transposing infobox tables .
We randomly split the original NQ train set into train and dev ( based on a hash of the page title ) and use all questions from the original NQ dev set as our test set .
To construct the corpus C , we extract all tables that appear in articles in all NQ sets .
NQ can contain the same Wikipedia page in different versions which leads to many almost identical tables .
We merge close duplicates using the following procedure .
For all tables that occur on the same Wikipedia page we flatten the entire table content , tokenize it and compute l 2 normalized unigram vectors of the token counts of each table .
We then compute the pair-wise cosine similarity of all tables .
We iterate over the table pairs in decreasing order of similarity and attempt to merge them into clusters .
This is essentially a version of single link clustering .
In particular , we will merge two tables if the similarity is > 0.91 , they do not occur on the same version of the page , their difference is rows is at most 2 and they have the same number of columns .
Experiments Details about the experimental setup are given Appendix A. Retrieval Baselines
We consider the following baselines as alternatives to DTR .
We use the BM25 ( Robertson and Zaragoza , 2009 ) implementation of Gensim ( ?eh?ek and Sojka , 2010 ) 1 .
To measure if a table-specific encoder is necessary , we implement DTR - TEXT , where the retriever is initialized from BERT instead of TAPAS .
To test whether the content of the table is relevant , we experiment with DTR - SCHEMA , where only the headers and title are used to represent tables .
Retrieval Results
Table 1 shows the test results for table retrieval ( dev results are in Appendix B ) .
We report recall at K ( R@K ) metrics as the fraction of questions for which the highest scoring K tables contain the reference table .
We find that all dense models that have been pretrained out - peform the BM25 baseline by a large margin .
The model that uses the TAPAS table embeddings ( DTR ) out -performs the dense baselines by more than 1 point in R@10 .
The addition of mined negatives ( DTR +hn ) yields an additional improvement of more than 5 points .
Mining negatives from DTR works better than mining negatives from BM25 ( DTR + hnbm25 , + 0.6 R@10 ) .
End-to- End QA Results for end-to- end QA experiments are shown in Table 2 ( dev results are in Appendix B ) .
We use the exact match ( EM ) and token F1 metrics as implemented in SQUAD ( Rajpurkar et al. , 2016 ) . 2
We additionally report oracle metrics which are computed on the best answer returned for any of the candidates .
We again find that all dense models out -perform the BM25 baseline .
A TAPAS - based reader outperforms a BERT reader by more than 3 points in EM .
The simple DTR model out -performs the baselines by more than 1 point in EM .
Hard negatives from BM25 ( + hnbm25 ) improve DTR 's performance by 1 point , while hard negatives from DTR ( +hn ) improve performance by 2 points .
We additionally perform a McNemar 's significance test for our proposed model , DTR + hn , and find that it performs significantly better ( p< 0.05 ) than all baselines .
Analysis
Analyzing the best model in Table 2 ( DTR +hn ) on the dev set , we find that 29 % of the questions are answered correctly , 14 % require a list answer ( which is out of scope for this paper ) , 12 % do not have any table candidate that contains the answer , for 11 % the model does not select a table that contains the answer , and for 34 % the reader fails to extract the correct span .
We further analyzed the last category by manually annotating 100 random examples .
We find that for 23 examples the answer is partially correct ( usually caused by inconsistent span annotations in NQ ) .
For 11 examples the answer is ambiguous ( e.g. , the release date of a movie released in different regions ) .
For 22 examples the table is missing context or does only contain the answer accidentally .
Finally , 44 examples are wrong , usually because they require some kind of table reasoning , like computing the maximum over a column , or using common sense knowledge .
Conclusion
In this paper we demonstrated that a retriever designed to handle tabular context can outperform other textual retrievers for open-domain QA on tables .
We additionally showed that our retriever can be effectively pre-trained and improved by hard negatives .
In future work we aim to tackle multimodal open-domain QA , combining passages and tables as context .
