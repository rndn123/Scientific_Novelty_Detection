title
SUMPUBMED : Summarization Dataset of PubMed Scientific Articles
abstract
Most earlier work on text summarization is carried out on news article datasets .
The summary in these datasets is naturally located at the beginning of the text .
Hence , a model can spuriously utilize this correlation for summary generation instead of truly learning to summarize .
To address this issue , we constructed a new dataset , SUMPUBMED , using scientific articles from the PubMed archive .
We conducted a human analysis of summary coverage , redundancy , readability , coherence , and informativeness on SUMPUBMED .
SUMPUBMED is challenging because ( a ) the summary is distributed throughout the text ( not- localized on top ) , and ( b ) it contains rare domain-specific scientific terms .
We observe that seq2seq models that adequately summarize news articles struggle to summarize SUMPUBMED .
Thus , SUMPUBMED opens new avenues for the future improvement of models as well as the development of new evaluation metrics .
Introduction Most of the existing summarization datasets , i.e. , CNN Daily Mail and DUC are news article datasets .
That is , the article acts as a document , and the summary is a short ( 10 - 15 lines ) manually written highlight ( i.e. , headlines ) .
In many cases , these highlights have significant lexical overlap with the few lines at the top of the article .
Thus , any model which can extract the top few lines , e.g. , extractive methods , performs adequately on these datasets .
However , the task of summarization is not merely limited to short- length news articles .
One could also summarize long and complex documents such as essays , research papers , and books .
In such cases , an extractive approach will most likely fail .
For successful summarization on these documents , one needs to ( a ) find information from the distributed ( non- localized ) locale in the large text , ( b ) perform paraphrasing , simplifying , and shortening of longer sentences and ( c ) combine information from multiple sentences to generate the summary .
Hence , an abstractive approach will perform better on such large documents .
One obvious source that contains such complex documents is the MEDLINE biomedical scientific articles , which are publicly available .
Furthermore , these articles are accompanied by abstracts and conclusions which summarize the documents .
Therefore , we constructed a scientific summarization dataset from pre-processed PubMed articles , named SUMPUBMED .
In comparison to the previous news-article based datasets , SUMPUBMED documents are longer , and the corresponding summaries cannot be extracted by selecting a few sentences from fixed locations in the document .
The dataset , along with associated scripts , are available at https://github.com/vgupta123/ sumpubmed .
Our contributions in this paper are : ?
We created a new scientific summarization dataset , SUMPUBMED , which has longer text documents and summaries with non-localized information from documents . ?
We analyzed the quality of summaries in SUMPUBMED on the basis of four parameters : readability , coherence , non-repetition , and informativeness using human evaluation .
?
We evaluated several extractive , abstractive ( seq2seq ) , and hybrid summarization models on SUMPUBMED .
The results show that SUMPUBMED is more challenging compared to the earlier news - based datasets .
?
Lastly , we showed that the standard summarization evaluation metric , ROUGE ( Lin , 2004 ) , correlates poorly with human evaluations on SUMPUBMED .
This indicates the need for a new evaluation metric for the scientific summarization task .
In Section 1 , we provided a brief introduction .
The remaining parts of the paper are organized as follows : in Section 2 we explain how SUMPUBMED was created .
In Section 3 , we explain how summaries were annotated by human experts .
We then move on to experiments in Section 4 .
We next discuss the results and analysis in Section 5 , followed by the related work in Section 6 .
Lastly , we move on to the conclusions in final Section 7 .
SUMPUBMED Creation SUMPUBMED is created from PubMed biomedical research papers , which has 26 million documents .
The documents are sourced from diverse literature , including MEDLINE , life science journals , and online books .
For SUMPUBMED creation we took 33 , 772 documents from Bio Med Central ( BMC ) .
BMC incorporates research papers related to medicine , pharmacy , nursing , dentistry , health care , health services , etc .
The research documents in BMC contain two subsections : Front and Body .
The front part of the document is basically the abstract and taken as the gold summary .
The body part which is taken as the main document contains three subsections : background , results , and conclusion .
Preprocessing
The average word count in the PubMed scientific articles is around 4 , 000 words for each document and 250 to 300 lines in every document .
Therefore , to create SUMPUBMED , we performed extensive preprocessing so that nontextual content is removed and the overall text is reduced to a more manageable size .
This extenstive pre-processing step is one of the main factors that sets SUMPUBMED apart from similar datasets ( Cohan et al. , 2018 ) .
During preprocessing , the non-textual content from the text was removed by : ( a ) replacing citations and digits in the content with < cit > and < dig > labels , ( b ) removing figures , tables , signatures , subscripts , superscripts , and their associated text ( e.g. , captions ) , and ( c ) removing the acknowledgments and references from the text .
All the preprocessing was done on a sentence level utilizing the Python regex library .
1 After preprocessing , we convert the final document to an XML format and use the SAX parser to parse it .
SAX vs DOM parser :
In SAX , events are triggered when the XML is being parsed .
When the parser is parsing the XML and encounters a tag starting ( e.g. , < something > ) , then it triggers the tagStarted event ( actual name of the event might differ ) .
Similarly , when the end of the tag is met while parsing ( < / something > ) , it triggers tagEnded .
Using a SAX parser implies one needs to handle these events and make sense of the data returned with each event .
One could also use the DOM parser , 2 where no events are triggered while parsing .
In DOM the entire XML is parsed , and a DOM tree ( of the nodes in the XML ) is generated and returned .
In general , DOM is easier to use but has a huge overhead of parsing the entire XML before one can start using it ; therefore , we use SAX instead .
An example of the front part , body part , and the XML file formed from the pre-processed text is shown in https://github.com/vgupta123/ sumpubmed/blob/master/template.pdf . ?
The Raw Text version is obtained after preprocessing when removing non-textual context is completed , followed by XML parsing .
Versions of SUMPUBMED ?
In the Noun phrases version , we processed the raw text version further to ensure that the summary and the text have the same named entities .
We found that standard Name Entity Recognition ( NER ) ( Finkel et al. , 2005 ) and Biomedical Named Entity Recognizer ( ABNER ) ( Settles , 2005 ) fail to pick the scientific named entities correctly .
Note that the main reason behind ABNER insufficiency is the presence of novel PubMed named entities that were not covered by any of the classes in the ABNER tool .
Therefore , we use a simple heuristic of noun intersection between summary and main -text noun phrases to obtain plausible entity sets .
This produced a shorter version of both the text and the summary than the original pair .
The SUMPUBMED versions statistics is given in Table 1 . The SUMPUBMED overall creation pipeline is shown in Figure 1 . 3 Human Annotation of SUMPUBMED Inspired from work on human evaluation of summaries by Friedrich et al . ( 2014 ) , we distributed 50 randomly chosen summaries from the noun- phrase versions of SUMPUBMED to 10 expert annotators ( graduate NLP students ) such that we have 3 annotation for each summary .
We asked these humanannotators to rate the summaries on a scale of 1 to 10 .
We created different document files , each having 10 pairs of summaries where we randomly shuffled between reference and generated summaries with respect to the placement on the page ( left or right ) .
The annotators evaluated the summaries based on the following criteria : ? Non-Repetition and no factual Redundancy ( Non - Re ) :
There should not be redundancy in the factual information , and no repetition of sentences is allowed .
? Coherence ( Coh ) : Coherence means " continuity of sense " .
The arguments have to be connected sensibly so that the reader can see consecutive sentences as being about one ( or a related ) concept .
? Readability ( Read ) : Consideration of general readability criteria such as good spelling , correct grammar , understandability , etc. in the summaries .
?
Informativeness , Overlap and Focus ( IOF ) :
How much information is covered by the summary .
The goal is to find the common pieces of information via matching the same keywords ( or key phrases ) , such as " Nematodes " , across the summary .
For overlaps , annotators compare the keywords ' ( or key- phrases ) occurrence frequency and ensure the summaries are on the same topic .
The average scores and standard deviations are shown in Table 2 .
Annotators found that for readability , coherence , and non-repetitiveness , the quality of summaries is satisfactory .
However , for informativeness and overlap , it is hard to evaluate summaries due to domain-specific technical terms .
ROUGE and Human Scores
For the 50 summaries evaluated by expert annotators , we calculated the Pearson 's correlation ( Pearson , 1895 ) between ROUGE ( Lin , 2004 ) scores ( ROUGE - 1 ( R- 1 ) , ROUGE - 2 ( R - 2 ) and ROUGE -L ( R- L ) ) in terms of precision , recall and F1 score with the humanevaluated scores .
ROUGE -n is an n-gram similarity measure that computes uni / bi/trigram and higher n-gram overlaps .
In R-L , L refers to the Longest Common Subsequence ( LCS ) overlap : a subsequence of matching words with the maximal length that is common in both texts with the order of words being preserved .
Pearson 's correlation value ( between ?1 and + 1 ) quantifies the degree to which quantitative and continuous variables are related to each other .
The Pearson 's correlations values are shown in Table 3 . ROUGE scores assume that a high-quality summary generated by a model should have common words and phrases with a gold-standard summary .
However , this is not always true because ( a ) there can be semantically similar meaning ( synonymous ) word usage , and ( b ) there can be the usage of text paraphrases ( similar information conveyed ) with a little lexical overlap in the reference summary text .
Therefore , merely considering lexical overlaps to evaluate summary quality is not sufficient .
A high ROUGE score may indicate a good summary , but a low ROUGE score does not necessarily indicate a bad summary .
Furthermore , while summarizing large documents , humans tend to utilize different paraphrasing / words to convey the same meaning in a shorter form .
Several studies by Cohan and Goharian ( 2016 ) ;
Dohare et al. ( 2017 ) argue that ROUGE is not an accurate estimator of the quality of a summary for scientific input , e.g. , biomedical text .
Hence , a weak correlation of ROUGE scores with human ratings on SUMPUBMED , as reported in Table 3 , should not be a surprise .
That is , all correlation values in Table 3 are close to zero , so we can conclude that Rouge scores are weakly related with human ratings on the SUMPUBMED .
Experiments
We have used the noun phrase version of SUMPUBMED in the abstractive summarization settings and the Hybrid version of SUMPUBMED in the extractive and the hybrid settings , i.e. , ( extractive + abstractive ) summarizations .
We split the dataset into train ( 93 % ) , test ( 3 % ) , and validation ( 4 % ) sets .
Before training , we wrote a script that first tokenizes all input files and then forms the vocabulary and chunked files for the train , test , and validation sets .
This step converts the input into a suitable format for the seq2seq models .
Baseline Models
We use the following models on SUMPUBMED for evaluation :
We use extractive , abstractive , and hybrid ( extractive + abstractive ) automatic summarization methods to evaluate SUMPUBMED .
Abstractive Methods
We use several modifications of seq2seq with attention , as described below : Seq2Seq with Attention ( Nallapati et al. , 2016 ) :
The encoder is a single layer bidirectional LSTM , while the decoder is a single layer unidirectional LSTM .
Both the encoder and decoder have same sized hidden states , with an attention mechanism over the source hidden states and a soft-max layer over the vocabulary to generate the words .
We use the same vocabulary for both the encoding and the decoding phase .
Seq2Seq with Pointer Generation Networks ( See et al. , 2017 ) :
The previous model has a computational decoder complexity because each time we have to apply the softmax over the entire vocabulary .
The model also outputs an excessive number of UNK tokens ( UNK is a special token utilized for out- of- vocabulary words ) in the target summary .
To address this issue , we use a pointer - generator network ( See et al . ( 2017 ) ) which integrates the basic seq2seq model ( with attention ) with a copying mechanism ( Gu et al . ( 2016 ) ) .
We call this model seq2seq for the rest of the paper .
The seq2Seq model with Pointer Generation Networks and Coverage Mechanism ( + cov ) ( Mi et al. , 2016 ) :
The summaries generated by the model discussed before may show repetition , like generating the same arrangement of words multiple times ( e.g. , " this bioinformatic approach this bioinformatic approach ... " ) .
This repetition of phrases is prominent when generating multi-line summaries .
The solu- tion to the problem of redundancy in summaries in seq2seq models is the coverage mechanism of Mi et al . ( 2016 ) .
This model penalizes repeated word generations by keeping track of the hitherto covered parts using attention distribution .
Criteria Prec Recall F1 R -1 R -2 R-L R-1 R -2 R-L R-1 R-2 R-L Non-Re - 0. Extractive Methods
There are several existing approaches to extractive summarization , mostly derived from LexRank ( Erkan and Radev , 2004 ) , and TextRank ( Mihalcea and Tarau , 2004 ) .
We use TextRank , which is an unsupervised approach for sentence extraction , and has been used successfully in many NLP applications ( Hulth , 2003 ) .
Hybrid Methods ( Extractive + Abstractive )
We also experimented with the hybrid approach for summarization .
First , we used extractive summarization using the TextRank ranking algorithm .
We then applied abstractive summarization on the extracted text .
We used the pointer- generator networks , followed by the coverage mechanism for the abstractive summarization .
In this setting , we have not perfomed any preprocessing before extractive summarization to decrease the length of the documents .
The extractive summarization step makes the text length sufficient to apply the abstractive summarization step on it quite easily .
Experimental Settings
While decoding seq2seq models ( for abstractive and hybrid models ) , we use a beam search ( Medress et al. , 1977 ) with a beam width of 4 .
Note that , Beam search is a greedy technique which chooses the most likely token from all generated tokens at each step to obtain the best b sequences ( the hyper- parameter b here represents the beam width ) .
Beam search is shown to be better than generating the first sequence .
We also experimented with varying target summary lengths ( i.e. , the number of decoding steps ) for seq2seq models .
We report both seq2seq models with and without coverage results for comparison .
We considered ROUGE - 1 ( R- 1 ) , ROUGE - 2 ( R - 2 ) , and ROUGE -L ( R- L ) 's precision , recall , and F1 score for evaluation .
Hyper-parameters
The hyper-parameters used for the seq2seq model is in We utilized tensorflow package 3 for models and ROUGE evaluation package pyrouge 4 for the evaluation metric .
We use a single GeF orce GT X T IT AN X with 12GB GP U memory taking on average 5 to 6 days per model for model training .
Results and Analysis Results on SUMPUBMED for abstractive methods , i.e. , seq2seq models ( with and without coverage ) , the extractive method of TextRank , and the hybrid approach , i.e. , TextRank + seq2seq ( with and without coverage ) are shown in Tables 6 , 7 , and 8 , respectively .
We also evaluated the seq2seq models on news datasets ( CNN / Daily Mail and DUC 2001 ) for comparison , as shown in Table 5 . Analysis :
In all three approaches , abstractive in words , the chances of non-covered words in the output summary also increase .
We notice in both Tables 6 and 8 that by adding the coverage ( + cov ) mechanism , the problem of repetition in summaries is solved to a great extent .
The ROUGE scores also show improvement after applying coverage to pointer - generator networks .
Thus , one can conclude that pointer generator networks effectively handle named entities and outof-vocabulary words , and the coverage mechanism is useful to avoid repetitive generation , which is essential for scientific summarization .
In Table 9 , we note that in terms of Precision ( Pr ) , the abstractive approach shows the best results .
However , the Recall ( Re ) of the extractive summarization model is always better than abstractive and hybrid approaches .
Furthermore , the R- 1 Re ( ROUGE -1 Recall ) and R-L Re ( ROUGE -L Recall ) for the hybrid models are approximately similar to the abstractive models .
We also provide a few qualitative example of summarization on CNN / DailyMail in Appendix Section A , on SUMPUBMED in Appendic Section B. 2019 ) released a large dataset of 1.3 million of U.S. patent documents along with human written summaries .
However , the closest datasets to SUMPUBMED are released by Cohan et al .
( 2018 ) ; Kedzie et al . ( 2018 ) ; Gidiotis and Tsoumakas ( 2019 ) .
Related Work
Comparison with SUMPUBMED : News datasets ' summary is located at the top of the article for most examples .
Social media datasets lack the scientific aspect , i.e. , complex domain-specific vocabulary and non-localized distributed information of SUMPUBMED .
Other works on the scientific datasets are by Cohan et al .
( 2018 ) ; Kedzie et al . ( 2018 ) ; Gidiotis and Tsoumakas ( 2019 ) .
The closest work to our approach is the PubMed dataset by Cohan et al . ( 2018 ) .
However , unlike SUMPUBMED , ( a ) no extensive preprocessing pipeline was applied to clean the text ( b ) a single version is released compared with SUMPUBMED 's several versions with distinct properties ( varying summary lengths , article lengths , and vocabulary sizes ) , ( c ) only level - 1 section headings instead of the whole PubMed document are used , and ( d ) there is a lack of human evaluation to assess data quality .
However , Cohan et al . ( 2018 ) do act as an powerful inspiration for our work .
Conclusion
We created a non-news , SUMPUBMED dataset , from the PubMed archive to study how various summarization techniques perform on task of scientific summarization on domain specific scientific texts .
These texts have essential information scattered throughout the whole text .
In contrast , earlier datasets with news stories appear to mostly have useful information in the first few lines of the document text .
We also conducted a human evaluation on aspects such as repetition , readability , coherence , and Informativeness for 50 summaries of 250 words .
Each summary is evaluated by 3 different individuals on the basis of four parameters : readability , coherence , non-repetition , and informativeness .
Due to the unavailability of any state- ofthe - art results on this new dataset , we built several baseline models ( extractive , abstractive , and hybrid model ) for SUMPUBMED .
To check the significance of our results , we studied the effectiveness of ROUGE through Pearson 's correlation analysis with human-evaluation and observed that many variants of ROUGE scores correlate poorly with human evaluation .
Our results indicate that ROUGE is possibly not a proper metric for SUMPUBMED .
