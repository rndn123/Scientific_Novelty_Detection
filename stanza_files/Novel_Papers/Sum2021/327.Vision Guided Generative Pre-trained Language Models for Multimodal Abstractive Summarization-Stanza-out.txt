title
Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization
abstract
Multimodal abstractive summarization ( MAS ) models that summarize videos ( vision modality ) and their corresponding transcripts ( text modality ) are able to extract the essential information from massive multimodal data on the Internet .
Recently , large-scale generative pretrained language models ( GPLMs ) have been shown to be effective in text generation tasks .
However , existing MAS models cannot leverage GPLMs ' powerful generation ability .
To fill this research gap , we aim to study two research questions : 1 ) how to inject visual information into GPLMs without hurting their generation ability ; and 2 ) where is the optimal place in GPLMs to inject the visual information ?
In this paper , we present a simple yet effective method to construct vision guided ( VG ) GPLMs for the MAS task using attention - based add - on layers to incorporate visual information while maintaining their original text generation ability .
Results show that our best model significantly surpasses the prior state - of- the - art model by 5.7 ROUGE -1 , 5.3 ROUGE - 2 , and 5.1 ROUGE -L scores on the How2 dataset ( Sanabria et al. , 2018 ) , and our visual guidance method contributes 83.6 % of the overall improvement .
Furthermore , we conduct thorough ablation studies to analyze the effectiveness of various modality fusion methods and fusion locations .
Introduction Multimodal abstractive summarization ( MAS ) aims to take advantage of data from multiple modalities and provides a short , concise and readable textual summary to let users quickly acquire their essential information ( Sanabria et al. , 2018 ; Palaskar et al. , 2019 ; . MAS has become an increasingly popular research area thanks to the proliferation of online multimedia content and the increasing availability of multimodal data .
As illustrated , some information is emphasized ( e.g. the key of g flat ) or only exists ( e.g. piano ) in the visual signal .
We also compare the human-generated reference summary and our model - generated summaries with / without video frames in the input data .
As illustrated in Figure 1 , the MAS models need to generate a concise summary by effectively utilizing two modalities : a video and its transcript .
Therefore , we emphasize that leveraging a powerful text generation model and an effective combination of the vision and text modalities are key to constructing good MAS models .
Recently , Transformerbased ( Vaswani et al. , 2017 b ) sequence-to-sequence ( Seq2Seq ) large-scale generative pre-trained language models ( GPLMs ) , such as BART , T5 ( Raffel et al. , 2019 ) , PEGASUS ( Zhang et al. , 2020a ) and ProphetNet ( Qi et al. , 2020 ) , have shown remarkable performance on text generation tasks , including abstractive text summarization .
However , leveraging and adapting GPLMs to MAS is still an unexplored research direction .
To explore this direction , two main questions need to be answered :
Firstly , how can we inject visual information into the text-only GPLMs so that the models can understand both modalities and allow cross-modal interactions , and more importantly , how can this injection operation be conducted without damaging GPLMs ' original text generation ability ?
Secondly , where is the optimal place in GPLMs to inject the visual information ?
This needs to be explored , as there are many sub-layers in the encoder and decoder of GPLMs and a sub-optimal location might result in unsatisfactory performance .
In this paper , to fill the research gap , we present a simple yet very effective method to construct vision guided ( VG ) GPLMs ( VG - BART and VG - T5 ) for the MAS task .
Specifically , to answer the first of the aforementioned questions , we insert attention - based add - on layers to GPLMs to incorporate visual information without modifying the original architecture .
In this way , all the pre-trained model weights can be used during fine - tuning so as to preserve their original text generation ability .
We try with two types of attention mechanisms for the text-vision fusion and interaction : 1 ) Cross-modal Dot-product Attention ; and 2 ) Cross-modal Multi-head Attention .
Moreover , we also investigate the effects of using a forget gate and a visual transformer encoder along with the attention mechanisms .
To answer the second question , we enumerate almost all possible locations in GPLMs for injecting add - on layers , and show a thorough comparison and analysis in Section 5 .
We evaluate our models on the How2 dataset ( Sanabria et al. , 2018 ) .
Experimental results demonstrate that our best model surpasses the prior state - of- the - art model by 5.7 ROUGE -1 , 5.3 ROUGE - 2 , and 5.1 ROUGE -L scores .
To ensure this improvement does not purely come from the GPLMs , we also evaluate the corresponding textonly model , and the results show that the injected visual guidance contributes 83.6 % of the overall improvement on average of all ROUGE scores .
Our contributions in this work are threefold : ?
To the best of our knowledge , we are the first to inject visual information into text-only GPLMs , and to use it for the MAS task . ?
We systematically study two research questions : 1 ) how to inject visual information into GPLMs without hurting their generation ability ; and 2 ) where is the optimal place in GPLMs to inject the visual information ? ?
Our model significantly outperforms the stateof - the- art model on the How2 dataset , and the injected visual guidance contributes 83.6 % of the overall improvement .
Related Work
Abstractive Text Summarization
Abstractive text summarization aims to generate short , concise and readable text that can capture the most salient information of the input documents .
Thanks to the Seq2Seq framework and attention mechanisms , deep neural networks have achieved remarkable results on summarization tasks ( Paulus et al. , 2017 ; Zhang et al. , 2020 b ; Yu et al. , 2021 ) . Recently , GPLMs Raffel et al. , 2019 ; Zhang et al. , 2020a ; Qi et al. , 2020 ) have been widely used in abstractive text summarization and have achieved start - of - theart performance .
The most significant difference between abstractive text summarization and multimodal abstractive summarization lies in whether the input contains data of more than one modality .
Multimodal Abstractive Summarization
Recently , many studies have been performed on multimodal learning ( Mroueh et al. , 2015 ; Antol et al. , 2015 ; Donahue et al. , 2015 ; Zadeh et al. , 2017 ; Dai et al. , , 2021 .
However , only a few have investigated MAS .
Vision -Language Large Pre-trained Transformer Models
With the remarkable success of large-scale unsupervised pre-training in NLP ( Devlin et al. , 2019 ; ... ( left ) .
To inject visual information , we insert add - on sub-layers ( the green dashed block ) by mainly leveraging two kinds of attention - based text-vision fusion mechanism ( right ) : 1 ) Cross-modal Dot-Product Attention ; and 2 ) Cross-modal Multi-head Attention .
Although we draw the add - on sub-layers in the encoder , they can also be placed in the decoder in a similar way .
We compare the effects of different injection locations in Section 5 .
Radford et al. , 2019 ) , pre-training large vision - language ( VL ) models has also become more and more popular in recent years .
Rather than designing task -specific architectures , pre-training results in a general backbone model by feeding it with a large amount of data and then fine -tune it to different downstream tasks .
Among the current VL pre-training work , most has been focusing on VL understanding by training BERT - style Transformer models ( Sun et al. , 2019 ; Tan and Bansal , 2019 ; Su et al. , 2020 ; and finetune them on various VL classification tasks ( Goyal et al. , 2017 ; Zellers et al. , 2019 ; Suhr et al. , 2019 ) .
These models usually receive a pair of text and image as input , where the image is processed into objects ( Zhang et al. , 2021 ) , patches ( Kim et al. , 2021 ) , or pixels 2020 ) , who proposed a dual-stream model for both VL classification and generation with video data .
However , compared to GPLMs in NLP such as BART and T5 ( Raffel et al. , 2019 ) , their text generation ability is limited as the training data is much smaller .
In this paper , we propose to tackle VL tasks and utilize the advantage of pre-training from a different angle by inserting add - on layers to the text-only GPLMs and fine-tuning them on multimodal tasks to incorporate visual information .
This takes advantage of GPLMs ' superior generation ability to generate vision - aware texts .
Of the very few works that have also considered this direction , Rahman et al . ( 2020 ) proposed the multimodal adaptation gate , which fuses data of other modalities to the textual embeddings in BERT .
However , their method requires all modalities to have the same sequence length , which is rare for most datasets .
Additionally , they only attempted to address the sentiment analysis task and did not explore text generation .
Vision Guided GPLMs
To take advantage of the superior text generation ability of the text-only Seq2seq GPLMs and adapt them to the MAS task , we present Vision guided ( VG ) GPLMs .
Specifically , we leverage BART and T5 ( Raffel et al. , 2019 ) to construct VG - BART and VG - T5 .
In this section , we start by revisiting the text-only Seq2seq GPLMs in Section 3.1 .
These serve as the backbone of our proposed model and also one of the baselines .
Then , we discuss the approach for extracting visual features from video clips in Section 3.2 , as well as how to further process them .
Finally , in Section 3.3 , we introduce two types of text-vision fusion mechanism to guide the GPLMs to generate vision - aware summaries .
Overview of GPLMs for Summarization Transformer - based ( Vaswani et al. , 2017 b ) Seq2Seq GPLMs generalize architectures like BERT ( Devlin et al. , 2019 ) and GPT ( Radford et al. , 2018 ) by including a bi-directional encoder and a unidirectional ( left-to- right ) decoder .
An overview of this architecture is depicted on the left side of Figure 2 ( except the green dashed block ) .
At the entry of the GPLM , the input text is first tokenized and converted to a sequence of token embeddings ?
R ? , in which is the sequence length and is the feature dimension .
To retain the positional information , positional encodings ( Vaswani et al. , 2017a ) ?
R ? are added to the token embeddings pointwisely ( Eq. 1 ) , which forms the input features 0 to the encoder .
0 = + ( 1 ) As illustrated in Figure 2 , the encoder is composed of a stack of encoder layers , each containing two sub-layers : 1 ) Multi-head Self-Attention ( MSA , Eq. 2 ) and 2 ) Feed-Forward Network ( FFN , Eq. 3 ) .
In addition , after each sub-layer , there is a residual connection ( He et al. , 2015 ; Wang et al. , 2019 ) followed by a layer normalization ( LN ) ( Ba et al. , 2016 ) . See Appendix A and B for more details of the MSA and FFN .
= LN ( MSA ( ?1 ) + ?1 ) ( 2 ) = LN ( FFN ( ) + ) ( 3 ) Similar to the encoder , the decoder also consists of a stack of decoder layers , but with two differences .
Firstly , the MSA is masked to prevent positions from attending to subsequent positions ( keep the decoder in a left-to- right direction ) .
Secondly , there is one more multi-head encoder-decoder attention sub-layer , which uses the decoder embeddings to attend over the output embeddings of the encoder to incorporate the encoded information .
Specifically , in our experiments , we adopt the pretrained BART and T5 ( Raffel et al. , 2019 ) , which both follow this architecture with different training schemes .
To fine- tune them on the abstractive text summarization task , the input to the encoder is the article or transcript , and the decoder learns to generate the summaries .
Video Feature Extraction
For each video clip , following previous works ( Sanabria et al. , 2018 ; Palaskar et al. , 2019 ; Khullar and Arora , 2020 ) , a 2048 - dimensional feature representation is extracted for every 16 non-overlapping frames using a 3D ResNeXt - 101 model ( Hara et al. , 2018 ) , which is pre-trained on the Kinetics dataset ( Kay et al. , 2017 ) .
Therefore , each data sample will have a sequence of 2048 - vision feature vectors of length .
These features can be used directly as the visual input to the text-vision fusion mechanism .
In addition , in order to better model the intramodal dynamics and enhance the vision specific temporal information , we further process the extracted sequence of visual features using a Transformer ( Vaswani et al. , 2017a ) encoder ( VTF ) with positional encodings .
Experiments illustrate that this additional encoding process can further boost the performance of our model ( Section 5 ) .
Text-vision Fusion
As exhibited in Figure 2 , we insert a third sub-layer ( the green dashed block ) into each encoder layer , which contains the text-vision fusion mechanism and also a residual connection followed by a layer normalization .
We propose two types of text-vision fusion mechanism , as shown on the right - hand side of the figure .
Given the textual input ?
R ? and visual input ?
R ? , the fusion mechanism produces vision guided output ?
R ? that has a same dimension as the textual input , which allows the continual stacking of layers .
Dot-product Attention Based Fusion .
Before performing dot-product attention between the textual and visual features , we first project the visual features to the same dimensional space as the textual features ( Eq. 4 ) .
Then , we calculate the dot-product and apply the softmax function to get the attention score matrix ( Eq. 5 ) .
Finally , the input textual features are concatenated with the attention weighted visual features and then projected by another linear transformation to output the vision guided textual features ( Eq. 6 ) .
= 1 , ? R ? ( 4 ) = Softmax ( ) , ? R ? ( 5 ) = Concat ( , ) 2 ( 6 ) Additionally , we build a variant of this fusion , which uses the linearly transformed visual features for the concatenation in Eq. 6 instead of the original .
A comparison of their performance is shown in Section 5 .
Multi-head Attention Based Fusion .
Inspired by prior works ( Yu et al. , 2019 ; Tsai et al. , 2019 ) , we propose a vision guided multi-head attention mechanism for the text-vision fusion .
The query is linearly projected from the input textual features , and the key and value are linearly projected from the visual features ( Eq. 7 - 9 ) .
Then , a crossmodal multi-head attention ( CMA ) is applied to get the text queried visual features ( Eq. 10 ) .
Finally , we obtain the vision guided output by concatenating the input textual features and , and linearly project it to the desired dimension ( Eq. 11 ) .
= , ? R ? ( 7 ) = , ? R ? ( 8 ) = , ? R ? ( 9 ) = CMA ( , , ) , ? R ? ( 10 ) = Concat ( , ) 3 ( 11 )
In addition , we also explore the effects of using a forget gate in the text-vision fusion .
Given the CMA output ?
R ? in Eq. 10 , we construct a forget gate mask ?
R ? ( Eq. 12 ) and do a point-wise multiplication with to output the updated ( Eq. 13 ) .
= Sigmoid( Concat ( , ) ) = ? ( 13 )
The forget gate can potentially remove redundant and noisy information from the video features , which also helps the model to learn to discard needless visual information to retain its pre-trained text generation ability .
4 Experimental Settings
Implementation Details Data pre-processing .
We pre-process the transcripts data by truncating or padding them into sequences of 512 tokens after tokenization .
For the videos , after the feature extraction as described in Section 3.2 , we also truncate or pad the sequence length to 256 .
Hyper-parameters .
We use BART - base and T5base as the pre-trained GPLMs to construct VG - BART and VG - T5 , in which = 6 for both encoder and decoder .
For the VTF mentioned in Section 3.2 , we use a 4 - layer encoder with 8 attention heads and a 2048 feed -forward dimension .
In the decoding stage , we use beam search with a beam size of 5 .
The decoding process will not stop until an endof-sequence ( EOS ) token is emitted or the length of the generated summary reaches to 64 tokens .
Following and Raffel et al. ( 2019 ) , we use learning rates 6e ?4 and 3e ?5 to finetune the pre-trained parts of model weights .
While for the newly added layers , we set the learning rate to 1.5e ?4 .
For all of our experiments , we use a batch size of 120 .
Optimizer .
During training , we use the Adam optimizer ( Kingma and Ba , 2015 ) with 1 = 0.9 , Input Method R-1 R-2 R-L B-1 B-2 B-
Baselines Apart from the text-only GPLMs BART and T5 ( Raffel et al. , 2019 ) , we use the following baselines to compare with our proposed models , including simple models that only accept text input , as well as prior state - of - the - art models that accept text and vision modalities .
S2S ( Luong et al. , 2015 ) .
S2S is a standard Seq2seq model that uses RNNs for both encoder and decoder with a global attention mechanism ( Bahdanau et al. , 2014 ) .
PG ( See et al. , 2017 ) .
The pointer generator ( PG ) network augments S2S by having a copy module https://github.com/PyTorchLightning/ pytorch -lightning to reproduce key information accurately as well as mitigating the out-of- vocabulary issue .
TF ( Vaswani et al. , 2017 b ) .
TF is the standard Transformer - based Seq2seq model , which proposes the novel multi-head attention mechanism .
HA ( RNN / Transformer ) ( Palaskar et al. , 2019 ) .
A multi-source Seq2seq model with hierarchical attention ( HA ) ( Libovick ?
and Helcl , 2017 ) that can integrates information from different modalities into a coherent output .
MFFG ( RNN / Transformer ) ( Liu et al. , 2020 ) .
The multistage fusion with forget gate ( MFFG ) model proposes a cross fusion block with forget gate and a hierarchical fusion decoder to improve multimodal generation .
Evaluation Metrics Following , we use ROUGE , BLEU , METEOR , and CIDEr to evaluate the summaries .
ROUGE -{1 , 2 , L} ( the standard metrics for abstractive summarization ) ( Lin and Hovy , 2003 ) and BLEU - {1 , 2 , 3 , 4 } ( Papineni et al. , 2002 ) are used to calculate the recall and precision of n-gram overlaps , respectively , between the references and the generated summaries .
MENTOR ( Denkowski and Lavie , 2011 ) is used to match the word stems , synonyms and paraphrases between the reference and the generated summary .
CIDEr is an image captioning metric to compute the cosine similarity between TF - IDF weighted n-grams .
In addition , We use Content F1 ( Palaskar et al. , 2019 ) to measure the F1 score of the content words of the generated summary based on a monolingual alignment .
Firstly , METEOR toolkit ( Banerjee and Lavie , 2005 ; Denkowski and Lavie , 2014 ) is used to obtain the alignment between the summaries and references .
Then , the function words and task -specific stop words are removed from the summaries and references .
Finally , the remaining content words from the summaries and references are treated as two bags of words , and the F1 scores are calculated over the alignment .
Content F1 focuses more on the content and it can avoid the increase of the ROUGE score from the stop words .
We use nlg-eval to compute the BLEU , MENTOR and CIDEr scores , and use rouge to compute ROUGE scores .
The implementation of Content F1 scores follows ( Palaskar et al. , 2019 ) .
Results and Analysis
Main Results From Table 1 , we can see that when there is only transcript in the input data , S2S and PG reach similar scores in terms of all evaluation metrics .
This could be attributed to the fact that PG tends to copy the content in the transcripts while the reference summaries in the How2 dataset have a great number of novel n-grams , which are defined to be novel with respect to the transcript .
We also observe that TF performs better than RNN - based models .
It is because TF can learn better relationships between words by multi-head attention mechanism and positional embeddings .
Furthermore , both text-only T5 and BART outperform all the baseline models by a large gap owe to their pre-trained text generation ability .
Compared to T5 , BART achieves higher scores mainly because it introduces a novel pre-training objective named sentence permutation .
https://github.com/Maluuba/nlg-eval https://github.com/ neural-dialogue-metrics / rouge
Sentence permutation requires the model to generate the original uncorrupted text from randomly shuffled sentences , which enhances the understanding of long text and benefits the summarization task .
Moreover , BART is even better than all previous multimodal models trained on transcript and video .
The visual guidance consistently boosts the performance of T5 and BART by a large step .
As shown in Table 2 , our best model VG - BART +FG + VTF with the cross-modal multi-head attention surpasses the previous state - of - the - art model ( MFFG ) by 5.7 ROUGE -1 , 5.3 ROUGE - 2 , and 5.1 ROUGE -L scores .
The visual guidance contributes 83.6 % of the overall improvement on average of all ROUGE scores .
The results of Content F1 scores in Table 1 show similar trends with other evaluation metrics .
By injecting visual information , the models can generate summaries with much richer content .
Table 2 shows that both forget gate ( FG ) and visual transformer encoder ( VTF ) benefit the model 's performance .
However , the Content F1 score is not boosted when combining FG and VTF together , which is contradictory to all other metrics .
We conjecture that it is because the Content F1 focuses more on the content aspect , it may have some variance compare to other metrics .
How to Inject Visual Information
As illustrated in Section 3.3 , we mainly adopt two text-vision fusion mechanisms to inject visual information , the cross-modal dot-product attention and multi-head attention .
As shown in Table 1 , for the VG - BART model , these two fusion mechanisms consistently improve its performance on all metrics by a comparable margin .
However , for the VG - T5 model , the cross-modal dot-product attention based fusion does not show any improvement compared to the text-only T5 , while the multi-head attention base fusion still increase its performance .
We think there are two reasons behind this phenomenon .
Firstly , as discussed in Section 5.1 , BART leverages the sentence permutation method as its pre-training objective , which increases its robustness on attentionbased fusion .
Secondly , multi-head attention can capture different key components in the visual information from multiple aspects , which makes it more potent than the dot-product based fusion .
Additionally , as mentioned in Section 3.3 , we build a variant of the dot-product attention based fusion , which achieves 66 .
and 61.4 ROUGE -L on VG - BART .
This comparable result shows that the variant does not provide further improvement .
To ensure the visual features really help in the learning and our add - on layers aid the understanding of them , we conduct further experiments by replacing the visual features in the input data with random noise of the same dimension and sequence length .
The noise is sampled from a uniform distribution 0 to 3 , in a similar value range of the original visual features .
As depicted in Table 3 , VG GPLMs with random noise as visual features achieve similar or slightly worse performance compared to the text-only GPLMs .
This shows the effectiveness of our method to keep GPLMs ' text generation ability .
Furthermore , compared to the dot-product attention based fusion , the multi-head fusion is better at retaining GPLMs ' performance , which again demonstrates its superiority .
As mentioned in Section 3 , we use a forget gate ( FG ) to deal with the redundancy and noisy information in the visual features .
Additionally , we further encode the visual features by a visual transformer encoder ( VTF ) .
Table 2 shows that using either FG or VTF can increase the performance of VG - BART .
Jointly leveraging them boosts the performance by 1.7 , 2.0 , and 1.9 of ROUGE -1 , ROUGE - 2 , and ROUGE -L , respectively .
Where to Inject Visual Information
As discussed in Section 1 , one of the main challenges of building VG GPLMs is to find the optimal location to inject the visual information ( i.e. , the text-vision fusion ) .
A sub-optimal location might lead to a less effective modality fusion and even hurt the GPLMs ' original text generation ability .
As GPLMs have a stack of layers in the encoder Encoder Layer ( BART - base ) R -1 R - 2 R -L and also the decoder , we explore this problem from two aspects : 1 ) which single layer has the best fusion effect ; and 2 ) does multiple times of fusion help GPLMs to understand the visual information better ?
As depicted in Table 4 and 5 , firstly , we enumerate each single layer in the encoder and decoder of our best model ( VG - BART + FG + VTF ) to perform the text-vision fusion .
In terms of ROUGE scores , we can clearly tell that injecting visual information into the encoder can generally boost the model 's performance by a large step , while injecting into the decoder only shows negligible improvement .
Furthermore , in the encoder , we observe that injecting at a higher layer ( closer to the encoder output ) brings more improvement .
Instead , in the decoder , there is no clear pattern showing the influence of injecting location .
We speculate that an early text-vision fusion in the encoder makes the visual information slightly fades away after passing through the stack of encoder layers .
Additionally , during the decoding stage , the model utilizes visual information better through the encoder-decoder attention layers than directly injecting into the decoder , which could potentially hurts the generation ability .
Secondly , as shown in the lower part of locations .
We observe that when fusing at all encoder layers simultaneously , the model converges to a much worse performance .
We conjecture that this causes the catastrophic forgetting of the pre-trained knowledge in GPLMs .
We find that fusing at the last several layers ( e.g. , 5 and 6 ) in the encoder is able to further improve the summarization performance .
Effects of the Forget Gate As mentioned in Section 3.3 , we apply a forget gate ( Eq.12 ) to filter out noise and let the model focus on more important visual information .
To have a deeper understanding of the effects of the forget gate , we calculate the average forget gate score ( averaged over the whole sequence ) for each sample from the How2 test set .
As shown in Figure 3 , most scores are distributed between 0.47 and 0.48 .
There is one data sample the score reaches 0.5 because its transcript is not available .
As illustrated in Table 6 , the model can still generate reasonable summary for it by paying more attention to the visual information .
The meaning of the generated summary is still highly aligned with the reference summary , which shows the capability and flexibility of our model to utilize visual information .
Conclusion and Future Work
In this paper , we introduce a simple yet effective method to construct vision guided large-scale generative pre-trained language models ( VG - BART and VG - T5 ) for the multimodal abstractive summarization task by inserting attention - based add - on layers .
We propose two types of attention mechanisms for the text-vision fusion and interaction : 1 ) Cross-modal Dot-product Attention ; and 2 ) Crossmodal Multi-head Attention .
Moreover , we also Transcript : transcript not available Summary from Transcript + Video : learn tips on how to write " cane " in chinese radicals with mandarin characters in the free video clip .
get free foreign language lessons from an expert .
Reference Summary : learn what ticks are in chinese calligraphy in this free video clip on languages and writing .
investigate the effects of using the forget gate and visual transformer encoder along with the attention mechanisms .
In addition , we enumerate almost all possible locations in GPLMs for injecting addon layers .
Experimental results show that our approaches significantly outperform the prior stateof - the- art on the How2 dataset .
Further analysis illustrates that multi-head attention is more robust than the dot-product attention and higher layers of the encoder is the optimal place to inject vision information .
For future work , we believe that our analyses on the how and where to inject visual information into GPLMs can be applied to other multimodal tasks .
Figure 1 : 1 Figure 1 : An example of MAS .
As input data , we show two representative video frames and the transcript , with [...] representing omitted unimportant text .
As illustrated , some information is emphasized ( e.g. the key of g flat ) or only exists ( e.g. piano ) in the visual signal .
We also compare the human-generated reference summary and our model- generated summaries with / without video frames in the input data .
