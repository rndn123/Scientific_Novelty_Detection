title
Unsupervised Extractive Summarization - Based Representations for Accurate and Explainable Collaborative Filtering
abstract
We pioneer the first extractive summarizationbased collaborative filtering model called ES - COFILT .
Our proposed model specifically produces extractive summaries for each item and user .
Unlike other types of explanations , summary - level explanations closely resemble real-life explanations .
The strength of ES - COFILT lies in the fact that it unifies representation and explanation .
In other words , extractive summaries both represent and explain the items and users .
Our model uniquely integrates BERT , K-Means embedding clustering , and multilayer perceptron to learn sentence embeddings , representation - explanations , and user-item interactions , respectively .
We argue that our approach enhances both rating prediction accuracy and user / item explainability .
Our experiments illustrate that ESCOFILT 's prediction accuracy is better than the other state - of - the - art recommender models .
Furthermore , we propose a comprehensive set of criteria that assesses the real-life explainability of explanations .
Our explainability study demonstrates the superiority of and preference for summary - level explanations over other explanation types .
Introduction Collaborative filtering ( CF ) approaches are the most dominant and outstanding models in recommender systems literature .
CF mainly focuses on learning accurate representations of users and items , denoting user preferences and item characteristics , respectively ( Chen et al. , 2018 ; Tay et al. , 2018 ) .
The earliest CF models learned such representations based on user-given numeric ratings , but employing them is an oversimplification of user preferences and item characteristics ( Koren et al. , 2009 ; Musto et al. , 2017 ) .
In this regard , review texts have been utilized to alleviate this issue .
1 . I was not expecting this Bible to be so beautiful when I pre-ordered it 5 months ago , but it arrived in the mail today and it is just gorgeous !
I love the concept of Bible journaling , but was always a bit intimidated by where / how to start .
This removes that concern through some beautifully done artwork and lettering .
I am ecstatic at the quality of this Bible !
2 . I brought this as I wanted a separate Bible to do Bible journaling .
It is very beautiful and has many images that can be coloured .
The pages are similar to Bible paper and cream in colour .
Overall a wonderful Bible to do journaling and meditate God's Word .
Generated Explanations ?
Review - Level : I brought this as I wanted a separate Bible to do Bible journaling .
It is very beautiful and has many images that can be coloured .
The pages are similar to Bible paper and cream in colour .
Overall a wonderful Bible to do journaling and meditate God's Word .
?
Word - Level : I brought this as I wanted a separate Bible to do Bible journaling .
It is very beautiful and has many images that can be coloured .
The pages are similar to Bible paper and cream in colour .
Overall a wonderful Bible to do journaling and meditate God's Word .
? Summary - Level : I was not expecting this Bible to be so beautiful when I pre-ordered it 5 months ago , but it arrived in the mail today and it is just gorgeous !
This removes that concern through some beautifully done artwork and lettering .
The pages are similar to Bible paper and cream in colour .
Overall a wonderful Bible to do journaling and meditate God's Word .
Table 1 : Illustration of the different types of explanations .
A review- level explanation is simply the highest weighted review .
A word-level explanation is comprised of highlighted words or tokens with the highest attention scores .
Our proposed summary - level explanation closely resembles real-life explanations , wherein the explanation text is derived from multiple reviews .
The primary benefit of using reviews as the source of features is that they can cover the inherently multi-faceted nature of user opinions .
Users can explain their rationales for the ratings they give to items .
Thus , reviews contain a large quantity of rich latent information that cannot be otherwise acquired solely from ratings ( Chen et al. , 2018 ) .
Still , a typical limitation exists for most reviewbased recommender systems recently ; the intrin-sic black - box nature of neural networks ( NN ) makes the explainability behind predictions obscure ( Ribeiro et al. , 2016 ; Wang et al. , 2018 b ) .
The intricate architecture of hidden layers has opaqued the decision -making processes of neural models ( Peake and Wang , 2018 ) .
Providing explanations is essential as they could help persuade users to develop further trust in a recommender system and make eventual purchasing decisions ( Peake and Wang , 2018 ; Ribeiro et al. , 2016 ; Zhang et al. , 2014 ) .
In light of this , current research efforts have attempted to improve the explainability aspect of recommender systems .
Common types of explanations include review- level and word-level .
In a review- level explanation , the attention mechanism is applied to measure every review 's contribution to the item ( or user ) embedding ( Chen et al. , 2018 ; Feng and Zeng , 2019 ) .
High -scoring reviews are then selected to serve as explanations .
On the other hand , in a word- level or token - level explanation , informative words in a local window or textual block are selected together ( Liu et al. , 2019a ; Pugoy and Kao , 2020 ; Seo et al. , 2017 ) .
Similar to the first mechanism , top words are chosen due to their high attention weights .
Evidently , review- level and word-level explanations are side-effects of applying the attention mechanism to reviews and words .
These have been integral and beneficial in formulating better user and item representations .
However , we contend that both types of explanations may not completely resemble real- life explanations .
In logic , an explanation is a set of intelligible statements usually constructed to describe and clarify the causes , context , and consequences of objects , events , or phenomena under examination ( Drake , 2018 ) .
Based on our example in Table 1 , the review- level explanation is exactly the same as the second item review , assuming that it has the higher attention weight .
Due to this , it also inadvertently disregards other possibly useful sentences from other reviews with lower attention scores .
Furthermore , even though the word-level explanation contains informative words , it may not be practical in an actual recommendation scenario since it typically appears as fragments .
Word- level explanations may not be intelligible enough due to humans ' natural bias toward sentences , which are defined to express complete thoughts ( Andersen , 2014 ) .
Therefore , in this paper , we propose the first extractive summarization - based collaborative filtering model , ESCOFILT .
For every item and user , our novel model generates extractive summaries that bear more resemblance to real- life explanations , as seen in Table 1 's last row .
Unlike a review- level explanation , a summary - level explanation ( which we also call extractive summary , representative summary , and representation - explanation in different sections of this paper ) is composed of informative statements gathered from different reviews .
As opposed to a word-level explanation , an ESCOFILT - produced explanation is more comprehensible as it can convey complete thoughts .
It should be noted that our model performs extractive summarization in an unsupervised manner since expecting ground - truth summaries for all items and users in a large dataset is unrealistic .
The strength of ESCOFILT lies in the fact that it uniquely unifies representation and explanation .
In other words , an extractive summary both represents and explains a particular item ( or user ) .
We argue that our approach enhances both rating prediction accuracy and user / item explainability , which are later validated by our experiments and explainability study .
Contributions
These are the main contributions of our paper :
Related Work Developing a CF model involves two crucial steps , i.e. , learning user and item representations and modeling user-item interactions based on those representations ( He et al. , 2018 ) .
One of the foundational works in utilizing NN for CF is neural collaborative filtering or NCF ( He et al. , 2017 ) .
Originally implemented for implicit feedback datadriven CF , NCF learns non-linear interactions between users and items by employing MLP layers as its interaction function .
DeepCoNN is the first deep learning - based model representing users and items from reviews in a coordinated manner ( Zheng et al. , 2017 ) .
The model consists of two parallel networks powered by convolutional neural networks ( CNN ) .
One network learns user behavior by examining all reviews he has written , and the other network models item properties by exploring all reviews it has received .
A shared layer connects these two networks , and factorization machines capture user-item interactions .
Another notable model is NARRE , which shares several similarities with DeepCoNN .
NARRE is also composed of two parallel CNN - based networks for user and item modeling ( Chen et al. , 2018 ) .
For the first time , this model incorporates the review- level attention mechanism that determines each review 's usefulness or contribution based on attention weights .
As a sideeffect , this also leads to review- level explanations ; reviews with the highest attention scores are presented as explanations .
These weights are then integrated into the representations of users and items to enhance embedding quality and prediction accuracy .
Other related studies include D-Attn ( Seo et al. , 2017 ) , MPCN ( Tay et al. , 2018 ) DAML ( Liu et al. , 2019a ) , and HUITA ( Wu et al. , 2019 ) .
These all employ different types of attention mechanisms to distinguish informative parts of a given data sample , resulting in simultaneous accuracy and explainability improvements .
D- Attn integrates global and local attention to score each word to determine its relevance in a review text .
MPCN is similar to NARRE , but the former relies solely on attention mechanisms without any need for convolutional layers .
DAML utilizes CNN 's local and mutual attention to learn review features , and HUITA incorporates a hierarchical , three - tier attention network .
Most of these aforementioned models take advantage of CNNs as automatic review feature ex-tractors .
Coupling them with mainstream word embeddings leads to the formulation of user and item representations .
However , such approaches fail to consider global context and word frequency information .
The two said factors are crucial as they can affect recommendation performance ( Pilehvar and Camacho-Collados , 2019 ; Wang et al. , 2018a ) .
To deal with such dilemmas , NCEM ( Feng and Zeng , 2019 ) and BENEFICT ( Pugoy and Kao , 2020 ) use a pre-trained BERT model to obtain review features .
BERT 's advantage lies in its full retention of global context and word frequency information ( Feng and Zeng , 2019 ) .
For explainability , NCEM similarly adopts NARRE 's review- level attention .
On the contrary , BENEFICT utilizes BERT 's self-attention weights in conjunction with a solution to the maximum subarray problem ( MSP ) .
BENEFICT 's approach produces an explanation based on a subarray of contiguous tokens with the largest possible sum of self-attention weights .
In summary , there appears to be a trend ; tackling explainability improves prediction and recommendation performance consequentially .
While most recommender models address this via attention mechanisms , our proposed model solves this by unifying representation and explanation in the form of extractive summaries .
As evidenced in the succeeding sections of this paper , we argue that our approach can further enhance CF 's accuracy and explainability .
Methodology ESCOFILT , whose architecture is illustrated in Figure 1 , has two parallel components that learn summarization - based user and item representations .
From Sections 3.2 to 3.3 , we will only discuss the item modeling process as it is nearly identical to user modeling , with their inputs as the only difference .
Definition and Notation
The training dataset ?
consists of N tuples , with the latter denoting the size of the dataset .
Each tuple follows this form : ( u , i , r ui , v ui ) where r ui and v ui respectively refer to the ground - truth rating and review accorded by user u to item i. Moreover , let V u = {v u1 , v u2 , ... , v uj } be the set of all j reviews written by user u.
Similarly , let V i = {v 1 i , v 2 i , ... , v ki } be the set of all k reviews received by item i.
Both V u and V i are obtained from scanning ? itself .
The input of ESCOFILT is a user-item pair ( u , i ) from each tuple in ? .
We particularly feed V u and V i to the model as they initially represent u and i .
The output is the predicted rating rui ?
R that user u may give to item i .
Thus , the rating prediction task R can be expressed as : R ( u , i ) = ( V u , V i ) ? rui ( 1 ) Its corresponding objective function , the mean squared error ( MSE ) , is given below : M SE = 1 |? | u , i? ( r ui ? rui ) 2 ( 2 )
Sentence Extraction and BERT Encoding First , the reviews in V i are concatenated together to form a single document .
A sentence segmentation component called Sentencizer ( by spaCy ) is utilized to split this document into individual sentences ( Gupta and Nishu , 2020 ) .
The set of all sentences in V i is now given by S i = {s i1 , s i2 , ... , s ig } where g refers to the total number of sentences .
Afterward , S i is fed to a pre-trained BERT LARGE model .
It should be noted that we opt not to use [ CLS ] representations as these may not necessarily provide the best sentence embeddings ( Miller , 2019 ) .
In this regard , we tap BERT 's penultimate encoder layer to obtain the contextualized word embeddings .
The word embeddings of each sentence in S i are stored in Si ? R g?w?1024 ; w pertains to the amount of words in a sentence , and 1024 is the embedding size of BERT .
Then , we average every sentence 's word embeddings in Si to produce the set of sentence embeddings S i = {s i1 , s i2 , ... , s ig } , with S i ? R g?1024 .
Embedding Clustering K-Means clustering is next performed to partition the sentence embeddings in S i into K clusters .
Its objective is to minimize the intra-cluster sum of the distances from each sentence to its nearest centroid , given by the following equation ( Xia et al. , 2020 ) : J i = K x=1 s iy ?Cx ||s iy ? c x || 2 ( 3 ) where c x is the centroid of cluster C x that is closest to the sentence embedding s iy .
The objective function J i is optimized for item i by running the assignment and update steps until the cluster centroids stabilize .
The assignment step assigns each sentence to a cluster based on the shortest sentence embedding -cluster centroid distance , provided by the formula below : d( s iy ) = argmin x=1 , ..., K {||s iy ? c x || 2 } ( 4 ) where d is a function that obtains the cluster closest to s iy .
Furthermore , the update step recomputes the cluster centroids based on new assignments from the previous step .
This is defined as : c x = 1 | C x | g y=1 {s iy | d( s iy ) = x } ( 5 ) where | C x | refers to the number of sentences that cluster C x contains .
By introducing clustering , redundant and related sentences are grouped in the same cluster .
Concerning this , K is derived using this equation : K = ? i ? g ( 6 ) where ?
i pertains to the item summary ratio , i.e. , the percentage of sentences that comprise an item 's extractive summary .
This subsequently implies that K denotes the actual number of sentences in the summary .
Sentences closest to each cluster centroid are selected and combined to form the item 's representation - explanation .
This is mathematically expressed as : e( C x ) = argmin y=1 , ... , g {||s iy ? c x || 2 } ItemRX i = 1 K K x=1 s i , e( Cx ) ( 7 ) where e is a function that returns the nearest sentence to the centroid c x of cluster C x , and ItemRX i ?
R 1?1024 is the representationexplanation embedding of item i .
Fusion Layers Inspired by NARRE ( Chen et al. , 2018 ) , we also draw some principles from the traditional latent factor model by incorporating rating - based hidden vectors that depict users and items to a certain extent .
These are represented by U serIV and ItemIV , both in R 1 ? m where m is the dimension of the latent vectors .
Such vectors are fused with their respective representation - explanation embeddings .
This is facilitated by these fusion levels , illustrated by the following formulas : f u = ( U serRX u ?
W u + b u ) + U serIV u f i = ( ItemRX i ?
W i + b i ) + ItemIV i f ui = [ f u , f i ] ( 8 ) where f u and f i pertain to the preliminary fusion layers and both are in R 1 ? m ; W u and W i are weight matrices in R 1024 ? m ; b u and b i refer to bias vectors ; and f ui ?
R 1?2 m denotes the initial user-item interactions from the third fusion layer and is later fed to the MLP .
Multilayer Perceptron and Rating Prediction
The MLP is necessary to model the CF effect , i.e. , to learn meaningful non-linear interactions between users and items .
An MLP with multiple hidden layers typically implies a higher degree of nonlinearity and flexibility .
Similar to the strategy of He et al . ( 2017 ) , ESCOFILT adopts an MLP with a tower pattern ; the bottom layer is the widest while every succeeding top layer has fewer neurons .
A tower structure enables the MLP to learn more abstractive data features .
Specifically , we halve the size of hidden units for each successive higher layer .
ESCOFILT 's MLP component is defined as follows : where h L represents the L-th MLP layer , and W L and b L pertain to the L-th layer 's weight matrix and bias vector , respectively .
As far as the MLP 's activation function is concerned , we select the rectified linear unit ( ReLU ) , which yields better performance than other activation functions ( He et al. , 2017 ) .
Finally , the MLP 's output is fed to one more linear layer to produce the predicted rating : h 1 = ReLU ( f ui ?
W 1 + b 1 ) h L = ReLU ( h L?1 ? W L + b L ) ( 9 rui = h L ? W L+1 + b L+1 ( 10 ) 4 Empirical Evaluation
Research Questions
In this section , we detail our experimental setup designed to answer the following research questions ( RQs ) : ? RQ1 : Does ESCOFILT outperform the other state - of- the - art recommender baselines ?
? RQ2 : Is embedding clustering effective ?
? RQ3 : Can our model produce explanations acceptable to humans in real life ?
Datasets , Baselines , and Evaluation Metric Table 2 summarizes the four public datasets 1 that we utilized in our study .
These datasets are Amazon 5 - core , wherein users and items are guaranteed to have at least five reviews each ( McAuley et al. , 2015 ; He and McAuley , 2016 ) .
The ratings across all datasets are in the range of [ 1 , 5 ] .
We split each dataset into training ( 80 % ) , validation ( 10 % ) , and test ( 10 % ) sets .
Next , to validate the effectiveness of ESCOFILT , we compared its prediction performance against four state - of- the - art baselines : ? BENEFICT ( Pugoy and Kao , 2020 ) :
This recent recommender model uniquely integrates BERT , MSP , and MLP to learn representations , explanations , and interactions .
All these recommender models employed the same dataset split .
We then computed the root mean square error ( RMSE ) on the test dataset ( ? ) , as indicated by the formula below .
RMSE is a widely used metric for evaluating a model 's rating prediction accuracy ( Steck , 2013 ) . RM SE = 1 |? | u , i? ( r ui ? rui ) 2 ( 11 )
Experimental Settings For ESCOFILT , we mainly based its summarization component on BERT Extractive Summarizer 2 by Miller ( 2019 ) .
We also utilized the pre-trained BERT LARGE model afforded by the Transformers library of HuggingFace 3 .
In our implementation 4 , the following hyperparameters were fixed : ?
Learning rate : 0.006 ? Quantity of MLP layers : 4 ? Item summary ratio ( ? i ) : 0.4 ? User summary ratio ( ? u ) : 0.4
On the other hand , we operated an exhaustive grid search over these hyperparameters : ?
Number of epochs : [ 1 , 30 ] ?
Latent vector dimension ( m ) : { 32 , 128 , 220 }
Due to its architectural similarity to ESCOFILT , we reimplemented BENEFICT by augmenting it with the pre-trained BERT LARGE model and adopting our model 's fusion and latent vector dimension strategies .
For DeepCoNN , MPCN , and NARRE , we employed the extensible NRRec framework 5 and retained the other hyperparameters reported in the framework ( Liu et al. , 2019 b ) .
For the four baselines , we also performed an exhaustive grid search over the following : ?
Number of epochs : [ 1 , 30 ] ?
Learning rates : { 0.003 , 0.004 , 0.006 }
All models , including ESCOFILT , used the same optimizer , Adam , which leverages the power of adaptive learning rates during training ( Kingma and Ba , 2014 ) .
This makes the selection of a learning rate less cumbersome , leading to faster convergence ( Chen et al. , 2018 ) .
Without special mention , the models shared the same random seed , batch size ( 128 ) , and dropout rate ( 0.5 ) .
We selected the model configuration with the lowest RMSE on the validation set .
We ran our experiments on NVIDIA GeForce RTX 2080 Ti .
Prediction Results and Discussion
Performance Comparison
The overall performances of our model and the other baselines are summarized in Table 3 .
It is essential to remark that although utilizing information derived from reviews is beneficial , a model 's performance can vary contingent on how the said information is considered .
These are our general findings :
First , our proposed model consistently outperforms all baselines across all datasets .
This ascertains the effectiveness of ESCOFILT and clearly answers RQ1 .
Moreover , this validates our case that coupling BERT ( a superior review feature extractor ) with embedding clustering enables user and item representations to have finer granularity and fewer redundancies .
Second , receiving the two lowest average RMSE values , BERT - based models ( ESCOFILT and BENEFICT ) have generally better prediction accuracies than the rest of the mostly CNN - powered baselines .
This particular observation verifies the necessity of integrating BERT in a CF architecture .
Unlike its mainstream counterparts , BERT produces more semantically meaningful embeddings that keep essential elements such as global context and word frequency information .
Efficacy of Embedding Clustering
This section further discusses the efficacy of K-Means embedding clustering , instrumental in producing user and item representative summaries .
Concerning this , we prepared three variants of our model .
There appears to be a trend as well : the secondbest and the third - best variants are ESCOFILT - I and ESCOFILT -U , respectively .
In some instances , ESCOFILT - I seems to be on par with the default ESCOFILT variant .
This implies that items stand to benefit more than users from embedding clustering .
One possible explanation is that each item normally receives a far greater quantity of reviews than each user actually writes , translating to more possibly extractable information and features .
Hence , item reviews have a more significant influence than user reviews in determining ratings .
Still , this does not immediately suggest that user embedding clustering is not helpful .
It needs to be integrated first with item embedding clustering via the MLP to discover relevant user-item interactions , leading to our original model 's performance .
Explainability Study
Real -Life Explainability Criteria
The assessment of explanations in existing recommender systems literature is generally limited to specific case studies .
Most of these relied on simple qualitative analysis of attention weights and highscoring reviews on selected samples ( Liu et al. , 2019a ; Seo et al. , 2017 ; Wu et al. , 2019 ) .
The assessment criterion provided in the NARRE and BENEFICT papers went a little further by asking human raters to score each explanation 's helpfulness or usefulness on a given Likert scale ( Chen et al. , 2018 ; Pugoy and Kao , 2020 ) .
Nevertheless , to the best of our knowledge , there does not appear to be a comprehensive set of criteria that assesses the real- life explainability of explanations .
We contend that it is increasingly necessary to measure how people actually perceive explanation texts generated by recommender models ; after all , these texts aim to explain entities in real life .
Hence , we propose the following explainability criteria , which are inspired by Zemla et al . ( 2017 ) : 1 . Coherence : " Parts of the explanation fit together coherently . "
2 . Completeness : " There are no gaps in the explanation . "
3 . Lack of Alternatives : " There are probably less to no reasonable alternative explanations . "
4 . Novelty : " I learned something new from the explanation . "
5 . Perceived Truth : " I believe this explanation to be true . "
6 . Quality : " This is a good explanation . "
7 . Visualization : " It is easy to visualize what the explanation is saying . "
Human Assessment of Explanations
We generated a total of 90 item explanations , 30 each from BENEFICT ( token - level ) , NARRE ( review- level ) , and ESCOFILT ( summary - level ) .
For pointwise evaluation , we asked two human judges to assess the explanations based on our proposed real - life explainability criteria on a five-point Likert scale .
For listwise evaluation , we instructed them to rank the three explanation types for every text according to helpfulness .
We further examined these results by determining the strength of agreement between the two judges , using Cohen 's Kappa coefficient ( ? ) wherein - 1 indicates a less than chance agreement , 0 refers to a random agreement , and 1 denotes a perfect agreement ( Borromeo and Toyama , 2015 ; Landis and Koch , 1977 ) .
Explainability Results and Discussion Table 4 summarizes the results of the human judges ' pointwise evaluation .
For five out of seven criteria , ESCOFILT - derived explanations have the highest explainability scores .
Specifically , summarylevel explanations are most coherent , most complete , most novel , and most truthful .
ESCOFILT 's strongest aspect is its perceived truth , obtaining a mean rating of 3.92 and ? = 0.28 that indicates a fair inter-judge agreement .
Interestingly , both ESCOFILT and NARRE have the best quality , with the same mean rating of 3.72 .
The Kappa coefficient is 0.11 , implying that the judges agree with each other to a certain extent .
Considering that a review- level explanation is simply the highest weighted review , our modelgenerated explanations are assessed on par with the former .
Furthermore , review- level explanations have the highest explainability scores in two other criteria , i.e. , lack of alternatives and visualization .
NARRE 's strongest aspect is that its explanations are easiest to visualize , having a mean rating of 3.92 and ? = 0.27 that denotes a fair inter-judge agreement .
Lastly , Figure 3 shows the results of the human judges ' listwise evaluation .
Our model produces the most helpful explanations ; such explanations are ranked first for almost 83 % of the items .
These are followed far behind by NARRE 's explanations , ranked first for nearly 17 % of the items .
None of BENEFICT 's explanations are ranked first .
With ? = 0.45 for ranking consistency , there is a moderate agreement between the judges .
In summary , these results clearly illustrate the superiority of summary - level explanations in real life that can present necessary guidance to users in making future purchasing decisions , thereby satisfying RQ3 .
Conclusion and Future Work
In this study , unifying representations and explanations , in the form of extractive summaries , have further enhanced collaborative filtering accuracy and explainability .
We have successfully developed a model that uniquely integrates BERT , embedding clustering , and MLP .
Our experiments on various datasets verify ESCOFILT 's predictive capability , and the human judges ' assessments validate its explainability in real life .
In the future , we shall consider expanding our model 's explainability capability by possibly incorporating other NLP principles such as abstractive summarization and natural language generation .
Figure 2 : 2 Figure 2 : Performance comparison of ESCOFILT variants for illustrating the effectiveness of embedding clustering .
