title
Low Resource Similar Language Neural Machine Translation for Tamil-Telugu
abstract
This paper describes the participation of team oneNLP ( LTRC , IIIT - Hyderabad ) for the WMT 2021 task , similar language translation 1 . We experimented with transformer based Neural Machine Translation and explored the use of language similarity for Tamil-Telugu and Telugu-Tamil .
We incorporated use of different subword configurations , script conversion and single model training for both directions as exploratory experiments .
Introduction Machine Translation ( MT ) is a field of Natural Language Processing which aims to translate a text from one natural language to another .
The meaning of the source text must be fully preserved in the resulting translated text in the target language .
Recent years have seen significant quality advancements in machine translation with the advent of Neural Machine Translation .
For the translation task , different types of machine translation systems have been developed and they are mainly categorized into Rule based Machine Translation ( RBMT ) ( Forcada et al. , 2011 ) , Statistical Machine Translation ( SMT ) ( Koehn , 2009 ) and Neural Machine Translation ( NMT ) ( Bahdanau et al. , 2014 ) .
Neural machine translation ( NMT ) shows high quality in terms of output fluency and translation quality , when large amounts of parallel data are available ( Barrault et al. , 2020 ) .
Unfortunately , for most language pairs , parallel data is either scare or non-existent .
To overcome this , unsupervised MT ( UMT ) ( Artetxe et al. , 2020 ) focuses on utilising monolingual data to generate synthetic parallel training data .
Other techniques like backtranslation ( Sennrich et al. , 2015 ) , ( Hoang et al. , 1 https://www.statmt.org/wmt21/similar.html 2018 ) , ( Feldman and Coto-Solano , 2020 ) or denoising ( Kim et al. , 2019 ) also rely on parallel corpora of other language pairs and / or large quantities of monolingual data .
This paper describes our experiments for very low resourced similar language translation .
For our work , we focused only on Tamil - Telugu language pair ( both directions ) and participated in a constrained setting .
We experimented only with Transformer ( Vaswani et al. , 2017 ) based Neural Machine Translation throughout .
Along with it , to tackle high agglutination of both languages , we explored the morph ( Virpioja et al. , 2013 ) induced sub-word segmentation with byte pair encoding ( BPE ) ( Sennrich et al. , 2016 ) . Similar to Multilingual Neural Machine Translation ( MNMT ) , we explored the use of a tag trick , where a token like " < 2 xx > " ( xx is language code ) is prefixed to each source sentence to indicate the desired target language ( Dabre et al. , 2020 ) .
Here , we trained a single model for both directions ( Tamil -Telugu and Telugu-Tamil ) on given parallel data and monolingual data under MNMT setting .
The sections of the paper are organised as following : Section 2 describes Data , Section 3 and 4 describe pre-processing and Training Configuration and in Section 5 we talk about results and we conclude in section 6 .
Data
We utilised provided parallel corpora for Tamil <-> Telugu MT task .
Apart form parallel corpus , we randomly selected 0.1 M monolingual corpora from IndicCorp monolingual corpus 2 for Tamil and Telugu .
Morph + BPE Segmentation Based on token / type ratio , both Tamil and Telugu are morphologically rich languages from Table - 1 .
Translating from ( and to ) morphologicallyrich agglutinative language is more difficult due to their complex morphology and large vocabulary .
We address this issue with morphology and BPE ( Sennrich et al. , 2016 ) based segmentation method as prescribed in ( Mujadia and Sharma , 2020 ) .
We utilized unsupervised Morfessor ( Virpioja et al. , 2013 ) by training it on monolingual data of Tamil and Telugu .
We then applied this trained Morfessor model on our corpora ( train , test , development ) to get meaningful stem , morpheme , suffix segmented sub-tokens for each word in a sentence .
Subsequently , we applied the subword algorithm on top of the morph segmentation and used the derived sequence in training .
Training as Multilingual Neural Machine Translation ( MNMT )
As an exploratory experiment , we configure a similar low resource machine translation problem as a multilingual machine translation problem .
For both translation directions ( Tamil-Telugu and Telugu-Tamil ) we trained a single model to take advantage of language similarity among these languages .
First , we converted both languages into Roman script using litcm 4 . Second , we prefixed " < 2TE > " for Tamil to Telugu and " < 2TA > " for Telugu to Tamil to the respective source sentences .
Apart from this , we also utilised monolingual data as a monolingual translation .
For this we prefixed " < 2TE > " for Telugu to Telugu and " < 2TA > " for Tamil to Tamil translation .
Training Configuration
Throughout all experiments , we used Transformer sequence to sequence architecture with the following configuration .
?
Morph + BPE based subword segmentation , Embedding size : 512
Transformer for encoder and decoder , rnn_size 512 , heads 4 encoder - decoder layers : 2 , label smoothing : 1.0 , dropout : 0.30 , Optimizer : Adam , Beam size : 4 ( train ) and 10 ( test ) , training steps : 20K
For these experiments , we used shared vocab across trainings .
We used Opennmt-py ( Klein et al. , 2020 ) toolkit with above configuration for our experiments .
Using the above described pre-processing and configuration , we performed experiments on word level , BPE level and morph + BPE level for input and output .
The results are discussed in following Result section .
( Papineni et al. , 2002 ) for Tamil-Telugu and Telugu-Tamil respectively on the development data .
To get trivial , non-translation baseline , we used aksharamukha 5 Table - 2 and Table - 3 show that non-translation baselines are also low in terms of BLEU scores which indicates that the task much harder even though languages are similar .
The results show that for low resource settings , transformer network based MT models can be improved with morph based segmentation along with byte pair encoding for morph rich languages .
Also , forming it as a Multilingual machine translation problem , along with monolingual data , it improves the quality of MT models .
This may be due to language similarity and use of monolingual data , as it is helping models to do better generalization by learning better source language encoding and target language fluency .
Conclusion
From our experiments , we conclude that linguistic feature such as morph based segmentation with subword segments along with MNMT is a promising approach for similar language translation .
Table - 1 describes the training and development data ( parallel and monolingual ) used in all our experiments under constrained setting .
Data Sents Token Type Train Tamil ( Parallel ) 40,147 0.68 M 74 K Telugu ( Parallel ) 40,147 0.72 M 90 K Development Tamil ( Parallel ) 1261 29 K 9 K Telugu ( Parallel ) 1261 30 K 10 K Tamil ( Mono ) 0.1M - - Telugu ( Mono ) 0.1M - - Table 1 : Tamil-Telugu WMT2021
Training data 3 Data Pre-Processing
To tokenize and clean both Tamil and Telugu cor- pora ( train , test , valid and monolingual ) , we used IndicNLP Tool 3 with in - house tokenizer as a first step .
Following subsections explain other pre- processing steps of experiments .
