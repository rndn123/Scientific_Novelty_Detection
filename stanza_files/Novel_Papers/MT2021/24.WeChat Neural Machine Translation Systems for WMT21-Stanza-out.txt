title
WeChat Neural Machine Translation Systems for WMT21
abstract
This paper introduces WeChat AI 's participation in WMT 2021 shared news translation task on English ?
Chinese , English ?
Japanese , Japanese ?
English and English ?
German .
Our systems are based on the Transformer ( Vaswani et al. , 2017 ) with several novel and effective variants .
In our experiments , we employ data filtering , large-scale synthetic data generation ( i.e. , back - translation , knowledge distillation , forward - translation , iterative in - domain knowledge transfer ) , advanced finetuning approaches , and boosted Self - BLEU based model ensemble .
Our constrained systems achieve 36.9 , 46.9 , 27.8 and 31.3 casesensitive BLEU scores on English ?
Chinese , English ?
Japanese , Japanese ?
English and English ?
German , respectively .
The BLEU scores of English ?
Chinese , English ?
Japanese and Japanese ?
English are the highest among all submissions , and that of English ?
German is the highest among all constrained submissions .
Introduction
We participate in the WMT 2021 shared news translation task in three language pairs and four language directions , English ?
Chinese , English ?
Japanese , and English ?
German .
In this year 's translation tasks , we mainly improve the final ensemble model 's performance by increasing the diversity of both the model architecture and the synthetic data , as well as optimizing the ensemble searching algorithm .
Diversity is a metric we are particularly interested in this year .
To quantify the diversity among different models , we compute Self - BLEU from the translations of the models on the valid set .
To be precise , we use the translation of one model as the hypothesis and the translations of other models as references to calculate an aver - * Equal contribution .
age BLEU score .
A higher Self - BLEU means this model is less diverse .
For model architectures ( Vaswani et al. , 2017 ; , we exploit several novel Transformer variants to strengthen model performance and diversity .
Besides the Pre-Norm Transformer , the Post-Norm Transformer is also used as one of our baselines this year .
We adopt some novel initialization methods ( Huang et al. , 2020 ) to alleviate the gradient vanishing problem of the Post- Norm Transformer .
We combine the Average Attention Transformer ( AAN ) and Multi-Head-Attention ( Vaswani et al. , 2017 ) to derive a series of effective and diverse model variants .
Furthermore , Talking - Heads Attention ( Shazeer et al. , 2020 ) is introduced to the Transformer and shows a significant diversity from all the other variants .
For the synthetic data generation , we exploit the large-scale back - translation ( Sennrich et al. , 2016a ) method to leverage the target-side monolingual data and the sequence - level knowledge distillation ( Kim and Rush , 2016 ) to leverage the source-side of bilingual data .
To use the source-side monolingual data , we explore forward -translation by ensemble models to get general domain synthetic data .
We also use iterative in-domain knowledge transfer to generate in- domain data .
Furthermore , several data augmentation methods are applied to improve the model robustness , including different token - level noise and dynamic top -p sampling .
For training strategies , we mainly focus on scheduled sampling based on decoding steps ( Liu et al. , 2021 b ) , the confidence - aware scheduled sampling ( Mihaylova and Martins , 2019 ; Duckworth et al. , 2019 ; Liu et al. , 2021a ) , the target denoising method and the Graduated Label Smoothing for in- domain finetuning .
For model ensemble , we select high-potential candidate models based on two indicators , namely model performance ( BLEU scores on valid set ) and model diversity ( Self - BLEU scores among all other models ) .
Furthermore , we propose a search algorithm based on the Self - BLEU scores between the candidate models with selected models .
We observed that this novel method can achieve the same BLEU score as the brute force search while saving approximately 95 % of search time .
This paper is structured as follows : Sec. 2 describes our novel model architectures .
We present the details of our systems and training strategies in Sec.
3 . Experimental settings and results are shown in Sec. 4 . We conduct analytical experiments in Sec. 5 .
Finally , we conclude our work in Sec. 6 .
Model Architectures
In this section , we describe the model architectures used in the four translation directions , including several different variants for the Transformer ( Vaswani et al. , 2017 ) .
Model Configurations
Deeper and wider architectures are used this year since they show strong capacity as the number of parameters increases .
In our experiments , we use multiple model configurations with 20 / 25 - layer encoders for deeper models and the hidden size is set to 1024 for all models .
Compared to our WMT20 models , we also increase the decoder depth from 6 to 8 and 10 as we find that gives a certain improvement , but deeper depths give limited performance gains .
For the wider models , we adopt 8/12/15 encoder layers and 1024/2048 for hidden size .
The filter sizes of models are set from 8192 to 15000 .
Note that all the above model configurations are applied to the following variant models .
Transformer with Different Layer- Norm
The Transformer ( Vaswani et al. , 2017 ) with Pre-Norm ( Xiong et al. , 2020 ) is a widely used architecture in machine translation .
It is also our baseline model as its performance and training stability is better than the Post - Norm counterpart .
Recent studies Huang et al. , 2020 ) show that the unstable training problem of Post-Norm Transformer can be mitigated by modifying initialization of the network and the successfully converged Post - Norm models generally outperform Pre-Norm counterparts .
We adopt these initialization methods ( Huang et al. , 2020 )
Average Attention Transformer
We also use Average Attention Transformer ( AAN ) as we used last year to introduce more model diversity .
In the Average Attention Transformer , a fast and straightforward average attention is utilized to replace the self-attention module in the decoder with almost no performance loss .
The context representation g i for each input embedding is as follows : g i = F F N ( 1 i i k=1 y k ) ( 1 ) where y k is the input embedding for step k and i is the current time step .
F F N ( ? ) denotes the position - wise feed - forward network proposed by Vaswani et al . ( 2017 ) .
In our preliminary experiments , we observe that the Self - BLEU scores between AAN and Transformer are lower than the scores between the Transformer with different configurations .
Weighted Attention Transformer
We further explore three weighting strategies to improve the modeling of history information from previous positions in AAN .
Compared to the average weight across all positions , we try three methods including decreasing weights with position increasing , learnable weights and exponential weights .
In our experiments ,
We observe exponential weights perform best among all these strategies .
The exponential weights context representation g i is calculated as follows : c i = ( 1 ? ?) y i + ? ? c i?1 ( 2 ) g i = F F N ( c i ) ( 3 ) where ? is a tuned parameter .
In our previous experiments , we test different alpha , including 0.3 , 0.5 , and 0.7 , on the valid set and we set the alpha to 0.7 in all subsequent experiments as it slightly outperform the others .
In the experiments , Mixed - AAN not only performs better but also shows strong diversity compared to the vanilla Transformer .
With four Mixed - AAN models , we reach a better ensemble result than the result with ten models which consist of deeper and wider standard Transformer .
We will further analyze the effects of different architectures from performance , diversity , and model ensemble in Sec. 5.1
Talking - Heads Attention In Multi-Head Attention , the different attention heads perform separate computations , which are then summed at the end .
Talking - Heads Attention ( Shazeer et al. , 2020 ) is a new variation that inserts two additional learned linear projection weights , W l and W w , to transform the attention -logits and the attention scores respectively , moving information across attention heads .
The calculation formula is as follows : Attention ( Q , K , V ) = sof tmax ( QK T ? d k W l ) W w V ( 4 ) We adopt this method in both encoders and decoders to improve information interaction between attention heads .
This approach shows the most remarkable diversity among all the above variants with only a slight performance loss .
System Overview
In this section , we describe our system used in the WMT 2021 news shared task .
We depicts the overview of our NMT system in Figure 2 , which can be divided into four parts , namely data filtering , large-scale synthetic data generation , in - domain finetuning , and ensemble .
The synthetic data generation part further includes the generation of general domain and in-domain data .
Next , we proceed to illustrate these four parts .
Data Filtering
We filter the bilingual training corpus with the following rules for most language pairs : ?
Normalize punctuation with Moses scripts except Japanese data . ?
Filter out the sentences longer than 100 words or exceed 40 characters in a single word .
?
Filter out the duplicated sentence pairs . ?
The word ratio between the source and the target words must not exceed 1:4 or 4:1 . ?
Filter out the sentences where the fast - text result does not match the origin language .
?
Filter out the sentences that have invalid Unicode characters .
Besides these rules , we filter out sentence pairs in which Chinese sentence has English characters in En- Zh parallel data .
The monolingual corpus is also filtered with the n-gram language model trained by the bilingual training data for each language .
All the above rules are applied to synthetic parallel data .
General Domain Synthetic Data Generation
In this section , we describe our techniques for constructing general domain synthetic data .
The general domain synthetic data is generated via large-scale back - translation , forward - translation and knowledge distillation to enhance the models ' performance for all domains .
Then , we exploit the iterative in- domain knowledge transfer in Sec 3.3 , which transfers in-domain knowledge to the vast source -side monolingual corpus , and builds our in- domain synthetic data .
In the following sections , we elaborate the above techniques in detail .
Large-scale Back -Translation Back-translation is the most commonly used data augmentation technique to incorporate the target side monolingual data into NMT ( Hoang et al. , 2018 ) .
Previous work ( Edunov et al. , 2018 ) has shown that different methods of generating pseudo corpus has a different influence on translation quality .
Following these works , we attempt several generating strategies as follows : ? Beam Search : Generate target translation by beam search with beam size 5 . ? Sampling Top -K : Select a word randomly from top -K ( K is set to 10 ) words at each decoding step .
? Dynamic Sampling Top-p : Selected a word at each decoding step from the smallest set whose cumulative probability mass exceeds p and the p is dynamically changing from 0.9 to 0.95 during data generation .
Note that we also use Tagged Back -Translation ( Caswell et al. , 2019 ) in En?De and Right- to - Left ( R2L ) back - translation in En?Ja , as we achieve a better BLEU score after using these methods .
Knowledge Distillation Knowledge Distillation ( KD ) has proven to be a powerful technique for NMT ( Kim and Rush , 2016 ; Wang et al. , 2021 ) to transfer knowledge from the teacher model to student models .
In particular , we first use the teacher models to generate synthetic corpus in the forward direction ( i.e. , En?Zh ) .
Then , we use the generated corpus to train our student models .
Notably , Right- to- Left ( R2L ) knowledge distillation is a good complement to the Left-to- Right ( L2R ) way and can further improve model performance .
Forward -Translation
Using monolingual data from the source language to further enhance the performance and robustness of the model is also an effective approach .
We use the ensemble model to generate high quality forward -translation data and obtain a stable improvement in En?Zh and En? De directions .
Iterative In-domain Knowledge Transfer
Since in- domain knowledge transfer delivered a massive performance boost last year , we still use this technique in En?Ja and En? De this year .
It is not applied to En? Zh because no significant improvement is observed .
We guess the reason is that the in-domain finetuning in the En? Zh direction does not bring a significant improvement compared to the other directions .
And in- domain knowledge transfer is aiming at enhancing the effect of finetuning , so this does not have a noticeable effect in the English - Chinese direction .
We first use normal finetuning in Sec. 3.5 to equip our models with in- domain knowledge .
Then , we ensemble these models to translate the source monolingual data into the target language .
We use 4 models with different architectures and training data as our ensemble model .
Next , we combine the source language sentences with the generated in - domain target language sentences as pseudoparallel corpus .
Afterwards , we retrain our models with both in - domain pseudo-parallel data and general domain synthetic data .
Data Augmentation
Once the pseudo-data is constructed , we further obtain diverse data by adding different noise .
Compared to previous years ' WMT competitions , we implement a multi-level static noise approach for our pseudo corpus : ?
Token - level : Noise on every single subword after byte pair encoding .
?
Word- level : Noise on every single word before byte pair encoding .
?
Span-level : Noise on a continuous sequence of tokens before byte pair encoding .
The different granularities of noise make the data more diverse .
The noise types are random replacement , random deletion and random permutation .
We apply the three noise types in a parallel way for each sentence .
The probability of enabling each of the three operations is 0.2 .
Furthermore , an on- the-fly noise approach is applied to the synthetic data .
By using on - the-fly noise , the model is trained with different noises in every epoch rather than all the same along this training stage .
In-domain Finetuning
A domain mismatch exists between the obtained system trained with large-scale general domain data and the target test set .
To alleviate this mismatch , we finetune these convergent models on small scale in - domain data , which is widely used for domain adaption ( Luong and Manning , 2015 ; Li et al. , 2019 ) .
We take the previous test sets as in-domain data and extract documents that are originally created in the source language for each translation direction ( Sun et al. , 2019 ) .
We also explore several advanced finetuning approaches to strengthen the effects of domain adaption and ease the exposure bias issue , which is more serious under domain shift .
Target Denoising .
In the training stage , the model never sees its own errors .
Thus the model trained with teacher - forcing is prune to accumulated errors in testing ( Ranzato et al. , 2016 ) .
To mitigate this training -generation discrepancy , we add noisy perturbations into decoder inputs when finetuning .
Thus the model becomes more robust to prediction errors by target denoising .
Specifically , the finetuning data generator chooses 30 % of sentence pairs to add noise , and keeps the remaining 70 % of sentence pairs unchanged .
For a chosen pair , we keep the source sentence unchanged , and replace the i-th token of the target sentence with ( 1 ) a random token of the current target sentence 15 % of the time ( 2 ) the unchanged i-th token 85 % of the time .
Graduated Label-smoothing .
Finetuning on a small scale in- domain data can easily lead to the over-fitting phenomenon which is harmful to the model ensemble .
It generally appears as the model over confidently outputting similar words .
To further preventing overfitting of in- domain finetuning , we apply the Graduated Label - smoothing approach , which assigns a higher smoothing penalty for high - confidence predictions , during in - domain finetuning .
Concretely , following the paper 's setting , we set the smoothing penalty to 0.3 for tokens with confidence above 0.7 , zero for tokens with confidence below 0.3 , and 0.1 for the remaining tokens .
Confidence -Aware Scheduled Sampling .
Vanilla scheduled sampling simulates the inference scene by randomly replacing golden target input tokens with predicted ones during training .
However , its critical schedule strategies are only based on training steps , ignoring the real-time model competence .
To address this issue , we propose confidence - aware scheduled sampling ( Liu et al. , 2021a ) , which quantifies real-time model competence by the confidence of model predictions .
At the t-th target token position , we calculate the model confidence conf ( t ) as follow : conf ( t ) = P ( y t |y <t , X , ? ) ( 5 ) Next , we design fine - grained schedule strategies based on the model competence .
The fine- grained schedule strategy is conducted at all decoding steps simultaneously : y t?1 = y t?1 if conf ( t ) ? t golden ?t?1 else ( 6 ) where t golden is a threshold to measure whether conf ( t ) is high enough ( e.g. , 0.9 ) to sample the predicted token ?t?1 .
We further sample more noisy tokens at highconfidence token positions , which prevents scheduled sampling from degenerating into the teacher forcing mode .
y t?1 = ? ? ? ? ? y t?1 if conf ( t ) ? t golden ?t?1 if t golden < conf ( t ) ? t rand y rand if conf ( t ) > t rand ( 7 ) where t rand is a threshold to measure whether conf ( t ) is high enough ( e.g. , 0.95 ) to sample the random target token ? rand .
Scheduled Sampling Based on Decoding Steps .
We propose scheduled sampling methods based on decoding steps from the perspective of simulating the distribution of real translation errors ( Liu et al. , 2021 b ) .
Namely , we gradually increase the selection probability of predicted tokens with the growth of the index of decoded tokens .
At the t-th decoding step , the probability of sampling golden tokens g ( t ) is calculated as follow :
Add m index to candidate list C 9 : end while 10 : return C ? Linear Decay : g( t ) = max ( , kt + b ) , where is the minimum value , and k < 0 and b is respectively the slope and offset of the decay .
? Exponential Decay : g( t ) = k t , where k < 1 is the radix to adjust the decay .
? Inverse Sigmoid Decay : g( t ) = k k+e t k , where e is the mathematical constant , and k ?
1 is a hyperparameter to adjust the decay .
Following our preliminary conclusions ( Liu et al. , 2021 b ) , we choose the exponential decay and set k to 0.99 by default .
Boosted Self-BLEU based Ensemble ( BSBE )
After we get numerous finetuned models , we need to search for the best combination for ensemble model .
Ordinary random or greedy search is oversimplified to search for a good model combination and enumerate over all combinations of candidate models is inefficient .
The Self- BLEU based pruning strategy we proposed in last year 's competition achieve definite improvements over the ordinary ensemble .
However , diversity is not the only feature we need to consider but the performance in the valid set is also an important metric .
Therefore , we combine Self - BLEU and valid set BLEU together to derive a Boosted Self - BLEU - based Ensemble ( BSBE ) algorithm .
Then , we apply a greedy search strategy in the top N ranked models to find the best ensemble models .
See algorithm 1 for the pseudo-code .
The algorithm takes as input a list of n strong single models M , BLEU scores on valid set for each model B , average Self - BLEU scores for each model S , the number of models n and the number of ensemble models c .
The algorithm return a list C consists of selected models .
We calculate the weighted score for each model as line 2 in the pseudo-code .
The weight calculated in line 3 is a factor to balance the scale of Self - BLUE and valid set BLEU .
Then the list C initially contains the model m top has a highest weighted score .
Next , we iteratively re-compute the average Self - BLEU between the remaining models in | M ? C| and selected models in C , based on which we select the model has minimum Self - BLEU score into C .
In our experiments , we save around 95 % searching time by using this novel method to achieve the same BLEU score of the Brute Force search .
We will further analyze the effect of Boosted Self - BLEU based Ensemble in section 5.2 .
Experiments And Results
Settings
The implementation of our models is based on Fairseq 1 for En?Zh and EN?
De , and OpenNMT 2 for En?Ja .
All the single models are carried out on 8 NVIDIA V100 GPUs , each of which has 32 GB memory .
We use the Adam optimizer with ?
1 = 0.9 , ? 2 = 0.998 .
The gradient accumulation is used due to the high GPU memory consumption .
The batch size is set to 8192 tokens per GPU and we set the " update-freq " parameter in Fairseq to 2 .
The learning rate is set to 0.0005 for Fairseq and 2.0 for OpenNMT .
We use warmup step = 4000 .
We calculate sacreBLEU 3 score for all experiments which is officially recommended .
Dataset
The statistics of all training data is shown in Table 1 .
For each language pair , the bilingual data is the combination of all parallel data released by WMT21 .
For monolingual data , we select data from News Crawl , Common Crawl and Extended Common Crawl , it is then divided into several parts , each containing 50 M sentences .
For general domain synthetic data , we use all the target monolingual data to generate backtranslation data and a part of source monolingual data ( about 80 to 100 million for different languages ) to get forward translation data .
For the in- domain pseudo- parallel data , we use the entire source monolingual data and bilingual data .
All the test and valid data from previous years are used as in-domain data .
We use the methods described in Sec. 3.1 to filter bilingual and monolingual data .
Pre-processing and Post-processing English and German sentences are segmented by Moses 4 , while Japanese use Mecab 5 for segmentation .
We segment the Chinese sentences with an in-house word segmentation tool .
We apply punctuation normalization in English , German and Chinese data .
Truecasing is applied to English ?
Japanese and English ?
German .
We use byte pair encoding BPE ( Sennrich et al. , 2016 b ) with 32 K operations for all the languages .
For the post-processing , we apply de-truecaseing and de-tokenizing on the English and German translations with the scripts provided in Moses .
For the Chinese translations , we transpose the punctuations to the Chinese format .
English ?
Chinese
The results of En?Zh on newstest 2020 are shown in Table 2 .
For the En?Zh task , filtering out part of sentence pairs containing English characters in Chinese sentences shows a significant improvement in the valid set .
After applying large-scale Back - Translation , we obtain + 2.0 BLEU score on the baseline .
We further gain + 0.62 BLEU score after applying knowledge distillation and + 0.24 BLEU from Forward - Translation .
Surprisingly , we observe that adding more BT data from different
In preliminary experiments , we select the best performing models as our ensemble combinations obtaining + 0.4 BLEU score .
On top of that , even after searching hundreds of models , no better results are obtained .
With BSBE strategies in Sec. 3.6 , a better model combination with less number of models are quickly searched , and we finally achieve 50.94 BLEU score .
Our WMT2021 English ?
Chinese submission achieves a Sacre-BLEU score of 36.9 , which is the highest among all submissions and chrF score of 0.337 .
English ?
Japanese
The results of En? Ja on newstest 2020 are shown in Table 2 .
For the En?Ja task , we filter out the sentence pairs containing Japanese characters in the English side and vice versa .
The Back-Translation and Knowledge Distillation improve the baseline from 35.78 to 36.66 .
Adding more BT data further brings in 0.56 improvements .
The improvement by finetuning is much larger than other directions , which is 5.32 BLEU .
We speculate that this is because there is less bilingual data for English and Japanese than for other languages , and the test results for Japanese are char level BLEU so this direction is more influenced by the in-domain finetuning .
Two In-domain knowledge transfers improve BLEU score from 37.22 to 43.69 .
Normal finetune still provides 0.54 improvements after in- domain knowledge transfer .
Then , we apply advanced finetuning methods to further get 0.19 BLEU improvements .
Our final ensemble result outperforms baseline 9.57 BLEU .
Japanese ?
English
The Ja?En task follows the same training procedure as En?Ja .
From Table 2 , we can observe that Back-Translation can provide 1.11 BLEU improvements from baseline .
Knowledge Distillation and more BT data can improve the BLEU score from 20.82 to 22.11 .
The finetuning improvement is 3.8 which is slightly less than the En? Ja direction but still larger than En?Zh and En?De .
We also apply two -turn in - domain knowledge transfer and further boost the BLEU score to 25.89 .
After normal finetuning , the BLEU score achieves 26. 27 .
The advanced finetuning methods provide a slight improvement on Ja?En .
After ensemble , we achieve 28.24 BLEU in newstest 2020 .
MODEL
English ?
German
The results of En?De on newstest2020 are shown in Table 2 .
After adding back - translation , we improve the BLEU score from 33.28 to 35.28 .
Knowledge Distillation further boosts the BLEU score to 36.58 .
The finetuning further brings in 2.63 improvements .
After injecting the in-domain knowledge into the monolingual corpus , we get another 0.31 BLEU gain .
We apply a post-processing procedure on En?De .
Specifically , we normalize the English quotations to German ones in German hypotheses , which brings in 1.3 BLEU improvements .
Analysis
To verify the effectiveness of our approach , we conduct analytical experiments on model variants , finetune methods , and ensemble strategies in this section .
Effects of Model Architecture
We conduct several experiments to validate the effectiveness of Transformer ( Vaswani et al. , 2017 ) variants we used and list results in Table 3 .
We also investigate the diversity of different variants and the impacts on the model ensemble .
The results is listed in Table 4 and Table 5
Effects of Boosted Self-BLEU based Ensemble
To verify the superiority of our Boosted Self - BLEU based Ensemble ( BSBE ) method , we randomly select 10 models with different architecture and training data .
For our submitted system , we search from over 500 models .
We use a greedy search algorithm ( Deng et al. , 2018 ) as our baseline .
The greedy search greedily selects the best performance model into candidate ensemble models .
If the selected model provides a positive improvement , we keep it in the candidates .
Otherwise , it is added to a temporary model list and still has a weak chance to be reused in the future .
One model from the temporary list can be reused once , after which it is withdrawn definitely .
We compare the results of greedy search , BSBE and Brute Force and list the ensemble model BLEU and the number of searches in Table 6 . Note that n is the number of models , which is 10 here .
For BSBE , we need to get the translation result of every model to calculate the Self - BLEU .
After that , we only need to perform the inference process once .
Effects of Advanced Finetuning
In this section , we describe our experiments on advanced finetuning in the four translation directions .
As shown in
Conclusion
We investigate various novel Transformer based architectures to build robust systems .
Our systems are also built on several popular data augmentation methods such as back - translation , knowledge distillation and iterative in-domain knowledge transfer .
We enhance our system with advanced finetuning approaches , i.e. , target denoising , graduated label smoothing and confidence - aware scheduled sampling .
A boosted Self - BLEU based model ensemble is also employed which plays a key role in our systems .
Our constrained systems achieve 36.9 , 46.9 , 27.8 and 31.3 case-sensitive BLEU scores on English ?
Chinese , English ?
Japanese , Japanese ?
English and English ?
German , respectively .
The BLEU scores of English ?
Chinese , English ?
Japanese and Japanese ?
English are the highest among all submissions , and that of English ?
German is the highest among all constrained submissions .
would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper .
to our training flows to stabilize the training of deep Post- Norm Transformer .
Our experiments have shown that the Post - Norm model has a good diversity compared to the Pre-Norm Model and slightly outperform the Pre-Norm Model .
We will further analyze the model diversity of different variants in Sec. 5.1 .
