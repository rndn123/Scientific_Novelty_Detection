title
Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation
abstract
Is bias amplified when neural machine translation ( NMT ) models are optimized for speed and evaluated on generic test sets using BLEU ?
We investigate architectures and techniques commonly used to speed up decoding in Transformer - based models , such as greedy search , quantization , average attention networks ( AANs ) and shallow decoder models and show their effect on gendered noun translation .
We construct a new gender bias test set , SimpleGEN , based on gendered noun phrases in which there is a single , unambiguous , correct answer .
While we find minimal overall BLEU degradation as we apply speed optimizations , we observe that gendered noun translation performance degrades at a much faster rate .
Introduction
Optimizing machine translation models for production , where it has the most impact on society at large , will invariably include speed- accuracy trade- offs , where accuracy is typically approximated by BLEU scores ( Papineni et al. , 2002 ) on generic test sets .
However , BLEU is notably not sensitive to specific biases such as gender .
Even when speed optimizations are evaluated in shared tasks , they typically use BLEU ( Papineni et al. , 2002 ; Heafield et al. , 2020 ) to approximate quality , thereby missing gender bias .
Furthermore , these biases probably evade detection in shared tasks that focus on quality without a speed incentive ( Guillou et al. , 2016 ) because participants would not typically optimize their systems for speed .
Hence , it is not clear if Neural Machine Translation ( NMT ) speed-accuracy optimizations amplify biases .
This work attempts to shed light on the algorithmic choices made during speed-accuracy optimizations source
That physician is a funny lady !
reference ?Esa m?dica / doctora es una mujer graciosa !
system A ?Ese :::: : m?dico es una dama graciosa !
system B ?Ese :::: : m?dico es una dama divertida !
system C ?Ese :::: : m?dico es una mujer divertida !
system D ?Ese :::: : m?dico es una dama divertida !
Table 1 : Translation of a simple source sentence by 4 different commercial English to Spanish MT systems .
All of these systems fail to consider the token " lady " when translating the occupation -noun , rendering it in with the masculine gender " doctor/m?dico " .
and their impact on gender biases in an NMT system , complementing existing work on data bias .
We explore optimizations choices such as ( i ) search ( changing the beam size in beam search ) ; ( ii ) architecture configurations ( changing the number of encoder and decoder layers ) ; ( iii ) model based speedups ( using Averaged attention networks ( Zhang et al. , 2018 ) ) ; and ( iv ) 8 - bit quantization of a trained model ..
Prominent prior work on gender bias evaluation forces the system to " guess " the gender ( Stanovsky et al. , 2019a ) of certain occupation nouns in the source sentence .
Consider , the English source sentence " That physician is funny . " , containing no information regarding the physician 's gender .
When translating this sentence into Spanish ( where the occupation nouns are explicitly specified for gender ) , an NMT model is forced to guess the gender of the physician and choose between masculine forms , doctor / m? dico or feminine forms doctora/ m?dica .
While investigating bias in these settings is valuable , in this paper , we hope to highlight that the problem is much worse - despite an explicit gender reference in the sentence , NMT systems still generate the wrong gender in translation ( see Table 1 ) , resulting in egregious errors where not only is the gender specification incorrect but the generated sentence also fails in morphological gender agreement .
To focus on these egregious errors , we construct a new data set , SimpleGEN .
In Simple - GEN , all source sentences include an occupation noun ( such as " mechanic " , " nurse " etc. ) and an unambiguous " signal " specifying the gender of the person being referred to by the occupation noun .
For example , we modify the previous example to " That physician is a funny lady " .
We call our dataset " Simple " because it contains all the information needed by a model to produce correctly gendered occupation nouns .
Furthermore , our sentences are short ( up to 12 tokens ) and do not contain complicated syntactic structures .
Ideally , SimpleGEN should obviate the need for an NMT model to incorrectly guess the gender of occupation nouns , but using this dataset we show that gender translation accuracy , particularly in female context sentences ( see Section 2 ) , is negatively impacted by various speed optimizations at a greater rate than a drop in BLEU scores .
A small drop in BLEU can hide a large increase in biased behavior in an NMT system .
Further illustrating how insensitive BLEU is as a metric to such biases .
2 SimpleGEN : A gender bias test set Similar to Stanovsky et al . ( 2019 b ) , our goal is to provide English input to an NMT model and evaluate if it correctly genders occupation -nouns .
We focus on English to Spanish ( En-Es ) and English to German ( En - De ) translation directions as occupation - nouns are explicitly specified for gender in these target languages while English is underspecified for such a morphological phenomenon which forces the model to attend to contextual clues .
Furthermore , these language directions are considered " high- resource " and often cited as exemplars for advancement in NMT .
A key differentiating characterization of our test set is that there is no ambiguity about the gender of the occupation -noun .
We achieve this by using carefully constructed templates such that there is enough contextual evidence to unambiguously specify the gender of the occupation -noun .
Our templates specify a " scaffolding " for sentences with keywords acting as placeholders for values ( see Table 2 ) .
For the occupation keywords such as f-occ-sg and m-occ-sg , we select the occupations for our test set using the U.S Department of Labor statistics of high- demand occupations .
1 A full list of templates , keywords and values is in table A6 .
Using our templates , we generate English source sentences which fall into two categories : ( i ) pro-stereotypical ( pro ) sentences contain either stereotypical male occupations situated in male contexts ( MOMC ) or female occupations in female contexts ( FOFC ) , and ( ii ) anti-stereotypical ( anti ) sentences in which the context gender and occupation gender are mismatched , i.e. male occupations in female context ( MOFC ) and female occupations in male contexts ( FOMC ) .
Note that we use the terms " male context " or " female context " to categorize sentences in which there is an unambiguous signal that the occupation noun refers to a male or female person , respectively .
We generated 1332 pro-stereotypical and anti-stereotypical sentences , 814 in the MOMC and MOFC subgroups and 518 in the FOMC and FOFC subgroups ( we collect more male stereotypical occupations compared to female , which causes this disparity ) .
To evaluate the translations of NMT models on SimpleGEN , we also create an occupation - noun bilingual dictionary , that considers the number and gender as well as synonyms for the occupations .
For example for the En-Es direction , the English occupation term ' physician " , has corresponding entries for its feminine forms in Spanish as " doctora " and " m?dica " and for its masculine forms " doctor " and " m?dico " ( See table A8 for our full dictionary ) .
By design , non-occupation keywords such as f-rel and f-n-sg specify the expected gender of the occupation -noun on the target side , enabling dictionary based correctness verification .
Speeding up NMT
There are several " knobs " that can be tweaked to speed up inference for NMT models .
Setting the beam-size ( bs ) to 1 during beam search is likely the simplest approach to obtain quick speedups .
Lowbit quantization ( INT8 ) is another recent approach which improves decoding speed and reduces the memory footprint of models ( Zafrir et al. , 2019 ; Quinn and Ballesteros , 2018 ) .
For model and architecture based speedups , we focus our attention on Transformer based NMT models which are now the work-horses in NLP and MT ( Vaswani et al. , 2017 ) .
While transformers are faster to train compared to their predecessors , Recurrent Neural Network ( RNN ) encoderdecoders ( Bahdanau et al. , 2014 ; Luong et al. , 2015 ) , transformers suffer from slower decoding speed .
Subsequently , there has been interest in improving the decoding speed of transformers .
Shallow Decoders ( SD ) : Shallow decoder models simply reduce the decoder depth and increase the encoder depth in response to the observation that decoding latency is proportional to the number of decoder layers ( Kim et al. , 2019 ; Miceli Barone et al. , 2017 ; Wang et al. , 2019 ; Kasai et al. , 2020 ) .
Alternatively , one can employ SD models without increasing the encoder layers resulting in smaller ( and faster ) models .
Average Attention Networks ( AAN ) : Average Attention
Networks reduce the quadratic complexity of the decoder attention mechanism to linear time by replacing the decoder-side self-attention with an average-attention operation using a fixed weight for all time-steps ( Zhang et al. , 2018 ) .
This results in a ? 3 - 4x decoding speedup over the standard transformer .
Experimental Setup
Our objective is not to compare the various optimization methods against each other , but rather surface the impact of these algorithmic choices on gender biases .
We treat all the optimization choices described in section 3 as data points available to conduct our analysis .
To this end , we train models with all combinations of optimizations described in section 3 using the Fairseq toolkit ( Ott et al. , 2019 ) .
Our baseline is a standard large transformer with a ( 6 , 6 ) encoder -decoder layer configuration .
For our SD models we use the following encoder-decoder layer configurations { ( 8 , 4 ) , ( 10 , 2 ) , ( 11 , 1 ) } .
We also train smaller shallow decoder ( SSD ) models without increasing the encoder depth { ( 6 , 4 ) , ( 6 , 2 ) , ( 6 , 1 ) } .
For each of these 7 configurations , we train AAN versions .
Next , we save quantized and non-quantized versions for the 14 models , and decode with beam sizes of 1 and 5 .
We repeat our analysis for English to Spanish and English to German directions , using WMT13 En-Es and WMT14 En - De data sets , respectively .
For the En-Es we limited the training data to 4 M sentence pairs ( picked at random without replacement ) to ensure that the training for the two language directions have comparable data sizes .
We apply Byte-Pair Encoding ( BPE ) with 32 k merge operations to the data ( Sennrich et al. , 2016 ) .
We measure decoding times and BLEU scores for the model 's translations using the WMT test sets .
Next , we evaluate each model 's performance on SimpleGEN , specifically calculating the percent of correctly gendered nouns , incorrectly gendered nouns as well as inconclusive results .
Table 3 shows an example of our evaluation protocol for an example source sentences and four possible translations .
We deem the first two as correct even though the second translation incorrectly translates " funny " as " feliz " since we focus on the translation of " physician " only .
The third translation is deemed incorrect because the masculine form " m?dico " is used and the last translation is deemed inconclusive since it is in the plural form .
We average these metrics over 3 trials , each initialized with different random seeds .
We obtained 56 data points for each language direction .
Analysis
Table 4a shows the performance of 6 selected models including a baseline transformer model with 6 encoder and decoder layers .
The first two columns ( time and BLEU ) were computed using the WMT test sets .
The remaining columns report metrics using SimpleGEN .
The algorithmic choices resulting in the highest speed - up , result in a 1.5 % and 4 % relative drop in BLEU for En-Es and En-De , respectively ( compared to the baseline model ) .
The pro-stereotypical ( pro ) column shows the percentage correct gendered translation for sentences where the occupation gender matches the context gender .
As expected the accuracies are relatively high ( 80.9 to 77.7 ) for all the models .
The 4a ) and stacked in Table 4 b ) .
We selected 6 models in both sections to highlight their effect on decoding time , BLEU and the % correctness on gender- bias metrics .
The last row for each section ( and each direction ) , shows the relative % drops in all the metrics between the fastest optimization method and the baseline .
For example , for En- Es the relative % drop of decoding time for Table 4a is calculated as 100 * ( 3662.8 ? 1993.5)/3662.8 .
last row in each section shows the maximum relative drop in each metric .
We find that for the prostereotypical column the maximum relative drop is 1.5 and 6.5 for Spanish and German , respectively , which is similar to the relative change in BLEU scores .
However , we find that the models are able to perform better on MOMC compared to FOFC suggesting biases even within the pro-stereotypical setting .
In the anti-stereotypical ( anti ) column , we observe below-chance accuracies of only 44.2 % and 39.7 % for the two language directions , even from our best model .
Columns FOFC and MOFC , show the difference in performance for sentences in the female context ( FC ) category in the presence of a stereotypical female occupation versus a stereotypical male occupation .
We see a large imbalance in performance in these two columns summarized in ?FC .
Similarly , ?MC summarizes the drop in performance when the model is confronted with stereotypical female occupations in a male context when compared to a male occupation in a male context .
This suggests that the transformer 's handling of grammatical agreement especially in cases where an occupation and contextual gender mismatch could be improved .
The speedups disproportionately affect female context ( FC ) sentences across all categories .
In terms of model choices , we find that AANs deliver moderate speed-ups and minimal BLEU reduction compared to the baseline .
However , AANs suffer the most degradation in terms of gender-bias . ? , ?FC and ?MC are the highest for the ANN model in both language directions .
On the other hand , greedy decoding with the baseline model has the smallest degradation in terms of gender-bias .
While Table 4a reveals the effect of select individual model choices , NMT practitioners , typically " stack " the optimization techniques together for large-scale deployment of NMT systems .
Table 4 b shows that stacking can provide ? 80 ? 81 % relative drop in decoding time .
However , we again see a disturbing trend where large speedups and small BLEU drops are accompanied with large drops in gender test performance .
Again , FC sentences disproportionately suffer large drops in accuracy , particularly in MOFC in the En- De direction , where we see a 53.2 % relative drop between the baseline and the fastest optimization stack .
While tables 4a and 4 b show select models , we illustrate and further confirm our findings using all the data points ( 56 models trained ) using scatter plots shown in fig .
1 . We see that relative % drop in BLEU aligns closely with the relative % drop in gendered translation in the pro-stereotypical setting .
In the case of German , the two trendlines are virtually overlapping .
However , we see a steep drop for the anti-stereotypical settings , suggesting that BLEU scores computed using a typical test set only captures the stereotypical cases and even small reduction in BLEU could result in more instances of biased translations , especially in female context sentences .
Related Work Previous research investigating gender bias in NMT has focused on data bias , ranging from assessment to mitigation .
For example , Stanovsky et al. ( 2019 b ) adapted an evaluation data set for co-reference resolution to measure gender biases in machine translation .
The sentences in this test set were created with ambiguous syntax , thus forcing the NMT model to " guess " the gender of the occupations .
In contrast , there is always an unambiguous signal specifying the occupation - noun 's gender in SimpleGEN .
Similar work in speech-translation also studies contextual hints , but their work uses real-world sentences with complicated syntactic structures and sometimes the contextual hints are across sentence boundaries resulting in genderambiguous sentences ( Bentivogli et al. , 2020 ) .
Zmigrod et al. ( 2019 ) create a counterfactual data-augmentation scheme by converting between masculine and feminine inflected sentences .
Thus , with the additional modified sentences , the augmented data set equally represents both genders .
Vanmassenhove et al. ( 2018 ) , Stafanovi ?s et al. ( 2020 ) and propose a dataannotation scheme in which the NMT model is trained to obey gender-specific tags provided with the source sentence .
While Escud ?
Font and Costa-juss ? ( 2019 ) employ pre-trained wordembeddings which have undergone a " debiasing " process ( Bolukbasi et al. , 2016 ; Zhao et al. , 2018 ) . and Costa-juss ?
and de Jorge ( 2020 ) propose domain-adaptation on a carefully curated data set that " corrects " the model 's misgendering problems .
consider variations involving the amount of parameter - sharing between different language directions in multilingual NMT models .
Conclusion
With the current mainstreaming of machine translation , and its impact on people 's everyday lives , bias mitigation in NMT should extend beyond data modifications and counter bias amplification due to algorithmic choices as well .
We focus on algorithmic choices typically considered in speed-accuracy trade offs during productionization of NMT models .
Our work illustrates that such trade offs , given current algorithmic choice practices , result in significant impact on gender translation , namely amplifying biases .
In the process of this investigation , we construct a new gender translation evaluation set , SimpleGEN , and use it to show that modern NMT architectures struggle to overcome gender biases even when translating source sentences that are syntactically unambiguous and clearly marked for gender .
