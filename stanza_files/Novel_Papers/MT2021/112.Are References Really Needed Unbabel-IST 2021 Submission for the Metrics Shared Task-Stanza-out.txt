title
Are References Really Needed ?
Unbabel-IST 2021 Submission for the Metrics Shared Task
abstract
In this paper , we present the joint contribution of Unbabel and IST to the WMT 2021 Metrics Shared Task .
With this year 's focus on Multidimensional Quality Metric ( MQM ) as the ground -truth human assessment , our aim was to steer COMET towards higher correlations with MQM .
We do so by first pretraining on Direct Assessments and then finetuning on z-normalized MQM scores .
In our experiments we also show that referencefree COMET models are becoming competitive with reference - based models , even outperforming the best COMET model from 2020 on this year 's development data .
Additionally , we present COMETINHO , a light- weight COMET model that is 19x faster on CPU than the original model , while also achieving state - of- theart correlations with MQM .
Finally , in the " QE as a metric " track , we also participated with a QE model trained using the OPENKIWI framework leveraging MQM scores and word-level annotations .
1
Crosslingual Optimized Metric for Evaluation of Translation hosted at : https://github.com/ Unbabel/ COMET
Introduction
In this paper , we present the joint contribution of Unbabel and IST to the WMT 2021 Shared Task on Metrics .
We participated in the segment- level and system - level tracks , as well as the " QE as a Metric " task .
Similar to our participation last year ( Rei et al. , 2020 b ) , most of the models are based on the COMET framework 1 ( Rei et al. , 2020a ) .
In last year 's shared task ( Mathur et al. , 2020 ) , COMET along with other trainable metrics such as PRISM ( Thompson and Post , 2020 ) and BLEURT ( Sellam et al. , 2020 ) showed superior correlations with the Direct Assessments ( DA ) collected for the News Translation Shared Task .
This year , we build on top of the models used last year to take into account that human assessments will be carried out using variants of the Multidimensional Quality Metric ( MQM ) ( Lommel et al. , 2014 ) framework and no longer based on DA ( Graham et al. , 2013 ) .
For this reason , we extended our training dataset to include DA evaluations from WMT ranging 2015 to 2020 , with the exception of en-de and zh-en for which we do not include the 2020 data given that the same is included in the MQM development data ( Freitag et al. , 2021 ) .
Finally , we fine- tuned these new models on the znormalized MQM scores provided for this year 's shared task .
One of the remaining redeeming qualities of automated metrics such as BLEU ( Papineni et al. , 2002 ) is that they are incredibly light -weight .
Despite the higher correlation with human judgement , trainable metrics tend to be slower to run .
In an effort to close this gap we present COMETINHO , a light - weight model based on the COMET framework that replaces the original XLM -R large encoder with MiniLMv2 ( Wang et al. , 2020 ) .
This model is approximately 19x faster at inference time compared to the original COMET model ( Rei et al. , 2020a ) and maintains state - of- the - art correlations with MQM in reference - based evaluations .
For the " QE as a metric " track , we show that reference - free evaluation models can reach surprisingly high correlations with human judgements and are competitive with their corresponding referencebased models .
Last year we also participated with a similar model in the Metrics Shared Task , but here we elaborate in more detail on the primary differences between this model architecture and other COMET models .
Finally , and for the first time , we submit and describe a reference - free model that in addition to learning from MQM scores also makes use of word - level error annotations .
This is possible this year given the shift in evaluation method from DA the main goal of this project is to develop a car for the blind .
Table 1 : Example of word- level OK and BAD tags produced by our OPENKIWI model trained with word-level annotation spans .
This translation received an overall sentence score of 0.2 and the model was able to identify that the words " blind driving " are translation errors giving a good insight on why the sentence score is low .
to MQM .
This model uses the OPENKIWI 2 architecture and its word- level tagging feature to predict OK / BAD word tags along with a sentence - level quality score .
The COMET Framework
For a more comprehensive description of the COMET architecture we direct the reader to the original paper ( Rei et al. , 2020a ) .
Below we will highlight some relevant features that contrast with the COMET reference -free model ( COMET - QE ) .
In COMET we encode segment- level representations using the pretrained , cross-lingual , model XLM - RoBERTa ( Conneau et al. , 2020 ) .
Even though we encode the source , the hypothesis , and the reference ( i.e. the human curated translation of the source ) separately , their embeddings are mapped into a shared feature space .
Subsequently , we obtain combined features using the three embeddings ( s , h , and r , for the source , hypothesis , and reference , respectively ) : h s , h r , |h ? s| , and |h ? r| .
These features , concatenated to r and h and the resulting vector is the input to a feedforward regressor .
Reference-free COMET
The architecture of the COMET model used in the " QE as a metric " task ( COMET - QE ) is very similar to the main COMET model ( Rei et al. , 2020a ) briefly described above and RUSE ( Shimanaka et al. , 2018 ) .
The biggest difference being that in the COMET - QE model the reference is not used and , consequently , the combination of features used as input to the feed -forward regressor are also different from reference - based COMET .
In this case , the combined features are simply : h s and |h ? s| ; the final vector to the feed -foward regressor being the concatenation of the latter features together with h and s .
A schematic representation is shown in Figure 1 . 3 Lightweight COMET : COMETINHO
Our light - weight version of the original COMET model is almost an exact replica in terms of architecture save that we replaced the underlying pre-trained encoder with MiniLMv2 ( Wang et al. , 2020 ) which is a distilled version of XLM -R large ( Conneau et al. , 2020 ) .
This distilled model is made available by HuggingFace Transformers ( Wolf et al. , 2020 ) : nreimers / mMiniLMv2-L6-H384-distilled-from-XLMR -Large Our COMETINHO models are 19x faster on CPU and 14.3x times faster on GPU than COMET models based on XLM -R large .
Also , in terms of disk footprint , these models are 5x smaller 3 .
The OPENKIWI Framework
When using the MQM framework for the calculation of the quality score , human annotators seek to identify and annotate error spans at the wordlevel , as well as the severity of those errors .
We leveraged these word-level annotations using the OPENKIWI framework ( Kepler et al. , 2019 ) , by transforming each word into an OK or BAD tag .
In the OPENKIWI architecture , in contrast with COMET - QE , source and hypothesis are jointly encoded .
A sentence pair representation is then obtained using average pooling over the hypothesis word embeddings and then used as features to a feed-forward regression layer that learns to produce a sentence level score .
At the same time , the word embeddings from the hypothesis are used to predict OK / BAD tags and therefore , the model is trained in a multitask setting ( regression and sequence labelling ) .
Corpora
In this year 's shared task the organisers provided a development set with MQM annotations for the ende and zh-en participating systems on WMT20 ( Freitag et al. , 2021 ) .
Apart from the official development data we used all the Direct Assessments available from previous years .
Multi-dimensional Quality Metric Corpus
In this corpus , for each language pair , each translation was annotated by 3 raters from a pool of 6 .
Following what is a common practice for the DA 's we convert the segment - level scores of each annotator into a z-normalized score and the final translation quality score is an average of the 3 z-scores .
Also , because the sign of these MQM annotations is the opposite of the Direct Assessments we invert the score .
Subsequently we generate a train and test split leaving 20 % of the documents for each language pair for testing .
This results in a total of 11230 en-de training samples and 15600 zh-en training samples , with testsets of 2950 and 4400 samples , respectively .
All results reported in this paper are with respect to the above train and test split .
The documents contained in each split are listed in the Appendix of this paper .
Annotators are not always consistent and the annotations of one annotator might differ from another .
With this in mind , we decided to calculate the Kendall 's Tau correlation between all annotators as a measure of inter-annotator agreement ( Figure 2 ) .
The interannotator Kendall Tau can then be used as a ceiling effect for the developed metrics which ideally should behave as an additional annotator .
For training of the OPENKIWI model described herein we used proprietary MQM data from the customer support domain , covering several industries such as tech industry and travel industry .
This data is composed by 1.1 M ( source , hypothesis ) pairs with corresponding MQM annotations from 38 language pairs mostly out -of-english .
Direct Assessments
Each year , the WMT News Translation shared task organisers collect human judgements in the form of Direct Assessments .
Those assessments are then used in the Metrics task to measure the correlation between metrics and therefore decide which metric works best .
In recent years researchers have been using these annotations to create trainable metrics that regress on these scores ( Shimanaka et al. , 2018 ; Sellam et al. , 2020 ; Rei et al. , 2020a ) .
We follow the same approach and use Direct Assessments ranging from 2015 to 2020 for training .
The collective corpora contain a total of 33 language pairs including low-resource languages such as English -Tamil ( en-ta ) and a total of 795269 tuples with source , hypothesis , reference and direct assessment z-score .
The only exception to this data is that we did not include the en-de and zh-en assessment from 2020 because they overlap with the MQM development data described in section 5.1 .
Segment- level task
The COMET framework is highly flexible and easy to adapt to different types of human judgements ( Rei et al. , 2020a ) .
This year we first pre-trained on the DA collected from 2015 to 2020 except for ende and zh-en as described above .
Like in Glushkova et al. ( 2021 ) model ( COMET - DA ) .
During our experiments we tested two ensembling techniques ; averaging the different model predictions and averaging the parameters from the 5 models .
Those two approaches had similar results but in the end we decided to use the later one for performance .
Subsequently , we fine- tuned each of the 5 models on the MQM data provided as development for another epoch .
As before , we performed weight averaging to obtain an ensemble of those models ( COMET - MQM ) .
In both the pre-training and finetuning we only perform 1 training epoch in order to ensure that the final models are able to generalise to many language pairs and do not overfit to the News domain .
This is especially important since the MQM dataset only contains en-de and zh-en .
For COMETINHO , as previously mentioned , we used the distilled version of XLM -R ( MiniLMv2 ) , available through Hugging Face , and we followed the same training recipe where we pre-train the model using DA 's for 1 epoch and then we adapt the model to the MQM data for another epoch .
System- level task
For the System- level task we compute the systemlevel score for each system by averaging the segment - level scores obtained .
This follows the same approach used to compute system-level scores based on segment - level human annotations such as DA 's and MQM which means that a met-ric that achieves strong segment - level correlation should also achieve strong system-level performances .
QE as a Metric Task
We trained a reference -free model ( COMET - QE ) in the same way we did for reference - based COMET models described in section 6 .
As described in section 2.1 , the primary difference between the two models is the inclusion or exclusion of the source as input .
Experimental Results
Segment- level task Reference - based segment- level correlations on the en-de and zh-en testsets are shown in Table 2 .
We used both Pearson and Kendall Tau correlation metrics to evaluate our models .
As baselines we used lexical metrics such as CHRF ( Popovi ? , 2015 ) and BLEU ( Papineni et al. , 2002 ) , an embedding - based metric BERTSCORE ( Zhang et al. , 2020 ) and three trainable -metrics ; BLEURT ( Sellam et al. , 2020 ) , PRISM ( Thompson and Post , 2020 ) and COMET - DA ( 2020 ) ( Rei et al. , 2020 b ) .
The fact that the COMET - DA ( 2021 ) gives higher correlations than the COMET - DA ( 2020 ) shows that adding more training data and combining checkpoints trained on different seeds already provides a boost in performance .
However , fine - tuning on the MQM development data was the most significant addition to previous work : the COMET - MQM ( 2021 ) model increased on average more than 0.1 Pearson correlation .
This improvement is consistent with regard to the two COMETINHO models ( with COMETINHO - MQM having notably higher correlations than COMETINHO - DA ) .
Nevertheless , the fact that COMETINHO - DA has competitive or state - of - the - art performance with all the other metrics such as BLEURT , PRISM , and BERTSCORE , while also being much faster , presents an ideal opportunity for future work to investigate the incorporation of trainable metrics into the training objectives of MT systems .
For reference - free metrics , the fine-tuning on the MQM data , on average , gave a boost in performance ( the only exception being the Pearson correlation for the en-de where COMET - QE - DA has a slightly higher correlation than COMET -QE - MQM ) .
Overall , it is somewhat surprising that COMET -QE -* ( 2021 ) and COMET -* ( 2021 ) show relatively comparable correlations , suggesting that using the reference as input for MT evaluation might be less useful than expected and could feasibly become redundant .
This surprising result was also reported by Kocmi et al . ( 2021 ) and is especially important since curating reference sentences is usually costly and time consuming and can introduce undesired bias in the evaluation .
Finally , the OPENKIWI model has competitive correlations when looking to other trainable metrics and to COMET models that were not fine-tuned on the MQM development data .
This add further weight to the suggestion above that references might not add substantial value to MT evaluation .
Its performance is even more surprising when considering the fact that this model was train with data from a completely different domain .
It is worth highlighting that the Kendall's Tau correlations for all models ( with exception of the two reference - based COMETINHO models ) are in the range obtained for correlations between different annotators , for en-de , Figure 1 .
This further validates the value of our models .
System-level task System-level results are presented in Table 3 where we report a Kendall Tau correlation defined as follows : ? = Concordant ? Discordant Concordant + Discordant ( 1 ) where Concordant defined as the number of times a metric agrees with humans that a given system x is better than a given system y and Discordant is the opposite .
These decisions are the computed for all combinations of systems in the testset .
Due to the low number of systems and the relative proximity of the ground - truth MQM system scores we also compare metrics on their ability to distinguish human references from MT outputs .
With reference to table 7 in the appendix we note that , for zh-en , all 8 MT systems demonstrate comparable performance but that there is a clear separation of human translations .
For that reason Table 3 also presents the Kendall Tau correlations considering only " Human " systems against MT systems where we can observe that reference -free metrics achieve better performance .
This results confirms the finding from last year 's shared task ( Mathur et al. , 2020 ) where COMET - QE was highlighted as being the only metric able to differentiate human translations from MT .
Related work Classic n-gram matching MT evaluation metrics such as BLEU ( Papineni et al. , 2002 ) have been adopted by the MT community as a primary form of MT evaluation , yet , in the recent years of the WMT Metrics shared task ( Bojar et al. , 2017 ; Ma et al. , 2018 Ma et al. , , 2019
Mathur et al. , 2020 ) these classic metrics have been outperformed first by embeddingbased alternatives and more recently by trainable metrics based on pre-trained models .
With the rise of word embeddings ( Pennington et al. , 2014 ; Peters et al. , 2018 ; Devlin et al. , 2019 ) , metrics such as BLEU2VEC ( T?ttar and Fishel , 2017 ) and MEANT 2.0 ( Lo , 2017 ) replaced the typical word / n- gram matching by fuzzy matches based on distributional word representations .
These metrics appeared for the first time at the WMT Metrics task in 2017 with MEANT 2.0- SRL achieving the highest results at segment-level .
In 2018 and 2019 YISI - 1 ( Lo , 2019 ) , a successor of MEANT 2.0 ( Lo , 2017 ) , was among the winners of the WMT Metrics task .
YISI - 1 ( Lo , 2019 ) mostly takes advantage of BERT embeddings ( Devlin et al. , 2019 ) to create soft alignments between hypothesis and reference .
Trainable metrics started as simple regressions based on lexical features ( e.g BLEND ( Ma et al. , 2017 ) ) but nowadays these metrics also use embeddings to extract features that are then used to regress on quality assessments .
The first of such metrics were RUSE ( Shimanaka et al. , 2018 ) and ESIM ( Mathur et al. , 2019 ) which were based on RNN encoders and worked mostly for English .
In 2020 , BLEURT ( Sellam et al. , 2020 ) and COMET ( Rei et al. , 2020a ) were proposed .
Both metrics used pre-trained transformer based encoders to extract sentence - level features that are then passed to a regression model ; the difference is that COMET also extracts features for the source segment which was something overlooked by predecessor metrics .
In the 2020 Metrics Shared task both COMET and BLEURT achieved some of the highest correlations with human judgements and shared the podium with PRISM ( Thompson and Post , 2020 ) 11 Conclusions
In this paper we present the Unbabel - IST 's contribution to the WMT 2021 Metrics shared task which for the first time , introduced evaluation using MQM .
Our specific contributions include ; the fine-tuning of Direct Assessment based models on MQM data which yields impressive gains on the described test sets and a new , lightweight COMET model which achieves comparable performance to its predecessors .
Such a light model can provide interesting opportunities for future work into the incorporation of modern metrics into MT training .
Finally , but perhaps our most important contributions ; we further validate the observations in ( Kocmi et al. , 2021 ) that QE as a metric is becoming competitive as an alternative to reference - based evaluation , and , we show that a word- level QE system can be successfully trained on MQM annotations and be competitive with current trainable metrics while providing some intuition about " what " is wrong with a specific translation .
A Appendix A.1 COMET Hyper-Parameters
In Table 5 is an excerpt of the training configuration used for training the COMET - DA model and Table 5 for the COMET -QE-DA .
Then these models are finetuned for 1 extra epoch with same hyperparameters except the learning_rate that is decreased to 1.0e ? 05 and the nr_frozen_epochs which we increase to 1 to completely freeze the encoder model .
A.2 OPENKIWI Hyper-Parameters
The hyperparameters used for the OpenKiwi model are expressed in Figure 1 : 1 Figure 1 : The COMET - QE model follows the dual encoder architecture proposed in RUSE ( Shimanaka et al. , 2018 ) but replacing the reference translation with the source sentence .
Figure 2 : 2 Figure2 : Kendall Tau Correlations between the en-de annotators used to develop the shared task development set( Freitag et al. , 2021 ) .
Table 2 : 2 Segment-level correlations on the en-de and zh-en testset .
we trained 5 models for 1 epoch each using 5 different seeds and created an ensembled
Table 3 : 3 System-level Kendall's Tau ( ? ) correlations for all system combinations ( on the left ) and Human vs MT ( on the right ) .
All systems Human vs MT en-de en-zh en-de en-zh N? Comparisons 45 45 21 16 Kendall Avg Kendall Avg BLEU 0.378 0.311 0.345 0.095 0.077 0.086 Baselines CHRF BERTSCORE ( F1 ) PRISM 0.444 0.422 0.433 0.143 0.000 0.072 0.356 0.356 0.356 0.143 0.000 0.072 0.444 0.422 0.433 0.143 0.077 0.110 COMET -DA ( 2020 ) 0.822 0.533 0.678 0.714 0.231 0.473 Ref. based COMET -DA ( 2021 ) COMET -MQM ( 2021 ) COMETINHO-DA COMETINHO -MQM 0.844 0.489 0.667 0.761 0.231 0.496 0.867 0.778 0.823 0.762 0.875 0.819 0.533 0.378 0.456 0.238 0.000 0.119 0.355 0.311 0.333 0.095 0.000 0.048 Ref. Free COMET -QE-DA ( 2021 ) COMET -QE-MQM ( 2021 ) 0.933 0.800 0.867 1.000 1.000 1.000 0.778 0.778 0.778 0.667 0.938 0.803 OPENKIWI 0.822 0.733 0.778 0.762 0.769 0.766
Table 4 : 4 Table 4 and follows the configurations proposed in the sample file of the github repository 4 . Hyperparameters for OPENKIWI MQM model 4 https://github.com/Unbabel/OpenKiwi/ blob/master/config/xlmroberta.yaml nr_frozen_epochs 0.3 keep_embeddings_frozen
True optimizer AdamW encoder_learning_rate 1.0e -05 learning_rate 3.1e-05 layerwise_decay 0.95 encoder XLM-RoBERTa pretrained _model xlm-roberta- large pool avg layer mix dropout 0.15 batch_size 4 gradient_accumulation_steps 4 hidden_sizes [ 3072 , 1024 ]
System epochs 1 batch_size 2 Encoder hidden_size 1024 Decoder bottleneck_size 1024 dropout 0.05 hidden_size 1024 Optimizer class_name adam encoder_learning _rate 0.0001 learning_rate_decay 1.0 learning _rate_decay_start 0 learning_rate 0.0001
Trainer training_steps 2180 early_stop_patience 10 validation_steps 0.5 gradient_accumulation_steps 4 gradient_max_norm 1.0
Table 5 : 5 Hyper-parameters for fine-tuning Referencebased COMET model on Direct Assessments .
nr_frozen_epochs 0.3 keep_embeddings_frozen
True optimizer AdamW encoder_learning_rate 1.0e -05 learning_rate 3.1e-05 layerwise_decay 0.95 encoder XLM-RoBERTa pretrained _model xlm-roberta- large pool avg layer mix dropout 0.15 batch_size 4 gradient_accumulation_steps 4 hidden_sizes [ 2048 , 1024 ] epochs 1
Table 6 : 6 Hyper-parameters for fine-tuning Referencefree COMET model on Direct Assessments .
en-de zh-en System MQM System MQM Human-B.0 0.794 Human-A.0 3.114 Human-A.0 0.933 Human-B.0 3.149 Human-P.0 1.547 Huoshan_Translate.919 5.077 Tohoku-AIP-NTT.890 2.043 Tencent_Translation .1249 5.163 OPPO.1535 2.284 OPPO.1422 5.309 Tencent_Translation .1520 2.333 THUNLP.1498 5.389 Online-B.1590 2.516 DeepMind.381 5.442 eTranslation .737 2.530 WeChat_AI.1525 5.469 Huoshan_Translate .832 2.600 DiDi_NLP.401 5.484 Online-A.1574 3.189 Online-B.1605 5.512
Table 7 : 7 System-level Ranking and corresponding MQM scores for the test split described in section 5.1
OpenKiwi hosted at : https://github.com/ Unbabel/ OpenKiwi
Contrastive inference times were tested using a 2.3 GHz Intel Core i5 for CPU , and using a Nvidia T4 for GPU .
