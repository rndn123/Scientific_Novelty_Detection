title
Modeling Task - Aware MIMO Cardinality for Efficient Multilingual Neural Machine Translation
abstract
Neural machine translation has achieved great success in bilingual settings , as well as in multilingual settings .
With the increase of the number of languages , multilingual systems tend to underperform their bilingual counterparts .
Model capacity has been found crucial for massively multilingual NMT to support language pairs with varying typological characteristics .
Previous work increases the modeling capacity by deepening or widening the Transformer .
However , modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective than going deeper or wider when increasing capacity .
In this paper , we propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality .
Unlike previous work which feeds the same input to several transformations and merges their outputs into one , we present a Multi-Input-Multi-Output ( MIMO ) architecture that allows each transformation of the block to have its own input .
We also present a task - aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions .
Our model surpasses previous work and establishes a new state - of - the - art on the large scale OPUS - 100 corpus while being 1.31 times as fast .
Introduction Multilingual translation between multiple language pairs with a single model ( Firat et al. , 2016a ; Johnson et al. , 2017 ; Aharoni et al. , 2019 ) has some advantages compared to bilingual systems ( Bahdanau et al. , 2015 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ; Barrault et al. , 2020 ) , e.g. , easy deployment , enabling transfer learning across languages and zero-shot translation .
* Corresponding author .
Despite their advantages , multilingual systems tend to underperform their bilingual counterparts as the number of languages increases ( Johnson et al. , 2017 ; Aharoni et al. , 2019 ) .
This is due to the fact that multilingual NMT must distribute its modeling capacity over different translation directions .
show that the model capacity is crucial for massively multilingual NMT to support language pairs with varying typological characteristics , and propose to increase the modeling capacity by deepening the Transformer .
However , compared to going deeper or wider , modeling cardinality based on aggregating a set of transformations with the same topology has been proven more effective when we increase the model capacity ( Xie et al. , 2017 ) .
In this paper , we efficiently increase the capacity of the multilingual NMT model by increasing the cardinality , i.e. stacking sub-layers that aggregate a set of transformations with the same topology .
Our main contributions are as follows : ?
We propose to efficiently increase the capacity of the multilingual NMT model by increasing cardinality , and present a novel MIMO design that allows transformations in the subsequent layer to take different outputs from the current layer as their inputs , unlike previous studies ( Xie et al. , 2017 ; Yan et al. , 2020 ) which feed the same input to several transformations and merge their outputs into one ; ?
We propose to learn a task - aware attention mechanism for the MIMO transformation , allowing the model to weigh different transformations of the set differently for specific translation directions ; ?
In our experiments on the OPUS - 100 corpus , our approach outperforms previous work and achieves a new state - of - the - art while being 1.31 times as fast .
Preliminaries
Zhang et al. ( 2020 ) overcome the capacity bottleneck of multilingual NMT via deepening NMT architectures .
Xie et al. ( 2017 ) present a highly modularized network architecture for image classification .
The network is constructed by repeating a building block that aggregates a set of transformations with the same topology .
For a given input i , the block adopts n networks of the same topology trans to process i and merges their outputs into the final output o of the layer : o = n k=1 trans k ( i ) ( 1 )
This design strategy exposes a new dimension , namely " cardinality " ( the size of the set of transformations ) , as an essential factor in addition to the dimensions of depth and width .
Xie et al. ( 2017 ) empirically show that increasing cardinality is more effective than going deeper or wider when we increase the capacity to improve classification accuracy .
Yan et al . ( 2020 ) present a multi-unit Transformer to efficiently improve the translation perfor - mance by increasing cardinality instead of depth .
However , their work implements stacks of input ?
performing multiple transformations ? merging blocks ( as illustrated in Figure 1 ( a ) ) , is developed for bilingual sentence - level transformation , and requires the additional design of a biasing module and sequential dependency that guide and encourage complementariness among different units .
By contrast , our work aims at efficiently increasing the capacity for multilingual translation , proposes the MIMO transformation ( Figure 1 ( c ) ) between stacked blocks , and naturally uses the translation task in attention form to guide individual transformations of the set to learn different representations for different translation directions .
3 Our Approach
Multi-Input-Multi-Output ( MIMO ) Transformation
In contrast to previous approaches ( Xie et al. , 2017 ; Yan et al. , 2020 ) that follow a stack of transformation -merging procedures ( Figure 1 ( a ) ) to increase cardinality , in our approach we allow our set of transformations to take different inputs .
Compared to using the same input , this may encourage transformations to learn complementary representations .
Furthermore , merging the outputs of different transformations into one is likely to incur information loss .
This is avoided in our approach .
We employ a MIMO transformation between stacked layers ( Figure 1 ( c ) ) to enable each transformation of the block to selectively learn to operate on its own unique input .
Specifically , we keep n outputs of the set of transformations to produce multiple inputs for the next layer instead of merging them into one .
The input i j k to the kth transformation of the jth layer trans j k is a weighted accumulation of the outputs o j?1 of the layer j ?
1 . i j k = n m=1 p j m * o j?1 m ( 2 ) where p j m are softmax - normalized learnable parameters to model translation task - aware attention for multilingual NMT described in Section 3.2 .
o j k is produced by trans j k with i j k as its input : o j k = trans j k ( i j k ) ( 3 )
In the case of a Transformer for multilingual NMT , trans j k can be either the multi-head attention or the feed -forward neural network .
We adopt a one-to-many transformation ( Figure 1 ( b ) ) for the self-attention layer in the first encoder / decoder layer to project one input from the embedding layer to multiple inputs to subsequent layers , and perform a many - to - one transformation ( Figure 1 ( d ) ) with the outputs of the feed -forward layer of the last decoder layer to build a single input for the classifier .
Task - Aware Attention
Rather than separating the multilingual NMT model into 2 parts : 1 ) the shared part for all language pairs trained on the full dataset ; 2 ) the language isolated part which will only be activated in the corresponding translation task and trained on the part of the whole dataset specifically for the language , we compute all transformations of each block regardless of the translation task , thus all model parameters can utilize and benefit from the whole training set .
At the same time , we introduce a task - aware attention mechanism to utilize different transformations of the block differently for specific translation directions .
Specifically , we learn an embedding v for each translation direction ( i.e. , to X ( e.g. , en , zh , de ) ) for each transformation to weightedly aggregate multiple outputs of the block below .
v is first normalized into a probability p : p = softmax ( v ) ( 4 ) Next , p is used in Equation 2 for weighted aggregation .
p is expected to assign a higher weight to corresponding transformations of the block which are more important for the translation direction .
Discussion
Increasing model capacity via increasing cardinality is more efficient than deepening a model or widening it ( Xie et al. , 2017 ; Yan et al. , 2020 ) .
Compared to widening a model , increasing cardinality removes connections between hidden units and reduces both parameters and computation .
Compared to deepening a model , increasing cardinality allows to parallelize the computation of all transformations of a set , accelerating both training and decoding .
Experiments
Settings
We conducted our experiments on the challenging massively many - to-many translation task on the OPUS - 100 corpus ( Tiedemann , 2012 ; Aharoni et al. , 2019 ; . We followed for experiment settings .
We implemented our approaches based on the Neutron implementation ( Xu and Liu , 2019 ) of the Transformer translation model .
Parameters were initialized under the Lipschitz constraint ( Xu et al. , 2020 ) .
We adopted BLEU ( Papineni et al. , 2002 ) for translation evaluation with the SacreBLEU toolkit ( Post , 2018 ) .
1 to , average BLEU over 4 selected typologically different target languages ( de , zh , br , te ) BLEU 4 , and average BLEU for zero-shot translation BLEU zero .
Main Results
For fair comparison , we use a 6 - layer model where each attention / FFN block contains 4 transformations , which leads to a similar number of parameters compared to the 24 - layer model of .
Results are shown in Table 1 .
Table 1 shows that our approach achieves better performance in all evaluations while being 1.31 times as fast .
Ablation Study
We study removing MIMO transformations and task - aware attention .
Results are shown in Table 2 .
Table 2 verifies that both mechanisms contribute to the performance .
We also examine different combinations of depth and cardinality .
Results are shown in Table 3 .
Table 3 shows that using 6 layers with 4 transformations in each block leads to the best perfor - Main en de fr ar zh ru 1 rw sv pt he ja sh 2 yi da it mt ko lt 3 gd nn ca fa th sr 4 de nb es ga vi mk 5 xh no mt yo bn lv mance .
Task - Aware Attention Weight Analysis
To verify whether task - aware attention learns to aggregate similar languages together , we extract the learned task - aware attention probabilities , flatten them into vectors , and select the languages with the top - 5 cosine similarity .
Results for several languages are shown in Table 4 .
Table 4 shows that close languages are aggregated together .
Related Work Multilingual NMT includes one- to- many ( Dong et al. , 2015 ) , many - to-many ( Firat et al. , 2016a ) and zero-shot ( Firat et al. , 2016 b ) scenarios .
A simple solution is to insert a target language token at the beginning of the input sentence ( Johnson et al. , 2017 ) .
Multilingual NMT has to handle different languages in one joint representation space , neglecting their linguistic diversity , especially for massively multilingual NMT ( Aharoni et al. , 2019 ; Freitag and Firat , 2020 ) .
Most studies focus on how to mitigate this representation bottleneck ( Zoph and Knight , 2016 ; Blackwood et al. , 2018 ; Wang et al. , 2018 ; Platanios et al. , 2018 ; Wang et al. , 2019a ; Tan et al. , 2019 b ; Wang et al. , 2019 b ; Tan et al. , 2019a ; Zhu et al. , 2020 ; Lyu et al. , 2020 ) .
There are also studies on the trade- off between shared and language -specific parameters Zhang et al. , 2021 ) , on the training of multilingual NMT ( Al - Shedivat and Parikh , 2019 ; Siddhant et al. , 2020 ; Wang et al. , 2020 b , a ) , and on analyzing translations from multilingual NMT ( Lakew et al. , 2018 ) or the trained model ( Kudugunta et al. , 2019 ; Oncevay et al. , 2020 ) .
Transferring a pre-trained multilingual NMT model can help improve the performance of downstream language pairs ( Kim et al. , 2019 ; Lin et al. , 2020 ) , especially for low-resource scenarios ( Dabre et al. , 2019 ) .
Multilingual data also has been proven useful for unsupervised NMT ( Sen et al. , 2019 ; Sun et al. , 2020 ) .
Conclusion
We propose to efficiently increase the capacity for multilingual NMT by increasing the cardinality .
We present a MIMO architecture that allows each transformation of the block to have its own input .
We also present a task - aware attention mechanism to learn to selectively utilize individual transformations from a set of transformations for different translation directions .
Our model surpasses previous work and establishes a new state - of - the - art on the large scale OPUS - 100 corpus while being 1.31 times as fast .
Figure 2 : 2 Figure 2 : The " Trans " unit .
