title
Huawei AARC 's Submissions to the WMT21 Biomedical Translation Task : Domain Adaption from a Practical Perspective
abstract
This paper describes Huawei Artificial Intelligence Application Research Center 's neural machine translation systems and submissions to the WMT21 biomedical translation shared task .
Four of the submissions achieve state - of- the - art BLEU scores based on the official - released automatic evaluation results ( EN? FR , EN ?IT and ZH?EN ) .
We perform experiments to unveil the practical insights of the involved domain adaptation techniques , including finetuning order , terminology dictionaries , and ensemble decoding .
Issues associated with overfitting and under-translation are also discussed .
Introduction General - purpose machine translation systems have limited capability in addressing domain-specific tasks ( Koehn and Knowles , 2017 ) , for example , the WMT biomedical translation shared task , due to the low availability for high-quality in - domain data .
In our WMT20 submission , various domain adaption technologies ( Bawden et al. , 2019 ( Bawden et al. , , 2020 have been applied including practical approaches finetuning on general - purpose models , back - translation ( Sennrich et al. , 2016 ) and leveraging in- domain dictionaries ( Peng et al. , 2020 b ) .
Despite achieving state- of- the- art ( SOTA ) BLEU scores for most of the submissions , few efforts were put in place to disclose the practical insights associated with these techniques .
This year , the Artificial Intelligence Application Research Center ( AARC ) participate in the WMT21 biomedical translation task for eight language directions between English and other four languages ( French , German , Italian , and Chinese ) .
The baseline model is an in- house general - purpose NMT model built upon the transformer - big architecture ( Vaswani et al. , 2017 ) .
Apart from presenting an overview of the proposed biomedical Neural * Co-first authors .
Machine Translation ( NMT ) system , we investigate the practical insights of the involved domain adaptation techniques , including finetuning order , terminology dictionaries , and ensemble decoding .
Issues associated with overfitting to in- domain data and under-translation are also discussed .
The Data
In this section we detail the bilingual and monolingual data used in this shared task ( Table 1 ) .
Bilingual Data In-domain bilingual data
In all directions , we use the in- domain data ( IND ) provided by the shared task organizers to finetune the base model .
1 The IND data consists of WMT - released bitexts from Pubmed , UFAL 2 and Medline .
3
We notice that the official release of IND data suffers from issues of misalignment between source and target sentences , and missing target sentences .
The translation of a source sentence may be misplaced in a different line or even appeared in multiple lines on the target side .
Moreover , a source sentence may have not been translated into in a target sentence .
A data processing pipeline is developed to address the issues mentioned above ( depicted in 3.4 ) .
The test data is the official release of the WMT19 shared task .
Augmented Bilingual Data
We collect indomain data from TAUS 4 for the English - French , English-Italian and English - Chinese language pairs ( depicted in Master 's and Doctoral Dissertations , we align the data on the sentence level by using a model proposed by Ac ?arc ? ic ?ek et al . ( 2020 ) .
This is done by finetuning a RoBERTa ( Liu et al. , 2019 ) filter model on the TAUS dataset and selecting the source- target sentence pairs above a normalized log-probability threshold of 90 % .
General-domain bilingual data
We observe that finetuning the base model with IND data alone may incur sub-optimal BLEU scores .
A conjecture is that the test data has a different distribution to that of the IND data .
We present a case to show that finetuning the base model on a mixture of general domain data ( OOD ) and IND data can produce minor improvements ( shown in 4.2 ) .
Monolingual Data
A batch of monolingual Medline data in English ( IND - BT . ) dated before July 2018 has been collected and back - translated for data augmentation .
The official released IND data from WMT is also back -translated .
The models used for backtranslation are from our last year 's competition ( Peng et al. , 2020 b ) .
The Approaches
The proposed systems are finetuned using the following methods .
All models are trained on one Tesla V100 GPU , taking approximately 8- 20 hours depending on the volumes of data involved .
Leveraging In-domain Dictionary
Leveraging domain-specific dictionaries is a viable solution for domain adaptation in NMT ( Peng et al. , 2020 a , b ) to enhance IND data coverage .
We collect lexicons from SNOMED - CT 5 , DOPPS 6 , WFOT 7 and generate a terminology dictionary which is subsequently attached to the end of training data .
Terminology is further entended to cover COVID -19 related terms obtained from Neulab .
8
Ensemble Ensembling methods is a machine learning technique that aggregates several base models to generate one optimal predictive model ( Garmash and Monz , 2016 ) .
We choose the top two models to ensemble in an attempt to produce a more general NMT model .
Architecture
To train the in- domain NMT model , we choose the in-house NMT system trained on general domain data as a baseline built upon the transformer - big architecture .
LazyAdam optimizer is used during the training phase with a learning rate of 1e ?5 and a warm - up period of 16,000 steps .
The dropout ratio is set to 0.1 , and the batch size for training and validation is 6,144 and 32 tokens , respectively .
The width of the beam search is 4 .
Early stopping is applied to the training .
Data Processing Several pre-processing techniques are introduced to ensure the quality of the data .
?
First , we perform punctuation normalization to standardize their formats using Moses library ( Koehn et al. , 2007 ) . ?
Then we carry out a primary data cleaning process to remove nonstandard sentences , including those with special characters , weblinks , extra spaces , and other bad cases .
?
According to the length of the sentence after segmentation and the proportion of rare words , we remove bitexts with more rare words in the sentences .
We further clean the data by skipping those sentence pairs with more than 100 subwords or less than one subword .
The bitexts with a source and target sentence length ratio of more than 2.5 are excluded .
A language detection tool 9 is used to filter out bitexts with abnormal language patterns , i.e. , sentences with undesirable langid . ?
An alignment model trained by fast-align ( Dyer et al. , 2013 ) 10 is used to score the corpus to remove misaligned parallel sentences .
After decoding , post- processing is performed to detokenize subwords and remove undesirable spaces between special characters and numbers , i.e. , converting " rs = 0.9148 " into " rs=0.9148 " .
Experimental Results and Analysis
The base systems are trained with OOD data and finetuned using IND data enhanced with monolingual data to produce the submitted results .
We 9 https://github.com/aboSamoor/polyglot 10 https://github.com/clab/fast align extract the OK - aligned data from the last two years ( WMT19 and WMT20 ) and produce the test data to evaluate the NMT models .
The BLEU scores are calculated using the MTEVAL script from Moses ( Koehn et al. , 2007 ) .
Results are shown in Table 2 .
The final two rows demonstrate the results of our submissions this year and the best official records released by the organizers .
Finetuning Order Does Matter
We identify the order of training is crucial in the experiment .
We perform the experiment under the following three training strategies : 1 . Strategy 1 ( S1 ) : the baseline is finetuned on the back - translation ( BT ) pseudo parallel corpus , followed by another finetuning using IND data .
2 . Strategy 2 ( S2 ) : the baseline is finetuned using the IND data , followed by another finetuning using the BT data .
Strategy 3 ( S3 ) : the baseline is finetuned using a mixture of BT and IND data .
Table 5 presents the results of this comparative study for French ?
English translation direction .
It can be observed that finetuning order generates significantly different BLEU scores , with Strategy 1 achieving a BLEU score + 8.89 higher than that from Strategy 2 .
We follow the training strategy 1 in WMT21 shared task to this end .
OOD Data Mixed Finetuning
We observe that finetuning the base model with IND data alone ( particularly with a limited amount of IND data ) may result in sub-optimal BLEU scores .
This may indicate overfitting to the training data , which has a different distribution to the test data .
We perform a series of experiments to disclose this issue .
As shown in Tables 6 and 7 , finetuning with a mixture of OOD and IND data generates minor improvements .
Interestingly , the experiment results are sensitive to the amount of OOD data involved .
Future work is planned to look into this issue in detail .
The Effect of Terminology Dictionaries
In this section , we perform an ablation study to show the effectiveness of terminology dictionaries .
The IND dictionaries are appended to bitexts as a part of the corpus to train NMT models .
Ensemble Decoding Ensemble decoding is applied to improve the generality of the NMT model by averaging the logarithmic probabilities of a decoded token .
It can be observed from Table 4 that ensemble decoding is marginally effective compared to well - learned NMT models .
This finding is consistent with that obtained from .
Under-translation with Overfitting Under-translation occurs when the NMT model fails to decode a portion of the input sentence .
One of Chinese ?
English models under-translates a particular sentence of the WMT21 test data .
For example , as shown in Table 8 , " ? ? " of the input has been left untranslated .
After increasing the width of the beam search , under-translation can be avoided .
In our opinion , under-translation may be caused by noisy IND data , in which the learned self-attentions are not differentiable during decoding .
By ensembling the affected model with the baseline , we successfully rectify the problem .
sentence example input
The disease duration ranged from 2 weeks to 60 months ( median , 4 months ) , and the affected segment was C All the patients were followed up 3 to 42 months ( median , 12 months ) .
prediction ?2 ?
input
The median age of the 30 patients was 56.5 ( 28- 80 ) years old , among them , 25 patients were primary plasma cell leukemia , and 5 patients were secondary plasma cell leukemia .
prediction 30?56.5 ( 28 input ?10 ? OS ? ?100%?60.6 % ( P=0.0007 ) ?
prediction
The 10 - year os rate was 100 % and 60.6 % respectively ( p=0.0007 ) .
Conclusion
This paper depicts Huawei 's neural machine translation systems and submissions to the WMT21 biomedical shared task .
We have achieved stateof - the- art BLEU scores for four of eight language pairs ( EN? FR , EN ?IT and ZH?EN ) based on the official - released results .
We also explore practical issues for the involved domain adaptation techniques , including the effects of finetuning order , terminology dictionaries , and ensemble decoding on enhancing the performances of cross-domain NMT .
We have discussed issues associated with overfitting and under-translation .
Table 1 1 as IND - Aug. ) to address the in-domain data scarcity issue .
For English - Chinese data , after collecting a portion of abstracts of China
