title
HW - TSC 's Submissions to the WMT21 Biomedical Translation Task
abstract
This paper describes the submission of Huawei Translation Service Center ( HW - TSC ) to WMT21 biomedical translation task in two language pairs : Chinese ?
English and German ?
English ( Our registered team name is HuaweiTSC ) .
Technical details are introduced in this paper , including model framework , data pre-processing method and model enhancement strategies .
In addition , using the wmt20 OK - aligned biomedical test set , we compare and analyze system performances under different strategies .
On WMT21 biomedical translation task , Our systems in English ?
Chinese and English ?
German directions get the highest BLEU scores among all submissions according to the official evaluation results .
Introduction
We have witnessed great progress made by neural machine translations ( Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) in recent years .
However , domain adaptation remains to be a tough issue .
As noted by Koehn and Knowles ( Koehn and Knowles , 2017 ) , translations by NMT systems in out-ofdomain scenarios are relatively poor , and highquality data in specific domains are difficult to obtain , which pose great challenges to certain translation tasks ( e.g. biomedical translation ) .
To address the domain adaptation issue , on one hand , we leverage data diversification ( Nguyen et al. , 2020 ) , forward translation and back translation ( Sennrich et al. , 2016a ; Edunov et al. , 2018 ) to generate synthetic in- domain corpora .
On the other hand , fine-tuning ( Sun et al. , 2019 ) and ensemble ( Freitag et al. , 2017 ; Li et al. , 2019 ) are used to further enhance system performances in the biomedical domain .
We introduce our data strategy in section 2 , and model architectures as well as model enhancement techniques in section 3 . Section 4 presents experimental results of both language pairs on the wmt20 OK - aligned biomedical test set .
Section 5 is a conclusion of our work .
Dataset 2.1 Data Source
Our baseline model is trained with out-of- domain WMT21 news data .
The sizes of bilingual and monolingual data for Chinese ?
English and German ?
English language pairs are shown in Table 1 .
With regard to in- domain data , we use both the bilingual data and monolingual data provided by the WMT21 Biomedical Translation Shared task .
For German ?
English task , we select Biomedical Translation and UFAL Medical Corpus as in- domain training data .
Besides , 21.43 M inhouse English monolingual data are used .
For Chinese ?
English task , the used in - house data includes : 1.35 M parallel data , 21.43 M
English monolingual data , and 36.11 M
Chinese monolingual data .
Table 2 shows the details of data in the biomedical domain for German ?
English and Chinese ?
English tasks .
Data Pre-processing
Our data pre-processing methods include : ?
Filter out repeated sentences ( Khayrallah and Koehn , 2018 ; . ?
Normalize punctuations using Moses ( Koehn et al. , 2007 ) . ?
Filter out sentences with repeated fragments .
?
Filter out sentences with mismatched parentheses and quotation marks .
?
Filter out sentences of which punctuation percentage exceeds 0.3 . ?
Filter out sentences with the character - toword ratio greater than 12 or less than 1.5 . ?
Filter out sentences with more than 120 words .
?
Apply langid ( Joulin et al. , 2017 ( Joulin et al. , , 2016 to filter sentences in other languages .
?
Use fast-align ( Dyer et al. , 2013 ) to filter sentence pairs with poor alignment .
It should be noted that for the German ?
English translation task , we employ joint SentencePiece model ( SPM ) ( Kudo and Richardson , 2018 ; Kudo , 2018 ) for word segmentation , with the size of the vocabulary set to 32k .
As for the Chinese ?
English translation task , Jieba tokenizer is used for Chinese word segmentation while Moses tokenizer for English word segmentation .
Byte Pair Encoding ( BPE ) ( Sennrich et al. , 2016 b ) is adopted for Chinese and English sub-word segmentation .
We train BPE models with 32,000 merge operations for both the source and target sides .
3 System overview 3.1 Model
Our system uses Transformer ( Vaswani et al. , 2017 ) model architecture , which adopts full self-attention mechanism to realize algorithm parallelism , accelerate model training speed , and improve translation quality .
Two Transformer deep-large model architectures are used in our experiments : ? Deep 25 - 6 ( Wang et al. , 2018 ; Li et al. , 2019 ) : Based on the Transformer - base model architecture , the deep 25 - 6 model features 25 - layer encoder , 6 - layer decoder , 1024 dimensions of word vector , 4096 - hidden-state , 16 - head self-attention and layer normalization .
We use the open-source Fairseq ( Ott et al. , 2019 ) for training .
The main parameters are as follows :
Each model is trained using 8 GPUs .
The size of each batch is set as 2048 , parameter update frequency as 32 , learning rate as 5e - 4 ( Vaswani et al. , 2017 ) and label smoothing as 0.1 ( Szegedy et al. , 2016 ) .
The number of warmup steps is 4000 , and the dropout is 0.1 .
We also use the Adam optimizer ( Kingma and Ba , 2015 ) with ?1 = 0.9 , ?2 = 0.98 .
In the inference phase ,
The beam-size is 8 , The length penalties for Chinese ?
English and German ?
English are set to 0.5 , and the length penalties for the other two directions are set to 1.5 .
Data augmentation Given the small size of in- domain bilingual data , how to generate more training data becomes a crucial issue for model performance enhancement in the biomedical field .
We adopt three data augmentation methods : ? Data diversification ( Nguyen et al. , 2020 ) : Data diversification is a simple but effective strategy to enhance the performance of NMT .
It uses predictions from multiple forward and backward models and then combines the results with raw data to train the final NMT model .
The method does not require additional monolingual data and is suitable for all types of NMT models .
It is more efficient than knowledge distillation and dual learning , and exhibits strong correlation with model integration .
In our Chinese ?
English and German ?
English systems , we use only the forward model and the backward model to create synthetic data and add the data to the original parallel corpora .
?
Forward translation : Forward translation usually refers to using source language monolinguals to generate synthetic data through beam search decoding , and then add synthetic data to the training data so as to increase the training data size .
Although merely using forward translation may not work well , forward translation can be used in conjunction with a back translation strategy , which also works better than using back translation alone .
We do not use forward translation for the German ?
English system task due to the lack of high-quality in- domain German monolinguals .
We then give up forward translation for the English ?
German direction because forward translation and back translation cannot be used jointly for better effects .
Ultimately , we only adopt forward translation for our Chinese ?
English systems .
?
Back translation ( Edunov et al. , 2018 ) : Back translation translates target side monolingual data back to the source language so as to increase the training data size , which has been proved to be an effective method to improve neural machine translation performances .
There are many methods for generating synthetic corpus through back translation .
In a non-extremely low-resource scenario , sampling or noisy beam search decoding method is more effective than beam search or greedy search , and the synthetic data generated by sampling or noisy beam search decoding method may introduce more diversity to training data .
In our experiment , sampling decoding is adopted .
We use back translation for all directions expect English ?
German , due to the lack of high-quality in- domain German monolinguals .
Training strategy
We first use in- domain training data to conduct incremental training with baseline models trained by WMT21 news data for domain transfer .
Then , we use three monolingual enhancement strategies , data diversity , forward translation and back translation , to create synthetic data and add them to the in-domain training data to further expand the scale of the training data , and then perform incremental training again .
In addition , we fine- tune our models with test sets from previous years of the same task in hope of further improving in -domain performances .
Specifically , we ensemble multiple models to forward translate the source side of test sets to increase the size of the training data , and then add noise ( Meng et al. , 2020 ) to the target side of the training data to achieve a better fine -tuning effect .
Finally , multiple models are ensembled to achieve better performance .
Algorithm 1 : Strategies for selecting ensemble models Input :
The list of all NMT models to be selected M := [ M 1 , ... , M n ] , n is the Number of M , and the test Set T ; Output :
The optimal model combination B := [ M i , ... , M j ] ;
Ensemble
For each translation task , we randomize two sets of training data and train four models using the two model architectures mentioned above .
In the course of our experiments , we find that directly ensemble all models does not necessarily perform better on test set than a single model .
To achieve a better ensemble effect , we design an algorithm , as shown in the algorithm 1 .
The core idea is to traverse all combinations of models and find the best one in the test set .
The experiment results show that ensemble with the best combination found by the traverse strategy is much better than simply ensemble all models .
In our experiment , the model combination that performs best on the wmt20 OK - aligned biomedical test set is used as the final submission .
Experimental result
We train baseline models using WMT21 news data , then incrementally train them using medical bilingual corpora and synthetic data generated by data augmentation techniques , fine - tune models with previous years ' test sets , and finally ensemble multiple models to produce submitted results .
We benchmark our submissions using the WMT20 OKalign test set .
BLEU scores are calculated using the MTEVAL script from Moses ( Koehn et al. , 2007 ) .
The results are shown in Table 3 and Table 4 .
Our models outperform last year 's official best results in three language directions .
The tables mainly show the results of deep 35 - 6 models .
Only in the last ensemble phase , multiple model architectures are used .
we compare our results with the best official results from last year .
We notice that our baseline models trained by WMT news data may also perform quite well in the biomedical field .
For example , in German ?
English ,
Our baseline model is only 2.2 BLEU below last year 's best result .
Chinese ?
English For Chinese ?
English task , we first train the baseline model on WMT21 news data .
German ?English For German ?
English task , the model training strategy used is similar to that for Chinese ?
English task , except data augmentation techniques .
As mentioned above , due to the lack of in- domain German monolingual data , we use data diversity and back translation strategies for German ?
English direction and only data diversity for English ?
German direction .
The German ?
English experiment results are shown in Table 4 . Data augmentation results in significant performance improvements , with 1.1 BLEU and 1.7 BLEU on German ?
English and English ?
German respectively .
Fine-tuning with previous years ' test sets has also improved the quality of in-domain translations .
On German ?
English , we fine - tune the model with wmt18 and wmt19 test sets and see an improvement of 1.1 BLEU .
On English ?
German , fine -tuning leads to an increase of 0.4 BLEU .
System English ?
German German ?
English baseline 33.8 39.5 + biomedical corpus 34.9 ( + 1 .
Ensemble the model combinations found through the ergodic approach contribute to 0.7 BLEU increase for German ?
English and 0.6 BLEU for English ?
German .
Ultimately , due to the lack of effective in- domain German monolingual data , we only surpass last year 's official best results on German ?
English direction .
Conclusion
This paper presents the submissions of HW - TSC to the WMT21 Biomedical Translation Task .
We perform experiments with a series of pre-processing and training strategies .
The effectiveness of each strategy is demonstrated by our experiment results .
Combining with data augmentation strategies , incremental training with in- domain data on the basis of a baseline model from new domain can effectively improve in-domain translation quality .
Our systems in English ?
Chinese and English ?
German directions get the highest BLEU scores among all submissions according to the official evaluation results .
1 Initialize the test set T 's maximum BLEU score maxbleu := 0 ; 2 Initialize the optimal model combination B := [ ] ; 3 for num ? range ( 1 , n ) do 4 Generate a list of model combination numlist , which is all possible combination of num models in M ; 5 for current model combination subnumlist ? numlist do 6 Calculate the current BLEU score curbleu of the current combined model on the test set T .;
