title
Data Augmentation with Unsupervised Machine Translation Improves the Structural Similarity of Cross-lingual Word Embeddings
abstract
Unsupervised cross-lingual word embedding ( CLWE ) methods learn a linear transformation matrix that maps two monolingual embedding spaces that are separately trained with monolingual corpora .
This method relies on the assumption that the two embedding spaces are structurally similar , which does not necessarily hold true in general .
In this paper , we argue that using a pseudo-parallel corpus generated by an unsupervised machine translation model facilitates the structural similarity of the two embedding spaces and improves the quality of CLWEs in the unsupervised mapping method .
We show that our approach outperforms other alternative approaches given the same amount of data , and , through detailed analysis , we show that data augmentation with the pseudo data from unsupervised machine translation is especially effective for mappingbased CLWEs because ( 1 ) the pseudo data makes the source and target corpora ( partially ) parallel ; ( 2 ) the pseudo data contains information on the original language that helps to learn similar embedding spaces between the source and target languages .
Introduction Cross-lingual word embedding ( CLWE ) methods aim to learn a shared meaning space between two languages ( the source and target languages ) , which is potentially useful for cross-lingual transfer learning or machine translation ( Yuan et al. , 2020 ; Artetxe et al. , 2018 b ; Lample et al. , 2018a ) .
Although early methods for learning CLWEs often utilize multilingual resources such as parallel corpora ( Gouws et al. , 2015 ; Luong et al. , 2015 ) and word dictionaries ( Mikolov et al. , 2013 ) , recent studies have focused on fully unsupervised methods that do not require any cross-lingual supervision ( Lample et al. , 2018 b ; Artetxe et al. , 2018a ; Patra et al. , 2019 ) .
Most unsupervised methods fall into the category of mapping - based methods , which generally consist of the following procedures : train monolingual word embeddings independently in two languages ; then , find a linear mapping that aligns the two embedding spaces .
The mappingbased method is based on a strong assumption that the two independently trained embedding spaces have similar structures that can be aligned by a linear transformation , which is unlikely to hold true when the two corpora are from different domains or the two languages are typologically very different ( S?gaard et al. , 2018 ) .
To address this problem , several studies have focused on improving the structural similarity of monolingual spaces before learning mapping ( Zhang et al. , 2019 ; Vuli ?
et al. , 2020 ) , but few studies have focused on how to leverage the text data itself .
In this paper , we show that the pseudo sentences generated from an unsupervised machine translation ( UMT ) system ( Lample et al. , 2018 c ) facilitates the structural similarity without any additional cross-lingual resources .
In the proposed method , the training data of the source and / or target language are augmented with the pseudo sentences ( Figure 1 ) .
We argue that this method facilitates the structural similarity between the source and target embeddings for the following two reasons .
Firstly , the source and target embeddings are usually trained on monolingual corpora .
The difference in the content of the two corpora may accentuate the structural difference between the two resulting embedding spaces , and thus we can mitigate that effect by making the source and target corpora parallel by automatically generated pseudo data .
Secondly , in the mapping - based method , the source and target embeddings are trained independently without taking into account the other language .
Thus , the embedding structures may not be optimal for CLWEs .
We argue that pseudo sentences generated by a UMT Figure 1 : Our framework for training CLWEs using unsupervised machine translation ( UMT ) .
We first train UMT models using monolingual corpora for each language .
We then translate all the training corpora and concatenate the outputs with the original corpora , and train monolingual word embeddings independently .
Finally , we map these word embeddings on a shared embedding .
system contain some trace of the original language , and using them when training monolingual embeddings can facilitate the structural correspondence of the two sets of embeddings .
In the experiments using the Wikipedia dump in English , French , German , and Japanese , we observe substantial improvements by our method in the task of bilingual lexicon induction and downstream tasks without hurting the quality as monolingual embeddings .
Moreover , we carefully analyze why our method improves the performance , and the result confirms that making the source and target corpora parallel does contribute to performance improvement , and also suggests that the generated translation data contain information about the original language .
Background and Related Work Cross-lingual Word Embeddings CLWE methods aim to learn a semantic space shared between two languages .
Most of the current approaches fall into two types of methods : joint- training approaches and mapping - based ap-proaches .
Joint- training approaches jointly train a shared embedding space given multilingual corpora with cross-lingual supervision such as parallel corpora ( Gouws et al. , 2015 ; Luong et al. , 2015 ) , documentaligned corpora ( Vulic and Moens , 2016 ) , or monolingual corpora along with a word dictionary ( Duong et al. , 2016 ) .
On the other hand , mapping - based approaches utilize monolingual embeddings that are already obtained from monolingual corpora .
They assume structural similarity between monolingual embeddings of different languages and attempt to obtain a shared embedding space by finding a transformation matrix W that maps source word embeddings to the target embedding space ( Mikolov et al. , 2013 ) .
The transformation matrix W is usually obtained by minimizing the sum of squared euclidian distances between the mapped source embeddings and target embeddings : argmin W |D | i Wx i ? y i 2 , ( 1 ) where D is a bilingual word dictionary that contains word pairs ( x i , y i ) and x i and y i represent the corresponding word embeddings .
Although finding the transformation matrix W is straightforward when a word dictionary is available , a recent trend is to reduce the amount of crosslingual supervision or to find W in a completely unsupervised manner ( Lample et al. , 2018 b ; Artetxe et al. , 2018a ) .
The general framework of unsupervised mapping methods is based on heuristic initialization of a seed dictionary D and iterative refinement of the transformation matrix W and the dictionary D , as described in Algorithm 1 .
In our experiment , we use the unsupervised mappingbased method proposed by Artetxe et al . ( 2018a ) .
Their method is characterized by the seed dictionary initialized with nearest neighbors based on similarity distributions of words in each language .
These mapping - based methods , however , are based on the strong assumption that the two independently trained embedding spaces have similar structures that can be aligned by a linear transformation .
Although several studies have tackled improving the structural similarity of monolingual spaces before learning mapping ( Zhang et al. , 2019 ; Vuli ?
et al. , 2020 ) , not much attention has been paid to how to leverage the text data itself .
Input :
The source embeddings X , the target embeddings Y Output :
The transformation matrix W Heuristically induce an initial seed word dictionary D while not convergence do Compute W given the word dictionary D from the equation ( 1 ) Update the word dictionary D by retrieving cross-lingual nearest neighbors in a shared embedding space obtained by W end return W Algorithm
1 : The general workflow of unsupervised mapping methods
In this paper , we argue that we can facilitate structural correspondence of two embedding spaces by augmenting the source or / and target corpora with the output from an unsupervised machine translation system ( Lample et al. , 2018 c ) .
Unsupervised Machine Translation Unsupervised machine translation ( UMT ) is the task of building a translation system without any parallel corpora ( Artetxe et al. , 2018 b ; Lample et al. , 2018 a , c ; Artetxe et al. , 2019 b ) . UMT is accomplished by three components : ( 1 ) a wordby - word translation model learned using unsupervised CLWEs ; ( 2 ) a language model trained on the source and target monolingual corpora ; ( 3 ) a backtranslation model where the model uses input and its own translated output as parallel sentences and learn how to translate them in both directions .
More specifically , the initial source - to- target translation model P 0 s?t is created by the word - byword translation model and the language model of the target language .
Then , P 1 t?s is learned in a supervised setting using the source original monolingual corpus paired with the synthetic parallel sentences of the target language generated by P 0 s?t . Again , another source - to- target translation model P 1 s?t is trained with the target original monolingual corpus and the outputs of P 0 s?t , and in the same way , the quality of the translation models is improved with an iterative process .
In our experiments , we adopt an unsupervised phrase - based statistical machine translation ( SMT ) method to generate a pseudo corpus because it produces better translations than unsupervised neural machine translation on low-resource languages ( Lample et al. , 2018 c ) .
The difference of the unsupervised SMT ( USMT ) model from its supervised counterpart is that the initial phrase table is derived based on the cosine similarity of unsupervised CLWEs , and the translation model is iteratively im -proved by pseudo parallel corpora .
Our proposed method utilizes the output of a USMT system to augment the training corpus for CLWEs .
Exploiting UMT for Cross-lingual Applications
There is some previous work on how to use UMT to induce bilingual word dictionaries or improve CLWEs .
Artetxe et al. ( 2019a ) explored an effective way of utilizing a phrase table from a UMT system to induce bilingual dictionaries .
Marie and Fujita ( 2019 ) generate a synthetic parallel corpus from a UMT system , and jointly train CLWEs along with the word alignment information ( Luong et al. , 2015 ) .
In our work , we use the synthetic parallel corpus generated from a UMT system not for joint - training but for data augmentation to train monolingual word embeddings for each language , which are subsequently aligned through unsupervised mapping .
In the following sections , we empirically show that our approach leads to the creation of improved CLWEs and analyze why these results are achieved .
Experimental Design
In this section , we describe how we obtain mapping - based CLWEs using a pseudo parallel corpus generated from UMT .
We first train UMT models using the source / target training corpora , and then translate them to the machine - translated corpora .
Having done that , we simply concatenate the machine - translated corpus with the original training corpus , and learn monolingual word embeddings independently for each language .
Finally , we map these embeddings to a shared CLWE space .
Corpora
We implement our method with two similar language pairs : English - French ( en-fr ) , English - German ( en-de ) , and one distant language pair : English - Japanese ( en-ja ) .
We use plain texts from Wikipedia dumps 1 , and randomly extract 10 M sentences for each language .
The English , French , and German texts are tokenized with the Moses tokenizer ( Koehn et al. , 2007 ) and lowercased .
For Japanese texts , we use kytea 2 to tokenize and normalize them 3 .
Training mapping - based CLWEs
Given tokenized texts , we train monolingual word embeddings using fastText 4 with 512 dimensions , a context window of size 5 , and 5 negative examles .
We then map these word embeddings on a shared embedding space using the open-source implementation VecMap 5 with the unsupervised mapping algorithm ( Artetxe et al. , 2018a ) .
Training UMT models
To implement UMT , we first build a phrase table by selecting the most frequent 300,000 source phrases and taking their 200 nearest - neighbors in the CLWE space following the setting of Lample et al . ( 2018 c ) .
We then train a 5 - gram language model for each language with KenLM ( Heafield et al. , 2013 ) and combine it with the phrase table , which results in an unsupervised phrase - based SMT model .
Then , we refine the UMT model through three iterative back -translation steps .
At each step , we translate 100k sentences randomly sampled from the monolingual data set .
We use a phrase table containing phrases up to a length of 4 except for initialization .
The quality of our UMT models is indicated by the BLEU scores ( Papineni et al. , 2002 )
Training CLWEs with pseudo corpora
We then translate all the training corpora with the UMT system and obtain machine - translated corpora , which we call pseudo corpora .
We concatenate the pseudo corpora with the original corpora , and learn monolingual word embeddings for each language .
Finally , we map these word embeddings to a shared CLWE space with the unsupervised mapping algorithm .
Models
We compare our method with a baseline with no data augmentation as well as the existing related methods : dictionary induction from a phrase table ( Artetxe et al. , 2019a ) and the unsupervised jointtraining method ( Marie and Fujita , 2019 ) .
These two methods both exploit word alignments in the pseudo parallel corpus , and to obtain them we use Fast Align 8 ( Dyer et al. , 2013 ) with the default hyperparameters .
For the joint- training method , we adopt bivec 9 to train CLWEs with the parameters used in Upadhyay et al . ( 2016 ) using the pseudo parallel corpus and the word alignments .
To ensure fair comparison , we implement all of these methods with the same UMT system .
Evaluation of Cross-lingual Mapping
In this section , we conduct a series of experiments to evaluate our method .
We first evaluate the performance of cross-lingual mapping in our method ( ? 4.1 ) and investigate the effect of UMT quality ( ? 4.2 ) .
Then , we analyze why our method improves the bilingual lexicon induction ( BLI ) performance .
Through carefully controlled experiments , we argue that it is not simply because of data augmentation but because : ( 1 ) the generated data makes the source and target corpora ( partially ) parallel ( ? 4.3 ) ; ( 2 ) the generated data reflects the co-occurrence statistics of the original language ( ? 4.4 ) .
Bilingual Lexicon Induction First , we evaluate the mapping accuracy of word embeddings using BLI .
BLI is the task of iden - 167 Method source ( en ) target en? fr fr?en en?de de?en en? ja ja?en orig .
psd. orig. psd. MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 MRR P@1 BLI from phrase
In each cell , the left cell shows the result of MRR , and the right cell shows the result of p@1 .
tifying word translation pairs , and is a common benchmark for evaluating CLWE methods .
In these experiments , we use Cross-Domain Similarity Local Scaling ( Lample et al. , 2018 b ) as the method for identifying translation pairs in the two embedding spaces .
For BLI scores , we adopt the mean reciprocal rank ( MRR ) and P@1 .
We use XLing - Eval 10 as test sets for En- Fr and En-Ge. For En-Ja .
We create the word dictionaries automatically using Google Translate 11 , following Ri and Tsuruoka ( 2020 ) .
Other than BLI from a phrase table , we train three sets of embeddings with different random seeds and report the average of the results .
We compare the proposed method with other alternative approaches in BLI as shown in Table 2 .
In all the language pairs , the mapping method with pseudo data augmentation achieves better performance than the other methods .
Here , one may think that the greater amount of data can lead to better performance , and thus augmenting both the source and target corpora shows the best performance .
However , the result shows that it is not necessarily the case : for our mapping method , augmenting only either the source or target , not both , achieves the best performance in many language pairs .
This is probably due to the presence of two pseudo corpora with different natures .
As for the two methods using word alignments ( BLI from phrase table ; joint training ) , we observe some cases where these models underperform the mapping methods , especially in English and Japanese pairs .
We attribute this to our relatively low-resource setting where the quality of the synthetic parallel data is not sufficient to per- form these methods which require word alignment between parallel sentences .
Effect of UMT quality
To investigate the effect of UMT quality on our method , we compare the accuracy of BLI on the CLWEs using pseudo data generated from UMT models of different qualities .
As a translator with low performance , we prepare models that perform fewer iterations on back -translation ( BT ) .
Note that we compare the results on the source-side ( English ) extension , where the quality of the translation is notably different .
As shown in Table 3 , we find that the better the quality of generated data , the better the performance of BLI .
Effect of sharing content
In the mapping method , word embeddings are independently trained by monolingual corpora that do not necessarily have the same content .
As a result , the difference in the corpus contents can hurt the structural similarity of the two resulting embedding spaces .
We hypothesize that using synthetic parallel data which have common contents for learning word embeddings leads to better structural correspondence , which improves cross-lingual mapping .
To verify the effect of sharing the contents using parallel data , we compare the extensions with a parallel corpus and a non-parallel corpus .
More concretely , we first split the original training data of the source and target languages evenly ( each denoted as Split A and Split B ) .
As the baseline , we train CLWEs with Split A .
We use the translation of Split A of the target language data for the parallel extension of the source data , and Split B for the nonparallel extension .
Also , we compare them with the extension with non-pseudo data , which is simply increasing the amount of the source language data by raw text .
Along with the BLI score , we show eigenvector similarity , a spectral metric to quantify the structural similarity of word embedding spaces ( S?gaard et al. , 2018 ) .
To compute eigenvector similarity , we normalize the embeddings and construct the nearest neighbor graphs of the 10,000 most frequent words in each language .
We then calculate their Laplacian matrices L1 and L2 from those graphs and find the smallest k such that the sum of the k largest eigenvalues of each Laplacian matrices is < 90 % of all eigenvalues .
Finally , we sum up the squared differences between the k largest eigenvalues from L1 and L2 and derive the eigen similarity .
Note that smaller eigenvector similarity values mean higher degrees of structural similarity .
Table 4 shows the BLI scores and eigenvector similarity in each extension setting .
The parallel extension method shows a slightly better BLI performance than the non-parallel extension .
This supports our hypothesis that parallel pseudo data make word embeddings space more suitable for bilingual mapping because of sharing content .
In eigenvector similarity , there is no significant improvement between the parallel and non-parallel corpora .
This is probably due to large fluctuations in eigenvector similarity values .
Surprisingly , the results show that augmentation using pseudo data is found to be much more effective than the extension of the same amount of original training data .
This result suggests that using pseudo data as training data is useful , especially for learning bilingual models .
Effect of reflecting the co-occurrence statistics of the language
We hypothesize that the translated sentences reflect the co-occurrence statistics of the original language , which makes the co-occurrence information on training data similar , improving the structural similarity of the two monolingual embeddings .
To verify this hypothesis , we experiment with augmenting the source language with sentences translated from a non-target language .
To examine only the effect of the co-occurrence statistics of language and avoid the effects of sharing content , we use the extensions with the non-parallel corpus .
Table 5 shows that BLI performance and eigenvector similarity improve with the extension from the same target language , but that is not the case if the pseudo corpus is generated from a non-target language .
These results indicate that our method can leverage learning signals on the other language in the pseudo data .
Downstream Tasks Although CLWEs were evaluated almost exclusively on the BLI task in the past , recently showed that CLWEs that perform well on BLI do not always perform well in other cross-lingual tasks .
Therefore , we evaluate our embeddings on the four downstream tasks : topic classification ( TC ) , sentiment analysis ( SA ) , dependency parsing ( DP ) , and natural language inference en-fr en-de en-ja
( NLI ) .
Topic Classification
This task is classifying the topics of news articles .
We use the MLDoc 12 corpus compiled by Schwenk and Li ( 2018 ) .
It includes four topics : CCAT ( Corporate / Industrial ) , ECAT ( Economics ) , GCAT ( Government / Social ) , MCAT ( Markets ) .
As the classifier , we implemented a simple light - weight convolutional neural network ( CNN ) - based classifier .
Sentiment Analysis
In this task , a model is used to classify sentences as either having a positive or negative opinion .
We use the Webis-CLS -10 corpus 13 .
This data consists of review texts for amazon products and their ratings from 1 to 5 .
We cast the problem as binary classification and define rating values 1 - 2 as " negative " and 4 - 5 as " positive " , and exclude the rating 3 .
Again , we use the CNN - based classifier for this task .
Dependency Parsing
We train the deep biaffine parser ( Dozat and Manning , 2017 ) with the UD English EWT dataset 14 ( Silveira et al. , 2014 ) .
We use the PUD treebanks 15 as test data .
Natural Language Inference
We use the English MultiNLI corpus for training and the multilingual XNLI corpus for evaluation ( Conneau et al. , 2018 ) . XNLI only covers French and German from our experiment .
We train the LSTM - based classifier ( Bowman et al. , 2015 ) , which encodes two sentences , concatenated the representations , and then feed them to a multi-layer perceptron .
12 https://github.com/facebookresearch/ MLDoc 13 https://webis.de/data/webis-cls-10. html 14 https://universaldependencies.org/ treebanks/en_ewt/index.html 15 https://universaldependencies.org/ conll17 /
In each task , we train the model using English training data with the embedding parameters fixed .
We then evaluate the model on the test data in other target languages .
Result and Discussion Table 6 shows the test set accuracy of downstream tasks .
For topic classification , our method obtains the best results in all language pairs .
Especially in En-Fr and En-Ja , a significant difference is obtained in Student 's t-test .
For sentiment analysis , we observe a significant improvement in En- De , but cannot observe consistent trends in other languages .
For dependency parsing and natural language inference , we observe a similar trend where the performance of our method outperforms other methods , although no significant difference is observed in the t-test .
The cause of the lower performance of joint-training compared with the mapping method is presumably due to the poor quality of synthetic parallel data as described in ? 4.1 .
In summary , given the same amount of data , the CLWEs obtained from our method tend to show higher performance not only in BLI but also in downstream tasks compared with other alternative methods , although there is some variation .
Analysis Monolingual Word Similarity
Our method uses a noisy pseudo corpus to learn monolingual word embeddings , and it might hurt the quality of monolingual embeddings .
To investigate this point , we evaluate monolingual embeddings with the word similarity task .
This task evaluates the quality of monolingual word embeddings by measuring the correlation between the cosine similarity in a vector space and manually created word pair similarity .
We use simverb - 3500 16 ( Gerz et al. , en- fr en-de en-ja corpus en fr en de en ja origin 1.60 ? 10 ?3 1.63 ? 10 ?3 1.51 ? 10 ?3 3.78 ? 10 ?3 1.52 ? 10 ?3 1.03 ? 10 ?3 pseudo 0.57 ? 10 ?3 0.57 ? 10 ?3 0.66 ? 10 ?3 0.59 ? 10 ?3 0.19 ? 10 ?3 0.17 ? 10 ?3 2016 ) consisting of 3500 verb pairs and men 17 ( Bruni et al. , 2014 ) consisting of 3000 frequent words extracted from web text .
Table 8 shows the results of word similarity .
The scores of monolingual word embeddings using a French and German pseudo corpus are maintained or improved , while they decrease in Japanese .
This suggests that the quality of monolingual word embeddings could be hurt due to the low quality of the pseudo corpus or differences in linguistic nature .
Nevertheless , the proposed method improves the performance of En-Ja's CLWE , which suggests that the monolingual word embeddings created with a pseudo corpus have a structure optimized for crosslingual mapping .
Application to UMT UMT is one of the important applications of CLWEs .
Appropriate initialization with CLWEs is crucial to the success of UMT ( Lample et al. , 2018 c ) .
To investigate how CLWEs obtained from our method affect the performance of UMTs , we compare the BLEU scores of UMTs initialized with CLWEs with and without a pseudo corpus at each iterative step .
As shown in Table 9 , we observe that initialization with CLWE using the pseudo data result in a higher BLEU score in the first step but does not improve the score at further steps compared to the CLWE without the pseudo data .
Marie and Fujita ( 2019 ) also demonstrate the same tendency in the CLWE with joint-training .
To investigate this point , we compare the lexical densities of the training corpus and the pseudocorpus used in the above experiments ( ? 4 , 5 ) using type-token ratio ( standardized to some extent as reported in Vanmassenhove et al . ( 2019 ) .
As a result , specific words might be easily mapped in CLWEs using a pseudo corpus 18 , and then the translation model makes it easier to translate phrases in more specific patterns .
Hence , the model cannot generate diverse data during back - translation , and the accuracy is not improved due to easy learning .
Conclusion and Future Work
In this paper , we show that training cross-lingual word embeddings with pseudo data augmentation improves performance in BLI and downstream tasks .
We analyze the reason for this improvement and found that the pseudo corpus reflects the co-occurrence statistics and content of the other language and that the property makes the structure of the embedding suitable for cross-lingual word mapping .
Recently , have shown that fully unsupervised CLWE methods fails in many language pairs and argue that researchers should not focus too much on the fully unsupervised settings .
Still , our findings that improve structural similarity of word embeddings in the fully unsupervised setting could be useful in semi-supervised settings , and thus we would like to investigate this direction in the future .
Table 1 : 1 in Table1 .
We use newstest2014 from WMT14 6 to evaluate En- Fr and En- De translation accuracy and the Tanaka corpus 7 for En- Ja evaluation .
BLEU scores of UMT .
en - fr en - de en - ja ? ? ? ? ? ? 19.2 19.1 10.3 13.7 3.6 1.4
