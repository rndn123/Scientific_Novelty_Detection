title
From Machine Translation to Code-Switching : Generating High -Quality Code-Switched Text
abstract
Generating code-switched text is a problem of growing interest , especially given the scarcity of corpora containing large volumes of real code-switched text .
In this work , we adapt a state - of - the - art neural machine translation model to generate Hindi-English codeswitched sentences starting from monolingual Hindi sentences .
We outline a carefully designed curriculum of pretraining steps , including the use of synthetic code-switched text , that enable the model to generate high-quality codeswitched text .
Using text generated from our model as data augmentation , we show significant reductions in perplexity on a language modeling task , compared to using text from other generative models of CS text .
We also show improvements using our text for a downstream code-switched natural language inference task .
Our generated text is further subjected to a rigorous evaluation using a human evaluation study and a range of objective metrics , where we show performance comparable ( and sometimes even superior ) to codeswitched text obtained via crowd workers who are native Hindi speakers .
Introduction Code-switching ( CS ) refers to the linguistic phenomenon of using more than one language within a single sentence or conversation .
CS appears naturally in conversational speech among multilingual speakers .
The main challenge with building models for conversational CS text is that we do not have access to large amounts of CS text that is conversational in style .
One might consider using social media text that contains CS and is more readily available .
However , the latter is quite different from conversational CS text in its vocabulary ( e.g. , due to the frequent use of abbreviated slang terms , hashtags and mentions ) , in its sentence structure ( e.g. , due to character limits in tweets ) and in its word forms ( e.g. , due to transliteration being commonly employed in social media posts ) .
This motivates the need for a generative model of realistic CS text that can be sampled to subsequently train models for CS text .
In this work , we tackle the problem of generating high-quality CS text using only limited amounts of real CS text during training .
We also assume access to large amounts of monolingual text in the component languages and parallel text in both languages , which is a reasonable assumption to make for many of the world 's languages .
We focus on Hindi-English CS text where the matrix ( dominant ) language is Hindi and the embedded language is English .
1 Rather than train a generative model , we treat this problem as a translation task where the source and target languages are monolingual Hindi text and Hindi-English CS text , respectively .
We also use the monolingual Hindi text to construct synthetic CS sentences using simple techniques .
We show that synthetic CS text , albeit being naive in its construction , plays an important role in improving our model 's ability to capture CS patterns .
We draw inspiration from the large body of recent work on unsupervised machine translation ( Lample et al. , 2018 a , b ) to design our model , which will henceforth be referred to as Translation for Code- Switching , or TCS .
TCS , once trained , will convert a monolingual Hindi sentence into a Hindi-English CS sentence .
TCS makes effective use of parallel text when it is available and uses backtranslation - based objective functions with monolingual text .
Below , we summarize our main contributions : 1 . We propose a state- of- the - art translation model that generates Hindi-English CS text starting from monolingual Hindi text .
This model requires very small amounts of real CS text , uses both supervised and unsupervised training objectives and considerably benefits from a carefully designed training curriculum , that includes pretraining with synthetically constructed CS sentences .
2 . We introduce a new Hindi-English CS text corpus in this work .
2 Each CS sentence is accompanied by its monolingual Hindi translation .
We also designed a crowdsourcing task to collect CS variants of monolingual Hindi sentences .
The crowdsourced CS sentences were manually verified and form a part of our new dataset .
3 . We use sentences generated from our model to train language models for Hindi-English CS text and show significant improvements in perplexity compared to other approaches .
4 . We present a rigorous evaluation of the quality of our generated text using multiple objective metrics and a human evaluation study , and they clearly show that the sentences generated by our model are superior in quality and successfully capture naturally occurring CS patterns .
Related Work Early approaches of language modeling for codeswitched text included class - based n-gram models ( Yeh et al. ) , factored language models that exploited a large number of syntactic and semantic features ( Adel et al. , 2015 ) , and recurrent neural language models ( Adel et al. , 2013 ) for CS text .
All these approaches relied on access to real CS text to train the language models .
Towards alleviating this dependence on real CS text , there has been prior work on learning code-switched language models from bilingual data ( Li and Fung , 2014 b , a ; Garg et al. , 2018 b ) and a more recent direction that explores the possibility of generating synthetic CS sentences .
( Pratapa et al. , 2018 ) presents a technique to generate synthetic CS text that grammatically adheres to a linguistic theory of code-switching known as the equivalence constraint ( EC ) theory ( Poplack , 1979 ; Sankoff , 1998 ) . Lee and Li ( 2020 ) proposed a bilingual attention language model for CS text trained solely using a parallel corpus .
Another recent line of work has explored neural generative models for CS text .
Garg et al. ( 2018a ) use a sequence generative adversarial network ( SeqGAN ( Yu et al. , 2017 ) ) trained on real CS text to generate sentences that are used to aid language model training .
Another GAN - based method proposed by Chang et al . ( 2019 ) aims to predict the probability of switching at each token .
Winata et al. ( 2018 ) and Winata et al . ( 2019 ) use a sequence - to-sequence model enabled with a copy mechanism ( Pointer Network ( Vinyals et al. , 2015 ) ) to generate CS data by leveraging parallel monolingual translations from a limited source of CS data .
Samanta et al. ( 2019 ) proposed a hierarchical variational autoencoder - based model tailored for code-switching that takes into account both syntactic information and language switching signals via the use of language tags .
( We present a comparison of TCS with both Samanta et al . ( 2019 ) and Garg et al . ( 2018a ) in Section 5.2.1 . )
In a departure from using generative models for CS text , we view this problem as one of sequence transduction where we train a model to convert a monolingual sentence into its CS counterpart .
Chang et al . ( 2019 ) ; Gao et al. ( 2019 ) use GAN - based models to modify monolingual sentences into CS sentences , while we treat this problem of CS generation as a translation task and draw inspiration from the growing body of recent work on neural unsupervised machine translation models ( Lample et al. , 2018 a , b ) to build an effective model of CS text .
The idea of using translation models for codeswitching has been explored in early work ( Vu et al. , 2012 ; Li and Fung , 2013 ; Dhar et al. , 2018 ) .
Concurrent with our work , there have been efforts towards building translation models from English to CS text ( Solorio et al. , 2021 ) and CS text to English .
While these works focus on translating from the embedded language ( English ) to the CS text or vice-versa , our approach starts with sentences in the matrix language ( Hindi ) which is the more dominant language in the CS text .
Also , ours is the first work , to our knowledge , to repurpose an unsupervised neural machine translation model to translate monolingual sentences into CS text .
Powerful pretrained models like mBART ( Liu et al. , 2020 ) have been used for codemixed translation tasks in concurrent work ( Gautam et al. , 2021 ) .
We will further explore the use of synthetic text with such models as part of future work .
Our Approach Figure 1 shows the overall architecture of our model .
This is largely motivated by prior work on unsupervised neural machine translation ( Lample et al. , 2018 a , b) .
The model comprises of three layers of stacked Transformer ( Vaswani et al. , 2017 ) encoder and decoder layers , two of which are shared and the remaining layer is private to each language .
Monolingual Hindi ( i.e. the source language ) has its own private encoder and decoder layers ( denoted by Enc p 0 and Dec p 0 , respectively ) while English and Hindi-English CS text jointly make use of the remaining private encoder and decoder layers ( denoted by Enc p 1 and Dec p 1 , respectively ) .
In our model , the target language is either English or CS text .
Ideally , we would like Enc p 1 and Dec p 1 to be trained only using CS text .
However , due to the paucity of CS text , we also use text in the embedded language ( i.e. English ) to train these layers .
Next , we outline the three main training steps of TCS .
( I ) Denoising autoencoding ( DAE ) .
We use monolingual text in each language to estimate language models .
In Lample et al. ( 2018 b ) , this is achieved via denoising autoencoding where an autoencoder is used to reconstruct a sentence given a noisy version as its input whose structure is altered by dropping and swapping words arbitrarily ( Lample et al. , 2018a ) .
The loss incurred in this step is denoted by L DAE and is composed of two terms based on the reconstruction of the source and target language sentences , respectively .
( II ) Backtranslation ( BT ) :
Once the layers are initialized , one can use non-parallel text in both languages to generate a pseudo-parallel corpus of backtranslated pairs ( Sennrich et al. , 2015 ) .
That is , a corpus of parallel text is constructed by translating sentences in the source language via the pipeline , Enc p 0 , Enc sh , Dec sh and Dec p 1 , and translating target sentences back to the source language via Enc p 1 , Enc sh , Dec sh and Dec p 0 .
The backtranslation loss L BT is composed of crossentropy losses from using these pseudo- parallel v w M = " > A A A B + 3 i c b V D L S s N A F L 3 x W e u r 1 q W b w S K 4 K k k V d F k U w W U F + 4 A 2 h M l 0 0 g 6 d S c L M R C w h v + L G h S J u / R F 3 / o 2 T m I W 2 H h g 4 n H M v c + 7 x Y 8 6 U t u 0 v a 2 V 1 b X 1 j s 7 J V 3 d 7 Z 3 d u v H d R 7 K k o k o V 0 S 8 U g O f K w o Z y H t a q Y 5 H c S S Y u F z 2 v d n 1 7 n f f 6 B S s S i 8 1 / O Y u g J P Q h Y w g r W R v F p 9 J L C e S p H e h C T z 0 t i z M 6 / W s J t 2 A b R M n J I 0 o E T H q 3 2 O x h F J B A 0 1 4 V i p o W P H 2 k 2 x 1 I x w m l V H i a I x J j M 8 o U N D Q y y o c t M i e 4 Z O j D J G Q S T N C z U q 1 N 8 b K R Z K z Y V v J v O k a t H L x f + 8 Y a K D S z d l Y Z x o a m 4 r P g o S j n S E 8 i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u q i n B W T x 5 m f R a T e e s 2 b o 7 b 7 S v y j o q c A T H c A o O X E A b b q E D X S D w C E / w A q 9 W Z j 1 b b 9 b 7 z + i K V e 4 c w h 9 Y H 9 9 n + J S s < / l a t e x i t >
Enc sh < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 C l z R t h u u S 4 G M 2 7 v r w B a 0 T G g j j A = " > A A A B + n i c b V D L S s N A F J 3 U V 6 2 v V J d u B o v g q i R V 0 G V R B J c V 7 A P a E C b T S T t 0 Z h J m J k q J + R Q 3 L h R x 6 5 e 4 8 2 + c x C y 0 9 c D A 4 Z x 7 m X N P E D O q t O N 8 W Z W V 1 b X 1 j e p m b W t 7 Z 3 f P r u / 3 V J R I T L o 4 Y p E c B E g R R g X p a q o Z G c S S I B 4 w 0 g 9 m V 7 n f v y d S 0 U j c 6 X l M P I 4 m g o Y U I 2 0 k 3 6 6 P O N J T y d N r g T M / V d P M t x t O 0 y k A l 4 l b k g Y o 0 f H t z 9 E 4 w g k n Q m O G l B q 6 T q y 9 F E l N M S N Z b Z Q o E i M 8 Q x M y N F Q g T p S X F t E z e G y U M Q w j a Z 7 Q s F B / b 6 S I K z X n g Z n M g 6 p F L x f / 8 4 a J D i + 8 l I o 4 0 c S c V n w U J g z q C O Y 9 w D G V B G s 2 N w R h S U 1 W i K d I I q x N W z V T g r t 4 8 j L p t Z r u a b N 1 e 9 Z o X 5 Z 1 V M E h O A I n w A X n o A 1 u Q A d 0 A Q Y P 4 A m 8 g F f r 0 X q 2 3 q z 3 n 9 G K V e 4 c g D + w P r 4 B B Y i U f g = = < / l a t e x i t >
Dec sh < l a t e x i t s h a 1 _ b a s e 6 4 = "
5 y q c H a Z e Q 2 t 4 y t T 0 k W q x F f W w F B A = " > A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K k k V d F n U h c s K 9 g F t C J P p p B 0 6 k 4 S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r Z X V t f W N z d J W e X t n d 2 / f r h y 0 V Z x K Q l s k 5 r H s B l h R z i L a 0 k x z 2 k 0 k x S L g t B O M r 3 O / 8 0 C l Y n F 0 r y c J 9 Q Q e R i x k B G s j +
X a l L 7 A e S Z H d U D L 1 M z W a + n b V q T k z o G X i F q Q K B Z q + / d U f x C Q V N N K E Y 6 V 6 r p N o L 8 N S M 8 L p t N x P F U 0 w G e M h 7 R k a Y U G V l 8 2 i T 9 G J U Q Y o j K V 5 k U Y z 9 f d G h o V S E x G Y y T y o W v R y 8 T + v l + r w 0 s t Y l K S a R m R + K E w 5 0 j H K e 0 A D J i n R f G I I J p K Z r I i M s M R E m 7 b K p g R 3 8 c v L p F 2 v u W e 1 + t 1 5 t X F V 1 F G C I z i G U 3 D h A h p w C 0 1 o A Y F H e I Z X e L O e r B f r 3 f q Y j 6 5 Y x c 4 h / I H 1 + Q P 2 C p R 0 < / l a t e x i t > L CE : Enc p0 Enc sh Dec sh Dec p1 ; Enc p1 Enc sh Dec sh Dec p0 < l a t e x i t s h a 1 _ b a s e 6 4 = "
J H o v l 8 M v H O X m Q N N t c Z y C 3 i / o N / U = " > A A A C p H i c n V H L S g M x F M 2 M 7 / q q u n Q T L Y q r M q O C o h t R i y 6 6 a N F q o a 1 D J r 2 1 w c y D 5 I 5 Y h v k y / 8 K d f 2 M 6 F t S 2 K y 8 E D u f c e 3 I f f i y F R s f 5 t O y Z 2 b n 5 h c W l w v L K 6 t p 6 c W P z Q U e J 4 t D g k Y x U 0 2 c a p A i h g Q I l N G M F L P A l P P o v V 0 P 9 8 R W U F l F 4 j 4 M Y O g F 7 D k V P c I a G 8 o r v 7 Y B h n z O Z V j M v v a p k b Y Q 3 T M 9 o R n N F B W k l 5 E a K P W e c 0 v 0 f 5 h q m M 7 H n G i r 3 p P S c G t v C u K / 7 P 1 8 n 8 4 o l p + z k Q S e B O w I l M o q a V / x o d y O e B B A i l 0 z r l u v E 2 E m Z Q s E l Z I V 2 o i F m / I U 9 Q 8 v A k A W g O 2 m + 5 I z u G a Z L e 5 E y L 0 S a s 7 8 r U h Z o P Q h 8 k z n s V I 9 r Q 3 K a 1 k q w d 9 p J R R g n C G b + / K N e I i l G d H g x 2 h U K O M q B A Y w r Y X q l v M 8 U 4 2 j u W j B L c M d H n g Q P h 2 X 3 q H x Y P y 5 d X I 7 W s U i 2 y S 4 5 I C 4 5 I R f k l t R I g 3 B r x 7 q x a l b d 3 r e r 9 p 3 d + E 6 1 r V H N F v k T 9 t M X z p P T Y g = = < / l a t e x i t > L BT : Enc p1 Enc sh Dec sh Dec p0 ; Enc p0 Enc sh Dec sh Dec p1 < l a t e x i t s h a 1 _ b a s e 6 4 = " o G v g J y H 7 E u F e g n v s W 5 X K A R p K 1 h 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 P t A = " > A A A C p H i c n V H J S g N B E O 0 Z 9 7 h F P X p p D Y q n M K O C o h d x Q Q 8 5 J G h M I I l D T 6 d i G n s W u m v E M M y X + R f e / B s 7 Y 0 B N c r K g 4 f F e 1 e t a / F g K j Y 7 z a d k z s 3 P z C 4 t L h e W V 1 b X 1 4 s b m o 4 4 S x a H O I x m p p s 8 0 S B F C H Q V K a M Y K W O B L a P g v V 0 O 9 8 Q p K i y h 8 w E E M n Y A 9 h 6 I n O E N D e c X 3 d s C w z 5 l M K 5 m X X j 5 k b Y Q 3 T M 9 o R n N F B e l N y I 0 U e + 4 4 p f s / z D V M Z 2 L P M V T u S e k 5 N b a F c V / n f 7 5 u 5 h V L T t n J g 0 4 C d w R K Z B R V r / j R 7 k Y 8 C S B E L p n W L d e J s Z M y h Y J L y A r t R E P M + A t 7 h p a B I Q t A d 9 J 8 y R n d M 0 y X 9 i J l X o g 0 Z 3 9 X p C z Q e h D 4 J n P Y q R 7 X h u Q 0 r Z V g 7 7 S T i j B O E M z 8 + U e 9 R F K M 6 P B i t C s U c J Q D A x h X w v R K e Z 8 p x t H c t W C W 4 I 6 P P A k e D 8 v u U f m w d l y 6 u B y t Y 5 F s k 1 1 y Q F x y Q i 7 I H a m S O u H W j n V r V a 2 a v W 9 X 7 H u 7 / p 1 q W 6 O a L f I n 7 K c v 7 T L T c A = = < / l a t e x i t > Enc p1 < l a t e x i t s h a 1 _ b a s e 6 4 = " G A 5 k o I / F K j T r L s s 2 e A Q 6 E p q a b j Y = " > A A A B + 3 i c b V D L S s N A F L 3 x W e u r 1 q W b w S K 4 K k k V d F k U w W U F + 4 A 2 h M l 0 0 g 6 d S c L M R C w h v + L G h S J u / R F 3 / o 2 T m I W 2 H h g 4 n H M v c + 7 x Y 8 6 U t u 0 v a 2 V 1 b X 1 j s 7 J V 3 d 7 Z 3 d u v H d R 7 K k o k o V 0 S 8 U g O f K w o Z y H t a q Y 5 H c S S Y u F z 2 v d n 1 7 n f f 6 B S s S i 8 1 / O Y u g J P Q h Y w g r W R v F p 9 J L C e S p H e h C T z 0 t h z M q / W s J t 2 A b R M n J I 0 o E T H q 3 2 O x h F J B A 0 1 4 V i p o W P H 2 k 2 x 1 I x w m l V H i a I x J j M 8 o U N D Q y y o c t M i e 4 Z O j D J G Q S T N C z U q 1 N 8 b K R Z K z Y V v J v O k a t H L x f + 8 Y a K D S z d l Y Z x o a m 4 r P g o S j n S E 8 i L Q m E l K N J 8 b g o l k J i s i U y w x 0 a a u q i n B W T x 5 m f R a T e e s 2 b o 7 b 7 S v y j o q c A T H c A o O X E A b b q E D X S D w C E / w A q 9 W Z j 1 b b 9 b 7 z + i K V e 4 c w h 9 Y H 9 9 p f Z S t < / l a t e x i t > Dec p0 < l a t e x i t s h a 1 _ b a s e 6 4 = "
U b c a R 7 1 z 2 6 A r a Q J R / y u M 5 M k W + c w = " > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f s S 7 d D B b B V U m q o M u i L l x W s A 9 o Q 5 h M p + 3 Q m S T M T M Q S 8 i t u X C j i 1 h 9 x 5 9 8 4 a b P Q 1 g M D h 3 P u 5 Z 4 5 Q c y Z 0 o 7 z b Z X W 1 j c 2 t 8 r b l Z 3 d v f 0 D + 7 D a U V E i C W 2 T i E e y F 2 B F O Q t p W z P N a S + W F I u A 0 2 4 w v c n 9 7 i O V i k X h g 5 7 F 1 B N 4 H L I R I 1 g b y b e r A 4 H 1 R I r 0 l p L M T 2 P f y X y 7 5 t S d O d A q c Q t S g w I t 3 / 4 a D C O S C B p q w r F S f d e J t Z d i q R n h N K s M E k V j T K Z 4 T P u G h l h Q 5 a X z 7 B k 6 N c o Q j S J p X q j R X P 2 9 k W K h 1 E w E Z j J P q p a 9 X P z P 6 y d 6 d O W l L I w T T U O y O D R K O N I R y o t A Q y Y p 0 X x m C C a S m a y I T L D E R J u 6 K q Y E d / n L q 6 T T q L v n 9 c b 9 R a 1 5 X d R R h m M 4 g T N w 4 R K a c A c t a A O B J 3 i G V 3 i z M u v F e r c +
F q M l q 9 g 5 g j + w P n 8 A W H + U o g = = < / l a t e x i t > Dec p1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 3 9 O k X S 3 p s y a E z L A t b n z d a T M A H L M = " > A A A B + 3 i c b V D L S s N A F L 2 p r 1 p f s S 7 d D B b B V U m q o M u i L l x W s A 9 o Q 5 h M p + 3 Q m S T M T M Q S 8 i t u X C j i Q c y Z 0 o 7 z b Z X W 1 j c 2 t 8 r b l Z 3 d v f 0 D + 7 D a U V E i C W 2 T i E e y F 2 B F O Q t p W z P N a S + W F I u A 0 2 4 w v c n 9 7 i O V i k X h g 5 7 F 1 B N 4 H L I R I 1 g b y b e r A 4 H 1 R I r 0 l p L M T 2 P f z X y 7 5 t S d O d A q c Q t S g w I t 3 / 4 a D C O S C B p q w r F S f d e J t Z d i q R n h N K s M E k V j T K Z 4 T P u G h l h Q 5 a X z 7 B k 6 N c o Q j S J p X q j R X P 2 9 k W K h 1 E w E Z j J P q p a 9 X P z P 6 y d 6 d O W l L I w T T U O y O D R K O N I R y o t A Q y Y p 0 X x m C C a S m a y I T L D E R J u 6 K q Y E d / n L q 6 T T q L v n 9 c b 9 R a 1 5 X d R R h m M 4 g T N w 4 R K a c A c t a A O B J 3 i G V 3 i z M u v F e r c +
F q M l q 9 g 5 g j + w P n 8 A W g S U o w = = < / l a t e x i t >
Hi Hi En/ CS En/ CS L DAE : Enc p0 Enc sh Dec sh Dec p0 ; Enc p1 Enc sh Dec sh Dec p1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 7 B R F 3 m o z r y I m V T T S 4 M D C q n C u Z p I = " > A A A C p X i c n V H L S g M x F M 2 M r 1 p f V Z d u g k V 0 V W Z U U H R T t Y I L B S t t F d o y Z N L b N j T z I L k j l m H + z K 9 w 5 9 + Y j g W 1 u v J C 4 H D O v S f 3 4 c d S a H S c d 8 u e m 1 9 Y X C o s F 1 d W 1 9 Y 3 S p t b L R 0 l i k O T R z J S T z 7 T I E U I T R Q o 4 S l W w A J f w q M / u p r o j 8 + g t I j C B o 5 j 6 A Z s E I q + 4 A w N 5 Z V e O w H D I W c y v c 2 8 t H Z x n X U Q X j A 9 o x n N J R W k 1 y E 3 W u w 5 s 5 Q e f j E 1 + J v 5 L M s 9 K T 2 n x r Y 4 6 + v + z 9 f N v F L Z q T h 5 0 N / A n Y I y m c a 9 V 3 r r 9 C K e B B A i l 0 z r t u v E 2 E 2 Z Q s E l Z M V O o i F m f M Q G 0 D Y w Z A H o b p p v O a N 7 h u n R f q T M C 5 H m 7 P e K l A V a j w P f Z E 4 6 1 b P a h P x L a y f Y P + 2 m I o w T B D N / / l E / k R Q j O j k Z 7 Q k F H O X Y A M a V M L 1 S P m S K c T S H L Z o l u L M j / w a t w 4 p 7 V D m s H 5 e r l 9 N 1 F M g O 2 S U H x C U n p E p u y D 1 p E m 7 t W j d W 3 X q w 9 + 0 7 u 2 G 3 P l N t a 1 q z T X 6 E 7 X 0 A i v v T r g = = < / l a t e x i t >
Synthetic CS text Apart from the use of parallel text and monolingual text employed in training TCS , we also construct large volumes of synthetic CS text using two simple techniques .
This synthetic CS text is nonparallel and is used to optimize both L DAE and L BT .
The role of the synthetic CS text is to expose TCS to various CS patterns ( even if noisy ) , thereby encouraging the model to code-switch .
The final step of finetuning using All - CS enables model to mimic switching patterns of real CS texts
The first technique ( named LEX ) is a simple heuristic - based technique that constructs a CS sentence by traversing a Hindi sentence and randomly replacing a word by its English translation using a bilingual lexicon ( Conneau et al. , 2017 ) .
The probability of replacing a word is chosen to match the switching distribution in real CS text .
The second technique ( named EMT ) is more linguistically aware .
Following the methodology proposed by Bhat et al . ( 2016 ) that is based on the embedded matrix theory ( EMT ) for code-switching , we apply clause substitution methods to monolingual text to construct synthetic CS text .
From inspecting English parse trees , we found that replacing embedded sentence clauses or subordinate clauses with their Hindi translations would likely produce CS text that appears somewhat natural .
Description of Datasets 4.1 A New Hindi-English CS Dataset
We introduce a new Hindi-English CS dataset , that we will refer to as All - CS .
It is partitioned into two subsets , Movie -CS and Treebank - CS , based on their respective sources .
Movie -CS consists of conversational Hindi-English CS text extracted from 30 contemporary Bollywood scripts that were publicly available .
3
The Hindi words in these sentences were all Romanized with potentially multiple non-canonical forms existing for the same Hindi token .
We employed a professional annotation company to convert the Romanized Hindi words into their respective backtransliterated forms rendered in Devanagari script .
We also asked the annotators to provide monolingual Hindi translations for all these sentences .
Using these monolingual Hindi sentences as a starting point , we additionally crowdsourced for CS sentences via Amazon 's Mechanical Turk ( MTurk ) ( Amazon , 2005 ) .
Table 1 shows two Hindi sentences from Movie-CS and Treebank - CS , along with the different variants of CS sentences .
Turkers were asked to convert a monolingual Hindi sentence into a natural - sounding CS variant that was semantically identical .
Each Turker had to work on five Hindi sentences .
We developed a web interface using which Turkers could easily copy parts of the Hindi sentence they wanted to retain and splice in English segments .
More details about this interface , the crowdsourcing task and worker statistics are available in Appendix A. All - CS comprises a second subset of CS sentences , Treebank - CS , that was crowdsourcing using MTurk .
We extracted 5292 monolingual Hindi sentences ( with sentence lengths less than or equal to 15 words ) from the publicly available Hindi Dependency Treebank that contains dependency parses .
4
These annotations parse each Hindi sentence into chunks , where a chunk is defined as
3158 that quantifies the amount of mixing between languages ( 0 denotes a purely monolingual corpus and 1 denotes equal mixing from both languages ) and I-Index measures the fraction of switching points in the corpus .
We observe Treebank - CS exhibits higher M-index and I-index values compared to Movie -CS indicating more code-switching overall .
All - CS also contains a non-trivial number of named entities ( NEs ) which are replaced by an NE tag in all our language modeling experiments .
Other Datasets Parallel Hindi-English
Text .
As described in Section 5 , TCS uses parallel text for supervised training .
For this purpose , we use the IIT Bombay English -Hindi Corpus ( Kunchukuttan et al. , 2017 ) containing parallel Hindi-English text .
We also construct a larger parallel corpus using text from the OpenSubtitles ( OpSub ) corpus ( Lison and Tiedemann , 2016 ) that is more conversational and hence more similar in style to Movie-CS .
We chose ~ 1 million English sentences ( OpSub - EN ) , where each sentence contained an embedded clause or a subordinate clause to support the construction of EMT lines .
We used the Google Translate API to obtain Hindi translations for all these sentences ( OpSub - HI ) .
Henceforth , we use OpSub to refer to this parallel corpus of OpSub-EN paired with OpSub-HI .
We extracted 318 K sentences from the IITB corpus after thresholding on length ( 5 - 15 ) and considering overlap in vocabulary with OpSub .
( One could avoid the use of an external service like Google Translate and use existing parallel text ) in conjunction with a word aligner to construct EMT lines .
OpSub , being more conversational in style , turns out to be a better pretraining corpus .
A detailed comparison of these choices is described in Appendix H. )
Datasets from existing approaches .
Synthetic ( I ) VACS ( Samanta et al. , 2019 ) is a hierarchical variational autoencoder - based model designed to generate CS text .
We train two VACS models , one on All - CS ( VACSv1 ) and the other on OpSub-EMT followed by All - CS ( VACSv2 ) .
( II ) Garg et al. ( 2018a ) use SeqGAN ( Yu et al. , 2017 ) - a GAN - based sequence generation model - to generate CS sentences by providing an RNNLM as the generator .
As with VACS , we train two SeqGAN 5 models , one on All - CS ( SeqGANv1 ) and one on OpSub-EMT followed by All - CS ( SeqGANv2 ) .
Samples are drawn from both SeqGAN and VACS by first drawing a random sample from the standard normal distribution in the learned latent space and then decoding via an RNN - based generator for SeqGAN and a VAE - based decoder for VACS .
We sample ~ 2 M lines for each dataset to match the size of the other synthetic datasets .
Experiments and Results First , we investigate various training curricula to train TCS and identify the best training strategy by evaluating BLEU scores on the test set of All - CS ( ?5.1 ) .
Next , we compare the output from TCS with synthetic CS text generated by other methods ( ?5.2 ) .
We approach this via language modeling ( ?5.2.1 ) , human evaluations ( ?5.2.2 ) and two downstream tasks - Natural Language Inference and Sentiment Analysis-involving real CS text ( ?5.2.3 ) .
Apart from these tasks , we also present four different objective evaluation metrics to evaluate synthetic CS text : BERTScore , Accuracy of a BERT - based classifier and two diversity scores ( ?5.3 ) .
Improving Quality of TCS Outputs
Table 3 shows the importance of various training curricula in training TCS ; these models are evaluated using BLEU ( Papineni et al. , 2002 ) C 2 against D 2 , we observe that OpSub-EMT is indeed a better choice for pretraining compared to OpSub- LEX .
Also , supervised finetuning with All - CS is clearly superior to unsupervised finetuning .
Henceforth , Systems D 1 and D 2 will be referred to as TCS ( U ) and TCS ( S ) , respectively .
While having access to parallel CS data is an advantage , we argue that the benefits of having parallel data only marginally increase after a threshold .
Figure 3 shows how BLEU scores vary when changing the amount of parallel CS text used to train D 2 .
We observe that BLEU increases substantially when we increase CS data from 1000 lines to 5000 lines , after which there is a trend of diminishing returns .
We also find that D 1 ( that uses the data in All - CS as non-parallel text ) is as good as the model trained using 4000 lines of parallel text .
Comparing TCS with Other Synthetic CS
Language Modeling
We use text generated by our model to train a language model ( LM ) and evaluate perplexities on the test set of All - CS to show how closely sentences from TCS mimic real CS text .
We use a state- of- the- art RNNLM model AWD - LSTM -LM Merity et al. ( 2018 ) as a blackbox LM and only experiment with different training datasets .
The model uses three LSTM layers of 1200 hidden units with weight tying and 300 - dimensional word embeddings .
In initial runs , we trained our language model on the large parallel / synthetic CS datasets and finetuned on the All - CS data .
However , this training strategy was prone to overfitting on All - CS data .
To counter this problem of forgetting during the pretrain- finetuning steps , we adopted the Mix-review strategy proposed by He et al . ( 2021 ) .
The training sentences from All - CS remain constant through the epochs and the amount of pretraining data is exponentially decayed with each epoch .
This greatly alleviates the forgetting problem in our model , and leads to better overall perplexities .
Additional details about these LMs are provided in Appendix E.
Human Evaluation
We evaluated the quality of sentences generated by TCS using a human evaluation study .
We sampled 150 sentences each , using both TCS ( U ) and TCS ( S ) , starting from monolingual Hindi sentences in the evaluation sets of All - CS .
The sentences were chosen such that they were consistent with the length distribution of All - CS .
For the sake of comparison , corresponding to the above-mentioned 150 monolingual Hindi samples , we also chose 150 CS sentences each from All - CS - LEX and All - CS -EMT .
Along with the ground - truth CS sentences from All - CS , this resulted in a total of 750 sentences .
6
These sentences were given to three linguistic experts in Hindi and they were asked to provide scores ranging between 1 and 5 ( 1 for worst , 5 for best ) under three heads : " Syntactic correctness " , " Semantic correctness " and " Naturalness " .
Table 5 shows that the sentences generated using TCS ( S ) and TCS ( U ) are far superior to the EMT and LEX sentences on all three criteria .
TCS ( S ) is quite close in overall quality to the real sentences and TCS ( U ) fares worse , but only by a small margin .
Table 6 shows some illustrative examples of code-switching using TCS ( U ) on test samples .
We also show some examples of code-switching 6
We only chose CS sentences from TCS that did not exactly match the ground - truth CS text .
within monolingual sentences from OpSub .
We observe that the model is able to introduce long contiguous spans of English words ( e.g. " meeting next week " , " but it is clear " , etc. ) .
The model also displays the ability to meaningfully switch multiple times within the same sentence ( e.g. , " i love you very much " , " but " , " friend " ) .
There are also interesting cases of English segments that appear to be ungrammatical but make sense in the CS context ( e.g. , " because i know main dish " , etc. ) .
Method
GLUECoS Benchmark GLUECoS ( Khanuja et al. , 2020 ) is an evaluation benchmark spanning six natural language tasks for code-switched English -Hindi and English - Spanish data .
The authors observe that M-BERT ( Pires et al. , 2019 ) OpSub-HI and select corresponding LEX , EMT and TCS ( S ) sentences .
M-BERT is then trained using the masked language modelling ( MLM ) objective on text from all 4 systems ( including OpSub - HI ) for 2 epochs .
We also train M-BERT on 21 K sentences from All - CS ( real CS ) .
Finally , these pretrained models are fine-tuned on the selected GLUECoS tasks .
( More details are in Appendix G. )
Table 7 lists the accuracies and F1 scores using different pretraining schemes for both NLI and sentiment analysis , respectively .
Plain monolingual pretraining by itself leads to performance improvements on both tasks , presumably due to domain similarity between GLUECoS ( movie scripts , social media etc. ) and OpSub .
As mentioned in Khanuja et al . ( 2020 ) , pretraining on CS text further improves performance for both NLI and SA .
Among the synthetic methods , TCS ( S ) has consistently better scores than LEX and EMT .
For SA , TCS ( S ) even outperforms pretraining on real CS text from All - CS .
Other Objective Evaluation Metrics BERTScore .
BERTScore is a recently - proposed evaluation metric for text generation .
Similarity scores are computed between each token in the candidate sentence and each token in the reference sentence , using contextual BERT embeddings ( Devlin et al. , 2018 ) of the tokens .
We use this as an additional objective metric to evaluate the quality of the sentences generated using TCS .
We use the real monolingual sentence as the reference and the generated CS sentence as the candidate , excluding sentences from TCS ( S ) and TCS ( U ) that exactly match the real sentence .
Since our data is Hindi-English CS text , we use Multilingual BERT ( M- BERT ) ( Pires et al. , 2019 ) for high-quality multilingual representations .
Table 8 outlines our main results on the test set of All - CS .
TCS sometimes generates purely monolingual sentences .
This might unfairly tilt the scores in favour of TCS since the reference sentences are also monolingual .
To discount for such biases , we remove sentences generated by TCS ( U ) and TCS ( S ) that are purely monolingual ( Row label " Mono " in BERTScore ) .
Sentences having < UNK > tokens ( labeled " UNK " ) are also filtered out since these tokens are only generated by TCS for out - of- vocabulary words .
" UNK & Mono " refers to applying both these filters .
EMT lines consistently show the worst performance , which is primarily due to the somewhat poor quality of translations involved in generating these lines ( refer to Appendix B ) .
With removing both monolingual and < UNK > tokens , we observe that TCS ( U ) and TCS ( S ) yield the highest BERTScores , even outperforming the BERTScore on real data obtained from the Turkers .
BERT - based Classifier .
In this evaluation , we use M-BERT ( Pires et al. , 2019 ) to build a classifier that distinguishes real CS sentences from synthetically generated ones ( fake ) .
When subject to examples from high-quality generators , the classifier should find it hard to tell apart real from fake samples .
We add a fully connected layer over the M-BERT base architecture that takes the [ CLS ] token as its input to predict the probability of the sentence being real or fake .
Fake sentences are drawn from the union of TCS ( U ) , TCS ( S ) , All - CS - LEX and All - CS -EMT .
In order to alleviate the class imbalance problem , we oversample the real sentences by a factor of 5 and shuffle the data .
The model converges after training for 5 epochs .
We see in Table 8 that the classification accuracy of whether a sample is fake or not is lowest for the outputs from TCS among the different generation techniques .
Measuring Diversity .
We are interested in finding out how diverse the predictions from TCS are .
We propose a simple measure of diversity in the CS variants that is based on how effectively sentences can be compressed using the gzip utility .
7
We considered using Byte Pair Encoding ( BPE ) ( Gage , 1994 ) as a measure of data compression .
However , BPE operates at the level of individual words .
Two word sequences " w1 w2 w3 " and " w3 w2 w1 " would be identically compressed by a BPE tokenizer .
We would ideally like to account for such diversity and not discard this information .
gzip uses Lempel - Ziv coding ( Ziv and Lempel , 1977 ) that considers substrings of characters during compression , thus allowing for diversity in word ordering to be captured .
Our diversity measure D is simply the following :
For a given set of CS sentences , run gzip on each sentence individually and sum the resulting file sizes ( S 1 ) .
Next , paste all the CS sentences into a single file and run gzip on it to get a file of size S 2 .
Then , D = S 1 ? S 2 . Smaller D scores indicate larger diversity .
If the variants of a sentence are dissimilar to one another and hence very diverse , then S 2 would be large thus leading to smaller values of D.
Table 8 shows the diversity scores for different techniques .
Both TCS ( S ) and TCS ( U ) have a higher diversity score compared to LEX and EMT .
TCS ( U ) exceeds even the responses received via MTurk ( Real ) in diversity .
We note here that diversity , by itself , is not necessarily a desirable trait .
Our goal is to generate sentences that are diverse while being natural and semantically meaningful .
The latter properties for text from TCS ( S ) and TCS ( U ) have already been verified in our human evaluation study .
Zhu et al . ( 2018 ) propose self - BLEU score as a metric to evaluate the diversity of generated data .
With Movie- CS , since there were no chunk labels associated with the sentences , they were tokenized into words .
A MTurk Task Details On MTurk , we selected workers with HIT approval rate of 90 % and location restricted to countries with significant Hindi speakers - Australia , Bahrain , Canada , India , Kuwait , Malaysia , Mauritius , Myanmar , Nepal , Netherlands , New Zealand , Oman , Pakistan , Qatar , Saudi Arabia , Singapore , South Africa , Sri Lanka , Thailand , United Arab Emirates , United Kingdom , United States of America .
It was clearly specified in the guidelines that the task must be attempted by native Hindi speakers .
Each response was manually checked before approving .
Turkers were paid $ 0.15 for working on 5 sentences ( roughly takes 3 - 4 minutes ) .
This amounts to $ 2.25 - $ 3/ hr which is in the ballpark of a median hourly wage on MTurk of ~ $ 2/hr ( Hara et al. , 2018 ) .
B EMT lines generation Following the methodology described in ( Bhat et al. , 2016 ) , we apply clause substitution methodology to produce EMT sentences .
To create OpSub-EMT , we start with the gold English sentence that contains either embedded sentence clauses ( S ) or subordinate clauses ( SBAR ) and swap one or more of them with their Hindi translations to produce an EMT synthetic CS sentence .
Due to the lack of gold English translations available for All - CS sentences , we used the Google Translate API to first acquire their English translation .
Many of the sentences in All - CS are shorter in length and do not contain the abovementioned clauses .
So , we also considered inverted declarative sentence clauses ( SINV ) , inverted question clauses ( SQ ) and direct question clauses ( SBARQ ) in addition to S and SBAR .
In case none of the clause level tags were present , we considered the following phrase level tags as switching candidates : Noun Phrase ( NP ) , Verb Phrase ( VP ) , Adjective Phrase ( ADJP ) and Adverb Phase ( ADVP ) .
Owing to the shorter length and lack of clauselevel tags , we switch only one tag per sentence for All - CS - EMT .
The choice of which clause to switch was made empirically by observing what switches caused the resulting sentence to resemble a naturally occurring CS sentence .
One can also use the toolkit provided by Rizvi et al . ( 2021 ) for generating EMT lines .
C Implementation Details : TCS
As an initialisation step , we learn the token embeddings ( Mikolov et al. , 2013 ) on the same corpus using skipgram .
The embedding dimension was set to be 256 and the encoder-decoder layers share these lookup tables .
Adam optimiser with a learning rate of 0.0001 was used to train the model .
Validation BLEU scores on ( HI ? ENG / CS ) translations and ( EN ? HI ? EN ) reconstructions were used as metrics to save the best model for TCS ( S ) and TCS ( U ) , respectively .
D Human Evaluation
The 150 samples evaluated in Table 5 were taken entirely from test / validation splits .
We undertook an alternate human evaluation experiment involving 100 real CS sentences and its corresponding CS sentences using LEX , EMT , TCS ( U ) and TCS ( S ) .
Out of these 100 sentences , 40 of them came entirely from the test and validation splits and the remaining 60 are training sentences which we filtered to make sure that sentences generated by TCS ( S ) and TCS ( U ) never exactly matched the real CS sentence .
The table below ( Table 9 ) reports the evaluations on the complete set of 100 sentences from 5 datasets .
We observe that the trend remains exactly the same as in Table 5 , with TCS ( S ) being very close to real CS sentences in its evaluation and TCS ( U ) trailing behind TCS ( S ) .
E Language Model Training
The AWD -LSTM language model was trained for 100 epochs with a batch size of 80 and a sequence length of 70 in each batch .
The learning rate was set at 30 .
The model uses NT - ASGD , a variant of the averaged stochastic gradient method , to update the weights .
The mix-review decay parameter was set to 0.9 .
This implies that the fraction of pretraining batches being considered at the end of n epochs is 0.9 n , starting from all batches initially .
Two decay coefficients { 0.8 , 0.9 } were tested and 0.9 was chosen based on validation perplexities .
F Code-switching examples
The sentences in Table 10 have been generated on the test and validation splits of All - CS as well as the OpSub dataset .
Overall , they depict how the model is able to retain context over long sentences ( e.g. " and social sectors " ) and perform meaningful switching over large spans of words ( e.g. " old conversation writer media " , " regularly security practices " ) .
We also note that at times , the model uses words which are different from the natural English translations of the sentence , which are appropriate within the context of a CS sentence ( e.g. the use of " manage " instead of " manageable " ) .
G Details of GLUECoS Experiments
For masked language modeling ( MLM ) , we select the default parameters for the learning rate ( 5e - 5 ) , batch masking probability ( 0.15 ) , sequence length ( 512 ) .
The models are trained for 2 epochs with a batch size of 4 and gradient accumulation step of 10 .
For task specific fine tuning we rely on the official training scripts provided by GLUECoS repository .
8
We train the models for 5 seed ( 0,1,2,3 and 4 ) and report mean and standard deviations of Accuracy and F1 for NLI and Sentiment Analysis respectively
H Additional Dataset and Experiments Dataset
The additional corpus on which experiments were performed is OPUS - 100 which was sampled from the original OPUS corpus ( Tiedemann , 2012 ) .
The primary difference between OpSub and OPUS - 100 is that OpSub does not have manual Hindi translations of its sentences and requires the use of an external API such as Google Translate for translation .
However , OPUS - 100 has manually annotated sentences as part of the corpus .
The source of OPUS - 100 ranges from movie subtitles to GNOME documentation to the Bible .
We extract 340 K sentences from OPUS - 100 corpus after thresholding on length ( 5 - 15 ) .
We offer this comparison of systems trained on OpSub and OPUS - 100 to show how our models fare when using two datasets that are very different in their composition .
LEX lines generation .
Generation of LEX lines is straightforward and requires only a bilingual lexicon .
For each monolingual Hindi sentence we generate ~ 5 sentences on OPUS - 100 resulting in OPUS - 100 - LEX ( to roughly match the size of OpSub - LEX ) .
EMT lines generation .
For generation of EMT lines we have two strategies depending on the availability of tools ( parsers , translation service , aligners , etc ) .
The first strategy requires a translation service ( either in- house or publicly available ) .
We substitute the embedded clause from parse trees of English sentences with their Hindi translations .
This strategy does not require a parallel Hindi corpus and has been previously used for generating OpSub-EMT and All -CS-EMT ( Described in detail in Appendix B ) .
The second strategy , that is used to generate OPUS - 100 - EMT , requires a parallel corpus , a constituent parser in English and a word aligner between parallel sentences .
OPUS - 100 sentences are aligned using SimAlign ( Jalili Sabet et al. , 2020 ) and embedded clauses from parse trees of English sentences are replaced by Hindi clauses using word aligners .
Here again , for each monolingual Hindi sentenece we generate ~ 5 EMT sentences ( strategy - 2 ) on OPUS - 100 resulting in OPUS - 100 - EMT .
Curriculum Training Experiments .
Table 11 provides a walkthrough of systems using various training curricula that are evaluated for two different choices of datasets - OpSub vs OPUS - 100 differing in the generation of EMT lines .
The models are evaluated using BLEU ( Papineni et al. , 2002 ) scores computed on the test set of All - CS .
The vo- 3 are replicated here for ease of comparison .
cabulary is generated by combining train sets of all datasets to be used in the curricula .
It is 126,576 when X = OpSub and 164,350 when X = OPUS - 100 ( OpSub shows a higher overlap in vocabulary with All - CS compared to OPUS - 100 ) .
The marginal difference in System O for OpSub and OPUS - 100 is attributed to differences in the size of the vocabulary .
OpSub being conversational in nature , is a better pretraining corpus compared to OPUS - 100 as seen from System A , the sources of the latter being GNOME documentations and The Bible , apart from movie subtitles .
The results for C 1 , C 2 , D 1 , D 2 are consistently better when X = OpSub versus when X = OPUS - 100 .
We choose to highlight four models from Table 11 which together demonstrate multiple use-cases of TCS in Table 12 . TCS ( LEX ) refers to ( C 2 , X=OpSub ) , TCS ( U ) refers to ( D 1 , X=OpSub ) , TCS ( S ) refers to ( D 2 , X=OpSub ) and TCS ( simalign ) refers to ( D 2 , X=OPUS - 100 ) .
Language Modelling Experiments .
Table 13 shows results from LM experiments ( using the same setup as in Section 5.2.1 ) .
The values for TCS ( S ) and TCS ( U ) have been reproduced here for ease of comparison .
( Note that TCS ( simalign ) does not perform as well as the other models since the sentences for training the language model are generated on OpSub for all the models here , but TCS ( simalign ) has been trained on OPUS - 100 . ) Evaluation Metrics .
Table 14 shows the results of the three objective evaluation metrics on the additional TCS models .
In comparison with the results in Table 8 , we observe that TCS ( LEX ) and TCS ( simalign ) perform comparably to TCS ( S ) and TCS ( U ) on all metrics .
t e x i t s h a 1 _ b a s e 6 4 = "
Q e 5 F C 3 z 9 z Y m V n r v 2 5 Z e S n b K 6
Figure 1 : 1 Figure 1 : Model architecture .
Each loss term along with all the network components it modifies are shown .
During unsupervised training with non-parallel text , LDAE and LBT are optimized while for supervised training with parallel text , LDAE and LCE are optimized .
Figure 2 : 2 Figure 2 : Distribution across overall sentence lengths and distribution across lengths of continuous English spans in Movie -CS and Treebank - CS .
Figure 3 : Variation of BLEU score with amount of All - CS parallel training data .
( a ) BERTScores on test split of All - CS .
Each row corresponds to a different data filter .
The numbers in parenthesis denote the number of sentences in the data after filtering .
( b) Accuracies from the classifier for samples generated by various methods as being fake .
The | Sentences | refer to size of dataset for each system .
TCS models have the lowest accuracy among synthetic methods .
( c ) Diversity Scores for different techniques using Gzip and Self - BLEU based diversity measures .
Figure 4 : 4 Figure 4 : A snapshot of the web interface used to collect Movie -CS and Treebank - CS data via Amazon Mechanical Turk .
Figure 4 4 Figure4depicts the portal used to collect data using Amazon 's Mechanical Turk platform .
The collection was done in two rounds , first for Movie -CS and then for Treebank - CS .
With Treebank - CS , the sentences were first divided into chunks and the Turkers were provided with a sentence grouped into chunks as shown in Figure4 .
They were required to switch at least one chunk in the sentence entirely to English so as to ensure a longer span of English words in the resulting CS sentence .
A suggestion box converted transliterated Hindi words into Devanagari and also provided English suggestions to aid the workers in completing their task .
With Movie- CS , since there were no chunk labels associated with the sentences , they were tokenized into words .
On MTurk , we selected workers with HIT approval rate of 90 % and location restricted to countries with significant Hindi speakers - Australia , Bahrain , Canada , India , Kuwait , Malaysia , Mauritius , Myanmar , Nepal , Netherlands , New Zealand , Oman , Pakistan , Qatar , Saudi Arabia , Singapore , South Africa , Sri Lanka , Thailand , United Arab Emirates , United Kingdom , United States of America .
It was clearly specified in the guidelines that the task must be attempted by native Hindi speakers .
Each response was manually checked before approving .
Turkers were paid $ 0.15 for working on 5 sentences ( roughly takes 3 - 4 minutes ) .
This amounts to $ 2.25 - $ 3/ hr which is in the ballpark of
) Requires parser along with parallel data Alignment can be generated using SimAlign
Table 1 : 1 ? ? ? life ? ? actually MTurk ? laughter therapy ? ? ? ? ? ? ? ?
MTurk but laughter therapy ? really ? ? ? life change ? ? MTurk ? ? ? therapy ? ? ? life ? ? ? ?
Two All - CS examples .
English translations in blue .
Movie-CS ? ? ? ? ? ? ? ? ? ? ? ? ? ( Eng ) ( But laughter medicine really changed my life ) ( Gold ) but laughter therapy Treebank - CS ? ? ? ? 7.20 ? ? ? ? ?
( Eng ) ( Income from the fair was estimated at Rs 7.20 crore ) MTurk fair ?
income 7.20 ? ? evaluate ? ? MTurk ? ? ? income 7.20 ? ? ? ? ?
Table 2 : 2 Table2 provides detailed statistics of the new CS dataset .
We also report two metrics proposed by Guzm ?n et al. ( 2017 ) to measure the amount of code-switching present in this new corpus .
Monolingual Index ( M- Index ) is a value between 0 and 1 Key statistics of CS datasets .
Quantity / Metric Movie-CS Treebank - CS All - CS | Train | 15509 5914 21423 | Test | 1500 1000 2500 | Valid | 500 500 1000 # Tokens 196300 87979 284279 # Hindi Sentences 9290 5292 14582 # NEs 4342 4810 9152 Fraction of NEs 0.0221 0.0547 0.0322 M-Index 0.5542 0.6311 0.5774 I-Index 0.2852 0.3434 0.3023
CS Datasets .
As mentioned in Section 3.1 , we use two simple techniques LEX and EMT to generate synthetic CS text , which in turn is used to train TCS in an unsupervised training phase .
For each Hindi monolingual sentence in OpSub , we generate two LEX and two EMT synthetic CS sentences giving us OpSub-LEX and OpSub-EMT , respectively .
We also generate five LEX and five EMT lines for each monolingual sentence in All - CS .
In order to generate EMT lines , we first translate the monolingual Hindi sentences in All - CS to English using Google Translate and then follow the EMT generation scheme .
This results in two datasets , All - CS - LEX and All - CS - EMT , which appear in later evaluations .
( Appendix B contains more details about EMT applied to OPUS and All - CS . )
Table 4 4 shows test perplexities using different training curricula and data generated using two prior approaches , VACS and SeqGAN .
Sentences generated using TCS yield the largest reductions in test perplexities , compared to all other approaches .
Pretraining Corpus | Train | Test PPL Test PPL OpSub All- CS OpSub + OpSub-LEX 4.00M 56.83 332.66 OpSub + OpSub-EMT 4.03 M 55.56 276.56 OpSub + VACSv1 4.05 M 64.77 335.79 OpSub + VACSv2 4.05 M 62.41 321.12 OpSub + SeqGANv1 4.03 M 57.32 336.62 OpSub + SeqGANv2 4.03 M 56.50 317.81 OpSub + TCS ( U ) 3.99 M 57.45 271.19 OpSub + TCS ( S ) 3.96 M 56.28 254.37
Table 4 : Test perplexities on All - CS using different pretrain - ing datasets .
? ? notice ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
( No i really love you but just like a friend ) ? ?
i love you very much ? ? but ? ? friend ? ? Generated using MovieCS ? ? ? ? ? ? ? ? ? ?
( I am glad you noticed ) i am happy Generated using TreebankCS ? ? ? ? ? ? ? ? ?
( Meeting will likely be next week ) meeting next week ? ? possibility ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
( He said that it would not be appropriate to name them but it is clear ) ? ? ? ? ? ? ? fair ? ? ? but it is clear Generated using OpSub ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
( You have to give yourself time to process those feelings within you ) Syntactic Semantic Naturalness Real 4.47?0.73 4.47?0.76 4.27?1.06 TCS ( S ) 4.21?0.92 4.14?0.99 3.77?1.33 TCS ( U ) 4.06?1.06 4.01?1.12 3.58?1.46 EMT 3.57?1.09 3.48?1.14 2.80?1.44 LEX 2.91?1.11 2.87?1.19 1.89?1.14
Table 5 : Mean and standard deviation of scores ( between 1 and 5 ) from 3 annotators for 150 samples from 5 datasets .
? ? ? ? emotions ? process ? ? ? ? ? ? time ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
( Because i know what the main dish will be ) because i know main dish ? ?
Table 6 : 6 Examples generated by TCS ( U ) on validation and test data .
For each example the first line is the monolingual sentence , followed by its English translation and finally the translation from TCS ( U ) .
More examples are in Appendix F.
Table 8 : 8 Evaluation Metric Real LEX EMT TCS ( S ) TCS ( U ) All ( 3500 ) 0.812 0.796 0.627 0.764 0.788 BERTScore Mono ( 3434 ) UNK ( 1983 ) 0.812 0.809 0.782 0.804 0.623 0.636 0.755 0.827 0.772 0.846 UNK & Mono ( 1857 ) 0.808 0.785 0.633 0.813 0.821 BERT - based Classifier | Sentences | Accuracy ( fake ) 4767 42.76 12393 96.52 12484 97.83 12475 80.31 12475 88.62 Diversity Gzip ( D ) Self-BLEU 22.13 61.3 24.12 29.7 33.17 24.6 21.37 63.6 17.59 64.2
Table 9 : 9 Mean and standard deviation of scores ( between 1 and 5 ) from 3 annotators for 100 samples from 5 datasets .
Method Syntactic Semantic Naturalness Real 4.36?0.76 4.39?0.80 4.20?1.00 TCS ( S ) 4.29?0.84 4.30?0.89 4.02?1.16 TCS ( U ) 3.96?1.06 3.93?1.13 3.52?1.45 EMT 3.47?1.25 3.53?1.23 2.66?1.49 LEX 3.10?2.16 3.05?1.35 2.01?1.32
Table 11 : 11 BLEU score on ( HI CS ) for different curricula measured on All - CS ( test ) .
X | Y represents starting with model X and further training using dataset Y. Values from Table Curriculum X=OpSub X=OPUS -100 O All - CS ( S ) 19.18 19.14 A IITB + X ( S ) 1.51 0.29 B A | All - CS ( S ) 27.84 25.63 C A | X- HI + X - LEX ( U ) 15.23 14.17 C1 C | All - CS ( U ) 32.71 31.48 C2 C | All - CS ( S ) 39.53 37.51 D A | X- HI + X-EMT ( U ) 17.73 15.03 D1 D | All - CS ( U ) 35.52 33.91 D2 D | All - CS ( S ) 43.15 40.32
Table 12 : 12
Use cases for different TCS models .
3169
Table 13 : 13
Test perplexities on OpSub and All - CS using different pretraining datasets .
Table 14 : 14 Evaluation metrics for the additional TCS models .
Please see Table8 for a comparison with other models .
Evaluation Metric TCS ( LEX ) TCS ( simalign ) All ( 3500 ) 0.773 0.768 BERTScore Mono ( 3434 ) UNK ( 1983 ) 0.769 0.832 0.753 0.829 UNK & Mono ( 1857 ) 0.817 0.822 BERT - based | Sentences | 12475 12475 Classifier Accuracy ( fake ) 84.17 82.98 Diversity Gzip ( D ) Self-BLEU 19.62 56.3 19.83 59.8
Given the non-trivial effort involved in collecting annotations from professional annotators and crowd workers , we focused on a single language pair ( Hindi-English ) and leave explorations on more language pairs for future work .
The new dataset and relevant code is available at : https://www.cse.iitb.ac.in/~pjyothi/TCS.
https://www.filmcompanion.in/category/fc-pro/scripts/
https://moifightclub.com/category/scripts/ 4 http://ltrc.iiit.ac.in/treebank_H2014/
https://github.com/suragnair/seqGAN
http://www.gzip.org/
However , using self - BLEU is slightly problematic in our setting as systems like LEX that switch words at random positions would result in low self - BLEU ( indicating high diversity ) .
This is indeed the case , as shown in Table8 - LEX , EMT give lower self - BLEU scores as compared to TCS .
However , note that the scores of the TCS models are comparable to that of real CS data.6 Conclusions
In this work , we present a neural translation model for CS text that transduces monolingual Hindi sentences into realistic Hindi-English CS text .
Text generated by our model is evaluated using a number of different objective metrics , along with LM , NLI and sentiment analysis tasks , and a detailed human evaluation study .
The role of synthetic data in training such models merits a more detailed investigation which we leave for future work .
https://github.com/microsoft/GLUECoS
