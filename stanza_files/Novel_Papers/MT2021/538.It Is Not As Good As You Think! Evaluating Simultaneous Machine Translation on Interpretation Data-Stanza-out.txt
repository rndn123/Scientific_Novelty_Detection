title
It is Not as Good as You Think !
Evaluating Simultaneous Machine Translation on Interpretation Data
abstract
Most existing simultaneous machine translation ( SiMT ) systems are trained and evaluated on offline translation corpora .
We argue that SiMT systems should be trained and tested on real interpretation data .
To illustrate this argument , we propose an interpretation test set and conduct a realistic evaluation of SiMT trained on offline translations .
Our results , on our test set along with 3 existing smaller scale language pairs , highlight the difference of up - to 13.83 BLEU score when SiMT models are evaluated on translation vs interpretation data .
In the absence of interpretation training data , we propose a translationto-interpretation ( T2I ) style transfer method which allows converting existing offline translations into interpretation - style data , leading to up - to 2.8 BLEU improvement .
However , the evaluation gap remains notable , calling for constructing large-scale interpretation corpora better suited for evaluating and developing SiMT systems .
1
Introduction Simultaneous interpretation ( SI ) is a task of translating natural language in real time .
SiMT systems are expected to generate interpreted text as if the text was produced by human interpreters while maintaining acceptable delay ( Ma et al. , 2019 ; Arthur et al. , 2021 ) .
However , most current SiMT systems are trained and evaluated on offline translations differing from real - life SI scenarios where translations are flexibly paraphrased , without compromising the source message ( He et al. , 2016 ; Paulik and Waibel , 2009 ) .
For instance , in Table 1 the interpretation sentence drops " at this point " and condenses " seriousness of this line of argument " to " agreement " ; it delivers the source message as reliably as the offline translation .
Prior work attempted to build interpretation corpora in a small scale ( Tohyama and Inagaki , 2004 ; Shimizu et al. , 2014 ; Bernardini et al. , 2016 ) , or constructed speech interpretation training corpora for MT tasks ( Paulik and Waibel , 2010 ) .
But , very little attempt has been made on empirically quantifying the evaluation gap .
An exception is Shimizu et al . ( 2013 ) which incorporated interpretation data in the training stage of a statistical MT system , but the lack of training data and the scale of evaluation set resulted in a marginal BLEU score difference .
2
We compile a genuine interpretation test set of 1 k utterances from the European Parliament ( EP ) Plenary focusing on German ?
English .
We examine the real performance gap of wait -k ( Ma et al. , 2019 ) , a state - of- the - art SiMT system , on our test set along with 3 smaller scale ( Bernardini et al. , 2016 ) translation and interpretation language - pairs and observe a drop of up - to 13.83 BLEU score .
In the absence of interpretation -style training data , we propose a simple and effective translation - tointerpretation ( T2I ) style transfer method to produce pseudo-interpretations from abundant offline translations .
Training on our T2I transferred data , we observe an improvement of ?2.8 BLEU score .
Our findings necessitate further developments towards constructing large-scale interpretation corpora , designing domain adaptive techniques and models more reflective of real-life interpretations .
German ?
English Interpretation Data
We provide an overview of our data construction and move full details in Appendix A.1 . Collection .
We crawled data from the EP Plenary 3 between 2008 and 2012 4 and downloaded 238 debates consisting of speech transcriptions , offline translations and interpretation videos .
We used Google speech API to transcribe the interpretation videos and normalize automatic speech recognition ( ASR ) outputs , yielding 323 - hour of transcriptions .
Cleaning , Alignment , and Segmentation .
We removed duplicates and the dialogues with non-German source sentences , while using available offline translations to retrieve named entities ; this resulted in 5,239 dialogues .
We filtered out dialogues with interpretations less than 4 words , and call the resulting interpretations Raw hereafter .
We further removed cases whose sources contained either ( 1 ) less than 20 tokens , ( 2 ) less than 150 words and included pre-defined signals , or ( 3 ) a different number of sentences from the corresponding offline translations .
and whose sources and offline translations had a different number of sentences .
Next a manual process was applied , including removals of dialogues with non-essential contents and truncation of interpretations whose first and last sentences did not match the corresponding offline translations ( mostly due to imperfect audio segmentation ) .
987 dialogues 5 were thus retained , each of which having 14.5 sentences on average .
We aligned translations with transcriptions ( interpretations ) .
For each dialogue , as the transcriptions may not be well segmented in the ASR process , we identified sentences in the transcriptions with stanza ( Qi et al. , 2020 ) , before segmenting them using dynamic programming .
Manual inspection revealed that there were a portion of mismatched pairs , which was due to occasional interpreting failure resulting from interpreters ' accumulated cognitive load ( Mizuno , 2017 ; Sudoh et al. , 2020 ) .
We further removed pairs the lengths of whose source and target were far off , and call it Clean , containing triples < source , translation , interpretation >.
Translation and Interpretation Test Sets .
To ensure the quality of interpretation data for evaluation , we hired a bilingual German - English speaker to annotate a randomly selected subset ( 107 dialogues ) of the 987 dialogues in two stages : segmentation and ASR error correction .
This gave us two versions of test set : Interpretation ASR , Interpretation .
In the first stage , the annotator was asked to match the correct target sentence ( s ) against each source sentence .
The annotator was asked to find interpretation text for each German sentence , when impossible , multiple sentences were allowed .
Additionally , to comply with human speaking styles , we allowed minor omissions of unimportant English texts as long as the main idea of German text was conveyed ( such as conjunctions ) .
In the second stage , the annotator was instructed to correct ASR errors while applying minimal changes to the sentences .
Ultimately , our test sets comprise 1,090 triples of < source , translation , interpretation > which were further cross checked to enforce quality control .
T2I Style Transfer
Offline translated texts and online interpreted texts differ in various aspects , including lengths , sentence structure and lexicon ; this is fundamentally contributed by the fact that interpreters use tactics to minimize delay and reduce the load of retention ( Mizuno , 2017 ; Camayd - Freixas , 2011 ) .
For example , interpreters tend to break a source sentence into several smaller chunks ( see He et al . ( 2016 ) for more tactics ) .
Yet , while exhibiting stylistic differences , both preserve the key source message .
As will be seen ( ?4.3 ) these differences amount to a significant evaluation gap .
While the ideal solution is using human annotators to create interpretation training corpora , in the absence of resources , we propose a simple technique , T2I style transfer , to convert existing translation data into interpretation -style data with a style transfer model .
Training such a model would require paired translations and interpretations , which are not available in large quantities .
Rather , our proposed approach allows fulfilling the goal of style transferring abundant translation data to interpretation - like data both in supervised settings , where Clean is leveraged , and unsupervised settings , where only Raw is used .
Supervised training Given that our Clean set consists of roughly 4.2 k triples , we opt for statistical MT systems which inherently require far less data for sequence - to-sequence mapping tasks compared to their neural counterparts .
Furthermore , conducting style transfer in the same language involves word replacement and ordering , which conforms with the behaviors of SMT systems that chunk an input sequence into segments , translate , and reorder the translated chunks ( Lopez , 2008 ) .
More specifically , we employ two classic statistical MT methods : phrase - based SMT ( PBMT ) ( Koehn et al. , 2003 ) and Hierarchical phrase - based MT ( HPBMT ) ( Chiang , 2005 ) .
6 A similar framework was tried by Xu et al . ( 2012 ) for text simplification .
We will describe the T2I pipeline process for unsupervised settings , as both settings have a similar process with different data configurations .
The main difference is that we use Clean instead of Raw , which will be detailed in ?4.1 .
Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings : the first stage is to convert interpretations in Raw to translation -style data by applying roundtrip translation on interpretations , with pretrained NMT models .
It is expected that the outputs after this round-tripping , denoted as Translation - FB , sit close to the translation domain , thus achieving the effects of interpretationto-translation .
The second stage is to train a style transfer model to learn the mapping between the data points in Translation - FB and their corresponding interpretations in Raw .
Lastly , we apply the trained style transfer model on offline Europarl translations and produce interpretation - like sequences which we call Pseudo -I. 7
Experiments
In this section , we present datasets details ( ?4.1 ) followed by the descriptions of our baselines and style transfer models ( ?4.2 ) .
We report results by underlining the performance gap between evaluation on translated and interpreted texts ( ?4.3 ) , and showing the effectiveness of our T2I style transfer both quantitatively and qualitatively ( ?4.4 ) .
We followed the instructions in Arthur et al . ( 2021 ) to preprocess data , and their hyperparameters for training all wait -k models .
For style transfer models , we used the standard setup for both PBMT and HPBMT .
8
Datasets
We conducted evaluation investigation on four languages pairs , including German ( DE ) , French ( FR ) , Polish ( PL ) , Italian ( IT ) ? English ( EN ) , and used Europarl v7 corpus ( Koehn , 2005 ) for training a SiMT model for each pair ( see Table 2 for data statistics ) .
For DE - EN , our annotated test set has 1,051 triples , for Interpretation ASR and Interpretation .
For the rest , we used EPTIC ( Bernardini et al. , 2016 ) , a small-scale parallel corpus with data collected from the EP Plenary ; it has source languages of FR , PL and IT , with 675 , 463 and 480 instances , respectively .
In the experiments of bridging the evaluation gap , Raw has 120,114 and 1,000 utterances for training and dev sets , while Clean has 4,240 triples , all used for training style transfer models .
To train PBMT , we augmented Clean by forward translating its source-side data to the target language , together with EPTIC , while using EPTIC to select the best weights for PBMT .
We deployed the trained style transfer models on translations of Europarl ( DE - EN ) to get Pseudo -I .
Pairing it with source sentences of Europarl gives us style transferred Europarl .
Model Baseline
We used wait -k ( with k=3 ) as SiMT systems for its simplicity and effectiveness ( Ma et al. , 2019 ) .
We compared the following wait -k baselines : i ) trained on Europarl ; ii ) adapted on Raw .
Performance was evaluated by BLEU 9 , average proportion ( AP ) and lagging ( AL ) ( Cho and Esipova , 2016 ; Ma et al. , 2019 ) percentage of read source tokens for every generated target token , while AL measures the number of lagged source tokens until all source tokens are read .
Style Transfer Models
In supervised settings , we used PBMT and HPBMT ; in unsupervised settings we only used HPBMT , as PBMT requires additional paired data to find the best weights .
We deployed Moses ( Koehn et al. , 2007 ) for above systems .
We also experimented with a Seq2Seq ( unsupervised ) model to compare .
Performance Gap
We train separate wait -k models for the four language pairs and report the evaluation results on their corresponding Translation Test and Interpretation Test 10 in Table 2 .
The observed significant gap of up - to 13.83 BLEU score ( 24.47 vs 10.64 for IT ) highlights the daunting task SiMT models face in real- life SI .
Interestingly , the gap for DE -EN is the lowest , and this is likely to be due to the fact that both are Germanic languages .
We explored the feasibility of narrowing the performance gap using our T2I method on DE-EN .
Being a head-final language , German is more difficult to interpret than head-initial languages ( e.g. , EN , FR , IT and PL ) , and interpreters must hold information until verb phrases are heard ( Mizuno , 2017 ) .
Furthermore , having created the datasets for German , our experimental setup was year / domainconsistent for training the baselines and styletransfer models , which allows us to isolate if the improvement was purely achieved by our T2I transfer method .
Full results are reported in Table 3 .
When wait -k was adapted on Source - FB , Raw , the lowest delay was seen , implying using interpretation corpora is effective in reducing delay .
Translation quality can be further boosted with our style transfer method , as discussed next .
The report also explicitly welcomes the Commission 's proposal for a horizontal directive covering all forms of discrimination .
The report also expressly welcomes the Commission 's proposal for a horizontal directive on the subject of anti-discrimination .
The report welcome the commission 's proposal for a horizontal directive on antidiscrimination legislation .
Table 4 : Examples of translation predicted by wait -k and translation predicted by a style transferred model , along with their source sentences and gold-translation .
Impacts of T2I Style Transfer Quantitative Analysis
Our approach yields significantly better results on Interpretation ASR compared to baselines .
Our best model outperformed pre-trained wait -k by 2.79 BLEU score .
On Interpretation , we see a similar trend but with a smaller margin .
We speculate the drop occurred because the T2I models were trained on ASR outputs , which is in the same domain as targets of Interpretation ASR .
Nevertheless , all T2I models work consistently well in supervised and unsupervised settings .
Moreover , our approach surpasses Seq2Seq by 6.47 points on Interpretation ASR , verifying that in lowresource settings SMT is superior to NN .
Our results , including adapting wait -k on Raw and using T2I to create training corpus , suggest that adequate numbers of paired translation , clean interpretation would lead to decreased delay and better translation quality .
The limitation , however , is that the BLEU score still remains relatively low , which is not surprising , for we only used a minimal number of parallel data in the style transfer process .
Hence , while our method does not remove the performance gap , it can still serve as a data augmentation technique to complement future interpretation training data .
Qualitative Analysis
To compare translations produced by the vanilla wait -k and its variants trained on T2I transferred data , we give examples in Table 4 along with their sources and gold translations .
In the first example , T2I variant is colloquial , implying interpreters giving up the original words and restating the source message ( Camayd - Freixas , 2011 ) .
T2I variant in the other example is a more condensed translation by dropping unimportant words ( Sudoh et al. , 2020 ) , such as " also expressly " and " the subject of " .
Both examples confirm human interpreters ' tactics ( He et al. , 2016 ) .
Conclusion
We investigated the SiMT evaluation gap when SiMT models were tested on interpretation vs translation , across four language pairs .
To the best of our knowledge , this is the first work quantifying this gap empirically .
To bridge the gap , we proposed a data augmentation style transfer technique to create parallel pseudo-interpretations from abundant offline translation data .
Our results show an improvement of 2.8 BLEU score .
We hope our work and the highlighted evaluation discrepancy can encourage further developments of datasets and models more reflective of real- world SI scenarios .
