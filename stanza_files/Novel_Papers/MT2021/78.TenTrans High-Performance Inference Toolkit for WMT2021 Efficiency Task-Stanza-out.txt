title
TenTrans High -Performance Inference Toolkit for WMT2021 Efficiency Task
abstract
The paper describes the TenTrans 's submissions to the WMT 2021 Efficiency Shared Task .
We explore training a variety of smaller compact transformer models using the teacherstudent setup .
Our model is trained by our self-developed open-source multilingual training platform TenTrans - Py 1 .
We also release an open-source high- performance inference toolkit 2 for transformer models and the code is written in C ++ completely .
All additional optimizations are built on top of the inference engine including attention caching , kernel fusion , early -stop , and several other optimizations .
In our submissions , the fastest system can translate more than 22,000 tokens per second with a single Tesla P4 while maintaining 38.36 BLEU on En- De newstest2019 .
Our trained models and more details are available in TenTrans - Decoding competition examples 3 .
Introduction
We participate in the GPU throughput track of the Workshop on Machine Translation ( WMT ) 2021 Efficiency Shared Task .
The efficiency task aims at exploring the different techniques for training and optimizing GPU models for high throughput while preserving the highest possible accuracy .
While we do not pay more attention to training techniques , we apply a variety of optimizations to improve the computation efficiency of our GPU models in the inference phase .
In terms of the training phase , we trained a variety of smaller compact student models using the common teacher -student training approach ( Hinton et al. , 2015 ; Kim and Rush , 2016 ) on our opensource multilingual training platform TenTrans - Py .
All of them are based on the deep transformer which has proven more effective and has lower training costs than the wide transformer models ( Wang et al. , 2019 ) .
For the inference phase , our strategy for the shared task includes attention caching , kernel fusion , early -stop , and several other optimizations .
All of these optimizations are employed in a high-optimized and C ++- based inference engine TenTrans - Decoding .
The paper is structured as follows :
Section 2 describes the data preparation and the training details , then Section 3 presents the variety of ours optimizations to improve decoding efficiency .
The detailed accuracy and efficiency results are shown in Section 4 .
Finally , we conclude our work in Section 5 .
Teacher-student Training
To train smaller compact student models , the teacher -student training approach ( Hinton et al. , 2015 ; Kim and Rush , 2016 ) is adopted .
First , a large model ( the teacher ) is trained on all available bilingual data , included synthetic data generated by the back - translation ( Sennrich et al. , 2015a ) method .
Multiple model ensembles are also typically used to build stronger teacher systems .
Then , all our small optimized models ( the student ) are created using sequence - level knowledge distillation ( Kim and Rush , 2016 ) and trained on data generated from the teacher model .
The sequencelevel knowledge distillation is a common technique that has proven successful for reducing the size of neural models , especially in NMT tasks .
Deep Transformer
Transformer networks ( Vaswani et al. , 2017 ) are the current state - of - the - art in many machine translation tasks , and the deep transformer ( Wang et al. , 2019 ) model , we use the Pre-Norm strategy ( Wang et al. , 2019 ) .
The layer normalization ( Ba et al. , 2016 ) is applied to the input of every sub-layer which the computation sequence could be expressed as : layer normalization ? multi-head attention / feedforward ? residual- add .
All of our models are based on deep transformer architecture .
Teacher & Student Models
The different model configurations for both teacher and student models are presented in Table 1 .
We train a teacher model and three student model variant with a different number of encoder layers N enc , decoder layers N dec , hidden size d model , and feedforward network size d f f .
We adopt a deep encoder and a shallow decoder architecture of all student models , and the number of decoder layers is set to 1 by default .
All of our models tie source embedding , target embedding , and softmax weights .
Data and Training Details Dataset Following the shared task setup , we limit our training data to the WMT 2021 English - German translation task .
The bilingual data used in the English - German task includes all the available corpora provided by WMT 2021 : Europarl v10 , ParaCrawl v7.1 , News Commentary , Wiki Titles v3 , Tilde Rapid corpus and WikiMatrix .
For monolingual data , we only use NewsCrawl2020 , Europarl v10 , and News Commentary for backtranslation .
Data preprocessing Then , we normalize punctuation and tokenize all data with the Moses tokenizer ( Koehn et al. , 2007 ) .
For the bitext datasets , we remain sentences no longer than 200 words as well as sentence pairs with a source / target length ratio between 0.3 and 2.0 .
The fast-align tools ( Dyer et al. , 2013 ) are applied to further obtain a cleaned and high-quality parallel corpus .
For the monolingual dataset , the sentences with words between 4 and 200 are remained .
See with 32 K split operations for subword segmentation ( Sennrich et al. , 2015 b ) .
Student training First , we train the teacher model on all available bilingual data , including synthetic data through the back - translation method , and we use English - German newstest2019 as the development set .
We ensemble four best models for building a stronger teacher .
Then , the English part of the bilingual data is translated by the teacher model and the resulting synthesized parallel data is used to train the student models .
Table 1 shows their evaluation scores on newstest2019 of different models .
The results correlate well with the expectation that more model parameters lead to better performance .
Our distillation student models show strong competitiveness even when the number of parameters is greatly reduced .
GPU Inference Optimizations
Implementation : TenTrans -Decoding TenTrans - Decoding is an open-source highoptimized inference engine for transformer models and the code is written in C ++.
TenTrans - Decoding 's goal is to offer a lightweight and rapid deployment of high- performance service solutions for executing models .
All additional optimizations are built on top of the inference engine .
Attention Caching
We apply the common technique of caching linear projections in Transformer decoder layers .
More specifically , we cache the linear transformations for keys and values before cross-attention layers and each step of decoder self-attention layers .
Kernel Fusion
To reduce kernel launching overhead and enhance the GPU computation efficiency , we implement many kernel fusion techniques for our Transformer models .
? Add_bias_residual_layerNormalization
For the layer normalization between two General Matrix Multiplications ( GEMMs ) , we reorganize the AddBias kernel , residual network , and LayerNormalization kernel into a single one .
?
Add_bias_ReLU
In the Feed-Forward network layers of the Transformer model , the AddBias kernel and ReLU kernel are fused into one .
?
Add_bias_residual
For the output of every encoder or decoder layer , we fuse the AddBias kernel and residual network .
?
Fused_multihead_attention
In addition to the fusion techniques above , we also fuse the attention layer by packing GEMMs and bias to further improve the computation efficiency .
3 : The decoding speed ( source tokens per second ) and SacreBLEU scores on newstest2019 for student- tiny - 20_1 .
The speed is measured by a single Tesla P4 GPU and the beam size is 4 .
x weightQ/K/V ->
Q / K / V ( GEMM 0 , 1 , 2 ) add bias to Q / K / V softmax ( QK / sqrt( size_per_head ) ) x V x weightQ ->
Q encoder_output x weightK/ V -> K / V ( GEMM 4 , 5 , 6 ) add bias to Q / K / V softmax ( QK / sqrt( size_per_head ) ) x V GEMM7
Early-stop
In batch decoding , the number of decoding ending steps between sentences is different .
The early - stop strategy which optimizes kernel function is adopted to avoid redundant computation .
For sentences that have been decoded in batch , there is no additional computation for these sentences until the whole batch has been decoded .
Sorted Batch & Greedy Search
In addition to the methods above , we sort all input sentences from shortest to longest , and the batch size is 128 in our settings .
The sorting makes the batches contain sentences of similar sizes which reduces the amount of padding and increases the computation efficiency .
During decoding , we use greedy search instead of beam search since we find the distillation model are insensitive to the beam size .
We skip the final softmax layer and simply get the maximum from the output logits .
Optimization Results
Table 3 shows the impact of different inference optimizations when decoding the Student-tiny - 20_1 student transformer model .
TenTrans - Decoding leads to a 2.62x speedup than the TenTrans - Py baseline without any inference optimizations .
Combine all the inference optimizations mentioned above , it can achieve a 7.23 x speedup with no accuracy loss over the baseline .
Table 4 presents all of our submissions and we only participate in the GPU - throughput track .
As details in Table 4 , we report our model configuration , model size , and metric for translation , including SacreBLEU scores on newstest2019 and the real translation time cost .
All of our systems are tested on a single Tesla P4 GPU .
All student models follow a deep encoder and a shallow decoder architecture , the number of decoder lay -
In this version , we do not pay more attention to the model size , memory footprint , and low precision inference ( e.g. , FP16 ) .
All operations on the model are based on FP32 floating -point numbers .
In the future version , we plan to optimize these points mentioned above .
Conclusion
This presents the TenTrans 's submissions to the 2021 Efficiency Shared Task of WMT .
We show the deep encoder and shallow decoder student models that training with sequence - level distillation can achieve a competitive performance both in speed and accuracy compared with the teacher baseline .
To further improve computation efficiency , we combine several optimizations including attention caching , kernel fusion , early - stop and sorted batch .
Finally , our fastest student model achieves a speedup of 3.58 x times , while only has one-sixth parameters of the teacher baseline .
In the future , we will apply low-precision inference ( e.g. , FP16 ) and more kernel fusion techniques to improve the computation efficiency of our GPU systems .
Furthermore , we will continue to explore a more efficient teacher -student training approach to obtain compact student models with competitive performance both in quality and speed .
Figure 1 1 Figure1 details the kernel fusion techniques of a transformer decoder layer .
The computation graph of a transformer can be reorganized into a more compact graph by fusing all the kernels between two GEMMs into a single one .
