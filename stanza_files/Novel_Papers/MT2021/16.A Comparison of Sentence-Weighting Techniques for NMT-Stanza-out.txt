title
A Comparison of Sentence-Weighting Techniques for NMT
abstract
Sentence weighting is a simple and powerful domain adaptation technique .
We carry out domain classification for computing sentence weights with 1 ) language model cross entropy difference 2 ) a convolutional neural network 3 ) a Recursive Neural Tensor Network .
We compare these approaches with regard to domain classification accuracy , and study the posterior probability distributions .
Then we carry out NMT experiments in the scenario where we have no in-domain parallel corpora , and only very limited in -domain monolingual corpora .
Here , we use the domain classifier to reweight the sentences of our out - of- domain training corpus .
This leads to improvements of up to 2.1 BLEU for German to English translation .
Introduction Neural Machine Translation ( NMT ) outperforms phrase based SMT for settings with large amounts of parallel data .
However , in general adding out - of- domain data during training does not particularly improve NMT translation quality and is sometimes even harmful .
For SMT domain adaptation is well understood and can be classified into two main approaches : 1 ) model centric techniques adapt the training objective on instance level ( e.g. , sentence weighting or regularization ) or model level ( e.g. , ensembling or language models ) , and 2 ) data centric techniques perform a sentence selection based on a score indicating the similarity between the sentence to be translated and in-domain data .
We combine ideas from model centric and data centric approaches .
We apply CNNs and Recursive Neural Tensor Networks ( RNTNs ) to compute domain scores for sentence weighting in NMT .
We compare with a Cross-Entropy classifier ( XenC ) as a well established baseline .
Our approach modifies the training objective so that every sentence pair is scaled by its individual weight , with sentences most similar to the in-domain data having most impact during training .
Our classifier is trained on small amounts of in- domain and out -of- domain monolingual data .
We then use the classifier to find useful sentences within the out-of- domain data , i.e. , sentences which are similar to the in-domain data .
We carry out intrinsic ( classification ) and extrinsic ( MT ) experiments applying sentence classification for domain adaptation .
The scores obtained by the CNN and RNTN are strongly peaked in comparison to the cross-entropy classifier , which is important for the NMT sentence weighting .
As the neural classifiers showed rather extreme probability score distributions in the intrinsic experiments , we studied various transformations of the scores which we use to find less peaked distributions .
The resulting distributions showed less extreme behavior while preserving the strong classification ability .
Applying our transformed scores to the task of sentence weighting for domain adaptation outperformed cross-entropy classifiers .
In summary , the contributions of this paper are as follows :
1 ) Neural classifiers show high confidence separating in - and out - of- domain data , higher than a cross-entropy classifier , hence posterior probabilities are distributed closely around the extremes 0 and 1 .
2 ) The CNN and RNTN classifiers do n't differ much from each other with respect to their score distributions , both are strongly peaked .
3 ) The extreme scores need to be transformed in order to be applied as weights in NMT , and we show how to do this effectively .
4 ) We show that using transformed CNN scores as weights during NMT training is better than a cross-entropy based classifier , which was the previous state - of - the - art solution .
Sentence-Weighting Techniques
In order to apply sentence weighting to the translation process , one first needs to come up with a method for scoring sentences with respect to how similar they are to in-domain data .
Here we carry out a comparison between an established baseline ( cross entropy ) to the two different techniques based on neural networks that we have discussed ( CNN and RNTN ) .
XenC : LM Cross-Entropy Difference Language model ( LM ) cross-entropy difference scoring is a widely used technique for MT domain adaptation .
The approach is implemented in the tool XenC Rousseau ( 2013 ) .
Here the difference between cross-entropy scores of sentences from the entire training corpus and the sentences of an in-domain corpus is computed .
We applied monolingual cross-entropy difference as proposed by ( Moore and Lewis , 2010 ) , which is defined as H( P LM ) = ?
1 n n i=1 logP LM ( w i | w i , . . . , w i?1 ) ( 1 ) where P LM is the probability of the word w i given the words w 1 to w i?1 for the language model LM . LM is estimated from the specified in - domain corpus .
The formula is applied to all sentences in the training data for the NMT system , and is then interpreted as the sentence weight .
XenC is not a neural system .
It applies statistical computation of cross-entropy given an LM .
The language model is a 4 - gram model and Kneser - Ney smoothing is applied Ney et al . ( 1994 ) .
This approach is widely used throughout various papers and systems with regard to domain adaptation .
It is mathematically relatively inexpensive and can therefore be computed very quickly even for extensive training corpora , without the need for GPU resources .
These factors make it a suitable baseline for our comparisons to neural classification systems .
CNN Classifier Convolutional neural networks ( CNN ) perform very well on tasks like image and sentence classification .
In our case , we are classifying sentences in two classes , in - domain and outof-domain .
We applied a plain vanilla system by Yoon Kim Kim ( 2014 ) , which consists of a simple CNN on top of pretrained word vectors .
CNNs consist of layers with convolving filters learning local features .
In this architecture one layer of convolution is applied on top of word vectors trained by Mikolov et al . ( 2013 ) on Google News .
This approach performed well on several sentence classification tasks ( Kim , 2014 ) .
Figure 1 shows this simple model architecture .
A sentence of length n ( shorter sentences are padded ) is represented as the concatenation of its word vectors .
Similar to computer vision tasks , filters are applied to words in a certain proximity to produce a new feature .
c i = f ( w ? x i:i+h?1 + b ) ( 2 ) b is a bias term and f a non-linear activation function .
The filter slides over the input sentence and therefore creates a feature map c = [ c 1 , c 2 , . . . , c n?h +1 ] ( 3 )
Then max-over - time pooling is applied , ? = max{c} , to capture the most important feature for each feature map .
Multiple filters are applied simultaneously and the max-pooling outputs form the penultimate layer .
The last layer is a fully connected softmax layer to output the probability distribution over the labels .
For regularization to reduce over-fitting and improve generalization , Dropout and constraining the l 2 ? norms of weight vectors is applied Krizhevsky et al . ( 2012 ) .
Dropout randomly drops out - i.e. setting to zero - a proportion p of hidden units ( in this case in the last layer ) during training .
Given the output of the max-pooling layer z = [ ?1 , ?2 , . . . , ?m ] , instead of y = w ? z + b ( 4 ) dropout uses y = w ? ( z ? r ) + b ( 5 ) with ? being element - wise multiplication and r ?
R m a " masking " vector of bernoulli distributed random variables with probability p of being 1 .
Furthermore a threshold s for l 2 ? norms in introduced , rescaling w to | | w | | 2 = s if | |w || 2 > s after a gradient descent step .
RNTN Classifier CNNs work on word vectors and filters , which aggregate local information within a sentence .
This is less expressive than richer forms of sentence representation , e.g. , parse trees , which take into account the grammatical structure .
To deal with parse trees for sentiment classification ( Socher et al. , 2013 ) introduced a recursive deep model , the Recursive Neural Tensor Network ( RNTN ) .
The representations of sentences within recursive neural models apply to variable length and syntactic type and is used for classification .
First , each sentence is parsed into a binary tree with leaf nodes being single words , represented by a vector .
Then the parent vectors will be computed in a bottom - up fashion using compositionality functions g.
The parent vectors themselves are recursively given as features to a classifier and their parents respectively .
Each word is represented by a d dimensional word vector .
These are fed into activation functions and ultimately used in sof tmax for classification .
Recursive Neural Network .
The simplest approach is the standard recursive neural network ( Goller and K?chler , 1996 ; Socher et al. , 2011 ) .
First , the parents whose children are already computed ( i.e. both children are words ) will be evaluated with an activation function f = tanh .
Following equations are used to evaluate the parent nodes according to Figure 2a : p 1 = f W b c , p 2 = f W a p 1 ( 6 ) where W ?
R dx2d is the main learning parameter .
Matrix -Vector RNN .
MV - RNNs are linguistically motivated in a sense that most of the parameters are linked with words and that the composition function depends on the actual words being combined .
Each word and subphrase are represented as a vector and a matrix , which are combined in the composition function .
Each word 's matrix initially is a dxd identity matrix with Gaussian noise .
These matrices will be trained to optimise classification .
Each sentence and subphrase is represented by a list of ( vector , matrix ) pairs and its parse tree .
Following the same example from Figure 2a , the computation is as follows : p 1 = f W Cb Bc , P 1 = f W M B C , ( 7 ) while the parent pair ( p 2 , P 2 ) is computed using ( p 1 , P 1 ) and ( a , A ) .
The vectors are fed into the softmax function for classifying each subphrase .
Recursive Neural Tensor Network .
Since MV - RNNs combine vectors with matrices , the number of parameters becomes very large , also depending on vocabulary size .
A fixed number of parameters would be more desirable .
The standard recursive neural network has to be extended for this purpose , because there , different from the MV - RNN , the input vectors only interact with each other implicitly .
In search for a single , more powerful composition function to perform better and aggregate meaning from subphrases , they proposed the Recursive Neural Tensor Network .
The output for a tensor product h ?
R d is computed as follows h = b c T V [ 1:d ] b c ; h i = b c T V i b c , ( 8 ) where V [ 1:d ] ?
R 2 dx2 dxd is the tensor that defines multiple bilinear forms .
The RNTN uses a definition very similar to the standard recursive neural network for computing p 1 : p 1 = f b c T V [ 1:d ] b c + W b c ( 9 )
The tensor V can directly relate input vectors and its slices can be interpretated as capturing specific types of composition , with a static number of parameters .
3 Intrinsic Evaluation : Domain Classification
Data
We study the interesting task of translation using limited in- domain monolingual corpora and larger out - of- domain parallel corpora , which is a realistic scenario .
All classifiers were trained on 30 k medical in- domain and 30 k out-domain sentences , selected from the UFAL corpus .
1
This training data was the same for all three classifiers to allow comparison .
The RNTN requires a certain input format , so the sentences were pre-processed by the Stanford Parser and brought into the necessary parse tree format .
For intrinsic evaluation , the classifiers were applied to gold standard test data .
Newstest 2017 was used as out -of- domain data , whereas the medical HimL test set 2 was used as in-domain data .
Both test sets contain about 2 k sentences .
The trained classifiers were applied to the test sets , in the next section we analysed the classification errors and compared the respective probability score distribution .
Figure 3 and Table 1 show the scoring outputs for in - and out -domain test data .
These histograms indicate how many sentences in the test set where assigned a certain score with bins of width 0.05 .
An output of 1 means high confidence for in- domain data and 0 means high confidence for out-domain data .
Evaluation on When comparing the results for the CNN and the RNTN , the differences are rather small , without obvious difference in shape of their distributions .
We see a dominating peak at the correct side of the spectrum , which shows these classifiers have a high degree of confidence in their decisions .
This peak diminishes rather quickly to then have a second minor peak around the other end of the spectrum .
This shape looks different for the cross entropy scoring .
It resembles a bell curve with its mean slightly skewed towards the correct side of the spectrum .
This shows a relatively unclear decision boundary between in - and out - of- domain data , since most of the sentences are scored rather in the middle between the two extremes .
These results should be taken with a grain of salt , as it is difficult to define pure in - domain and out -of- domain data .
Discussions in the European Parliament ( as found in the Europarl corpus ) can revolve around medical topics , while being labeled as out-of- domain .
Patient information as found in the data by the Health in my Language ( HimL ) project can include phrases of a more general nature , while being labeled in- domain .
Such effects are not taken into account in our work .
We applied the classifiers to the source ( German ) side of the NMT training data , leading to scores that can be used as weights during training the NMT system .
Figure 4 shows the distribution of the scores for the CNN and the Cross Entropy classifier .
Since we do have English data for the same 30 K sentences , we also looked at this classification problem , but the graphs are very similar , so they are not presented .
The similarity of English and German suggests that our work may apply well to other languages .
The scores by the XenC classifier look similar to a normal distribution , with its mean around 0.5- 0.6 .
Most of the sentences are scored with similar values , indicating an average importance during learning .
There are few outliers , overall the distribution is rather narrow with a low standard deviation .
The scores by the CNN classifier look significantly different .
Instead of the expected normal distribution , most of the weights are below 0.1 with a few scores above 0.95 .
This means that the classifier is very confident in it 's decisions .
This high level of confidence is also visible in Figure 3 .
Extrinsic Evaluation : Neural Machine Translation
In this section we first present our score transformations , and then we present the experiments and results .
In initial experiments ( which we present in detail later ) , we found that without applying score transformations instance weighting training of NMT models does not converge .
During sentence weighting , the probability score from the classifiers is multiplied with the learning rate .
As mentioned previously , the high classification confidence in neural classifiers lead to a vast majority of sentences scored very close to 0 , setting the learning rate during training very low .
This restricts the Transformer to only learn fully on a small subset of its original training data .
We suppose the rather extreme original probability scores let the NMT starve for data .
For the purpose of sentence weighting , the data distributions from the classifier outputs are problematic in a sense that they put most of the mass to the borders of the distribution , i.e. , almost all of the scores are very close to 0 or 1 .
This impacts the sentence weighting techniques significantly , since a score that is almost 0 effectively excludes these sentences from the data set .
We therefore applied several score transformations to obtain a normalized score distribution , as we describe next .
Parabolic Transformation .
The first approach is to multiply each of the scores with a linear function to increase the very low scores and decrease the very high scores .
Here we chose a simple linear function by taking an educated guess without doing further hyperparameter optimisation .
For every score x we applied the function f ( x ) = x * ( ?4.2 * x + 5 ) ( 10 ) which results in a parabola with its peak around x = 0.5 .
A parabola in this shape increases low scores and decreases high scores .
Its parameters were an educated guess , leading to competitive results in preliminary experiments .
Sigmoidal Transformation .
The second approach is to limit the scores into a certain interval using a sigmoid function .
We tried different hyperparameters indicating different intervals according the following function ? * 1/ ( 1 + exp ( ?6 * ( x ? 0.5 ) ) ) + ( 1 ? ?) /2 ( 11 ) indicating the interval [ 0.5 ? ?/2 , 0.5 + ?/2 ] .
These functions are shown in Figure 5a , leading to a normalised distribution on the NMT training data shown in Figure 5 b . Quantile Transformation .
The previous approaches lead to narrower and flatter data distributions .
As a third approach , we made the distribution completely uniform .
The second attempt was to " normalise " the quantiles by considering the negatively classified ( 0-0.5 ) and the positive ( 0.5 - 1 ) sentences separately and then performing the quantile transformation on both subsets individually .
Both categories were transformed into quantiles according to their own distribtion and then transformed back into the respective interval .
Experiments and Results
For our translation experiments we applied Marian ( Junczys - Dowmunt et al. , 2018 ) because of its ability to incorporate sentence weighting .
It offers a transformer ( Vaswani et al. , 2017 ) implementation that closely follows the original architecture .
This setup is shown to achieve state - of - the - art results .
Marian is C ++ based , which makes it very time efficient .
We assume a scenario with a sufficient amount of parallel out - of- domain data , but only a small amount of monolingual in-domain data on the source side .
We use the classifiers we trained before .
3 M out - of- domain sentences ( of which 2 M are from Europarl , see the UFAL corpus web page ) from the UFAL corpus are used for training NMT .
We report on two wellknown MT test sets ( Cochrane and NHS24 ) which are both from the medical domain .
Table 2 gives an overview of all performed experiments .
A baseline transformer model ( Table 2 , row 1 ) was trained without any domain specific adaptation .
Since we assume we have 30 K of monolingual in- domain data , we wanted to evaluate whether giving the NMT system access to this data could be effective .
Since we had a translation of this 30 K available , we actually fine-tuned on parallel data ( i.e. , we assumed perfect translation of the 30K , so this is an upper bound of the gains that could be obtained ) .
The results ( row 2 ) show that this is too little data to make much of a difference in translation quality ( 0.2 to 0.4 BLEU gains ) , which is not surprising given the very large out - of- domain corpus .
The strong results we present below are qualitatively different from having access to a small amount of in- domain data to train on ( even small amounts of in- domain parallel data ) .
The results for the the XenC classifier ( row 3 ) serve as a stronger baseline for our results with the neural classifiers .
We also tried to directly apply the scores from the neural classifiers , but this led to bad or unstable models that did not coverge ( not shown in table ) .
Too many sentences are scored too close to 0 , letting their impact vanish , not allowing the training to converge .
As discussed earlier and shown in Figure 4 for the CNN , most of the probability mass of the CNN 's score distribution is concentrated at the extremes , 0 and 1 , leading to many sentences having nearly no impact during training ( this is similar for the RNTN as well ) .
This is similar to training with too little data , as weighting a sentence very close to 0 skips the sentence .
These effects can be repaired by adding + 1 to the classifier scores ( rows 4 - 6 ) , leading to improvements over the baseline for all trained systems , especially for the two neural classifiers .
Further experiments focused on the CNN because it outperforms the RNTN and is simpler .
Following this we looked at score transformations .
The scores from the CNN were manipulated by various sigmoidal transformations ( rows 7 - 9 ) , as its results in the first experiments looked most promising .
As the qualitative analysis already showed in Figure 5 b , after the sigmoidal transformation the CNN scores look more natural .
The experiment results indicate that this transformation also lead to major improvements ( rows 7 - 9 ) , producing the best result ( row 8 ) among our experiments , an improvement over the baseline of 2.1 BLEU .
The sigmoid transformation keeps the CNN 's ability to clearly distinguish between in- domain and out-domain sentences from the test sets - much clearer than XenC .
Another possibility of combining the CNN 's classifying power and the XenC 's natural score distribution , is averaging their scores ( rows 10,11 ) .
This also lead to improvements over the baseline but could not beat the CNN in combination with the sigmoidal transformation ( row 8 ) .
MT Finally , as adding + 1 to the scores improved the results for all classifiers , we also applied + 1 to the previously described transformations ( rows 12 - 17 ) .
This still lead to minor improvements over the baseline system , but was harmful to the CNN and its sigmoidal transformation .
In summary we saw that classifier outputs might be too extreme in their distribution , which can be normalised by transformations to even outperform baseline approaches .
Neural classifiers show stronger abilities to distinguish between in- domain and out -of- domain data than cross-entropy based classifiers , resulting in higher BLEU scores when applied in sentence weighting .
Related Work Domain adaptation strategies can be separated into four categories : data selection , data generation , instance weighting and model interpolation Chu and Wang ( 2018 ) .
We focus our discussion on data selection and instance weighting , as these are closely related to our approach .
Data-centric methods .
Models are trained using in - domain and out - of- domain data to evaluate out - of- domain data and compute a similarity score .
Using a cut-off threshold on these scores the training data can be selected .
Language Models Moore and Lewis ( 2010 ) ; Axelrod et al . ( 2011 ) ; Duh et al. ( 2013 ) or joint models Cuong and Sima'an ( 2014 ) ; Durrani et al. ( 2015 ) can traditionally be applied to score corpora .
Recently convolutional neural networks ( CNN ) Chen et al . ( 2016 ) were used .
Our work has similarities to this work but uses instance weighting rather than data selection .
In settings where the amount of parallel training corpora is not sufficient , generating pseudo-parallel sentences by information retrieval Utiyama and Isahara ( 2003 ) , self-enhancing Lambert et al . ( 2011 ) or parallel word embeddings Marie and Fujita ( 2017 ) .
Aside from generating sentences , other approaches generate monolingual n-grams Wang et al . ( 2014 ) or parallel phrase pairs Chu ( 2015 ) .
In general , data-centric methods ( data selection and data generation ) are not SMT specific and can be directly applied to NMT .
However , because these methods are not directly related to NMT 's training criterion , they only lead to minor improvements Wang et al . ( 2017 a ) .
Model-centric methods .
Instance Weighting is a technique from SMT and was introduced to NMT as well Wang et al . ( 2017 b ) .
An in- domain language model was trained to measure the similarity between sentences and the in-domain data via cross-entropy .
The weights are then integrated into the training objective .
We improve on their work by using state - of - the - art neural classifiers and showing that they are more effective than cross-entropy .
Two works that are closer to our work are Wang et al . ( 2018 ) and Chen et al . ( 2017 ) .
In Wang et al. ( 2018 ) they generate sentence embeddings for all in- domain sentences and then measure the distance between every sentence and the in-domain core .
The underlying assumption is that the core of all in- domain sentence embeddings is a typical representative and proximity in their sentence embeddings indicates being part of the same domain .
This approach is appropriate when we have in - domain parallel text , but we study a different scenario , with no access to in- domain parallel text , which means the encoder has no access to in-domain training examples .
In Chen et al. ( 2017 ) a domain classifier is incorporated into the NMT system , using features from the encoder to distinguish between in - domain and out -of- domain data .
The classifier probabilities are used to weight sentences with regard to their similarity to in- domain data , when training the neural network .
Scaling the loss function is similar to multiplying the learning rate with the instance weight .
The classifier and NMT are trained at the same time , whereas we chose an approach with pretrained neural classifiers which are trained on a small amount of monolingual data ( the scenario we study ) with no access to parallel in-domain data .
Finally , while some previous work we have mentioned did look at various ways to use domain classification , such previous work has not focused on how to weight the classifier probabilities for effective use in NMT , which we showed is important for obtaining translation quality improvements , particularly when using neural classifiers which can be overconfident .
Conclusion Neural classifiers have high confidence when separating in- domain from out - of- domain data , leading to a strong decision boundary .
Classification results are good , but the boundary was too drastic , resulting in a poor score distribution with most mass near 0 and 1 .
This can be fixed by adding + 1 , keeping sentences with a low score as they are and giving a bonus to sentences with a higher score .
The scores from , e.g. , a CNN , can be transformed by a sigmoid function , making the score distribution more natural while keeping its strong decision boundary .
Cross-entropy approaches lead to a poor score distribution .
Sigmoid CNN scores performed best .
Our MT experiments showed that neural classifiers can be used to score out - of- domain data effectively .
Our work showed that simple transformations of classifier outputs are necessary .
The use of the transformed scores by applying sentence weighting on the NMT training data improves translation quality .
Our research shows that results from CNNs trained on domain classification achieve significant domain adaptation effects in NMT .
It was important to carry out light - weight score transformations .
We outperformed baseline experiments by up to 2.1 BLEU points .
Figure 1 : 1 Figure 1 : CNN model architecture .
