title
TenTrans Multilingual Low-Resource Translation System for WMT21 Indo-European Languages
Task
abstract
This paper describes TenTrans ' submission to WMT21 Multilingual Low-Resource Translation shared task for the Romance language pairs .
This task focuses on improving translation quality from Catalan to Occitan , Romanian and Italian , with the assistance of related high- resource languages .
We mainly utilize back - translation , pivot-based methods , multilingual models , pre-trained model fine-tuning , and in- domain knowledge transfer to improve the translation quality .
On the test set , our best-submitted system achieves an average of 43.45 case -sensitive BLEU scores across all low-resource pairs .
Our data , code , and pretrained models used in this work are available in TenTrans evaluation examples 1 .
Introduction
We participate in the WMT21 Multilingual Low-Resource Translation shared task .
This task focuses on the multilinguality in the cultural heritage domain for two Indo-European language families : North -Germanic and Romance .
We devote the research into translations among Romance languages , including Catalan?
Occitan , Catalan?
Romanian , Catalan ?
Italian .
Additionally , this task explicitly encourages the use of data of four related highresource languages ( Spanish , French , Portuguese and English ) in the same linguistic family .
For the model architecture , we adopt a universal encoder-decoder architecture that shares parameters across all languages ( Johnson et al. , 2017 ) .
And almost all of the subsequent experiments are based on Transformer base model ( Vaswani et al. , 2017 ) .
To effectively exploit low and high resource data in the multilingual low-resource scenario , we explore several approaches , and each approach shows effectiveness .
We employ back - translation ( Sennrich et al. , 2016a ) and pivot-based methods to augment the training corpus .
In terms of knowledge transfer , we explore the pre-trained model and the multilingual model that trained with both low and high resource language pairs .
Moreover , we extract in- domain corpus by a domain classifier and adapt the model to the target domain by in- domain fine-tuning .
This paper is structured as follows : Section 2 introduces used datasets , data statistic and preprocessing pipeline .
Section 3 describes the details of different approaches .
In Section 4 we present experimental settings and results .
Section 5 draws a brief conclusion of our work in the WMT21 .
Data
Datasets
The training datasets are majorly provided by the publicly available OPUS ( Tiedemann , 2012 ) repository .
We use almost all available datasets provided in the task , including Europarl , JW300 , WikiMatrix , MultiCCAligned , OPUS - 100 , Bible , ELRC , and 167.2 K
It - Ro pairs in TED talks as well as 15 M / 360K sentence pairs of En-It / En - Ro extracted from Wikipedia dumps .
For datasets that can be found through the resources search form on the top-level website of OPUS , we use opus-tools 2 to extract low-resource language pairs .
As for rest of the data , we download them in the usual way .
Statistics of different datasets are showed in Table 1 .
Data Pre-processing Cleaning datasets is necessary when the datasets are noisy and of low quality .
We partially refer to M2M - 100 3 ( Fan et al. , 2020 ) data pre-processing procedures to filter bilingual sentences .
We remove sentences with more than 50 % punctuation , deduplicate training data and remove all instances of evaluation data from the bilingual training data .
We tokenize all data and normalize punctuation with the Moses tokenizer ( Koehn et al. , 2007 ) .
To enable open-vocabulary and share information among languages , we use joint Byte-Pair-Encoding ( BPE ) with 32 K split operations for subword segmentation ( Sennrich et al. , 2016 b ) .
We also remove sentences longer than 512 as well as sentence pairs with a source / target length ratio exceeding 3 .
For monolingual data , we still employ those rules except the length ratio filter .
See Table 2 for the statistics of low-resource bilingual data , Table 3 for the statistics of high- low resource bilingual data and Table 4 for the statistics of low-resource monolingual data .
3 System Overview
Base Systems
In multilingual translation scenarios , one can employ multi-task learning framework using multiple encoders or multiple decoders ( Luong et al. , 2016 ; Dong et al. , 2015 ; Firat et al. , 2016 ) .
Either , one can employ a unified model consisting of a shared encoder and a shared decoder for all the language pairs ( Johnson et al. , 2017 ) .
We experiment with these two models and conduct the conclusion that a universal encoder-decoder model outperforms the model with multiple decoders .
The unified architecture is adopt in subsequent experiments in this work .
Parameters and vocabulary are shared among all language pairs and this helps the generalization across languages improving the translation for the low-resource language pairs ( Aharoni et al. , 2019 ) .
We also train three separate bilingual models to be regarded as contrastive model with multilingual model .
Furthermore , we jointly train on Catalan , Occitan , Romanian , Italian four low-resource languages simultaneously to obtain a many - to -many multilingual model .
Detailed results of base systems are shown in Table 6 .
We use the Transformer ( Vaswani et al. , 2017 ) as our model architecture for all of our systems .
We experiment with increasing network capacity but we find that deep and wide model architectures bring training hurdles .
So almost all subsequent models are based on the Transformer base architecture ( Vaswani et al. , 2017 ) as implemented in TenTrans 4 , expect for pre-trained model M2M - 100 trained using FAIRSEQ 5 ( Ott et al. , 2019 ) .
Back-translation Back-translation ( briefly , BT ) ( Sennrich et al. , 2016a ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system .
In this work , for translation direction with more than 5 million bilingual data such as Catalan ?
Italian , we train a dedicated bilingual BT ( 2019 ) , we set sampling temperature T = 5 .
Table 6 shows results on validation set of our baseline systems .
Obviously , the universal encoderdecoder model outperforms the model with separate decoders for each target language by 7 BLEU on average .
Compared to the bilingual baseline system , our universal multilingual 1 - to - 3 baseline system performs great improvement on low-resource languages , at the cost of sacrificing performance on relatively rich language Italian .
However , the jointly trained multilingual 4 - to - 4 system shows performance degradation .
We ascribe this phenomenon to multilingual model capacity is split for more translation directions , from 3 directions to 12 translation directions in this case .
Pivot- based Method Pivot- based approaches are prevalent when addressing the data scarcity problem in machine translation , nonetheless , they suffer from cascaded translation errors : the mistakes made in the source-topivot translation will be propagated to the pivotto-target translation ( Dabre et al. , 2020 ) .
Another pivot-based approach used in zero-resource transla - tion scenario is that the pivot side of the pivot-target parallel corpus is back - translated to the source language , creating a synthetic source - target parallel corpus ( Lakew et al. , 2018 ; Gu et al. , 2019 ) .
In this work , we adopt the latter pivot-based method .
In practice , we consider four high-resource languages
En , Es , Fr , Pt as pivot languages , thus we train a pivot-to-source multilingual model to back translate four pivot languages in pivot-to - target parallel data into source language .
Owing to relatively rich data of Catalan-Italian , we only perform experiments on low-resource languages of Occitan and Romanian .
To balance distribution between genuine parallel data and synthetic parallel data , we oversample genuine data to be of the same magnitude as synthetic data .
We can combine all synthetic parallel data generated from back -translation and pivot-based method with genuine parallel data to jointly train a multilingual model from scratch , which is named Combine - All .
Source side of this model comprised of four rich-resource and four low-resource languages , and target side of this model is comprised of four low-resource languages .
Pre-trained Model Fine-tuning Because of the recent popularity of using large scale pre-training models to fine- tune specific languages and tasks , we employ the M2M - 100 , a true Many - to - Many multilingual translation model ( Fan et al. , 2020 ) that can translate between 100 languages which cover four task languages .
Our experiments are based on the M2M - 100 1.2B model due to its better performance than the 418 M model .
In the subsequent fine-tuning procedure , we follow the parameters setting in fine-tuning mBART ( Liu et al. , 2020 ) .
In three task directions , we try fine-tuning M2M - 100 model with genuine bilingual data ( Bilingual FT ) and fine-tuning with genuine multilingual data ( Multilingual FT ) .
Moreover , we try fine-tuning the M2M - 100 1.2B model using Combine - All data with four high- resource plus low-resource languages as the source side and four low-resource languages as the target side .
Unfortunately , M2M -100 model trains on Sen-tencePiece ( Kudo and Richardson , 2018 ) rather than Byte-Pair- Encoding so that the fine-tuned model can not be directly combined with the models that listed above for ensembling .
We utilize synthetic Catalan-Occitan , Catalan-Romanian data generated through sentence - level knowledge distillation ( Kim and Rush , 2016 ) to train a ' student ' model so as to incorporate knowledge of ' teacher ' model M2M - 100 1.2B into ' student ' model .
Concretely , in Catalan ?
Occitan direction , we employ multilingual fine-tuning on M2M - 100 1.2B model using Combine - All data for 200K updates ( 1.1 M updates for each epoch ) , after that , we continue with bilingual fine-tuning using genuine Catalan - Occitan parallel data .
As for Catalan ?
Romanian direction , we directly use the pre-trained model without fine-tuning .
We continue to train on 8 - to - 4 multilingual model ( See Section 3.3 ) in three task translation directions with data obtained through knowledge distillation and finally get a new model named M2M -KD .
We do not implement knowledge distillation in Catalan ?
Italian direction since we find other systems perform equivalently to the pre-trained model .
If time permitted , we believe that more improvements will be observed .
Domain Adaptation
Domains of training data are various , whereas validation and hidden test data belong to the cultural
System Ca- Oc Ca heritage domain .
Owing to the domain discrepancy , adapting models to the cultural heritage domain ( Luong et al. , 2015 ) is required .
Due to the scarcity of in- domain data , we utilize pre-trained language model multilingual Bert 6 ( Devlin et al. , 2019 ) to train a domain classifier for extracting in-domain sentences from genuine bilingual data .
To train the domain classifier , we consider validation data of three languages Ca , Ro , It as positive samples , and randomly sample the lowresource side of high - low resource bilingual data as negative samples .
Then classifier is exploited to score the source sentences ( Ca / Ro / It ) .
We select sentence pairs whose source is predicted to be positive with a probability greater than threshold 0.7 to construct in- domain corpus .
In the end , we pick out 60K Catalan-Occitan , 297K Catalan-Romanian and 815 K Catalan-Italian data respectively as in-domain corpus .
We fine- tune 8 - to - 4 multilingual model on the in-domain corpus in three task translation directions and then get the In-domain - FT model .
For the purpose of preventing overfitting , we set the max-tokens to be 2 K with a learning rate of 3e - 5 and we force fine-tuning to stop when finishing the first epoch .
Note that we do not perform fine-tuning on the validation set .
Experiments
Settings Except that the pre-training experiments are trained on 4 NVIDIA V100 GPUs , the rest of our experiments are carried out with 8 NVIDIA P40 GPUs .
Except for the pre-training experiments , the rest of our experiments use the following settings .
Our models apply Adam ( Kingma and Ba , 2015 ) as optimizer to update the parameters with ?
1 = 0.9 and ? 2 = 0.98 .
We set the label smoothing and dropout rate to 0.1 .
The initial learning rate is set to 5e - 4 varied under a warm - up strategy with 4000 steps .
In the training stage , batch size is 8 K tokens per GPU .
We use uncased BLEU scores calculated with Moses multi-bleu.pl 7 toolkit as the evaluation metric .
And we choose model checkpoints based on the BLEU score on average of the validation set .
Main Results
Table 7 shows that the translation quality is largely improved with different systems .
Although minority systems encounter the problem of average performance degradation on the validation set , they contribute to at least one translation direction .
Back - translation gives a solid improvement by nearly 0.8 BLEU on average .
Pivot- based method offers 1?2 BLEU in Catalan?
Romanian , Catalan ?
Italian directions , however , pivot degrades in Catalan ?
Occitan direction .
When we train an 8 - to - 4 multilingual model jointly with both the high and low resource languages , the model shows an absolute improvement in three task directions of 6 BLEU on average score .
It can be explained by that a larger quantity of genuine data leads to robust encoder / decoder or knowledge can be transferred from high- resource into lowresource languages .
As for the pre-trained model , we notice that M2M - 100 1.2B model performs very well in Catalan ?
Romanian , Catalan ?
Italian directions without fine-tuning .
And we find that average bilingual fine-tuning outperforms multilingual fine-tuning by about 2.6 BLEU .
We also observe some systems hold a comparable performance with M2M - 100 1.2B model in Catalan ?
Romanian and Catalan ?
Italian directions when training data is abundant .
Further experiments include the in-domain finetuning and M2M - KD based on the multilingual 8 - to - 4 system .
In - domain fine- tuning is restricted to in- domain data size , but we also obtain a solid improvement of 1.5 BLEU on average , especially in Catalan ?
Occitan direction .
M2 M -KD model yields a greater improvement that we get the best BLEU in Catalan?
Occitan , Catalan ?
Romanian directions with 65.18 , 32.85 respectively .
Ultimately , to take advantages of multiple single models , two or three top performing models are ensembled to be the submitted systems .
Conclusions
In this paper , we present the system TenTrans submitted for the WMT21 Multilingual Low-Resource Translation for Indo-European Languages shared task .
We focus on Romance languages , translating from Catalan to Occitan , Romanian and Italian .
Back - translation , pivot-based method , multilingual model , knowledge distillation using pretrained model , domain adaptation and ensembles are employed and proven effective in the experiments .
Our best submitted system achieves an average of 43.45 case-sensitive BLEU score across all low-resource languages pairs .
Table 1 : 1 Number of sentences in different datasets .
' LRL - LRL ' means the bilingual data between low resource languages , e.g. Ca-Ro. ' HRL - LRL ' means the bilingual data between high- low resource languages , e.g. En-Ro . ' 1 M ' means we do not use that data though it is provided .
Note that OPUS provides En- Oc / Ro / It bilingual pairs , but we also use the target side Oc / Ro as monolingual data due to lacking data .
Bilingual WikiMatrix MultiCCAligned Bible/ Europarl JW300 ELRC OPUS LRL -LRL 2.7M 11.5M 386.5K 1.2M 0.022 K - HRL-LRL 8.9M 68.7 M 5.9M 12.2 M 2.7 M 2.0M Monolingual WikiMatrix MultiCCAligned Bible/ Europarl JW300 ELRC OPUS Oc 342.3 K - - - - 35.8 K Ro 3.8M 132.6 M 470.1 K 54.5 M 1.1M 1M
It 9.1M 175.2M 23.3 M 66.2M 1.6M 1M Ca- Oc Ca-Ro Ca-It No filter 138.7 K 2.2 M 6.3 M Filtered 138.7 K 2.1M 5.8M It - Ro It - Oc Oc - Ro No filter 7.2 M 122K 81 K Filtered 6.9M 122K 81K
Table 2 : 2 Number of sentences in low-resource bilingual data .
Table 3 : 3 Number of sentences in high- low resource bilingual data .
It Oc Ro No filter 275 M 378 K 193.5 M Filtered 38.3 M 225K 13.4M
Table 4 : 4 Number of sentences in low-resource monolingual data .
It as the target side .
We randomly extract 2 K sentence pairs from training data as the validation set for each high - low resource languages pairs .
BPE codes and multilingual vocabulary are shared among all languages , but a shared multilingual vocabulary runs the risk of favoring high- resource languages over others , due to the imbalance of the dataset size the vocabulary is extracted .
To reduce the effect of imbalanced dataset size , we apply a temperature sampling strategy named Vocabulary Sampling to construct a joined vocabulary .
Following Arivazhagan et al. languages ( Spanish , French , Portuguese and En- glish ) combined with four target - task low-resource languages together , resulting in an 8 - to - 4 multi - lingual model with Ca , Oc , Ro , BT System It - Ca Oc - Ca Ro-Ca Bilingual model 37.74 - - Multilingual 4- to-4 31.41 23.72 51.02
Table 5 : BLEU scores ( % ) for reverse models evaluated on the validation data .
model Italian ?
Catalan to translate Italian mono- lingual data into Catalan .
For other translation directions with less than 5 million bilingual data , we use the jointly pre-trained many - to - many multi- lingual model with four low-resource languages as its source and target side ( see Section 3.1 ) to back translate Occitan and Romanian monolingual data into Catalan .
Beam search with beam size 5 is used when generating the synthetic sentences .
Detailed results of reverse models are shown in Table 5 .
3.3 Multilingual Model Arivazhagan et al. ( 2019 ) shows that multilin- gual models can improve the translation perfor - mance of medium and low resource languages , as multilingual models are often trained on greater quantities of data compared to individual models .
So we utilize high- low resource paired data such as English ?
Occitan in addition to low-resource bilingual data during training .
Training on high- resource and low-resource language pairs together may bring knowledge transfer ( Zoph et al. , 2016 ) , especially when languages are from the same lin- guistic family .
In the experiment , we train on four high- resource
Table 6 : 6 BLEU scores ( % ) for baseline systems evaluated in the validation data .
And the numbers represent languages used in models , e.g. 1 - to - 3 means source side of model is Ca but target side consists of Oc , Ro , It , and 4 - to - 4 means both the source and target side of model consist of four languages Ca , Oc , Ro and It .
Base System Ca- Oc Ca-Ro Ca-It Average BLEU 30.02 - - Bilingual models - 21.51 - 28.48 - - 33.91 Separate Decoders Multilingual 1-to-3 25.82 22.03 32.96 26.90 Universal Models Multilingual 1-to-3 43.40 24.16 32.92 33.78 Multilingual 4-to-4 41.44 22.81 31.01 31.75
Table 7 : 7 BLEU scores ( % ) for different systems on the validation data .
The number 8 means source side of model consists of both four high- resource languages and four low-resource languages , 4 means target side of model consists of four low-resource languages Ca , Oc , Ro and It. ' ?' is the submitted primary system .
'* ' is the submitted contrastive system .
- Ro Ca-It Average BLEU
https://pypi.org/project/ opustools -pkg/
https://github.com/pytorch/fairseq/ tree/master/examples/m2m_100
https://github.com/TenTrans/TenTrans 5 https://github.com/pytorch/fairseq
https://huggingface.co/ bert-base-multilingual-cased
https://github.com/moses-smt/ mosesdecoder/blob/master/scripts / generic/multi-bleu.perl
