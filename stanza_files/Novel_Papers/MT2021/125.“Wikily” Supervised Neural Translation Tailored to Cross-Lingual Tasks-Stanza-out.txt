title
" Wikily " Supervised Neural Translation Tailored to Cross-Lingual Tasks
abstract
We present a simple but effective approach for leveraging Wikipedia for neural machine translation as well as cross-lingual tasks of image captioning and dependency parsing without using any direct supervision from external parallel data or supervised models in the target language .
We show that first sentences and titles of linked Wikipedia pages , as well as crosslingual image captions , are strong signals for a seed parallel data to extract bilingual dictionaries and cross-lingual word embeddings for mining parallel text from Wikipedia .
Our final model achieves high BLEU scores that are close to or sometimes higher than strong supervised baselines in low-resource languages ; e.g. supervised BLEU of 4.0 versus 12.1 from our model in English - to - Kazakh .
Moreover , we tailor our " wikily " supervised translation models to unsupervised image captioning , and cross-lingual dependency parser transfer .
In image captioning , we train a multitasking machine translation and image captioning pipeline for Arabic and English from which the Arabic training data is a translated version of the English captioning data , using our wikily - supervised translation models .
Our captioning results on Arabic are slightly better than that of its supervised model .
In dependency parsing , we translate a large amount of monolingual text , and use it as artificial training data in an annotation projection framework .
We show that our model outperforms recent work on cross-lingual transfer of dependency parsers .
Introduction
Developing machine translation models without using bilingual parallel text is an intriguing research problem with real applications : obtaining a large volume of parallel text for many languages is hard if not impossible .
Moreover , translation models could be used in downstream cross-lingual tasks in which annotated data does not exist for some languages .
There has recently been a great deal of interest in unsupervised neural machine translation ( e.g. Artetxe et al .
( 2018a ) ; Lample et al. ( 2018 a , c ) ; Conneau and Lample ( 2019 ) ; Song et al . ( 2019a ) ; ; Tae et al . ( 2020 ) ) .
Unsupervised neural machine translation models often perform nearly as well as supervised models when translating between similar languages , but they fail to perform well in low-resource or distant languages or out-of- domain monolingual data ( Marchisio et al. , 2020 ) .
In practice , the highest need for unsupervised models is to expand beyond high resource , similar European language pairs .
There are two key goals in this paper :
Our first goal is developing accurate translation models for low-resource distant languages without any supervision from a supervised model or gold -standard parallel data .
Our second goal is to show that our machine translation models can be directly tailored to downstream natural language processing tasks .
In this paper , we showcase our claim in cross-lingual image captioning and cross-lingual transfer of dependency parsers , but this idea is applicable to a wide variety of tasks .
We present a fast and accurate approach for learning translation models using Wikipedia .
Unlike unsupervised machine translation that solely relies on raw monolingual data , we believe that we should not neglect the availability of incidental supervisions from online resources such as Wikipedia .
Wikipedia contains articles in nearly 300 languages and more languages might be added in the future , including indigenous languages and dialects of different regions in the world .
Different from similar recent work ( Schwenk et al. , 2019a ) , we do not rely on any supervision from supervised translation models .
Instead , we leverage the fact that many first sentences in linked Wikipedia pages are rough glish in which the titles , first sentences , and also the image captions are rough translations of each other .
Our method learns a seed bilingual dictionary from a small collection of first sentence pairs , titles and captions , and then learns cross-lingual word embeddings .
We make use of cross-lingual word embeddings to extract parallel sentences from Wikipedia .
Our experiments show that our approach improves over strong unsupervised translation models for low-resource languages : we improve the BLEU score of English !
Gujarati from 0.6 to 15.2 , and English !
Kazakh from 0.8 to 12.1 .
In the realm of downstream tasks , we show that we can easily use our translation models to generate high-quality translations of MS - COCO ( Chen et al. , 2015 ) and Flickr ( Hodosh et al. , 2013 ) datasets , and train a cross-lingual image captioning model in a multi-task pipeline paired with machine translation in which the model is initialized by the parameters from our translation model .
Our results on Arabic captioning show a BLEU score of 5.72 that is slightly better than a supervised captioning model with a BLEU score of 5.22 .
As another task , in dependency parsing , we first translate a large amount of monolingual data using our translation models and then apply transfer using the annotation projection method ( Yarowsky et al. , 2001 ; Hwa et al. , 2005 ) .
Our results show that our approach performs similarly compared to using gold -standard parallel text in high- resource scenarios , and significantly better in low-resource languages .
A summary of our contribution is as follows : ?
We propose a simple , fast and effective ap-proach towards using the Wikipedia mono- Supervised neural machine translation Supervised machine translation uses a parallel text P = {( s i , t i ) } n i=1 in which each sentence s i 2 l 1 is a translation of t i 2 l 2 .
For having a high-quality translation model , we usually need a large amount of parallel text .
Neural machine translation uses sequence - to-sequence models with attention ( Cho et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) for which the likelihood of training data is maximized by maximizing the log-likelihood of predicting each target word given its previous predicted words and source sequence : L( P ) = n X i=1 |t i | X j=1 log p( t i , j |t i , k< j , s i ; ? )
1 Our code : https://github.com/rasoolims/
ImageTranslate , and our modification to Stanza for training on partially projected trees : https://github.com/ rasoolims/stanza
2 Figure 1 : A pair of Wikipedia documents in Arabic and English , along with a same image with two captions .
translations , and furthermore , many captions of the same images are similar sentences , sometimes translations .
Figure 1 shows a real example of a pair of linked Wikipedia pages in Arabic and English in which the titles , first sentences , and also the image captions are rough translations of each other .
Our method learns a seed bilingual dictionary from a small collection of first sentence pairs , titles and captions , and then learns cross-lingual word embeddings .
We make use of cross-lingual word embeddings to extract parallel sentences from Wikipedia .
Our experiments show that our approach improves over strong unsupervised translation models for low-resource languages : we improve the BLEU score of English ?
Gujarati from 0.6 to 15.2 , and English ?
Kazakh from 0.8 to 12.1 .
In the realm of downstream tasks , we show that we can easily use our translation models to generate high-quality translations of MS - COCO ( Chen et al. , 2015 ) and Flickr ( Hodosh et al. , 2013 ) datasets , and train a cross-lingual image captioning model in a multi-task pipeline paired with machine translation in which the model is initialized by the parameters from our translation model .
Our results on Arabic captioning show a BLEU score of 5.72 that is slightly better than a supervised captioning model with a BLEU score of 5.22 .
As another task , in dependency parsing , we first translate a large amount of monolingual data using our translation models and then apply transfer using the annotation projection method ( Yarowsky et al. , 2001 ; Hwa et al. , 2005 ) .
Our results show that our approach performs similarly compared to using gold -standard parallel text in high- resource scenarios , and significantly better in low-resource languages .
A summary of our contribution is as follows : 1 ) We propose a simple , fast and effective approach towards using the Wikipedia monolingual data for machine translation without any explicit supervision .
Our mining algorithm easily scales on large comparable data using limited computational resources .
We achieve very high BLEU scores for distant languages , especially those in which current unsupervised methods perform very poorly .
2 ) We propose novel methods for leveraging our current translation models in image captioning .
We show that how a combination of translating caption training data , and multi-task learning with English captioning as well as translation improves the performance .
Our results on Arabic shows results slightly superior to that of a supervised captioning model trained on gold -standard datasets .
3 ) We propose a novel modification to the annotation projection method to be able to leverage our translation models .
Our results on dependency parsing performs better than previous work in most cases , and performs similarly to using gold -standard parallel datasets .
Our translation and captioning code and models are publicly available online 1 .
Background Supervised neural machine translation Supervised machine translation uses a parallel text P = {( s i , t i ) } n i=1 in which each sentence s i ?
l 1 is a translation of t i ?
l 2 . Neural machine translation uses sequence - to-sequence models with attention ( Cho et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) for which the likelihood of training data is maximized by maximizing the loglikelihood of predicting each target word given its previous predicted words and source sequence : L( P ) = n i=1 |t i | j=1 log p( t i , j |t i , k< j , s i ; ? ) where ? is a collection of parameters to be learned .
Unsupervised neural machine translation Unsupervised neural machine translation does not have access to any parallel data .
Instead , it tailors monolingual datasets M l 1 and M l 2 for learning multilingual language models .
These language models usually mask parts of every input sentence , and try to uncover the masked words ( Devlin et al. , 2019 ) .
The monolingual language models are used along with iterative back - translation ( Hoang et al. , 2018 ) to learn unsupervised translation .
An input sentence s is translated to t using current model ? , then the model assumes that ( t , s ) is a goldstandard translation , and uses the same training objective as of supervised translation .
Dependency parsing Dependency parsing algorithms capture the best scoring dependency trees for sentences among an exponential number of possible dependency trees .
A valid dependency tree for a sentence s = s 1 , . . . , s n assigns heads h i for each for word s i where 1 ? i ? n , 0 ? h i ? n and h i = i .
The zeroth word represents a dummy root token as an indicator for the root of the sentence .
For more details about efficient parsing algorithms , we encourage the reader to see K?bler et al . ( 2009 ) .
Annotation projection Annotation projection is an effective method for transferring supervised annotation from a rich-resource language to a low-resource language through translated text ( Yarowsky et al. , 2001 ) .
Having a parallel data P = {( s i , t i ) } n i=1 , and supervised source annotations for source sentences s i , we transfer those annotations through word translation links 0 ? a ( j ) i ?
|t i | for 1 ? j ?
|s i | where a ( j ) i = 0 shows a null alignment .
The alignment links are learned in an unsupervised fashion using unsupervised word alignment algorithms ( Och and Ney , 2003a ) .
In dependency parsing , if h i = j and a ( j ) = k and a ( i ) = m , we project a dependency k ? m ( i.e. h m = k ) to the target side .
Previous work Collins , 2017 , 2019 ) has shown that annotation projection only works when a large amount of translation data exists .
In the absence of parallel data , we create artificial parallel data using our translation models .
Figure 2 shows an example of annotation projection using translated text .
Learning Translation from Wikipedia
The key component of our approach is to leverage the multilingual cues from linked Wikipedia pages across languages .
Wikipedia is a great comparable data in which many of its pages explain entities in the world in different languages .
In most cases , first sentences define or introduce the mentioned entity in that page ( e.g. Figure 1 ) .
Therefore , we observe that many first sentence pairs in linked Wikipedia documents are rough translations of each other .
Moreover , captions of images in different languages are usually similar but not necessarily direct translations of each other .
We leverage this information to extract many parallel sentences from Wikipedia without using any external supervision .
In this section , we describe our algorithm which is briefly shown in Figure 3 .
Data Definitions
For languages e and f in which e is English and f is a low-resource target language of interest , there are Wikipedia documents w e = {w ( i , j ) as the jth sentence in the ith document for language l .
A subset of these documents are aligned ( using Wikipedia languages links ) .
Thus we have an aligned set of document pairs in which we can easily extract many sentence pairs that are potentially translations of each other .
A smaller subset F is the set of first sentences in Wikipedia ( w ( e ) ( i,1 ) , w ( f ) ( i , 1 ) ) in which documents i and i are linked and their first sentence lengths are in a similar range .
In addition to text content , Wikipedia has a large set of images .
Each image comes along with one or more captions , sometimes in different languages .
A small subset of these images have captions both in English and the target language .
We refer to this set as C .
We use the set of all caption pairs ( C ) , title pairs ( T ) , and first sentences ( F ) as the seed parallel data : S = F ? C ? T .
Bilingual Dictionary Extraction and Cross-Lingual Word Embeddings
Having the seed parallel data S , we run unsupervised word alignment ( Dyer et al. , 2013 ) in both English - to- target , and target - to - English directions .
We use the intersected alignments to extract highly confident word-to - word connections .
Finally , we pick the most frequently aligned word for each word in English as translation .
This set serves as a bilingual dictionary D. Given two monolingual trained word embeddings v e ?
R Ne?d and v f ?
R N f ?d , and the extracted bilingual dictionary D , we use the method of Faruqui and Dyer ( 2014 ) to project these two embedding vectors to a shared cross-lingual space .
2
This method uses a bilingual dictionary along with The International Crisis Group recently suggested moving responsibility for pension to state level , to eliminate some of the problems .
Grupul International de Criza a sugerat recent mutarea responsabilitatii pentru pensii la nivelul statului , pentru a elimina unele dintre probleme .
Supervised neural machine translation Supervised machine translation uses a parallel text P = {( s i , t i ) } n i=1 in which each sentence s i 2 l 1 is a translation of t i 2 l 2 .
For having a high-quality translation model , we usually need a large amount of parallel text , e.g. the Arabic-English United Nations parallel text ( Ziemski et al. , 2016 ) contains n ? 18 M sentences .
Neural machine translation uses sequence - to-sequence models with attention ( Cho et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) for which the likelihood of training data is maximized by maximizing the loglikelihood of predicting each target word given its previous predicted words and source sequence : L( P ) = n X i=1 |t i | X j=1 log p( t i , j |t i , k< j , s i ; ? ) where ? is a collection of parameters to be learned .
In sequence - to-sequence models , the input s i is usually converted to vector representations using contextualized embeddings and attention ( Vaswani et al. , 2017 ) .
The model ? can be extended to bidirectional - translating in both language directions - as well as multilingual ( Firat et al. , 2016 ; Johnson et al. , 2017 ; Siddhant et al. , 2020 ; Tang et al. , 2020 ) .
Unsupervised neural machine translation Unsupervised neural machine translation does not have access to any parallel data .
Instead , it tailors monolingual datasets M l 1 and M l 2 for learning multilingual language models .
These language models usually mask parts of every input sentence , and try to uncover the masked words ( Devlin et al. , 2019 ) .
In this work , we mainly use the MASS model ( Song et al. , 2019 ) , in which a contiguous span of words are masked , and the decoder predicts the masked words .
These monolingual language models are used along with iterative backtranslation ( Hoang et al. , 2018 ) to learn unsupervised translation .
In other words , an input sentence s is translated to t 0 using current model ?.
Then the model assumes that ( t 0 , s ) is a gold-standard translation , and uses the same training objective as of supervised neural translation .
The main assumption here is that languages have distributional similarities and these similarities can be captured by pretrained multilingual language models ( Conneau et al. , 2020 ) .
Dependency parsing Dependency parsing algorithms capture the best scoring dependency trees for sentences among an exponential number of possible dependency trees .
A valid dependency tree for a sentence s = s 1 , . . . , s n assigns heads h i for each for word s i where 1 ? i ? n , 0 ? h i ? n and h i 6 = i .
The zeroth word represents a dummy root token as an indicator for the root of the sentence .
In this paper , we use the state - of - the - art dependency parsing models from Stanza ( Qi et al. , 2020 ) .
Figure 2 shows an example of a dependency parse tree with the Universal Dependencies annotation scheme ( Zeman et al. , 2020 ) .
For more details about dependency parsing , we encourage the reader to see K?bler et al . ( 2009 ) . Definitions : 1 ) e is English , f is the foreign language , and g is a language similar to f , 2 ) learn_dict ( P ) extracts a bilingual dictionary from parallel data P , 3 ) t ( x| m ) translates input x given model m , , 4 ) pretrain ( x ) pretrains on monolingual data x using MASS ( Song et al. , 2019a ) Inputs : 1 ) Wikipedia documents w ( e ) , w ( f ) , and w ( g ) , 2 ) Monolingual word embedding vectors ve and v f , 3 ) Set of linked pages from Wikipedia COMP , their aligned titles T , and their first sentence pairs F , 4 ) Set of paired image captions C , and 5 ) Gold-standard parallel data P ( e , g ) .
Algorithm : ? Learn bilingual dictionary and embeddings S = F ? C ? T D ( f , e ) = learn_dict ( S ) D ( g , e ) = learn_dict ( P ( e , g ) ) Related language Learn ve ?
v e and v f ?
v f using D ( f, e ) ? D ( g , e ) ?
Mine parallel data Extract comparable sentences Z from COMP Extract P ( f, e ) from Z. P ( f, e ) = P ( f, e ) ? T Mined Data ?
Train MT with pretraining and back- translation ?0 = pretrain ( w ( e ) ? w ( f ) ? w ( g ) ) MASS Training ? = train ( P ( f, e ) ? P ( g ,e ) |?0 ) NMT Training P ( e?f ) = ( t ( w ( f ) |? ) , w ( f ) ) P ( f ?e ) = ( t ( w ( e ) |? ) , w ( e ) ) P ( f,e ) = P ( e?f ) ? P ( f ?e ) ? P ( f,e ) ? = train ( P ( f, e ) |?0 ) canonical correlation analysis ( CCA ) to learn two projection matrices to map each embedding vector to a shared space v e ?
R Ne?d and v f ?
R N f ?d where d ? d. ? * = bt_train ( w ( e ) , w ( f ) |? ) Output : ? *
Mining Parallel Sentences
We use cross-lingual embedding vectors v e ?
R Ne?d and v f ?
R N f ?d for calculating the cosine similarity between pairs of words .
Moreover , we use the extracted bilingual dictionary to boost the accuracy of the scoring function .
For a pair of sentences ( s , t ) where s = s 1 . . . s n and t = t 1 . . . t m , after filtering sentence pairs with different numerical values ( e.g. sentences containing 2019 in the source and 1987 in the target ) , we use a modified version of cosine similarity between words : sim(s i , t j ) = 1.0 , if ( s i , t j ) ?
D cos(s i , t j ) , otherwise Using the above definition of word similarity , we use the average -maximum similarity between pairs of sentences .
score(s , t ) = n i=1 max m j=1 sim(s i , t i ) n From a pool of candidates , we pick those pairs that have the highest score in both directions .
Leveraging Similar Languages
In many low-resource scenarios , the number of paired documents is very small , leading to a small number and often noisy extracted parallel sentences .
To alleviate this problem to some extent , we assume to have another language g in which g has a large lexical overlap with the target language f ( such as g=Russian and f = Kazakh ) .
We assume that a parallel data exists between language g and English , and we can use it both as an auxiliary parallel data in training , and also for extracting extra lexical entries for the bilingual dictionaries : as shown in Figure 3 , we supplement the extracted bilingual dictionary from seed parallel data with the bilingual dictionary extracted from related language parallel data .
Translation Model
We use a standard sequence - to-sequence transformer - based translation model ( Vaswani et al. , 2017 ) with a six-layer BERT - based ( Devlin et al. , 2019 ) encoder-decoder architecture from HuggingFace ( Wolf et al. , 2019 ) and Pytorch ( Paszke et al. , 2019 ) with a shared SentencePiece ( Kudo and Richardson , 2018 ) vocabulary .
All input and output token embeddings are summed up with the language id embedding .
First tokens of every input and output sentence are shown by the language ID .
Our training pipeline assumes that the encoder and decoder are shared across different languages , except that we use a separate output layer for each language in order to prevent input copying ( Artetxe et al. , 2018 b ; Sen et al. , 2019 ) .
We pretrain the model on a tuple of three Wikipedia datasets for the three languages g , f , and e using the MASS model ( Song et al. , 2019a ) .
The MASS model masks a contiguous span of input tokens , and recovers that span in the output sequence .
To facilitate multi-task learning with image captioning , our model has an image encoder that is used in cases of image captioning ( more details in ?4.1 ) .
In other words , the decoder is shared between the translation and captioning tasks .
We use the pretrained ResNet - 152 model ( He et al. , 2016 ) from Pytorch to encode every input image .
We extract the final layer as a 7 ? 7 grid vector ( g ? R 7?7?dg ) , and project it to a new space by a linear transformation ( g ? R 49 ? dt ) , and then add location embeddings ( l ? R 49 ? dt ) by using entry -wise addition .
Afterwards , we assume that the 49 vectors are encoded text representations as if a sentence with 49 words occurs .
This is similar to but not exactly the same as the Virtex model ( Desai and Johnson , 2021 ) .
Back - Translation : One-shot and Iterative Finally , we use the back -translation technique to improve the quality of our models .
Backtranslation is done by translating a large amount of monolingual text to and from the target language .
The translated texts serve as noisy input text along with the monolingual data as the silverstandard translations .
Previous work ( Sennrich et al. , 2016 b ; Edunov et al. , 2018 ) has shown that back - translation is a very simple but effective technique to improve the quality of translation models .
Henceforth , we refer to this method as one- shot back - translation .
Another approach is to use iterative back - translation ( Hoang et al. , 2018 ) , the most popular approach in unsupervised translation ( Artetxe et al. , 2018 b ; Conneau and Lample , 2019 ; Song et al. , 2019a ) .
The main difference from one - shot translation is that the model uses an online approach , and updates its parameters in every batch .
We empirically find one- shot back - translation faster to train but with much less potential to reach a high translation accuracy .
A simple and effective way to have both a reliable and accurate model is to first initialize a model with one - shot back - translation , and then apply iterative backtranslation .
The model that is initialized with a more accurate model reaches a higher accuracy .
Cross-Lingual Tasks
In this section , we describe our approaches for tailoring our translation models to cross-lingual tasks .
Note that henceforth we assume that our translations model training is finished , and we have access to trained translation models for cross-lingual tasks .
Cross-Lingual Image Captioning Having gold - standard image captioning training data I = { ( I i , c i ) }
n i=1 where I i is the image as pixel values , and c i = c ( 1 ) i , . . . , c k i i as the textual description with k i words , our goal is to learn a captioning model that is able to describe new ( unseen ) images .
As described in ?3.5 , we use a transformer decoder from our translation model and a ResNet image encoder ( He et al. , 2016 ) for our image captioning pipeline .
Unfortunately , annotated image captioning datasets do not exist in many languages .
Having our translation model parameter ? * , we can use its translation functionality to translate each caption c i to c i = translate ( c i |? * ) .
Afterwards , we will have a translated annotated dataset I = { ( I i , c i ) } n i=1 in which the textual descriptions are not gold -standard but translations from the English captions .
Figure 4 shows a real example from MS - Coco ( Chen et al. , 2015 ) in which Arabic translations are provided by our translation model .
Furthermore , to augment our learning capability , we initialize our decoder with decoding parameters of ? * , and also continue training with both English captioning and translation .
Cross-Lingual Dependency Parsing
Assuming that we have a large body of monolingual text , we translate that monolingual text to create artificial parallel data .
We run unsupervised word alignments on the artificial parallel text .
Following previous work ( Rasooli and Collins , 2015 ; Ma and Xia , 2014 ) , we run Giza ++ ( Och and Ney , 2003 b ) alignments on both source - to- target and target - to -source directions , and extract intersected alignments to keep high - precision one - to - one alignments .
We run a supervised dependency parser of English as our rich-resource language .
Then , we project dependencies to the target language sentences via word alignment links .
Inspired by previous work ( Rasooli and Collins , 2015 ) , to remove noisy projections , we keep those sentences that at least 50 % of words or 5 consecutive words in the target side have projected dependencies .
Experiments
In this section , we provide details about our experimental settings and results for translation , captioning , and dependency parsing .
We put more details about our settings as well as thorough analysis of our results in the supplementary material .
Datasets and Settings Languages
We focus on four language pairs : Arabic-English , Gujarati-English , Kazakh -English , and Romanian - English .
We choose these pairs to provide enough evidence that our model works in distant languages , morphologically - rich languages , as well as similar languages .
As for similar languages , we use Persian for Arabic ( written with very similar scripts and have many words in common ) , Hindi for Gujarati ( similar languages ) , Russian for Kazakh ( written with the same script ) , and Italian for Romanian ( Romance languages ) .
Monolingual and Translation Datasets
We use a shared SentencePiece vocabulary ( Kudo and Richardson , 2018 ) with size 60K .
Table 1 shows the sizes of Wikipedia data in different languages .
For evaluation , we use the Arabic- English UN data ( Ziemski et al. , 2016 ) Table 1 : Data sizes for different pairs .
We use a sample of English sentences with similar sizes to each data .
et al. , 2016 ) for Romanian - English .
Following previous work ( Sennrich et al. , 2016a ) , diacritics are removed from the Romanian data .
More details about other datasets and their sizes , we refer the reader to the supplementary material .
Pretraining
We pretrain four models on 3 - tuples of languages via a single NVIDIA Geforce RTX 2080 TI with 11 GB of memory .
We create batches of 4 K words , run pretraining for two million iterations where we alternate between language batches , and accumulate gradients for 8 steps .
We use the apex library 3 to use FP - 16 tensors .
This whole process takes four weeks in a single GPU .
We use the Adam optimizer ( Kingma and Ba , 2015 ) with inverse square root and learning rate of 10 ?4 , 4000 warm - up steps , and dropout probability of 0.1 .
Translation Training Table 1 shows the sizes of different types of datasets in our experiments .
We pick comparable candidates for sentence pairs whose lengths are within a range of half to twice of each other .
As we see , the final size of mined datasets heavily depends on the number of paired English - target language Wikipedia documents .
We train our translation models initialized by pretrained models .
More details about our hyperparameters are in the supplementary material .
All of our evaluations are conducted using Sacre-BLEU ( Post , 2018 ) except for en?ro in which we use BLEU score ( Papineni et al. , 2002 ) from Moses decoder scripts ( Koehn et al. , 2007 ) for the sake of comparison to previous work .
Image Captioning
We use the Flickr ( Hodosh et al. , 2013 ) and MS - Coco ( Chen et al. , 2015 ) datasets for English 4 , and the gold-standard Arabic Flickr dataset ( ElJundi .
et al. , 2020 ) Dependency Parsing
We use the Universal Dependencies v2.7 collection ( Zeman et al. , 2020 ) for Arabic , Kazakh , and Romanian .
We use the Stanza ( Qi et al. , 2020 ) pretrained supervised models for getting supervised parse trees for Arabic and Romanian , and use the UDPipe ( Straka et al. , 2016 ) pretrained model for Kazakh .
We translate about 2 million sentences from each language to English , and also 2 million English sentences to Arabic .
We use a simple modification to Stanza to facilitate training on partially projected trees by masking dependency and label assignments for words with missing dependencies .
All of our training on projected dependencies is blindly conducted with 100k training steps with default parameters of Stanza ( Qi et al. , 2020 ) .
As for gold -standard parallel data , we use our supervised translation training data for Romanian - English and Kazakh - English and use a sample of 2 million sentences from the UN Arabic - English data due to its large size that causes word alignment significant slowdown .
For Kazakh wikily projections , due to low supervised POS accuracy , we use the projected POS tags for projected words and supervised tags for unprojected words .
We observe a two percent increase in performance by using projected tags .
Translation Results
Table 2 shows the results of different settings in addition to baseline and state - of - the - art results .
We see that Arabic as a clear exception needs more rounds of training : we train our Arabic model once again on mined data by initializing it by our back -translation model .
5
We have not seen fur-ther improvement by back -translation .
To have a fair comparison , we list the best supervised models for all language pairs ( to the best of our knowledge ) .
In low-resource settings , we outperform strong supervised models that are boosted by backtranslation .
In high- resource settings , our Arabic models achieve very high performance but regarding the fact that the parallel data for Arabic has 18 M sentences , it is quite impossible to reach that level of accuracy .
Figure 5 shows a randomly chosen example from the Gujarati-English development data .
As depicted , we see that the model after back - translation reaches to somewhat the core meaning of the sentence with a bit of divergence from exactly matching the reference .
The final iterative backtranslation output almost catches a correct translation .
We also see that the use of the word " creative " is seen in Google Translate output , a model that is most likely trained on much larger parallel data than what is currently available for public use .
In general , unsupervised translation performs very poorly compared to our approach in all directions .
Captioning Results
Table 4 shows the final results on the Arabic test set using the SacreBLEU measure ( Post , 2018 ) .
First , we should note that similar to ElJundi .
et al. ( 2020 ) , we see lower scales of BLEU scores due to morphological richness in Arabic .
We see that if we initialize our model with the translation model and multitask it with translation and also English captioning , we achieve much higher performance .
It is interesting to observe that translating the English output on the test data to Arabic achieves a much lower result .
This is a strong indicator of the strength of our approach .
We also see that supervised translation fails to perform well .
This might due to the UN translation training dataset which has a different domain from the caption dataset .
Furthermore , we see that our model outperforms Google Translate which is a strong machine translation system , and that is actually what is being used as seed data for manual revision in the Arabic dataset .
Finally , it is interesting to see that our model outperforms supervised captioning .
Multi-tasking make translation performance slightly worse .
Figure 6 shows a randomly picked example with is improving both translation and captioning , but our further investigation shows that it is actually due to lack of training for Arabic .
We have tried the same procedure for other languages but have not observed any further gains .
( Zeman et al. , 2020 ) .
Previous work has used different sub-versions of the Universal Dependencies data in which slight differences are expected .
Input ? ? ? ? ? ? ? ? ? ? ? ? ? .
Outputs
Unsupervised
Ut numerous ? it the mother , onwards , in theover ?
? exualit theotherit theIN ? 19 First sentences + captions + titles A view of the universe from the present to the present day .
Mined Corpora
For example , if the ghazal is more popular than ghazal .
+ Related Language
We need to become more creative than before .
+
One - shot back- translation
For example , we must become more creative than before .
+ Iterative back- translation Meanwhile , we 'll have to become more constructive than before .
Google Translate
That means we have to be more creative than before .
Reference
That means we have to be more constructive than before .
different model outputs .
We see that the two outputs from our approach with multi-tasking are roughly the same but one of them as more syntactic order overlap with the reference while both orders are correct in Arabic as a free-word order language .
The word means " orange " which is close to that means " red " .
The word means " slide " which is correct but other meanings of this word exist in the reference .
In general , we observe that although superficially the BLEU scores for Arabic is low , it is mostly due to its lexical diversity , free-word order , and morphological complexity .
Dependency Parsing Results
Table 3 shows the results for dependency parsing experiments .
We see that our model performs very high in Romanian with a UAS of 74 which is much higher than that of Ahmad et al . ( 2019 ) and slightly lower than that of Rasooli and Collins ( 2019 ) which uses a combination of multi-source annotation projection and direct model transfer .
Our work on Arabic outperforms all previous work and performs even better than using gold -standard parallel data .
One clear highlight is our result in Kazakh .
As mentioned before , by projecting the part- of-speech tags , we achieve roughly 2 percent absolute improvement .
Our final results on Kazakh are significantly higher than that of using gold -standard parallel text ( 7 K sentences ) .
6 Related Work has shown that unsupervised translation models often fail to provide good translation systems for distant languages .
Our work solves this problem by leveraging the Wikipedia data .
Using pivot languages has been used in previous work ( Al - Shedivat and Parikh , 2019 ) , as well as using related languages ( Zoph et al. , 2016 ; Nguyen and Chiang , 2017 ) .
Our work only explores a simple idea of adding one similar language pair .
Most likely , adding more language pairs and using ideas from recent work might improve the performance .
Wikipedia is an interesting dataset for solving NLP problems including machine translation ( Li et al. , 2012 ; Patry and Langlais , 2011 ; Lin et al. , 2011 ; Tufi ? et al. , 2013 ; Barr?n- Cede?o et al. , 2015 ; Wijaya et al. , 2017 ; Ruiter et al. , 2019 ; Srinivasan et al. , 2021 ) .
The WikiMatrix data ( Schwenk et al. , 2019a ) is the most similar effort to ours in terms of using Wikipedia , but with using supervised translation models .
Bitext mining has a longer history of research ( Resnik , 1998 ; Resnik and Smith , 2003 ) in which most efforts are spent on using a seed supervised translation model ( Guo et al. , 2018 ; Schwenk et al. , 2019 b ; Artetxe and Schwenk , 2019 ; Schwenk et al. , 2019a ; Jones and Wijaya , 2021 ) .
Recently , a number of papers have focused on unsupervised extraction of parallel data ( Ruiter et al. , 2019 ; Hangya and Fraser , 2019 ; Keung et al. , 2020 ; Tran et al. , 2020 ; Kuwanto et al. , 2021 ) . Ruiter et al. ( 2019 ) focus on using vector similarity of sentences to extract parallel text from Wikipedia .
Their work does not leverage structural signals from Wikipedia .
Cross-lingual and unsupervised image captioning has been studied in previous work ( Gu et al. , 2018 ; Feng et al. , 2019 ; Song et al. , 2019 b ; Gu et al. , 2019 ; Gao et al. , 2020 ; Burns et al. , 2020 ) .
Unlike previous work , we do not have a supervised translation model .
Cross-lingual transfer of dependency parser have a long history .
We encourage the reader to read a recent survey on this topic ( Das and Sarkar , 2020 ) .
Our work does not use goldstandard parallel data or even supervised translation models to apply annotation projection .
Conclusion
We have described a fast and effective algorithm for learning translation systems using Wikipedia .
We show that by wisely choosing what to use as seed data , we can have very good seed parallel data to mine more parallel text from Wikipedia .
We have also shown that our translation models can be used in downstream cross-lingual natural language processing tasks .
In the future , we plan to extend our approach beyond Wikipedia to other comparable datasets like the BBC World Service .
A clear extension of this work is to try our approach on other cross-lingual tasks .
Moreover , as many captions of the same images in Wikipedia are similar sentences and sometimes translations , multimodal machine translation Caglayan et al. , 2019 ; Hewitt et al. , 2018 ; Yao and Wan , 2020 ) based on this data or the analysis of the data , such as whether more similar languages may share more similar captions ( Khani et al. , 2021 ) are other interesting avenues .
cross-lingual CCA tool ( Faruqui and Dyer , 2014 ) to extract 150 - dimensional vectors .
This tool can be run on a single CPU within a few hours .
