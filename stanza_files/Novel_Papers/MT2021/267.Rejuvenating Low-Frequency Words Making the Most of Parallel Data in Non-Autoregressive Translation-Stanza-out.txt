title
Rejuvenating Low-Frequency Words : Making the Most of Parallel Data in Non-Autoregressive Translation
abstract
Knowledge distillation ( KD ) is commonly used to construct synthetic data for training non-autoregressive translation ( NAT ) models .
However , there exists a discrepancy on lowfrequency words between the distilled and the original data , leading to more errors on predicting low-frequency words .
To alleviate the problem , we directly expose the raw data into NAT by leveraging pretraining .
By analyzing directed alignments , we found that KD makes low-frequency source words aligned with targets more deterministically but fails to align sufficient low-frequency words from target to source .
Accordingly , we propose reverse KD to rejuvenate more alignments for lowfrequency target words .
To make the most of authentic and synthetic data , we combine these complementary approaches as a new training strategy for further boosting NAT performance .
We conduct experiments on five translation benchmarks over two advanced architectures .
Results demonstrate that the proposed approach can significantly and universally improve translation quality by reducing translation errors on low-frequency words .
Encouragingly , our approach achieves 28.2 and 33.9 BLEU points on the WMT14 English - German and WMT16 Romanian - English datasets , respectively .
Our code , data , and trained models are available at https://github.com/ longyuewangdcu/RLFW -NAT .
* Liang Ding and Longyue Wang contributed equally to this work .
Work was done when Liang Ding and Xuebo Liu were interning at Tencent AI Lab .
Introduction Recent years have seen a surge of interest in nonautoregressive translation ( NAT , Gu et al. , 2018 ) , which can improve the decoding efficiency by predicting all tokens independently and simultaneously .
The non-autoregressive factorization breaks conditional dependencies among output tokens , which prevents a model from properly capturing the highly multimodal distribution of target translations .
As a result , the translation quality of NAT models often lags behind that of autoregressive translation ( AT , Vaswani et al. , 2017 ) models .
To balance the trade- off between decoding speed and translation quality , knowledge distillation ( KD ) is widely used to construct a new training data for NAT models ( Gu et al. , 2018 ) .
Specifically , target sentences in the distilled training data are generated by an AT teacher , which makes NAT easily acquire more deterministic knowledge and achieve significant improvement .
Previous studies have shown that distillation may lose some important information in the original training data , leading to more errors on predicting low-frequency words .
To alleviate this problem , Ding et al . ( 2021 b ) proposed to augment NAT models the ability to learn lost knowledge from the original data .
However , their approach relies on external resources ( e.g. word alignment ) and human-crafted priors , which limits the applicability of the method to a broader range of tasks and languages .
Accordingly , we turn to directly expose the raw data into NAT by leveraging pretraining without intensive modification to model architectures ( ?2.2 ) .
Furthermore , we analyze bilingual links in the distilled data from two alignment directions ( i.e. source - to- target and target- to- source ) .
We found that KD makes low-frequency source words aligned with targets more deterministically but fails to align low-frequency words from target to source due to information loss .
Inspired by this finding , we propose reverse KD to recall more alignments for low-frequency target words ( ?2.3 ) .
We then concatenate two kinds of distilled data to maintain advantages of deterministic knowledge and low-frequency information .
To make the most of authentic and synthetic data , we combine three complementary approaches ( i.e. raw pretraining , bidirectional distillation training and KD finetuning ) as a new training strategy for further boosting NAT performance ( ?2.4 ) .
We validated our approach on five translation benchmarks ( WMT14 En-De , WMT16 Ro-En , WMT17 Zh-En , WAT17 Ja-En and WMT19 En-De ) over two advanced architectures ( Mask Predict , Ghazvininejad et al. , 2019 ; Levenshtein Transformer , Gu et al. , 2019 ) .
Experimental results show that the proposed method consistently improve translation performance over the standard NAT models across languages and advanced NAT architectures .
Extensive analyses confirm that the performance improvement indeed comes from the better lexical translation accuracy especially on low-frequency tokens .
Contributions
Our main contributions are : ?
We show the effectiveness of rejuvenating lowfrequency information by pretraining NAT models from raw data . ?
We provide a quantitative analysis of bilingual links to demonstrate the necessity to improve low-frequency alignment by leveraging both KD and reverse KD . ?
We introduce a simple and effective training recipe to accomplish this goal , which is robustly applicable to several model structures and language pairs .
2 Rejuvenating Low-Frequency Words
Preliminaries Non-Autoregressive Translation
Given a source sentence x , an AT model generates each target word y t conditioned on previously generated ones y <t , leading to high latency on the decoding stage .
In contrast , NAT models break this autoregressive factorization by producing target words in parallel .
Accordingly , the probability of generating y is computed as : p( y| x ) = T t=1 p(y t |x ; ? ) ( 1 ) where T is the length of the target sequence , and it is usually predicted by a separate conditional distribution .
The parameters ? are trained to maximize the likelihood of a set of training examples according to L ( ? ) = arg max ? log p( y|x ; ? ) .
Typically , most NAT models are implemented upon the framework of Transformer ( Vaswani et al. , 2017 ) . Knowledge Distillation
Gu et al. ( 2018 ) pointed out that NAT models suffer from the multimodality problem , where the conditional independence assumption prevents a model from properly capturing the highly multimodal distribution of target translations .
Thus , the sequence - level knowledge distillation is introduced to reduce the modes of training data by replacing their original target - side samples with sentences generated by an AT teacher ( Gu et al. , 2018 ; Ren et al. , 2020 ) .
Formally , the original parallel data Raw and the distilled data ? ?
KD can be defined as follows : Raw = {( x i , y i ) }
N i=1 ( 2 ) ? ? KD = {( x i , f s ?t ( x i ) ) |x
i ? Raw s } N i=1 ( 3 ) where f s ?t represents an AT - based translation model trained on Raw data for translating text from the source to the target language .
N is the total number of sentence pairs in training data .
As shown in Figure 1 ( a ) , well - performed NAT models are generally trained on ? ? KD data instead of Raw .
Pretraining with Raw Data Motivation Gao et al. ( 2018 ) showed that more than 90 % of words are lower than 10e - 4 frequency in WMT14 En - De dataset .
This token imbalance problem biases translation models towards overfitting to frequent observations while neglecting those low-frequency observations ( Gong et al. , 2018 ; Nguyen and Chiang , 2018 ; .
Thus , the AT teacher f s ?t tends to generate more high - frequency tokens and less low-frequency tokens during constructing distilled data ? ? KD .
On the one hand , KD can reduce the modes in training data ( i.e. multiple lexical choices for a source word ) , which lowers the intrinsic uncertainty and learning difficulty for NAT Ren et al. , 2020 ) , making it easily acquire more deterministic knowledge .
On the other hand , KD aggravates the imbalance of high-frequency and low-frequency words in training data and lost some important information originated in raw data .
Ding et al. ( 2021 b ) revealed the side effect of distilled training data , which cause lexical choice errors for low-frequency words in NAT models .
Accordingly , they introduced an extra bilingual data-dependent prior objective to augments NAT models the ability to learn the lost knowledge from raw data .
We use their findings as our departure point , but rejuvenate low-frequency words in a more simple and direct way : directly exposing raw data into NAT via pretraining .
Our Approach Many studies have shown that pretraining could transfer the knowledge and data distribution , especially for rare categories , hence improving the model robustness ( Hendrycks et al. , 2019 ; Mathis et al. , 2021 ) .
Here we want to transfer the distribution of lost information , e.g. lowfrequency words .
As illustrated in Figure 1 ( b ) , we propose to first pretrain NAT models on Raw data and then continuously train them on ? ? KD data .
The raw data maintain the original distribution especially on low-frequency words .
Although it is difficult for NAT to learn high-mode data , the pretraining can acquire general knowledge from authentic data , which may help better and faster learning further tasks .
Thus , we early stop pretraining when the model can achieve 90 % of the best performance of raw data in terms of BLEU score ( Platanios et al. , 2019 ) 1 . In order to keep the merits of low-modes , 1 In preliminary experiments , we tried another simple strategy : early - stop at fixed step according to the size of training data ( e.g. training 70 K En-De and early stop at 20K / 30K / 40K , respectively ) .
We found that both strategies achieve KD " indicate syntactic data distilled by KD and reverse KD , respectively .
The subscript " S " or " T " is short for source - or target-side .
The low-frequency words are highlighted with colors and italics are incorrect translations .
we further train the pretrained model on distilled data ? ? KD .
As it is easy for NAT to learn deterministic knowledge , we finetune the model for the rest steps .
For fair comparison , the total training steps of the proposed method are same as the traditional one .
In general , we expect that this training recipe can provide a good trade - off between raw and distilled data ( i.e. high -modes and complete vs. low-modes and incomplete ) .
Bidirectional Distillation Training Analyzing Bilingual Links in Data KD simplifies the training data by replacing low-frequency target words with high- frequency ones .
This is able to facilitate easier aligning source words to target ones , resulting in high bilingual coverage ( Jiao et al. , 2020 ) .
Due to the information loss , we argue that KD makes lowfrequency target words have fewer opportunities to align with source ones .
To verify this , we propose a method to quantitatively analyze bilingual links from two directions , where low-frequency words similar performance .
are aligned from source to target ( s ? t ) or in an opposite direction ( t ? s ) .
The method can be applied to different types of data .
Here we take s ?
t links in Raw data as an example to illustrate the algorithm .
Given the WMT14 En- De parallel corpus , we employ an unsupervised word alignment method 2 ( Och and Ney , 2003 ) to produce a word alignment , and then we extract aligned links whose source words are low-frequency ( called s ? t LFW Links ) .
Second , we randomly select a number of samples from the parallel corpus .
For better comparison , the subset should contains the same i in Equation ( 2 ) as that of other type of datasets ( e.g. i in Equation ( 3 ) for ? ? KD ) .
Finally , we calculate recall , precision , F1 scores based on low-frequency bilingual links for the subset .
Recall ( R ) represents how many low-frequency source words can be aligned to targets .
Precision ( P ) means how many aligned low-frequency links are correct according to human evaluation .
F1 is the harmonic mean between precision and recall .
Similarly , we can analyze t ?
s LFW
Links by considering low-frequency targets .
Table 1 shows the results on low-frequency links .
Compared with Raw , ? ? KD can recall more s ?
t LFW links ( 73.4 vs. 66.4 ) with more accurate alignment ( 89.2 vs. 73.3 ) .
This demonstrates the effectiveness of KD for NAT models from the bilingual alignment perspective .
However , in the t ? s direction , there are fewer LFW links ( 69.9 vs. 72.3 ) with worse alignment quality ( 79.1 vs. 80.6 ) in ? ? KD than those in Raw .
This confirms our claim that KD harms NAT models due to the loss of lowfrequency target words .
Inspired by these findings , it is natural to assume that reverse KD exhibits complementary properties .
Accordingly , we conduct the same analysis method on ? ? KD data , and found better t ?
s links but worse s ?
t links compared with Raw .
Take the Zh- En sentence pair in Table 2 for example , ? ? KD retains the source side lowfrequency Chinese words " ? " ( Raw S ) but generates the high-frequency English words " Heckman " instead of the golden " Hackman " ( ? ? KD T ) .
On the other hand , ? ? KD preserves the low-frequency English words " Hackman " ( Raw T ) but produces the high-frequency Chinese words " ? " ( ? ? KD S ) .
Our Approach Based on analysis results , we propose to train NAT models on bidirectional distil-lation by concatenating two kinds of distilled data .
The reverse distillation is to replace the source sentences in the original training data with synthetic ones generated by a backward AT teacher .
3 According to Equation 3 , ? ? KD can be formulated as : ? ? KD = {( y i , f t ?s ( y i ) ) |y i ? Raw t } N i=1 ( 4 ) where f t ?s represents an AT - based translation model trained on Raw data for translating text from the target to the source language .
Figure 1
To prove the universality of our approach , we further experiment on different data volumes , which are sampled from WMT19 En- De .
4 The Small and Medium corpora respectively consist of 1.0M and 4.5 M sentence pairs , and Large one is the whole dataset which contains 36 M sentence pairs .
We preprocess all data via BPE ( Sennrich et al. , 2016 ) with 32 K merge operations .
We use tokenized BLEU ( Papineni et al. , 2002 ) as the evaluation metric , and sign-test ( Collins et al. , 2005 ) for statistical significance test .
The translation accuracy of lowfrequency words is measured by AoLC ( Ding et al. , 2021 b ) , where word alignments are established based on the widely - used automatic alignment tool GIZA ++ ( Och and Ney , 2003 ) .
Models
We validated our research hypotheses on two state - of- the - art NAT models : ? Mask -Predict ( MaskT , Ghazvininejad et al. 2019 ) that uses the conditional mask LM ( Devlin et al. , 2019 ) to iteratively generate the target sequence from the masked input .
We followed its optimal settings to keep the iteration number as 10 and length beam as 5 .
? Levenshtein Transformer ( LevT , Gu et al. 2019 ) that introduces three steps : deletion , placeholder and token prediction .
The decoding iterations adaptively depends on certain conditions .
We closely followed previous works to apply sequence - level knowledge distillation to NAT ( Kim and Rush , 2016 ) 2015 ) to train our models .
We followed the common practices ( Ghazvininejad et al. , 2019 ; Kasai et al. , 2020 ) to evaluate the performance on an ensemble of top 5 checkpoints to avoid stochasticity .
Note that the total training steps of the proposed approach ( in ?2.2? 2.4 ) are identical with those of the standard training ( in ?2.1 ) .
Taking the best training strategy ( Raw ? ? ? KD + ? ? KD ? ? ? KD ) for example , we empirically set the training step for each stage is 20K , 20 K and 30K , respectively .
And Ro - En models respectively need 8K , 8 K and 9 K steps in corresponding training stage .
Results Comparison with Previous Work Table 3 lists the results of previous competitive NAT models ( Gu et al. , 2018 ; Lee et al. , 2018 ; Kasai et al. , 2020 ; Gu et al. , 2019 ; Ghazvininejad et al. , 2019 ) on the WMT16 Ro - En and WMT14 En- De benchmark .
We implemented our approach on top of two advanced NAT models ( i.e. Mask - Predict and Levenshtein Transformer ) .
Compared with standard NAT models , our training strategy significantly and consistently improves translation performance ( BLEU ? ) across different language pairs and NAT models .
Besides , the improvements on translation performance are mainly due to a increase of translation accuracy on low-frequency words ( ALF ? ) , which reconfirms our claims .
For instance , our method significantly improves the standard Mask - Predict model by + 0.8 BLEU score with a substantial + 3.6 increase in ALF score .
Encouragingly , our approach push the existing NAT models to achieve new SOTA performances ( i.e. 28.2 and 33.9 BLEU on En-De and Ro-En , respectively ) .
It is worth noting that our data- level approaches neither modify model architecture nor add extra training loss , thus do not increase any latency ( " Speed " ) , maintaining the intrinsic advantages of non-autoregressive generation .
We must admit that our strategy indeed increase the amount of computing resources due to that we should train f t ?s AT teachers for building ? ? KD data .
Results on Other Language Pairs Table 4 lists the results of NAT models on Zh-En and Ja- En language pairs , which belong to different language families ( i.e. Indo-European , Sino-Tibetan and Japonic ) .
Compared with baselines , our method significantly and incrementally improves the translation quality in all cases .
For Zh-En , LFR achieves on average + 0.8 BLEU improvement over the traditional training , along with increasing on average + 3.0 % accuracy on low-frequency word translation .
For long- distance language pair Ja-En , our method still improves the NAT model by on average + 0.7 BLEU point with on average + 2.2 ALF .
Furthermore , NAT models with the proposed training strategy perform closely to their AT teachers ( i.e. 0.2 ? BLEU ) .
This shows the effectiveness and universality of our method across language pairs .
The small and medium datasets are sampled from the large WMT19 En- De dataset , and evaluations are conducted on the same testset .
" ? " indicates statistically significant difference ( p < 0.05 ) from corresponding baselines .
Results on Domain Shift Scenario
The lexical choice must be informed by linguistic knowledge of how the translation model 's input data maps onto words in the target domain .
Since low-frequency words get lost in traditional NAT models , the problem of lexical choice is more severe under domain shift scenario ( i.e. models are trained on one domain but tested on other domains ) .
Thus , we conduct evaluation on WMT14 En- De models over five out - of- domain test sets ( M?ller et al. , 2020 ) , including law , medicine , IT , Koran and movie subtitle domains .
As shown in Table 5 , standard NAT models suffer large performance drops in terms of BLEU score ( i.e. on average - 2.9 BLEU over AT model ) .
By observing these outputs , we found a large amount of translation errors on low-frequency words , most of which are domain-specific terminologies .
In contrast , our approach improves translation quality ( i.e. on average - 1.4 BLEU over AT model ) by rejuvenating low-frequency words to a certain extent , showing that LFR increases the domain robustness of NAT models .
Results on Different Data Scales
To confirm the effectiveness of our method across different data sizes , we further experiment on three En- De datasets at different scale .
The small - and mediumscale training data are randomly sampled from WM19 En- De corpus , containing about 1.0M and 4.5 M sentence pairs , respectively .
The large-scale one is collected from WMT19 , which consists of 36 M sentence pairs .
We report the BLEU scores on same testset newstest2019 for fair comparison .
We employs base model to train the small- scale AT teacher , and big model with large batch strategy ( i.e. 458 K tokens / batch ) to build the AT teachers for medium - and large-scale .
As seen in
Analysis
We conducted extensive analyses to better understand our approach .
All results are reported on the Mask - Predict models .
Accuracy of Lexical Choice
To understand where the performance gains come from , we conduct fine - grained analysis on lexical choice .
We divide " All " tokens into three categories based on their frequency , including " High " , " Medium " and " Low " .
Following Ding et al. ( 2021 b ) , we measure the accuracy of lexical choice on different frequency of words .
.5 72.3 73.7 81.7 75.3 64.8 76.5 83.5 76.7 68.9
Table 8 : Analysis on different frequency words in terms of accuracy of lexical choice .
We split " All " words into " High " , " Medium " and " Low " categories .
Shades of cell color represent differences between ours and KD .
Model En ALF scores , confirming the necessity of our work .
Takeaway : 1 ) Pretraining is more effective than combination on utilizing data manipulation strategies ; 2 ) raw data and bidirectional distilled data are complementary to each other ; 3 ) it is indispensable to finetune models on ? ?
KD in the last stage .
Our Approach Works for AT Models
Although our work is designed for NAT models , we also investigated whether our LFT method works for general cases , e.g. autoregressive models .
We used Transformer - BIG as the teacher model .
For fair comparison , we leverage the Transformer - BASE as the student model , which shares the same model capacity with NAT student ( i.e. MaskT ) .
The result lists in Table 11 .
As seen , AT models also suffer from the problem of low-frequency words when using knowledge distillation , and our approach also works for them .
Takeaway :
Our method works well for general cases through rejuvenating more low-frequency words .
Related Work Low-Frequency Words
Benefiting from continuous representation learned from the training data , NMT models have shown the promising performance .
However , Koehn and Knowles ( 2017 ) point that low-frequency words translation is still one of the key challenges for NMT according to the Zipf 's law ( Zipf , 1949 ) .
For AT models , Arthur et al . ( 2016 ) Bogoychev and Sennrich ( 2019 ) show that forwardand backward - translations ( FT / BT ) could both boost the model performances , where FT plays the role of domain adaptation and BT makes the translation fluent .
Fadaee and Monz ( 2018 ) sample the monolingual data with more difficult words ( e.g. rare words ) to perform BT , achieving significant improvements compared with randomly sampled BT .
Nguyen et al. ( 2020 ) diversify the data by applying FT and BT multiply times .
However , different from AT , the prerequisite of training a well - performed NAT model is to perform KD .
We compared with related works in Table 10 and found that our approach consistently outperforms them .
Note that all the ablation studies focus on exploiting the parallel data without augmenting additional data .
Non-Autoregressive Translation
A variety of approaches have been exploited to bridge the performance gap between NAT and AT models .
Some researchers proposed new model architectures ( Lee et al. , 2018 ; Ghazvininejad et al. , 2019 ; Gu et al. , 2019 ; Kasai et al. , 2020 ) , aided with additional signals Ran et al. , 2019 ; Ding et al. , 2020 ) , introduced sequential information Shao et al. , 2019 ; Guo et al. , 2020 ; Hao et al. , 2021 ) , and explored advanced training objectives Du et al. , 2021 ) .
Conclusion
In this study , we propose simple and effective training strategies to rejuvenate the low-frequency information in the raw data .
Experiments show that our approach consistently and significantly improves translation performance across language pairs and model architectures .
Notably , domain shift is an extreme scenario to diagnose low-frequency translation , and our method significant improves them .
Extensive analyses reveal that our method improves the accuracy of lexical choices for low-frequency source words , recalling more low-frequency words in translations as well , which confirms our claim .
Figure 1 : 1 Figure 1 : An illustration of different strategies for training NAT models .
" distill " and " reverse distill " indicate sequence - level knowledge distillation with forward and backward AT teachers , respectively .
The data block in transparent color means source - or target - side data are synthetically generated .
Best view in color .
