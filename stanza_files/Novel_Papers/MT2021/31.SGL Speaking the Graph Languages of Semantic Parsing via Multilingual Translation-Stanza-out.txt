title
SGL : Speaking the Graph Languages of Semantic Parsing via Multilingual Translation
abstract
Graph - based semantic parsing aims to represent textual meaning through directed graphs .
As one of the most promising general - purpose meaning representations , these structures and their parsing have gained a significant interest momentum during recent years , with several diverse formalisms being proposed .
Yet , owing to this very heterogeneity , most of the research effort has focused mainly on solutions specific to a given formalism .
In this work , instead , we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation ( MNMT ) , and propose SGL , a many - to - many seq2seq architecture trained with an MNMT objective .
Backed by several experiments , we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation : we report competitive performances on AMR and UCCA parsing , especially once paired with pre-trained architectures .
Furthermore , we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing : SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and , once these examples are included as well , sets an unprecedented state of the art in this task .
We release our code and our models for research purposes at https : //github.com/SapienzaNLP/sgl.
Introduction
Being able to associate natural language text with well - defined and machine - actionable meaning representations , i.e. the task of semantic parsing ( SP ) , is one of the holy grails in Natural Language Processing ( NLP ) and Understanding ( Navigli , 2018 ) .
Considering how a breakthrough in this direction would empower NLP systems to explictly make sense of natural language , the evergrowing interest semantic parsing has been receiving really comes as no surprise .
Graph - based formalisms such as Abstract Meaning Representation ( Banarescu et al. , 2013 , AMR ) , Elementary Dependency Structures ( Oepen and L?nning , 2006 , EDS ) , Prague Tectogrammatical Graphs ( Haji ?
et al. , 2012 , PTG ) , Universal Conceptual Cognitive Annotation ( Abend and Rappoport , 2013 , UCCA ) , inter alia , are emerging as the de facto standard for general - purpose meaning representations and have shown potential in Machine Translation ( Song et al. , 2019 ) , Text Summarization ( Hardy and Vlachos , 2018 ) , Human-Robot Interaction ( Bonial et al. , 2020 ) , and as evaluation metrics ( Sulem et al. , 2018 ; Xu et al. , 2020 b ) .
These formalisms propose encoding meaning through directed graphs , however , each of them builds upon different linguistic assumptions , aims to target different objectives and , at a more practical level , assigns different functions to nodes and edges .
For instance , while AMR uses nodes to encode concepts and edges to express the semantic relations between them , UCCA proposes using text tokens as terminal nodes and building graphs on top of them .
As a result of this heterogeneous landscape , often referred to as framework -specific balkanization ( Oepen et al. , 2020 ) , graph - based semantic parsing has seen a proliferation of framework -specific solutions .
However , approaches capable of competitively scaling across formalisms represent a natural desideratum , and recent works have started to explore this direction , examining the usage of multi-task learning in different architectures ( Hershcovich et al. , 2018 ; Oepen et al. , 2019 ) , or casting different formalisms under a unified framework where models can be trained to perform graph transduction ( Zhang et al. , 2019 b ) .
Nevertheless , despite achieving promising results , research in this direction has been hindered by the general lack of training data that afflicts semantic parsing .
Indeed , due to the inherent complexity of this task , annotated corpora are still scarce and prohibitively expensive to expand .
In this work , we set ourselves to address these issues and , in particular , we propose Speak the Graph Languages ( SGL ) , a many - to - many seq2seq architecture which we show to competitively scale across formalisms and across languages .
1
The key idea is to train a seq2seq model with a Multilingual Neural Machine Translation ( MNMT ) objective , where , given an input text and an identifier denoting the desired output formalism , a single shared model has to learn to translate towards the corresponding linearized graph .
We use AMR and UCCA as our cases in point to show the effectiveness of this framework .
In particular , we show that , once the learning procedure also considers large parallel corpora coming from Machine Translation ( MT ) , this configuration becomes an effective approach for framework - independent parsing via a single model .
Even more interestingly , this model scales surprisingly well to cross-lingual parsing and is capable of navigating through translation paths like IT ? AMR , 2 which it has never seen during training .
The contributions of this work are therefore as follows : ?
We reframe semantic parsing towards multiple formalisms and from multiple languages as multilingual machine translation ; ?
On AMR parsing , our framework achieves competitive performances , surpassing most of its current competitors once paired with a pre-trained Transformer ; ?
We outperform all current alternatives in crosslingual AMR parsing without ever seeing non-English to AMR examples at training time and push the current state of the art even further once we include these examples ; ?
On UCCA parsing , we reach competitive results , outperforming a strong BERT - powered baseline ( Hershcovich and Arviv , 2019 ) .
We release our code and our models for research purposes .
Related Work
Our work is mainly concerned with semantic parsing in UCCA and AMR , considering also the cross -
1 By across languages , we mean that the model is capable of performing cross-lingual semantic parsing as defined for AMR by Damonte and Cohen ( 2018 ) .
Unless otherwise specified , we will follow this perspective throughout the paper .
2 IT stands for Italian .
lingual setting introduced by Damonte and Cohen ( 2018 ) for the latter .
Semantic Parsing Arguably among the formalisms that have drawn the most interest , AMR has seen the emergence of a rich yet dedicated literature , with recent approaches that can be roughly clustered into two groups .
On the one hand , several graph - based solutions have been proposed ( Lyu and Titov , 2018 ; Zhang et al. , 2019a , b ; Zhou et al. , 2020 ; Cai and Lam , 2020 ) ; among these solutions , Zhou et al . ( 2020 ) show the effectiveness of enhancing an aligner - free parser with latent syntactic information , whereas Cai and Lam ( 2020 ) present an iterative method to build and refine AMR graphs incrementally .
On the other hand , translation - based approaches , where seq2seq models are trained to translate from natural language text to linearized graphs , have been shown to reach competitive performances , despite the scarcity of training data ( Konstas et al. , 2017 ; van Noord and Bos , 2017 ; Ge et al. , 2019 ) .
Continuing this latter direction and arguably closest to our work , Xu et al . ( 2020a ) and Bevilacqua et al . ( 2021 ) show that these models , once paired with adequate pre-training , can behave on par or better than dedicated and more sophisticated graph - based alternatives , surpassing the performances of Cai and Lam ( 2020 ) .
In particular , similarly to our work , Xu et al . ( 2020a ) leverage a multilingual framework inspired by Johnson et al . ( 2017 ) and explore the possibility of pre-training on a range of related tasks , including MT ; however , their focus is limited to showing the effectiveness of transfer learning from related tasks to English AMR parsing .
Conversely , here we show that the benefits of multilingual seq2seq frameworks are not limited to English TEXT - to - AMR but , rather , that they enable astonishing performances on unseen translation paths such as IT ?
AMR and competitive results on other frameworks , using UCCA as our case in point .
In this sense , we continue the recent cross -framework trend formally started by the shared task of Oepen et al . ( 2019 ) , exploring the possibility of using translation - based approaches for framework - independent parsing , as opposed to the transition - based parsers proposed in that seminal work .
Our findings are in line with the recent results reported by Oepen et al . ( 2020 ) and , in particular , by Ozaki et al . ( 2020 ) , where the authors cast semantic parsing in multiple formalisms as translation towards a novel Plain Graph Notation [ < root_0 > L [ < L_0 > T [ After ] ] H [ < H_0 > P [ < P_0 >
T [ graduation ] ] A * [ < A_0 > T [ John ] ] ] U [ < U_0 > T [ , ] ] ] H [ < H_1 > A [ < A_1 > R [ < R_0 >
T [ to ] ] C [ < C_0 >
T [ Paris ] ] ] A < A_0 > P [ < P_1 >
T [ moved ] ] ] ( d ) UCCA Linearization Figure 1 : AMR and UCCA graphs , along with their linearizations , for the sentence " After graduation , John moved to Paris " .
To ease readability , linearizations are shown with newlines and indentation ; however , when fed to the neural model , they are in a single - line single -space format . ( PGN ) they devise .
However , whereas they train different independent models for each framework , we explore the possibility of using a single multilingual model .
Cross-lingual AMR
While most of the research effort in the AMR community has been focused on English only , the seminal work of Damonte and Cohen ( 2018 ) gave rise to an interesting new direction , i.e. exploring the extent to which AMR can act as an interlingua .
The authors introduced a new problem , cross-lingual AMR parsing , and defined it as the task of recovering , given a sentence in any language , the AMR graph corresponding to its English translation .
Using an adapted version of the transition - based parser originally proposed by Damonte et al . ( 2017 ) and training it on silver data generated through annotation projection , they examined whether AMR graphs could be recovered starting from non-English sentences .
Even though their models fell short when compared to MT alternatives , 3 their work showed promising results and suggested that , despite translation divergences , AMR could act effectively as an interlingua .
Annotation projection has been focal in subsequent work as well .
Blloshmi et al. ( 2020 ) propose an aligner - free cross-lingual parser , thus disposing of the need for word alignments in the annotation projection pipeline ; their parser manages to outperform MT alternatives when both annotation projection and these baselines have access to comparable amounts of data .
Conversely , Sheth et al. ( 2021 ) leverage powerful contextualized word embeddings to improve the foreign - text- to- English - AMR alignments , surpassing all previous approaches and , most importantly , the yet -unbeaten MT baselines that have access to larger amounts of data .
We stand out from previous research and show that , as a matter of fact , annotation projection techniques are not needed to perform cross-lingual AMR parsing .
By jointly training on parallel corpora from MT and the EN ?
SP data we have , we find that a multilingual model can navigate unseen translation paths such as IT ?
AMR effectively , outperforming all current approaches by a significant margin ; yet , annotation projection is naturally beneficial and , when its training data are taken into account as well , SGL pushes performances even further .
Speak the Graph Languages ( SGL )
In this section , we describe SGL , our proposed approach to graph - based semantic parsing .
We first explain the graph linearizations we employ for AMR and UCCA , along with their delinearizations ( ?3.1 ) .
We then describe the seq2seq modelling approach we use ( ?3.2 ) and , finally , we present our multilingual framework ( ?3.3 ) .
Graph Linearizations
We now describe how we convert the considered meaning representations into translatable text sequences ( linearization ) , along with their reverse process ( delinearization ) .
For AMR parsing , as in van Noord and Bos ( 2017 ) , we first simplify AMR graphs by removing variables and wiki links .
We then convert these stripped AMR graphs into trees by duplicating coreferring nodes .
At this point , in order to obtain the final linearized version of a given AMR , we concatenate all the lines of its PENMAN notation ( Goodman , 2020 ) together , replacing newlines and multiple spaces with single spaces ( Figure 1a and 1 b ) .
Conversely , delinearization is performed by assigning a variable to each predicted concept , per-forming Wikification , 4 restoring co-referring nodes and , where possible , repairing any syntactically malformed subgraph .
5
For both phases , we use the scripts released by van Noord and Bos ( 2017 ) .
6 For UCCA parsing , we employ a Depth -First Search ( DFS ) approach : starting from the root , we navigate the graph , using square brackets to delimit subgraph boundaries and special variables to denote terminal and non-terminal nodes ; remote edges are denoted by a special modifier appended to their labels , while re-entrancies , that is , edges whose target is a node already seen , are handled by simply entering the respective variable ( Figure 1 c and 1d ) .
Similarly to AMR , delinearization is performed by back - parsing this sequence into a UCCA graph , repairing malformed subgraphs when possible ; 7 additionally , as terminal nodes are anchored in UCCA , we remove those whose anchoring is impossible .
The linearization and delinearization scripts for this schema are released along with the rest of our code .
Sequence-to-sequence Modelling
We employ neural seq2seq models based upon the Transformer architecture ( Vaswani et al. , 2017 ) .
This architecture is essentially composed of two building blocks , namely , a Transformer encoder and a Transformer decoder .
The encoder is a stack of N identical layers , each made up of two sublayers : the first is a multi-head self-attention mechanism , while the second is a position - wise fully connected feed -forward network .
The decoder follows a similar architecture , presenting , however , an additional sub-layer that performs multi-head attention over the output of the encoder .
Within this work , we use two different kinds of Transformer architecture , Cross and mBART .
Cross is a randomly initialized Transformer closely following the architecture depicted by Vaswani et al . ( 2017 ) , except for a significant difference : we leverage a factorized embedding parameterization ( Lan et al. , 2020 ) , that is , we decompose the large vocabulary embedding matrix into two smaller matrices .
While the first of these represents the actual embedding matrix and projects one- hot vectors into an embedding space whose dimension is lower than the Transformer hidden size , the second one takes care of projecting these intermediate representations towards the actual Transformer hidden space .
This technique significantly reduces the number of parameters and , within the context of our experiments , did not show any significant performance penalty .
On the other hand , mBART is a multilingual Transformer pre-trained in many languages over large-scale monolingual corpora .
As AMR and UCCA are naturally not included among the supported languages in the vocabulary , we apply an architectural change to mBART and increase its vocabulary with two new language ids .
More specifically , we augment its embedding matrix by adding two additional vectors , which we randomly initialize as in Tang et al . ( 2021 ) .
Multilingual Framework
In order to empower our models to support translation from and towards multiple languages , we employ a data-driven approach : we replace the start token of the decoder with a special tag specifying the language the encoder representations should be unrolled towards .
Figure 2 shows an example of this schema .
It is worth pointing out that , while for Cross we do not feed the source language to the encoder , when using the mBART model we follow its input format and do provide it .
Once data have been tagged according to this schema , we train a many - to -many translation model on both the semantic parsing and Englishcentric parallel corpora .
8 Considering that our focus is on semantic parsing , we perform oversampling on the AMR and UCCA datasets .
Furthermore , when considering the parallel corpora from MT , we flip the training direction with probability 0.5 , hence allowing our model to see at training time both the X ? EN and EN ?
X training directions ; we argue that this stochastic flip benefits our models in multiple ways : ? As EN ?
X shares the source language with both EN ? AMR and EN ? UCCA , this results in positive transfer ; ?
As AMR , UCCA and EN are significantly related , X ? EN also results in positive transfer ( similar target language ) ; ?
Finally , X ? EN allows our model to navigate unseen translation paths ( i.e. zero-shot ) such as IT ? AMR and thus tackle tasks like crosslingual AMR parsing .
Experimental Setup
We assess the effectiveness of our proposed approach by evaluating its performance on all translation paths where the target language is a graph formalism , the only exception being X ? UCCA , with X any language but English .
This choice is motivated by the fact that , differently from AMR where cross-lingual AMR aims to produce Englishbased meaning representations ( Damonte and Cohen , 2018 ) , UCCA builds graphs on top of its tokens which are , consequently , inherently in the same language as the input text ; we leave exploring this direction to future work .
Models
We choose to use both Cross , a randomly initialized Transformer , and mBART , a multilingual pretrained Transformer , to better grasp the effects of this joint multilingual framework in different regimes .
In particular , we explore the following configurations : ? models trained only on a single semantic parsing task ( AMR or UCCA parsing ) and without considering any parallel data , denoted by Cross st and mBART st ; ? models trained on both semantic parsing tasks and the MT data , denoted by Cross mt and mBART mt .
Furthermore , so as to explore whether the training schedules we use result in underfitting for AMR and UCCA , we also consider Cross f t mt and mBART f t mt , that is , Cross mt and mBART mt fine-tuned with a training schedule biased towards the semantic parsing formalism that is being considered .
9
Datasets and Preprocessing AMR For AMR parsing , we use AMR - 2.0 ( LDC2017T10 ) and its recently released expansion , AMR -3.0 ( LDC2020 T02 ) , amounting , respectively , to 39 260 and 59 255 manually - created sentence - graph pairs .
Cross-Lingual AMR
We use Abstract Meaning Representation 2.0 - Four Translations ( Damonte and Cohen , 2020 ) to investigate the performance of SGL on cross-lingual AMR parsing .
This corpus contains translations of the sentences in the test set of AMR - 2.0 in Chinese ( ZH ) , German ( DE ) , Italian ( IT ) and Spanish ( ES ) .
UCCA
We replicate the setting of the CoNLL 2019 Shared Task ( Oepen et al. , 2019 ) .
We train our models using the freely available 10 UCCA portion of the training data ; this corpus amounts to 6 572 sentence - graph pairs , drawn from the English Web Treebank ( 2012T13 ) and English Wikipedia articles on celebrities .
As no official development set was included in the data release , following Hershcovich and Arviv ( 2019 ) , we reserve 500 instances and use them as the validation set .
To the best of our knowledge , the full evaluation data have not been released yet and , therefore , we compare with state - of - the - art alternatives and report results only on The Little Prince , a released subset consisting of 100 manually - tagged sentences sampled from the homonymous novel .
Parallel Data
We use English-centric parallel corpora in four languages , namely , Chinese , German , Italian and Spanish ; we employ Mul-tiUN ( Tiedemann , 2012 ) for Chinese and Spanish , ParaCrawl ( Espl ?
et al. , 2019 ) for German , and Europarl ( Tiedemann , 2012 ) for Italian .
We perform a mild filtering over all the available parallel sentences and then take the first 5 M out of these .
11 Preprocessing
We do not perform any preprocessing or tokenization , except for the graph linearizations explained in ?3.1 and Chinese simplification .
12 Instead , we directly apply subword tokenization with a Unigram Model ( Kudo , 2018 ) .
When working with Cross in a single - task setting on AMR or UCCA , we follow Ge et al . ( 2019 ) and use a vocabulary size of 20 k subwords ; instead , when working in the multilingual setting , we increase this value to 50 k so as to better accommodate the increased amount of languages .
Conversely , when using mBART , we always use the original vocabulary consisting of 250 k subwords .
Evaluation
We evaluate AMR and cross-lingual AMR parsing by using the Smatch score 13 , a metric that computes the overlap between two graphs .
Furthermore , in order to have a better picture of the systems ' performances , we also re-port the fine- grained scores as computed by the evaluation toolkit 14 of Damonte et al . ( 2017 ) .
For UCCA parsing , we employ the official evaluation metric 15 of the shared task , conceptually similar to the Smatch score .
Results
We now report the results SGL achieves focusing on the following translation paths : i ) EN ? AMR ( ?5.1 ) ; ii ) X ? AMR , with X any language among Chinese , German , Italian and Spanish ( ?5.2 ) ; iii ) EN ? UCCA ( ?5.3 ) .
AMR Parsing
We report the Smatch and fine- grained scores that SGL and its current state - of - the - art alternatives attain on AMR - 2.0 in Xu et al . ( 2020a ) .
Considering the similarity between the two approaches , this difference is likely caused by the increased number of tasks our model is asked to handle .
Once we replace Cross with mBART , all performances rise significantly .
In particular , even mBART st , a single - task variant with no additional data , outperforms all its alternatives except for SPRING and SPRING bart ( Bevilacqua et al. , 2021 ) , highlighting the potential of fully pre-trained Transformer language models for translation - based approaches .
mBART mt and mBART f t mt push performances further up , showing that the MT data are beneficial even in this pretrained setting and that the multi-task training set , which enables a single shared model to scale across formalisms and languages , is not detrimental to English AMR parsing .
However , arguably more interesting is the comparison between the performances of mBART models and SPRING , which , in contrast , builds upon the English-only BART .
In particular , as SPRING bart outperforms even mBART f t mt , this finding suggests that , as expected , BART is more suitable than mBART when dealing with English AMR .
However , as we show in ?5.2 , our choice is beneficial for cross-lingual AMR parsing and results in an interesting trade- off .
Finally , we also evaluate SGL on AMR - 3.0 and report the results of Cross f t mt , mBART st and mBART f t mt when trained on this dataset ( Figure 1 bottom ) .
Overall , we witness a similar trend compared to AMR -2.0 .
Cross-lingual AMR Parsing
We now show the performances of SGL on crosslingual AMR parsing in terms of Smatch score over Chinese ( ZH ) , German ( DE ) , Italian ( IT ) and Spanish ( ES ) .
For comparison , we report the results of the systems proposed by Damonte and Cohen ( 2018 , AMREAGER ) , Blloshmi et al . ( 2020 , XL - AMR ) and Sheth et al . ( 2021 ) ; along with their best systems , we also show the strongest MT baseline reported in Damonte and Cohen ( 2018 , AMREAGER M T ) and the zero-shot configuration explored in Blloshmi et al . ( 2020 falling short only when compared to the recent work of Sheth et al . ( 2021 ) ; in particular , it surpasses the strong AMREAGER M T baseline .
The most interesting aspect of this result is that Cross f t mt attains these performances without ever seeing at training time any X ?
AMR translation path ; this is in marked contrast with all previous literature and with the systems we report in Table 2 .
This finding clearly highlights the effectiveness of transfer learning and , by extension , of our proposed framework in this setting .
Secondly , the performances mBART st achieve are astounding under multiple perspectives .
First , to the best of our knowledge , it is the first reported result of AMR systems achieving competitive performances on cross-lingual AMR parsing in a fully zero-shot configuration : mBART st is fine- tuned solely on EN ? AMR and then applied directly to X ? AMR translation ; especially when compared to XL - AMR ? , the only similar approach we are aware of , the gap is significant .
Second , among the languages we consider , the case of Chinese is especially interesting as it appears to require constrained decoding in order to work properly : in particular , we restrict the model to generate only subwords whose characters belong to the English alphabet .
16
If we were to perform ZH ?
AMR parsing with no additional decoding machinery , as for the other languages , performances would be significantly lower , with mBART st attaining only 31.9 .
This performance drop is caused by the model leaving some nodes of the graph untranslated , i.e. named entities left written in Chinese ( ? rather than Obama ) , which disrupts the auto-regressive nature of the decoding procedure and , besides , eventually results in a penalized Smatch score .
Finally , despite the larger amount of pre-training mBART has been exposed to , its bigger capacity and better Smatch score on English , mBART st still falls short when compared to Cross f t mt , highlighting the benefits of seeing related translation directions at training time . , XL - AMR ? ) .
mBART mt pushes the bar further up , with performances on German , Spanish and Italian that are now only roughly 10 points behind their English counterparts .
As mBART mt significantly outperforms mBART st , this result shows that , despite the massive pretraining , parallel data are still beneficial for cross-lingual AMR .
Moreover , differently from English AMR , mBART f t mt does not yield an improvement and , in fact , performances slightly drop on average .
While the scores mBART mt attains are already unprecedented , it is natural to wonder whether annotation projection ( AP ) might yield a further beneficial effect .
To this end , similarly to Blloshmi et al . ( 2020 ) , we translate the input sentences of AMR - 2.0 into the four languages under consideration 17 and build a training set for each language by pairing the translated sentence with the original AMR graph .
We further fine- tune mBART f t mt , including also these new datasets among the training data .
This model , which we denote by mBART f t mt + AP , surpasses further mBART mt , clearly underlining the beneficial effect of this technique .
Finally , following Sheth et al. ( 2021 ) , we also report the results of SGL when evaluated on the machine - translated test set ; 18 similarly to their findings , we observe that , as the mismatch between the training and test set is reduced , our parser performs better in this setting than on the human-translated one .
UCCA Parsing
We report in Table 3 the performance of SGL on UCCA parsing .
We compare our approach with the original multi-task baseline ( Oepen et al. , 2019 ) and 3 transition - based parsers that participated ; in particular , we report the score of Che et al . ( 2019 ) , the system that ranked first in both all-framework and UCCA parsing .
First of all , we note the result of Cross st ; while its performance is far below the score Che et al . ( 2019 ) achieve , it still outperforms the original proposed baseline by more than 10 points .
Furthermore , to the best of our knowledge , apart from the recent works proposed in the latest shared task of Oepen et al . ( 2020 ) , this is the first reported result of translation - based approaches on UCCA parsing .
Once plugged into our multilingual framework , UCCA benefits from transfer learning to an even greater extent than AMR parsing , likely owing to the smaller amount of training data :
Cross mt and especially Cross f t mt significantly reduce the gap between SGL and Che et al . ( 2019 ) , with Cross f t mt outperforming the multi-task transition - based approach of Hershcovich and Arviv ( 2019 ) .
The usage of mBART pushes up the system 's performance further , with mBART st achieving 77.0 and mBART mt 79.9 ; differently from AMR , mBART f t mt suffers from overfitting and its performance is actually lower than that of mBART mt .
Even though these scores are lower than those of Che et al . ( 2019 ) , we argue that such results are still incredibly promising as they demonstrate the effectiveness of SGL in tackling cross -framework semantic parsing .
Indeed , these results show that multilingual translation - based approaches allow for a single model to jointly accommodate different formalisms , each potentially linearized according to a different linearization scheme .
Furthermore , we believe there is a significant margin for improvement on both the linearization used and the model ; for instance , we did not consider node ids such as < root_0 > as special tokens , but instead had the unigram tokenizer handle them as if they were normal words .
Finally , we wish to point out that direct comparability between our system and those reported is hindered by the fact that our training setting is significantly different from theirs ; in particular , we limit ourselves to two frameworks only and leverage resources ( the parallel corpora from MT ) whose usage was forbidden to the shared task participants .
19 Nevertheless , we believe that their results are needed here to better contextualize the performances SGL obtains .
6 Analysis : is MT the one helping ?
Although the performances of Cross mt are remarkable , mBART st achieves competitive results on cross-lingual parsing and fares even better on English parsing .
While mBART st admittedly features a massive amount of pre-training , this pre-training is over monolingual corpora and , as such , the model has never seen any parallel data .
We therefore wonder to what extent the parallel nature of the additional MT data we use is crucial for Cross mt .
To answer this question , we treat our MT corpora as monolingual data by sampling , for each instance , either the source or target side and converting the translation task into a denoising one : given an instance EN ?
IT , we sample either EN or IT with equal probability , denoting the result by Z , and convert the instance into g ( Z ) ?
Z , where g is a noising function that corrupts the input text .
We follow and choose a noising function that masks 35 % of the words by random sampling a span length from a Poisson distribution ( ? = 3.5 ) .
Applying this noisification scheme to the MT data , we train a model identical to Cross mt and denote it by Cross N mt .
As shown in Table 4 , in this data regime , the parallel nature is crucial both for English and , especially , for cross-lingual parsing .
While Cross N mt does yield a significant boost over Cross st , when 19 Allowed resources are specified at : http://svn.
nlpl.eu/mrp/2019/public/resources.txt compared instead to Cross mt , it is 4 points behind on UCCA parsing and only half way on AMR parsing .
The difference is even more marked in the cross-lingual setting , where Cross N mt simply does not work .
Conclusion
In this work , we presented SGL , a novel framing of semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation .
That is to say , given a sentence and the desired output formalism , a many - to - many neural model has to learn to translate from the input sentence to the corresponding linearized graph .
Within this framework , we show that we can address the paucity of annotated data that afflicts semantic parsing effectively by performing the learning procedure jointly on large parallel corpora coming from MT , and leveraging the power of pre-trained Transformer language models .
Using AMR and UCCA as our cases in point , we report competitive performances on their parsing , especially once pre-trained models enter the picture .
Furthermore , we find that the benefit MT data provide goes beyond merely improving Englishcentric parsing , yielding astonishing performances on cross-lingual AMR parsing as well , and allowing SGL to outperform all existing approaches by a large margin .
Most interestingly , differently from all previous literature , this result is attained without ever explicitly seeing at training time the translation paths the model is tested upon .
Once we use annotation projection and include these data as well , performances rise even further , attaining unprecedented results .
As future work , thanks to the nimbleness with which we can add new languages , we plan to assess the scalability of this framework as more formalisms are taken into account .
