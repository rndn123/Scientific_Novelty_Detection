title
The University of Edinburgh 's Bengali-Hindi Submissions to the WMT21 News Translation Task
abstract
We describe the University of Edinburgh 's Bengali ?
Hindi constrained systems submitted to the WMT21 News Translation task .
We submitted ensembles of Transformer models built with large-scale back -translation and finetuned on subsets of training data retrieved based on similarity to the target domain .
For both translation directions , our submissions are among the best-performing constrained systems according to human evaluation .
Introduction
We present the University of Edinburgh 's participation in the WMT21 news translation shared task on the Bengali? Hindi ( Bn? Hi ) and Hindi?Bengali ( Hi? Bn ) language pairs .
We followed the constrained condition , i.e. only using the data provided by the organizers .
The training data for these language pairs consisted of noisy crawled data , and was mostly out - of- domain with respect to the validation and test domain .
Therefore , most of our efforts concentrated on fine-tuning models to adapt to the target domain .
We also explore multiple back -translation methods , and ensembles of models trained and fine-tuned with different methods .
Building our systems consisted of the following steps , each of which is described in more detail in the remaining sections of this paper : ?
Cleaning the noisy parallel data ( Section 3 ) .
?
Training ensembles of Transformer models on the cleaned provided data for back - translation ; and using the back - translated data along with the clean parallel data to train new models ( Section 4 ) .
?
Fine-tuning the models on subsets of training data retrieved that are similar to the target domain , based on different similarity measures ( Section 5 ) .
?
Ensembling various models and decoding with optimal parameters ( Section 6 ) .
We also report some methods that we tried to use but did not work in Section 8 .
Model Configuration
Our models follow the Transformer - Big architecture ( Vaswani et al. , 2017 ) : 6 layers of encoders and decoders , 16 heads , an embedding size of 1024 , a unit size of 4096 , etc .
We found that smaller Transformer architectures performed worse .
All models are trained with the same vocabulary of 32 k SentencePiece subwords ( Kudo and Richardson , 2018 ) to allow ensembling .
We use a shared vocabulary between source and target , as well as tied embeddings ( Press and Wolf , 2017 ) .
We tried other vocabulary sizes too : 5 k , 10k , and 20k , though all of them had similar performance .
We also included several special tokens in the vocabulary , of which we finally used only one for tagged back - translation ( Caswell et al. , 2019 ) .
We train models with 32GB dynamic batch size and an optimizer delay ( Bogoychev et al. , 2018 ) of 3 with the Adam optimizer ( Kingma and Ba , 2015 ) under a learning rate of 0.0003 , until we see no improvement within 10 consecutive validation steps .
All models were trained with the Marian NMT toolkit ( Junczys - Dowmunt et al. , 2018 ) 1 3 Datasets and Cleaning
Corpora
All our models are trained in the constrained scenario - even more specifically , we only use data provided for the news translation task for these specific language pairs .
This consists of 3.3 M parallel sentences from the CCAligned corpus ( El - Kishky et al. , 2020 ) , along with monolingual data in both languages .
The details of the corpora used along with their sizes are shown in
Cleaning Since the CCAligned corpus is built from web crawls and is known to be very noisy ( Caswell et al. , 2021 ) , we focused on cleaning the parallel data before training translation models .
Our main approaches are rule-based and heuristic cleaning methods , along with language identification and language model filters .
Our final systems used the following cleaning methods for the parallel corpus : De-duplication Duplicate sentence pairsaround 17.3 % of the corpus - were removed .
Splitting multi-language sentences
We observed large chunks of the corpus where the sentences on the Bengali side also had their English translations attached in the same line .
Some rough punctuation and script- based heuristics were used to remove the English segments from these lines .
The roughness of these heuristics also affected a large number of other lines , mostly noisy ones containing non-lexical information , but we observed no degradation of quality due to this inaccuracy .
We also found some such sentences on the Hindi side , but they were less frequent and removal showed no improvement in quality , so we did not split Hindi sentences in this way for our final models .
Language ID filtering
We used publicly available FastText language identification models ( Joulin et al. , 2016 ( Joulin et al. , , 2017 2 to filter out lines in wrong languages .
We get the top 3 predictions for each line , throw out lines where the right language does not appear in the top 3 for one or both sides , sort by the language prediction probabilities , and based on manual inspection , arrive at minimum threshold probabilities of 0.6 for Bengali lines and 0.4 for Hindi lines , above which lines are retained .
Language model filtering We used KenLM ( Heafield , 2011 ) to train separate trigram language models for Bengali and Hindi , on all provided Extended CommonCrawl monolingual data , and used these to score the parallel data .
We retain sentences with log 10 probabilities greater than - 4 .
Training with Synthetic Data
In each language direction , we trained 4 models with different seeds .
We then ensembled these 4 models to back - translate ( Sennrich et al. , 2016 ) all the provided monolingual data .
We used this translated data in many different ways as described in the remainder of this section .
Tagged back - translation Following Caswell et al. ( 2019 ) , we prefixed a special < __BT_ _ > token to all back - translated news monolingual data , combined the data with the clean parallel data , and trained new models .
Two -step training
We first trained models on all the back - translated data only , then once that converged , continued training on the clean parallel data .
Since the amount of monolingual data far exceeds the amount of parallel data , this training regime gave us better results than mixing parallel and back - translated data at the same time .
The latter method would also involve finding the right amount of back - translated data to sample / select , since using it all would overwhelm the parallel training data .
Forward translation
We also trained models on parallel data along with all the back - translations and all forward translations , i.e. instead of strictly keeping target monolingual data on the target side and synthetic back - translated data on the source side , we used both directions of translated data .
To adapt our models to the target domain , we retrieved sentences from the training corpora which are similar to the source side of validation and test sets based on different similarity measures , and then fine - tuned the models on these subsets of data .
The remainder of this section describes the different methods to retrieve the relevant subsets of data .
The number of sentence pairs retrieved by each of these methods which are then used for fine- tuning is shown in Based on vocabulary overlap
The simplest method is to retrieve any sentence pairs whose source texts have 1 , 2 , or 3 non-punctuation bigrams which occur on the source side of the validation and test sets .
Due to the large mismatch between training corpus and target domain , this method retrieves a surprisingly small proportion of the training corpus , as shown in Table 2 . Based on language model scoring
We trained ngram language models on the validation and test set or validation set data only , scored the parallel data with these language models , then kept sentences scoring above a certain threshold .
Even though the small size of the validation data means that the language model is probably not very good , we still see some improvements by fine-tuning on data retrieved this way .
Based on TF -IDF similarity
We first adapted the document aligner 4 from ParaCrawl ( Ba?n et al. , 2020 ) to work at sentence level .
This tool uses the translation of a source text ( Uszkoreit et al. , 2010 ) to match potential target text using cosine similarity of TF - IDF - weighted word frequency vectors .
In this case , we match the source side of our validation and test sets with the parallel text to find potential " matches " .
This method retrieves too few matches with only the validation set , but we got a few thousand sentence pairs ( Table 2 ) from a combination of validation and test sets .
Following Chen et al. ( 2020 b ) , we also developed a variant where we first cluster each source sentence with another X sentences in the validation and test sets based on n-gram TF - IDF vector cosine similarity , then treat the cluster as a single query and compare it against each source sentence in the parallel training data .
We always picked the top 20 K resulting pairs .
Through manual inspection , we found that the resulting corpus is very reasonable when we cluster the whole validation and test sets as one query , making the fine-tuning essentially a test domain adaptation process .
Fine-tuning on the validation set Since the validation data is the only domainspecific data we had , similar to Chen et al . ( 2020a ) , we fine- tuned all our final models on a portion of the validation set ( we used 95 % of the data instead of 75 % ) until it stopped improving on the rest of the validation set .
This was done as a final additional step after the other kinds of fine-tuning described previously .
6 Ensembles and Decoding Parameters
Ensembles
As shown in Table 3 , our primary submissions consist of ensembles of multiple models trained and fine-tuned in different ways .
Due to the component models not being very high-quality , we observed that this type of ensemble produces more robust translations than simple ensembles of models trained identically with different seeds .
Optimal decoding hyperparameters
Using an initial ensemble of 4 models , we swept a wide range of values of beam size and length normalization hyperparameters to decode the validation set .
We find that optimizing these can result in an improvement of up to 0.5 BLEU on the validation set .
We obtained the best scores with a beam size of 16 , and a length normalization parameter of 1.3 for Bn? Hi and 0.7 for Hi?
Bn , and used these values to decode the test set .
3 , but note that all models were fine-tuned on the validation set before ensembling .
Model
Sentence splitting
In the source texts of the test set , we observed many instances of more than one sentence in one line .
Since our models are trained on single sentences , we chose to run a sentence splitter on the test source , translate , and rejoin the translated sentences .
For this purpose , we used the Moses sentence splitter ( Koehn et al. , 2007 ) 5 for Bengali text , and the IndicNLP sentence splitter ( Kunchukuttan , 2020 ) for Hindi .
Numeral transliteration
Due to the fact that numerals in the Latin script are often used in Bengali and Hindi text , which is reflected by the web crawled training data , our models tend to generate a mix of Latin and Bengali / Hindi numerals , sometimes even in the same sentence .
To ensure consistency , we transliterated 5 https://github.com/moses-smt/ mosesdecoder/blob/master/scripts/ems / support/split-sentences.perl all Bengali or Hindi numerals in our test outputs to their Latin script counterparts ( it is equally feasible to convert Latin numerals to the target language ) .
While this may not help in terms of automatic metrics ( we lose 0.3- 0.5 BLEU after this step ) , we believe human evaluators would prefer consistency in this regard .
Results
Table 3 shows BLEU 6 and ChrF 7 scored using sacreBLEU ( Post , 2018 ) on the validation sets .
We see that fine-tuning on the retrieved subsets of data consistently results in quality gains .
We tried many different ensembles and , upon visual inspection , found that models fine-tuned on data retrieved on the basis of similarity to validation and test sets were not necessarily better than those from validation sets only .
Unsuccessful Attempts
In this section , we document some methods that we tried to use , but which did not work at all or did not result in better systems .
Dual conditional cross-entropy filtering Our initial cleaning effort was to use dual conditional cross-entropy ( Junczys - Dowmunt , 2018 ) to selffilter the parallel data , which yielded no useful results .
We also randomly split the data into two halves , trained translation models on each half , to score and filter the other half of the data - this method did not work either .
We conclude that these methods are not suitable in this scenario where we do not have any clean data , however small , to train the initial cleaning model .
Copied monolingual data
We attempted to synthesize training data by copying ( Currey et al. , 2017 ) and transliterating 8 monolingual data in the target language to source .
In this way , we obtained pseudo parallel data that could potentially improve the decoder side of a translation model without harming the encoder much .
Transfer learning
We also explored utilizing dataset from another language in the form of model pre-training .
Following Aji et al. ( 2020 ) , we initialize our Bengali ?
Hindi model weights , excluding the embeddings , from our English ?
German submission to WMT21 ( Chen et al. , 2021 ) .
These methods above did not increase BLEU , except that transliterated monolingual data brought a tiny improvement .
Model pre-training achieved the convergence faster , but did not achieve better final BLEU .
Consequently , we did not carry out any further experiments with these methods .
ernment is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .
5 Fine-tuning to the Target Domain 5.1 Fine-tuning on retrieved sentences
Unlike many of the other language pairs in the news translation task , the Bengali-Hindi pair does not include any known in - domain training corpora .
The training data is aligned from documents obtained through untargeted web crawling ( El - Kishky et al. , 2020 ) , and thus contains out -of- domain and noisy text .
On the other hand , the target domain , reflected in the validation and test sets , consists of Wikipedia content 3 .
Table 1 . 1 Corpus Lines ( M ) Parallel 3.36 + deduplication and filtering 2.03 Monolingual Bn NewsCrawl 10.1 Bn CommonCrawl 49.6 Hi NewsCrawl 46.1 Hi CommonCrawl 202
Table 1 : 1 Bn and Hi corpora used in our submissions .
Table 2 . 2 Retrieval Source Lines ( K ) Bn Hi 1 bigram overlap dev 448 891 2 bigram overlap dev 243 597 3 bigram overlap dev 158 445 1 bigram overlap dev , test 487 932 2 bigram overlap dev , test 273 639 3 bigram overlap dev , test 183 479 LM threshold - 2.5 dev 50 175 LM threshold - 2.0 dev , test 12 13 TF -IDF dev , test 5.6 27.9 TF - IDF cluster dev , test 20 20
Table 2 : 2 Number of training sentence pairs retrieved for fine-tuning by different methods .
Table 3 : 3 Validation set BLEU and ChrF scores for our models .
Bn? Hi Hi?
Bn BLEU ChrF BLEU ChrF
Table 4 : 4 Test set BLEU and ChrF scores for our primary submissions .
Model numbers refer to models from Table
Table 5 : 5 Human evaluation results .
Our submissions are in bold .
Systems within a cluster are considered tied .
Table 4 4 reports the automatic scores of our final submitted systems on the test sets .
As shown in Table5 , according to human evaluation conducted by the task organizers , our systems rank at the top ( tied ) among all the constrained submissions for both translation directions .
https://github.com/marian-nmt/marian
https://fasttext.cc/docs/en/ language-identification.html
Despite it being part of the ' news translation ' task
https://github.com/bitextor/bitextor/ tree/master/ document-aligner
signature : BLEU+case.mixed+numrefs.1+smooth.exp+ tok.13a+version.1.5.1 7 signature : chrF2+numchars.6+space.false+version.1.5.1
https://github.com/ indic-transliteration / indic_ transliteration_py
