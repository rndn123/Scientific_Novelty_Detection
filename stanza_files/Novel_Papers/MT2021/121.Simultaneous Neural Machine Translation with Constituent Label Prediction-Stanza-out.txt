title
Simultaneous Neural Machine Translation with Constituent Label Prediction
abstract
Simultaneous translation is a task in which translation begins before the speaker has finished speaking , so it is important to decide when to start the translation process .
However , deciding whether to read more input words or start to translate is difficult for language pairs with different word orders such as English and Japanese .
Motivated by the concept of prereordering , we propose a couple of simple decision rules using the label of the next constituent predicted by incremental constituent label prediction .
In experiments on Englishto- Japanese simultaneous translation , the proposed method outperformed baselines in the quality - latency trade- off .
Introduction Simultaneous machine translation is a task in which the machine starts outputting a translation before reading the entire input sentence .
This task is more difficult than full-sentence translation because it translates the initial part of a sentence without the context of the latter part .
This involves a trade- off between delay and quality of the translation ; using a longer context should improve translation quality at the cost of a longer delay , and vice versa .
In practice , we should control the latency so that it 's not too large , but we may also need to allow a long latency depending on the situation .
Most of the recent simultaneous translation models Arivazhagan et al. , 2019 ; Raffel et al. , 2017 ; Arivazhagan et al. , 2019 ; Ma et al. , 2020 b ; Dalvi et al. , 2018 ; Gu et al. , 2017 ; Alinejad et al. , 2018 ; Cho and Esipova , 2016 ; Zheng et al. , 2020
Zhang et al. , 2020 ) are based on neural machine translation ( NMT ) , although earlier studies were based on statistical machine translation ( Rangarajan Sridhar et al. , 2013 ; Grissom II et al. , 2014 ; Oda et al. , , 2015 .
In simultaneous NMT , there are two major approaches : those in which a latency hyperparameter is given before the training and those in which it is given at the time of inference .
The former approach requires training a model individually for each pre-defined latency setting , while the latter approach uses a single model for different latency conditions .
Most human simultaneous interpreters would not need such long training to slightly adjust latency , while it takes much more time to learn other languages to develop their translation skill .
Therefore , the latter approach is closer to the learning process of human simultaneous interpreters .
wait -k ) is a simple simultaneous NMT method of the former approach that waits k tokens before starting to translate .
It also has variants within the latter approach called test- time wait -k , in which k is determined at the inference time .
wait -k had better performance than test-time wait -k in that study 's experiments .
There is another method in the latter approach that uses Meaningful Unit ( Zhang et al. , 2020 ) .
In this model , chunk - based incremental decoding is done at inference time by segmentation with a boundary predictor .
This model outperformed baselines of the former approach .
They refined their basic boundary predictor to deal with sentence pairs in which full-sentence translation needs longdistance reordering .
However , its training process is very complicated :
It first generates monotonic translations , fine - tunes the NMT model with them , then generates an oracle boundary with the model , and finally fine- tunes a boundary - prediction model based on BERT ( Devlin et al. , 2019 ) .
Simultaneous translation is still difficult for language pairs such as English - Japanese , which often require long-distance reordering .
To tackle the reordering problem , we propose an inputsegmentation method for simultaneous translation , using a couple of simple rules and incremental prediction of the label of a syntactic constituent coming immediately after the input existing so far .
This Source sentence I bought a pen .
Monotonic translation watashi wa katta pen wo .
Full-sentence translation watashi wa pen wo katta .
Boundary prediction I / bought a pen .
Simultaneous translation watashi wa / pen wo katta .
Table 2 : Example of English- to - Japanese translation using proposed method with segment - boundary prediction is not dependent on the trained NMT .
Therefore , once we create it , it is reusable for other models .
Our proposed method is inspired by Head Finalization ( Isozaki et al. , 2010 ) .
Head Finalization reorders words of the source sentence before translating from an SVO ( Subject - Verb- Object ) language to an SOV language in full-sentence statistical machine translation .
This method moves a syntactic head into a later position so that the word order of the source language ( e.g. , English ) becomes similar to that of the target language ( e.g. , Japanese ) .
This enables us to monotonically translate from English , which is a typical SVO language , to Japanese , a typical SOV language .
Recent NMT models like Transformer ( Vaswani et al. , 2017 ) works well on reordering in general , so this kind of pre-reordering is not usually used .
However , simultaneous translation monotonically reads input words one by one , and therefore the difference in word order remains a problem .
As shown in Table 1 , monotonic translation often becomes unnatural compared to full-sentence translation .
The part " bought a pen " should be translated to pen wo katta by reversing the word order .
Therefore , after reading the word " bought , " it is important to wait for future words without starting to translate it .
In this case , " I " is the last word that does not require reordering .
This word is regarded as a segment boundary to start a partial translation .
Table 2 shows an example of our proposed segmentation .
Suppose we predict the next constituent label as a verb phrase ( VP ) after reading an input word " I . "
This shows the possibility that the next words should be reordered , so the " I " becomes the boundary .
Once detecting the boundary , NMT model starts to translate " I " into " watashi wa . "
After that , the model restarts to read the remaining input words , then translates " bought a pen " into " pen wo katta . "
The total output of simultaneous translation based on the proposed segmentation is the same as that of full-translation in this simple example .
By ensuring that the Verb and its Object of the source sentence are included in a single segment , it is possible to output translation while maintaining the SOV - like structure of the target language .
In experiments on English-to - Japanese simultaneous translation , the proposed method outperformed baselines in the quality - latency trade- off .
Related work
In statistical machine translation , there are several approaches to finding boundaries of segments for simultaneous translation .
proposed a method to choose segment boundaries that maximize the BLEU score .
Rangarajan Sridhar et al. ( 2013 ) proposed segmentation strategies based on lexical cues .
In NMT , there have been many studies on simultaneous translation .
The amount of latency is decided either before training or at inference time .
wait -k is the simplest variant using fixed latency :
It simply waits for k tokens before starting translation .
The latency policy can be learned from a parallel corpus together with an NMT model .
MILk ( Arivazhagan et al. , 2019 ) and other approaches ( Raffel et al. , 2017 ; Ma et al. , 2020 b ) used a latency - augmented loss function in training to balance latency and accuracy .
In contrast , the latency policy can be learned with a pre-trained NMT model , such as test-time wait -k and STATIC - RW ( Dalvi et al. , 2018 ) .
These have fixed policies that wait for the fixed number of tokens before translation , but there are other models that learn a more flexible policy for a given pre-trained NMT model .
Some studies use reinforcement learning to learn an adaptive READ / WRITE policy ( Grissom II et al. , 2014 ; Satija and Pineau , 2016 ; Gu et al. , 2017 ; Alinejad et al. , 2018 ) .
Training by reinforcement learning can be unstable depending on the condition .
One method that does not use reinforcement learning is wait - if -* ( Cho and Esipova , 2016 ) , which translates and segments jointly to maximize the translation quality .
Zheng et al . ( 2020 ) extended wait -k to an adaptive policy by adaptively choosing the strategy at inference .
There is another method that generates oracle READ / WRITE actions by a pre-trained NMT model and predicts actions using a neural network model .
Meaningful Unit ( Zhang et al. , 2020 ) works along the same lines and has outperformed baselines such as MILk and wait -k .
With respect to the use of syntactic clues for simultaneous translation , Oda et al . ( 2015 ) proposed a method to incrementally parse an incomplete sentence by predicting unseen syntactic constituents on the right and left side of each segment .
They concatenated the predicted constituents and the words in a segment and then input the result into tree2string translation .
They decided to wait for more tokens or output the translation depending on where the constituents appear in the translation result .
Our proposed method is based on chunk - based simultaneous translation using chunk boundary detection with simple rules on next-constituent labels .
It basically segments an input before a verb phrase .
This is much simpler and easier to implement than the work by Zhang et al . ( 2020 ) and Oda et al . ( 2015 ) .
Proposed Method Figure 1 shows a step-by-step example of our proposed method described in this section .
Standard Simultaneous Translation
A standard NMT for full sentences is represented by the following equation : p f ull ( Y | X ) = | Y | t=1 P ( y t | X , y <t ) , ( 1 ) where X = x 1 , x 2 , ... , x n is an input sentence consisting of n tokens and Y = y 1 , y 2 , ... , y m is a predicted target language sentence consisting of m tokens .
A simultaneous NMT uses only a prefix of the input to predict a target language token : p simul ( Y | X ) = | Y | t=1 P ( y t | x g ( t ) , y <t ) , ( 2 ) where g ( t ) is a monotonic non-decreasing function representing the number of read source tokens to output the tth target token .
Chunk - based Simultaneous Translation
We use chunk - based incremental decoding for our simultaneous translation model and a full-sentence NMT model trained in a standard manner .
However , at the time of inference , we translate the current prefix upon chunk segmentation while keeping the previously translated output unchanged .
Suppose we have already translated input chunks X i?1 = X 1 , X 2 , ... , X i?1 into an output prefix also represented by chunks : Y i?1 = Y 1 , Y 2 , ... , Y i?1 , while translating the next input chunk X i into Y i .
We restart the translation from the beginning using all of the available input chunks X i 1 .
This is similar to an approach called retranslation that generates translations from scratch for every new input word ( Niehues et al. , 2016 ; Arivazhagan et al. , 2020 ) , but we apply forced decoding to Y i?1 in the output prefix .
The probability of the prefix Y i can be denoted as follows : p pref ix ( Y i | X i ) = p f ull ( Y i?1 | X i ) ? p chunk ( Y i | X i , Y i?1 ) . ( 3 )
The first term is calculated in the same way as the standard full-sentence NMT in Eq. ( 1 ) through forced decoding , and the second term is decomposed as follows , letting Y i = y i 1 , y i 2 , ... , y i | Y i | : p chunk ( Y i | X i , Y i?1 ) = | Y i | t=1 P ( y i t | X i , Y i?1 , y i < t ) .
( 4 )
This can be more efficient than an incremental Transformer that refreshes the encoder for every input word , since our chunkbased translation refreshes the encoder for every input chunk , which usually consists of multiple words .
Chunk Segmentation
We use constituent labels for our rule- based chunk segmentation as follows .
Incremental Constituent Label Prediction
We predict the label of a syntactic constituent coming after a sentence prefix at the current time-step .
We call this process Incremental Constituent Label Prediction ( ICLP ) .
Here , we define this next constituent as the one coming next to the sentence prefix in pre-order tree traversal .
However , this label prediction is not easy without observations on the next constituent .
In this work , we allow one look - ahead , where we read one more word and predict the label of the constituent starting from that word .
This causes an additional delay by one word but improves the prediction accuracy .
Suppose we have an input sequence W = [ w 1 , w 2 , ... , w | W | ] .
The one look - ahead ICLP predicts the constituent label c i upon the observation of w i , as follows : c i = argmax c ? C p( c |w ?i ) , ( 5 ) where C is a set of constituent labels .
Only a prefix word subsequence is fed into the ICLP , so previous label predictions do not affect later ones .
We can train the ICLP model as a multiclass classifier using a set of training instances in the form of prefix-label pairs .
One sentence generates several instances for training data : ( w 1 , c 1 ) , ( w 1 , w 2 , c 2 ) , ( w 1 , w 2 , w 3 , c 3 ) , ( w 1 , w 2 , w 3 , w 4 , c 4 ) , and so on .
We implemented the ICLP model in two different ways using LSTM ( Hochreiter and Schmidhuber , 1997 ) and BERT ( Devlin et al. , 2019 ) .
Segmentation Rules
Table 3 shows an example of a result by the one look - ahead ICLP .
We use one basic and two supplemental rules for chunk segmentation as follows .
?
Segment the input coming just before constituents labeled S and VP . ?
If the previous label is S or VP , do not segment the input . ?
If the chunk is shorter than the minimum length , do not segment the input .
In incremental translation from Subject - Verb-Object to Subject - Object - Verb , the subject can be translated before observing the verb coming next , but the verb should be translated after observing the object .
Therefore , the chunk boundary should be between the subject and verb , not between verb and object .
To achieve this , we employ a simple rule to segment a chunk just before VP .
We also include S in the rule just as with VP because S ( simple declarative clause ) often appears in the form of a unary branch " ( S ( VP ... ) ) " as shown in Table 3 .
However , in cases such as " can save " in the example , VP occurs again immediately after the segmentation before " can . "
The basic rule suggests segmentation before " save , " but it does not seem appropriate .
Therefore , we introduce the minimum segment size to avoid such over-segmentation as a hyperparameter to control the accuracy - latency trade-off .
If the hyperparameter is larger than one , the chunk segmentation after " You " in the example does not occur .
Experimental setup 4.1 Dataset and preprocessing We conducted experiments on English - Japanese ( En - Ja ) translation .
We also tried English - German ( En - De ) translation to investigate the difference in language pairs .
For En-Ja , the model was trained on 17.9 M sentence pairs from WMT2020 and fine-tuned on 223 K sentence pairs from IWSLT2017 .
We used 5312 sentence pairs for the development set from dev2010 , tst2011 , tst2012 , and tst2013 of IWSLT .
We evaluated the model on 1442 sentence pairs from dev2021 of IWSLT .
For En- De , the model was trained on 4.5 M sentence pairs from WMT2014 and fine-tuned on 206 K sentence pairs from IWSLT2017 .
We used 5589 sentence pairs for the development set from dev2010 , tst2011 , tst2012 , and tst2013 of IWSLT .
We evaluated the model on 1,080 sentence pairs from tst2015 of IWSLT .
We tokenized English and German sentences with tokenizer .
perl in Moses ( Koehn et al. , 2007 ) and Japanese sentences with MeCab ( Kudo , 2005 ) .
For each language pair , we used subwords based on Byte Pair Encoding ( BPE ) ( Sennrich et al. , 2016 ) with a shared vocabulary of 16 K entries .
To develop the subword vocabulary , we used all of the in-domain training sentences ( IWSLT ) and one million out - of- domain sentences ( WMT ) .
We trained the ICLP models using Penn Treebank 3 ( Marcus et al. , 1993 ) for training , excluding a randomly selected one percent of sentences reserved for the development set .
We used NAIST - NTT TED Talk Treebank for the evaluation set .
The number of training , development , and test instances ( e.g. , the number of labels to be predicted ) were 2.8 M , 27.9 K , and 21.9 K , respectively .
Note that multiple ICLP instances are induced from what a single parse tree generates .
Model settings
We compared the following four models .
All of them were based on the Transformer - base ( Vaswani et al. , 2017 ) .
wait -k
The range of k is [ 2 , 4 , 6 , ... , 30 ] .
Meaningful Unit
The hyperparameter is p , which is the threshold of the probability of a boundary .
The ranges of p are [ 0.5 , 0.1 , 0.15 , ... , 0.95 ] , [ 0.99 , 0.991 , 0.992 , ... , 0.999 ] , and [ 0.9991 , 0.9992 , ... , 0.9999 ] . Monotonic translation of Meaningful Unit was generated from the fine-tuning dataset by the fine- tuned NMT model .
We used their refined Meaningful Unit method , which improved the translation quality at low latency ( Zhang et al. , 2020 ) 1 . They used a two look - ahead boundary predictor in their experiments .
We additionally tried a one look - ahead predictor because it is not certain how many future words should be used for the predictor .
Fixed -size segmentation
This simply segments an input with a fixed length specified by a hyperparameter f , which means the boundary comes every f subwords or words .
The range of f is [ 2 , 4 , 6 , ... , 30 ] for words and [ 4 , 8 , 12 , ... , 60 ] for subwords .
ICLP
The hyperparameter is m , which means the minimum number of words in one segment .
The range of m is [ 1 , 2 , 3 , . . . , 29 ] .
We controlled hyperparameters to adapt to a wide range of latency .
The hyperparameter is given both in the training and at the inference time for wait -k , but it is given only at the inference time for other models .
Therefore , we trained the wait -k model for each k while in other approaches a single NMT model is commonly used .
We used fairseq ( Ott et al. , 2019 ) to implement these models and basically followed the official baseline for IWSLT 2021 2,3 to set the hyperparameters .
We saved checkpoints every 5000 updates for pre-training and every 200 updates for finetuning .
Other hyperparameters were the same for pre-training and fine-tuning .
We stopped training early with patience 4 .
The max-tokens for the mini batch size was 4096 , and weights were updated every 4 mini batches .
We set the learning rate to 0.0007 and trained the model on a single GPU .
The last three models used the same NMT model .
We used beam search within chunks in a standard way and chose 1 - best hypotheses at the end of chunk translation .
The beam size was four for the chunkbased and full-sentence models .
We used greedy decoding for wait -k .
We implemented two types of ICLP models as mentioned earlier .
For the LSTM - based ICLP , we used two -layered unidirectional LSTMs to encode an input sentence with a fully connected layer for the constituent label prediction .
The numbers of dimensions for embedding and hidden states are 512 .
We tokenized English sentences using tokenizer .
perl in Moses and Byte Pair Encoding ( Sennrich et al. , 2016 ) with a vocabulary of 16 K entries .
For the BERT - based ICLP , we used a BERT - based classifier with an additional fully connected layer over the [ CLS ] token , implemented using Huggingface transformers ( Wolf et al. , 2020 ) with a pre-trained model bert- base -uncased and the corresponding subword tokenizer .
For both models , the input was a subword sequence , so the constituent label prediction was made upon the observation of an end-of-word subword .
The following training conditions were commonly applied to both models : learning rate of 5e - 5 , training batch size of 512 instances , checkpoints saved at the end of every epoch , and early stopping with the patience of three epochs .
Evaluation
We used SimulEval ( Ma et al. , 2020a ) to evaluate the quality and latency of simultaneous translation .
BLEU ( Papineni et al. , 2002 ) was used to evaluate quality .
We used Average Lagging ( AL ) to evaluate the latency .
AL is widely used and defined by the following equation : AL g ( X , Y ) = 1 ? g ( | X | ) ?g( | X | ) t=1 g( t ) ? t ? 1 ? . ( 6
Results
We illustrate the results of English - Japanese translation in Figure 2 .
Our proposed method outperformed baselines in a wide range of AL .
Most of the points of the proposed method appear to the upper-left of the other methods , thus showing the best performance .
We compared the use of segmentation rules based on VP and VP +S .
The points shifted to the left by adding S as boundary because it increased the number of boundaries and decreased latency .
Although we tried the different look - ahead lengths of one and two for the boundary predictor of Meaningful Unit , our proposed model outperformed both of these models in a wide range of latency .
The difference between wait -k and the models using the full-sentence translation model was large in the quality - latency trade- off .
Surprisingly , the fixed - size segmentation was also effective .
When the segment size was fixed , it did not make a large difference in the result , regardless of whether the unit was a subword or a word .
Analysis
Length ratio Figure 3 shows the length ratios of translation hypotheses and references with different latency parameters .
Too large a ratio decreases the BLEU score and makes the content delivery difficult both in text ( subtitles ) and speech ( text-to-speech ) .
The length ratio of wait -k was unstable compared to other models because it was trained individually for each k .
Except for wait -k , the length ratios were large in the range of small latency , probably due to the condition mismatch between training and inference .
These NMT models were trained on full sentences , but they were used to translate short segments in the inference .
Therefore , they tend to output longer segment translations than expected .
Their ratios gradually decrease as AL increases and the length of segments becomes closer to the length of full sentences .
calculated as the number of subwords in a segment , and the previous segment was concatenated to the next segment when the previous segment has no translation output .
Segment length distribution Segmentation with fixed size 16 has some segments shorter than size 16 because the sentence length is not always a multiple of 16 .
Compared with ICLP model , Meaningful Unit has wider distribution , and the most segments consist of two subwords .
These short segments have less context information and can output longer segment translation than expected .
This would be one of the reason why our proposed method outperformed Meaningful Unit .
Controlling latency In Figures 7 and 8 rameters from 0.9996 to 0.9999 were also the same as that of a full-sentence translation model .
In contrast , our proposed method can easily control latency because it uses the minimum chunk length as an intuitive hyperparameter to adjust it .
How many words to wait Compared with the fixed - size segmentation model , our proposed model and Meaningful Unit have a disadvantage in AL , which is caused by the lookahead approach .
Despite this disadvantage , our proposed approach outperformed the fixed - size segmentation in a wide range of AL .
This means the benefit of looking at the future words and finding a better boundary outweighed the above disadvantage .
Performance of ICLP
Tables 5 and 6 show the results in precision and recall of the one look - ahead ICLP models .
The LSTM - based ICLP was better in precision , but the 9 compares them in the downstream simultaneous translation .
The lines connected by dots nearly overlapped , so there was no large difference in BLEU score .
LSTM is more efficient than BERT in incremental processes , so it is suitable for practical usage .
Table 7 shows the results by the ICLP model without one look - ahead approach .
Compared with Table 5 , the scores are much lower .
One lookahead approach was important to improve its performance .
En - De translation
We conducted additional experiments in En- De translation to investigate the performance in a different language .
German is another language with different word order from English especially in verbs and also suffers from the reordering problem .
Figure 10 shows the results .
This is almost the opposite of the results of the En- Ja translation .
The proposed boundary decision rules used for En - Ja translation were not so effective for En- De translation , so we need to find other rules to detect boundaries in En- De translation .
Conclusion
We proposed a novel segmentation method for simultaneous translation that uses simple rules and ICLP .
Our proposed method is simple , and it outperformed the baselines in the quality - latency trade- off in En-Ja translation .
On the other hand , the proposed method did not work effectively in En- De translation due to the smaller word order differences than those in En- Ja translation .
In future work , we expect to extract segmentation rules automatically and apply these rules to other language pairs as well .
of full-sentence parser ( S ( NP ( PRP I ) ) ( VP ( VBD bought ) ( NP ( DT a ) ( NN pen ) ) ) )
Output watashi wa pen wo katta .
