title
Zero - Shot Cross-Lingual Transfer of Neural Machine Translation with Multilingual Pretrained Encoders
abstract
Previous work mainly focuses on improving cross-lingual transfer for NLU tasks with a multilingual pretrained encoder ( MPE ) , or improving the performance on supervised machine translation with BERT .
However , it is under-explored that whether the MPE can help to facilitate the cross-lingual transferability of NMT model .
In this paper , we focus on a zero-shot cross-lingual transfer task in NMT .
In this task , the NMT model is trained with parallel dataset of only one language pair and an off-the-shelf MPE , then it is directly tested on zero-shot language pairs .
We propose SixT , a simple yet effective model for this task .
SixT leverages the MPE with a two -stage training schedule and gets further improvement with a position disentangled encoder and a capacity - enhanced decoder .
Using this method , SixT significantly outperforms mBART , a pretrained multilingual encoderdecoder model explicitly designed for NMT , with an average improvement of 7.1 BLEU on zero-shot any - to - English test sets across 14 source languages .
Furthermore , with much less training computation cost and training data , our model achieves better performance on 15 any - to - English test sets than CRISS and m2m - 100 , two strong multilingual NMT baselines .
Introduction Multilingual pretrained encoders ( MPE ) such as mBERT ( Wu and Dredze , 2019 ) , XLM ( Conneau and Lample , 2019 ) , and XLM -R ( Conneau et al. , 2020 ) have shown remarkably strong results on zero-shot cross-lingual transfer mainly for natural language understanding ( NLU ) tasks , including named entity recognition ( NER ) , question answering ( QA ) and natural language inference ( NLI ) .
These methods jointly train a Transformer ( Vaswani et al. , 2017 )
In the zero-shot cross-lingual NMT transfer task , the model is trained with parallel dataset of only one language pair ( such as De-En ) and a multilingual pretrained encoder .
The trained model is tested on many - to- one language pairs ( like Fi/ Hi / Zh-En ) in a zero-shot manner .
Monolingual text of the to -be-tested source languages is not available in this task .
masked language modeling task in multiple languages .
The pretrained model is then fine-tuned on a downstream NLU task using labeled data in a single language and evaluated on the same task in other languages .
With this pretraining and finetuning approach , the MPE is able to generalize to other languages that even do not have labeled data .
Given that MPE has achieved great success in crosslingual NLU tasks , a question worthy of research is how to perform zero-shot cross-lingual transfer in the NMT task by leveraging the MPE .
Some work ( Zhu et al. , 2020 ; Yang et al. , 2020 ; Weng et al. , 2020 ; Imamura and Sumita , 2019 ) explores approaches to improve NMT performance by incorporating monolingual pretrained Transformer encoder such as BERT ( Devlin et al. , 2019 ) .
However , simply replacing the monolingual pretrained encoder in previous studies with MPE does not work well for cross-lingual transfer of NMT ( see baselines in Table 2 ) .
Others propose to fine- tune the encoder-decoder - based multilingual pretrained model for cross-lingual transfer of NMT Lin et al. , 2020 ) .
It is still unclear how to conduct cross-lingual transfer for NMT model with existing multilingual pretrained encoders such as XLM -R .
In this paper , we focus on a Zero-shot crosslingual ( X ) NMT
Transfer task ( ZeXT , see Figure 1 ) , which aims at translating multiple unseen languages by leveraging an MPE .
Different from unsupervised or multilingual NMT , only an MPE and parallel dataset of one language pair such as German - English are available in this task .
The trained model is directly tested on many - to - one test sets in a zero-shot manner .
We propose a Simple cross-lingual ( X ) Transfer NMT model ( SixT ) which can directly translates languages unseen during supervised training .
We initialize the encoder and decoder embeddings of SixT with the XLM -R and propose a two -stage training schedule that trades off between supervised performance and transferability .
At the first stage , we only train the decoder layers , while at the second stage , all model parameters are jointly optimized except the encoder embedding .
We further improve the model by introducing a position disentangled encoder and a capacity - enhanced decoder .
The position disentangled encoder enhances cross-lingual transferability by removing residual connection in one of the encoder layers and making the encoder outputs more language - agnostic .
The capacity - enhanced decoder leverages a bigger decoder than vanilla Transformer base model to fully utilize the labelled dataset .
Although trained with only one language pair , the SixT model alleviates the effect of ' catastrophic forgetting ' ( Serra et al. , 2018 ) and can be transferred to unseen languages .
SixT significantly outperforms mBART with an average improvement of 7.1 BLEU on zeroshot any - to - English translation across 14 source languages .
Furthermore , with much less training computation cost and training data , the SixT model gets better performance on 15 any - to - English test sets than CRISS and m2m - 100 , two strong multilingual NMT baselines .
1
Problem Statement
The zero-shot cross-lingual NMT transfer task ( ZeXT ) explores approaches to enhance the crosslingual transferability of NMT model .
Given an MPE and parallel dataset of a language pair l s - tol t , where l s and l t are supported by the MPE , we aim to train an NMT model that can be transferred to multiple unseen language pairs l i z - to - l t , where l i z = l s and l i z is supported by the MPE .
The learned NMT model is directly tested between the unseen language pairs l i z - to -l t in a zero-shot manner .
Different from multilingual NMT ( Johnson et al. , 2017 ) , unsupervised NMT ( Lample et al. , 2018 ) or zero-resource NMT through pivoting , neither the parallel nor monolingual data in the language l i z is directly accessible in the ZeXT task .
The model has to rely on the offthe-shelf MPE to translate from language l i z .
The challenge to this task is how to leverage an MPE for machine translation while preserving its crosslingual transferability .
In this paper , we utilize XLM -R , which is jointly trained on 100 languages , as the off-the-shelf MPE .
The ZeXT task calls for approaches to efficiently build a many - to- one NMT model that can translate from 100 languages supported by XLM -R with parallel dataset of only one language pair .
The trained model could be useful for translating resource -poor languages .
It can further extend to scenarios where datasets of more language pairs are available .
In addition , while currently the cross-lingual transferability of different MPEs is mainly evaluated on cross-lingual NLU tasks , the ZeXT task provides a new perspective for the evaluation , which can hopefully facilitate the research on MPEs .
Approach
Initialization and Fine-tuning Strategy
For downstream tasks like cross-lingual NLI / QA , only an output layer is added to the pretrained encoder at the fine-tuning stage .
In contrast , an entire decoder is added on top of the MPE when the model is adapted to NMT task .
The conventional strategy that fine-tunes all parameters reduces the cross-lingual transferability in the pretrained encoder due to the catastrophic forgetting effect .
Therefore , we make an empirical exploration on how to initialize and fine- tune the NMT model with an MPE .
The NMT model can be divided into four parts in our method : encoder embedding , encoder layers , decoder embedding , and decoder layers .
With an MPE , each part can be trained with one of the following methods , namely , ? Rand : randomly initialized and trained ; ? Fix : initialized from the MPE and fixed ; ? FT : initialized from the MPE and trained .
We compare different fine-tuning strategies for these modules in a greedy manner . 3 ) ) , the encoder layers ( Strategy ( 4 ) -( 5 ) ) , the decoder embedding ( Strategy ( 6 ) -( 7 ) ) and the decoder layers ( Strategy ( 8 ) -( 10 ) ) with MPE sequentially .
Each time we compare the strategy of ' FT ' and ' Fix ' which fine - tunes the corresponding module or keeps it fixed , respectively .
Since Strategy ( 8 ) -( 9 ) use a larger decoder than the rest ones due to decoder layer initialization , we add Strategy ( 10 ) whose decoder size is the same as Strategy ( 8 ) -( 9 ) for fair comparison .
The best BLEU is bold and underlined .
the encoder embedding , the encoder layers , the decoder embedding , and the decoder layers , sequentially .
The details of experimental settings are in the Section 4.1 .
From the results shown in Table 1 , we observe that it is the best to initialize the encoder embedding , the encoder layers and the decoder embedding with XLM -R and keep their parameters frozen , while randomly initializing the decoder layers ( see Figure 2 ) .
More discussions are in the Section 4.2 .
Two -stage training Since we freeze the encoder and only train the decoder layers , the model is able to perform translation while preserving the transferability of the encoder .
However , freezing most of the parameters limits the capacity of the NMT model , especially when the training data goes large .
Therefore , we propose a second training stage to further improve the translation performance by jointly fine-tuning all parameters except encoder embedding of the NMT .
2 Since the decoder has been well adapted to the encoder at the first stage , we expect the model can be slightly fine-tuned to improve the translation capacity without losing the 2 According to our preliminary experiment , the average BLEU is 0.2 lower when the encoder embedding is also learned at the second stage .
Besides , freezing encoder embedding leads to higher computational efficiency .
transferability of the encoder .
Model
The training strategy and generalization objective of our model are different from vanilla Transformer .
This motivates us to propose a new model that can further improve on zero-shot translations .
The proposed model consists of a position disentangled encoder and a capacity - enhanced decoder , which aims at enhancing the cross-lingual transferability of the encoder and fully utilizing the labelled data , respectively .
Position disentangled encoder
The representations from XLM -R initialized encoder have a strong positional correspondence to the source sentence .
The word order information inside is language -specific and may hinder the cross-lingual transfer from supervised source language to unseen languages .
Inspired by Liu et al . ( 2021 ) , we propose to relax this structural constraint and make the encoder outputs less position - and languagespecific .
More specifically , at the second stage , we remove the residual connection after the selfattention sublayer in one of the encoder layers i during training and inference .
3
The other encoder layers remain the same .
The hidden states in this i th encoder layer are calculated as the following pseudo code : h[ i ] = SelfAttn ( h [ i - 1 ] ) h [ i ] = LayerNorm ( h [ i ] ) # No residual connection here h[ i ] = h[ i ] + LayerNorm ( FFN ( h [ i ] ) ) where SelfAttn is the encoder self-attention sublayer , FFN is the feed -forward sublayer and LayerNorm is the layer normalization .
Liu et al . ( 2021 ) aim at training a language - agnostic encoder for NMT using parallel corpus from scratch .
Compared with them , our method shows that it 's possible to make a pretrained multilingual encoder more language - agnostic by relaxing the position constraint during fine-tuning .
Capacity - enhanced decoder
Some previous work ( Zhu et al. , 2020 ; Yang et al. , 2020 ) incorporates BERT into NMT and configures the decoder size as Vaswani et al . ( 2017 ) .
For example , to train an NMT on Europarl De- En training dataset , the default decoder configuration is Transformer base ( Gu et al. , 2018 ; Currey et al. , 2020 ) .
However , our model relies more on the decoder to learn from the labeled data , as the encoder is mainly responsible for cross-lingual transfer .
This is also reflected in our training strategy : at the first stage only the decoder parameters are optimized , while at the second stage the encoder is only slightly fine-tuned to preserve its transferability .
Model settings
We use the XLM -R base model as the off-the-shelf MPE .
The model is implemented on fairseq toolkit ( Ott et al. , 2019 ) .
We set Transformer encoder the same size as the XLM -R base model .
For the decoder , we use the same hyper-parameter setting as the encoder .
We denote model with such configuration as SixT and use this configuration for our NMT models through the paper unless otherwise stated .
The encoderdecoder attention modules are randomly initialized .
We remove the residual connection at the 11 - th ( penultimate ) encoder layer , which is selected on the validation dataset .
For the empirical exploration in Table 1 , we use two model configurations .
For Strategy ( 1 ) -( 7 ) where decoder layers are trained from scratch , we use a smaller decoder denoted as BaseDec .
This model configuration is denoted as SixT small .
For the rest strategies , we follow the configuration of SixT and denote its decoder as BigDec .
Table 12 in Appendix presents the details of different model configurations .
Training and evaluation
The Adam optimizer ( Kingma and Ba , 2015 ) with ?
1 = 0.9 and ?
2 = 0.98 is used for training .
We use label smoothing with value 0.1 .
The learning rate is 0.0005 and warmup step is 4000 at the first stage .
For the second stage , we set the learning rate as 0.0001 and do not use warmup .
All the drop-out probabilities are set to 0.3 .
We use eight GPUs and the batch size is set as 4096 tokens per GPU .
Maximum updates number is 200k for the first stage and 30 k for the second stage .
We use beam search ( beam size is 5 ) and do not tune length penalty .
We evaluate the results with sacrebleu 5 .
If not specified , the best checkpoint is selected by zero-shot cross-lingual transfer performance on the validation set for all experiments .
We refer the reader to Section B in Appendix for more training details .
Baselines
We compare our model with vanilla Transformer and five conventional methods to apply pretrained Transformer encoder on NMT task .
The pretrained encoders in these methods are replaced with XLM - R base for fair comparison .
? Vanilla Transformer .
The encoder is with the same size of XLM -R base , the decoder uses the size of BaseDec .
All model parameters are randomly initialized .
? + XLM -R fine-tune encoder ( Conneau and Lample , 2019 ) .
The encoder is initialized with XLM -R .
All parameters are trained .
? + XLM -R fine-tune all ( Conneau and Lample , 2019 ) .
All parameters except those of cross attention module are initialized with XLM -R and directly fine-tuned .
? + XLM -R as encoder embedding ( Zhu et al. , 2020 ) .
The XLM -R output is leveraged as the encoder input of the NMT .
The XLM -R model is fixed during training .
?
+ Recycle XLM -R for NMT ( Imamura and Sumita , 2019 ) .
The method initializes the encoder with XLM -R and only trains decoder at the first step .
Then all are trained at the second step .
? XLM -R fused model ( Zhu et al. , 2020 ) .
The XLM -R output is fused into encoder and decoder separately with attention mechanism .
The encoder embedding is initialized from XLM -R to facilitate 5 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a + version .1.5.0 transfer .
The parameters of XLM -R are frozen during training .
Results
The results of the empirical exploration in the Section 3.1 are shown in Table 1 . Since Strategy ( 8 ) -( 9 ) use a larger decoder than the rest ones , we add Strategy ( 10 ) whose decoder size is the same as Strategy ( 8 ) -( 9 ) for fair comparison .
Overall , we observe that it is best to use a big decoder and initialize the decoder embedding and all encoder parameters with XLM -R , and to train the decoder layers from scratch ( Strategy ( 10 ) ) .
To verify the effect of a capacity enhanced decoder in the ZeXT task , we train vanilla Transformer with the same size of Strategy ( 7 ) ( with BaseDec ) and Strategy ( 10 ) ( with BigDec ) using the same training corpus .
6
The vanilla Transformer model with BaseDec and BigDec obtains a BLEU score of 23.5 and 22.9 on the De- En test set , respectively .
The big decoder improves the performance of SixT , but fails to improve that of vanilla Transformer .
This proves the effectiveness of BigDec to improve the zero-shot translation performance of our model .
Table 2 illustrates the performance of the proposed SixT comparing with the baselines .
SixT gets 18.3 average BLEU and improves over the best baseline by 5.4 average BLEU , showing that SixT successfully learns to translate while preserving the cross-lingual transferability of XLM -R .
For all language pairs , SixT obtains better transferring scores .
In contrast , vanilla Transformer can hardly transfer and the other baselines do not well transfer to the distant languages .
In addition to zero-shot performance , SixT also achieves the best result on De- En test set .
Note that the best checkpoint is selected with zero-shot validation set for all methods .
Previous work ( Conneau et al. , 2020 ; Hu et al. , 2020 ) mainly uses XLM -R for cross-lingual transfer on NLU tasks .
The experiments demonstrate that XLM -R can be also utilized for zero-shot neural machine translation if it is fine-tuned properly .
We leave the exploration of cross-lingual transfer using XLM -R for other NLG tasks as the future work .
Ablation Study
We conduct an ablation study with the proposed SixT on the Europarl De- En training set , as shown in Table 3 .
Overall , SixT obtains the best zero-shot translation results , demonstrating the importance of all three components .
From the results of ( 1 ) to ( 3 ) , TwoStage and BigDec along improve the zeroshot translation performance by 0.8 and 0.4 average BLEU over ( 1 ) , respectively .
However , combining them together brings a significant improvement of 2.6 average BLEU over ( 1 ) .
This indicates that TwoStage and BigDec are complementary to each other , thus it is important to use them together .
The results of ( 6 ) ? ( 5 ) confirms our claim : without using BigDec , the performance of SixT drops by 1.8 average BLEU .
We also observe that the supervised task ( De -En ) improves with TwoStage and BigDec ( from results of ( 1 ) to ( 4 ) ) while degrades with Resdrop ( see results of ( 2 ) ? ( 5 ) and ( 4 ) ? ( 6 ) ) .
This is expected since Resdrop helps to build a more language - agnostic encoder .
Although Resdrop degrades supervised performance , it improves zero-shot translation .
The zero-shot performance is related with both supervised performance and model transferability .
By either enhancing the supervised performance ( with TwoStage and BigDec ) or the model transferability ( with Resdrop ) , the overall performance of zero-shot translation can be improved .
Analysis Comparison with multilingual NMT
In this part , we compare SixT with mBART , CRISS ( Tran et al. , 2020 ) and m2m-100 ( Fan et al. , 2020 ) on any - to - English test sets .
mBART is a strong pretrained multilingual encoderdecoder based Transformer explicitly designed for NMT .
We follow their setting and directly fine-tune all model parameters on WMT19 De - En training set .
CRISS and m2m - 100 are the state - of - the - art unsupervised and supervised multilingual NMT models , respectively .
The CRISS model is initialized with the mBART model and iteratively finetuned on 1.8 billion sentences covering 90 language pairs .
m2m-100 is trained with 7.5 billion parallel sentences across 2200 translation directions .
The results of CRISS and m2m - 100 are listed as reference , because CRISS and m2m - 100 are manyto-many NMT models whose performance may degrade due to the competitions among different target languages ( Aharoni et al. , 2019 ; , while SixT is a many - to- one NMT model .
( small ) model are reported .
To compare with these models , we train a manyto- one SixT large model with WMT19 German- English training data , which only consists of 41 million sentences pairs .
It only requires a pretrained XLM -R large model and do not contain any data in other languages .
We remove the residual connection after the self-attention sublayer of the 23 - th ( penultimate ) encoder layer .
The dataset and model configuration details are in Table 9 and 12 in the appendix .
From the results in Table 4 , the SixT large model is significantly better than mBART and slightly better than CRISS and m2m - 100 .
The averaged BLEU across all languages is 7.1 , 0.5 and 1.4 higher than mBART , CRISS and m2m - 100 7 , respectively .
The SixT model has larger model size , nevertheless , the results of SixT are impressive given that SixT does not use any monolingual or parallel texts except German - English training data .
The performance gain over mBART shows that with proper fine-tuning strategy , the pretrained multilingual encoder has better cross-lingual transfer ability on NMT tasks .
In addition , with large-scale German- English parallel data , the SixT model transfers well 7 The 1.2B m2m - 100 model is larger than our model ( 737 M parameters ) and gets 2.2 more average BLEU than SixT .
to distant resource-poor languages like Ne and Si , which indicates a promising approach to translate resource -poor languages .
The SixT performance might be further improved with the data of more languages pairs .
We leave this as future work .
Language transfer v.s. language distance
In this part , we explore the relationship between the cross-lingual transfer performance and the language distance .
We train the SixT models on different supervised language pairs including De-En , Es-En , Fi-En , Hi-En and Zh-En , and then directly apply them to all test sets , as seen in We observe that the cross-lingual transfer generally works better when the SixT model is trained on source languages in the same language family .
The performance on Ko- En is one exception , where Hi-En achieves the best transfer performance .
We also notice that the vocabulary overlapping ( even character overlapping ) between Hindi and Korean is low , showing that significant vocabulary sharing is not a requirement for effective transfer .
When trained on 3.5 million Hi- En sentence pairs , SixT obtains promising results on the Ne-En and Si- En translation , with a BLEU score of 16.7 and 9.6 , respectively .
As comparison ,
The vanilla Transformer supervised with FLoRes training set only receives 14.5 and 7.2 BLEU score on the same test sets .
Therefore , another approach to translate resource - poor languages is to train SixT on similar high- resource language pairs .
As a comparison , we train vanilla Transformer configured as Transformer big 9 without MPE initialization with the same training sets and validation sets .
The poor zero-shot cross-lingual performance of vanilla Transformer indicates that the XLM -R initialized encoder is essential and can produce language - agnostic representations .
Performance on the supervised language pair
To study whether the SixT model gains the crosslingual transfer ability at the cost of performance degradation on the supervised language pair , we compare the vanilla Transformer big model 10 and SixT model on the supervised translation task .
The performance of SixT is lower than that of vanilla Transformer when more than 20 M parallel sentences are available , but it gets better performance with fewer parallel sentences .
The Hindito - English is an exception where SixT has lower BLEU .
When large amount of bi-text data is given , the SixT model size is expected to be increased to fully digest the bi-text .
For example , if we re- Performance with other target language
To build many - to- one NMT model with other target language , we train two SixT models on WMT16 En-De and WMT19 En- De , respectively .
We use Fi-De as validation language pair and Fr/Cs / Ru/ Nl - De as test language pairs .
From the results shown in Table 8 , SixT can obtain reasonable transferring scores to unseen source languages when target language is not English .
Again , the results confirm that the cross-lingual transfer ability improves with larger training data .
Related Work Zero-shot cross-lingual transfer learning Multilingual pretrained models , such as mBERT ( Wu and Dredze , 2019 ) , XLM -R ( Conneau et al. , 2020 ) , mBART , and mT5 ( Xue et al. , 2021 ) , have achieved success on zero-shot crosslingual transfer for various NLP tasks .
The models are pretrained on large-scale multilingual corpora with a shared vocabulary .
After pretrained , it is fine-tuned on labeled data of downstream tasks in one language and directly tested in other languages in a zero-shot manner .
While multilingual pretrained models with encoder-decoder- based architecture Chi et al. , 2020 ) work well on cross-lingual transfer for NLG tasks , multilingual pretrained encoders ( Wu and Dredze , 2019 ; Conneau and Lample , 2019 ; Conneau et al. , 2020 ) are mainly applied to cross-lingual NLU tasks ( Hu et al. , 2020 ) .
In this work , we explore how to fine-tune an off-the-shelf multilingual pretrained encoder for zero-shot cross-lingual transfer in neural machine translation , a typical NLG task .
Pretrained models for NMT Some previous works ( Imamura and Sumita , 2019 ; Conneau and Lample , 2019 ; Yang et al. , 2020 ; Weng et al. , 2020 ; Ma et al. , 2020 ; Zhu et al. , 2020 ) explore methods to integrate pretrained language encoders into the NMT model to improve supervised translation performance .
For instance , Zhu et al . ( 2020 ) propose BERT - fused model , in which they first use BERT to extract representations for an input sentence , and then fuses the representations into both the encoder and decoder via the attention mechanism .
Another line of works Song et al. , 2019 ; Lin et al. , 2020 ) propose novel encoder-decoder - based multilingual pretrained language models and fine-tune such models for NMT .
For example , propose mBART , an encoder-decoder - based Transformer explicitly designs for NMT and demonstrate that mBART can be fine-tuned for supervised and zero-shot NMT .
Different from them , we leverage MPE for zeroshot translation instead of supervised translation .
Among the previous works , Wei et al . ( 2021 ) is the most similar with ours .
They fine-tune their MPE on NMT with a two -stage strategy .
However , their work focuses on improving the MPE for a more universal representation across languages and lacks in - depth study of cross-lingual NMT .
In contrast , we aim at leveraging an MPE for machine translation while preserving its ability of cross-lingual transfer .
Conclusion
In this paper , we focus on the zero-shot crosslingual NMT transfer ( ZeXT ) task which aims at leveraging an MPE for machine translation while preserving its ability of cross-lingual transfer .
In this task , only a multilingual pretrained encoder such as XLM -R and one parallel dataset such as Figure1 :
In the zero-shot cross-lingual NMT transfer task , the model is trained with parallel dataset of only one language pair ( such as De-En ) and a multilingual pretrained encoder .
The trained model is tested on many - to- one language pairs ( like Fi/ Hi / Zh-En ) in a zero-shot manner .
Monolingual text of the to -be-tested source languages is not available in this task .
