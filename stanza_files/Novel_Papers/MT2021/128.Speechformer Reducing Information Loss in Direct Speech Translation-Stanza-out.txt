title
Speechformer : Reducing Information Loss in Direct Speech Translation
abstract
Transformer - based models have gained increasing popularity achieving state - of - the - art performance in many research fields including speech translation .
However , Transformer 's quadratic complexity with respect to the input sequence length prevents its adoption as is with audio signals , which are typically represented by long sequences .
Current solutions resort to an initial sub-optimal compression based on a fixed sampling of raw audio features .
Therefore , potentially useful linguistic information is not accessible to higher - level layers in the architecture .
To solve this issue , we propose Speechformer , an architecture that , thanks to a reduced memory usage in the attention layers , avoids the initial lossy compression and aggregates information only at a higher level according to more informed linguistic criteria .
Experiments on three language pairs ( en?de/es / nl ) show the efficacy of our solution , with gains of up to 0.8 BLEU on the standard MuST - C corpus and of up to 4.0 BLEU in a low resource scenario .
Introduction Speech - to- text translation ( ST ) has been traditionally approached with cascade architectures consisting of a pipeline of two sub-components ( Stentiford and Steer , 1988 ; Waibel et al. , 1991 ) : an automatic speech recognition ( ASR ) , which transforms the audio input into a textual representation , and a machine translation ( MT ) model , which projects the transcript into the target language .
A more recent approach consists in directly translating speech into target text using a single model ( B?rard et al. , 2016 ; Weiss et al. , 2017 ) .
This direct solution has interesting advantages ( Sperber and Paulik , 2020 ) : i ) it can better exploit audio information ( e.g. prosody ) during the translation phase , ii ) it has lower latency , and iii ) it is not affected by error propagation .
The authors contributed equally .
Thanks to these advantages , the initially huge performance gap with cascade systems has gradually closed ( Ansari et al. , 2020 ) , motivating research towards further improvements .
Direct ST models are fed with features extracted from the audio with high frequency ( usually every 10 ms ) .
This , on average , makes the resulting input sequence of vectors ?10 times longer than the corresponding text , leading to an intrinsically redundant ( i.e. long and repetitive ) representation .
For this reason , it is not possible to process speech data with a vanilla Transformer encoder ( Vaswani et al. , 2017 ) , whose self-attention layers have quadratic memory complexity with respect to the input length .
State - of- the- art architectures tackle the problem by collapsing adjacent vectors in a fixed way , i.e. by mapping a predefined number of vectors ( usually 4 ) into a single one , either using strided convolutional layers ( B?rard et al. , 2018 ; Di Gangi et al. , 2019 ; Wang et al. , 2020a ) or by stacking them ( Sak et al. , 2015 ) .
As a positive side effect , these length reduction solutions lower input redundancy .
As a negative side effect , they disregard the variability over time of the amount of linguistic and phonetic information in audio signals ( e.g. due to pauses and speaking rate variations ) by giving equal weight to all features .
In doing this , relevant features are penalized and considered equally important to the irrelevant ones , resulting in an information loss .
Recently , Salesky et al. ( 2019 ) obtained considerable translation quality gains by collapsing consecutive vectors with the same phonetic content instead of compressing them in a fixed way .
Zhang et al. ( 2020 ) also showed that selecting a small percentage ( ? 16 % ) of input time steps based on their informativeness improves ST quality .
On the downside , these approaches respectively require adding a model that performs phoneme classification and a pre-trained adaptive feature selection layer on top of an ASR encoder , losing the compactness of direct solutions at the risk of error propagation .
In direct ST , Liu et al. ( 2020 ) and Gaido et al . ( 2021 ) addressed the problem with a transcript / phoneme- based compression leveraging Connectionist Temporal Classification ( CTC - Graves et al. 2006 ) .
However , since these methods are applied to the representation encoded by Transformer layers , the initial content -unaware downsampling of the input is still required for memory reasons , at the risk of losing important information .
To avoid initial fixed compression , we propose Speechformer : the first Transformer - based architecture that processes full audio content maintaining the original dimensions of the input sequence .
Inspired by recent work on reducing the memory complexity of the attention mechanism ( Wang et al. , 2020 b ) , we introduce a novel attention layer - the ConvAttention - whose memory requirements are reduced by means of convolutional layers .
As the benefits of avoiding the initial lossy compression might be outweighed by the increased redundancy of the encoded audio features , we aggregate the high- level representation of the input sequence in a linguistically informed way , as in ( Liu et al. , 2020 ; Gaido et al. , 2021 ) .
In other words , we collapse vectors representing the same linguistic atomic content ( words , sub-words , pauses ) into a single element , since they express the same linguistic information .
The usage of the ConvAttention and of the linguistically motivated compression produces a considerably shorter , yet informative , sequence that fits the memory requirements of vanilla Transformer encoder layers .
Experiments on three language directions ( en?de/es / nl ) show that the proposed architecture outperforms a state - of - the - art ST model by up to 0.8 BLEU points on the standard MuST - C corpus and obtains significantly larger gains ( up to 4.0 BLEU ) in a low resource setting where the amount of training data is reduced to 100 hours .
Model
In this section , we first introduce a novel attention layer that enables to process raw audio features without downsampling ( ?2.1 ) .
Then , we present an architecture that leverages this attention mechanism in the first encoder layers and reduces the redundancy of the more informative but longer resulting sequences with CTC compression ( ?2.2 ) .
ConvAttention layer State- of- the- art ST models employ convolutional neural networks to sample the feature sequence to a lower dimension ( typically by a factor of 4 ) , enabling the use of Transformer layers otherwise impossible given their memory consumption .
Outside ST , the Linformer architecture ( Wang et al. , 2020 b ) has been recently proposed to reduce the quadratic complexity of the product between the attention matrix ( resulting from the product of the query - Q - and key - K - matrices ) and the value ( V ) matrix by applying a linear projection to K and V .
These projections bring the dimension of the sequence length of K and V to a fixed value , yielding a linear memory complexity .
However , a direct application of this architecture to ST is problematic due to the high variability in audio lengths .
On one side , mapping those sequences to a fixed dimension can cause an excessive information loss , with a consequent performance drop .
On the other , it poses technical issues : the linear projection matrix has size n ?
k , where n is the maximum input length and k is the fixed dimension .
If the input has a length n shorter than n , which is a common case due to the high variability in length of audio sequences , only the first n weights of the matrix are updated .
This results in gradients of different dimensions across GPUs , leading to training failures due to inconsistencies .
To avoid the aforementioned problems , we propose the adoption of ConvAttention ( Figure 1 ) , in which the linear projections of the Linformer architecture are substituted , both in K and V , with a single 1D convolutional layer .
Hence , the length of the sequences used in the scaled dot-product attention depends on the stride of the convolution , a hyper-parameter we named compression factor ( ? ) , which controls the memory complexity of the ConvAttention .
Namely , being n the temporal dimension of K and V , the convolution output length is n ? and the complexity of the ConvAttention is O ( ( n ? ) 2 ) , i.e. a 1 ? 2 factor lower than a vanilla Transformer self-attention .
For instance , setting ? to 4 leads to the same memory consumption as standard ST models with an initial ?4 subsampling ( i.e. with two initial convolutional layers with stride 2 ) .
Notice that the output sequence length is still equal to the input sequence length as it depends on the length of Q that is not modified .
Speechformer
The introduction of ConvAttention layers allows us to avoid sub-optimal fixed compressions that disregard the variability over time in the amount of audio information .
However , since an encoder consisting only of ConvAttention layers does not compress the length of the original input sequence , the decoder will be fed with long and redundant sequences that are difficult to attend , leading to potential performance degradation .
To overcome this problem , as in ( Liu et al. , 2020 ; Gaido et al. , 2021 ) , we apply a content - informed compression to high - level hidden states trained using the CTC loss ( Graves et al. , 2006 ) to represent the linguistic content .
Specifically , the CTC loss produces a prediction for each input time step and then merges equal predictions for consecutive time steps .
The resulting sequence is compared with the reference , which is the sequence of subwords representing the transcript of the input utterance .
CTC compression , similarly to the loss computation , collapses consecutive features corresponding to the same predictions , averaging them .
After this operation , the sequence is reduced to a representation dimensionally closer to its textual content , which can be processed by the original attention mechanism without the need of approximations .
Speechformer ( see Figure 2 ) , is composed of E L ConvAttention layers up to a CTC compression layer , after which there are E T Transformer encoder layers .
The E L ConvAttention layers are meant to learn the linguistic content of the input audio while the E T Transformer encoder layers are in charge of learning higher - level semantic representations , i.e. the encoder outputs , which the decoder has to convert into a text in the target language .
We also maintain the two 1D convolutional layers before the ConvAttention layers but without striding , so that no sub-sampling is applied to the input .
We make this choice both to keep the number of parameters comparable to the existing architectures , and to let the model learn a better representation of the Table 1 : BLEU on MuST - C en-de dev set varying the compression factor ? and 1D convolutional kernel size .
The scores are obtained without label smoothing .
input before feeding it to the attention mechanism .
Experimental Settings
We experimented on three languages of MuST - C ( Cattoni et al. , 2021 ) : English - German , English - Spanish , and English - Dutch .
To ensure the reproducibility of our work , all training details are provided in the Appendix and the code - based on fairseq ( Ott et al. , 2019 ) - is released open source .
1 Following ( Wang et al. , 2020 b ) , we share the convolution parameters of the ConvAttention layers both among K and V and among the attention heads .
We select the compression factor and the 1D convolution kernel size with a set of preliminary experiments on the en-de validation set .
The compression factor ( ? ) is chosen among 4 , 8 , and 16 , since 4 is the minimum value that avoids out -ofmemory issues .
The kernel size is set either equal to or twice as the value of ?.
Table 1 shows that the combination of a compression factor of 4 and a kernel size of 8 leads to better performance compared to the other combinations .
Consequently , in all our experiments we use this setting .
We initialize the ConvAttention weights of Speechformer with those of a pre-trained ST model Table 2 : BLEU score ( average over 3 runs ) on English ?
Dutch ( en-nl ) , English ?
German ( en-de ) , and English ?
Spanish ( en-es ) of MuST - C tst-COMMON ( tst ) and the dev ( validation ) set .
The * symbol indicates statistically significant improvements over the baseline .
Statistical significance is computed with a t-test ( Student , 1908 ) , whose null hypothesis is that the mean of the considered experiment is not higher than the mean of the baseline .
We consider the result statistically significant if we can reject the null hypothesis with 95 % confidence .
having only ConvAttention layers in the encoder , since , in the initial random state , the CTC - based compression might not properly reduce the input sequence , leading to out - of-memory issues in the following Transformer encoder layers .
Notice that the pre-training does not improve performance .
Indeed , Gaido et al. ( 2021 ) already showed that the encoder pre-training improves the baseline performance only without the additional CTC loss and that the results obtained by training without CTC loss and with encoder pre-training are identical to those achieved with the additional CTC loss .
These findings have been confirmed in our experiments : i ) initializing the encoder of the baseline with either an ASR or an ST encoder did not bring any improvement , and ii ) our results are on par with those obtained with encoder pre-training and no additional CTC loss .
We do not include the results with encoder pre-training of the baselines , as they do not bring any additional insight .
Results
We compare our proposed model to a strong baseline represented by a Transformer - based model with initial fixed sub-sampling ( Wang et al. , 2020a ) and its baseline + compression variant that includes the average CTC compression strategy , as per ( Gaido et al. , 2021 ) .
We choose to also develop the second baseline to make the comparison with Speechformer fair since they both use the CTC compression strategy .
Table 2 reports the results computed with SacreBLEU 2 ( Post , 2018 ) .
For each experiment , we report the average over 3 runs to ensure that performance differences do not depend on the fluctuations of particularly good or bad runs .
First , it can be noticed that our baseline is in line with state - of- the - art architectures trained only 2 BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.5.0 on MuST -C ( Wang et al. , 2020a ; Inaguma et al. , 2020 ) .
Second , the addition of CTC compression to the baseline model does not bring benefits .
This confirms the findings of Gaido et al . ( 2021 ) , who showed that applying CTC compression using transcripts produces differences in score that are not statistically significant .
Speechformer , instead , results in statistically significant improvements over the baseline in all language directions , with BLEU gains ranging from 0.5 ( for en-nl ) to 0.8 ( for ende ) .
As the CTC compression is not helpful for the baseline , we also evaluate a model ( Plain ConvAttention ) whose encoder is a stack of ConvAttention layers , i.e. without vanilla Transformer - encoder layers and any form of compression .
The drop in performance with respect to Speechformer varies between 0.4 and 0.8 BLEU on all language pairs , supporting our hypothesis that a non-compressed encoder output is too redundant to be effectively attended by the decoder .
Low -Resource Settings .
We suppose that the higher gains on en-de may be related to the size of the training data .
Indeed , the en-de section of MuST - C used for training is the smallest one , containing 20 % fewer data than the en-es section and 10 % less than the en-nl one .
Thus , we study Speechformer 's performance in different data conditions by progressively reducing the amount of training data .
For this analysis , we select the enes section of MuST - C as it contains the highest number of hours ( 478h ) among the three languages , and we experiment with three subsets , respectively containing 385h ( corresponding to the amount of training data for en-de ) , 200h , and 100h ( which can be considered a limited quantity given that the number of hours is respectively less than half and one fourth of the available data ) .
Figure 3 shows that the gains obtained by Speechformer over the baseline do not vary significantly between 385h and 478h ( 0.5 vs 0.6 BLEU ) .
We can then conclude that the gain variation between en-de and en-es does not depend on the smaller size of the en-de training set .
However , in the low resource settings ( 200h and 100h ) , the gains obtained by the Speechformer are much larger , amounting to 1.1 BLEU with 200h and 4.0 BLEU with 100h .
To validate the robustness of these results , we also experimented on the en-de language pair and obtained consistent results : Speechformer outperforms the baseline by 1.5 BLEU ( 19.6 vs 18.1 BLEU ) with 200h of training data and by 1.9 BLEU ( 9.7 vs 7.8 BLEU ) with 100h of training data , achieving a considerable relative improvement of more than 24 % .
Although it brings consistent and significant gains in higher resource scenarios , these experiments show that Speechformer is particularly fruitful in low-resource settings .
We leave to future work the assessment of the behavior of Speechformer in unrestricted data conditions ( e.g. when using large ASR corpora to generate pseudo- labelled ST training data ) .
Inference Time .
The ConvAttention layers process the whole input sequences , which are 4 times larger than those elaborated by the baseline attention mechanism .
Thereby , a slow-down at inference time is expected , especially for the Plain Con-vAttention , whose encoder layers are all ConvAttention layers .
The last column of Table 2 confirms that the Plain ConvAttention architecture is 1.8 times slower than the baseline , i.e. the inference time is nearly twice .
Speechformer is also slower than the baseline , but the overhead amounts to only 30 % instead of 80 % .
Moreover , it can be noticed that the size of the attention matrix - and therefore the corresponding computational cost - can be controlled in the Speechformer with the compression factor ( ? ) hyper-parameter .
We leave to future studies the analysis of the trade- off between overall translation quality and inference time , which is usually irrelevant in offline ST , but becomes critical in simultaneous scenarios .
Manual Analysis .
Lastly , we inspected the baseline and Speechformer outputs to better understand the reason behind the improvements brought by our architecture .
This qualitative analysis was conducted on a sample of 200 sentences of the ende test set - the language direction showing the largest gap between the systems ( + 0.8 , see Table 2 ) - by a professional linguist with C2 German level .
It emerged ( see the Appendix for examples ) that Speechformer tends to have better wordordering , a typical problem arising when translating from an SVO language like English to an SOV language like German .
Furthermore , Speechformer outputs display a better punctuation positioningattributable to an improved handling of pauses and prosody - and a reduction of the number of audio misunderstandings and omissions .
Together with the overall BLEU gains , these findings provide us with interesting hints about the potential of Speechformer .
Conclusion
In the wake of previous works showing the benefits of a content - informed compression over fixed downsampling of the audio features , we proposed Speechformer : the first ST Transformer - based model able to encode the whole raw audio features without any sub-optimal initial subsampling typical of current state - of - the - art models .
Our solution is made possible by the introduction of a modified attention mechanism - the ConvAttention - that reduces the memory complexity to O ( ( n ? ) 2 ) .
As the plain application of ConvAttention layers leads to redundant sequences , high - level hidden states are compressed with a CTC - based strategy to obtain a compact , yet informative representation that can be processed by vanilla Transformer encoder layers .
Experiments on three language pairs show that Speechformer significantly outperforms a stateof - the- art ST model by 0.5- 0.8 BLEU , reaching a peak of 4 BLEU points in a low resource scenario .
It was a way that parents could figure out which were the right public schools for their kids .
