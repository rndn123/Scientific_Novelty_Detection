title
Quality Estimation Using Dual Encoders with Transfer Learning
abstract
This paper describes POSTECH 's quality estimation systems submitted to Task 2 of the WMT 2021 quality estimation shared task : Word and Sentence -Level Post-editing Effort .
We aim to improve the stability of recently proposed quality estimation models , which usually have a single encoder based on the selfattention mechanism to simultaneously process both of the two input data : a source sequence and its machine translation ; considering that such models are not propped up by pre-trained language models ' monolingual word representations , which are generally accepted as reliable representations for various natural language processing tasks .
Therefore , our model first uses two pre-trained monolingual encoders and then exchanges their output information through two additional cross attention networks .
According to the official leaderboard , our systems outperform the baseline systems in terms of the Matthews correlation coefficient for machine translations ' wordlevel quality estimation and in terms of the Pearson 's correlation coefficient for sentencelevel quality estimation by 0.4126 and 0.5497 respectively .
Introduction Quality estimation ( QE ) is the task of estimating the quality of given machine translations without regard to their reference translations ( Blatz et al. , 2004 ; Specia et al. , 2009 ) .
As reference translations are generally unavailable in real life , QE should help to treat output texts of machine translation ( MT ) systems .
QE can be categorized into several subtasks , and this round of the WMT QE task has three subtasks , yet we focus on Task 2 : Word and Sentence - Level Post-editing Effort .
In Task 2 , while sentence - level QE aims to predict the Human-Targeted Translation Edit Rate ( HTER , Snover et al. 2006 ) , which measures the edit distance between an MT output ( mt ) and its human post-edited text ( pe ) , word - level QE aims to predict OK - BAD tags for three sequences of tokens : the sequence of words in a source text ( src ) depending on whether they are correctly translated referring to mt ; the sequence of words in mt depending on their correctness ; and < GAP > tokens , which each represent the gap between two adjacent words , depending on the existence of any missing words .
As other recent QE models do , our method also applies transfer learning , considering that pretrained language models ( LM ) have been successfully applied to various natural language processing ( NLP ) tasks including QE ; many previous studies Hu et al. , 2020 ; Wu et al. , 2020 ; Lee , 2020 ; Moura et al. , 2020 ; Nakamachi et al. , 2020 ; Rubino , 2020 ) that apply pre-trained LMs to QE have adopted multilingual or crosslingual LMs such as multilingual - BERT ( Pires et al. , 2019 ) , XLM ( Conneau and Lample , 2019 ) , and XLM -R ( Conneau et al. , 2020 ) to process the two input data src and mt .
Such cross-lingual LMs have a single Transformer ( Vaswani et al. , 2017 ) encoder using only the self-attention mechanism to create vector representations of the input data and predict the labels .
However , it appears possible to further improve the stability of those models , considering that they are not propped up by pre-trained LMs ' monolingual word representations , which are generally accepted as reliable representations for various NLP tasks .
With this background , we propose a QE model that has two separate pre-trained encoders that each produce monolingual representations of src and mt , respectively .
On top of each encoder , we add a cross attention network for the learning of the cross-lingual context between src and mt ; these networks will produce two sets of cross-lingual representations for QE .
We conduct simple experiments to compare the performance of our systems and ensembles of them with that of the baseline systems and that of other submitted systems for ( Clark et al. , 2020 )
Task 2 .
Experimental results imply that although our systems do not always outperform the baseline systems , they do in terms of the Matthews correlation coefficient ( MCC ) for mt 's word - level QE and in terms of the Pearson 's correlation coefficient ( PCC ) for sentence - level QE by 0.4126 and 0.5497 respectively .
Related Work Because our model does not confine its monolingual encoders to specific pre-trained LMs , all pretrained LMs can be considered relevant .
Among them , most of the recently proposed pre-trained LMs are denoising autoencoders , of which the pretraining task is usually to select about 15 % of tokens in unlabeled input sequences and apply the attention mechanism to those tokens ( Yang et al. , 2019 ) or is to mask certain tokens ( Devlin et al. , 2019 ) and then restore them .
However , in our experiments , our systems use ELECTRA ( Clark et al. , 2020 ) . ELECTRA introduces " replaced token detection " as an additional pre-training task and let the language model learn to distinguish between real input tokens and specious but artificially generated tokens .
In detail , when the generator network predicts the tokens in the masked positions , some of the predicted tokens are corrupted , and then this output sequence is fed into a Transformer - based discriminator network , which predicts whether each token in the fed sequence is the same as the original one or is a replaced one ( Figure 1 ) .
We suppose that this process and QE are similar to each other in that both of them predict the soundness of the given tokens , so ELECTRA would be one of the most appropriate pre-trained LMs for our QE model 's monolingual encoders , especially for Task 2 .
Model Description
Our model uses two ELECTRAs : one ELECTRA 1 that is pre-trained with English corpora and the other ELECTRA 2 pre-trained with German corpora .
Figure 2 depicts the overall structure of our model .
Dual Monolingual Encoders
Our model has dual encoders : a pre-trained English ELECTRA processing src and a pre-trained German ELECTRA processing mt .
These encoders will produce reliable monolingual representations of src and mt respectively to provide these refined representations to the upper cross attention networks .
Because unlike other pre-trained QE models that have a single Transformer encoder being fed with the concatenation of src and mt , our model lets the two different encoders process the two input data respectively , we exclude the segment embeddings , which are used to distinguish one language from another , and assign different positional embeddings to each input data .
In addition , for sentence - level QE , mt 's special token < CLS > is used to predict the HTER .
Cross Attention Networks
We attach a cross attention network to each pretrained encoder ; it learns the cross-lingual context information by using the encoders ' refined monolingual representations of the two input data .
Although the structure of a cross attention network is identical to that of the encoders , the cross attention networks are not pre-trained , so we train them after the random initialization of their parameters .
We find that applying transfer learning to the cross attention networks is not available due to the absence of pre-trained language models that are pre-trained to perform cross attention on cross-lingual input data by using one side as a query vector and the other side as both a key vector and a value vector just as the Transformer decoder performs multihead attention on the output of the Transformer encoder .
Sentence -Level QE
To predict the HTER for sentence - level QE we employ the final hidden vector m < CLS > of the mt-side cross attention network , which is the final representation of the < CLS > token , as the representation of the mt sequence as a whole .
After this representation passes through double linear layers with the GELU ( Hendrycks and Gimpel , 2016 ) activation function , the HTER of the given mt sentence is estimated as follows .
l = W h m < CLS > + b 0 ?HTER = w T h GELU ( l ) + b 1 ( 1 ) We have trainable parameters W h ?
R H?H , w h ?
R H , b 0 ? R H , and b 1 ? R ; H denotes hidden vectors ' dimension .
We use the mean squared error of this estimator , that is , the difference between the estimated HTER ?HTER and the ground truth HTER value y HTER , as the training loss L HTER = MSE ( ?
HTER , y HTER ) . ( 2 )
Word - Level QE src-Side Prediction
We use the final hidden vector s ( i ) ( i ? { 1 , ... , | S | } , where | S | is the number of tokens in the tokenized src sequence ) of the srcside cross attention network corresponding to each token in src to predict OK or BAD in the token 's position .
After each of these representations passes through a linear layer , the word- level probability of the corresponding token being OK or BAD is predicted with a sigmoid activation function : P ( i ) s = sigmoid ( w T s s ( i ) ) , ( 3 ) where w s ?
R H is a trainable parameter .
We use the binary cross-entropy loss function ; we also introduce an extra hyperparameter k s to prevent our model from being overfitted because the statistics of the ratio between the number of OK tags and that of BAD tags in our training data ( Table 1 ) can misguide the model to have the tendency of outputting OK even when it should output BAD .
The src-side loss is as follows : L src = 1 | S| | S | i=1 k s y ( i ) s logP ( i ) s + ( 1 ? y ( i ) s ) log ( 1 ? P ( i ) s ) , ( 4 ) where y ( i ) s is a ground - truth OK - BAD tag .
mt - Side Prediction
We use the final hidden vector m ( i ) ( i ? { 1 , ... , |M |} , where | M | is the number of tokens in the tokenized mt sequence ) of the mt-side cross attention network corresponding to each token in mt to predict OK or BAD in the token 's position .
We estimate the probabilities of the word tokens P ( i ) m = sigmoid ( w T m m ( i ) ) , ( 5 ) where w m ?
R H is a trainable parameter .
We also use the final hidden vector m ( j ) ( j ? { 1 , ... , |M |+ 1 } including the vector in the position of the last < SEP > token to predict OK or BAD for the last < GAP > token .
We estimate the probabilities of the < GAP > tokens P ( j ) g = sigmoid ( w T g m ( j ) ) , ( 6 ) where w g ?
R H is a trainable parameter .
The mt-side prediction loss equals the sum of the losses for word tokens and < GAP > tokens : L mt = L m + L g , ( 7 ) where L m = 1 | M | |M | i=1 k m y ( i ) m logP ( i ) m + ( 1 ? y ( i ) m ) log ( 1 ? P ( i ) m ) , ( 8 ) and L g = 1 ( | M | + 1 ) |M |+1 j=1 k g y ( j ) g logP ( j ) g + ( 1 ? y ( j ) g ) log ( 1 ? P ( j ) g ) , ( 9 ) y ( i ) m and y ( j ) g being ground - truth OK - BAD tags for a word token and a < GAP > token respectively and hyperparameters k m and k g being introduced for the same reason why we introduce k s .
Finally , we define the word- level loss and the overall QE loss of our model as follows .
L word = L src + L mt ( 10 ) L QE = L word + L HTER ( 11 ) 4 Experiments
Datasets
In our experiments , we used the eSCAPE ( Negri et al. , 2018 ) between src and mt , and TER .
Then , we created a tuple of labels ( T src , T word mt , T gap mt , the TER ( Snover et al. , 2006 ) ) for each triplet 3 . Finally , we tokenized and truncated both of the artifical data and the WMT 2021 official data by using a pre-trained tokenizer based on WordPiece ( Wu et al. , 2016 ) .
QE Pre-training After obtaining about three million of artificial triplets , we made the final QE pre-training data by joining the artificial training data and the official human-labeled data together ; especially , we augmented the quantity of the latter by replication to allow our systems to learn from both kinds of training data relatively more evenly during the QE pre-training .
Our systems learn to predict all kinds of labels jointly ( L QE , Eqn. 11 ) considering the close correlation among the subtasks in Task 2 .
We used 1,000 triplets in the WMT 2021 's official development dataset as validation data .
Fine-Tuning
We used only the WMT 2021 human-labeled data for fine-tuning .
In contrast with the QE pretraining , we fine- tuned our systems to each subtask : the prediction of the sentence - level task ( L HTER , Eqn. 2 ) and the word- level task ( L word , Eqn. 10 ) Considering the overproportion of OK tags in our training data ( Table 1 ) , we set a large k s , k m , and k g ( ? 3.2.2 ) in our experiments .
Ensemble Learning Besides single fine- tuned systems , we also made ensembles of our best fine- tuned systems , each of which has a different random seed from that of the others .
In detail , after fine-tuning several single systems with different random seeds , for each seed , we picked out the top two systems , each of which is different from the other in certain variable training conditions such as how its cross attention networks have been randomly initialized in that instance , in terms of their performance on our validation dataset .
Finally , we averaged the weights of the systems element-wisely for better generalization and made the ensembles .
Hyperparameters
We used ELECTRA - base ( Clark et al. , 2020 ) s as pre-trained monolingual LMs for our dual monolingual encoders 4 .
In the QE pre-training , we used get_schedule_with _warmup 5 as our learning rate scheduler with 3,000 warm - up steps .
We used the AdamW ( Loshchilov and Hutter , 2018 ) optimizer that has a weight decay with ?=0.5 , ? 1 =0.9 , ? 2 =0.999 , and = 1e - 8 , together with gradient clipping .
Setting a batch size of 64 for both the QE pre-training and fine-tuning , we set a learning rate of 1e?5 and a tuple of ( k s = 1 , k m = 1 , k g = 3 ) for the QE pre-training and a learning rate of 5e?5 and a tuple of ( k s = 2 , k m = 2 , k g = 4 ) for the fine-tuning , respectively .
We validated the performance of our systems on our validation set every 5,000 steps during the QE pre-training and every 200 steps during the fine-tuning , respectively ; we applied early stopping with a patience of 30 .
Results
In comparison with our single system , our ensembles report an improved PCC , mt-side words MCC , and mt-side < GAP >s MCC of about 0.5497 , 0.4296 , and 0.1225 respectively ( Table 2 ) .
Compared to other systems submitted to the WMT 2021 English - German QE Task 2 , our systems outperform the baseline systems in terms of the sentence - level PCC ( Table 3 ) and the mt-side words MCC ( Table 4 ) .
Our systems are inferior to the baseline systems in terms of the src-side MCC and the mt-side < GAP >s MCC by a narrow margin ( Table 4 ) .
However , because our systems have a smaller number of parameters than other submitted systems , we expect that it is possible to improve the performance of our systems by adopting larger pre-trained LMs such as ELECTRA - large ( Clark et al. , 2020 ) .
Conclusion
We model our systems submitted to Task 2 of the WMT 2021 QE shared task on our proposed model , which uses dual pre-trained monolingual encoders and two additional cross attention networks to pro-cess the two input data src and mt more effectively considering that the latest Transformer - based QE models are not propped up by pre-trained monolingual word representations .
We expect that the cross attention networks enable the two pre-trained monolingual encoders to exchange cross-lingual information without losing their stability and to learn the subtasks of Task 2 jointly and also separately .
Experimental results partially supports this expectation : according to the official leaderboard , our systems outperform the baseline systems in terms of the mt-side words MCC and the sentence - level PCC by 0.4126 and 0.5497 respectively , although they do not in terms of the src-side MCC and the mt-side < GAP >s MCC .
Neverhteless , it appears possible to improve the performance of our systems by adopting larger pre-trained LMs , and thus , our future work will explore such aspects and other related new methods .
Figure 1 : 1 Figure 1 : A diagram depicting the training task of ELECTRA ( Clark et al. , 2020 )
