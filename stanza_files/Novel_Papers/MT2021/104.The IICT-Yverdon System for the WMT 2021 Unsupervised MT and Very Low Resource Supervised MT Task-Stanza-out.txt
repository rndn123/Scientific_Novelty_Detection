title
The IICT - Yverdon System for the WMT 2021 Unsupervised MT and Very Low Resource Supervised MT Task
abstract
In this paper , we present the systems submitted by our team from the Institute of ICT ( HEIG - VD / HES - SO ) to the Unsupervised MT and Very Low Resource Supervised MT task .
We first study the improvements brought to a baseline system by techniques such as back -translation and initialization from a parent model .
We find that both techniques are beneficial and suffice to reach performance that compares with more sophisticated systems from the 2020 task .
We then present the application of this system to the 2021 task for low-resource supervised Upper Sorbian ( HSB ) to German translation , in both directions .
Finally , we present a contrastive system for HSB - DE in both directions , and for unsupervised German to Lower Sorbian ( DSB ) translation , which uses multi-task training with various training schedules to improve over the baseline .
Introduction
In this paper , we present the systems submitted to the WMT 2021 task on Unsupervised MT and Very Low Resource Supervised MT .
We first build a series of baseline systems , driven mostly by considerations of simplicity , trained on data from the 2020 edition of the task , for translation between Upper Sorbian ( HSB ) and German ( DE ) .
These systems , described in Section 3 , enable us to quantify the merits of using additional back - translated data ( Sennrich et al. , 2016 ) and of initializing the system for a low-resource pair with parameters learned on a high- resource pair ( same target language and related source language ) .
The systems described above serve as the basis for our 2021 baseline submitted to the shared task , for DE?HSB and HSB ?
DE , presented in Section 4 , which improves upon our 2020 baseline with the addition of more parallel data , and achieves competitive performance with the use of back -translation and parent-initialization only .
However , this approach does not lead to an effective baseline for unsupervised German to Lower Sorbian ( DSB ) translation ( Section 5 ) .
In Section 6 , we present experiments with a contrastive system that implements multi-task learning , with several schedules , in which denoising tasks together with translation are presented to the systems in increasing order of complexity , leading to more robust HSB ?
DE systems , together with a strategy of diverse ensembling .
We also use our DE ?
HSB system to initialize a multi-task DE ?
DSB system for the unsupervised task , although in this case the performance is not competitive .
Datasets
We use various Upper Sorbian datasets from the 2020 edition of the task , and additional WMT data , as presented in Table 1 .
The monolingual HSB data from 2020 comes from three sources : sorbian_institute _monolingual consists of a mix of high - and medium-quality HSB data provided by the Sorbian Institute ; witaj_monolingual consists of high-quality HSB data from the Witaj Sprachzentrum ; finally , web_monolingual consists of web-scraped noisier HSB data gathered by the Center for Information and Language Processing from LMU Munich ( Fraser , 2020 ) .
We kept from all datasets only sentences that have strictly more than 2 and strictly fewer than 301 words .
3 Baseline HSB ?DE System on 2020 Data 2018 ) as implemented in SentencePiece .
1 We apply 32,000 merges and the other parameters of SentencePiece are kept to default values .
We obtain 600k sentences of HSB data from sorbian_institute_monolingual , witaj_monolingual and train.hsb- de , the latter being the HSB side of the 2020 training data .
We do not use web_monolingual as it appears to be noisy , due to the collection process .
For CS and DE , 600k sentences are selected randomly from the monolingual corpora listed in Table 1 .
The vocabulary generated by Sentence - Piece is converted from log probabilities to frequencies using the spm_to_vocab .
py tool from the OpenNMT - py toolkit .
Using a common Sentence - Piece model for the three languages is not obligatory , but appeared to improve the performance by 2 - 3 BLEU points in most cases .
System Parameters and Results
We use OpenNMT - py ( Klein et al. , 2017 ) for our experiments .
2 We start with Transformer - Base ( Vaswani et al. , 2017 ) ( 78 M parameters ) but also experiment with Transformer - Big ( 245 M parameters ) , with their main parameters described in Table 2 .
We apply the same regularization and optimization procedures to the two models .
We accumulate gra-dients over 2 batches and train on 2 GPUs , with a batch_size of 1 k for Base and and 2 k for Big .
We use the " noam " learning rate schedule ( Vaswani et al. , 2017 ) with its values at each step multiplied by two , and 8 k warmup steps .
We evaluate and save checkpoints every 5 k steps .
Final translations are generated with a beam width of 5 , ensembling the last two checkpoints in these experiments .
We report BLEU scores ( Papineni et al. , 2002 ) obtained with SacreBLEU ( Post , 2018 ) 3 ) .
The improvement of this single enrichment with imperfect data of the initial low-resource system thus exceeds 4 BLEU points .
Initialization with Parameters from a High-Resource Pair
The second technique we use for improvement is transfer from a high- resource pair ( Zoph et al. , 2016 ; Kocmi and Bojar , 2018 ) , i.e. initialization with parameters from an MT system trained on such a pair .
As Upper Sorbian has many similarities with Czech , which is a high- resource language , we initialize the HSB - DE model with the parameters of a model trained for CS ?
DE , then train it with the same data as in the previous subsection .
Firstly , the CS ?DE model is trained using Europarl and News Commentary , and reaches a BLEU score of 27.13 on a sample test set extracted from these two corpora .
The resulting HSB ?
DE system reaches BLEU scores of 55.99 / 47.53 , a further increase of about 3 BLEU points ( third line of Table 3 ) .
The use of an even larger dataset further improves performance : the addition of the JW300 corpus ( Agi? and Vuli? , 2019 ) to the CS ?
DE training data increases BLEU by half a point ( 56.5 on ' dev ' ) .
The rather small increase could be attributed to the large difference in domains between JW300 and the HSB / DE data .
Since back - translation can provide very large amounts of data , we also trained a Transformer Big ( with the parameters shown in Table 2 ) with the addition of the monolingual German corpora of Europarl and JW300 backtranslated into Upper Sorbian .
This model reaches 58.08 / 49.99 BLEU points respectively on ' dev ' and ' devtest ' , improving performance by more than 1.5 BLEU points .
This is currently our best baseline model for HSB ?
DE , obtained with two simple augmentation techniques only .
We can compare this score with three of the highest -scoring systems on the 2020 HSB ?
DE ' devtest ' set , noting some of the differences between them and our baseline .
Scherrer et al. ( 2020 ) achieved a BLEU score of 56.9 using backtranslation and bilingual pre-training with CS?DE , but also scheduled multitask with several monolingual and multilingual tasks .
Knowles et al. ( 2020 ) achieved a BLEU score of 58.9 using iterative backtranslation , multiplication of the HSB data for BPE training , and character - and word - level lexical modifications of Czech to make it more similar to Upper Sorbian .
Libovick ?
et al. ( 2020 ) achieved a score of 56.0 with much larger corpora for back - translation and CS ?
DE pre-training ( 14 M lines ) and the use of an unsupervised CS ?
HSB system to translate the CS side of the DE / CS parallel data into HSB .
Initialization with Parameters from Other High-Resource Pairs
We studied the role of the closeness between Upper Sorbian and the high- resource source language used for initialization , by reproducing the above initialization experiments ( CS?DE ) with Polish and French instead of Czech .
Polish is a West Slavic language just as Czech and Upper Sorbian , although geographically more remote , whereas French is a Romance language : we thus expected the former to outperform the latter .
The improvement brought by the additional rounds of back - translation is quite marginal , therefore we do not pursue this approach , and focus on a system which is initialized from a parent high- resource pair and trained with original and back - translated data , where the latter comes from a reverse system trained only with the original parallel HSB - DE data provided by the shared task .
Baseline HSB ?DE Low Resource Systems for 2021
Given the results of the previous section , we choose the Transformer - Big for our 2021 baseline .
We change the dropout level from 0.3 to 0.1 since our experiments revealed an increase in performance with the latter value .
Furthermore , we add the 87,502 sentences of additional parallel HSB - DE training data provided in 2021 to the datasets used in our 2020 baseline .
We use the same Sentence - Piece model with DE , HSB , and CS data that we used for our 2020 baseline system , with approximately 700k lines for each language .
At translation time , after observing a number of out - of- vocabulary tokens , we replace the unknown tokens with the source token that has the highest attention weight .
We do not make any further changes regarding our 2020 Transformer - Big model .
The scores of our baseline systems on 2020 and 2021 data are synthesized in Table 3 for the various techniques we experimented with .
Our baseline HSB ?
DE model with combined 2021 and 2020 data is system # 5 in Table 3 : it reaches BLEU scores of 59.29 on the ' dev ' set and 51.86 on the ' devtest ' set after training for 150,000 steps and by ensembling the best 4 saved checkpoints .
For our DE?HSB model , we obtain 57.22 on the ' dev ' set and 49.95 on the ' devtest ' set after training for 85,000 steps and by ensembling the best 4 saved checkpoints .
After the submission to the 2021 shared task , we continued training the above HSB ?
DE model up to 300,000 steps - Ensembling the last 4 saved checkpoints , BLEU scores were close to the ones shown in the last line of Table 3 , reaching 59.42 on the ' dev ' set and 51.37 on the ' devtest ' set .
However , several checkpoints gained almost 2 BLEU points on ' dev ' , pointing to the potential benefits of training for a longer time .
Baseline for Unsupervised DE?DSB Translation Moreover , we studied the same techniques for translating Lower Sorbian ( DSB ) , for which no parallel resources are provided .
We translated the monolingual DSB data provided by the organizers with our HSB ?
DE model , hypothesizing that the differences between DSB and HSB are small enough to obtain an acceptable DSB - DE pseudo- parallel corpus , with high-quality text on the DSB side , following insights from our experience with Swiss - German dialects ( Honnet et al. , 2018 ) .
We use the parameters from our best DE ?
HSB model to initialize a DE?DSB model that we train for 120k steps with the DSB - DE pseudo-parallel data .
When ensembling the best 4 checkpoints , we reach BLEU scores of 8.25 / 8.22 without observing any significant increase of the scores during training .
In fact , the initial score , which is the performance of a DE?HSB model on the DE - DSB ' devtest ' data , is even slightly higher .
An even lower BLEU score was reached when using our CS ?
DE model to translate monolingual DSB data into DE to obtain a pseudo-parallel corpus , thus confirming the finding that this approach does not lead to pseudo- parallel corpora of sufficient quality .
Therefore , we did not submit these translations to the 2021 shared task .
Contrastive HSB ?DE and DE?DSB Systems using Multi-Task Learning
In contrast to the baseline systems presented above , we study an innovative approach , in which we train multitask systems with denoising auxiliary tasks that are presented in order of increasing complexity .
This insight is drawn from curriculum learning ( Bengio et al. , 2009 ) .
We thus test whether increasing the complexity of the tasks makes it easier for an NMT model to learn the simple tasks first , and the harder ones later in training .
As Raffel et al. ( 2020 ) showed , source- tosource pre-training and multitasking improves translation , but not enough to compete with stateof - the - art setups .
Therefore , instead , we perform target - to - target and source + target - to - target denoising .
Considering their findings , we decide not to introduce special tokens into our vocabulary , such as mask tokens ( instead just deleting the tokens with wish to mask ) , or sentence and language separators .
Finally , due to computational constraints , we use the Transformer - Base as our architecture .
Data and Auxiliary Tasks
For our contrastive system we consider two new monolingual corpora in Czech and in German : the document - separated news crawls from WMT20 ( Barrault et al. , 2020 ) , consisting of text extracted from online newspapers .
They contain 17 M lines and 43 M lines respectively in each language .
To keep training time within acceptable limits , we sample 1.4 M lines from these corpora ( including empty lines that serve as document- separators ) , we apply the same length - based filtering criterion ( 2 < L < 301 ) as for our baseline data , and we also delete all sentences that are made of more than 15 % non-alphabetic characters .
The resulting Czech corpus is 1.3 M lines and 131,644 documents long , and the German corpus is 1.2 M lines and 130,891 documents long .
For our document - level denoising tasks , we first divide into " chunks " a tokenized documentseparated corpus so that each chunk is no more than 500 subwords in length , made up of consecutive lines in the same document ; we only select documents made of at least 3 sentences .
In Table 4 we list all corpora that we use to create our auxiliary data , including monolingual corpora back - translated with our baseline systems .
The DE?DSB back - translated data was obtained with a baseline DE ?HSB model .
We make use of the four following auxiliary denoising tasks ( the main task being of course standard sentence - level translation , with all parallel and back - translated data ) , with the first two inspired by Devlin et al . ( 2019 ) ; Raffel et al. ( 2020 ) and Conneau and Lample ( 2019 ) : 1 . Masking ( MASK ) : randomly delete 15 % of words of a line on the source side , but keep the full original sequence on the target side .
2 . Translation Language Modeling ( TLM ) : concatenate the source and target sentences from a parallel corpus , and apply separately the MASK algorithm to each one .
The target is the original target sentence .
3 . Mask Document First Words ( MF ) : for each chunk , leave the first sentence untouched , and for the remaining ones delete the first word of each sentence , with the target being the full original sequence in the same language .
4 . Next Sentence Generation ( NSG ) : for each chunk , leave all the sentences untouched except the last one , of which delete all but the two longest words ; the model has to output the full original sequence .
Keeping the two longest words ( in characters ) is based on the assumption that they are the most informative ones in the sentence .
The denoising tasks are listed above by increasing complexity .
Indeed , MASK , as a monolingual sentence - level task , is the simplest denoising task we present , with TLM following , as it includes a context in a different language which needs to be identified .
The two document- level tasks are more complex , as they require a larger context .
In particular , NSG is harder than MF , since it consists of reconstructing a whole sentence with just two words from the original sequence , forcing the model to look for a more abundant context to estimate the correct answer .
Furthermore , predicting the first word requires to take into account exclusively intersentential context , whereas masking a single random word allows also for the use of intra-sentential context , with the latter providing more direct context than the former .
4 ) , and backtranslated corpora ( 3 ) used for our contrastive system trained with multi-tasking .
Each corpus is assembled from the raw datasets presented in Table 1 with the filtering setup described in Subsection 6.1 .
For bilingual corpora , we indicate the number of words in each language .
Corpus
Training Schedules
All our models translate to one target language only , therefore the target side of our datasets is always the same language , be it for the monolingual denoising tasks or for TLM .
Since all datasets correspond to sequence - to- sequence tasks , we are in essence simply removing and introducing datasets during training .
The specific splits of the tasks in each training schedule have been manually set , guided by the reasons given below , without any attempt for fine-tuning .
All the hyperparameters of the models are those presented in Section 3 , with the only exception of the parameters of CS ?
DE models for initialization , which were trained on 4 GPUs to reduce training time .
When we introduce new tasks during the training of a model , we continue training from the last checkpoint of the previous task .
Training CS ?
DE models .
Both directions are trained according to the same schedule , shown in Table 5 , with simply the source and target languages switched .
First , we train for 30 k steps with a TLM task , then we train for another 30 k steps with a mixture of the MF auxiliary task ( 50 % of the samples ) and the main translation task ( 50 % ) .
Then we continue for another 30 k steps , changing MF to NSG .
Finally , we finish with 30 k steps on translation only .
In total , the model is being trained for 30 k steps ( 25 % ) with TLM , 15 k steps ( 12.5 % ) with MF , 15 k steps ( 12.5 % ) with NSG , and 60 k steps ( 50 % ) with the main task , i.e. sentence - level translation .
Steps ?1000 Task 0-30 30-60 60-90 90-120 TLM 100 % MF 50 % NSG 50 % Translation 50 % 50 % 100 % Table 5 : Training schedule of the parent models in CS?DE .
For each direction , the model is only trained to output target language , so corpora differ depending on the direction ( see 6.1 ) .
Both models are trained for 120k steps with three auxiliary denoising tasks and the main sentence - level translation task .
HSB ?DE .
The schedules of the child models are shown in Table 6 for the ( DE , HSB ) pair .
For HSB ?
DE , we continue training from the best scoring checkpoint of the last 60 k steps of the parent CS ?
DE model , and start with a TLM task for 60 k steps .
Then , we introduce back - translated data only for 60 k steps .
We continue with 60 k steps with true parallel data only .
Additionally , we train two more models by continuing to train another 60 k steps from the best scoring checkpoint ( which is also the last one saved ) , with one of the models having its learning rate schedule reset .
Although at first performance worsens due to a more aggressive learning rate during the warmup steps , the model ends up converging to a score similar to the one we obtain if we continue to train without resetting the learning rate schedule .
The goal is to emulate a multiple - run seeding strategy for ensembling , by achieving a different weight distribution among the two models .
We additionally train a randomly - initialized model with parallel data only , for 60 k steps , also for ensembling .
We generate our translations of the test data with an ensembling of 16 models : the best 4 checkpoints from the parallel- only randomly - initialized model , the best 4 of our main setup during the first 60 k steps of parallel - only training , and the 4 checkpoints each for the two runs that continued to train with , and respectively without , resetting the learning rate schedule .
DE?HSB .
We continue training from the bestscoring checkpoint of the last 60 k steps of DE?CS , and provide it with a MASK task for 60 k steps , since the model has not seen the target language at all during pre-training , for this direction .
Then , we provide the model with a TLM task for 60 k steps .
Since in this direction we have much less backtranslated data than in the opposite , we decide to train for 60 k more steps with 50 % of the samples being from the back - translated data , and the other 50 % from the true parallel corpora .
Finally , we continue training two more models in the same manner as explained for the HSB ?
DE direction .
We additionally train a randomly - initialized parallel data only model for 60 k steps for ensembling .
We translate with the same ensembling setup as described for the HSB ?
DE direction .
Our child DE ?HSB models show that the scheduled training improves results over the baseline .
The HSB ?
DE model with a training schedule ( system 2 in Table 9 ) , trained with a lighter architecture ( Base vs. Big ) and lower quality parent model ( 19.8 vs. 24.5 ) , achieves a higher BLEU score than the system in Section 4 , as shown in The scores of our DE ?
DSB model ( Table 10 ) show that the quality of the back - translated data with our HSB ?
DE model improved slightly with the addition of the MASK monolingual task , but not with the addition of a DE?
HSB translation task .
However , when including in the ensemble the models trained on a DE?HSB task , scores improved from 8.7 to 9.6 on the ' devtest ' set .
This was the version submitted to the shared task on unsupervised MT ( DE? DSB ) .
Finally , as we can see in Table 11 , even with our possibly suboptimally trained parent models and lighter architecture , the strategy of diverse ensembles and scheduled multi-task training improved over our best performing baselines given in Section 4 for all directions of the low-resource MT task .
System
HSB ?DE
Conclusion
In this work , we showed that non-iterative backtranslation and parent-model transfer learning provide improvements for translation in a low-resource setting .
Furthermore , multi-task scheduled training with monolingual or cross-lingual tasks also resulted in better models .
In particular , child models starting with Translation Language Modeling tasks and Masking tasks improved over the baseline in all translation directions .
Finally , our strategy of ensembling diverse models also produced higher scores than a mere checkpoint ensemble strategy .
Table 1 : 1 Monolingual and parallel corpora with their languages and numbers of lines ( sentences ) and words , before and after filtering by length ( keeping sentences with more than 2 and fewer than 301 words ) .
