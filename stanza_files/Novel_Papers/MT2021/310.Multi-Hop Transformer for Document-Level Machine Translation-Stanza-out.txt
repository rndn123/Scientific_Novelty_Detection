title
Multi-Hop Transformer for Document -Level Machine Translation
abstract
Document - level neural machine translation ( NMT ) has proven to be of profound value for its effectiveness on capturing contextual information .
Nevertheless , existing approaches 1 ) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process ; and 2 ) feed ground - truth target contexts as extra inputs at the training time , thus facing the problem of exposure bias .
We approach these problems with an inspiration from human behavior - human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse .
To this end , we propose a novel Multi-Hop Transformer ( MHT ) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process .
Specifically , our model serves the sentence - level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively .
Experiments on four widely used document translation tasks demonstrate that our method can significantly improve documentlevel translation performance and can tackle discourse phenomena , such as coreference error and the problem of polysemy .
Introduction Neural machine translation ( NMT ) employs an endto-end framework ( Sutskever et al. , 2014 ) and has advanced promising results on various sentencelevel translation tasks ( Bahdanau et al. , 2015 ; Gehring et al. , 2017 ; Vaswani et al. , 2017 ; Wan et al. , 2020 ) .
However , most of NMT models handle sentences independently , regardless of the linguistic context that may appear outside the current sentence ( Tiedemann and Scherrer , 2017a ) .
This makes NMT insufficient to fully resolve the typical context-dependent phenomena problematic , ?
These authors contributed equally to this work .
* Corresponding author .
e.g. coreference ( Guillou , 2016 ) , lexical cohesion ( Carpuat , 2009 ) , as well as lexical disambiguation ( Gonzales et al. , 2017 ) .
Recent studies ( Tu et al. , 2018 ; Maruf et al. , 2019 ; Tan et al. , 2019 ; Kim et al. , 2019 ; Zheng et al. , 2020 ; Sun et al. , 2020 ; Ma et al. , 2020 ) have proven to be effective on tackling discourse phenomena via feeding NMT with contextual information , e.g. sourceside ( Wang et al. , 2017 ; Voita et al. , 2018 ; or target - side context sentences ( Bawden et al. , 2018 ; Miculicich et al. , 2018 ) .
Despite their successes , these methods simply merge the representations of context sentences together , lacking a mechanism to explicitly characterize the inter-sentence reasoning upon the context .
Another shortage in existing document- level NMT is the problem of exposure bias .
Most of methods utilized the ground - truth target context for training but the generated translations for inference , leading to inconsistent inputs at training and testing time ( Ranzato et al. , 2015 ; Koehn and Knowles , 2017 ) .
Intuitively , human translators tend to acquire useful context information from the reasoning process among sentences , thus figuring out the correct meaning when they encounter ambiguity during translation .
Sukhbaatar et al. ( 2015 ) and empirically verified that modeling multi-hop reasoning among sentences benefits to the language understanding task , e.g text comprehension .
Voita et al. ( 2019 ) showed that documentlevel NMT model can profit from relative positions with respect to context sentences , which to some extent confirms the importance of the relationship among sentences .
Meanwhile , Xia et al. ( 2017 ) demonstrated that sentence - level NMT could be improved by a two -pass draft-editing process , of which the second - pass decoder refines the target sentence generated by a first - pass standard decoder .
Accordingly , we propose to improve document- level NMT using a novel framework - Multi-Hop Transformer , which imitates draft-editing and reasoning process of human translators .
Specifically , we implement an explicit reasoning process by exploiting source and target antecedent sentences with concurrently stacked attention layers , thus performing the progressive refinement on the representations of the current sentence and its translation .
Besides , we leverage the draft to present context information on the target side during both training and testing , alleviating the problem of exposure bias .
We conduct experiments on four widely used document translation tasks : English -German and Chinese-English TED , English -Russian Opensubtitles , as well as English - German Europarl - 7 datasets .
Experimental results demonstrate that our method significantly outperforms both context-agnostic and context - aware methods .
The qualitative analysis confirms the effectiveness of the proposed multihop reasoning mechanism on resolving many linguistic phenomena , such as word sense disambiguation and coreference resolution .
Our contributions are mainly in : ?
We propose the Multi-Hop Transformer .
To the best of our knowledge , this is the first pioneer investigation that introduces multi-hop reasoning into document- level NMT .
?
The proposed model takes target context drafts into account at the training time , which devotes to avoid the training - generation discrepancy .
?
Our approach significantly improves document-level translation performance on four document - level translation tasks in terms of BLEU scores and solves some context- dependent phenomena , such as coreference error and polysemy .
Preliminary Transformer NMT is an end-to - end framework to build translation models .
Vaswani et al. ( 2017 ) propose a new architecture called Transformer which adopts self-attention network for both encoding and decoding .
Both its encoder and decoder consist of multiple layers , each of which includes a multi-head self-attention and a feed-forward sublayer .
Additionally , each layer of the decoder applys a multi-head cross attention to capture information from the encoder .
Transformer has shown superiority in a variety of NLP tasks .
Therefore , we construct our models upon this advanced architecture .
Document- level NMT
In order to correctly translate the sentence with discourse phenomena , NMT models need to look beyond the current sentence and integrate contextual sentences as auxiliary inputs .
Formally , let X = ( x 1 , x 2 , ... , x I ) be a source - language document composed of I sentences , where x i = ( x i 1 , x i 2 , ... , x i N ) denotes the i th sentence containing N words .
Correspondingly , the target - language document also consists of I sentences , Y = ( y 1 , y 2 , ... , y I ) , where y i = ( y i 1 , y i 2 , ... , y i M ) denotes the i th sentence involving M words .
Document - level NMT incorporates contextual information from both source side and target side to autoregressively generate the best translation result that has highest probability : P ? ( y i |x i ) = M m=1 P ? ( y i m |y i < m , x i , X ?i , Y ?i ) ( 1 ) where y i <m is the sequence of proceeding tokens before position m .
X ?i and Y ?i denote the context sentences of the i th sentence .
Related Work Several studies have explored multi-input models to leverage the contextual information from source-side ( Jean et al. , 2017 ; or target- side sentences Miculicich et al. , 2018 ) .
For the former , propose a new encoder to represent document - level context from previous source-side sentences .
Tiedemann and Scherrer ( 2017 b ) and Junczys - Dowmunt ( 2019 ) utilize the concatenation of previous source-side sentences as input , while Voita et al . ( 2018 ) make use of gate mechanism to balance the weight between current source sentence and its context .
For the latter , Miculicich et al . ( 2018 ) propose a hierarchical attention ( HAN ) framework to capture the target contextual information in the decoder .
Bawden et al. ( 2018 ) , and Maruf et al . ( 2019 ) from the contexts sentence by sentence and then perform reasoning to figure out the exact meaning .
We attribute that such reasoning process is also beneficial to machine translation task .
Recent successes in text comprehension communities have to some extent supported our hypothesis ( Hill et al. , 2015 ; Kumar et al. , 2016 ) .
For example , Sukhbaatar et al. ( 2015 ) propose a multi-hop end-to - end memory network , which can renew the query representation with multiple computational steps ( which they term " hops " ) .
Dhingra et al. ( 2016 ) extend an attention - sum reader to multi-turn reasoning with a gating mechanism .
In addition , introduce multi-hop attention , which used multiple turns to effectively exploit and reason over the relation among queries and documents .
Softmax ? ? ? ? ? ? ? ?+? ? ? ? ? ? ? ? ? ? ? ? ? ?+? ? ? ? ? 6 ? 6 ? Translation Source Sentence i Target Sentence i Draft Sentence Context Draft Sentence i-1 Context Draft Sentence i-k+1 Context Draft Sentence i-k Source Context Sentence i-1 Source Context Sentence i-k+1 Source Context Sentence i-k
Source-side Sentence Encoder
Multi
In this paper , we propose to bring the idea of multi-hop into document translation and aim at mimicking the multi-step comprehension and revising process of human translators .
Contrast with those models for text comprehension which scan the query and document for multiple passes , our model iteratively focuses on different context sentences , which captures the inter-sentence reasoning semantics of contextual sentences to incrementally refine the representation of current sentence .
Multi-Hop Transformer
With this mind , we propose a novel method called Multi-Hop Transformer , which models the reasoning process among multiple contextual sentences in both source side and target side .
The source-side contexts are directly acquired from the document .
The target- side contexts , called target - side drafts in this paper , are generated by a sentence - level NMT model .
These contexts are fed into the Multi-Hop Transformer with pre-trained encoders .
The overall architecture of our proposed model is illustrated in Figure 1 , which consists of three components : ? Sentence Encoder :
This component contains two pre-trained encoders , one of which is called source -side sentence encoder and the other is called target - side sentence encoder .
These encoders generate representations for source-side contexts and target-side drafts respectively .
?
Multi-Hop Encoder :
We extend the original Transformer encoder with a novel multi-hop encoder to efficiently perform sentence - bysentence reasoning on source-side contexts and generate the representation for the current sentence .
?
Multi-Hop Decoder : Similarly , a multi-hop decoder is proposed to acquire information from the target-side drafts and models the translation probability distribution .
Sentence Encoder
We use multi-layer and multi-head self-attention architecture ( Vaswani et al. , 2017 ) to obtain the representations for source-side contexts and target-side drafts .
Similar to the encoder of Transformer , sentence encoder contains a stack of six identical layers , each of which consists of two sub-layers .
The first sub-layer is a multi-head attention ( Q , K , V ) , which takes a query Q , a key K and a value V as inputs .
The second sub-layer is a fully connected feed -forward network ( FFN ) .
Source -Side Sentence Encoder .
This encoder is utilized to generate the representations for sourceside contexts , as shown in Figure 1 .
For the current sentence s = x i to be translated , we use the previous sentences X ?i = ( x i? k , x i?k+1 , ... , x i?1 ) in the same document as the source- side context , specially denoted as c i?k s , c i?k+1 s , ... , c i?1 s for clarity .
k is the context window size .
For the j th context , we obtain the A ( n ) c i ? j s which denotes the n th hidden layer representation of c i? j s as follows : A ( n ) c i? j s = MHA ( H ( n?1 ) c i? j s , H ( n?1 ) c i? j s , H ( n?1 ) c i? j s ) ( 2 ) where n = 1 , 2 , ... , 6 . MHA represents the standard Multi-Head Attention function ( Vaswani et al. , 2017 ) . j denotes the distance between the context sentence and current sentence .
Target - Side Sentence Encoder .
Most existing works use ground - truth target - side contexts as the input of decoder during training ( Voita et al. , 2019 ) .
However , the target contexts at training and testing are drawn from different distributions , leading to the inconsistency between training and testing .
To alleviate this problem , we instead make use of target - side context drafts generated from a pretrained sentence - level translation model .
Similar to source-side sentence encoder , this target - side context draft encoder is used to obtain the context representation A
Multi-Hop Encoder
The multi-hop encoder contains a stack of 6 identical layers , each of which contains the following sub-layers : Self-Attention Layer .
The first sub-layer makes use of multi-head self-attention to encode the information of current source sentence s and obtains the representation A ( n ) s . Multi-Hop Attention Layer .
The second sublayer uses a multi-hop attention to perform sentence - by-sentence reasoning on c s in sentence order as shown in Figure 1 .
Each reasoning step , also called a hop , is implemented by a multi-head attention layer .
The first hop takes representation A s i?k = MHA ( A ( n ) s , A ( n ) c i? k s , A ( n ) c i? k s ) ( 3 )
The other hops are implemented : B ( n ) s i? j = MHA ( B ( n ) s i?j?1 , A ( n ) c i? j s , A ( n ) c i? j s ) ( 4 ) where j = k?1 , k?2 , ... , 1 . j denotes the distance between the context sentence and current sentence .
Context Gating .
The information of current source sentence is crucial in translation while the contextual information is auxiliary .
In order to avoid excessive utilization of contextual information , a context gating mechanism Yang et al. , , 2019 ) is introduced to dynamically control the weight between context sentences and current sentence : ? = ?( W a A ( n ) s + W b B ( n ) s i?1 ) , ( 5 ) where ? is the logistic sigmoid function and ? is the context gate .
W a and W b denote the weight matrices of A ( n ) s and B ( n ) s i?1 , respectively .
H ( n ) s = ? A ( n ) s + ( 1 ? ? ) B ( n ) s i?1 ( 6 ) 3957
Finally , we obtain the representation Enc s = H ( 6 ) s as the final output of the multi-hop encoder .
Multi-Hop Decoder Similarly , the multi-hop decoder involves a stack of 6 identical layers .
Each of them contains five sub-layers .
Self-Attention Layer .
The first sub-layer utilizes multi-head self-attention to encode the information of current target sentence t and obtains the representation A F ( n ) t = MHA ( A ( n ) t , A ( n ) d , A ( n ) d ) .
( 7 ) Multi-Hop Attention Layer .
Similar to the encoder , a multi-hop reasoning process is performed on the target-side contexts .
The target-side drafts are generated from corresponding source sentences by a pre-trained sentence - level NMT model .
The first hop takes representation F ( n ) t as the query and the representation A t i? k = MHA ( F ( n ) t , A ( n ) c i? k t , A ( n ) c i? k t ) ( 8 )
The other hops are achieved : B ( n ) t i? j = MHA ( B ( n ) t i?j?1 , A ( n ) c i? j t , A ( n ) c i? j t ) ( 9 ) where j = k?1 , k?2 , ... , 1 . j denotes the distance between the context draft and current target draft .
Context Gating .
Same as the multi-hop encoder , the final output of multi-hop decoder is computed as : G ( n ) t = ? F ( n ) t + ( 1 ? ? ) B ( n ) t i?1 ( 10 ) where ? is used to regulate the weight of target -side contextual information .
Encoder -Decoder Attention Layer .
Finally , we use an encoder-decoder attention layer to integrate the output of multi-hop encoder Enc s with the current target representation G ( n ) t . H ( n ) t = MHA ( G ( n ) t , Enc s , Enc s ) ( 11 ) where H ( n ) t represents the final representation of decoder .
Experiments 4.1 Datasets
To evaluate the effectiveness of the proposed MHT , we conduct experiments on four widely used document translation tasks , including the TED Talk ( Cettolo et al. , 2012 ) with two language pairs , Opensubtitles and Europarl7 .
All datasets are tokenized and truecased with the Moses toolkit ( Koehn et al. , 2007 ) , and splited into sub-word units with a joint BPE model ( Sennrich et al. , 2016 ) with 30 K merge operations .
The datasets are described as follows : ? TED Talk ( English - German ) :
We use the dataset of IWSLT 2017 MT English - German track for training , which contains transcripts of TED talks aligned at sentence level .
dev2010 is used for development and tst2016 - 2017 for evaluation .
Statistically , there are 0.21 M sentences in the training set , 9 K sentences in the development set , and 2.3 K sentences in the test set .
? TED Talk ( Chinese-English ) :
We use the corpus consisting of 0.2 M sentence pairs extracted from IWSLT 2014 and 2015 Chinese - English track for training .
dev2010 involves 0.8 K sentences for development and tst2010 - 2013 contains 5.5 K sentences for test .
? Opensubtitles ( English - Russian ) :
We make use of the parallel corpus from .
The training set includes 0.3 M sentence pairs .
There are 6 K sentence pairs in development set , and 9 K in test set .
? Europarl7 ( English - German ) :
The raw Europarl v7 corpus ( Koehn , 2005 ) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using .
We process the raw data and extract the parallel corpus as same as Maruf
Baselines
We compare our model against four NMT systems as follows : ?
Transformer :
The state- of- the - art contextagnostic NMT model ( Vaswani et al. , 2017 ) . ? CA - Transformer : A context - aware transformer model ( CA - Transformer ) with an additional context encoder to incorporate document contextual information into model . ? CA- HAN : A context- aware hierarchical attention networks ( CA - HAN ) which integrate document contextual information from both source side and target side ( Miculicich et al. , 2018 ) . ? CADec : A two - pass machine translation model ( Context - Aware Decoder , CADec ) which first produces a draft translation of the current sentence , then corrects it using context ( Voita et al. , 2019 ) .
Implementation Details
Our model is implemented on the open-source toolkit Thumt ( Zhang et al. , 2017 ) .
Adam optimizer ( Kingma and Ba , 2014 ) is applied with an initial learning rate 0.1 .
The size of hidden dimension and feed -forward layer are set to 512 and 2048 respectively .
Encoder and decoder have 6 layers with 8 heads multi-head attention .
Dropout is 0.1 and batch size is set to 4096 .
Beam size is 4 for inference .
Translation quality is evaluated by the traditional metric BLEU ( Papineni et al. , 2002 ) on tokenized text .
Context window size is set to 3 , consistent with the experiments in Section 5.2 .
To initialize the source-side sentence encoder in Section 3.1 , a sentence - level NMT model is trained from source language to target language using the corresponding datasets without additional corpus .
The encoder of this trained model is used to initialize the source -side context encoder .
Also , we utilize the trained model to translate the source-side sentences and obtain the target-side drafts .
Similarly , we train a sentence - level model from target language to source language to initialize the targetside encoders in Section 3.1 .
In order to reduce the computational overhead , we share the parameters among the sentence encoders on the same side .
The settings of these two sentence - level NMT models are consistent with our baseline Transformer model .
Results
Table 1 summarizes the BLEU scores of different systems on four tasks .
As seen , our baseline and re-implemented existing methods outperform the reported results on the same data , which we believe makes the evaluation convincing .
Clearly , our model MHT significantly improves translation quality in terms of BLEU on these tasks , and obtains the best average results that gain 0.38 , 0.69 and 1.57 BLEU points over CADec , CA - Transformer and CA - HAN respectively .
These results demonstrate the universality and effectiveness of the proposed approach .
Moreover , without in - 3959 troducing large-scale pre-trained language models , our translation systems achieve new state - of - the - art translation qualities across three examined translation tasks , which are TED ( En- De ) , Opensubtitles ( En - Ru ) and Europarl7 ( En - De ) .
Overall , our experiments indicate the following two points : 1 ) explicitly modeling underlying reasoning semantics by a multi-hop mechanism indeed benefits neural machine translation , and 2 ) the improvements of our model are not from enlarging the network .
Analysis
In this section , to gain further insight , we explore the effectiveness of several factors of our model , including 1 ) multi-hop attention ; 2 ) context window size ; 3 ) reasoning direction ; 4 ) sides for introducing context ; and 5 ) target contexts .
Moreover , we show qualitative analysis on discourse phenomena to better understand the advantage of our model .
Multi-Hop Attention
To further investigate the effect of multi-hop reasoning , we compare our multi-hop attention with two baseline context modeling methods , including " Concat " and " Hierarchical Attention " .
Table 2
Context Window Size
As shown in Figure 2 , we conduct experiments with different context window sizes to explore its effect .
When the window size is less than 4 , the model obtains more information from contexts and achieves better performance as the window size gets larger .
However , when window size is increased to 4 , we find that the performance does n't improve further , but decreases slightly .
This phenomenon shows that contexts far from the target sentence may be less relevant and cause noise ( Kim et al. , 2019 ) .
Therefore , we choose the window size 3 for our model MHT .
Reasoning Direction In Table 3 , we conduct an ablation study to investigate the effect of reasoning direction on MHT model .
L2R denotes the MHT model with natural reasoning direction , which encodes context sentences from left to right by multi-hop layers , while R2L indicates the MHT model encoding context sentences with an opposite direction .
We observe that integrating reasoning processes by multi-hop attention with both direction can improve the effect of Transformer due to the incorporation of extra context information .
Besides , MHT model reasoning with natural sentence order outperforms the MHT model with an opposite reasoning direction .
This is within our expectation since the L2R reasoning is consistent with the reading and reasoning direction of human being .
Different Sides for Introducing Context
As shown in
Draft vs. Reference
In training , the context draft sentences can be the drafts from a pre-trained MT system or the context references , while only the generated drafts are accessible during inference .
Table 5 shows the BLEU scores of the MHT models using generated drafts and context references during training .
We can see that the MHT model using drafts as contexts outperforms the MHT model directly using target - side context references , possibly because using context references faces the problem of exposure bias and the drafts generated from pre-trained translation system can bridge the gap between training and testing data .
Table 5 : The performance of the MHT models using drafts or context references .
Qualitative Analysis
We present the translated results from baselines and our model in Table 6 to explore how multihop reasoning mitigate the impact of common discourse phenomena in translation process .
According to Case 1 in Table 6 , the noun " hum " in source sentence is translated to " der Summen " by Transformer and CA - Transformer , which fail to understand the correct coreference .
In German , " der " is a masculine article .
The correct article is neutral article " das " because the " hum " is from a machine .
MHT can perform a reasoning process to leverage the context information effectively and figure out the " hum " is from an engine according to Context 2 .
Case 2 indicates that MHT can understand the exact meaning of a polysemous word , benefiting from the reasoning process among the contexts .
In this case , Transformer , CA - Transformer and CA - HAN all translates the noun " show " into " zeigt " , which means " display " .
The translation is clearly wrong in this context .
The correct meaning of " show " is TV shows like " Breaking Bad " according to the Context 1 .
In contrast , our model can take previous contexts in consideration and reason out the exact meaning of the polysemous word .
Conclusion
In with an inspiration from human reasoning behavior to explicitly model the human-like draft-editing and reasoning process .
Experimental results on four widely used tasks show that our model can achieve better performance than both context -agnostic and context - aware strong baseline .
Furthermore , the qualitative analysis shows that the multi-hop reasoning mechanism is capable of solving some discourse phenomena by capturing context semantics more accurately .
Figure 1 : 1 Figure 1 : Illustration of Multi-Hop Transformer .
c i? j s and c i? j t indicate the j th previous sentence in the source side and target side respectively .
d denotes the draft of current source sentence .
All drafts are generated by a pre-trained sentence - level NMT model .
The modules inside dashed box are the proposed multi-hop attention layers , which gradually refine the representation of current sentence .
Finally , the context gate ? is used to control the contextual information .
