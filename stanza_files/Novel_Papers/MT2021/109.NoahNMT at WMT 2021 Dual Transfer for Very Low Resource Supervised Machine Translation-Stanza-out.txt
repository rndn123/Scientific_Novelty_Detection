title
NoahNMT at WMT 2021 : Dual Transfer for Very Low Resource Supervised Machine Translation
abstract
This paper describes the NoahNMT system submitted to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .
The system is a standard Transformer model equipped with our recent technique of dual transfer .
It also employs widely used techniques that are known to be helpful for neural machine translation , including iterative backtranslation , selected finetuning , and ensemble .
The final submission achieves the top BLEU for three translation directions .
Introduction
In this paper , we describe the NoahNMT system submitted to one of the WMT 2021 shared tasks .
The shared task features both unsupervised machine translation and very low resource supervised machine translation .
As our core technique is mainly suitable for low resource supervised machine translation , we participated in four translation directions between Chuvash - Russian ( chv-ru ) and Upper Sorbian - German ( hsb - de ) .
Our core technique is called dual transfer ( Zhang et al. , 2021 ) , which belongs to the family of transfer learning .
It transfers from both high resource neural machine translation model and pretrained language model to improve the quality of low resource machine translation .
During the preparation for the shared task , we conducted additional experiments that supplement the original paper , including the choice of parent language , the validation of Transformer big model , and the usage of dual transfer along with iterative back - translation .
In addition , we also applied proven techniques to strengthen the quality of our system , including selected finetuning and ensemble .
Our final submission achieves the top BLEU on the blind test sets for three translation directions : chv?ru , ru?chv , and hsb?de .
Approach
In this section , we describe the techniques used in our system .
Interested readers are encouraged to check out the original papers for further details .
Dual Transfer
We reproduced the illustration of dual transfer from the original paper ( Zhang et al. , 2021 ) , as shown in Figure 1 .
This illustration shows the case of general transfer , where the high resource translation direction is A?B , and the low resource translation direction is P?Q.
As discussed in the original paper , in many cases , it is possible to use shared target transfer ( B=Q ) or shared source transfer ( A=P ) .
Taking chv?ru as an example , we can choose en?ru as the high resource translation direction , resulting in an instance of shared target transfer .
In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al. , 2019 ) .
Iterative Back -Translation Iterative back - translation ( Hoang et al. , 2018 ) is an extension of back - translation ( Sennrich et al. , 2016 a ) .
It can exploit both sides of monolingual data of a language pair , and produces translation models for both directions , which is suitable for this shared task .
The initial models for generating synthetic parallel data are produced by using dual transfer with low resource authentic parallel data .
In each iteration of iterative back - translation , we use the latest model to greedily decode a disjoint subset of 4 m monolingual sentences 1 to generate synthetic parallel data .
Then a new model is trained on a mixture of authentic and synthetic parallel data .
With the use of dual transfer , model training can start from [ A ] PLM emb . [ A ] PLM body A and B mono . ( 1 ) [ P ] PLM emb . [ A ] PLM body P and Q mono . ( 2 ) [ A ] NMT encoder emb . [ A ] NMT encoder body [ B ]
NMT decoder emb .
A?B parallel ( 3 ) [ P ] NMT encoder emb . [ P ] NMT encoder body [ Q ]
NMT decoder emb .
P?Q parallel ( 4 ) [ B ] PLM emb . [ B ] PLM body [ Q ] PLM emb. [
Selected Finetuning Selected finetuning aims to deal with the domain difference that may exist between the test set and the training set .
Given the source side of the test set , we try to select similar source sentences from the training set , and then finetune the translation model on the selected subset of training sentence pairs .
We use BM25 ( Robertson and Zaragoza , 2009 ) to calculate the similarity between two sentences for retrieval .
The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv?ru BLEU kk 18.47 en 18.61 retrieval C is given by s ( D , Q ) = L Q i=1 IDF ( q i ) ? ( k + 1 ) ? TF ( q i , D ) k ?
1 ? b + b ? L D Lavg + TF ( q i , D ) , where the query sentence Q is a sequence of L Q subwords {q i } L Q i=1 , IDF ( q i ) is the Inverse Document Frequency for q i in the corpus C , TF ( q i , D ) is the Term Frequency for q i in the sentence D , L D is the length of the sentence D , L avg is the average length of the corpus C , k and b are hyperparameters , which are set as 1.5 and 0.75 , respectively .
Based on the BM25 score , we calculate the similarity between a source test sentence ( as the query sentence ) and the source sentences in the training set to obtain the top 500 sentences .
After performing the selection for all the source test sentences , we merge them and remove duplicates to obtain the set for finetuning .
3 Experimental Setup
Data
We collected allowed data for the involved languages and followed the same preprocessing pipeline of punctuation normalization and tokenization , using scripts from Moses 2 .
The English monolingual data came from the English original side of ru-en back - translated news 3 , but its automatic translation to Russian was discarded .
The provided Chuvash - Russian dictionary was not used .
Each language was encoded with byte pair encoding ( BPE ) ( Sennrich et al. , 2016 b ) .
The BPE codes and vocabularies were learned on each language 's monolingual data , and then used to segment parallel data .
We used 32 k merge operations for all languages .
After BPE segmentation , we discarded sentences with more than 128 subwords , and cleaned parallel data with length ratio 1.5 .
Training data statistics is provided in Table 1 . Note that we experimented with Kazakh ( kk ) data ( Section 4.1 ) , but did not use it for our final submission .
Evaluation on test sets is given by SacreBLEU 4 ( Post , 2018 ) , after BPE removal and detokenization .
Hyperparameters
We use Transformer ( Vaswani et al. , 2017 ) that follow the implementation of BERT 5 .
The absolute position embeddings are also learned as in BERT .
The encoder and decoder embeddings are independent because each language manages its own vocabulary , but we tie the decoder input and output embeddings ( Press and Wolf , 2017 ) .
We apply dropout with probability 0.1 .
We use LazyAdam as the optimizer .
Learning rate warms up for 16,000 steps and then follows inverse square root decay .
The peak learning rate is 5 ? 10 ?4 for parent translation models , and 1 ? 10 ?4 for child translation models .
Early stopping occurs when the validation BLEU does not improve for 10 checkpoints .
We set checkpoint frequency to 2,000 updates for parent translation models and 1,000 updates for child translation models .
The batch size is 6,144 tokens per GPU and 8 NVIDIA V100 GPUs are used .
Hyperparameters for BERT are the same as in the original paper ( Zhang et al. , 2021 ) .
For selected finetuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1 ? 10 ?5 .
We finetune for 10,000 updates , and save a checkpoint every 100 updates .
The checkpoint with the highest validation BLEU is kept .
Results
The Choice of Parent Language
In our preliminary experiments , we found it beneficial to use a closely related language as the parent language .
It is clear that there are several factors that should be taken into account , such as the degree of closeness , and the amount of resource for training the parent model .
For Upper Sorbian , Czech ( cs ) is closely related to it , and Czech - German has a good amount of parallel data , so we directly choose Czech as the parent language .
Chuvash , however , is a rather isolated language in the Turkic family .
The closest language with usable data is Kazakh ( kk ) , but the amount of parallel data for Kazakh - Russian is relatively small , and we found it to be quite noisy .
Therefore , we considered using English ( en ) as the parent language of Chuvash .
Even though English is unrelated to Chuvash and they use different scripts , English -Russian has more parallel data that can guarantee the quality of the parent model .
We conducted an experiment with Transformer base .
Results in Table 2 indicate that English can serve as an eligible parent for Chuvash .
Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .
The Effect of Transformer Big
The original paper ( Zhang et al. , 2021 ) evaluated dual transfer only with Transformer base .
In this shared task , we scale up to Transformer big .
We also face a more realistic setting where the monolingual data for the low resource languages ( chv and hsb ) are quite scarce .
Therefore it is worth testing the effect of scaling up .
Results in Table 3 show that Transformer big brings consistent improvements .
We also report the runtime of each step in dual transfer for NMT chv?ru with Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size .
In the following experiments and our final submission , we use Transformer big models .
Iterative Back-Translation
We ran five iterations of iterative back -translation .
Results are shown in Table 5 .
The best BLEU scores are attained with two or three iterations .
Another observation is that iterative back -translation brings larger improvements for chv?ru and hsb?de than ru?chv and de?hsb .
This is probably because the monolingual data for chv and hsb are small in quantity .
Selected Finetuning
We only use selected finetuning for the chv-ru pair because parallel data for hsb-de is scarce .
In order to test the effect of selected finetuning , we start from the models of Iteration 2 in Table 5 . Results in Table 6 indicate that selected finetuning gives modest improvements .
Ensemble
We validate the effectiveness of ensemble on hsb?de and de?hsb , by performing ensemble decoding from the five models from iterative back - translation .
Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 .
Final Submission
For chv?ru and ru?chv , we perform selected finetuning starting from the best models from iterative back - translation ( Iteration 2 for chv?ru , Iteration 3 for ru?chv ) .
Note that the selected training subsets are different from those in Section 4.4 because the selection is based on the source side of the blind test sets .
We finetune five times with different random seeds for model ensemble .
For hsb?de and de?hsb , we ensemble the five models from iterative back - translation .
Conclusion
In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation .
These experiments , as well as the good results of the final submission , show that dual transfer can work in synergy with several widely used techniques in realistic scenarios .
Table 1 : 1 Figure 1 : Dual transfer from pretrained language model and high resource A?B neural machine translation to low resource P?Q neural machine translation .
Dashed lines represent initialization .
Parameters in striped blocks are frozen in the corresponding step , while other parameters are trainable .
Different colors represent different languages .
Data used in each step is also listed .
Training data statistics .
B ] PLM body [ B ] PLM body [ B ] NMT decoder body [ Q ] NMT decoder body [ B ] NMT decoder emb . [ A ] PLM body [ A ] NMT encoder emb. language code # sentence ( pair ) cs-de 15 m hsb- de 0.1 m kk-ru 3.9 m en-ru 17 m chv-ru 0.7 m cs 90 m de 100 m hsb 0.8 m kk 17 m en 54 m ru 110 m chv 3 m the initial parameters as shown in Step ( 4 ) of Figure 1 .
This has the additional benefit of reducing training time , because convergence is faster than training from random initialization .
