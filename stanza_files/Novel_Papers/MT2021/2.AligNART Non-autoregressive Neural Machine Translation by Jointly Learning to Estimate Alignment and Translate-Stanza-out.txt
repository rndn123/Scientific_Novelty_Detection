title
AligNART : Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate
abstract
Non-autoregressive neural machine translation ( NART ) models suffer from the multi-modality problem which causes translation inconsistency such as token repetition .
Most recent approaches have attempted to solve this problem by implicitly modeling dependencies between outputs .
In this paper , we introduce AligNART , which leverages full alignment information to explicitly reduce the modality of the target distribution .
AligNART divides the machine translation task into ( i ) alignment estimation and ( ii ) translation with aligned decoder inputs , guiding the decoder to focus on simplified one - to - one translation .
To alleviate the alignment estimation problem , we further propose a novel alignment decomposition method .
Our experiments show that AligNART outperforms previous non-iterative NART models that focus on explicit modality reduction on WMT14 En?De and WMT16 Ro?En .
Furthermore , AligNART achieves BLEU scores comparable to those of the state - of- the - art connectionist temporal classification based models on WMT14 En?De .
We also observe that AligNART effectively addresses the token repetition problem even without sequence - level knowledge distillation .
Introduction
In the neural machine translation ( NMT ) domain , non-autoregressive NMT ( NART ) models ( Gu et al. , 2018 ) have been proposed to alleviate the low translation speeds of autoregressive NMT ( ART ) models .
However , these models suffer from degenerated translation quality ( Gu et al. , 2018 ; Sun et al. , 2019 ) .
To improve the translation quality of NART , several studies on NART iteratively refine decoded outputs with minimal iterations ( Ghazvininejad et al. , 2019 ; Kasai et al. , 2020a ; Guo et al. , 2020 ; Saharia et al. , 2020 ) ; other recent works target to improve NART without iteration ( Qian et al. , 2021 ; Gu and Kong , 2021 ) .
One of the significant limitations of non-iterative NART models is the multi-modality problem .
This problem originates from the fact that the models should maximize the probabilities of multiple targets without considering conditional dependencies between target tokens .
For example , in English - to - German translation , a source sentence " Thank you very much . " can be translated to " Danke sch ?n. " or " Vielen Dank . " .
Under the conditional independence assumption , the non-iterative NART models are likely to generate improper translations such as " Danke Dank . " or " Vielen sch ?n. "
( Gu et al. , 2018 ) .
For the same reason , other inconsistency problems such as token repetition or omission occur frequently in non-iterative NART ( Gu and Kong , 2021 ) .
There are two main methods for non-iterative NART to address the multi-modality problem .
Some works focus on an implicit modeling of the dependencies between the target tokens ( Gu and Kong , 2021 ) .
For example , , Saharia et al. ( 2020 ) , and Gu and Kong ( 2021 ) modify the objective function based on dynamic programming , whereas Qian et al . ( 2021 ) provide target tokens to the decoder during training .
On the other hand , other works focus on an explicit reduction of the modality of the target distribution by utilizing external source or target sentence information rather than modifying the objective function .
For example , Akoury et al . ( 2019 ) and Liu et al . ( 2021 ) use syntactic or semantic information ; Gu et al . ( 2018 ) , Zhou et al . ( 2020 b ) , and Ran et al . ( 2021 ) use the alignment information between source and target tokens .
However , previous explicit modality reduction methods show suboptimal performance .
Zhou et al . ( 2020 b ) and Ran et al . ( 2021 ) extract fertility ( Brown et al. , 1993 ) and ordering information in word alignments , which enables the modeling of several types of mappings except for many - to - one and many - to - many cases .
We hypothesize that leveraging entire mappings significantly reduces the modality and is the key to performance improvement .
In this work , we propose AligNART , a noniterative NART model that mitigates the multimodality problem by utilizing complete information in word alignments .
AligNART divides the machine translation task into ( i ) alignment estimation and ( ii ) non-autoregressive translation under the given alignments .
Modeling all the type of mapping guides ( ii ) more close to one - to - one translation .
In AligNART , a module called Aligner is simply augmented to NAT ( Gu et al. , 2018 ) which estimates alignments to generate aligned decoder inputs .
However , it is challenging to estimate the complex alignment information using only source sentence during inference .
Specifically , Aligner should simultaneously predict the number of target tokens corresponding to each source token and their mapping .
To overcome this problem , we further propose alignment decomposition which factorizes the alignment process into three subprocesses : duplication , permutation , and grouping .
Each sub-process corresponds to much feasible sub-problems : one- to - many mapping , ordering , and many - to - one mapping , respectively .
Our experimental results show that AligNART outperforms previous non-iterative NART models of explicit modality reduction on WMT14 En?De and WMT16 Ro?En .
AligNART achieves performance comparable to that of the recent stateof - the- art non-iterative NART model on WMT14 En?De .
We observe that the modality reduction in AligNART addresses the token repetition issue even without sequence - level knowledge distillation ( Kim and Rush , 2016 ) .
We also conduct quantitative and qualitative analyses on the effectiveness of alignment decomposition .
Background Given a source sentence x = {x 1 , x 2 , ... , x M } and its translation y = {y 1 , y 2 , ... , y N } , ART models with encoder-decoder architecture are trained with chained target distributions and infer the target sentence autoregressively : p( y|x ) = N n=1 p(y n |y <n , x ) .
( 1 ) At each decoding position n , the decoder of the model is conditioned with previous target tokens y <n = {y 1 , ... , y n?1 } , which is the key factor of performance in ART models .
Previous target tokens reduce the target distribution modality and provide information about the target sentence .
However , the autoregressive decoding scheme enforces the decoder to iterate N times to complete the translation and increases the translation time linearly with respect to the length of the target sentence .
Non-iterative NART models ( Gu et al. , 2018 ; Sun et al. , 2019 ; Sun and Yang , 2020 ) assume conditional independence between the target tokens to improve the translation speed : p( y|x ) = p( N | x ) ? N n=1 p(y n | x ) , ( 2 ) where N is the predicted target length to parallelize the decoding process .
Non-iterative NART models provide only the length information of the target sentence to the decoder , which is insufficient to address the multi-modality problem .
AligNART
Model Overview
Given the word alignments between the source and target sentences A ? { 0 , 1 } N ?M , we factorize the task into ( i ) alignment estimation and ( ii ) translation with aligned decoder inputs as follows : p( y|x ) = p( A| x ) ? N n=1 p(y n |x , A ) , ( 3 ) where M and N are the lengths of the source and target sentences , respectively .
Although we can also modify the negative log-likelihood loss to model dependencies between outputs such as connectionist temporal classification ( CTC ) loss ( Graves et al. , 2006 ) , we focus on the effect of the introduction of alignment as additional information .
AligNART is based on the encoder-decoder architecture , with an alignment estimation module called Aligner as depicted in Figure 1a .
The encoder maps the embedding of the source tokens into hidden representations h = {h 1 , h 2 , ... , h M }. Aligner constructs the aligned decoder inputs d = {d 1 , d 2 , ... , d N } as follows : where r n is the number of non-zero elements in the n-th row of A.
Given the aligned decoder inputs , the decoder is guided to focus on a one-toone translation from d n to y n .
One - to - one mapping significantly reduces the modality of the target distribution .
d n = 1 r n M m=1 A n , m ? h m . ( 4 The key component of AligNART , Aligner , models a conditional distribution of alignments
A given the source sentence x during training , and aligns encoder outputs using the estimated alignments during inference , as depicted in Figure 1 b .
The ground truth of the alignments is extracted using an external word alignment tool .
However , alignment estimation given only the source sentence is challenging since the alignment consists of two components related with target tokens : ?
The number of target tokens that correspond to each encoder output h m . ?
The positions of the target tokens to which h m corresponds .
The Aligner decomposes the alignment for effective estimation , which is described in Section 3.2 .
Aligner
To alleviate the alignment estimation problem , we start by factorizing the alignment process as shown in Figure 1 b .
First , we copy each encoder output h m by the number of target tokens mapped to h m , which is denoted as c m = n A n , m .
Given the duplicated encoder outputs h , we have to predict the positions of target tokens to which each element in h is mapped .
We further decompose the remaining prediction process into permutation and grouping , since noniterative NART models have no information about the target length N during inference .
In the permutation process , h is re-ordered into d such that elements corresponding to the same target token are placed adjacent to each other .
In the grouping process , each element in d is clustered into N groups by predicting whether each element is mapped to the same target token as the previous element .
r n = m A n , m denotes the number of elements in the n-th group which is equivalent to r n in Equation 4 . Finally , we can derive the decoder inputs d in Equation 4 by averaging the elements in each group in d .
In summary , we decompose the alignment estimation task into three sequential sub-tasks : duplication , permutation , and grouping .
Alignment Decomposition
As shown in Figure 1 b , we factorize the alignment matrix
A into duplication , permutation , and grouping matrices that correspond to each process .
h = {h 1,1 , ... , h 1 , c 1 , ... , h M,1 , ... , d M , c M } denotes the duplicated encoder outputs where h i , j is the j-th copied element of h i .
Similarly , d = {d 1,1 , ... , d 1, r 1 , ... , d N,1 , ... , d N , r N } denotes the permuted encoder outputs where d i , j is the jth element in the i-th group .
The number of nonzero elements in the alignment matrix is defined as L = m c m = n r n .
Duplication Matrix Aligner copies h m by c m to construct the duplicated encoder outputs h with a duplication matrix D ? { 0 , 1 } L?M . Let C m = m i=1 c i and C 0 = 0 .
Then , we can define D using c m as follows : D l , m = 1 if C m?1 < l ?
C m 0 else .
( 5 ) We index h by the following rule : ?
For any h m , i and h m , j ( i < j ) , which are matched to d x i ,y i and d x j , y j , respectively , x i ?
x j and y i ? y j .
The duplication matrix D contains similar information to fertility ( Gu et al. , 2018 ) . Permutation Matrix Aligner re-orders h to construct d with a permutation matrix P ? { 0 , 1 } L?L .
Since all the indexed elements in h and d are distinct , the permutation matrix P is uniquely defined .
Grouping Matrix
Aligner finally aggregates d to construct d , the aligned decoder inputs , with a grouping matrix G ? { 0 , 1 } N ?L .
Let R n = n i=1 r i and R 0 = 0 .
Then , G can be defined using r n as follows : G n,l = 1 if R n?1 < l ?
R n 0 else .
( 6 ) We index d by the following rule : ?
For any d n, i and d n , j ( i < j ) , which are matched to h x i ,y i and h x j , y j , respectively , x i ?
x j and y i ? y j .
We can derive the aligned decoder inputs by separately estimating the decomposed matrices D , P , and G , which approximately correspond to one-tomany mapping , ordering , and many - to - one mapping , respectively .
The decomposed matrices have an easily predictable form while recovering the complete alignment matrix .
Training Aligner consists of three prediction sub-modules : duplication , permutation , and grouping predictors .
Each of them estimates the decomposed alignment matrix as follows : p( A| x ) = p( G|x , P , D ) ?p( P |x , D ) ?p( D| x ) .
( 7 )
The duplication predictor learns to classify the number of copies of h m .
The duplication loss is defined as follows : L D = ?
1 M M m=1 log p m ( c m ) , ( 8 ) where p m is the predicted probability distribution of the duplication at the position m .
To discriminate copied elements in h , we add copy position embedding to {h m ,1 , ... , h m, cm } for the next two predictors .
The permutation predictor takes the duplicated encoder outputs h as inputs .
We simplify the permutation prediction problem into a classification of the re-ordered position .
For the permutation loss , we minimize the KL - divergence between the prediction P pred and the ground truth P GT . L P = ?
1 L i j P GT i , j log P pred i , j . ( 9 ) Given the permuted encoder outputs , the grouping predictor conducts a binary classification task of whether d l is assigned to the same group as d l?1 .
Let the label at the position l be g l .
Then , we define g l from G as follows : g l = 1 if G * , l = G * , l?1 and l > 1 0 else .
( 10 ) The grouping loss is defined as follows : L G = ?
1 L L l=1 log p l ( g l ) , ( 11 ) where p l is the predicted probability distribution of the grouping predictor at position l .
Our final loss function is defined as the sum of the negative log-likelihood based translation loss L T and alignment loss L A : L = L T +L A = L T +?L D +?L P +?L G , ( 12 ) where we set ? = ? = ? = 0.5 for all the experiments .
Inference During inference , Aligner sequentially predicts the duplication , permutation , and grouping matrices to compute the aligned decoder inputs d as depicted in Figure 1 b .
The duplication predictor in Aligner infers ?m at each position m ; then , we can directly construct a duplication matrix D using Equation 5 .
The permutation predictor predicts the distribution of the target position P pred .
We obtain a permutation matrix P that minimizes the KL - divergence as follows : P = arg min P ( ?
i j P i , j log P pred i , j ) . ( 13 ) We utilize the linear sum assignment problem solver provided by Jones et al . ( 2001 ) to find P .
The grouping predictor infers the binary predictions ?l from the permuted encoder outputs .
We construct a grouping matrix ? using ?l and Equations 6 and 10 .
With a predicted alignment matrix ? = ? ? P ? D , Aligner constructs the decoder inputs using Equation 4 , and the decoder performs translation from the aligned inputs .
Decoding Strategies
For the re-scoring based decoding method , we select candidates of alignments using the predicted distributions in the duplication and grouping predictors .
We identify m positions in the outputs of the duplication predictor , where the probability of the predicted class is low .
We then construct a 2 mcandidate pool where the predictions in part of the m positions are replaced with the second probable class .
Next , we identify the top -a candidates with the highest joint probabilities .
Similarly , we construct a 2 l - candidate pool and identify b candidates in the grouping predictor for the a candidates .
Finally , we rank a ?
b translations for the alignments candidates using a teacher ART model and select the best translation among them .
Architecture of AligNART
We use the deep-shallow ( 12 - 1 for short ) Transformer ( Vaswani et al. , 2017 ) architecture ( i.e. , 12layer encoder and 1 - layer decoder ) proposed by Kasai et al . ( 2020 b ) for two reasons .
First , a deeper encoder assists Aligner to increase the estimation accuracy of the alignment matrix during inference .
Second , the deep-shallow architecture improves the inference speed since the encoder layer has no cross-attention module compared to the decoder layer .
The architecture of the duplication , permutation , and grouping predictor is shown in the Appendix .
Alignment Score Filtering Some alignment tools such as GIZA ++ ( Och and Ney , 2003 ) provide an alignment score for each sentence pair as a default .
Samples with low alignment scores are more likely to contain noise caused by sentence pairs or alignment tools .
For GIZA + + , we filter out a fixed portion of samples with low alignment scores to ease the alignment estimation .
Since the pair of long sentences tends to be aligned with a low score , we apply the same filtering portion for each target sentence length .
Experimental Setups
Datasets and Preprocessing
We evaluate our method on two translation datasets : WMT14 English - German ( En- De ) and WMT16 English -Romanian ( En - Ro ) .
WMT14 En-De / WMT16
En - Ro datasets contain 4.5M /610 K training pairs , respectively .
For WMT14 En- De dataset , we use preprocessing pipelines provided by fairseq 1 ( Ott et al. , 2019 ) .
For WMT16 En - Ro dataset , we use the preprocessed corpus provided by Lee et al . ( 2018 ) .
Preprocessed datasets share a vocabulary dictionary between the source and target languages .
We use fast align ( FA ) ( Dyer et al. , 2013 ) and GIZA ++ ( GZ ) , which is known to be more accurate than fast align , as word alignment tools .
All the corpus are passed to the alignment tools at the subword-level .
We filter out samples where the maximum number of duplications exceed 16 .
We explain the details of the alignment processing in the Appendix .
We use the sequence - level knowledge distillation method ( KD ) for the distillation set .
Transformer ART models are trained to generate the distillation set for each translation direction .
Models and Baselines
We compare our model with several non-iterative NART baselines , and divide the non-iterative NART models into two types as aforementioned : implicit dependency modeling and explicit modality reduction ( see Table 1 ) .
We also train the ART models and deep-shallow NAT for the analysis .
Our models are implemented based on fairseq .
23.5 27.9 --30.8 31.5 --NAT -EM ( Sun and Yang , 2020 ) 24.5 27.9 24 ?16.4 ----NARLVM
25 AligNART is implemented based on the deep-shallow Transformer architecture .
We set d model /d hidden to 512/2048 and the dropout rate to 0.3 .
The number of heads in multi-head attention modules is 8 except for the last attention module of the permutation predictor which is 1 .
We set the batch size to approximately 64 K tokens for all the models we implement .
All these models we implement are trained for 300K / 50 K steps on En-De / En- Ro datasets , respectively .
For AligNART , we average 5 checkpoints with the highest validation BLEU scores in the 20 latest checkpoints .
For optimization , we use Adam optimizer ( Kingma and Ba , 2015 ) with ? = ( 0.9 , 0.98 ) and = 10 ?8 .
The learning rate scheduling follows that of Vaswani et al . ( 2017 ) , starting from 10 ?7 and warms up to 5e - 4 in 10 K steps .
We use the label smoothing technique with ls = 0.1 for the target token distribution and each row of permutation matrix .
The translation latency is measured on an NVIDIA Tesla V100 GPU .
which can be explored in the future work .
Results
Main Results
Table 2 shows the BLEU scores with re-scoring decoding strategies of the non-iterative NART models .
We set m = l = 4 , a = 4 , and b = 2 for 8 candidates .
AligNART outperforms the baselines on En?De and Ro?En , and shows performance similar to that of GLAT on De?En .
In non-iterative NART for explicit modality reduction , AligNART shows the best performance on En?De and Ro?En .
Analysis of Aligner Components
In this section , we investigate the accuracy , example , and ablation results of Aligner components as shown in Table 3 , 4 , and 5 , respectively .
Note that we partially provide the ground truth D or P matrices during the accuracy measurement .
Knowledge Distillation In Table 3 , a comparison of accuracy between raw and distilled datasets shows that KD significantly decreases multi-modality of each component .
After KD , Alig-NART shows marginally reduced accuracy on the raw dataset , but high prediction accuracy in each component on the distillation set , resulting in increased BLEU scores .
Alignment Tool Before KD , AligNART using fast align and GIZA ++ have accuracy bottlenecks in permutation and duplication predictors , respectively , as shown in Table 3 .
The results imply that the alignment tools have different degrees of multimodality on the D , P , and G matrices , which can be explored in the future work .
Qualitative Study
Table 4 shows an example of addressing the multi-modality problem .
Deepshallow NAT monotonically copies the encoder outputs and suffers from repetition and omission problems .
AligNART ( FA ) does not show the inconsistency problems thanks to the well - aligned decoder inputs , which significantly reduces the modality of the target distribution .
We also conducted a case study on predicted alignments and their translations during re-scoring as shown in the Appendix .
Ablation Study
We conduct an analysis of alignment estimation by ablating one of the predictors during inference .
We ablate each module in Aligner by replacing the predicted matrix with an identical matrix I .
The results in Table 5 indicate that each module in Aligner properly estimates the decomposed information in word alignments .
However , there is an exception in GIZA ++ where many - toone mapping does not exist , resulting in performance equal to that without the grouping predictor .
We observe that AligNART achieves BLEU scores comparable to those of CTC - based models on En? De even with the ground truth word alignments of partial information .
Analysis of Modality Reduction Effects
To evaluate the modality reduction effects of Alig- NART , we conducted experiments on two aspects : BLEU score and token repetition ratio .
Table 6 shows the BLEU scores on WMT14 En-De. For En?De , AligNART using fast align without KD higher BLEU scores than previous models without KD and deep-shallow NAT with KD .
The results indicate that our method is effective even without KD , which is known to decrease data complexity ( Zhou et al. , 2020a ) .
On the other hand , alignments from GIZA ++ without KD are more complex for AligNART to learn , resulting in lower BLEU scores than deep-shallow NAT with KD .
measured the token repetition ratio as a proxy for measuring multimodality .
The token repetition ratio represents the degree of the inconsistency problem .
In Table 7 , the token repetition ratio of AligNART is less than that of the CMLM - base ( Ghazvininejad et al. , 2019 ) of 5 iterations , AXE , and GLAT .
We also observe that the decline in the token repetition ratio from Aligner is significantly larger than that from KD .
Combined with the results from information adequately alleviates the token repetition issue even in the case where the BLEU score is lower than that of deep-shallow NAT with KD .
Ablation Study
We conduct several extensive experiments to analyze our method further as shown in Table 8 and 9 . Each of our method consistently improves the performance of AligNART .
Cross Attention
As shown in Table 8 , we ablate the cross attention module in the decoder to observe the relationship between aligned decoder inputs and alignment learning of the cross attention module .
We train AligNART and deep-shallow NAT without a cross attention module for comparison .
AligNART without the cross attention module has a smaller impact on the BLEU score than the deep-shallow NAT .
The cross attention module is known to learn alignments between source and target tokens ( Bahdanau et al. , 2015 ) , and the result implies that aligned decoder inputs significantly offload the role of the cross attention module .
Deep NART as shown in Table 8 .
The results indicate that the deep encoder assists alignment estimation , whereas the shallow decoder with aligned inputs has a lower impact on performance degeneration .
Alignment Score Filtering
We investigate the trade- off between the alignment score filtering ratio and BLEU score using AligNART ( GZ ) presented in Table 9 .
Samples with low alignment scores are more likely to contain noise caused by distilled targets or an alignment tool .
We observe that filtering out of 5 % of the samples improves the BLEU score in both the directions .
Surprisingly , increasing the filtering ratio up to 20 % preserves the performance thanks to the noise filtering capability .
6 Related Work 6.1 Non-iterative NART After Gu et al. ( 2018 ) proposed NAT , non-iterative NART has been investigated in various directions to maximize translation speed while maintaining translation quality .
Shao et al. ( 2019 ) , Shao et al . ( 2020 ) , and address the limitations of conventional cross entropy based objectives that overly penalize consistent predictions .
Lee et al. ( 2018 ) , Ma et al. ( 2019 ) , , and introduce latent variables to model the complex dependencies between target tokens .
Saharia et al. ( 2020 ) and Gu and Kong ( 2021 ) apply CTC loss to the NMT domain .
Qian et al. ( 2021 ) provide target tokens to the decoder during training using the glancing sampling technique .
Alignment in Parallel Generative Models
In other domains , such as text- to-speech ( Ren et al. , 2019 ; Kim et al. , 2020 ; Donahue et al. , 2020 ) , a common assumption is a monotonicity in the alignments between text and speech .
Given this assumption , only a duration predictor is required to alleviate the length - mismatch problem between text and speech .
On the other hand , modeling the alignment in the NMT domain is challenging since the alignment contains additional ordering and grouping information .
Our method estimates an arbitrary alignment matrix using alignment decomposition .
Improving NMT with Enhanced Information
To alleviate the multi-modality problem of NART models , Gu et al .
( 2018 )
Alignment is considered as a major factor in machine translation ( Li et al. , 2007 ; Zhang et al. , 2017 ) . Alkhouli et al. ( 2018 ) decompose the ART model into alignment and lexical models .
Song et al. ( 2020 ) use the predicted alignment in ART models to constrain vocabulary candidates during decoding .
However , the alignment estimation in NART is much challenging since the information of decoding outputs is limited .
In NART , Gu et al. ( 2018 ) , Zhou et al . ( 2020 b ) , and Ran et al . ( 2021 ) exploit partial information from the ground truth alignments .
In contrast , we propose the alignment decomposition method for effective alignment estimation in NART where we leverage the complete alignment information .
Conclusion and Future Work
In this study , we leverage full alignment information to directly reduce the degree of the multimodality in non-iterative NART and propose an alignment decomposition method for alignment estimation .
AligNART with GIZA ++ shows performance comparable to that of the recent CTCbased implicit dependency modeling approach on WMT14 En- De and modality reduction capability .
However , we observe that AligNART depends on the quality of the ground truth word alignments , which can be studied in the future work .
Furthermore , we can study on the combination of Alig- NART and implicit dependency modeling methods .
Row and column correspond to input and output tokens , respectively .
y i denotes the i-th subword of the target sentence .
x i denotes the i-th word of the source sentence and x i j denotes the j-th subword of the i-th word of the source sentence .
we derive the source subword to target subword matrix A using the alignment tool .
A ws is achieved by clipping the maximum value of A?S to 1 .
A ws reduces the search space because of the assumption that source tokens duplicate , permute , and group at the word-level .
However , there is a trade- off between the simplicity and resolution of information .
The recovered source subword to target subword matrix A ws ?
S loses the subword - level information as shown in the rightmost matrix in Figure 3 .
