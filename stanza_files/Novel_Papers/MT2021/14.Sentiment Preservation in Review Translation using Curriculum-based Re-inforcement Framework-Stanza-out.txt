title
Sentiment Preservation in Review Translation using Curriculum - based Re-inforcement Framework
abstract
Machine Translation ( MT ) is a common approach to feed humans or machines in a cross-lingual context .
However , there are some expected drawbacks .
Studies suggest that in the crosslingual context , MT system often fails to preserve different stylistic and pragmatic properties of the source text ( e.g. sentiment , emotion , gender traits , etc. ) to the target translation .
These disadvantages can degrade the performance of any downstream Natural Language Processing ( NLP ) applications , such as sentiment analyser , that heavily relies on the output of the MT systems ( especially in a low-resource setting ) .
The susceptibility to sentiment polarity loss becomes even more severe when an MT system is employed for translating a source content that lacks a legitimate language structure ( e.g. review text ) .
Therefore , while improving the general quality of the Neural Machine Translation ( NMT ) output ( e.g. adequacy ) , we must also find ways to minimize the sentiment loss in translation .
In our current work , we present a deep re-inforcement learning ( RL ) framework in conjunction with the curriculum learning to fine - tune the parameters of a full-fledged NMT system so that the generated translation successfully encodes the underlying sentiment of the source without compromising the adequacy , unlike the previous method .
We evaluate our proposed method on the English -Hindi ( product domain ) and French - English ( restaurant domain ) review datasets , and found that our method ( further ) brings a significant improvement over a full-fledged supervised baseline for the machine translation and sentiment classification tasks .
Introduction Product and / or service reviews available in the e-commerce portals are predominantly in the English language , and hence a large number of population can not understand these .
Machine Translation ( MT ) system can play a crucial role in bridging this gap by translating the usergenerated contents , and directly displaying them , or making these available for the downstream Natural Language Processing ( NLP ) tasks e.g. sentiment classification 1 ( Ara? jo et al. , 2020 ; Barnes et al. , 2016 ; Mohammad et al. , 2016 ; Kanayama et al. , 2004 ) .
However , numerous studies ( Poncelas et al. , 2020a ; Afli et al. , 2017 ; Mohammad et al. , 2016 ; Sennrich et al. , 2016a ) have found a significant loss of sentiment during the automatic translation of the source text .
The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin .
For example , a noisy review contains several peculiarities and informal structures , such as shortening of words ( e.g. " awesome " as " awsm " ) , acronyms ( e.g. " Oh My God " as " OMG " ) , phonetic substitution of numbers ( e.g. " before " as " b4 " ) , emphasis on characters to define extremity of the emotion ( e.g. " good " as " gooooooood " ) , spelling mistakes , etc .
Unfortunately , even a pervasively used commercial neural machine translation ( NMT ) system , Google Translate , is very brittle and easily falters when presented with such noisy text , as illustrated through the following example .
Review Text ( English ) : I found an awsome product .
( Positive ) Google Transliteration ( Hindi ) : mujhe ek ajeeb utpaad mila .
( Neutral )
The example shows how the misspelling of a sentiment bearing word " awesome " gets this positive expression translated to a neutral expression .
In the above context , if an unedited raw MT output is directly fed to the downstream sentiment classifier , it might not get the expected classification accuracy .
Thus , in this work we propose a deep-reinforcement - based framework to adapt the parameters of a pre-trained neural MT system such that the generated translation improves the performance of a cross-lingual multi-class sentiment classifier ( without compromising the adequacy ) .
More specifically , we propose a deep actor-critic ( AC ) reinforcement learning framework in the ambit of curriculum learning ( CL ) to alleviate the issue of sentiment loss while improving the quality of translation in a cross-lingual setup .
The idea of actor-critic is to have two neural networks , viz . ( i ) . an actor ( i.e. a pre-trained NMT ) that takes an action ( policy - based ) , and ( ii ) .
a critic that observes how good the action taken is and provides feedback ( value-based ) .
This feedback acts as a guiding signal to train the actor .
Further , to better utilize the data , we also integrate curriculum learning into our framework .
Recently , Tebbifakhr et al. ( 2019 ) demonstrated that an MT system ( actor ) can be customised to produce a controlled translation that essentially improves the performance of a cross-lingual ( binary ) sentiment classifier .
They achieved this task -specific customisation of a " generic - MT " system via a policy - based method that optimizes a task-specific metric , i.e . F 1 score ( see Section 2 ) .
However , this often miserably fails to encode the semantics of the source sentence .
Recent studies ( Xu et al. , 2018 ) demonstrated that the non-opinionated semantic content improves the quality of a sentiment classifier .
Accordingly , the transfer of such information from the source to the target can be pivotal for the quality of the sentiment classifier in a cross-lingual setup .
Towards this , we investigate the optimization of a harmonic-score- based reward function in our proposed RL - based framework that ensures to preserve both sentiment and semantics .
This function operates by taking a weighted harmonic mean of two rewards : ( i ) . content preservation score measured through Sentence- level BLEU or SBLEU ; and ( ii ) .
sentiment preservation score measured through a function that performs element - wise dot product between a predicted sentiment distribution and the gold sentiment distribution .
Empirical results , unlike Tebbifakhr et al . ( 2019 ) , suggest that our RL - based fine-tuning framework , tailored to optimize the harmonic reward , preserves both sentiment and semantics in a given NMT context .
Additionally , we also found that the above fine-tuning method in the ambit of curriculum learning achieves an additional performance gain of the MT system over a setting where curriculum based fine - tuning is not employed .
The core of curriculum learning ( CL ) ( Bengio et al. , 2009 ) is to design a metric that scores the difficulty of training samples , which is then used to guide the order of presentation of samples to the learner ( NMT ) in an easy - to - hard fashion .
To the best of our knowledge , this is the very first work that studies the curriculum learning ( CL ) for NMT from a new perspective , i.e. given a pre-trained MT model , the dataset to fine-tune , and the two tasks viz .
sentiment and content preservation ; we utilize a reward - based metric ( i.e. harmonic score ) to define the difficulty of the tasks and use it to score the data points .
The use of harmonic reward based scoring / ranking function implicitly covers the tasks ' overall difficulty through a single metric .
Moreover , understanding that obtaining a gold-standard polarity annotated data is costlier , the fine-tuning of pre-trained NMT model is performed by re-using only a small subset of the supervised training samples that we annotated with respect to ( w.r.t ) their sentiment .
Empirical results suggest that additionally enriching a random small subset of the training data with extra sentiment information , and later re-using them for the fine-tuning of the referenced model via our proposed framework ( c.f. Section 3 ) observes an additional gain in BLEU and F 1 score over a supervised baseline .
We summarize the main contributions and / or the key attributes of our current work as follows : ( i ) .
create a new domain-specific ( i.e. product review ) parallel corpus , a subset of which is annotated for their sentiment ; ( ii ) .
propose an AC - based fine-tuning framework that utilizes a novel harmonic mean- based reward function to meet our two -fold objectives , viz .
enabling our NMT model to preserve ; ( a ) .
the non-opinionated semantic content ; and ( b ) .
the source sentiment during translation .
( iii ) .
Additionally , we utilize the idea of CL during the RL fine-tuning of the pre-trained model and try to learn from easy to hard data , where hard corresponds to the instances with lower harmonic reward .
To the best of our knowledge , this is the first work in NMT that studies CL in the ambit of RL fine-tuning .
Related Work
The use of translation - based solution for cross-lingual sentiment classification is successfully leveraged in the literature ( Wu et al. , 2021 ; Tebbifakhr et al. , 2020 ; Ara?jo et al. , 2020 ; Poncelas et al. , 2020 b ; Fei and Li , 2020 ; Tebbifakhr et al. , 2019 ; Akhtar et al. , 2018 ; Barnes et al. , 2016 ; Balahur and Turchi , 2012 ; Kanayama et al. , 2004 ) which suggest an inspiring use-case of the MT system , and brings motivation for this piece of work .
Given the context of this work , we look at the pieces of works that address the preservation of sentiment in the automatic translation .
In one of the early works , Chen and Zhu ( 2014 ) used a lexicon - based consistency approach to design a list of sentiment - based features and used it to rank the candidates of t-table in a Phrase based MT system .
Lohar et al . ( 2017
Lohar et al. ( , 2018 prepared the positive , negative and neutral sentiment -specific translation systems to ensure the cross-lingual sentiment consistency .
Recently , Tebbifakhr et al. ( 2019 ) proposed Machine -Oriented ( MO ) Reinforce , a policybased method to pursue a machine -oriented objective 2 in a sentiment classification task unlike the traditional human-oriented objective 3 .
It gives a new perspective for a use-case of the MT system ( i.e. machine translation for machine ) .
To perform this task-specific adaption ( i.e. produce output to feed a machine ) , Tebbifakhr et al . ( 2019 ) adapted the REINFORCE of Williams ( 1992 ) by incorporating an exploration -oriented sampling strategy .
As opposed to one sampling of REINFORCE , MO Reinforce samples k times , ( k = 5 ) , and obtains a reward for each sample from the sentiment classifier .
A final update of the model parameters are done w.r.t the highest rewarding sample .
Although they achieved a performance boost in the sentiment classification task , they had to greatly compromise with the translation quality .
In contrast to Tebbifakhr et al . ( 2019 ) , we focus on performing a task -specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL .
The adapted NMT system , thus obtained , is expected to produce a more accurate ( high-quality ) translation as well as improve the performance of a sentiment analyser .
Bahdanau et al . ( 2017 ) ; Nguyen et al. ( 2017 ) , unlike us , used the popular AC method , and focused only on preserving the semantics ( translation quality ) of a text .
Additionally , we develop a CL based strategy to guide the training .
Recently , Zhao et al. ( 2020 ) also studied AC method in the context of NMT .
However , they used this method to learn the curriculum for re-selecting influential data samples from the existing training set that can further improve the performance ( translation quality ) of a pre-trained NMT system .
Methodology Firstly , we perform the pre-training of a NMT model until the convergence using the standard log-likelihood ( LL ) training on the supervised dataset ( c.f. Table 1 : ( A ) ) .
The model , thus obtained , acts as our referenced MT system / actor .
To demonstrate the improvements brought by the proposed curriculum - based AC fine-tuning over the above LL - based baseline in the sentiment preservation and machine translation tasks , we carry out the task -specific adaption of the pre-trained LL - based MT model ( actor ) by re-using a subset of the supervised training samples .
It is worth mentioning here that , in the fine-tuning stage , the actor does not observe any new sentence , rather re-visit ( randomly ) a few of the supervised training samples which are now additionally annotated with their sentiment ( c.f. Section 4 ) .
Actor-critic Overview :
Here , we present a brief overview of our AC framework which is discussed at length in the subsequent section .
In the AC training , the actor ( NMT ) receives an input sequence , s , and produces a sample translation , t , which is evaluated by the critic model .
The critic feedback is used by the actor to identify those actions that bring it a better than the average reward .
In the above context , a feedback of a random critic would be useless for training the actor .
Hence , similar to the actor we warm up the critic for one epoch by feeding it samples from the pre-trained actor , while the actor 's parameters are frozen .
We then fine- tune these models jointly so that - as the actor gets better w.r.t its action , the critic gets better at giving feedback ( see Section 4.2 for the dataset and reward used in the pre-training and fine- tuning stages ) .
The details of the loss functions that the actor and critic minimizes are discussed in Section 3.1 .
Furthermore , to better utilize the data , we finally integrate CL into our AC framework ( our proposed approach ) .
Empirical results ( Section 5.1 ) show that during fine-tuning , presenting the data in an easy - to - hard fashion yields a better learned actor model over the one obtained via vanilla ( no-curriculum based ) fine-tuning .
Our proposed framework brought improvements over several baselines without using any additional new training data in the two translation tasks , i.e. ( i ) .
English -Hindi 4 and ( ii ) .
French-English 5 . Since our proposed framework is a combination of RL via AC method and CL , we first present the details of the main components of the AC model alongside their training procedure in Section 3.1 .
The details of the reward model are presented in Section 3.2 , and then introduce the plausibility of CL in Section 3.3 .
Finally , we describe our proposed CL - based AC framework in Algorithm 1 .
Proposed Fine-tuning Method
The architecture of our AC - based framework is illustrated in Figure 1 .
It has three main components viz . ( i ) .
an actor : the pre-trained neural agent ( NMT ) whose parameters define the policy and the agent takes action , i.e. sample translations according to the policy ( ii ) .
a reward model : a score function used to evaluate the policy .
It provides the actual ( true ) estimated reward to the translations sampled from the model 's policy .
To ensure the preservation of sentiment and content in translation , the chosen reward model gives two constituent rewards - a classifier - based score and a SBLEU score ( Section 3.2 ) , respectively , and ( iii ) .
a critic : a deep neural function approximator that predicts an expected value ( reward ) for the sampled action .
This is then used to center the true reward ( step ( ii ) ) from the environment ( see Equation 2 ) .
Subtracting critic estimated reward from the true reward helps the actor to identify action that yields extra reward beyond the expected return .
We employ a critic with the same architecture as of the actor .
We see from the lower - left side of Figure 1 that , for each input sentence ( s ) , we draw a single sample ( t ) from the actor , which is used for both estimating gradients of the actor and the critic model as explained in the subsequent section .
Critic Network training :
During the RL training , we feed a batch of source sentences , B j ( s ) , to the critic encoder and the corresponding sampled translations obtained from the actor , B j t , to the decoder of the critic model .
The critic decoder then predicts the rewards ( i.e. value estimates , V ? , predicted for each time step of the decoder ) , and accordingly updates its parameters supervised by the actual ( or true ) rewards , R ( t , s ) 6 ( steps to obtain this reward is discussed in Section 3.2 ) from the environment .
The objective of the critic network is , thus , to find its parameter value ? that minimizes the mean square error ( MSE ) between the true reward ( see R in Figure 1 ) from the environment , and the critic estimated reward ( i.e. values predicted by the critic , see V ? in Figure 1 ) .
Accordingly , the MSE loss that the critic minimizes is as in Equation ( 1 ) , where ? being the critic decoding step .
? ? L crt ( ? ) ? n ? =1 V ( t<? , s ) ? R ( t , s ) ? ? V ( 1 ) Note that in this work we explore the setting , where the reward , R , is observable only at time step ?
= n of the actor ( a scalar for each complete sentence ) .
Thus , to calculate the difference terms in Equation 1 for n steps , we use the same terminal reward , R , in all the intermediate time steps of the critic decoder .
Actor Network training :
To update the actor ( G ) parameters , ? , we use the policy gradient loss ; weighted by a reward which is centered via the critic estimated value ( i.e. the critic estimated value , V , is subtracted from the true reward , R , from the environment ) , as in equation ( 2 ) .
The updated reward is finally used to weigh the policy gradient loss , as shown in ( 3 ) , where ? being the decoding step of the actor .
R? ( t , s ) = R ( t , s ) ? V ( t< ? , s ) ( 2 ) ? ? L pg actor ( ? ) ? n ? =1 R ? ? ? log G ? ( t? | t< ? ) ( 3 )
The actor and the critic both are global -attention based recurrent neural networks ( RNN ) .
Algorithm 1 summarizes the overall update framework .
We run this algorithm for mini-batches .
Defining Rewards
As our primary goal is to optimize the performance of the pre-trained NMT system towards sentiment classification and machine translation tasks , accordingly we investigate the utility of the following three reward functions ( i.e. true reward , R in Equation 1 as R 1 , R 2 , R 3 ) for optimization through our vanilla AC method .
Please note , for brevity we only choose the reward that serves the best to our purpose ( i.e. harmonic reward as it ensures both , an improved cross-lingual sentiment projection , and a high quality translation with our vanilla AC approach , as discussed in Section 5.1 ) for our subsequently proposed curriculum - based experiment .
The three types of feedbacks we explored are : ( i ) .
Sentence - level BLEU as a reward to ensure the content preservation , also referred as R 1 , is calculated following the Equation ( 4 ) R 1 = SBLEU ( t , t ) ( 4 ) ( ii ) .
Element - wise dot product between the gold sentiment distribution and predicted sentiment distribution ( e.g. [ 1 , 0 , 0 ] and [ 0.2 , 0.1 , 0.7 ] in Figure 1 evaluates to scalar value 0.2 ) taken from the softmax layer of the target language classifier to ensure sentiment preservation , also referred as R 2 .
To simulate the target language classifier , we fine - tune the pre-trained BERT model ( Devlin et al. , 2019 ) .
The tuned classifier ( preparation steps discussed in Section 4.1 ) is used to obtain the reward R 2 as in Equation ( 5 ) .
R 2 = P( s ) gold ? P ( t ) bert ( 5 ) and , ( iii ) .
Harmonic mean of ( i ) and ( ii ) as a reward , also referred to as R 3 to ensure the preservation of both sentiment and semantic during the translation , as in Equation ( 6 ) .
R 3 = ( 1 + ? 2 ) ( 2 ? R 1 ? R 2 ) ( ? 2 ? R 1 ) + R 2 ( 6 ) where ? is the harmonic weight which is set to 0.5 .
Curriculum Construction
The core of CL is ( i ) .
to design an evaluation metric for difficulty , and ( ii ) .
to provide the model with easy samples first before the hard ones .
In this work , the notion of difficulty is derived from the harmonic reward , R 3 , as follows .
Let , X = {x i } N i=1 = ( s i , t i ) N i=1 denotes the RL training data points .
To measure the difficulty of say , i th data point , ( s i , t i ) , we calculate the reward , R 3 using ( ti , s i ) .
In order to obtain the corresponding sample translation , ti , we use the LL - based model ( pre-trained actor ) .
We do this for the N data points .
Finally , we sort the RL training data points from easy , i.e. , with high harmonic reward , to hard as recorded on their translations .
In
Datasets and Experimental Setup
In this section , we first discuss the datasets used in different stages of experiments followed by the steps involved in the creation of datasets , the baselines used for comparison , and the model implementation details .
Dataset : Our NMT adaptation experiments are performed across two language pairs from different families with different typological properties , i.e. English to Hindi ( henceforth , En-Hi ) and French - English ( henceforth , Fr-En ) .
We use the following supervised datasets for the pre-training and validation of LL - based NMT in En-Hi and Fr- En tasks , ( i ) .
For En-Hi task , we use a newly created domain-specific parallel corpus ( see section 4.1 ) whose sources were selected from an e-commerce site .
This corpus is released as a part of this work .
Statistics of the dataset is shown in Table 1 : ( A ) , row ( ii ) . ( ii ) .
For Fr- En task , we concatenate a recently released domain-specific parallel corpus , namely Foursquare ( 4SQ ) corpus 7 ( Berard et al. , 2019 ) with first 60 K sentences from OpenSubtitles 8 corpus to simulate a low-resource setting .
The basic statistics of this dataset are shown in Table 1 : ( A ) , row ( i ) .
For RL fine-tuning of the LL-based NMT ( s ) , we use the corresponding RL datasets from Table 1 : ( B ) .
In each task , the RL trainset sentences are a subset of human translated sentences drawn from the supervised training samples and additionally annotated with respect to sentiment .
For En-Hi task , these sentences are randomly sampled from the supervised training corpus ( c.f. Table 1 : ( A ) , row ( ii ) ) , and for Fr- En we use 4SQ - HT dataset ( c.f. Table 1 : Algorithm 1 Proposed algorithm ( Curriculum based fine- tuning process ) .
In the vanilla ( i.e. no curriculum - based ) approach , we skip steps 5 to 7 .
Obtain the sample translations B m ( t ) from the actor for the given source sentences , Bm ( s ) .
11 : Obtain R1 , R2 and finally observe the rewards R3 .
12 : Feed the source sentences , Bm (s ) to the critic encoder and sampled translations , B m ( t ) to the decoder .
13 : Obtain the predicted rewards , V ? , using the critic model .
14 : Update the critic 's parameter using ( 1 ) .
15 : Obtain final reward R using ( 2 ) .
16 : Update the actor 's parameter using ( 2 ) in ( 3 ) .
17 : end for 18 : end for ( A ) , row ( i ) ) .
To evaluate the performance of all the NMT system ( s ) we use the corresponding RL testsets from Table 1 .
Data Creation
To the best of our knowledge , there is no existing ( freely available ) sentiment annotated parallel data for English -Hindi in the review domain .
Hence , we crawl the electronic products reviews from a popular e-commerce portal ( i.e. Flipkart ) .
These reviews were translated into Hindi using the Google Translate API .
A significant part of this automated translation is then verified by a human translator with a post-graduate qualification and proficient in English and Hindi language skills .
One more translator was asked to verify the translation .
The detailed statistics of new in - domain parallel corpus are shown in Table 1 : ( A ) , row ( ii ) .
Further , a subset of human translated product reviews is selected randomly for sentiment annotation .
Three annotators who are bilingual and experts in both Hindi and English took part in the annotation task .
Details of the instructions given to the annotators and translators are mentioned below .
Instructions to the Translators :
For translation , following were the instructions : ( i ) .
experts were asked to read the Hindi sentence carefully ; ( ii ) .
source and target sentences should carry the same semantic and syntactic structure ; ( iii ) .
they were instructed to carefully read the translated sentences , and see whether the fluency ( grammatical correctness ) and adequacy are preserved ; ( iv ) .
they made the correction in the sentences , if required ; ( v ) . vocabulary selection at Hindi ( target ) side should be user friendly ; ( vi ) . transliteration of an English can also be used , especially if this is a named entity ( NE ) .
Instructions to the Annotators :
The annotators have post-graduate qualification in linguistics , possessing good knowledge of English and Hindi both .
They have prior experience in judging the quality of machine translation and sentiment annotation .
For sentiment labeling , annotators were asked to follow the guidelines as below : ( i ) .
they were instructed to look into the sentiment class of the source sentence ( Tebbifakhr et al. , 2019 ) ( English ) , locate its sentiment bearing tokens ; ( ii ) .
they were asked to observe both of these properties in the translated sentences ; ( iii ) .
they were asked to annotate the source sentences into the four classes , namely positive , negative , neutral and others .
The further detailed instructions for sentiment annotation are given as below : ( i ) .
Select the option that best captures the sentiment being conveyed in the sentences : - positive-negativeneutral-others -( ii ) .
Select positive if the sentence shows a positive attitude ( possibly toward an object or event ) .
e.g. great performance and value for money .
( iii ) .
Select negative if the sentence shows a negative attitude ( possibly toward an object or event ) .
e.g. please do not exchange your phone on flipkart they fool you .
( iv. )
Select neutral if the sentence shows a neutral attitude ( possibly toward an object or event ) or is an objective sentence .
Objective sentences are sentences that do not carry any opinion , e.g. facts are objective expressions about entities , events and their properties .
e.g. ( a ) .
the selfie camera is 32 mp .( objective ) , ( b ) .
after doing research on the latest phones , i bought this phone . ( neutral ) .
( iv) Select others for sentences that do not fall in above three categories , e.g. ( a ) .
if the sentence is highly ungrammatical and hard to understand . ( b ) .
if the sentence expresses both positive and negative sentiment , i.e. mixed polarity .
These annotation guidelines were decided after thorough discussions among ourselves .
After we had drafted our initial guidelines , the annotators were asked to perform the verification of the translated sentences , and sentiment annotation for the 100 sentences .
The disagreement cases were thereafter discussed and resolved through discussions among the experts and annotators .
Finally , we came up with the set of instructions as discussed above to minimize the number of disagreement cases .
Class - wise statistics of the sentiment - annotated dataset for En-Hi task are shown in Table 1 : ( B ) .
Additionally , the same annotators also annotated a part of the 4SQ corpus ( i.e. target 9 ( English ) sentences from the 4SQ - HT training and 4SQ - test set ) to obtain the sentiment annotated RL dataset for the Fr- En task ( c.f. Table 1 : ( B ) ) .
For sentiment classification , the inter-annotator agreement ratio ( Fleiss , 1971 ) is 0.72 for En-Hi , and 0.76 for Fr-En .
We manually filtered the RL datasets to only include the positive , negative and neutral sentences as per the the manual annotations .
We refer these sentiment - annotated corpora as the RL dataset ( s ) .
Classifier training :
In order to build the target language sentiment analyser , we use the BERT - based 10 language model .
The classifier is first pre-trained using the target -side sentences of the supervised training corpus .
Classifier pre-training is followed by the task-specific fine -tuning using the target -side sentences of the RL training set .
For example , to build the target language English classifier for the Fr- En task , the classifier is first pre-trained using the English sentences from the supervised dataset ( c.f. Table 1 : ( A ) , row ( i ) ) followed by fine-tuning by using polarity - labelled English sentences from the RL training corpus ( c.f. Table 1 : ( B ) , row ( i ) ) .
Baselines :
Other than the supervised baseline , we also compare our CL - based AC finetuning framework with the following state - of- the - art RL - based fine- tuning frameworks , i.e. ( 1 ) .
REINFORCE , and ( 2 ) .
Machine -Oriented Reinforce .
Additionally , we also conduct the ablation study to better analyse the utility of harmonic reward in the task through our vanilla AC method as follows : ( 3 ) .
MT ac bert : AC fine-tuning with sentiment reward only ; ( 4 ) .
MT ac bleu : AC finetuning with content reward only ; ( 5 ) .
MT ac har : AC fine-tuning with both the rewards .
Finally , for brevity we choose the best performing AC - reward model for the proposed curriculum - based learning .
Hyper-parameters Setting
In all our experiments , we use an NMT system based on Luong et al . ( 2015 ) , using a single layer bi-directional RNN for the encoder .
All the encoder-decoder parameters are uniformly initialized in the range of [ -0.1,0.1 ] .
The sizes of embedding and hidden layers are set to 256 and 512 , respectively .
The Adam optimizer ( Abdalla and Hirst , 2017 ) with ?1 = 0.9 , ?2 = 0.99 is used and the gradient vector is clipped to magnitude 5 .
We set the dropout to 0.2 and use the input feeding with learning rate ( lr ) and batch size ( bs ) set to 1e ? 3 and 64 .
We first perform supervised pre-training of the NMT using the parallel corpora from Table 1 : ( A ) , and select the best model parameters according to the perplexity on the development set ( c.f. Table 1 : ( A ) ) .
We refer the actor-thus obtained - as MT LL , that acts as a trained policy in the RL training ( refer to the upper left side of Figure 1 ) .
Then , we keep the actor fixed and warm - up the critic for one epoch with SBLEU reward on the supervised training samples ( c.f.
Here , superscripts mo , r and ac refers to the Machine -Oriented Reinforce , REINFORCE and actor-critic approach , respectively to fine- tune the LL - based model ( s ) ( M LL , column ( iii ) ; En-Hi : row ( i ) , Fr-En : row ( ii ) ) , and the subscripts bleu , bert and har refers to the corresponding rewards ( i.e. SBLEU ( R1 ) , classifier ( R2 ) , and harmonic mean ( R3 ) ) optimized via the policy gradient method .
( D ) .
Results of curriculum - based fine-tuning and other baselines .
Proposed approach is M ac har + CL .
* significant at p < .05 , ** significant at p < .01 of 1e ? 3 and bs of 64 .
We employ the same encoder-decoder configuration as of the actor for the critic .
In the RL training , we jointly train the actor and the critic model with lr of : - 1e ? 6 ( Fr-En ) ; 1e ? 5 ( En - Hi ) , respectively and bs of 4 on the RL datasets with harmonic reward .
For sampling the candidate translation in the RL training , we use multinomial sampling , with a sampling length of 50 tokens .
We run the experiments three times with different seed values , and record the F 1 and BLEU scores ( c.f. Table 1 : ( C ) ) to evaluate the performance of the sentiment analysers and customised MT systems on the RL testset and report the average of the runs in Section 5 .
For all the RL - based models , the fine-tuning steps maximize the chosen average reward discussed in Section 3.2 on the RL devset .
The fine-tuning continues for a maximum of 20 epochs ( including the baselines ) .
The best epoch is chosen based on the performance observed on the RL devset ( i.e. the best average rewarding epoch ) .
All the sentences are tokenized .
As an extra pre-processing step , we lowercase all the English , French , and normalize all the Hindi sentences .
Tokens in the training sets are segmented into sub-word units using the Byte-Pair Encoding ( BPE ) technique ( Sennrich et al. , 2016 b ) with 4,000 merge operations .
For evaluating our customised MT systems over all the baselines , we use the relevant RL testsets .
Results and Analysis
We first present the results of fine-tuning the pre-trained MT through different RL - based methods , i.e. ( i ) .
REINFORCE ( ii ) .
MO Reinforce , and ( iii ) . vanilla AC ( ours ) in Section 5.1 .
Further , to better analyse the utility of harmonic reward ( R 3 ) in sentiment and content preservation task over the previously studied rewards ( i.e. SBLEU : R 1 or BERT : R 2 ) in the context of NMT ( Tebbifakhr et al. , 2020 ( Tebbifakhr et al. , , 2019 Nguyen et al. , 2017 ; Bahdanau et al. , 2017 ; Wu et al. , 2018 ; Ranzato et al. , 2016 ) , we additionally present the fine-tuning results of the vanilla AC method with the following two types of rewards : ( i ) . only content , i.e. R 1 and ( ii ) .
only sentiment , i.e. R 2 as a reward .
We choose the best performing reward model ( i.e. R 3 ) among the AC - based NMT ( s ) .
At last , we discuss the results in the context of our curriculum - based AC framework .
To evaluate the translation quality we record the BLEU score of the RL testset when translated from the relevant models .
To validate our claim that translations obtained by our proposed MT system can further improve the performance of the sentiment classifier in a cross-lingual setup over the baselines , we do the following .
We apply the target language sentiment classifier to the translations obtained by the LL - based NMT system vs. all the customised RL - based NMT systems , and record their F 1 scores .
Evaluation Results
As shown in Table 1 : ( C ) , the full-fledged LL - based NMT ( s ) ( trained until convergence as observed on the development sets , column ( iii ) . ) obtain the following BLEU points ( 25.02 , 27.87 ) and F 1 scores ( 75.31 , 73.14 ) for the Fr-En , En-Hi tasks , respectively .
We then perform fine-tuning of the pre-trained models through our vanilla AC harmonic approach ( by re-visiting only a subset of samples from the existing supervised training sets which are now additionally annotated with their sentiment ) .
We see for both Fr- En and En- Hi that our harmonic-rewardbased models can obtain a significant performance boost ( further ) over the pre-trained baselines in both the optimized ( targeted ) metrics , i.e. BLEU improved to 25.18 ( + 0.16 ) , 28.13 ( + 0.26 ) and F 1 scores reached to 75.39 ( + 0.08 ) , 73.29 ( + 0.15 ) in both the language pairs .
This is not the case with other reinforcement - based fine-tuned models - MO Reinforce 11 and REINFORCE that optimizes a single reward for which we observe non-optimized reward drop in at least one language - pair .
For example , if we consider the MO Reinforce for the Fr- En task , the nonoptimized metric - BLEU drops by ?0.03 point ( despite an improvement of + 0.02 point in the optimized metric , F 1 score ) , and for the REINFORCE in En- Hi task both BLEU and F 1 score drop by ?0.12 and ?0.02 points , respectively .
This establishes the efficacy of our reinforcement method .
Further , when we see the results form critic- based fine-tuning of the LL model via two commonly used reward routines ( R 1 , R 2 - column ( vi ) . and ( vii ) . ) .
As expected , we see an improvement in the targeted metric ( e.g. for R 1 - based model the optimized reward is BLEU .
We can see improvement in BLEU ) .
However , to our surprise , we found that the improvement in BLEU score does not have a high correlation with the performance in the sentiment classification task .
For example , in the En-Hi task , the critic model with R 1 as a reward ( columns ( vii ) . ) observed the highest BLEU score ( 28.14 ) but the highest F 1 score ( 73.29 ) is observed from the R 3 based model ( column ( viii ) . ) .
This suggests the effectiveness of the harmonic reward which successfully improves both BLEU and F 1 score over the supervised baselines for both the language pairs .
For the sake of brevity , we choose the harmonic model for our curriculum experiment .
When comparing the performance in the context of our proposed curriculum - based AC framework , the results from Table 1 : ( D ) show that our method is better at producing coherent as well as sentiment - rich translation .
By comparing row ( i ) . and row ( vi ) . , we can see that in both Fr- En and En-Hi task , merely learning in an easy - to - hard fashion brings the highest improvement in BLEU scores over the supervised baselines , i.e. + 0.24 , +0.31 point for the Fr-En and En-Hi tasks , respectively .
F 1 scores are also improved by + 0.07 and + 0.08 point , respectively .
All these improvement are statistically significant 12 . Furthermore , we also observe that the CL - based fine -tuning observes a faster convergence over the vanilla approach .
Error Analysis for English -Hindi task
Although our proposed method outperforms the LL - based baseline in the sentiment classification task , we also observe several failure cases .
To investigate this , we observe the sentimentconflicting cases , i.e. selected those samples from ours ' model where there is an observed disagreement between the predicted and the gold sentiment .
From these samples , we filter those examples where the source ( English ) sentences have an explicit presence of the positive or the negative sentiment expression .
Unsurprisingly , we found the main reason for sentiment loss was still the low-translation quality .
Secondly , to better understand what policy is learned by our-proposed NMT that brings the observed improvement in the sentiment - classifier performance , we investigate those translations where the LL model has a predicted ( by the classifier ) sentimentdisagreement , whereas ours ' shows an agreement , both with the gold sentiment .
We present below one such example . .
We can see that our proposed model indeed learned to translate the sentiment expressions to their preferred variant ( positive sentiment bearing expression satisfy translated as santusht ) .
Conclusion
In this paper , we have proposed a curriculum - based deep re-inforcement learning framework that successfully encodes both the underlying sentiment and semantics of the text during translation .
In contrast to the REINFORCE - based frameworks ( Williams , 1992 ; Tebbifakhr et al. , 2019 ) ( actor only models ) , ours is a critic-based approach that helps the actor learns an efficient policy to select the actions , yielding a high return from the critic .
Besides , with the support of curriculum learning , it can be more efficient .
This is also established ( empirically ) through the observed additional boost ( significant at p < .05 ) in BLEU score over the baselines .
Further , we have manually created a domain-specific ( product reviews ) polarity - labelled balanced bilingual corpus for English -Hindi , that could be a useful resource for research in the similar areas .
We shall make the data and our codes available to the community .
Figure 1 : 1 Figure 1 : An illustration of the Actor-Critic Method
