title
Lightweight Adapter Tuning for Multilingual Speech Translation
abstract
Adapter modules were recently introduced as an efficient alternative to fine-tuning in NLP .
Adapter tuning consists in freezing pretrained parameters of a model and injecting lightweight modules between layers , resulting in the addition of only a small number of taskspecific trainable parameters .
While adapter tuning was investigated for multilingual neural machine translation , this paper proposes a comprehensive analysis of adapters for multilingual speech translation ( ST ) .
Starting from different pre-trained models ( a multilingual ST trained on parallel data or a multilingual BART ( mBART ) trained on non-parallel multilingual data ) , we show that adapters can be used to : ( a ) efficiently specialize ST to specific language pairs with a low extra cost in terms of parameters , and ( b ) transfer from an automatic speech recognition ( ASR ) task and an mBART pretrained model to a multilingual ST task .
Experiments show that adapter tuning offer competitive results to full fine-tuning , while being much more parameter -efficient .
Introduction
The question of versatility versus specialization is often raised in the design of any multilingual translation system : is it possible to have a single model that can translate from any source language to any target one , or does it have to be multiple models each of which is in charge of one language pair ?
The former is referred to as a multilingual model , while the latter are bilingual ones .
These two paradigms have their own strengths and limitations .
From a practical point of view , a multilingual model seems to be highly desirable due to its simplicity in training and deployment , in terms of both time and space complexities .
However , in terms of accuracy , a multilingual model could be outperformed by its bilingual counterparts , especially on high- resource language pairs .
In practice , a certain trade- off between the aforementioned factors ( and thus more generally between versatility and specialization ) has often to be made , and depending on the application , one can be favored more than the other .
One way to move along the spectrum between multilingual and bilingual models is to use adapter tuning which consists in freezing pre-trained parameters of a multilingual model and injecting lightweight modules between layers resulting in the addition of a small number of language -specific trainable parameters .
While adapter tuning was investigated for multilingual neural machine translation ( NMT ) ( Bapna and Firat , 2019 ) , to our knowledge , this paper proposes the first comprehensive analysis of adapters for multilingual speech translation .
Our contributions are the following : ( 1 ) we show that both versatility and specialization can be achieved by tuning language -specific adapter modules on top of a multilingual system .
Bilingual models with higher accuracy than the original multilingual model are obtained , yet keeping a low maintenance complexity ; ( 2 ) starting from a different initialization point , we show that adapters can also be used as a glue to connect off - the-shelf systems ( an automatic speech recognition ( ASR ) model and a multilingual denoising auto-encoder mBART ) to perform the multilingual ST task .
Extensive experiments on the MuST - C dataset ( Di Gangi et al. , 2019 ) show that adapter- based fine- tuning can achieve very competitive results to full fine - tuning - while being much more parameter - efficient - in both standard and low-resource settings .
Our code based on FAIRSEQ S2T is publicly available .
1
Related Work Adapter layers ( or adapters for short ) were first proposed in computer vision ( Rebuffi et al. , 2017 ) , then explored for text classification tasks in NLP ( Houlsby et al. , 2019 ) .
Adapters are generally inserted between the layers of a pre-trained network and finetuned on the adaptation corpus .
Bapna and Firat ( 2019 ) studied adapters in the context of NMT and evaluated them on two tasks : domain adaptation and massively multilingual NMT .
Philip et al. ( 2020 ) later introduced monolingual adapters for zero-shot NMT .
Other research groups made contributions on the use of adapters in NLP ( Pfeiffer et al. , 2020 b ( Pfeiffer et al. , , 2021 ) and a framework built on top of HuggingFace Transformers library ( Wolf et al. , 2020 ) was also released to facilitate the downloading , sharing , and adapting state - of - the - art pretrained models with adapter modules ( Pfeiffer et al. , 2020a ) .
Also very relevant to our paper is the work of Stickland et al . ( 2021 ) where adapters are used to adapt pre-trained BART and mBART25 ( multilingual BART pre-trained on 25 languages ) to machine translation .
As far as speech processing is concerned , adapters were mostly used in ASR ( Kannan et al. , 2019 ; Lee et al. , 2020 ; Winata et al. , 2020 ; Zhu et al. , 2020 ) .
Recently , they have also been explored for ST as well but in a limited scope .
Escolano et al. ( 2020 ) addressed a very specific setting ( zero-shot ST ) , while used only a single adapter after a Transformer encoder .
Adapters for Speech Translation
In this section , we describe the integration of adapters into a given backbone model for speech translation .
As the Transformer ( Vaswani et al. , 2017 ) has become increasingly common in speech processing , 2 it will be used as our backbone .
Our method , however , can be easily applied to any other architectures , e.g. , dual- decoder Transformer .
Adapter modules can be introduced into a Transformer in a serial or parallel fashion .
Consider a layer represented by a function f that produces an output y from an input x , i.e. , y = f ( x ) .
This can be an entire encoder or decoder layer , or just one of their sub-layers ( e.g. , the self-attention or the final feed -forward network ( FFN ) component ) .
Suppose that our adapter layer is represented by a function g.
The new " adapted " output is then given by : y serial = g ( f ( x ) ) , y parallel = f ( x ) + g ( x ) .
Intuitively , a serial adapter modifies the output directly , while a parallel one performs the operations in parallel before merging its output to the layer .
In Figure 1a , we show an example of serial adapters being integrated to the Transformer , or more precisely to its FFN sub-layers .
A common adapter module ( Bapna and Firat , 2019 ) is presented in Figure 1 b .
Here g is a small FFN with a residual connection .
The first linear layer is typically a down projection to a bottleneck dimension , and the second one projects the output back to the initial dimension .
Bottleneck allows us to limit the number of parameters .
Other adapter architectures also exist , e.g. , Stickland and Murray ( 2019 ) explored parallel adapters consisting of a multi-head attention ( MHA ) layer in a multi-task setup .
For multilingual ST , we adopt the following general recipe for adapter- based fine-tuning .
Starting from a pre-trained backbone , an adapter is added for each language pair and then finetuned on the corresponding bilingual data ( while the rest of the backbone is frozen ) .
The pre-trained backbone plays a crucial role in this recipe .
We explore two common scenarios to obtain this pre-trained model , namely refinement and transfer learning .
We present them in details , together with extensive experimental results , in Section 5 and 6 .
In the next section , we present our experimental setup .
Experimental Setup
Dataset MuST -C
We evaluate our recipes on MuST -C ( Di Gangi et al. , 2019 ) , a large-scale one-to-many ( 6 , 8 , 14 , 18 ) and perform statistical significance test using bootstrap re-sampling ( Koehn , 2004 ) .
Results passing the test ( compared to the corresponding multilingual baselines , with p-value < 0.05 ) are marked with a star .
ST dataset from English to eight target languages including Dutch ( nl ) , French ( fr ) , German ( de ) , Italian ( it ) , Portuguese ( pt ) , Romanian ( ro ) , Russian ( ru ) , and Spanish ( es ) .
Each direction includes a triplet of speech , transcription , and translation .
Sizes range from 385 hours ( pt ) to 504 hours ( es ) .
MuST -C-Imbalanced
We built a low-resource version of MuST -C , called MuST - C-Imbalanced , in which we randomly keep only X % of the original training data , where X = 100 for es , fr ; X = 50 for ru , it ; X = 20 for nl , ro ; and X = 10 for de , pt ( same order of the languages in the original MuST - C if we sort them in decreasing amount of data ) .
The amount of speech data ranges from 41 hours ( de ) to 504 hours ( es ) in this version , better reflecting real-world data imbalance scenarios .
Implementation details
Our implementation is based on the FAIRSEQ S2T toolkit .
We experiment with two architectures : a small Transformer model with dimension D = 256 and a medium one where D = 512 .
All experiments use the same encoder with 12 layers .
The decoder has 6 layers , except for the transfer learning scenario where we used the mBART decoder for initialization .
We used 8 k and 10k unigram vocabulary ( Kudo and Richardson , 2018 ) for bilingual and multilingual models , respectively .
The speech features are 80 - dimensional log mel filter - bank .
Utterances having more than 3000 frames are removed for GPU efficiency .
We used SpecAugment ( Park et al. , 2019 ) with Lib-riSpeech basic ( LB ) policy for data augmentation .
We used the Adam optimizer ( Kingma and Ba , 2015 ) with learning rate linearly increased for the first 10 K steps to a value ? max , then decreased proportionally to the inverse square root of the step counter .
For all adapter experiments , ? max is set to 2e ?3 .
For the others , however , we perform a grid search over three values { 2e? 3 , 2e?4 , 2e?5 } and select the best one on the dev set , as they are more sensitive to the learning rate .
Refinement
In this section , a fully trained multilingual ST backbone is further refined on each language pair to boost the performance and close potential gaps with bilingual models .
We compare adapter tuning with other fine-tuning approaches as well as the bilingual and multilingual baselines ( the latter being the starting point for all fine- tuning approaches ) ( Bapna and Firat , 2019 ) . these backbones , we either add language -specific adapters and train them only , or we finetune the backbone on each language pair , either fully or partially .
All these trainings are performed on MuST -C .
The results are shown in Table 1 .
There are two main blocks corresponding to two architectures : D = 256 ( small ) and D = 512 ( medium ) .
Rows 1 and 9 provide the bilingual baselines , while rows 2 and 10 serve as the multilingual baselines for each block .
In addition , we compare adapter-tuning with full fine-tuning and multilingual - training ( baseline ) on MuST - C-Imbalanced .
Table 2 displays the results for this set of experiments .
Bilingual vs. Multilingual
For the small architecture ( D = 256 ) , the bilingual models slightly outperform their multilingual counterpart ( rows 1 , 2 ) .
Looking further into the performance of each language pair , the multilingual model is able to improve the results for 4 out of 8 pairs ( de , nl , pt , ru ) , mainly those in the lower - resource direction , but the joint multilingual training slightly hurts the performance of higher - resource pairs such as es , fr , it , and ro .
Finally , we observe that the medium model ( D = 512 ) performs better in the multilingual setting than the bilingual one ( rows 9 , 10 ) .
Adapter tuning vs .
Fine-tuning Both recipes yield improvements over the multilingual baseline and recover the lost performance of higher - resource directions compared to the bilingual baseline for the small model ( D = 256 ) .
For the medium one ( D = 512 ) , one adapter tuning ( row 14 ) can slightly improve the scores in all directions and even approach the results of the best fine-tuning experiment ( row 17 ) while maintaining much lower model sizes ( 95.5 M vs. 8 ? 36.3 M parameters ) .
Low-resource scenario
The obtained results on small models show that adapter-tuning achieved the best performance , producing clear improvements over the baseline , especially for the low-resource languages : + 1.1 BLEU on average on nl , ro , de , pt ; + 0.3 BLEU on average on es , fr , ru , it ; which is competitive to full fine-tuning ( + 0.9 and + 0.4 BLEU , respectively ) while being more parameterefficient as well as simpler for training and deployment ( one model with adapters versus eight separate models ) .
For larger models , however , the improvement is smaller : + 0.4 BLEU on average on the lower - resource pairs and + 0.1 on the higherresource ones ; while those of full fine- tuning are + 0.4 and roughly no improvement , respectively .
Results on test set We select the best-performing fine-tuning recipes on the dev set ( rows 16 and 17 in Table 1 ) for evaluation on the test set .
For reference , we also include the multilingual baseline ( row 10 ) .
Moreover , to go beyond conventional finetuning approaches , we also compare our recipes with a contemporary work in which only several components of the network are finetuned .
For a fair comparison , we did not use large pre-trained components such as wav2vec ( Baevski et al. , 2020 ) or mBART but instead considered the same pre-trained compo- Table 4 : BLEU on MuST - C dev set for transfer learning from pre-trained ASR and mBART models .
We compare the results with the bilingual baselines ( trained from scratch ) , shown in row 1 ( which is identical to row 1 in Table 1 ) .
The column " Finetune xattn " means updating the cross-attention parameters .
We refer to Table 1 for other notation .
nents used in our previous experiments .
Following , we considered six variants : fine-tuning LayerNorm + Attention in the encoder ( LNA - E ) , or the decoder ( LNA - D ) , or both ( LNA - E, D ) ; each with or without the length adapter .
We found that adding the length adapter did not help in our experiments .
Table 3 shows that our approach compares favorably with in terms of both performance and parameter -efficiency .
Other comments
For small models , the encoder adapters boost the performance ( 0.3 - 0.4 BLEU on average ) in all directions ( rows 3 and 4 , 5 and 6 , Table 1 ) , indicating that language -specific adapters can tweak the encoder representations to make them better suited for the decoder .
In larger models , however , the impact of the encoder adapters is varied depending on languages and bottleneck dimensions .
We also notice that increasing the bottleneck dimension slightly improves performance while remaining parameter -efficient .
Fine-tuning remains the best option to optimize the models in most cases but leads to much larger model sizes .
The adapter-tuning approach is competitive to finetuning while being much more parameter -efficient .
Transfer Learning
In this section , we show that adapters can be used to combine available pre-trained models to perform a multilingual ST task .
In particular , we initialize the encoder using a pre-trained ASR encoder ( on MuST - C ) 3 provided by and the decoder using mBART50 , a multilingual denoising auto-encoder pre-trained on 50 languages .
We tune language independent crossattention and language -specific adapters on top of these backbone models ( using MuST - C as well ) .
The results presented in Table 4 highlight that fine - 3 Pre-training on ASR data and then transferring to ST is not new but rather standard .
See , e.g. , Bansal et al. ( 2019 ) . tuning cross-attention is crucial to transfer to multilingual ST ( rows 3 and 5 show poor results without doing so ) .
Adding adapters to the backbone decoder ( row 4 ) or to both encoder and decoder ( row 6 ) further boosts performance , demonstrating the ability of adapters to connect off - the-shelf models in a modular fashion .
The best- performing model in this recipe ( row 6 ) also outperforms bilingual systems ( row 1 ) despite having fewer trainable parameters ( 190 M vs. 248M ) .
It is also important to mention that while we experiment on 8 target languages of MuST - C corpus , the multilingual ST model of row 2 should be practically able to decode into 50 different target languages .
Investigating such a zero-shot ST scenario is left for future work .
Conclusion
We have presented a study of adapters for multilingual ST and shown that language -specific adapters can enable a fully trained multilingual ST model to be further specialized in each language pair .
With these adapter modules , one can efficiently obtain a single multilingual ST system that outperforms the original multilingual model as well as multiple bilingual systems while maintaining a low storage cost and simplicity in deployment .
In addition , adapter modules can also be used to connect available pre-trained models such as an ASR model and a multilingual denoising auto-encoder to perform the multilingual speech - to - text translation task .
