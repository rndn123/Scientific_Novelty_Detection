title
Tune In : The AFRL WMT21 News-Translation Systems
abstract
This paper describes the Air Force Research Laboratory ( AFRL ) machine translation systems and the improvements that were developed during the WMT21 evaluation campaign .
This year , we explore various methods of adapting our baseline models from WMT20 and again measure improvements in performance on the Russian - English language pair .
Introduction
As part of the 2021 Conference on Machine Translation ( wmt , 2021 ) news-translation shared task , the AFRL human language technology team participated in the Russian - English portion of the competition .
We experiment with OpenNMT - tf 1 ( Klein et al. , 2018 ) and Marian 2 ( Junczys - Dowmunt et al. , 2018 ) transformer ( Vaswani et al. , 2017 ) models trained as part of our WMT20 ( Gwinnup and Anderson , 2020 ) efforts and apply varying continuedtraining and fine-tuning approaches ( Luong and Manning , 2015 ; Freitag and Al - Onaizan , 2016 ) , including a new method to select a fine-tuning set from a separate , larger corpus not used in training .
We submit an OpenNMT - based transformer system fine-tuned on newstest test sets from 2014 - 2017 as our primary entry , and a Marian - based transformer system fine -tuned on newstest test sets from 2014 - 2018 as a contrast .
Data and Preprocessing Since most of our efforts focus on fine-tuning existing models this year , we reuse the training corpus from our WMT20 systems which includes the following parallel corpora : Commoncrawl ( Smith et al. , 2013 ) , Yandex 3 , UN v1.0 ( Ziemski et al. , 2016 ) , Paracrawl 4 ( Espl ?
et al. , 2019 ) , Wikimatrix ( Schwenk et al. , 2019 ) , and backtranslated data from our WMT17 system ( Gwinnup et al. , 2017 ) as well as Edinburgh 's WMT17 system ( Sennrich et al. , 2017 ) yielding a raw corpus of over 76.3 million lines .
The new Russian - English version 8 Paracrawl corpus is reserved for tuning set selection as described in Section 2.3 .
Data Preparation
We re-use the fastText ( Joulin et al. , 2016 b , a ) based language ID filtered corpus with an ID threshold of 0.8 as described in Gwinnup and Anderson ( 2020 ) , shown in Table 1 , allowing us to make concrete progress comparisons to last year 's systems .
Data Augmentation with Speech Recognition - like output
In order to build a larger pool of training data , we have created Automatic Speech Recognition ( ASR ) - like training data for the Russian - English translation task .
Whereas written text can include upper and lowercase characters , punctuation , special symbols , and numbers written using digits , transcripts produced by ASR systems are typically uncased with no punctuation , no special symbols , and numbers written as spoken ( e.g. , 4.1 % rendered as " four point one percent " ) .
In previous experiments on an English - German spoken language translation task ( Ore et al. , 2020 ) , we found that we could get an improvement in BLEU score by formatting the MT training data such that the source language text matched the output format of our ASR system , while leaving the target language text unmodified .
We applied a similar procedure to the Russian side of the Russian - English training corpus using the text2norm.pl script from ru2sphinx .
the original training data , effectively doubling the size of the corpus .
Selecting Tuning Sets from Representative Data
We performed experiments involving automatic selection of fine-tuning corpora .
Given a monolingual application corpus , we wish to test the possibility of selecting an appropriate fine-tuning set to improve a general - purpose neural MT system 's performance on that application corpus .
We anticipate such techniques to be of increasing importance , especially for high- value application corpora , as computational costs of subcorpus selection and fine-tuning continue to decrease .
Method
We performed subselection as in Erdmann and Gwinnup ( 2019 ) , which can flexibly incorporate a text quality metric and multiple parallel text corpora .
In short , this algorithm tries to simultaneously optimize the quality of the subset 's text and its coverage of the vocabulary present in given application corpora .
Of special note is our use of clustering to select data .
We hierarchically applied the MAPPER algorithm ( Singh et al. , 2007 ) to cluster sentence vectors of a monolingual corpus .
The clusters deemed useful were then used to assign fuzzy clustering to the application corpus and the corpus from which we subselect .
This clustering information was included as one of the text corpora .
Application
The application corpus we used was the Russian side of newstest2019 and newstest 2020 , totalling 6777 lines .
The pool of possible parallel text for subselection we took to be the given 12.6 M - line subset of Russian - English version 8 ParaCrawl corpus with LASER score at least 1.1 .
For subselection algorithms , we first preprocessed the Russian text , applying a 90k- element joint BPE .
We used the algorithm in Erdmann and Gwinnup ( 2019 ) to subselect a corpus , using 3 - grams in the vocabulary coverage .
As a text quality metric in this algorithm we used either the provided Bicleaner scores ( S?nchez - Cartagena et al. , 2018 ; Ram?rez-S?nchez et al. , 2020 ) or the word- averaged scores provided by OpenNMT 's scoring functionality , using the untuned OpenNMT model we developed for this year 's task .
In order to provide meaningful comparisons with our baseline fine- tuning set of newstest 2014-2018 , we matched its size by always subselecting a fine-tuning set with fifteen thousand lines .
Fine-tuning was performed using a singlemodel Marian - based untuned MT system as a baseline .
Sentence vector clustering was learned using a 570 M - line monolingual Russian corpus built from the concatenation of monolingual CommonCrawl ( Smith et al. , 2013 ) data provided by WMT organizers as part of our WMT18 efforts towards pretraining word embeddings .
The word vectors were trained using word2vec ( Mikolov et al. , 2013 ) on this corpus , after applying a 90k - element joint BPE .
These embeddings have a dimensionality of 512 to match our Marian transformer - base system configuration as described in Gwinnup et al . ( 2018 ) .
A randomly - chosen 100k - line subset of the corpus was used to find the clustering .
Several methods of converting word vectors to sentence vectors were considered , and we empirically chose a " softened sum " of the word vectors w i as the sentence vector s : s = w i log ( 1 + number of words in sentence ) .
Clusters were considered to be useful if they covered between 1 % and 5 % of this corpus .
In this case there were 19 such clusters , having between 1000 and 5000 representatives each .
These clusters were found to have qualitative meaning to a Russian linguist : clusters with relatively high representation in our application corpus tended to be news - like , and clusters with relatively high representation in ParaCrawl tended to be noisier .
We computed membership of a given sentence vector in a fuzzy clustering sense , with weight of cluster i defined as z i = ( min distance / distance i ) 4 where we use Euclidean distance , and the minimum is taken over all 19 clusters .
Although the exact form is empirical , note that the weight has a maximum of unity at the closest cluster and that a cluster will get lower weight if it is farther from the sentence vector .
This fuzzy clustering was computed once using k-means ( distance is to cluster mean ) and once using single - linkage ( distance is to nearest member ) clustering .
These two membership clusters were then averaged .
Coverage of the clusters was encouraged by including the clustering as another text corpus in our standard algorithm ( Erdmann and Gwinnup , 2019 ) - each sentence vector was converted into a 100 - word " sentence , " where each cluster 's " word " appeared a number of times relative to the magnitude of its weight in the line 's clustering 6 .
Naturally , coverage of these clustering words was computed using only unigrams .
Results
Table 2 shows the results of our fine-tuning experiments .
The " clustering " and " metric " columns designate whether clustering was incorporated and whether Bicleaner ( " Bic " ) or NMT scoring ( " NMT " ) was used as the text quality metric .
We see consistent gains over the untuned set , even on newstest 2021 , which was not used in the selection .
The three subselection methods produced similar results on the three test sets .
Fine-tuning with our selected sets did not 6 For example , using a 10 - word sentence for brevity , this process would convert the fuzzy cluster membership vector [ 0.2 , 0.0 , 0.8 , 1.0 ] into the sentence " 0 2 2 2 2 3 3 3 3 3 " .
produce consistent improvement over our baseline fine-tuning using newstest 2014 - 2018 .
Compared to this baseline fine-tuning , the new sets improved performance on newstest2019 ( roughly + 0.7 BLEU ) , but they lowered performance on newstest2020 ( roughly ?0.7 BLEU ) and the unseen newstest 2021 ( roughly ?1.1 BLEU ) .
Our generated fine-tuning sets did not show a consistent benefit for this task , so they were not used in our submission systems .
Without further information , we cannot attribute the quality of the results to the method , the quality of data in ParaCrawl , or other causes .
Our method generates a pseudo in- domain set for an unknown application domain , using only source -side data of the application corpus .
This generated set can be used for fine-tuning , training , or other purposes in natural language processing .
We believe that such techniques warrant further investigation , especially for an application corpus where the domain is unknown or human-curated parallel data are unavailable .
Machine Translation Systems
OpenNMT -tf
The OpenNMT - tf system trained for this task used the configuration for a big deep transformer network .
We used the following network hyperparameters : ? 1024 embedding size with SentencePiece ( Kudo and Richardson , 2018 ) using a model with a vocabulary size of 40 K trained on this ru-en corpus of 16,805,109 bi-text .
This was one of our WMT20 submitted systems ( Systems 3 and 4 in Table 3 ) .
Additionally the corpus was processed as described in Section 2.2 to resemble ASR output and the resulting data was combined with the above for a final count of 33,610,218 bitext .
The network was trained for 10 epochs of this training data using a batch size of 3124 and an effective batch size of 49984 using the lazy Adam ( Kingma and Ba , 2015 ) optimizer with beta1=0.9 , beta2=0.998 and learning rate 2.0 .
This a system that had been originally trained for speech translation application but showed improvements in text translation as well .
The final submitted system continued training an additional 2 epochs using the unfiltered data described in Table 1 .
This was done to try to take advantage of the larger data set and not having the computational resources or time to train a new system with with the larger data set in time for submission deadline .
The output was an average of the last 8 checkpoints of training .
Checkpoints were saved every 5000 steps .
The system was then tuned with three epochs of newstest data from years 2014 - 2017 ( Systems 5 and 6 in Table 3 ) .
Marian Our Marian systems utilize the transformer architecture in the transformer - base configuration .
We use the WMT14 newstest2014 test set for validation during training and the following network hyperparameters : ? 512 embedding size ?
2048 hidden units ?
6 layer encoder ?
6 layer decoder ?
8 transformer heads ?
Tied embeddings for source , target and output layers ?
Layer normalization ?
Label smoothing ?
Learning rate warm - up and cool-down
We experimented with tuning these systems with the concatenation of WMT newstest sets from 2014 - 2018 yielding a tuning corpus of 14,820 parallel sentences .
For each of the five separate transformer models trained for the Marian transformerbase ensemble systems in Gwinnup and Anderson ( 2020 ) , continued training was performed for 10 epochs on the concatenated tests sets .
An ensemble of the five resulting tuned models is then used to decode newstest sets from 2019 - 2021 .
Resulting scores reported by SacreBLEU are shown as Row 2 in Table 3 , while the baseline , untuned ensemble is shown as Row 1 .
We note gains between + 2.0 and + 3.5 BLEU as measured by SacreBLEU over the baseline ensemble system depending on test set .
Experimental Results Results reported here and in Table 3 for Marian systems were scored with SacreBLEU ( Post , 2018 ) while results for OpenNMT systems were score with mult-bleu-detok .
perl from the Moses toolkit ( Koehn et al. , 2007 ) .
Internal comparisons between the two scoring methods have been in agreement .
All scores are on detokenized cased output .
The primary submission system was the OpenNMT - tf configuration described in section 3.1 and shown in Table 3 as onmt+ asr-tune .
It resulted in official scores of 38 .83 BLEU -A , 39.56 BLEU -B , 0.64 chrf-all , 0.63 chrf -A , and 0.64 for chrf -B on the 2021 test-set .
Post evaluation a model with the OpenNMT - tf configuration described in section 3.1 was trained on all the unfiltered data ( approx .
76 M million bi-text ) .
The results are shown in Table 3 as onmtlarge .
The baseline onmt-large system was approx-imately + 1 BLEU better that the baseline onmtasr system while the onmt- asr system which continued training with two epochs of the large data set and tuned with newstest 2014 - 2017 ( onmt -+ asrtune ) was + 2.5 BLEU better than the baseline onmtlarge system which was trained with 10 epochs and comparable to the onmt-large system tuned with newstest 2014- 2017 .
Experiments were conducted on both onmt+ asr and onmt-large with tuning sets comprised of different combinations of the supplied news test sets from 2014 to 2019 .
Tune7 is news test sets from 2014 - 2017 ( 11,820 bi-text ) , tune8 is news test sets from 2014- 2018 ( 14,820 bi-text ) , and tune9 is news test sets from 2014 - 2019 ( 16,820 bi-text ) .
Systems were tuned for three epochs using these tuning sets .
Generally performance dropped off or decreased slightly with more than 3 epochs of tuning .
To be consistent across systems and tuning sets we are only reporting results for 3 epochs .
As can be seen in Table 3 all three tuning sets provided significant improvements over the baseline systems , generally in the range of + 3.5 BLEU on test 2021 .
For onmt + asr there was little difference in tuning with tune7 or tune8 whereas tune9 was approximately + 0.4 BLEU better than those two .
For onmt- large tune7 did not provide as much benefit as tune8 and tune9 which were basically the same , less than 0.1 BLEU difference between the two .
Conclusion
While our two submission systems employ a standard method of fine-tuning to adapt models towards a test set , we find that our methods to sample a similarly - sized tuning corpus from a larger body of text while only using information about the source side of that data yields a reasonable improvement in translation quality .
Such a technique could be useful in adapting translation models to specific domains where only the source language of a text source is available .
Using actual in- domain data , such as the provided news development sets , for fine-tuning provide a substantial gain in translation quality .
Such data is not always available and thus other selection techniques as described in Section 2.3 come into play .
Future work will investigate combining the two approaches to see if additional gains can be obtained .
The authors would like to thank Emily Conway and Braeden Bowen for their assistance in human Table 2 : 2 Tuning sets and resultant BLEU scores .
? 4096 hidden units ?
12 layer encoder ?
12 layer decoder ?
16 transformer heads ? dropout 0.3 ? attention dropout 0.1 ? feed forward network dropout 0.1 ? embeddings for source , target and output lay - ers were not tied ?
Layer normalization ?
Label smoothing 0.1 ?
Learning rate warm - up 8000 steps
The corpus used for the initial model con- sisted of commoncrawl , paracrawl v1 , and news - commentary - v13 from wmt19 and was processed
