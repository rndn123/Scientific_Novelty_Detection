title
abstract
The idea behind the proposed framework is simple , they suggest that decoupling language models and the functions with which the token .
Sentence Permutation random shuffling of the document 's sentences .
Document Rotation a token is chosen randomly to be the start of the document , the section before the starting token is appended at the end .
Masked Seq2Seq ( MASS ) masking a span containing 50 % of the tokens and train to predict the masked tokens .
Results of the first experiment Figure 3 : Table of results for the first experiment From the results of these first experiments , the authors draw some important conclusions .
Token masking is crucial
Only the configurations with token masking or its variations achieve consistently great performance on different tasks .
Left-to- right pre-training improves NLG
The Classical Language Model objective despite not doing well in inference or question answering tasks achieves SOTA on ELI5 ( Explain Like I'm 5 ) .
Bidirectional encoders are crucial for QA Ignoring future context hinders the performance of left-to- right models .
While pre-training techniques and LM objectives are important , the authors make note of the fact that they do not provide the full picture .
They report that their permuted language model performs much worse than XLNet because BART lacks some of the valuable architectural innovations introduced in XLNet .
Results of the large-scale pre-training experiment After the comparative experiment , the authors trained a 12 layered , transformer - based architecture for autoencoding , and using If you want to summarize some text of your own we have set up a Google Colab notebook using the Hugging Face library .
Upgrade Open in app 5 Figure 1 : 51 Figure 1 : Diagram of the framework introduced in the paper
Figure 2 : How different text-noising techniques corrupt the text
Upgrade Open in app
21/02/2022 , 21:42 BART : Are all pretraining techniques created equal ?
| by Antonio Lopardo | DAIR.AI | Medium https://medium.com/dair-ai/bart-are-all-pretraining-techniques-created-equal-e869a490042e
3/5Intuitively , the techniques that work at the sentence level should help the LM learn the different roles of sentences in a paragraph or longer text and in the process help dealing with language generation ( NLG ) tasks .
Besides the pre-training techniques , the authors also compare different LM objectives focusing on the ones used by BERT and GPT as well as techniques that tried to incorporate the best of both worlds : Autoregressive , left to right , LM ( GPT - 2 ) Masked LM ( BERT ) replace 15 % of the token with [ MASK ] and predict the corresponding words .
Permuted LM ( XLNet ) left to right , autoregressive LM training but with the order of the words to predict chosen at random .
Multitask Masked LM ( UniLM ) combination of right - to- left , left- to - right , using bi-direction .
? of the time using each with shared parameters .
similar hyperparameters to RoBERTa .
They used both a form of token masking at 30 % and sentence permutation as pre-training textnoising techniques and run the model on 160 GB of news , books , stories , and web text , similar to what 's done in RoBERTa .
Upgrade Open in app
21/02/2022 , 21:42 BART : Are all pretraining techniques created equal ?
| by Antonio Lopardo | DAIR.AI | Medium https://medium.com/dair-ai/bart-are-all-pretraining-techniques-created-equal-e869a490042e 4/5
Figure 4 : 5 Figure 5 455 Figure 4 : Table of results for the Large Scale pre-training
