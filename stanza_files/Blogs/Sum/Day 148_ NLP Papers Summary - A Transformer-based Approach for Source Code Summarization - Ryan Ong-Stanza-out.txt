title
Day 148 : NLP Papers Summary -A Transformer -Based Approach For Source Code Summarization
abstract
Objective and Contribution Utilised a simple transformer - based model with relative position representations and copy attention mechanism to generate SOTA results for source code summarisation .
We found that the absolute encoding of source code tokens ' position hinders the performance of summarisation whereas the relative encoding signi cantly improves the performance .
The objective is to encode source code and generate a readable summary that describes the functionality of the program .
Datasets
We have two evaluation datasets : the Java and Python dataset from GitHub as shown below .
Our evaluation metrics are BLEU , METEOR , and ROUGE -L.
Methodology
Our proposed model is the vanilla Transformer .
We encoded the code and summary as sequence of embeddings .
The vanilla Transformer has stacked of multi-head attention and linear transformation layers in the encoder and decoder .
We also included the copy attention in the Transformer to allow the model the ability to copy rare tokens from the source code .
POSITION REPRESENTATIONS
Here , we explored the absolute position encoding on the sequential order of source code tokens and the pairwise relationship encoding in Transformer .
The absolute position encoding aims to capture the order information of source tokens , however , we show that the order information is actually not helpful in learning source code representations and leads to bad summarisation .
We found that it is the mutual interactions between tokens that in uence the meaning of the source code , which it 's why we explore pairwise relationship encoding .
To capture this pairwise relationships between input tokens , we capture the relative positional representations of two position i and j for each token .
NLP Papers Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization/
3/9 Results
As shown below , our full model outperformed all the baseline models .
In fact , the base model trained on the dataset without the CamelCase and snake_case code token processing , outperformed all baseline models except on the ROUGE -L metric .
Our baseline models did n't incorporate copy attention mechanism and we shown that the copy attention mechanism does improve the performance of our full model .
Ablation studies IMPACT OF POSITION REPRESENTATION
Table 3 below showcase the performance of performing absolute position encoding on source and targets .
It showcase the decrease in performance when include the absolute position encoding .
Table 4 showcase the bene t of learning pairwise relationship between source code tokens .
We experimented with different clipping distance and whether we should include the bidirectional information .
The performance of different clipping distance are very similar to the performance of our full model and models that include the directional NLP Papers Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization/ 4/9 VARYING MODEL SIZE AND NUMBER OF LAYERS
Our results below showcase that a deeper model ( more layers ) performs better than a wider model ( more neurons per layer ) .
We suspect that deeper model is more bene cial in source code summarisation as it depends more on semantic information than syntactic .
Papers Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarizationPapers
Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarizationPapers
Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarizationNLP
Papers Summary -A Transformer - based Approach for Source Code Summarization - Ryan Ong https://ryanong.co.uk/2020/05/27/day-148-nlp-papers-summary-a-transformer-based-approach-for-source-code-summarization/ 8/9 Day 365 : NLP Papers Summary - A Survey on Knowledge Graph Embedding Ryan 30th December 2020 Day 364 : Ryan's PhD Journey - OpenKE - PyTorch Library Analysis + code snippets for 11 KE models
