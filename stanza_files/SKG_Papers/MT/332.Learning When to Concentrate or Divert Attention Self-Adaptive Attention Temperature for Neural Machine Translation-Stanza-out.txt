title
Learning When to Concentrate or Divert Attention : Self-Adaptive Attention Temperature for Neural Machine Translation
abstract
Most of the Neural Machine Translation ( NMT ) models are based on the sequence - tosequence ( Seq2Seq ) model with an encoderdecoder framework equipped with the attention mechanism .
However , the conventional attention mechanism treats the decoding at each time step equally with the same matrix , which is problematic since the softness of the attention for different types of words ( e.g. content words and function words ) should differ .
Therefore , we propose a new model with a mechanism called Self-Adaptive Control of Temperature ( SACT ) to control the softness of attention by means of an attention temperature .
Experimental results on the Chinese-English translation and English - Vietnamese translation demonstrate that our model outperforms the baseline models , and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality .
Introduction
In recent years , Neural Machine Translation ( NMT ) has become the mainstream method of machine translation as it , in a great number of cases , outperforms most models based on Statistical Machine Translation ( SMT ) , let alone the linguistics - based methods .
One of the most popular baseline models is the sequence-to-sequence ( Seq2Seq ) model ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; with attention mechanism .
However , the conventional attention mechanism is problematic in real practice .
The same weight matrix for attention is applied to all decoder outputs at all time steps , which , however , can cause inaccuracy .
Take a typical example from the perspective of linguistics .
Words can be categorized into two types , function word , and content word .
Function words and content words execute different functions in the construction of a sentence , which is relevant to syntactic structure and semantic meaning respectively .
Our motivation is that the attention mechanism for different types of words , especially function word and content word , should be different .
When decoding a content word , the attention scores on the source-side contexts should be harder so that the decoding can be more focused on the concrete word that is semantic referent in the source text .
But when decoding a function word , the attention scores should be softer so that the decoding can pay attention to its syntactic constituents in the source text that may be several words instead of one word .
To tackle the problem mentioned above , we propose a mechanism called Self-Adaptive Control of Temperature ( SACT ) to control the softness of attention for the RNN - based Seq2Seq model 1 .
We set a temperature parameter , which can be learned by the model based on the attention in the previous decoding time steps as well as the output of the decoder at the current time step .
With the temperature parameter , the model is able to automatically tune the degree of softness of the distribution of the attention scores .
To be specific , the model can learn a soft distribution of attention which is more uniform for generating function word and a hard distribution which is sparser for generating content words .
Our contributions in this study are in the following : ( 1 ) .
We propose a new model for NMT , which contains a mechanism called Self-Adaptive Control of Temperature ( SACT ) to control the softness of the attention score distribution . ( 2 ) .
Experimental results demonstrate that our model outperforms the attention - based Seq2Seq model in both Chinese-English and English - Vietnamese translation , with a 2.94 BLEU point and 2.19 BLEU score advantage respectively 2 . ( 3 ) .
The analysis shows that our model is more capable of translating long texts , compared with the baseline models .
Our Model
As is mentioned above , our model is substantially a Seq2Seq framework improved by the SACT mechanism .
In this section , we first briefly describe the Seq2Seq model , then introduce the SACT mechanism in detail .
Seq2Seq Model
We implement the encoder with bidirectional Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) , where the encoder outputs from two directions at each time step are concatenated , and we implement the decoder with unidirectional LSTM .
We train our model with the Cross-Entropy Loss , which is equivalent to the maximum likelihood estimation .
In the following , we introduce the details of our proposed attention mechanism .
Self-Adaptive Control of Temperature
In our assumption , due to the various functions of words , decoding at each time step should not use the identical attention mechanism to extract the required information from the source-side contexts .
Therefore , we propose our Self-Adaptive Control of Temperature ( SACT ) to improve the conventional attention mechanism , so that the model can learn to control the scale of the softness of attention for the decoding of different words .
In the following , we present the details of our design of the mechanism .
We set a temperature parameter ? to control the softness of the attention at each time step .
The temperature parameter ? can be learned by the model itself .
In our assumption , the temperature parameter is learned based on the information of the decoding at the current time step as well as the attention in the previous time steps , referring to the information about what has been translated and what is going to be translated .
Specifically , it 2
What should be mentioned is that though the " Transformer " model is recently regarded as the best , the model architecture is not the focus of our study .
Furthermore , our proposed mechanism can also be applied to the aforementioned model , which will be a part of our future study .
is defined as below : ? t = ? ?t ( 1 ) ? t = tanh ( W c ct ?1 + U s s t ) ( 2 ) where s t is the output of the LSTM decoder as mentioned above , ct ?1 is the context vector generated by our attention mechanism at the last time step ( initialized with the initial state of the decoder for the decoding at the first time step ) , and ? is a hyper-parameter , which decides the upper bound and the lower bound of the scale for the softness of attention .
To be specific , ? should be a number larger than 1 3 .
The range of the output value of tanh function is ( ? 1 , 1 ) , so the range of the ? is ( 1 ? , ? ) .
Furthermore , the temperature parameter is applied to the conventional attention mechanism .
Different from the conventional attention mechanism , the temperature parameter is applied to the computation of attention score ? so that the scale of the softness of attention can be changed .
We define the new attention score and context vector as ? and c , which are computed as : ct = n i=1 ?t , i h i ( 3 ) ?t , i = exp ( ?
?1 t e t, i ) n j=1 exp ( ?
?1 t e t , j ) ( 4 ) From the definition above , it can be inferred that when the temperature increases , the distribution of the attention score ?
is smoother , meaning that softer attention is required , and when the temperature is low , the distribution is sparser , meaning that harder attention is required .
Therefore , the model can tune the softness of the attention distribution self -adaptively based on the current output for the decoder and the history of attention , and learns when to attend to only corresponding words and when to attend to more relevant words for further syntactic and semantic information .
Experiment
In the following , we introduce the experimental details , including the datasets and the experiment setting .
Model ( Cettolo et al. , 2015 ) .
The validation set is the TED tst2012 with 1553 sentences and the test set is the TED tst2013 with 1268 sentences .
The English vocabulary is 17.7 K words and the Vietnamese vocabulary is 7 K words .
The evaluation metric is also BLEU as mentioned above 5 .
Setting Our model is implemented with PyTorch on an NVIDIA 1080 Ti GPU .
Both the size of word embedding and the size of the hidden layers in the encoder and decoder are 512 .
Gradient clipping for the gradients is applied with the largest gradient norm 10 in our experiments .
Dropout is used with the dropout rate set to 0.3 for the Chinese - English translation and 0.4 for the English - Vietnamese translation , in accordance with the evaluation on the development set .
Batch size is set to 64 .
We use Adam optimizer ( Kingma and Ba , 2014 ) to train the model 6 .
Baselines
In the following , we introduce our baseline models for the Chinese -English translation and the English - Vietnamese translation respectively .
For the Chinese-English translation , we compare our model with the most recent NMT systems , illustrated in the following .
Moses is an open source phrase - based translation system with default configurations and a 4 - gram language model trained on the training data for the target language ; RNNSearch is an attention - based Seq2Seq with fine-tuned hyperparameters ; Coverage is the attention - based Seq2Seq model with a coverage model ( Tu et al. , 2016 ) ; MemDec is the attention - based Seq2Seq model with the external memory .
For the English - Vietnamese translation , the models to be compared are presented below .
RNNSearch
The attention - based Seq2Seq model as mentioned above , and we present the results of ; NPMT is the Neural Phrase - based Machine Translation model by Huang et al . ( 2017 ) .
Results and Analysis
In the following , we present the experimental results as well as our analysis of temperature and case study .
Results
We present the performance of the baseline models and our model on the Chinese-English translation in Table 1 .
As to the recent models on the same task with the same training data , we extract their results from their original articles .
Compared with the baseline models , our model with the SACT for the softness of attention achieves
We present the results of the models on the English -Vietnamese translation in Table 2 . Compared with the attention - based Seq2Seq model , our model with the SACT can outperform it with a clear advantage of 2.17 BLEU score .
We also display the most recent model NPMT ( Huang et al. , 2017 ) trained and tested on the dataset .
Compared with NPMT , our model has an advantage of BLEU score of 1.43 .
It can be indicated that for low-resource translation , the information from the deconvolution - based decoder is important , which brings significant improvement to the conventional attention - based Seq2Seq model .
Analysis
In order to verify whether the automatically changing temperature can positively impact the performance of the model , we implement a series of models with fixed values , ranging from 0.8 to 1.2 , for the temperature parameter .
From the results shown in Figure1 , it can be found that the automatically changing temperature can encourage the model to outperform those with fixed temperature parameter .
Furthermore , as our model generates a temperature parameter at each time step of decoding , we present the heatmaps of two translations from the testing on the NIST 2003 for the Chinese-English translation on Figure 2 .
From the heatmaps , it can be found that the model can adapt the temperature parameter to the generation at the current time step .
In Figure 2 ( a ) , when translating words such as " to " and " from " , which are syntactic -relevant prepositions and both lack direct corresponding words in the source text or pronoun such as " they " , whose corresponding word " tamen " in the source may be a part of the possessive case or the objective case , the temperature parameter increases to soften the attention distribution so that the model can attend to more relevant elements for accurate extraction of the information from the source-side contexts .
On the contrary , when translating content words or phrases such as " pay attention " and " nuclear " , where there are direct corresponding words " zhuyi " and " hezi " in the source text , the temperature decreases to harden the attention distribution so that the model can focus on the corresponding information in the source text for accurate translation .
In Figure 2 ( b ) , the temperature parameters for the punctuations are high as they are highly connected to the syntactic structure and those for the content words with concrete correspondences such as location " paris " , name of organization " xinhua " , name of person " wang " and nationality " french " .
Case Study
We present two examples of the translation of our model in comparison with the translation of the conventional attention - based Seq2Seq model and the golden translation .
In Table 3 ( a ) , it can be found that the translation of the conventional Seq2Seq model does not give enough credit to the word " chengzhang " ( meaning " growth " ) , while our model can not only concentrate on the word but also recognize the word as a noun ( " chengzhang " in Chinese can be both noun and verb ) .
Even compared with the golden translation , the translation of our model seems better , which is a grammatical and coherent sentence .
In translation about the increase in the crude oil , it wrongly connects the increase with the threat of war in Iraq .
In contrast , as our model has more capability of analyzing the syntactic structure by softening the attention distribution in the generation of syntax - relevant words , it extracts the causal relationship in the source text and generates the correct translation .
Related Work Most systems for Neural Machine Translation are based on the sequence- to-sequence model ( Seq2Seq ) ( Sutskever et al. , 2014 ) , which is an encoder-decoder framework ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ) .
To improve NMT , a significant mechanism for the Seq2Seq model is the attention mechanism Vaswani et al . ( 2017 ) applied the fully - attention - based model to NMT and achieved the state - of - the - art performance .
To further evaluate the effect of our attention temperature mechanism , we will implement it to the " Transformer " model in the future .
Besides , the studies on the at-Source : ? ? ? ? ? ? ?
Gold : growth of mobile phone users in mainland china to slow down Seq2Seq : mainland cell phone users slow down SACT : the growth of cell phone users in chinese mainland will slow down ( a ) Source : ? ? 12 ? , ? ? ? ? ? ? ? ? , ? ? ? ? ? ? ?
Gold : since december last year , the price of crude oil on the international market has kept rising due to the general strike in venezuela and the threat of war in iraq .
Seq2Seq : since december last year , the international market has continued to rise in the international market and the threat of the iraqi war has continued to rise .
SACT : since december last year , the international market of crude oil has continued to rise because of the strike in venezuela and the war in iraq . tention mechanism have also contributed to some other tasks ( Lin et al. , 2018 b ; Liu et al. , 2018 )
Beyond the attention mechanism , there are also important methods for the Seq2Seq that contribute to the improvement of NMT .
Ma et al. ( 2018 ) incorporates the information about the bag-of-words of the target for adapting to multiple translations , and Lin et al . ( 2018 c ) takes the target context into consideration .
Conclusion and Future Work
In this paper , we propose a novel mechanism for the control over the scope of attention so that the softness of the attention distribution can be changed adaptively .
Experimental results demonstrate that the model outperforms the baseline models , and the analysis shows that our temperature parameter can change automatically when decoding diverse words .
In the future , we hope to find out more patterns and generalized rules to explain the model 's learning of the temperature .
Figure 1 : 1 Figure 1 : BLEU scores of the Seq2Seq models with fixed values for the temperature parameter .
Models are tested on the test set of the English - Vietnamese translation .
