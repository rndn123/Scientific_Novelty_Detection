title
Smaller Alignment Models for Better Translations : Unsupervised Word Alignment with the â„“ 0 - norm
abstract
Two decades after their invention , the IBM word - based translation models , widely available in the GIZA ++ toolkit , remain the dominant approach to word alignment and an integral part of many statistical translation systems .
Although many models have surpassed them in accuracy , none have supplanted them in practice .
In this paper , we propose a simple extension to the IBM models : an ?
0 prior to encourage sparsity in the word-to - word translation model .
We explain how to implement this extension efficiently for large-scale data ( also released as a modification to GIZA + + ) and demonstrate , in experiments on Czech , Arabic , Chinese , and Urdu to English translation , significant improvements over IBM Model 4 in both word alignment ( up to + 6.7 F1 ) and translation quality ( up to + 1.4 Bleu ) .
Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines .
Although state - of - the - art translation models use rules that operate on units bigger than words ( like phrases or tree fragments ) , they nearly always use word alignments to drive extraction of those translation rules .
The dominant approach to word alignment has been the IBM models ( Brown et al. , 1993 ) together with the HMM model ( Vogel et al. , 1996 ) .
These models are unsupervised , making them applicable to any language pair for which parallel text is available .
Moreover , they are widely disseminated in the open-source GIZA ++ toolkit ( Och and Ney , 2004 ) .
These properties make them the default choice for most statistical MT systems .
In the decades since their invention , many models have surpassed them in accuracy , but none has supplanted them in practice .
Some of these models are partially supervised , combining unlabeled parallel text with manually - aligned parallel text ( Moore , 2005 ; Taskar et al. , 2005 ; Riesa and Marcu , 2010 ) .
Although manually - aligned data is very valuable , it is only available for a small number of language pairs .
Other models are unsupervised like the IBM models ( Liang et al. , 2006 ; Grac ?a et al. , 2010 ; Dyer et al. , 2011 ) , but have not been as widely adopted as GIZA ++ has .
In this paper , we propose a simple extension to the IBM / HMM models that is unsupervised like the IBM models , is as scalable as GIZA ++ because it is implemented on top of GIZA + + , and provides significant improvements in both alignment and translation quality .
It extends the IBM / HMM models by incorporating an ?
0 prior , inspired by the principle of minimum description length ( Barron et al. , 1998 ) , to encourage sparsity in the word-to - word translation model ( Section 2.2 ) .
This extension follows our previous work on unsupervised part-ofspeech tagging ( Vaswani et al. , 2010 ) , but enables it to scale to the large datasets typical in word alignment , using an efficient training method based on projected gradient descent ( Section 2.3 ) .
Experiments on Czech- , Arabic- , Chinese - and Urdu-English translation ( Section 3 ) demonstrate consistent significant improvements over IBM Model 4 in both word alignment ( up to + 6.7 F1 ) and translation quality ( up to + 1.4 Bleu ) .
Our implementation has been released as a simple modification to the GIZA ++ toolkit that can be used as a drop- in replacement for GIZA + + in any existing MT pipeline .
Method
We start with a brief review of the IBM and HMM word alignment models , then describe how to extend them with a smoothed ?
0 prior and how to efficiently train them .
IBM Models and HMM
Given a French string f = f 1 ? ? ? f j ? ? ? f m and an English string e = e 1 ? ? ? e i ? ? ? e ? , these models describe the process by which the French string is generated by the English string via the alignment a = a 1 , . . . , a j , . . . , a m .
Each a j is a hidden variables , indicating which English word e a j the French word f j is aligned to .
In IBM Model 1 - 2 and the HMM model , the joint probability of the French sentence and alignment given the English sentence is P(f , a | e ) = m j=1 d( a j | a j?1 , j ) t ( f j | e a j ) .
( 1 )
The parameters of these models are the distortion probabilities d( a j | a j?1 , j ) and the translation probabilities t( f j | e a j ) .
The three models differ in their estimation of d , but the differences do not concern us here .
All three models , as well as IBM Models 3 - 5 , share the same t.
For further details of these models , the reader is referred to the original papers describing them ( Brown et al. , 1993 ; Vogel et al. , 1996 ) . Let ? stand for all the parameters of the model .
The standard training procedure is to find the parameter values that maximize the likelihood , or , equivalently , minimize the negative log-likelihood of the observed data : ? = arg min ? ? log P(f | e , ? ) ( 2 ) = arg min ? ? ? ? ? ? ? ? ? log a P(f , a | e , ? ) ? ? ? ? ? ? ? ( 3 ) This is done using the Expectation - Maximization ( EM ) algorithm ( Dempster et al. , 1977 ) .
MAP - EM with the ?
0 - norm Maximum likelihood training is prone to overfitting , especially in models with many parameters .
In word alignment , one well -known manifestation of overfitting is that rare words can act as " garbage collectors " ( Moore , 2004 ) , aligning to many unrelated words .
This hurts alignment precision and rule-extraction recall .
Previous attempted remedies include early stopping , smoothing ( Moore , 2004 ) , and posterior regularization ( Grac ?a et al. , 2010 ) .
We have previously proposed another simple remedy to overfitting in the context of unsupervised part- of-speech tagging ( Vaswani et al. , 2010 ) , which is to minimize the size of the model using a smoothed ?
0 prior .
Applying this prior to an HMM improves tagging accuracy for both Italian and English .
Here , our goal is to apply a similar prior in a word-alignment model to the word- to- word translation probabilities t( f | e ) .
We leave the distortion models alone , since they are not very large , and there is not much reason to believe that we can profit from compacting them .
With the addition of the ?
0 prior , the MAP ( maximum a posteriori ) objective function is ?
= arg min ? ? log P (f | e , ? ) P ( ? ) ( 4 ) where P ( ? ) ? exp ? ? ? 0 ( 5 ) and ? ?
0 = e , f 1 ? exp ?t( f | e ) ? ( 6 ) is a smoothed approximation of the ?
0 - norm .
The hyperparameter ?
controls the tightness of the approximation , as illustrated in Figure 1 . Substituting back into ( 4 ) and dropping constant terms , we get the following optimization problem : minimize ? log P(f | e , ? ) ? ?
e , f exp ?t( f | e ) ? ( 7 ) subject to the constraints f t( f | e ) = 1 for all e. ( 8 ) We can carry out the optimization in ( 7 ) with the MAP -EM algorithm ( Bishop , 2006 ) . EM and MAP -EM share the same E-step ; the difference lies in the M-step .
For vanilla EM , the M-step is : ? = arg min ? ? ? ? ? ? ? ? ? ? ? e , f E [ C( e , f ) ] log t( f | e ) ? ? ? ? ? ? ? ? ? ( 9 ) again subject to the constraints ( 8 ) .
The count C(e , f ) is the number of times that f occurs aligned to e.
For MAP - EM , it is : ? = arg min ? ? e , f E [ C( e , f ) ] log t( f | e ) ? ? e , f exp ?t ( f | e ) ? ( 10 )
This optimization problem is non-convex , and we do not know of a closed - form solution .
Previously ( Vaswani et al. , 2010 ) , we used ALGENCAN , a nonlinear optimization toolkit , but this solution does not scale well to the number of parameters involved in word alignment models .
Instead , we use a simpler and more scalable method which we describe in the next section .
Projected gradient descent Following Schoenemann ( 2011 b ) , we use projected gradient descent ( PGD ) to solve the M-step ( but with the ?
0 - norm instead of the ? 1 - norm ) .
Gradient projection methods are attractive solutions to constrained optimization problems , particularly when the constraints on the parameters are simple ( Bertsekas , 1999 ) .
Let F ( ? ) be the objective function in ( 10 ) ; we seek to minimize this function .
As in previous work ( Vaswani et al. , 2010 ) , we optimize each set of parameters { t (? | e ) } separately for each English word type e.
The inputs to the PGD are the expected counts E [ C ( e , f ) ] and the current word-toword conditional probabilities ?.
We run PGD for K iterations , producing a sequence of intermediate parameter vectors ?
1 , . . . , ? k , . . . , ? K .
Each iteration has two steps , a projection step and a line search .
Projection step
In this step , we compute : ? k = ? k ? s?F ( ? k ) ? ( 11 )
This moves ? in the direction of steepest descent ( ? F ) with step size s , and then the function [ ? ] ? projects the resulting point onto the simplex ; that is , it finds the nearest point that satisfies the constraints ( 8 ) .
The gradient ?F (? k ) is ?F ?t( f | e ) = ? E [ C ( f , e ) ] t( f | e ) + ? ? exp ?t ( f | e ) ? ( 12 )
In contrast to Schoenemann ( 2011 b ) , we use an O( n log n ) algorithm for the projection step due to Duchi et. al. ( 2008 ) , shown in Pseudocode 1 .
Pseudocode 1 Project input vector u ?
R n onto the probability simplex .
v = u sorted in non-decreasing order ?
= 0 for i = 1 to n do if v i ?
1 i i r=1 v r ? 1 > 0 then ?
= i end if end for ? = 1 ? ? r=1 v r ?
1 w r = max{v r ? ? , 0 } for 1 ? r ? n return w Line search Next , we move to a point between ? k and ? k that satisfies the Armijo condition , F (? k + ? m ) ? F (? k ) + ? ?F (? k ) ? ? m ( 13 ) where ? m = ? m (? k ? ? k ) and ? and ? are both constants in ( 0 , 1 ) .
We try values m = 1 , 2 , . . . until the Armijo condition ( 13 ) is satisfied or the limit m = 20 Pseudocode 2 Find a point between ? k and ? k that satisfies the Armijo condition .
F min = F (? k ) ? min = ? k for m = 1 to 20 do ? m = ? m ? k ? ? k if F (? k + ? m ) < F min then F min = F (? k + ? m ) ? min = ? k + ? m end if if F (? k + ? m ) ? F (? k ) + ? ?F (? k ) ? ? m then break end if end for ?
k+1 = ? min return ? k+ 1 is reached .
( Note that we do n't allow m = 0 because this can cause ?
k + ? m to land on the boundary of the probability simplex , where the objective function is undefined . )
Then we set ?
k+ 1 to the point in {? k } ? {? k + ? m | 1 ? m ? 20 } that minimizes F . The line search algorithm is summarized in Pseudocode 2 .
In our implementation , we set ? = 0.5 and ? = 0.5 .
We keep s fixed for all PGD iterations ; we experimented with s ? { 0.1 , 0.5 } and did not observe significant changes in F-score .
We run the projection step and line search alternately for at most K iterations , terminating early if there is no change in ? k from one iteration to the next .
We set K = 35 for the large Arabic- English experiment ; for all other conditions , we set K = 50 .
These choices were made to balance efficiency and accuracy .
We found that values of K between 30 and 75 were generally reasonable .
Experiments
To demonstrate the effect of the ?
0 - norm on the IBM models , we performed experiments on four translation tasks : Arabic-English , Chinese-English , and Urdu-English from the NIST Open MT Evaluation , and the Czech-English translation from the Workshop on Machine Translation ( WMT ) shared task .
We measured the accuracy of word alignments generated by GIZA ++ with and without the ?
0 -norm , and also translation accuracy of systems trained using the word alignments .
Across all tests , we found strong improvements from adding the ?
0 - norm .
Training
We have implemented our algorithm as an opensource extension to GIZA ++.
1 Usage of the extension is identical to standard GIZA + + , except that the user can switch the ?
0 prior on or off , and adjust the hyperparameters ? and ?.
For vanilla EM , we ran five iterations of Model 1 , five iterations of HMM , and ten iterations of Model 4 .
For our approach , we first ran one iteration of Model 1 , followed by four iterations of Model 1 with smoothed ?
0 , followed by five iterations of HMM with smoothed ?
0 . Finally , we ran ten iterations of Model 4 .
2 We used the following parallel data : ? Chinese-English : selected data from the constrained task of the NIST 2009 Open MT Evaluation .
3 ? Arabic-English : all available data for the constrained track of NIST 2009 , excluding United Nations proceedings ( LDC2004E13 ) , ISI Automatically Extracted Parallel Text ( LDC2007E08 ) , and Ummah newswire text ( LDC2004T18 ) , for a total of 5.4+ 4.3 million words .
We also experimented on a larger Arabic- English parallel text of 44 + 37 million words from the DARPA GALE program .
?
Urdu-English : all available data for the constrained track of NIST 2009 .
1 The code can be downloaded from the first author 's website at http://www.isi.edu/ ?avaswani/giza-pp-l0.html. 2 GIZA ++ allows changing some heuristic parameters for efficient training .
Currently , we set two of these to zero : mincountincrease and probcutoff .
In the default setting , both are set to 10 ?7 .
We set probcutoff to 0 because we would like the optimization to learn the parameter values .
For a fair comparison , we applied the same setting to our vanilla EM training as well .
To test , we ran GIZA ++ with the default setting on the smaller of our two Arabic- English datasets with the same number of iterations and found no change in F-score . - , in ( c ) ) .
In particular , the baseline system demonstrates typical " garbage -collection " phenomena in proper name " shuqing " in both languages in ( a ) , number " 4000 " and word " l?ib ? n " ( lit. " guest " ) in ( b ) , word " troublesome " and " l?l ? " ( lit. " land-route " ) in ( c ) , and " blockhouses " and " di?ob ? o " ( lit. " bunker " ) in ( d ) .
We found this garbage-collection behavior to be especially common with proper names , numbers , and uncommon words in both languages .
Most interestingly , in ( c ) , our smoothed -?
0 system correctly aligns " extremely " to " h?n h?n h?n h?n " ( lit .
" very very very very " ) which is rare in the bitext .
1 : Adding the ?
0 - norm to the IBM models improves both alignment and translation accuracy across four different language pairs .
The word trans column also shows that the number of distinct word translations ( i.e. , the size of the lexical weighting table ) is reduced .
The ?sing .
column shows the average fertility of once-seen source words .
For Czech - English , the year refers to the WMT shared task ; for all other language pairs , the year refers to the NIST Open MT Evaluation .
*
Half of this test set was also used for tuning feature weights .
? zh? ? le ? s?ge ? di?ob?o ? . ( c ) ( d ) ?
A corpus of 4 million words of Czech-English data from the News Commentary corpus .
4
We set the hyperparameters ? and ? by tuning on gold-standard word alignments ( to maximize F1 ) when possible .
For Arabic-English and Chinese- English , we used 346 and 184 hand - aligned sentences from LDC2006E86 and LDC2006E93 .
Similarly , for Czech - English , 515 hand - aligned sentences were available ( Bojar and Prokopov ? , 2006 ) .
But for Urdu-English , since we did not have any gold alignments , we used ? = 10 and ? = 0.05 .
We did not choose a large ? , as the dataset was small , and we chose a conservative value for ?.
We ran word alignment in both directions and symmetrized using grow-diag-final ( Koehn et al. , 2003 ) .
For models with the smoothed ?
0 prior , we tuned ? and ? separately in each direction .
Alignment First , we evaluated alignment accuracy directly by comparing against gold -standard word alignments .
The results are shown in the alignment F1 column of Table 1 .
We used balanced F-measure rather than alignment error rate as our metric ( Fraser and Marcu , 2007 ) .
Following Dyer et al. ( 2011 ) , we also measured the average fertility , ?sing . , of once-seen source words in the symmetrized alignments .
Our alignments show smaller fertility for once-seen words , suggesting that they suffer from " garbage collection " effects less than the baseline alignments do .
The fact that we had to use hand -aligned data to tune the hyperparameters ? and ? means that our method is no longer completely unsupervised .
However , our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters , as shown in Table 2 .
As we will see below , we still obtained strong improvements in translation quality when hand -aligned data was unavailable .
We also tried generating 50 word classes using the tool provided in GIZA ++.
We found that adding word classes improved alignment quality a little , but more so for the baseline system ( see Table 3 ) .
We used the alignments generated by training with word classes for our translation experiments .
Table 3 : Adding word classes improves the F-score in both directions for Arabic- English alignment by a little , for the baseline system more so than ours .
Figure 2 shows four examples of Chinese - English alignment , comparing the baseline with our smoothed -?
0 method .
In all four cases , the baseline produces incorrect extra alignments that prevent good translation rules from being extracted while the smoothed -?
0 results are correct .
In particular , the baseline system demonstrates typical " garbage collection " behavior ( Moore , 2004 ) in all four examples .
Translation
We then tested the effect of word alignments on translation quality using the hierarchical phrasebased translation system Hiero ( Chiang , 2007 ) .
We used a fairly standard set of features : seven inherited from Pharaoh ( Koehn et al. , 2003 ) ond language model , and penalties for the glue rule , identity rules , unknown - word rules , and two kinds of number / name rules .
The feature weights were discriminatively trained using MIRA ( Chiang et al. , 2008 ) .
We used two 5 - gram language models , one on the combined English sides of the NIST 2009 Arabic-English and Chinese - English constrained tracks ( 385 M words ) , and another on 2 billion words of English .
For each language pair , we extracted grammar rules from the same data that were used for word alignment .
The development data that were used for discriminative training were : for Chinese -English and Arabic- English , data from the NIST 2004 and NIST 2006 test sets , plus newsgroup data from the GALE program ( LDC2006E92 ) ; for Urdu-English , half of the NIST 2008 test set ; for Czech - English , a training set of 2051 sentences provided by the WMT10 translation workshop .
The results are shown in the Bleu column of Table 1 .
We used case-insensitive IBM Bleu ( closest reference length ) as our metric .
Significance testing was carried out using bootstrap resampling with 1000 samples ( Koehn , 2004 ; Zhang et al. , 2004 ) .
All of the tests showed significant improvements ( p < 0.01 ) , ranging from + 0.4 Bleu to + 1.4 Bleu .
For Urdu , even though we did n't have manual alignments to tune hyperparameters , we got significant gains over a good baseline .
This is promising for languages that do not have any manually aligned data .
Ideally , one would want to tune ? and ? to maximize Bleu .
However , this is prohibitively expensive , especially if we must tune them separately in each alignment direction before symmetrization .
We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation quality .
For the smaller Arabic- English corpus , we symmetrized all combinations of the two top-scoring alignments ( according to F1 ) in each direction , yielding four sets of alignments .
Table 4 shows Bleu scores for translation models learned from these alignments .
Unfortunately , we find that optimizing F1 is not optimal for Bleu-using the second - best alignments yields a further improvement of 0.5 Bleu on the NIST 2009 data , which is statistically significant ( p < 0.05 ) .
Related Work Schoenemann ( 2011a ) , taking inspiration from Bodrumlu et al . ( 2009 ) , uses integer linear programming to optimize IBM Model 1 - 2 and the HMM with the ?
0 - norm .
This method , however , does not outperform GIZA ++.
In later work , Schoenemann ( 2011 b ) used projected gradient descent for the ?
1 norm .
Here , we have adopted his use of projected gradient descent , but using a smoothed ?
0 - norm .
Liang et al. ( 2006 ) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions .
Grac ?a et al. ( 2010 ) explore modifications to the HMM model that encourage bijectivity and symmetry .
The modifications take the form of constraints on the posterior distribution over alignments that is computed during the E-step .
Mermer and Sarac ?lar ( 2011 ) explore a Bayesian version of IBM Model 1 , applying sparse Dirichlet priors to t.
However , because this method requires the use of Monte Carlo methods , it is not clear how well it can scale to larger datasets .
Conclusion
We have extended the IBM models and HMM model by the addition of an ?
0 prior to the word-to - word translation model , which compacts the word-toword translation table , reducing overfitting , and , in particular , the " garbage collection " effect .
We have shown how to perform MAP - EM with this prior efficiently , even for large datasets .
The method is implemented as a modification to the open-source toolkit GIZA + + , and we have shown that it significantly improves translation quality across four different language pairs .
Even though we have used a small set of gold-standard alignments to tune our hyperparameters , we found that performance was fairly robust to variation in the hyperparameters , and translation performance was good even when goldstandard alignments were unavailable .
We hope that our method , due to its simplicity , generality , and effectiveness , will find wide application for training better statistical translation systems .
Figure 1 : 1 Figure 1 : The ? 0 - norm ( top curve ) and smoothed approximations ( below ) for ? = 0.05 , 0.1 , 0.2 .
