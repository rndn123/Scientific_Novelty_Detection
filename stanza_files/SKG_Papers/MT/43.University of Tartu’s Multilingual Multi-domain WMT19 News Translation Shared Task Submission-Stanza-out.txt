title
University of Tartu's Multilingual Multi-domain WMT19 News Translation Shared Task Submission
abstract
This paper describes the University of Tartu 's submission to the news translation shared task of WMT19 , where the core idea was to train a single multilingual system to cover several language pairs of the shared task and submit its results .
We only used the constrained data from the shared task .
We describe our approach and its results and discuss the technical issues we faced .
Introduction
Typically the majority of WMT news translation shared task submissions are based on language pair-specific machine translation ( MT ) systems ( Bojar et al. , 2016 ( Bojar et al. , , 2017 ( Bojar et al. , , 2018 .
However , recently several multilingual approaches to MT have been proposed ( e.g. Johnson et al. , 2017 ; V?zquez et al. , 2018 ; Aharoni et al. , 2019 ) .
With them as inspiration , the goal of this paper is to describe our submission to the WMT ' 2019 news translation shared task , where we trained a single multilingual translation system using the constrained parallel and monolingual data for several language pairs .
In addition to multilinguality we wanted to incorporate the multiple text domains that constitute the constrained set of parallel corpora in the WMT shared task .
We approach multi-domain NMT using the method of ( Tars and Fishel , 2018 ) : namely , by treating domains as separate languages , therefore creating a " double-multilingual " system .
In addition to multilinguality and multi-domain NMT our submission has more common features , like data filtering , ensembles of several models and fine-tuning on back - translated monolingual data .
Below we describe the architecture of our approach in Section 2 , experimental setup in Sec-tion 3 , results and analysis in Section 4 and conclude the paper in Section 5 .
Architecture
Our model is a neural MT system based on autoregressive self-attention in the encoder and decoder ( Vaswani et al. , 2017 ) .
We achieve multilinguality in a similar fashion to ( Johnson et al. , 2017 ) : using an additional input specifying the output language , so that the system would know which language to generate .
Differently from Johnson et al. ( 2017 ) , who include the output language into the input segment itself , we use word factors ( Hieber et al. , 2017 ) and specify the output language as a factor of each input token .
In addition to multilinguality , our NMT system also uses the information on which domain the parallel or monolingual corpora come from .
The WMT data consist of a variety of text domains ( parliamentary speeches , crawled web and news texts , press releases , Wikipedia titles , etc. ) and it has been shown ( Tars and Fishel , 2018 ) that multidomain NMT can get much better results than the default approach of mixing heterogeneous corpora together , as well as yield more efficient solutions than fine-tuning to each domain separately .
Our solution is to specify the output text domain as another word factor .
One peculiarity of multilingual NMT is that the model performs back -translation for itself , therefore avoiding the necessity of training more than one translation system .
Experiments
Model Setup
We use the Sockeye ( Hieber et al. , 2017 ) implements word factors together with the Transformer .
We use traditional transformer NMT architecture with 6 layers for both encoder and decoder , with the transformer model size 1024 , transformer attention heads 16 , batch size 6000 , with a shared byte-pair encoded ( BPE ) ( Sennrich et al. , 2015 ) vocabulary of size 90000 .
SentencePiece 1 are used to extract BPE vocabulary .
The embedding size for source factors is 8 .
There are 6 different language factors and 4 different domain factors .
All other parameters were kept as default .
Models are trained on 4 Tesla V100 GPUs .
Data
All of the available WMT constrained data for all languages was downloaded and then fed through a data pipeline .
The data pipeline consisted of 6 steps : 1 . Filtering Data filtering included several steps : it filtered out empty / too long sentences , sentences with too many nonalphanumeric characters , sentences where the length difference was too big , and also sentences automatically identified as a different language than the expected one .
Tokenization
The data was tokenized with MosesTokenizer .
3 . Truecasing A
Truecasing model was trained for every language separately , then applied on all the data .
4 . SentencePiece A SentencePiece model was trained on one big text file which included all data , low-resource language pairs like EN - LT were upscaled and high- resource language pairs like CZ - EN were downscaled .
In total 50 M lines of text were used for Senten-cePiece model with vocabulary size 90K .
1 https://github.com/google/ sentencepiece 5 . Factoring
Then the source factors for target domain and target language were generated for all data .
6 . Sharding Sockeye uses shards to handle massive datasets , which means that a big dataset is divided into more manageable dataset sizes .
Each shard is of equal size .
A shard size of 1 M was used .
Due to time constraints we deviated from the original plan of including all WMT ' 2019 language pairs and only included languages that use the Latin script in our submissions .
The final data set sizes are shown in Table 1 .
In order to generate the domain factors we grouped some of the domains by the apparent similarity of texts , additionally grouping smaller corpora together : ?
News - Rapid2019 , Rapid2016 , EESC , dev dataset from previous years , EMEA2016 , ECB2017 , news ( from CzEng ) , Newscommentary ?
Subs - Subtitles from the CzEng corpus ?
Off - Parts of the CzEng corpus , Europarl ?
Other - Everything else Additionally , monolingual data was extracted for back -translation and fine-tuning , mainly News Crawl corpora was used .
For every language pair 3 M sentences were extracted , with the exception of Lithuanian , where the news crawl size is smaller , and thus other monolingual data like Wiki dumps and Europarl were used .
Results and Analysis Results are presented in Table 2 .
We separate the results of our baseline system , trained on parallel data only , and the fine-tuned system that was trained further on monolingual data , backtranslated by the baseline system .
Our baseline performed reasonably well , however the goal was to achieve state - of - the - art results after doing fine -tuning on back - translated news data .
As a result of this second step unexpectedly the model started confusing the output language and generating the output in a different language than requested : for example generating Czech or English instead of Finnish .
Automatic language identification with FastText 2 shows the baseline model only produced output in the wrong language in 1.22 % of cases , whereas after just a day of fine-tuning on in- domain data , the percentage of translations our model got wrong jumped up to 60. 24 % .
Mostly our ensemble model got English right and other languages wrong .
Our ensemble model was done by using 2 snapshots of baseline model and 2 snapshots of fine-tuned model .
For human evaluations published in ( Bojar et al. , 2019 ) our model ( called TartuNLP -c ) performed similarly to other multilingual systems noted as Online -X in the findings paper .
Online systems are freely available online systems like Google Translate , Bing Translate etc .
Our models performed worse than single language pair NMT systems .
We suspect that the reason for the wrong language output lies in two factors : ? wrong language segments in monolingual crawled data .
This mainly occurs in non-English languages like Czech , Finnish and Lithuanian and affects the output side of back - translated data .
Before the submission deadline we did not have language - filtering in the data preparation pipeline , which might have caused this effect .
? wrong language output by our model .
This affects the input side of the back - translated data .
While this does not occur often , filtering out the wrong - language translations should still help learn a more precise translation model .
We are investigating alternative explanations to this behavior further .
Conclusions and Future Work
We have described a multilingual multi-domain neural machine translation approach that can be trained on a mixture of different language pairs and text domains .
Our results are modest , mainly due to failing to properly fine - tune the systems on back - translated news texts .
Precise reasons for failing the finetuning are under investigation .
Other future work includes including more languages and domains , testing online continuous back -translation and experimenting with other ways of providing the output language and domain information to the NMT model .
Table 1 : 1 machine translation framework for our experiments .
The main reason behind this choice is that Sockeye Dataset sizes after filtering .
Shown number of parallel sentences .
CZ-EN DE-EN DE-FR EN -FI EN-LT TOTAL NEWS 2534352 5985498 4372033 2656508 1803323 17351714 OFF
11462432 1797854 1687074 1725792 615219 17288371 SUBS 37251088 - - - - 37251088 OTHER 10932478 34457911 7585341 4012589 1290931 58279250 TOTAL 62180350 42241263 13644448 8394889 3709473 130170423
Table 2 : 2 Results of our multilingual baseline model , trained on parallel data and the fine-tuned model that was further trained on back - translated monolingual data .
Baseline Fine-tune EN -CS 22.8 - DE-EN 29.9 - EN-DE 39.6 - DE-FR 32.4 30.7 EN -FI 18.6 - EN-LT 12.7 - FI-EN 22.1 24.8 FR -DE 25.9 - LT-EN 24.5 25.3
Table 3 : 3 Number of sentences which are classified as having a wrong language after translation using the FastText language classifier .
