title
Inducing a Discriminative Parser to Optimize Machine Translation Reordering
abstract
This paper proposes a method for learning a discriminative parser for machine translation reordering using only aligned parallel text .
This is done by treating the parser 's derivation tree as a latent variable in a model that is trained to maximize reordering accuracy .
We demonstrate that efficient large-margin training is possible by showing that two measures of reordering accuracy can be factored over the parse tree .
Using this model in the pre-ordering framework results in significant gains in translation accuracy over standard phrasebased SMT and previously proposed unsupervised syntax induction methods .
Introduction
Finding the appropriate word ordering in the target language is one of the most difficult problems for statistical machine translation ( SMT ) , particularly for language pairs with widely divergent syntax .
As a result , there is a large amount of previous research that handles the problem of reordering through the use of improved reordering models for phrase - based SMT , hierarchical phrase - based translation ( Chiang , 2007 ) , syntax - based translation ( Yamada and Knight , 2001 ) , or preordering ( Xia and McCord , 2004 ) .
In particular , systems that use sourcelanguage syntax allow for the handling of longdistance reordering without large increases in The first author is now affiliated with the Nara Institute of Science and Technology .
decoding time .
However , these require a good syntactic parser , which is not available for many languages .
In recent work , DeNero and Uszkoreit ( 2011 ) suggest that unsupervised grammar induction can be used to create source-sentence parse structure for use in translation as a part of a pre-ordering based translation system .
In this work , we present a method for inducing a parser for SMT by training a discriminative model to maximize reordering accuracy while treating the parse tree as a latent variable .
As a learning framework , we use online large-margin methods to train the model to directly minimize two measures of reordering accuracy .
We propose a variety of features , and demonstrate that learning can succeed when no linguistic information ( POS tags or parse structure ) is available in the source language , but also show that this linguistic information can be simply incorporated when it is available .
Experiments find that the proposed model improves both reordering and translation accuracy , leading to average gains of 1.2 BLEU points on English - Japanese and Japanese - English translation without linguistic analysis tools , or up to 1.5 BLEU points when these tools are incorporated .
In addition , we show that our model is able to effectively maximize various measures of reordering accuracy , and that the reordering measure that we choose has a direct effect on translation results .
Preordering for SMT Machine translation is defined as transformation of source sentence F = f 1 . . . f J to target sentence E = e 1 . . . e I .
In this paper , we take Figure 1 : An example with a source sentence F reordered into target order F , and its corresponding target sentence E. D is one of the BTG derivations that can produce this ordering .
the pre-ordering approach to machine translation ( Xia and McCord , 2004 ) , which performs translation as a two step process of reordering and translation ( Figure 1 ) .
Reordering first deterministically transforms F into F , which contains the same words as F but is in the order of E. Translation then transforms F into E using a method such as phrase - based SMT ( Koehn et al. , 2003 ) , which can produce accurate translations when only local reordering is required .
This general framework has been widely studied , with the majority of works relying on a syntactic parser being available in the source language .
Reordering rules are defined over this parse either through machine learning techniques ( Xia and McCord , 2004 ; Li et al. , 2007 ; Genzel , 2010 ; Dyer and Resnik , 2010 ; Khalilov and Sima'an , 2011 ) or linguistically motivated manual rules ( Collins et al. , 2005 ; Xu et al. , 2009 ; Carpuat et al. , 2010 ; Isozaki et al. , 2010 b ) .
However , as building a parser for each source language is a resourceintensive undertaking , there has also been some interest in developing reordering rules without the use of a parser ( Rottmann and Vogel , 2007 ; Tromble and Eisner , 2009 ; DeNero and Uszkoreit , 2011 ; Visweswariah et al. , 2011 ) , and we will follow this thread of research in this paper .
In particular , two methods deserve mention for being similar to our approach .
First , DeNero and Uszkoreit ( 2011 ) learn a reordering model through a three -step process of bilingual grammar induction , training a monolingual parser to reproduce the induced trees , and training a reordering model that selects a reordering based on this parse structure .
In contrast , our method trains the model in a single step , treating the parse structure as a latent variable in a discriminative reordering model .
In addition Tromble and Eisner ( 2009 ) and Visweswariah et al . ( 2011 ) present models that use binary classification to decide whether each pair of words should be placed in forward or reverse order .
In contrast , our method uses traditional contextfree - grammar models , which allows for simple parsing and flexible parameterization , including features such as those that utilize the existence of a span in the phrase table .
Our work is also unique in that we show that it is possible to directly optimize several measures of reordering accuracy , which proves important for achieving good translations .
1
Training a Reordering Model with Latent Derivations
In this section , we provide a basic overview of the proposed method for learning a reordering model with latent derivations using online discriminative learning .
Space of Reorderings
The model we present here is based on the bracketing transduction grammar ( BTG , Wu ( 1997 ) ) framework .
BTGs represent a binary tree derivation D over the source sentence F as shown in Figure 1 . Each non-terminal node can either be a straight ( str ) or inverted ( inv ) production , and terminals ( term ) span a nonempty substring f .
2
The ordering of the sentence is determined by the tree structure and the non-terminal labels str and inv , and can be built bottom - up .
Each subtree represents a source substring f and its reordered counterpart f .
For each terminal node , no reordering occurs and f is equal to f .
For each non-terminal node spanning f with its left child spanning f 1 and its right child spanning f 2 , if the non-terminal symbol is str , the reordered strings will be concatenated in order as f = f 1 f 2 , and if the non-terminal symbol is inv , the reordered strings will be concatenated in inverted order as f = f 2 f 1 .
We define the space of all reorderings that can be produced by the BTG as F , and attempt to find the best reordering F within this space .
3
Reorderings with Latent Derivations
In order to find the best reordering F given only the information in the source side sentence F , we define a scoring function S( F | F ) , and choose the ordering of maximal score : ? = arg max F S ( F | F ) .
As our model is based on reorderings licensed by BTG derivations , we also assume that there is an underlying derivation D that produced F .
As we can uniquely determine F given F and D , we can define a scoring function S( D|F ) over derivations , find the derivation of maximal score ?
= arg max D S( D|F ) and use ? to transform F into F . Furthermore , we assume that the score S( D|F ) is the weighted sum of a number of feature functions defined over D and F S( D|F , w ) = ?
i w i ?
i ( D , F ) where ?
i is the ith feature function , and w i is its corresponding weight in weight vector w .
Given this model , we must next consider how to learn the weights w .
As the final goal of our model is to produce good reorderings F , it is natural to attempt to learn weights that will allow us to produce these high-quality reorderings .
Evaluating Reorderings
Before we explain the learning algorithm , we must know how to distinguish whether the F produced by the model is good or bad .
This section explains how to calculate oracle reorderings , and assign each F a loss and an accuracy according to how well it reproduces the oracle .
Calculating Oracle Orderings
In order to calculate reordering quality , we first define a ranking function r(f j |F , A ) , which indicates the relative position of source word f j in the proper target order ( Figure 2 ( a ) ) .
In order to calculate this ranking function , we define A = a 1 , . . . , a J , where each a j is a set of the indices of the words in E to which f j is aligned .
4 Given these alignments , we define an ordering function a j 1 < a j 2 that indicates that the indices in a j 1 come before the indices in a j 2 .
Formally , we define this function as " the first index in a j 1 is at most the first index in a j 2 , similarly for the last index , and either the first or last index in a j 1 is less than that of a j 2 . "
Given this ordering , we can sort every alignment a j , and use its relative position in the sentence to assign a rank to its word r( f j ) .
In the case of ties , where neither a j 1 < a j 2 nor a j 2 < a j 1 , both f j 1 and f j 2 are assigned the same rank .
We can now define measures of reordering accuracy for F by how well it arranges the words in order of ascending rank .
It should be noted that as we allow ties in rank , there are multiple possible F where all words are in strictly ascending order , which we will call oracle orderings .
Kendall's ?
The first measure of reordering accuracy that we will consider is Kendall 's ?
( Kendall , 1938 ) , a measure of pairwise rank correlation which has been proposed for evaluating translation reordering accuracy ( Isozaki et al. , 2010a ; Birch et al. , 2010 ) and pre-ordering accuracy .
The fundamental idea behind the measure lies in comparisons between each pair of elements f j 1 and f j 2 of the reordered sentence , where j 1 < j 2 . Because j 1 < j 2 , f j 1 comes before f j 2 in the reordered sentence , the ranks should be r(f j 1 ) ? r( f j 2 ) in order to produce the correct ordering .
Based on this criterion , we first define a loss L t ( F ) that will be higher for orderings that are further from the oracle .
Specifically , we take the sum of all pairwise orderings that do not follow the expected order L t ( F ) = J?1 ? j 1 =1 J ? j 2 =j 1 +1 ?( r( f j 1 ) > r( f j 2 ) ) where ?(? ) is an indicator function that is 1 when its condition is true , and 0 otherwise .
An example of this is given in Figure 2 ( b ) .
To calculate an accuracy measure for ordering F , we first calculate the maximum loss for the sentence , which is equal to the total number of non-equal rank comparisons in the sentence 5 max F L t ( F ) = J?1 ? j 1 =1 J ? j 2 =j 1 +1 ?( r( f j 1 ) = r( f j 2 ) ) .
( 1 ) 5
The traditional formulation of Kendall 's ?
assumes no ties in rank , and thus the maximum loss can be calculated as J ( J ? 1 ) / 2 .
Finally , we use this maximum loss to normalize the actual loss to get an accuracy A t ( F ) = 1 ? L t ( F ) max F L t ( F ) , which will take a value between 0 ( when F has maximal loss ) , and 1 ( when F matches one of the oracle orderings ) .
In Figure 2 ( b ) , L t ( F ) = 2 and max F L t ( F ) = 8 , so A t ( F ) = 0.75 .
Chunk Fragmentation
Another measure that has been used in evaluation of translation accuracy ( Banerjee and Lavie , 2005 ) and pre-ordering accuracy ) is chunk fragmentation .
This measure is based on the number of chunks that the sentence needs to be broken into to reproduce the correct ordering , with a motivation that the number of continuous chunks is equal to the number of times the reader will have to jump to a different position in the reordered sentence to read it in the target order .
One way to measure the number of continuous chunks is considering whether each word pair f j and f j+1 is discontinuous ( the rank of f j+1 is not equal to or one greater than f j ) discont ( f j , f j +1 ) = ?( r( f j ) = r( f j +1 ) ? r( f j ) + 1 = r( f j +1 ) ) and sum over all word pairs in the sentence to create a sentence - based loss L c ( F ) = J?1 ? j=1 discont ( f j , f j + 1 ) ( 2 )
While this is the formulation taken by previous work , we found that this under-penalizes bad reorderings of the first and last words of the sentence , which can contribute to the loss only once , as opposed to other words which can contribute to the loss twice .
To account for this , when calculating the chunk fragmentation score , we additionally add two sentence boundary words f 0 and f J+ 1 with ranks r( f 0 ) = 0 and r(f J+ 1 ) = 1 + max f j ? F r( f j ) and redefine the summation in Equation ( 2 ) to consider these words ( e.g. Figure 2 ( c ) ) .
Similarly to Kendall's ? , we can also define an accuracy measure between 0 and 1 using the maximum loss , which will be at most J + 1 , which corresponds to the total number of comparisons made in calculating the loss 6 A c ( F ) = 1 ? L c ( F ) J + 1 . In Figure 2 ( c ) , L c ( F ) = 3 and J + 1 = 6 , so A c ( F ) = 0.5 .
Learning a BTG Parser for Reordering
Now that we have a definition of loss over reorderings produced by the model , we have a clear learning objective : we would like to find reorderings F with low loss .
The learning algorithm we use to achieve this goal is motivated by discriminative training for machine translation systems ( Liang et al. , 2006 ) , and extended to use large-margin training in an online framework ( Watanabe et al. , 2007 ) .
Learning Algorithm
Learning uses the general framework of largemargin online structured prediction ( Crammer et al. , 2006 ) , which makes several passes through the data , finding a derivation with high model score ( the model parse ) and a derivation with minimal loss ( the oracle parse ) , and updating w if these two parses diverge ( Figure 3 ) .
In order to create both of these parses efficiently , we first create a parse forest encoding a large number of derivations D i according to the model scores .
Next , we find the model parse ? i , which is the parse in the forest D i that maximizes the sum of the model score and the loss S( D k | F k , w ) +L( D k | F k , A k ) .
It should be noted that here we are considering not only the model score , but also the derivation 's loss .
This is necessary for loss-driven large-margin training ( Crammer et al. , 2006 ) , and follows the basic intuition that during training , we would like to make it easier to select negative examples with large loss , causing these examples to be penalized more often and more heavily .
We also find an oracle parse Di , which is selected solely to minimize the loss L( D k | F k , A k ) .
One important difference between the model we describe here and traditional parsing models is that the target derivation
Dk is a latent variable .
Because many D k achieve a particular reordering F , many reorderings F are able to minimize the loss L( F k | F k , A k ) .
Thus it is necessary to choose a single oracle derivation to treat as the target out of many equally good reorderings .
DeNero and Uszkoreit ( 2011 ) resolve this ambiguity with four features with empirically tuned scores before training a monolingual parser and reordering model .
In contrast , we follow previous work on discriminative learning with latent variables ( Yu and Joachims , 2009 ) , and break ties within the pool of oracle derivations by selecting the derivation with the largest model score .
From an implementation point of view , this can be done by finding the derivation that minimizes L( D k | F k , A k ) ? ?S( D k | F k , w ) , where ? is a constant small enough to ensure that the effect of the loss will always be greater than the effect of the score .
Finally , if the model parse ?k has a loss that is greater than that of the oracle parse
Dk , we update the weights to increase the score of the oracle parse and decrease the score of the model parse .
Any criterion for weight updates may be used , such as the averaged perceptron ( Collins , 2002 ) and MIRA ( Crammer et al. , 2006 ) , but we opted to use Pegasos ( Shalev - Shwartz et al. , 2007 ) as it allows for the introduction of regularization and relatively stable learning .
To perform this full process , given a source sentence F k , alignment A k , and model weights w we need to be able to efficiently calculate scores , calculate losses , and create parse forests for derivations D k , the details of which will be explained in the following sections .
Scoring Derivation Trees First , we must consider how to efficiently assign scores S( D|F , w ) to a derivation or forest during parsing .
The most standard and efficient way to do so is to create local features that can be calculated based only on the information included in a single node d in the derivation tree .
The score of the whole tree can then be expressed as the sum of the scores from each node : S( D|F , w ) = ? d?D S( d|F , w ) = ? d?D ? i w i ? i ( d , F ) .
Based on this restriction , we define a number of features that can be used to score the parse tree .
To ease explanation , we represent each node in the derivation as d = s , l , c , c + 1 , r , where s is the node 's symbol ( str , inv , or term ) , while l and r are the leftmost and rightmost indices of the span that d covers .
c and c + 1 are the rightmost index of the left child and leftmost index of the right child for non-terminal nodes .
All features are intersected with the node label s , so each feature described below corresponds to three different features ( or two for features applicable to only non-terminal nodes ) .
? ? lex : Identities of words in positions f l , f r , f c , f c+ 1 , f l?1 , f r+1 , f l f r , and f c f c+ 1 . ? ? class : Same as ? lex , but with words abstracted to classes .
We use the 50 classes automatically generated by Och ( 1999 ) 's method that are calculated during alignment in standard SMT systems .
? ? balance : For non-terminals , features indicating whether the length of the left span ( c ? l + 1 ) is lesser than , equal to , or greater than the length of the right span ( r ? c ) .
? ? table : Features , bucketed by length , that indicate whether " f l . . . f r " appears as a contiguous phrase in the SMT training data , as well as the log frequency of the number of times the phrase appears total and the number of times it appears as a contiguous phrase ( DeNero and Uszkoreit , 2011 ) .
Phrase length is limited to 8 , and phrases of frequency one are removed .
? ? pos : Same as ? lex , but with words abstracted to language - dependent POS tags .
? ? cf g : Features indicating the label of the spans f l . . . f r , f l . . . f c , and f c+ 1 . . . f r in a supervised parse tree , and the intersection of the three labels .
When spans do not correspond to a span in the supervised parse tree , we indicate " no span " with the label " X " ( Zollmann and Venugopal , 2006 ) .
Most of these features can be calculated from only a parallel corpus , but ? pos requires a POS tagger and ? cf g requires a full syntactic parser in the source language .
As it is preferable to have a method that is applicable in languages where these tools are not available , we perform experiments both with and without the features that require linguistic analysis tools .
Finding Losses for Derivation Trees
The above features ? and their corresponding weights w are all that are needed to calculate scores of derivation trees at test time .
However , during training , it is also necessary to find model parses according to the loss-augmented scoring function S( D | F , w ) + L( D|F , A ) or oracle parses according to the loss L( D|F , A ) .
As noted by Taskar et al . ( 2003 ) , this is possible if our losses can be factored in the same way as the feature space .
In this section , we demonstrate that the loss L ( d |F , A ) for the evaluation measures we defined in Section 4 can ( mostly ) be factored over nodes in a fashion similar to features .
Factoring Kendall's ? For Kendall's ? , in the case of terminal nodes , L t ( d = term , l , r |F , A ) can be calculated by performing the summation in Equation ( 1 ) .
We can further define this sum recursively and use memoization for improved efficiency L t ( d |F , A ) =L t ( term , l , r ? 1 |F , A ) + r?1 ? j=l ?( r( f j ) > r( f r ) ) .
( 3 ) For non-terminal nodes , we first focus on straight non-terminals with parent node d = str , l , c , c + 1 , r , and left and right child nodes d l = s l , l , lc , lc + 1 , c and d r = s r , c + 1 , rc , rc +
1 , r .
First , we note that the loss for the subtree rooted at d can be expressed as L t ( d|F , A ) =L t ( d l |F , A ) + L t ( d r |F , A ) + c ? j 1 =l r ? j 2 =c + 1 ?( r( f j 1 ) > r( f j 2 ) ) .
In other words , the subtree 's total loss can be factored into the loss of its left subtree , the loss of its right subtree , and the additional loss contributed by comparisons between the words spanning both subtrees .
In the case of inverted terminals , we must simply reverse the comparison in the final sum to be ?( r( f j 1 ) < r( f j 2 ) ) .
Factoring Chunk Fragmentation
Chunk fragmentation loss can be factored in a similar fashion .
First , it is clear that the loss for the terminal nodes can be calculated efficiently in a fashion similar to Equation ( 3 ) .
In order to calculate the loss for non-terminals d , we note that the summation in Equation ( 2 ) can be divided into the sum over the internal bi-grams in the left and right subtrees , and the bi-gram spanning the reordered trees L c ( d |F , A ) =L c ( d l |F , A ) + L c ( d r |F , A ) + discont ( f c , f c + 1 ) .
However , unlike Kendall's ? , this equation relies not on the ranks of f c and f c+1 in the original sentence , but on the ranks of f c and f c+1 in the reordered sentence .
In order to keep track of these values , it is necessary to augment each node in the tree to be d = s , l , c , c + 1 , r , tl , tr with two additional values tl and tr that indicate the position of the leftmost and rightmost words after reordering .
Thus , a straight nonterminal parent d with children d l = s l , l , lc , lc + 1 , c , tl , tlr and d r = s r , c + 1 , rc , rc + 1 , r , trl , tr will have loss as follows L c ( d |F , A ) =L c ( d l |F , A ) + L c ( d r |F , A ) + discont ( f tlr , f trl ) with a similar calculation being possible for inverted non-terminals .
Parsing Derivation Trees Finally , we must be able to create a parse forest from which we select model and oracle parses .
As all feature functions factor over single nodes , it is possible to find the parse tree with the highest score in O ( J 3 ) time using the CKY algorithm .
However , when keeping track of target positions for calculation of chunk fragmentation loss , there are a total of O( J 5 ) nodes , an unreasonable burden in terms of time and memory .
To overcome this problem , we note that this setting is nearly identical to translation using synchronous CFGs with an integrated bigram LM , and thus we can employ cube-pruning to reduce our search space ( Chiang , 2007 ) .
Experiments
Our experiments test the reordering and translation accuracy of translation systems using the proposed method .
As reordering metrics , we use Kendall 's ? and chunk fragmentation comparing the system F and oracle F calculated with manually created alignments .
As translation metrics , we use BLEU ( Papineni et al. , 2002 ) , as well as RIBES ( Isozaki et al. , 2010a ) , which is similar to Kendall's ? , but evaluated on the target sentence E instead of the reordered sentence F .
All scores are the average of three training runs to control for randomness in training ( Clark et al. , 2011 ) .
For translation , we use Moses ( Koehn et al. , 2007 ) with lexicalized reordering of pre-ordering : original order with F ? F ( orig ) , pre-orderings learned using the 3 - step process of DeNero and Uszkoreit ( 2011 ) ( 3step ) , and the proposed model with latent derivations ( lader ) .
7 Except when stated otherwise , lader was trained to minimize chunk fragmentation loss with a cube pruning stack pop limit of 50 , and the regularization constant of 10 ?3 ( chosen through cross-validation ) .
We test our systems on Japanese -English and English - Japanese translation using data from the Kyoto Free Translation Task ( Neubig , 2011 ) .
We use the training set for training translation and language models , the development set for weight tuning , and the test set for testing ( Table 1 ) .
We use the designated development and test sets of manually created alignments as training data for the reordering models , removing sentences of more than 60 words .
As default features for lader and the monolingual parsing and reordering models in 3 - step , we use all the features described in Section 5 .
2 7 Available open-source : http://phontron.com/lader except ? pos and ? cf g .
In addition , we test systems with ? pos and ? cf g added .
For English , we use the Stanford parser ( Klein and Manning , 2003 ) for both POS tagging and CFG parsing .
For Japanese , we use the KyTea tagger for POS tagging , 8 and the EDA word - based dependency parser ( Flannery et al. , 2011 ) with simple manual head-rules to convert a dependency parse to a CFG parse .
Effect of Pre-ordering Table 2 shows reordering and translation results for orig , 3 - step , and lader .
It can be seen that the proposed lader outperforms the baselines in both reordering and translation .
9
There are a number of reasons why lader outperforms 3 - step .
First , the pipeline of 3 - step suffers from error propogation , with errors in monolingual parsing and reordering resulting in low overall accuracy .
10 Second , as Section 5.1 describes , lader breaks ties between oracle parses based on model score , allowing easyto-reproduce model parses to be chosen during training .
In fact , lader generally found trees that followed from syntactic constituency , while 3 - step more often used terminal nodes that spanned constituent boundaries ( as long as the phrase frequency was high ) .
Finally , as Section 6.2 shows in detail , the ability of lader to maximize reordering accuracy directly allows for improved reordering and translation results .
It can also be seen that incorporating POS tags or parse trees improves accuracy of both lader and 3 - step , particularly for English - Japanese , where syntax has proven useful for pre-ordering , and less so for Japanese - English , where syntactic pre-ordering has been less successful ( Sudoh et al. , 2011 b ) .
We also tested Moses 's implementation of hierarchical phrase - based SMT ( Chiang , 2007 ) , which achieved BLEU scores of 23.21 and 19.30 for English - Japanese and Japanese - English respectively , approximately matching lader in accuracy , but with a significant decrease in decoding speed .
Further , when pre-ordering with lader and hierarchical phrase - based SMT were combined , BLEU scores rose to 23.29 and 19.69 , indicating that the two techniques can be combined for further accuracy improvements .
Effect of Training Loss
Table 3 shows results when one of three losses is optimized during training : chunk fragmentation ( L c ) , Kendall 's ?
( L t ) , or the linear interpolation of the two with weights chosen so that both losses contribute equally ( L t + L c ) .
In general , training successfully maximizes the criterion it is trained on , and L t + L c achieves good results on both measures .
We also find that L c and L c + L t achieve the best translation results , which is in concert with , who find chunk fragmentation is better correlated with translation accuracy than Kendall 's ? .
This is an important result , as methods such as that of Tromble and Eisner ( 2009 ) word comparisons equivalent to L t , which may not be optimal for translation .
Effect of Automatic Alignments
Table 4 shows the difference between using manual and automatic alignments in the training of lader .
lader is able to improve over the orig baseline in all cases , but when equal numbers of manual and automatic alignments are used , the reorderer trained on manual alignments is significantly better .
However , as the number of automatic alignments is increased , accuracy improves , approaching that of the system trained on a smaller number of manual alignments .
Conclusion
We presented a method for learning a discriminative parser to maximize reordering accuracy for machine translation .
Future work includes application to other language pairs , development of more sophisticated features , investigation of probabilistic approaches to inference , and incorporation of the learned trees directly in tree-to-string translation .
Figure 2 : 2 Figure 2 : An example of ( a ) the ranking function r(f j ) , ( b ) loss according to Kendall 's ? , ( c ) loss according to chunk fragmentation .
