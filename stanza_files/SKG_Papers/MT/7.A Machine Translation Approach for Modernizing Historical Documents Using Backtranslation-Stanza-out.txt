title
A Machine Translation Approach for Modernizing Historical Documents Using Backtranslation
abstract
Human language evolves with the passage of time .
This makes historical documents to be hard to comprehend by contemporary people and , thus , limits their accessibility to scholars specialized in the time period in which a certain document was written .
Modernization aims at breaking this language barrier and increase the accessibility of historical documents to a broader audience .
To do so , it generates a new version of a historical document , written in the modern version of the document 's original language .
In this work , we propose several machine translation approaches for modernizing historical documents .
We tested these approaches in different scenarios , obtaining very encouraging results .
Introduction Historical documents are an important part of our cultural heritage .
With the aim of their preservation , there is an increase need of digitalazing -creating a digital text version which can be searched and automatically processed - of historical documents [ 1 ] .
However , there is an additional difficulty created by their linguistic properties :
On the one hand , human language evolves with the passage of time .
On the other hand , due to a lack of a spelling convention , orthography changes depending on the author and time period in which a given document was written .
These problems make historical documents harder to read , and makes it harder to digitalize them ( since their digital text version needs to be searched and automatically processed ) .
The orthography problem has been well researched in the literature [ 2 , 3 , 4 , 5 , 6 , 7 ] .
The proposed solution that aims to solve this problem is known as spelling normalization , and its goal is to adapt the document 's spelling to modern standards in order to achieve an orthography consistency and increase the document 's read-ability .
However , while is true that spelling normalization makes historical documents easier to read , they are still hard to comprehend by contemporary people .
This problem limits the accessibility of historical documents to scholars specialized in the time period in which the document was written .
Modernization aims at breaking the language barrier , generating a new version of a historical document in the modern version of the language in which the document was originally written ( see Fig.
1 for an example ) .
Therefore , not only the orthography is updated .
The lexicon and grammar are also modified in order to match the modern use of the document 's language .
The main drawback of this solution is that part of the document 's original intention could be lost in the process ( e.g. , part of the rhyme in Fig. 1 is lost for the sake of clarity ) .
Nonetheless , the document 's clarity is increased and , thus , its accessibility to a broader audience .
To the best of our knowledge , modernization of historical documents has been less researched in the literature .
A shared task was organized in order to translate historical text to contemporary language [ 9 ] .
The shared task 's main goal was spelling normalization .
However , they also tackle modernization using a set of rules .
Finally , there was an approach to modernize historical documents using Statistical Machine Translation ( SMT ) [ 10 ] .
In this work , we tackle modernization by using an SMT and Neural Machine Translation ( NMT ) approach .
Additionally , since a frequent problem when working with historical documents is the scarce availability of parallel training data [ 5 ] , we created two small parallel corpora ( see Section 3.1 ) and generated synthetic data using backtranslation [ 11 ] .
Our main contributions are the followings : ?
First use , to the best of our knowledge , of NMT and backtranslation for historical documents mod- ernization .
?
Comparison of approaches based on SMT and NMT .
?
Experimented with three historical corpora-two of which were created for this work - from three different time periods and two different languages .
The rest of this document is structure as follows : Section 2 introduces the different Machine Translation ( MT ) approaches used in our work .
Then , Section 3 describes the experiments conducted in order to assess our proposal .
After that , Section 4 presents and discusses the results of those experiments .
Finally , in Section 5 , conclusions are drawn .
Machine Translation
In this section , we present the machine translation approaches used in our work .
Statistical Machine Translation Given a source sentence x , SMT aims at finding its best translation ? [ 12 ] : ? = arg max y P r(y | x ) ( 1 )
For years , the prevailing approach to compute this expression have been phrase - based models [ 13 ] .
These models rely on a log-linear combination of different models [ 14 ] : namely , phrase - based alignment models , reordering models and language models ; among others [ 15 , 16 ] .
However , more recently , this approach has shifted into neural models ( see Section 2.2 ) .
Neural Machine Translation NMT is the neural approach to compute Eq. ( 1 ) .
Frequently , it relies on a Recurrent Neural Network ( RNN ) encoder-decoder framework .
In this framework , the source sentence is projected into a distributed representation at the encoding step .
At the decoding step , the decoder generates its translation word by word [ 17 ] .
The system 's input is a sequence of words in the source language .
Each source word is linearly projected to a fixed - sized real- valued vector through an embedding matrix .
These word embeddings are feed into a bidirectional [ 18 ]
Long Short - Term Memory ( LSTM ) [ 19 ] network , resulting in a sequence of annotations produced by concatenating the hidden states from the forward and backward layers .
The model features an attention mechanism [ 20 ] , which allows the decoder to focus on parts of the input sequence , computing a weighted mean of annotations sequence .
A soft alignment model computes these weights by weighting each annotation with the previous decoding state .
The decoder is another LSTM network , conditioned by the representation computed by the attention model and the last word generated .
Finally , a deep output layer [ 21 ] computes a distribution over the target language vocabulary .
The model is trained by means of stochastic gradient descend , applied jointly to maximize the log-likelihood over a bilingual parallel corpus .
At decoding time , the model approximates the most likely target sentence with beam-search [ 17 ] .
Backtranslation Backtranslation [ 11 ] has become the norm when building state - of- the - art NMT systems , especially in resourcepoor scenarios [ 22 ] .
It is a useful technique to increase the training data by creating synthetic text from monolingual data .
Given a monolingual corpus in the target language , and an MT system trained to translate from the target language to the source language , the synthetic data is generated by translating the monolingual corpus with the MT system .
After that , the synthetic data is used as the source part of the corpus , and the monolingual data as the target part .
Finally , this new corpus is mixed with the available training data in order to train a new MT system .
In this work , to generate the synthetic data , we translate the monolingual data using an ad-hoc SMT system trained with the corpus ' training partition .
Additionally , since the datasets are considerable small , prior to mixing the synthetic corpus with the training partition , we replicate several times the training data in order to match the size of the synthetic data and avoid overfitting [ 23 ] .
Finally , we trained an NMT system with this new corpus .
Experimental Framework
In this section , we present the corpora and metrics , and describe the MT systems used during the experimental session .
Corpora
The first corpus used to assess our proposal was the Dutch Bible [ 9 ] .
This corpus consists in a collection of different versions of the Dutch Bible .
More precisely , a version from 1637 , another from 1657 , another from 1888 and another from 2010 .
All versions contain the same text except for the 2010 version , which is missing the last books .
Moreover , the authors mentioned that the translation from this last version is not very reliable .
Additionally , due to Dutch not evolving significantly during this period , 1637 and 1657 versions are fairly similar .
For this reason , we decided to only use the 1637 version - considering this as the original document - and the 1888 version - considering 19 th century Dutch as modern Dutch .
To create the synthetic corpus ( see Section 2.3 ) , we collected all 19 th century Dutch books available at the Digitale Bibliotheek voor de Nederlandse letteren 1 and used them as monolingual data .
The second corpus we used was El Quijote .
We built this corpus using a version [ 24 ] of the original 17 th century Spanish novel by Miguel de Cervantes , and a 21 st century version modernized by Andr?s Trapiello [ 25 ] .
The first step was to split each document into sentences .
Since the 17 th century version was faithful to the original manuscript ( in which each document line is formed by a very few words ) , we replaced line breaks by spaces to create a single sentence , and removed empty lines .
For consistency , we did the same to the 21 st century version .
After that , we split each document into sentences by adding line breaks to relevant punctuation ( i.e. , dots , quotation marks , admiration marks , etc ) .
Then , to ensure consistency , we checked special symbols ( e.g. , quotation marks ) and made sure that the same character was used in both versions .
Finally , in order to create a parallel corpus , we aligned both documents using Hunalign [ 26 ] .
Since the content of this corpus was a novel , we decided that , to create the synthetic corpus , it would be best to use monolingual data coming from Spanish literature .
For this reason and , considering that Spanish has n't changed significantly over the last decades , we decided to collect free- of- right late 20 th century Spanish novels from Project Gutenberg 2 . Finally , as a third corpus , we selected El Conde Lucanor .
We built this corpus using a version of the original 14 th century Spanish novel by Don Juan Manuel , and a 21 st century version modernized by Luis L?pez Nieves [ 27 ] .
To create the parallel version , we followed the same steps than with El Quijote .
However , unlike with El Quijote , the resulting corpus was too small to be able to use for training an MT system .
Therefore , we decided to use it only as a test .
Unable to find a suitable training corpus , we decided to test El Conde Lucanor using the systems created for El Quijote - despite the fact that the original documents were written three centuries apart from one another .
Table 1 presents the corpora statistics .
Metrics
In order to asses our proposal , we made use of the following well -known metrics : ? BiLingual Evaluation Understudy ( BLEU ) [ 28 ] : computes the geometric average of the modified n-gram precision , multiplied by a brevity factor that penalizes short sentences .
? Translation Error Rate ( TER ) [ 29 ] : computes the number of word edit operations ( insertion , substitution , deletion and swapping ) , normalized by the number of words in the final translation .
Confidence intervals ( p = 0.05 ) were computed for all metrics by means of bootstrap resampling [ 30 ] .
MT Systems
We trained the SMT systems with Moses [ 31 ] , following the standard procedure : we estimated a 5 - gram language model-smoothed with the improved KneserNey method-using SRILM [ 32 ] , and optimized optimized the weights of the log-lineal model with MERT [ 33 ] .
Additionally , we lowercased and tokenized the corpora using the standard scripts and , later , truecased the translated text using Moses ' truecaser .
To train the NMT systems , we used OpenNMT [ 34 ] .
We used LSTM units , following the findings from [ 35 ] .
We set the size of the word embedding and LSTM units to 1024 .
We used Adam [ 36 ] with a learning rate of 0.0002 [ 37 ] .
The beam size was set to 6 .
Finally , the corpora were lowercased and tokenized - and , later , truecased and detokenized - using OpenNMT 's tools .
In order to reduce the vocabulary , we applied Byte Pair Encoding ( BPE ) [ 38 ] to both SMT and NMT systems .
We trained the models with a joint vocabulary of 32000 BPE units .
Results
In this section , we present and discuss the results of the experiments conducted in order to assess our proposal .
Table 2 presents the experimental results .
Dutch Bible contained an additional baseline which consisted in generating a modernized version of the text by applying a set of rules to the original document [ 9 ] .
This second baseline improved significantly ( close to 40 BLEU points and 30 TER points ) the standard baseline of considering the original document as the modernized version .
However , the SMT approach improved those results even more ( near 30 BLEU points and 15 TER points of improvement with respect to the second baseline , and 70 BLEU points and 50 TER points with respect to the standard baseline ) .
The NMT approach yielded better results than the standard baseline ( and improvement of around 25 BLEU points and 5 TER points ) , but worse results than the second baseline and the SMT approach .
Most likely , this is due to the training corpus being too small , which is a well -known problem in NMT .
Finally , the backtranslation approach yielded the worst results .
These results represent an improvement over the standard baseline in term of BLEU ( around 4 points ) , and a deterioration in terms of TER ( around 8 points ) .
Most likely , this is due to the monolingual data used for backtranslation not being similar enough to the training data .
The experiments using El Quijote behaved similarly - taking into account that the only available baseline is the standard one - to Dutch Bible :
The SMT approached yielded the best results ( an improvement of Table 2 : Experimental results .
Baseline system corresponds to considering the original document as the modernized version .
Baseline 2 came with the Dutch Bible and is a modernized version of the text generated by applying a set of rules to the original document [ 9 ] .
SMT and NMT are the SMT and NMT approaches respectively .
NMT Synthetic is the NMT system trained with the synthetic data generated through backtranslation .
Best results are denoted in bold .
close to 22 BLEU points and 14 TER points ) .
The results yielded by the NMT approach were not significantly different to the baseline in terms of BLEU , and represented close to a 10 points deterioration in terms of TER .
In this case , however , the backtranslation approach yielded nearly a 10 points improvement in terms of BLEU , and the same TER results as the NMT approach .
Not being able to obtain enough suitable training data for El Conde Lucanor , we used the same systems than for El Quijote .
However , these documents were written three centuries apart from one another ( El Conde Lucanor is written in 14 th century Spanish and El Quijote in 17 th century Spanish ) .
Therefore , the obtained results contained a low translation quality .
Nonetheless , it is worth noting that the SMT approach yielded improvements over the baseline ( around 3 BLEU points and 6 TER points ) .
However , the NMT and backtranslating approached yielded a deterioration of 3 BLEU points ( in both cases ) and 10 and 75 TER points respectively .
In general , SMT yielded the best results in all cases .
NMT was able to improve Dutch Bible 's baseline , yielding similar results to El Quijote 's baseline and worse results than El Conde Lucanor 's baseline .
Finally , despite being successfully used in resources - poor scenarios , backtranslation was only able to improve results for the experiment using El Quijote , and these results were worse than the ones yielded by the SMT approach .
Qualitative Analysis
Table 2 shows some examples of sentences modernized using the different MT approaches .
The first example is a sentence from El Quijote .
The hypothesis generated by the SMT approach is very closed to the reference .
The main differences are a change in the order of actions ( the original sentence says Y dejando de comer , se levant ? , which is changed by the hypothesis into Y levant ? ndose , dej ? de comer ) and some changes in the conjugation of verbs ( e.g. , dejando is changed into dej ? ) .
However , the main goal of modernization is not to generate a perfect equivalent version , but to make the document easier to comprehend - making the overall meaning more important than the exact choice of words .
While sentences like these are penalized by the automatic metrics , they accomplish modernization 's goal .
The hypothesis generated by the NMT approach follows the same structure than the SMT hypothesis ( it makes the same reordering and conjugation changes ) but contains non-existent words ( e.g. , ancen ) and has some errors ( e.g , a los pies in stead of puesto a caballo ) .
Therefore , some parts are easier to comprehend than in the original version , but the meaning of the sentence is not clear .
Finally , the hypothesis generated by the backpropagation approach is almost the same as the one generated by the SMT approach ( the only change is fierded in stead of fiereza ) .
While this hypothesis is less correct than the SMT one , they are both equally easy to comprehend .
The second example is from El Conde Lucanor , whose experiments were conducted using the systems trained with El Quijote .
As a result , all the hypothesis are hard to comprehend .
While the automatic metrics heavily penalize the backtranslation approach , in this case , is the one which is closer to modern Spanish .
Moreover , it is the only hypothesis which preserves the name of the main characters ( Lucanor and Patronio ) .
Looking through the whole texts , the SMT and NMT hypothesis frequently changed the characters named into non-existent words , while the backtranslation approach rarely modified them .
Finally , having trained the systems with El Quijote has a visible effect in the SMT and El Quijote Original : Y , leuantandose , dex ?
de comer , y fue a quitar la cubierta de la primera imagen , que mostro ser la de San Iorge puesto a cauallo , con vna serpiente enroscada a los pies , y la lanc ? a atrauessada por la boca , con la fierec ? a que suele pintarse .
Modernized : Y dejando de comer , se levant ?
y fue a quitar la cubierta de la primera imagen , que result ?
ser la de san Jorge a caballo , con una serpiente enroscada a los pies y la lanza atraves ?
ndole la boca , con la fiereza que suele pintarse .
SMT : Y levant ? ndose , dej ?
de comer , y fue a quitar la cubierta de la primera imagen , que mostr ?
ser la de San Jorge puesto a caballo , con una serpiente enroscada a los pies , y la lanza atravesada por la boca , con la fiereza que suele pintarse .
NMT : Y levant ? ndose , dej ?
de comer , y fue a quitar la cubierta de la primera ancen , que mostr ?
ser la de San Marorge a los pies y la lanza ahabesada por la boca ; NMT Synthetic : Y levant ? ndose , dej ?
de comer , y fue a quitar la cubierta de la primera imagen , que mostr ?
ser la de San Jorge puesto a caballo , con una serpiente enroscada a los pies , y la lanza atravesada por la boca , con la fierded que suele pintarse .
El Conde Lucanor Original : - Se? or conde Lucanor - dixo Patronio- , vien entiendo que el m?o consejo non vos faze grant mengua , pero vuestra voluntad es que vos diga lo que en esto entiendo , et vos conseje sobre ello , fazerlo he luego .
Modernized : - Se?or Conde Lucanor - dijo Patronio- , bien s? que mi consejo no os hace mucha falta , pero , como confi? is en m? , SMT : -- Se? or conde Lucanor - dijo Patroniorosa , vien entiendo que el m?o consejo non vos face grant mengua , pero vuestra voluntad es que vos diga lo que en esto entiendo , et vos aconseje en ello , ferlo he luego .
NMT : Se?or conde Olcanor dijo dijo Pacasos - dijo en entiendo que el m?o consejo non os fazo felimengua y vuestra merced es que vos diga lo que en esto entiendo .
NMT Synthetic : - Se? or conde Lucanor - dijo Patronio , vien entiendo que el m?o consejo non es face grant mengua , pero vuestra voluntad es que vos diga lo que en esto entiendo , et vos conseje sobre ello , tambi?n yo he dicho .
Table 3 : Examples of modernizing a sentence using the different MT approaches .
SMT and NMT are the SMT and NMT approaches respectively .
NMT Synthetic is the NMT system trained with the synthetic data generated through backtranslation .
NMT approaches ( with verbs conjugations such as ferlo , or expression such as vuestra merced ) .
This effect , however , is not so visible in the backtranslation hypothesis .
All in all , neither hypothesis accomplishes the goal of improving the comprehension of the original sentence .
Nonetheless , it is worth noting that this was a tricky example since the original sentence is modernized into a much shorter sentence ( which reflects that 14 th century Spanish used longer expressions than modern Spanish ) .
Conclusions and Future Work
In this work we proposed several machine translation approaches to modernize historical documents in order to break the language barrier and increase their accessibility to a broader audience .
We tested our approaches using three historical datasets ( two of which were created for this work ) from three different time periods and two different languages .
Our first approach was based in SMT and yielded , for all cases , the best results .
With the exception of the dataset for which there were not available any suitable training data , this approach yielded significant improvements of around 22 to 67 BLEU points and 14 to 48 TER points .
Since the available training data was fairly small , the approach based on NMT produced less satisfactory results .
While it was able to yield improvements for one dataset , the rest of the experiment resulted in either not significantly different than the baseline , or yielding a deterioration in terms of translation quality .
Finally , despite being successfully used in resourcespoor scenarios , backtranslation was only able to improve results for one dataset , and only in terms of BLEU .
Our best hypothesis is that historical documents are very language -specific and , therefore , choosing the monolingual corpus to use for creating the synthetic data is extremely important .
While we tried to create the monolingual datasets using similar topics , the corpora 's topics were too specific : religious texts , a cavalry novel and medieval tales .
In a future work , we would like to research the relation between the domains of the monolingual and training corpora deeper .
Additionally , we want to explore the use of data selection techniques for constructing the monolingual corpus to use for backtranslation , and to create a training partition for cases in which we do not have suitable training data available ( as was the case with El Conde Lucanor ) .
Figure 1 : 1 Figure 1 : Example of modernizing a historical document .
The original text is Shakespeare Sonnet 18 .
The modernized version of the Sonnet was obtained from [ 8 ] .
Table 1 : 1 Corpora statistics .
| S | stands for number of sentences , | T | for number of tokens and | V | for size of the vocabulary .
Monolingual refers to the monolingual data used to create the synthetic data .
M denotes million and K thousand .
Dutch Bible El Quijote El Conde Lucanor Train Development Test Monolingual 35.2 K |T | 870.4/862.4 K 283.3/283.2K 10K | S| 53.8/42.8K 31.7/31.3 K | V | 2000 2000 | S| 56.4/54.8 K 53.2/53.2K |T | 9.1/7.8K 10.7/10.6K |V | 5000 2000 | S| 41.8/42.0K |T | 145.8/140.8K 10.5/9.0K 8.9/9.0K | V | 4.1M 567.0K | S| 88.3 M 9.5M |T | |V | 2.0M 470.4K
------2252 62.0/56.7 K 7.4/8.6K ---
NMT Synthetic 17.4 ? 0.5 65.6 ? 1.7 45.2 ? 1.3 50.6 ? 3.5 3.1 ? 0.2 165.1 ? 8.2 System Dutch Bible BLEU TER El Quijote BLEU TER El Conde Lucanor BLEU TER Baseline Baseline 2 SMT NMT 13.5 ? 0.3 57.0 ? 0.3 36.5 ? 0.8 43.3 ? 1.1 5.8 ? 0.3 89.6 ? 1.0 ----50.8 ? 0.4 26.5 ? 0.3 80.1 ? 0.5 9.9 ? 0.3 58.9 ? 1.0 29.4 ? 1.2 8.4 ? 0.3 83.8 ? 1.0 38.0 ? 0.6 51.7 ? 2.2 37.4 ? 1.2 51.5 ? 2.0 2.7 ? 0.2 99.5 ? 2.0
Proceedings of the 15 th International Workshop on Spoken Language Translation Bruges , Belgium , October 29-30 , 2018
http://dbnl.nl/ 2 https://www.gutenberg.org/
Proceedings of the 15 th International Workshop on Language Translation Bruges , Belgium , October 29-30 , 2018
