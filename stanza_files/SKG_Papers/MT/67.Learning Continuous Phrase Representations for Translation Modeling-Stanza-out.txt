title
Learning Continuous Phrase Representations for Translation Modeling
abstract
This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations , whose distributed nature enables the sharing of related phrases in their representations .
A pair of source and target phrases are projected into continuous - valued vector representations in a low-dimensional latent space , where their translation score is computed by the distance between the pair in this new space .
The projection is performed by a neural network whose weights are learned on parallel training data .
Experimental evaluation has been performed on two WMT translation tasks .
Our best result improves the performance of a state - of - the - art phrase - based statistical machine translation system trained on WMT 2012 French - English data by up to 1.3 BLEU points .
Introduction
The phrase translation model , also known as the phrase table , is one of the core components of phrase - based statistical machine translation ( SMT ) systems .
The most common method of constructing the phrase table takes a two - phase approach ( Koehn et al. 2003 ) .
First , the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data .
The second phase , which is the focus of this paper , is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data .
Phrase - based SMT systems have achieved state - of - the - art performance largely due to the fact that long phrases , rather than single words , are used as translation units so that useful context information can be captured in selecting translations .
However , longer phrases occur less often in training data , leading to a severe data sparseness problem in parameter estimation .
There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model ( e.g. , DeNero et al.
2006 ; Wuebker et al. 2010 ; He and Deng 2012 ; Gao and He 2013 ) .
This paper revisits the problem of scoring a phrase translation pair by developing a Continuous -space Phrase Translation Model ( CPTM ) .
The translation score of a phrase pair in this model is computed as follows .
First , we represent each phrase as a bag-of-words vector , called word vector henceforth .
We then project the word vector , in either the source language or the target language , into a respective continuous feature vector in a common low-dimensional space that is language independent .
The projection is performed by a multi-layer neural network .
The projected feature vector forms the continuous representation of a phrase .
Finally , the translation score of a source-target phrase pair is computed by the distance between their feature vectors .
The main motivation behind the CPTM is to alleviate the data sparseness problem associated with the traditional counting - based methods by grouping phrases with a similar meaning across different languages .
This style of grouping is made possible because of the distributed nature of the continuous -space representations for phrases .
No such sharing was possible in the original symbolic space for representing words or phrases .
In this model , semantically or grammatically related phrases , in both the source and the target languages , would tend to have similar ( close ) feature vectors in the continuous space , guided by the training objective .
Since the translation score is a smooth function of these feature vectors , a small change in the features should only lead to a small change in the translation score .
The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT .
Motivated by recent studies on continuous - space language models ( e.g. , Bengio et al.
2003 ; Mikolov et al. 2011 ; Schwenk et al. , 2012 ) , we use a neural network to project a word vector to a feature vector .
Ideally , the projection would discover those latent features that are useful to differentiate good translations from bad ones , for a given source phrase .
However , there is no training data with explicit annotation on the quality of phrase translations .
The phrase translation pairs are hidden in the parallel source - target sentence pairs , which are used to train the traditional translation models .
The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences , as measured by BLEU , which contain the phrase pair .
In order to overcome this challenge and let the BLEU metric guide the projection learning , we propose a new method to learn the parameters of a neural network .
This new method , via the choice of an appropriate objective function in training , automatically forces the feature vector of a source phrase to be closer to the feature vectors of its candidate translations .
As a result , the BLEU score is improved when these translations are selected by an SMT decoder to produce final , sentence - level translations .
The new learning method makes use of the L-BFGS algorithm and the expected BLEU as the objective function defined on N-best lists .
To the best of our knowledge , the CPTM proposed in this paper is the first continuous - space phrase translation model that makes use of joint representations of a phrase in the source language and its translation in the target language ( to be detailed in Section 4 ) and that is shown to lead to significant improvement over a standard phrasebased SMT system ( to be detailed in Section 6 ) .
Like the traditional phrase translation model , the translation score of each bilingual phrase pair is modeled explicitly in our model .
However , instead of estimating the phrase translation score on aligned parallel data , our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by projecting them into a common , continuous space that is language independent .
The rest of the paper is organized as follows .
Section 2 reviews previous work .
Section 3 reviews the log-linear model for phrase - based SMT and Sections 4 presents the CPTM .
Section 5 describes the way the model parameters are estimated , followed by the experimental results in Section 6 .
Finally , Section 7 concludes the paper .
Related Work Representations of words or documents as continuous vectors have a long history .
Most of the earlier latent semantic models for learning such vectors are designed for information retrieval ( Deerwester et al.
1990 ; Hofmann 1999 ; Blei et al. 2003 ) .
In contrast , recent work on continuous space language models , which estimate the probability of a word sequence in a continuous space ( Bengio et al.
2003 ; Mikolov et al. 2010 ) , have advanced the state of the art in language modeling , outperforming the traditional n-gram model on speech recognition ( Mikolov et al.
2012 ; Sundermeyer et al. 2013 ) and machine translation ( Mikolov 2012 ; Auli et al. 2013 ) .
Because these models are developed for monolingual settings , word embedding from these models is not directly applicable to translation .
As a result , variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space ( Dumais et al.
1997 ; Platt et al.
2010 ; Vinokourov et al. 2002 ; Yih et al. 2011 ; Gao et al. 2011 ; Huang et al. 2013 ; Zou et al. 2013 ) .
In principle , a phrase table can be derived using any of these cross-lingual models , although decoupling the derivation from the SMT training often results in suboptimal performance ( e.g. , measured in BLEU ) , as we will show in Section 6 .
Recently , there is growing interest in applying continuous -space models for translation .
The most related to this study is the work of continuous space n-gram translation models ( Schwenk et al. 2007 ; Schwenk 2012 ; Son et al. 2012 ) , where the feed -forward neural network language model is extended to represent translation probabilities .
However , these earlier studies focused on the ngram translation models , where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model .
Therefore , it is not clear how their approaches can be applied to the phrase translation model 1 , which is much more version of such a model can be trained efficiently because the factor models used by Son et al .
cannot be applied directly .
widely used in modern SMT systems .
In contrast , our model learns jointly the representations of a phrase in the source language as well as its translation in the target language .
The recurrent continuous translation models proposed by Kalchbrenner and Blunsom ( 2013 ) also adopt the recurrent language model ( Mikolov et al. 2010 ) .
But unlike the n-gram translation models above , they make no Markov assumptions about the dependency of the words in the target sentence .
Continuous space models have also been used for generating translations for new words ( Mikolov et al. 2013a ) and ITG reordering ( Li et al. 2013 ) .
There has been a lot of research on improving the phrase table in phrase - based SMT ( Marcu and Wong 2002 ; Lamber and Banchs 2005 ; Denero et al.
2006 ; Wuebker et al. 2010 ; Zhang et al. , 2011 ; He and Deng 2012 ; Gao and He 2013 ) .
Among them , ( Gao and He 2013 ) is most relevant to the work described in this paper .
They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT .
In this study we use the same objective function to learn the continuous representations of phrases , integrating the strengths associated with these earlier studies .
The Log-Linear Model for SMT Phrase - based SMT is based on a log-linear model which requires learning a mapping between input ? ? ? to output ? ? ?.
We are given ?
Training samples ( ? ? , ? ? ) for ? = 1 ? ? , where each source sentence ? ? is paired with a reference translation in target language ? ? ; ?
A procedure GEN to generate a list of N-best candidates GEN ( ? ? ) for an input ? ? , where GEN in this study is the baseline phrasebased SMT system , i.e. , an in-house implementation of the Moses system ( Koehn et al. 2007 ) that does not use the CPTM , and each ? ? GEN (? ? ) is labeled by the sentence - level BLEU score ( He and Deng 2012 ) , denoted by sBleu ( ? ? , ? ) , which measures the quality of ? with respect to its reference translation ? ? ; ?
A vector of features ? ? ? ? that maps each ( ? ? , ? ) to a vector of feature values 2 ; and ?
A parameter vector ? ? ? ? , which assigns a real-valued weight to each feature .
The components GEN ( . ) , ? and ? define a loglinear model that maps ? ? to an output sentence as follows : ? * = argmax ( ? , ? ) ? GEN (? ? ) ? T ?(? ? , ? , ? ) ( 1 ) which states that given ? and ? , argmax returns the highest scoring translation ? * , maximizing over correspondences ?.
In phrase - based SMT , ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases .
Since computing the argmax exactly is intractable , it is commonly performed approximatedly by beam search ( Och and Ney 2004 ) . Following Liang et al. ( 2006 ) , we assume that every translation candidate is always coupled with a corresponding ? , called the Viterbi derivation , generated by ( 1 ) .
A Continuous -Space Phrase Translation Model ( CPTM )
The architecture of the CPTM is shown in Figures 1 and 2 , where for each pair of source and target phrases ( ? ? , ? ? ) in a source-target sentence pair , we first project them into feature vectors ? ? ? and ? ? ? in a latent , continuous space via a neural network with one hidden layer ( as shown in Figure 2 ) , and then compute the translation score , score ( ? ? , ? ? ) , by the distance of their feature vectors in that space .
We start with a bag-of-words representation of a phrase ? ? ? ? , where ? is a word vector and ? is the size of the vocabulary consisting of words in both source and target languages , which is set to 200 K in our experiments .
We then learn to project ? to a low-dimensional continuous space ? ? : ?(? ) : ? ? ? ? ?
The projection is performed using a fully connected neural network with one hidden layer and tanh activation functions .
Let ?
1 be the projection matrix from the input layer to the hidden layer and ?
2 the projection matrix from the hidden layer to the output layer , we have ? ? ?(? ) = tanh ( ?
2 T ( tanh ( ? 1 T ? ) ) ) ( 2 ) Figure 2 .
A neural network model for phrases giving rise to their continuous representations .
The model with the same form is used for both source and target languages .
The translation score of a source phrase f and a target phrase e can be measured as the similarity ( or distance ) between their feature vectors .
We choose the dot product as the similarity function 3 : score ( ? , ? ) ? sim ? (? ? , ? ? ) = ? ? T ? ? ( 3 ) According to ( 2 ) , we see that the value of the scoring function is determined by the projection matrices ? = {?
1 , ? 2 }. The CPTM of ( 2 ) and ( 3 ) can be incorporated into the log-linear model for SMT ( 1 ) by 3
In our experiments , we compare dot product and the cosine similarity functions and find that the former works better for nonlinear multi-layer neural networks , and the latter works better for linear neural networks .
For the sake of clarity , we choose dot product when we describe the CPTM and its training in Sections 4 and 5 , respectively .
4
The baseline SMT needs to be reasonably good in the sense that the oracle BLEU score on the generated n-best introducing a new feature ? ?+ 1 and a new feature weight ? ?+1 .
The new feature is defined as ? ?+1 ( ? ? , ? , ?) = ? sim ? ( ? ? , ? ? ) ( ? , ? ) ?
( 4 ) Thus , the phrase - based SMT system , into which the CPTM is incorporated , is parameterized by ( ? , ? ) , where ? is a vector of a handful of parameters used in the log-linear model of ( 1 ) , with one weight for each feature ; and ? is the projection matrices used in the CPTM defined by ( 2 ) and ( 3 ) .
In our experiments we take three steps to learn ( ? , ? ) : 1 . We use a baseline phrase - based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses 4 . 2 .
We set ? to that of the baseline system and let ? ?+1 = 1 , and optimize ? w.r.t. a loss function on training data 5 . 3 . We fix ? , and optimize ? using MERT ( Och 2003 ) to maximize BLEU on dev data .
In the next section , we will describe Step 2 in detail as it is directly related to the CPTM training .
lists needs to be significantly higher than that of the top - 1 translations so that the CPTM can be effectively trained .
5
The initial value of ? ?+ 1 can also be tuned using the dev set .
However , we find in a pilot study that it is good enough to set it to 1 when the values of all the baseline feature weights , used in the log-linear model of ( 1 ) , are properly normalized , such as by setting ? ? = ? ? /? for ? = 1 ? ? , where ? is the unnormalized weight value of the target language model . ( 5 ) where sBleu ( ? ? , ? ) is the sentence - level BLEU score , and ?(?|? ? ) is the translation probability from ? ? to ? computed using softmax as ?(?|? ? ) = exp (? T ?(? ? , ? , ? ) ) ? exp (? T ?(? ? , ? ? , ? ) ) ? ? ?GEN (? ? ) ( 6 ) where ?
T ? is the log-linear model of ( 1 ) , which also includes the feature derived from the CPTM as defined by ( 4 ) , and ? is a tuned smoothing factor .
Let ?(? ) be a loss function which is differentiable w.r.t. the parameters of the CPTM , ?.
We can compute the gradient of the loss and learn ?
using gradient - based numerical optimization algorithms , such as L-BFGS or stochastic gradient descent ( SGD ) .
Computing the Gradient Since the loss does not explicitly depend on ? , we use the chain rule for differentiation : ?(? ) ? = ? ?(? ) ?sim ? (? ? , ? ? ) ?sim ? (? ? , ? ? ) ? ( ? , ? ) = ? ? ( ? , ? ) ?sim ? ( ? ? , ? ? ) ? ( ? , ? ) ( 7 ) which takes the form of summation over all phrase pairs occurring either in a training sample ( stochastic mode ) or in the entire training data ( batch mode ) . ? ( ? , ? ) in ( 7 ) is known as the error term of the phrase pair ( ? , ? ) , and is defined as ? ( ? , ? ) = ? ?(? ) ?sim ? ( ? ? , ? ? ) ( 8 )
It describes how the overall loss changes with the translation score of the phrase pair ( ? , ? ) .
We will leave the derivation of ? ( ? , ? ) to Section 5.1.2 , and will first describe how the gradient of sim ? (? ? , ? ? ) w.r.t. ? is computed .
Computing ? ? (? ? , ? ? ) / ?
Without loss of generality , we use the following notations to describe a neural network : ? ? ? is the projection matrix for the l-th layer of the neural network ; ? ? is the input word vector of a phrase ; ? ? ? is the sum vector of the l-th layer ; and ? ? ? = ?(? ? ) is the output vector of the l-th layer , where ? is an activation function ;
Thus , the CPTM defined by ( 2 ) and ( 3 ) can be represented as ?
1 = ? 1 T ? ? 1 = ?(?
1 ) ? 2 = ? 2 T ? 1 ? 2 = ?(? 2 ) sim ? (? ? , ? ? ) = (? ? 2 ) T ? ? 2
The gradient of the matrix ?
2 which projects the hidden vector to the output vector is computed as : ?sim ? (? ? , ? ? ) ? 2 = ?(? ? 2 ) T ? 2 ? ? 2 + ( ? ? 2 ) T ? ? 2 ? 2 = ? ? 1 ( ? ? 2 ? ? ? ( ? ? 2 ) ) T + ? ? 1 ( ? ? 2 ? ? ? ( ? ? 2 ) ) T ( 9 ) where ? is the element- wise multiplication ( Hadamard product ) .
Applying the back propagation principle , the gradient of the projection matrix mapping the input vector to the hidden vector ?
1 is computed as ?sim ? (? ? , ? ? ) ?
1 = ? ? (? 2 (? ? 2 ? ? ? ( ? ? 2 ) ) ? ? ? (? ? 1 ) ) T +? ? (? 2 (? ? 2 ? ? ? (? ? 2 ) ) ? ? ? (? ? 1 ) ) T ( 10 )
The derivation can be easily extended to a neural network with multiple hidden layers .
Computing ? ( ? , ? )
To simplify the notation , we rewrite our loss function of ( 5 ) and ( 6 ) over one training sample as where ( 14 ) ?(? ) = ?xBleu ( ? ) = ? G ( ? ) Z ( ? ) ( 11 U ( ? , ? ) = sBleu ( ? ? , ? ) ? xBleu ( ? ) .
The Training Algorithm
In our experiments we train the parameters of the CPTM , ? , using the L-BFGS optimizer described in Andrew and Gao ( 2007 ) , together with the loss function described in ( 5 ) .
The gradient is computed as described in Sections 5.1 .
Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima ( Bengio 2009 ) , we find that in our task that the L-BFGS minimizes the loss in a desirable fashion empirically when iterating over the complete training data ( batch mode ) .
For example , the convergence of the algorithm was found to be smooth , despite the non-convexity in our loss .
Another merit of batch training is that the gradient over all training data can be computed efficiently .
As shown in Section 5.1 , computing ?sim ? ( x ? , x ? ) /?
requires large-scale matrix multiplications , and is expensive for multi-layer neural networks .
Eq. ( 7 ) suggests that ?sim ? ( x ? , x ? ) /? and ? ( ? , ? ) can be computed separately , thus making the computation cost of the former term only depends on the number of phrase pairs in the phrase table , but not the size of training data .
Therefore , the training method described here can be used on larger amounts of training data with little difficulty .
As described in Section 4 , we take three steps to learn the parameters for both the log-linear model of SMT and the CPTM .
While steps 1 and 3 can be easily parallelized on a computer cluster , the CPTM training is performed on a single machine .
For example , given a phrase table containing 16 M pairs and a 1 M - sentence training set , it takes a couple of hours to generate the N-best lists on a cluster , and about 10 hours to train the CPTM on a Xeon E5-2670 2.60 GHz machine .
For a non-convex problem , model initialization is important .
In our experiments we always initialize ?
1 using a bilingual topic model trained on parallel data ( see detail in Section 6.2 ) , and ?
2 as an identity matrix .
In principle , the loss function of ( 5 ) can be further regularized ( e.g. by adding a term of ?
2 norm ) to deal with overfitting .
However , we did not find clear empirical advantage over the simpler early stop approach in a pilot study , which is adopted in the experiments in this paper .
Experiments
This section evaluates the CPTM presented on two translation tasks using WMT data sets .
We first describe the data sets and baseline setup .
Then we present experiments where we compare different versions of the CPTM and previous models .
Experimental Setup Baseline .
We experiment with an in- house phrase - based system similar to Moses ( Koehn et al. 2007 ) , where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings ? ? ( ?|? ) and vice versa ? ? ( ?|? ) , as well as lexical weighting estimates ? ? ( ?|? ) and ? ? ( ?|? ) , word and phrase penalties , a linear distortion feature , and a lexicalized reordering feature .
The baseline includes a standard 5 - gram modified Kneser - Ney language model trained on the target side of the parallel corpora described below .
Log-linear weights are estimated with the MERT algorithm ( Och 2003 ) . Evaluation .
We test our models on two different data sets .
First , we train an English to French system based on the data of WMT 2006 shared task ( Koehn and Monz 2006 ) .
The parallel corpus includes 688 K sentence pairs of parliamentary proceedings for training .
The development set contains 2000 sentences , and the test set contains other 2000 sentences , all from the official WMT 2006 shared task .
Second , we experiment with a French to English system developed using 2.1 M sentence pairs of training data , which amounts to 102M words , from the WMT 2012 campaign .
The majority of the training data set is parliamentary proceedings except for 5 M words which are newswire .
We use the 2009 newswire data set , comprising 2525 sentences , as the development set .
We evaluate on four newswire domain test sets from 2008 , 2010 and 2011 as well as the 2010 system combination test set , containing 2034 to 3003 sentences .
In this study we perform a detailed empirical comparison using the WMT 2006 data set , and verify our best models and results using the larger WMT 2012 data set .
The metric used for evaluation is case insensitive BLEU score ( Papineni et al. 2002 ) .
We also perform a significance test using the Wilcoxon signed rank test .
Differences are considered statistically significant when the p-value is less than 0.05 .
Results of the CPTM
Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set , where Row 1 is the baseline system .
Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM .
Rows 5 to 7 present the results of previous models .
Row 8 is our best system .
Table 2 shows the main results on the WMT 2012 data set .
CPTM is the model described in Sections 4 .
As illustrated in Figure 2 , the number of the nodes in the input layer is the vocabulary size ?.
Both the hidden layer and the output layer have 100 nodes 6 .
That is , ? 1 is a ? ? 100 matrix and ?
2 a 100 ? 100 matrix .
The result shows that CPTM leads to a substantial improvement over the baseline system with a statistically significant margin of 1.0 BLEU points as in Table 1 .
We have developed a set of variants of CPTM to investigate two design choices we made in developing the CPTM : ( 1 ) whether to use a linear projection or a multi-layer nonlinear projection ; and ( 2 ) whether to compute the phrase similarity using word - word similarities as suggested by e.g. , the lexical weighting model ( Koehn et al. 2003 ) .
We these variants on the WMT 2006 data set , as shown in Table 1 . CPTML ( Row 3 in Table 1 ) uses a linear neural network to project a word vector of a phrase ? to a feature vector ? : ? ? ?(? ) = ?
T ? , where ? is a ? ? 100 projection matrix .
The translation score of a source phrase f and a target phrase e is measured as the similarity of their feature vectors .
We choose cosine similarity because it works better than dot product for linear projection .
CPTMW ( Row 4 in Table 1 ) computes the phrase similarity using word - word similarity scores .
This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations , such as the lexical weighting model ( Koehn et al. 2003 ) and the word factored n-gram translation model ( Son et al. 2012 ) .
Let ? denote a word , and ? and ? the source and target phrases , respectively .
We define sim ( ? , ? ) = where sim ? ( ? , ? ) ( or sim ? ( ? , ? ) ) is the wordphrase similarity , and is defined as a smooth approximation of the maximum function where ? is the tuned smoothing parameter .
Similar to CPTM , CPTMW also uses a nonlinear projection to map each word ( not a phrase vector as in CPTM ) to a feature vector .
Two observations can be made by comparing CPTM in Row 2 to its variants in Table 1 . First of all , it is more effective to model the phrase translation directly than decomposing it into wordword translations in the CPTMs .
Second , we see that the nonlinear projection is able to generate more effective features , leading to better results than the linear projection .
We also compare the best version of the CPTM i.e. , CPTM , with three related models proposed previously .
We start the discussion with the results on the WMT 2006 data set in Table 1 . Rows 5 and 6 in Table 1 are two state - of - theart latent semantic models that are originally trained on clicked query - document pairs ( i.e. , clickthrough data extracted from search logs ) for query - document matching ( Gao et al. 2011 ) .
To adopt these models for SMT , we view source - target sentence pairs as clicked query - document pairs , and trained both models using the same methods as in Gao et al . ( 2011 ) on the parallel bilingual training data described earlier .
Specifically , BTLMPR is an extension to PLSA , and is the best performer among different versions of the Bi-Lingual Topic Model ( BLTM ) described in Gao et al . ( 2011 ) . BLTM with Posterior Regularization ( BLTMPR ) is trained on parallel training data using the EM algorithm with a constraint enforcing a source sentence and its paralleled target sentence to not only share the same prior topic distribution , but to also have similar fractions of words assigned to each topic .
We incorporated the model into the log-linear model for SMT ( 1 ) as 7 Gao and He ( 2013 ) reported results of MRF models with different feature sets .
We picked the MRF using phrase features only ( MRFP ) for comparison since we are mainly interested in phrase representation .
follows .
First of all , the topic distribution of a source sentence ? ? , denoted by ?(?|? ? ) , is induced from the learned topic-word distributions using EM .
Then , each translation candidate ? in the N-best list GEN ( ? ? ) is scored as ?(?|? ? ) = ? ? ?(?|? ) ?(?|? ? ) ? ? ?(? ? |? ) can be similarly computed .
Finally , the logarithms of the two probabilities are incorporated into the log-linear model of ( 1 ) as two additional features .
DPM is the Discriminative Projection Model described in Gao et al . ( 2011 ) , which is an extension of LSA .
DPM uses a matrix to project a word vector of a sentence to a feature vector .
The projection matrix is learned on parallel training data using the S2 Net algorithm ( Yih et al. 2011 ) . DPM can be incorporated into the log-linear model for SMT ( 1 ) by introducing a new feature ? ?+ 1 for each phrase pair , which is defined as the cosine similarity of the phrases in the project space .
As we see from Table 1 , both latent semantic models , although leading to some slight improvement over Baseline , are much less effective than CPTM .
Finally , we compare the CPTM with the Markov Random Field model using phrase features ( MRFP in Tables 1 and 2 ) , proposed by Gao and He ( 2013 ) MRFp estimates one translation score for each phrase pair explicitly without parameter sharing , while in CPTM , all phrases share the same neural network that projects raw phrases to the continuous space , providing a more smoothed estimation of the translation score for each phrase pair .
The results in Tables 1 and 2 show that CPTM outperforms MRFP on most of the test sets across the two WMT data sets , but the difference between them is often not significant .
Our interpretation is that although CPTM provides a better smoothed estimation for low-frequent phrase pairs , which otherwise suffer the data sparsity issue , MRFp provides a more precise estimation for those high - frequent phrase pairs .
That is , CPTM and MRFp capture complementary information for translation .
We thus combine CPTM and MRFP ( Comb in Tables 1 and 2 ) by incorporating two features , each for one model , into the log-linear model of SMT ( 1 ) .
We observe that for both translation tasks , accuracy improves by up to 0.8 BLEU over MRFP alone ( e.g. , on the news2008 test set in Table 2 ) .
The results confirm that CPTM captures complementary translation information to MRFp .
Overall , we improve accuracy by up to 1.3 BLEU over the baseline on both WMT data sets .
Conclusions
The work presented in this paper makes two major contributions .
First , we develop a novel phrase translation model for SMT , where joint representations are exploited of a phrase in the source language and of its translation in the target language , and where the translation score of the pair of source - target phrases are represented as the distance between their feature vectors in a low-dimensional , continuous space .
The space is derived from the representations generated using a multilayer neural network .
Second , we present a new learning method to train the weights in the multilayer neural network for the end-to - end BLEU metric directly .
The training method is based on L-BFGS .
We describe in detail how the gradient in closed form , as required for efficient optimization , is derived .
The objective function , which takes the form of the expected BLEU computed from N-best lists , is very different from the usual objective functions used in most existing architectures of neural networks , e.g. , cross entropy ( Hinton et al. 2012 ) or mean square error ( Deng et al. 2012 ) .
We hence have provided details in the derivation of the gradient , which can serve as an example to guide the derivation of neural network learning with other non-standard objective functions in the future .
Our evaluation on two WMT data sets show that incorporating the continuous - space phrase translation model into the log-linear framework significantly improves the accuracy of a state- ofthe - art phrase - based SMT system , leading to a gain up to 1.3 BLEU .
Careful implementation of the L-BFGS optimization based on the BLEUcentric objective function , together with the associated closed - form gradient , is a key to the success .
A natural extension of this work is to expand the model and learning algorithm from shallow to deep neural networks .
The deep models are expected to produce more powerful and flexible semantic representations ( e.g. , Tur et al. , 2012 ) , and thus greater performance gain than what is presented in this paper .
Figure 1 . 1 Figure 1 .
The architecture of the CPTM , where the mapping from a phrase to its continuous representation is shown in Figure 2 .
Table 1 : 1 BLEU results for the English to French task using translation models and systems built on the WMT 2006 data set .
The superscripts ? and ? indicate statistically significant difference ( p < 0.05 ) from Baseline and CPTM , respectively .
1 |?| ? ? sim ? ( ? , ? ) + 1 |?| ? ? sim ? ( ? , ? ) where sim ? ( ? , ? ) ( or sim ? ( ? , ? ) ) is the word - phrase similarity , and is defined as a smooth ap- proximation of the maximum function sim ? ( ? , ? ) ? sim ( ? , ? ? ) exp(?sim ( ? , ? ? ) ) ? ? ? = ? exp(?sim ( ? , ? ? ) ) ? ? ? training is too slow to perform a detailed study within a rea- sonable time .
Therefore , all the models reported in this paper use 100 nodes .
56 ? 25.52 ? 21.64 ? 25.22 ? Table 2 : 2 7 , on both the WMT 2006 and WMT 2012 datasets .
MRFp is a state - of - the - art large scale discriminative training model that uses the same expected BLEU training criterion , which has proven to give superior performance across a range of MT tasks recently ( He and Deng 2012 , Setiawan and Zhou 2013 , Gao and He 2013 ) .
BLEU results for the French to English task using translation models and systems built on the WMT 2012 data set .
The superscripts ? and ? indicate statistically significant difference ( p < 0.05 ) from Baseline and MRFp , respectively .
same expected BLEU based objective function , CPTM and MRFp model the translation relationship between two phrases from different angles .
# Systems dev news2011 news2010 news2008 newssyscomb2010 1 Baseline 23.58 25.24 24.35 20.36 24.14 2 MRFP 24.07 ? 26.00 ? 24.90 20.84 ? 25.05 ? 3 CPTM 24.12 ? 26.25 ? 25.05 ? 21.15 ? 24.91 ? 4 Comb ( 2 + 3 ) 24.46 ? 26 .
Unlike CPTM , MRFp is a linear model that simply treats each phrase pair as a single feature .
Therefore , although both are trained using the
Niehues et al. ( 2011 ) use different translation units in order to integrate the n-gram translation model into the phrasebased approach .
However , it is not clear how a continuous
Our baseline system uses a set of standard features suggested in Koehn et al . ( 2007 ) , which is also detailed in Section 6 .
We can achieve slightly better results using more nodes in the hidden and output layers , say 500 nodes .
But the model
