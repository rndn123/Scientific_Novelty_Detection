title
Investigating Lowresource Machine Translation for EnglishtoTamil
abstract
Statistical machine translation ( SMT ) which was the dominant paradigm in machine translation ( MT ) research for nearly three decades has re cently been superseded by the endtoend deep learning approaches to MT .
Although deep neu ral models produce stateoftheart results in many translation tasks , they are found to under perform on resourcepoor scenarios .
Despite some success , none of the presentday bench marks that have tried to overcome this prob lem can be regarded as a universal solution to the problem of translation of many lowresource languages .
In this work , we investigate the performance of phrasebased SMT ( PBSMT ) and neural MT ( NMT ) on a rarelytested low resource languagepair , EnglishtoTamil , tak ing a specialised data domain ( software localisa tion ) into consideration .
In particular , we pro duce rankings of our MT systems via a social media platformbased human evaluation scheme , and demonstrate our findings in the lowresource domainspecific text translation task .
Introduction
In recent years , MT researchers have proposed ap proaches to counter the data sparsity problem and to improve the performance of NMT systems in low resource scenarios , e.g. augmenting training data from source and / or target monolingual corpora ( Sen nrich et al. , 2016a ?
Chen et al. , 2019 , unsupervised learning strategies in the absence of labeled data ( Artetxe et al. , 2018 ?
Lample et al. , 2018 , exploit ing training data involving other languages ( Firat et al. , 2017 ?
Johnson et al. , 2017 , multitask learning ( Niehues and Cho , 2017 ) , selection of hyperparam eters ( Sennrich and Zhang , 2019 ) , and pretrained language model finetuning ( Liu et al. , 2020 ) .
De spite some success , none of the existing benchmarks can be viewed as an overall solution as far as MT for lowresource languagepairs is concerned .
For examples , the backtranslation strategy of Sennrich et al . ( 2016a ) is less effective in lowresource set tings where it is hard to train a good backtranslation model ( Currey et al. , 2017 ) ? unsupervised MT does not work well for distant languages ( Marie and Fu jita , 2018 ) due to the difficulty of training unsuper vised crosslingual word embeddings for such lan guages ( S?gaard et al. , 2018 ) and the same is ap plicable in the case of transfer learning too ( Mon toya et al. , 2019 ) .
Hence , this line of research needs more attention from the MT research commu nity .
In this context , we refer interested readers to some of the papers ( Bentivogli et al. , 2016 ?
Castilho et al. , 2017 that compared PBSMT and NMT on a variety of usecases .
As for lowresource scenar ios , as mentioned above , many studies ( e.g. Koehn and Knowles ( 2017 ) ? ?stling and Tiedemann ( 2017 ) ?
Dowling et al. ( 2018 ) ) found that PBSMT can pro vide better translations than NMT , and many found the opposite results ( Casas et al. , 2019 ?
Sen et al. , 2019 ?
Sennrich and Zhang , 2019 .
Hence , the find ings of this line of MT research have indeed yielded a mixed bag of results , leaving the way ahead unclear .
In Ramesh et al. ( 2020 ) , we investigated the performance of PBSMT and NMT systems on two rarelytested underresourced languagepairs , EnglishtoTamil and HinditoTamil , taking a spe cialised data domain ( software localisation ) into ac count .
In particular , in Ramesh et al . ( 2020 ) , we carried out a comprehensive manual error analysis on the translations produced by our PBSMT and NMT systems .
This current work extends the work of Ramesh et al . ( 2020 ) in the following ways : ( a ) we present a social media platformbased human eval uation scheme for measuring the quality of transla tions generated by different MT systems , and ( b ) we select the PBSMT and NMT systems of the English toTamil translation task from Ramesh et al . ( 2020 ) and a commercial MT system , compare their perfor mances , and produce rankings of the three MT sys tems in terms of the length of the sentences to be translated using our proposed social media platform based human evaluation scheme .
The remainder of the paper is organized as follows .
Section 2 explains the experimental setup including the descriptions on our MT systems and details of the data sets used .
Section 3 presents the results with discussions and analysis , while Section 4 concludes our work with avenues for future work .
Experimental Setups
The MT systems
This section provides an overview of the PBSMT and NMT systems used for experimentation .
1 To build our PBSMT systems we used the Moses toolkit ( Koehn et al. , 2007 ) .
We used a 5 gram language model trained with modified Kneser Ney smoothing ( Kneser and Ney , 1995 ) using the KenLM toolkit ( Heafield et al. , 2013 ) .
Our PB SMT loglinear features include : ( a ) 4 translational features ( forward and backward phrase and lexi cal probabilities ) , ( b ) 8 lexicalised reordering proba bilities ( wbemslrbidirectionalfeallff ) , ( c ) 5 gram LM probabilities , ( d ) 5 OSM features ( Durrani et al. , 2011 ) , and ( e ) wordcount and distortion penalties .
The weights of the parameters are optimized using the margininfused relaxed algorithm ( Cherry and Foster , 2012 ) on the development set .
For decod ing , the cubepruning algorithm ( Huang and Chiang , 2007 ) is applied , with a distortion limit of 12 .
To build our NMT systems , we used the Open NMT toolkit ( Klein et al. , 2017 ) .
The NMT systems are Transformer models ( Vaswani et al. , 2017 ) .
The tokens of the training , evaluation and validation sets are segmented into subword units using BytePair Encoding ( BPE ) ( Sennrich et al. , 2016 b ) .
Recently , Sennrich and Zhang ( 2019 ) demonstrated that com monly used hyperparameter configurations do not provide the best results in lowresource settings .
Ac cordingly , we carried out a series of experiments in order to find the best hyperparameter configuration for Transformer in our lowresource settings .
In par ticular , we found that the following configuration lead to the best results in our lowresource transla tion settings : ( i ) the BPE vocabulary size : 8,000 , ( ii ) the sizes of encoder and decoder layers : 4 and 6 , re spectively , ( iii ) learningrate : 0.0005 , ( iv ) batch size ( token ) : 4,000 , and ( v) Transformer head size : 4 . As for the remaining hyperparameters , we followed the recommended best setup from Vaswani et al . ( 2017 ) .
The validation on the development set is performed using three cost functions : crossentropy , perplexity and BLEU ( Papineni et al. , 2002 ) .
The early stop ping criteria is based on crossentropy ?
however , the final NMT system is selected as per highest BLEU score on the validation set .
The beam size for search is set to 12 .
Choice of Languages
In an attempt to test MT on lowresource scenarios , we chose English and an Indian language : Tamil .
English and Tamil are Germanic and Dravidian lan guages , respectively , so the languages we selected for investigation are from different language fami lies and morphologically divergent to each other .
En glish is a less inflected language , whereas Tamil is a morphologically rich and highly inflected language .
Our investigation is from a less inflected language to a highly inflected language .
With this , we compare translation in PBSMT and NMT with a translation pair involving two morphologically divergent lan guages .
Data Used
This section presents the datasets used for MT sys tem building ( Ramesh et al. , 2020 ) .
For experi mentation we used data from three different sources : OPUS 2 ( Tiedemann , 2012 ) , WikiMatrix 3 ( Schwenk et al. , 2019 ) and PMIndia 4 ( Haddow and Kirefu , 2020 ) .
Corpus statistics are shown in Table 1 .
We carried out experiments using two different setups : ( i ) in the first setup , the MT systems were built on a training set compiled from all data domains listed above ?
we call this setup MIXED , and ( ii ) in the second setup , the MT systems were built on a train ing set compiled only from different software local isation data from OPUS , viz .
GNOME , KDE4 and Ubuntu ?
we call this setup IT .
The development and test set sentences were randomly drawn from these localisation corpora .
We adopted a number of standard cleaning rou tines for removing noisy sentences from the training corpora ( Ramesh et al. , 2020 ) .
In order to perform to kenisation for English , we used the standard tool in the Moses toolkit .
For tokenising and normalising Tamil sentences , we used the Indic NLP library .
5 3 Results and Discussion
Automatic Evaluation
We present the comparative performance of the PB SMT and NMT systems in terms of the widely used automatic evaluation metric BLEU ( Papineni et al. , 2002 ) .
Additionally , we performed statistical sig nificance tests using bootstrap resampling methods ( Koehn , 2004 ) . Sections 3.1.1 and 3.1.2 present the performance of the MT systems on the MIXED and IT setups , respectively .
The MIXED Setup
We show the BLEU scores on the test set in Table 2 .
The PBSMT and NMT systems produce rela tively low BLEU scores on the test set given the diffi culty of the translation pairs .
However , these BLEU scores underestimate the translation quality , given the relatively free word order in Tamil , and the fact that we have just a single reference translation set for evaluation .
We see from Table 2 that PBSMT sur passed NMT by a large margin in terms of BLEU , and found that the difference in the BLEU scores of the MT systems is statistically significant .
The IT Setup
This section presents the results obtained on the IT setup .
The BLEU scores of the MT systems are re ported in Table 3 .
When we compare the BLEU scores of this table with those of Table 2 , we see a huge rise in terms of the BLEU scores for PBSMT and NMT , and the improvements are found to be sta tistically significant .
As far as the IT setup is concerned , the PBSMT system outperforms the NMT system statistically
In a nutshell , when we compare PBSMT and NMT , we see that PBSMT is always the leading system across the training data setups ( MIXED and IT ) .
Reasons for very low BLEU Scores
The BLEU scores reported in the sections above are very low .
We looked at the translations of the test set sentences by the MT systems and compared them with the reference translations .
We found that de spite being good in quality , in many cases the trans lations were penalised heavily by the BLEU metric as a result of many ngram mismatches with the cor responding reference translations .
This happened mainly due to the nature of target language ( Tamil ) in question , i.e.
Tamil is a free word order language .
This is indeed responsible for the increase in non overlapping ngram counts .
We also found that trans lations contain lexical variations of Tamil words of the reference translation , again resulting in the in crease of the nonoverlapping ngram counts .
We show some of such translations in Table 4 . (
The MT System Ranking
Evaluation Plan
We further assess the quality of our MT systems ( the EnglishtoTamil PBSMT and NMT systems ) via a manual evaluation scheme .
For this , we select our PBSMT and NMT systems from the MIXED and IT setups .
Additionally , we considered Google Trans late ( GT ) 6 in this ranking task in order to compare it with PBSMT and NMT .
We randomly sampled a set of 100 source sentences from the test set ( cf. Ta ble 1 ) , and their translations by the MT systems in cluding GT .
In order to conduct this evaluation , we developed a webpage that was made available online and accessible to the evaluators who ranked the MT systems according to their translation quality .
We placed the sentences of the test set into three sets based on the sentence length measure ( source side ) , i.e. number of words ( nw ) < =3 , 3< nw < =9 , and nw >9 .
We call these sets sentencelength sets .
We recall Table 1 where the average sentence length of the English IT corpus is 7 .
This is the justification for our choice sentence length range .
We sam pled 100 sentences from the test set in such a way that the sentences are equally distributed over the sentencelength sets .
Thus , the first , second and third sentencelength sets contain 34 , 33 and 33 sen tences , respectively .
The webpage displays 10 sen tences together with the translations by the MT sys tems , which are taken from the sentencelength sets , with a minimum of 3 sentences from each set .
The evaluators who are native speakers of Tamil with good knowledge of English were instructed to rank the MT systems as per the quality of the translations from best to worst .
It was also possible that the eval uators could provide the same rank to more than one translation .
We disseminated the MT system ranking task via a variety of popular social media platforms , e.g. Linked In 7 and Facebook .
8
If we ask the evaluators to rank a large number of sentences , it is quite likely that they would not participate in the task .
Even if some people might like to participate in the task , they may lose interest in the middle and quit .
Therefore , we displayed translations in batches ( i.e. 10 source sentences and their translations ) on our webpage at any one time .
We did not consider any partial sub missions .
We observed that a total of 38 and 60 eval uators participated in the task for the MIXED and IT setups , respectively .
The submissions were then analysed to produce the final rankings of the MT sys tems .
In order to measure agreement in judgement , we used Fleiss 's Kappa .
9
The next section presents the ranking results .
Ranking Results
We adopted the idea of bilingual group pairwise judgements as in Papineni et al . ( 2002 ) in order to rank the MT systems .
We take the pairwise scores of three MT systems and linearly normalise them across the three systems .
We show our ranking re sults for the MIXED setup in the left half of Table 5 .
We see from the table that NMT is found to be the winner for first sentencelength set ( nw < =3 ) fol lowed by GT and PBSMT .
As for the other sentence lengthbased sets , GT becomes the winner followed by PBSMT and NMT .
The same trend is observed when the systems are ranked ignoring the sentence length measure .
We recall Table 2 where we pre sented the BLEU scores of our EnglishtoTamil MT systems ( PBSMT : 9.56 BLEU points and NMT : 4.35 BLEU points ) .
Additionally , we evaluated GT on our test set in order to compare it with PBSMT and NMT in this setting , and found that the GT MT system produced a 4.37 BLEU points on the test set .
We see that PBSMT is to the best choice and GT and NMT both are comparable if the MT systems are ranked according to the automatic evaluation scores .
Therefore , the automatic evaluation results contra dict the human ranking results above .
Using the submissions from the ranking task we also obtain the distributions of the translations by the PBSMT , NMT and GT MT systems over the three ranking positions , which are shown in the up per graph of Figure 1 .
We see here that the majority of the translations that the evaluators tagged as ' best ' ( cf. ' first ' in the upper graph of Figure 1 ) were from GT followed by NMT and PBSMT .
In case of the ' worst ' position ( cf. ' third ' in the upper graph of Fig ure 1 ) , we see that the majority of the translations are from the NMT systems followed by the PBSMT and GT MT systems .
When we look at the second posi tion , we see that PBSMT is the winner and NMT and GT are nearly neckandneck .
The ranking results for the IT setup are presented kappa
PB - SMT NMT GT in the right half of Table 5 .
This time , we see that NMT is the winner for first sentencelength set ( nw < =3 ) followed by PBSMT and GT .
As for the other sentencelengthbased sets and whole test set ( 100 sentences ) , PBSMT becomes the winner fol lowed by NMT and GT .
The distributions of the translations by the MT systems over the three rank ing positions are shown in the lower graph of Figure 1 .
We see that the majority of the translations that are tagged as ' best ' were from PBSMT followed by NMT and GT .
In case of the ' worst ' position , we see that the majority of the translations are from the GT system followed by the NMT and PBSMT systems .
When we look at the second position , we see that NMT is the winner and PBSMT is not far behind , and the same is true for PBSMT and GT too .
As for the first set of sentences ( i.e. short sen tences ( nw < =3 ) ) , we observed that the translations by the NMT systems are found to be more mean ingful compared to those by the other MT systems .
This is true for both the MIXED and IT setups .
As an example , the English sentence ' Nothing ' is trans lated as ? ? ( ' nothing ' ) in Tamil by the NMT system , which , however , is translated as ? ( ' anything ' ) in Tamil by the PBSMT sys tem .
On completion of our ranking process , we com puted the interannotator agreements using Fleiss 's Kappa for the three ranking positions first , second and third , which are 74.1 , 58.4 and 67.3 , respec tively , for the MIXED setup and 75.3 , 55.4 and 70.1 , respectively , for the IT setup .
A Kappa coefficient between 0.60.8 represents substantial agreement .
In this sense , there is substantial agreement among the evaluators when they select positions for the MT sys tems .
Conclusion
In this paper , we investigated NMT and PBSMT in resourcepoor conditions .
For this , we chose a spe cialised data domain ( software localisation ) for trans lation and a rarelytested morphologically divergent lowresource languagepair , EnglishtoTamil .
We studied translations in two setups , i.e. training data compiled from ( i ) freely available variety of data do mains ( e.g. political news , Wikipedia ) , and ( ii ) ex clusively software localisation data domains .
In ad dition to an automatic evaluation , we randomly se lected one hundred sentences from the test set , and ranked our MT systems via a social media platform based human evaluation scheme .
We also consid ered a commercial MT system , Google Translate , in this ranking task .
We found that use of indomain data only at train ing has a positive impact on translation from English toTamil .
We looked at the translations produced by our MT systems and found that in many cases , the BLEU scores underestimate the translation quality mainly due to relatively free word order in Tamil .
In this regard , both Shterionov et al . ( 2018 ) and Way ( 2018 ) note that BLEU may be underreporting the difference in quality seen when using NMT systems , with the former attempting to measure the level of underreporting using a set of novel metrics .
Way ( 2018 ) reminds the MT community how important subjective evaluation is in MT and there is no easy replacement of that in MT evaluation .
We refer the interested readers to Way ( 2019 ) who also drew at tention to this phenomenon .
From our human ranking task we found that sentencelength could be a crucial factor for the per formance of the NMT systems in lowresource sce narios , i.e. NMT turns out to be bestperforming for very short sentences ( number of words <= 3 ) .
This finding indeed does not correlate with the find ings of our automatic evaluation process , where PB SMT is found to be the bestperforming , and GT and 123 NMT are comparable .
This finding could be inter est to translation service providers who use MT in their production for lowresource languages and may exploit the MT models based on the length of the source sentences to be translated .
GT becomes the winner followed by PBSMT and NMT for the sentences of other lengths ( number of words > 3 ) in the MIXED setup , and PBSMT be comes the winner followed by NMT and GT for the sentences of other lengths ( number of words > 3 ) in the IT setup .
Overall , the human evaluators ranked GT as the first choice , PBSMT as the second choice and NMT as the third choice MT systems in the MIXED setup .
As for the IT setup , PBSMT was the first choice , NMT was the second choice and GT was the third choice MT systems .
We believe that the findings of this work pro vide significant contributions to this line of MT re search .
In future , we intend to consider more lan guages from different language families .
We also plan to include stringbased MT evaluation metrics such as chrF ( Popovi ? , 2015 ) in our investigation , which have been shown to better reflect the actual performance improvement of NMT .
Figure 1 : 1 Figure 1 : Distributions of translations over three positions ( Mixed ( top ) and IT ( bottom ) setups ) .
