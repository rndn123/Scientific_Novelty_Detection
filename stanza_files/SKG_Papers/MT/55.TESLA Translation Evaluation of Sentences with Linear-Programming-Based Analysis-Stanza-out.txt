title
TESLA : Translation Evaluation of Sentences with Linear-programming - based Analysis
abstract
We present TESLA -M and TESLA , two novel automatic machine translation evaluation metrics with state - of - the - art performances .
TESLA -M builds on the success of METEOR and MaxSim , but employs a more expressive linear programming framework .
TESLA further exploits parallel texts to build a shallow semantic representation .
We evaluate both on the WMT 2009 shared evaluation task and show that they outperform all participating systems in most tasks .
Introduction
In recent years , many machine translation ( MT ) evaluation metrics have been proposed , exploiting varying amounts of linguistic resources .
Heavyweight linguistic approaches including RTE ( Pado et al. , 2009 ) and ULC ( Gim?nez and M?rquez , 2008 ) performed the best in the WMT 2009 shared evaluation task .
They exploit an extensive array of linguistic features such as parsing , semantic role labeling , textual entailment , and discourse representation , which may also limit their practical applications .
Lightweight linguistic approaches such as ME-TEOR ( Banerjee and Lavie , 2005 ) , MaxSim ( Chan and Ng , 2008 ) , wpF and wpBleu ( Popovi ? and Ney , 2009 ) exploit a limited range of linguistic information that is relatively cheap to acquire and to compute , including lemmatization , part-ofspeech ( POS ) tagging , and synonym dictionaries .
Non-linguistic approaches include BLEU ( Papineni et al. , 2002 ) and its variants , TER ( Snover et al. , 2006 ) , among others .
They operate purely at the surface word level and no linguistic resources are required .
Although still very popular with MT researchers , they have generally shown inferior performances than the linguistic approaches .
We believe that the lightweight linguistic approaches are a good compromise given the current state of computational linguistics research and resources .
In this paper , we devise TESLA -M and TESLA , two lightweight approaches to MT evaluation .
Specifically : ( 1 ) the core features are Fmeasures derived by matching bags of N-grams ; ( 2 ) both recall and precision are considered , with more emphasis on recall ; and ( 3 ) WordNet synonyms feature prominently .
The main novelty of TESLA - M compared to METEOR and MaxSim is that we match the Ngrams under a very expressive linear programming framework , which allows us to assign weights to the N-grams .
This is in contrast to the greedy approach of METEOR , and the more restrictive maximum bipartite matching formulation of MaxSim .
In addition , we present a heavier version TESLA , which combines the features using a linear model trained on development data , making it easy to exploit features not on the same scale , and leaving open the possibility of domain adaptation .
It also exploits parallel texts of the target language with other languages as a shallow semantic representation , which allows us to model phrase synonyms and idioms .
In contrast , METEOR and MaxSim are capable of processing only word synonyms from WordNet .
The rest of this paper is organized as follows .
Section 2 gives a high level overview of the evaluation task .
Sections 3 and 4 describe TESLA -M and TESLA , respectively .
Section 5 presents experimental results in the setting of the WMT 2009 shared evaluation task .
Finally , Section 6 concludes the paper .
Overview
We consider the task of evaluating machine translation systems in the direction of translating the source language to the target language .
Given a reference translation and a system translation , the goal of an automatic machine translation evaluation algorithm such as TESLA ( -M ) is to output a score predicting the quality of the system translation .
Neither TESLA - M nor TESLA requires the source text , but as additional linguistic resources , TESLA makes use of phrase tables generated from parallel texts of the target language and other languages , which we refer to as pivot languages .
The source language may or may not be one of the pivot languages .
TESLA -M
This section describes TESLA -M , the lighter one among the two metrics .
At the highest level , TESLA - M is the arithmetic average of Fmeasures between bags of N-grams ( BNGs ) .
A BNG is a multiset of weighted N-grams .
Mathematically , a BNG
B consists of tuples ( b i , b W i ) , where each b i is an N-gram and b W i is a positive real number representing its weight .
In the simplest case , a BNG contains every N-gram in a translated sentence , and the weights are just the counts of the respective N-grams .
However , to emphasize the content words over the function words , we discount the weight of an N-gram by a factor of 0.1 for every function word in the Ngram .
We decide whether a word is a function word based on its POS tag .
In TESLA -M , the BNGs are extracted in the target language , so we call them bags of target language N-grams ( BTNGs ) .
Similarity functions
To match two BNGs , we first need a similarity measure between N-grams .
In this section , we define the similarity measures used in our experiments .
We adopt the similarity measure from MaxSim as s ms .
For unigrams x and y , ? If lemma( x ) = lemma( y ) , then s ms = 1 . ?
Otherwise , let a = I(synsets ( x ) overlap with synsets ( y ) ) b = I ( POS ( x ) = POS ( y ) ) where I ( ? ) is the indicator function , then s ms = ( a + b ) / 2 .
The synsets are obtained by querying WordNet ( Fellbaum , 1998 ) .
For languages other than English , a synonym dictionary is used instead .
We define two other similarity functions between unigrams : s lem ( x , y ) = I( lemma ( x ) = lemma( y ) ) s pos ( x , y ) = I ( POS ( x ) = POS ( y ) )
All the three unigram similarity functions generalize to N-grams in the same way .
For two N-grams x = x 1,2 , ... , n and y = y 1,2 , ... , n , s( x , y ) = 0 if ?i , s( x i , y i ) = 0 1 n n i=1 s( x i , y i ) otherwise 3 .
Matching two BNGs
Now we describe the procedure of matching two BNGs .
We take as input the following :
1 . Two BNGs , X and Y .
The ith entry in X is x i and has weight x W i ( analogously for y j and y W j ) .
2 . A similarity measure , s , that gives a similarity score between any two entries in the range of 0 to 1 .
Intuitively , we wish to align the entries of the two BNGs in a way that maximizes the overall similarity .
As translations often contain one- to - many or many - to - many alignments , we allow one entry to split its weight among multiple alignments .
An example matching problem is shown in Figure 1a , where the weight of each node is shown , along with the similarity for each edge .
Edges with a similarity of zero are not shown .
The solution to the matching problem is shown in Figure 1 b , and the overall similarity is 0.5 ? 1.0 + 0.5 ? 0.6 + 1.0 ? 0.2 + 1.0 ? 0.1 = 1.1 .
Mathematically , we formulate this as a ( realvalued ) linear programming problem 1 . The variables are the allocated weights for the edges
The value of the objective function is the overall similarity S. Assuming X is the reference and Y is the system translation , we have w( x i , y j ) ?i , j
We maximize i , j s( x i , y j ) w( x i , y j ) subject to w( x i , y j ) ?
0 ?i , j j w( x i , y j ) ? x W i ? i i w( x i , y j ) ? y W Precision = S j y W j Recall = S i x W i The F-measure is derived from the precision and the recall : F = Precision ? Recall ? ? Precision + ( 1 ? ? ) ? Recall
In this work , we set ? = 0.8 , following MaxSim .
The value gives more importance to the recall than the precision .
Scoring
The TESLA - M sentence - level score for a reference and a system translation is the arithmetic average of the BTNG F-measures for unigrams , bigrams , and trigrams based on similarity functions s ms and s pos .
We thus have 3 ? 2 = 6 features for TESLA -M .
We can compute a system- level score for a machine translation system by averaging its sentencelevel scores over the complete test set .
Reduction
When every x W i and y W j is 1 , the linear programming problem proposed above reduces to weighted bipartite matching .
This is a well known result ; see for example , Cormen et al . ( 2001 ) for details .
This is the formalism of MaxSim , which precludes the use of fractional weights .
If the similarity function is binary - valued and transitive , such as s lem and s pos , then we can use a much simpler and faster greedy matching procedure : the best match is simply g min( x i =g x W i , y i =g y W i ) .
TESLA
Unlike the simple arithmetic average used in TESLA -M , TESLA uses a general linear combination of three types of features : BTNG Fmeasures as in TESLA -M , F-measures between bags of N-grams in each of the pivot languages , called bags of pivot language N-grams ( BPNGs ) , and normalized language model scores of the system translation , defined as 1 n log P , where n is the length of the translation , and P the language model probability .
The method of training the linear model depends on the development data .
In the case of WMT , the development data is in the form of manual rankings , so we train SVM rank ( Joachims , 2006 ) on these instances to build the linear model .
In other scenarios , some form of regression can be more appropriate .
The rest of this section focuses on the generation of the BPNGs .
Their matching is done in the same way as described for BTNGs in the previous section .
Phrase level semantic representation Given a sentence - aligned bitext between the target language and a pivot language , we can align the text at the word level using well known tools such as GIZA ++ ( Och and Ney , 2003 ) or the Berkeley aligner ( Liang et al. , 2006 ; Haghighi et al. , 2009 ) .
We observe that the distribution of aligned phrases in a pivot language can serve as a semantic representation of a target language phrase .
That is , if two target language phrases are often aligned to the same pivot language phrase , then they can be inferred to be similar in meaning .
Similar observations have been made by previous researchers ( Bannard and Callison - Burch , 2005 ; Callison - Burch et al. , 2006 ; Snover et al. , 2009 ) .
We note here two differences from WordNet synonyms : ( 1 ) the relationship is not restricted to the word level only , and ( 2 ) the relationship is not binary .
The degree of similarity can be measured by the percentage of overlap between the semantic representations .
For example , at the word level , the phrases good morning and hello are unrelated even with a synonym dictionary , but they both very often align to the same French phrase bonjour , and we conclude they are semantically related to a high degree .
Segmenting a sentence into phrases
To extend the concept of this semantic representation of phrases to sentences , we segment a sentence in the target language into phrases .
Given a phrase table , we can approximate the probability of a phrase p by : P r( p ) = N ( p ) p N ( p ) ( 1 ) where N ( ? ) is the count of a phrase in the phrase table .
We then define the likelihood of segmenting a sentence S into a sequence of phrases ( p 1 , p 2 , . . . , p n ) by : P r( p 1 , p 2 , . . . , p n | S ) = 1 Z( S ) n i=1 P r( p i ) ( 2 ) where Z( S ) is a normalizing constant .
The segmentation of S that maximizes the probability can be determined efficiently using a dynamic programming algorithm .
The formula has a strong preference for longer phrases , as every P r( p ) is a small fraction .
To deal with out - of- vocabulary ( OOV ) words , we allow any single word w to be considered a phrase , and if N ( w ) = 0 , we set N ( w ) = 0.5 instead .
BPNGs as sentence level semantic representation Simply merging the phrase-level semantic representation is insufficient to produce a sensible sentence - level semantic representation .
As an example , we consider two target language ( English ) sentences segmented as follows :
We can collect the N-gram statistics of this ensemble of French sentences efficiently from the confusion network representation .
For example , the trigram Bonjour , Querrien 2 would receive a weight of 0.9 ? 1.0 = 0.9 in Figure 2 . As with BTNGs , we discount the weight of an N-gram by a factor of 0.1 for every function word in the N-gram , so as to place more emphasis on the content words .
The collection of all such N-grams and their corresponding weights forms the BPNG of a sentence .
The reference and system BPNGs are then matched using the algorithm outlined in Section 3.2 .
Scoring
The TESLA sentence - level score is a linear combination of ( 1 ) BTNG F-measures for unigrams , bigrams , and trigrams based on similarity functions s ms and s pos , ( 2 ) BPNG
F-measures for unigrams , bigrams , and trigrams based on similarity functions s lem and s pos for each pivot language , and ( 3 ) normalized language model scores .
In this work , we use two language models .
We thus have 3 ? 2 features from the BTNGs , 3 ? 2 ? # pivot languages features from the BPNGs , and 2 features from the language models .
Again , we can compute system-level scores by averaging the sentence - level scores .
Experiments
Setup
We test our metrics in the setting of the WMT 2009 evaluation task ( Callison - Burch et al. , 2009 ) .
The manual judgments from WMT 2008 are used as the development data and the metric is evaluated on WMT 2009 manual judgments with respect to two criteria : sentence level consistency and system level correlation .
The sentence level consistency is defined as the percentage of correctly predicted pairs among all the manually judged pairs .
Pairs judged as ties by humans are excluded from the evaluation .
The system level correlation is defined as the average Spearman 's rank correlation coefficient across all translation tracks .
Pre-processing
We POS tag and lemmatize the texts using the following tools : for English , OpenNLP POS - tagger 3 and WordNet lemmatizer ; for French and German , TreeTagger 4 ; for Spanish , the FreeLing toolkit ( Atserias et al. , 2006 ) ; and for Czech , the Morce morphological tagger 5 . For German , we additionally perform noun compound splitting .
For each noun , we choose the split that maximizes the geometric mean of the frequency counts of its parts , following the method in ( Koehn and Knight , 2003 ) : max n, p 1 , p 2 , ... , pn n i=1 N ( p i ) 1 n The resulting compound split sentence is then POS tagged and lemmatized .
Finally , we remove all non-alphanumeric tokens from the text in all languages .
To generate the language model features , we train SRILM ( Stolcke , 2002 ) trigram models with modified Kneser - Ney discounting on the supplied monolingual Europarl and news commentary texts .
We build phrase tables from the supplied news commentary bitexts .
Word alignments are produced by the Berkeley aligner .
The widely used phrase extraction heuristic in is used to extract phrase pairs and phrases of up to 4 words are collected .
Into-English task
For each of the BNG features , we generate three scores , for unigrams , bigrams , and trigrams respectively .
For BPNGs , we generate one such triple for each of the four pivot languages supplied , namely Czech , French , German , and Spanish .
1 compares the scores of TESLA and TESLA - M against three participants in WMT 2009 under identical settings 6 : ULC ( a heavyweight linguistic approach with the best performance in WMT 2009 ) , MaxSim , and ME-TEOR .
The results show that TESLA outperforms all these systems by a substantial margin , and TESLA - M is very competitive too .
Out - of- English task A synonym dictionary is required for target languages other than English .
We use the freely available Wiktionary dictionary 7 for each language .
For Spanish , we additionally use the Spanish WordNet , a component of FreeLing .
Only one pivot language ( English ) is used for the BPNG .
For the English - Czech task , we only have one language model instead of two , as the Europarl language model is not available .
Tables 2 and 3 show the sentence - level consistency and system-level correlation respectively of TESLA and TESLA - M against the best reported results in WMT 2009 under identical setting .
The results show that both TESLA and TESLA - M give very competitive performances .
Interestingly , TESLA and TESLA - M obtain similar scores in the out - of - English task .
This could be because we use only one pivot language ( English ) , compared to four in the into-English task .
We plan to investigate this phenomenon in our future work .
Conclusion
This paper describes TESLA -M and TESLA .
Our main contributions are : ( 1 ) we generalize the bipartite matching formalism of MaxSim into a more expressive linear programming framework ; 6
The original WMT09 report contained erroneous results .
The scores here are the corrected results released after publication .
7 www.wiktionary.org
Figure 1 : A BNG matching problem
