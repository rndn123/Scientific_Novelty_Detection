title
Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting
abstract
Unsupervised machine translation ( MT ) has recently achieved impressive results with monolingual corpora only .
However , it is still challenging to associate source -target sentences in the latent space .
As people speak different languages biologically share similar visual systems , the potential of achieving better alignment through visual content is promising yet under-explored in unsupervised multimodal MT ( MMT ) .
In this paper , we investigate how to utilize visual content for disambiguation and promoting latent space alignment in unsupervised MMT .
Our model employs multimodal back -translation and features pseudo visual pivoting in which we learn a shared multilingual visual-semantic embedding space and incorporate visuallypivoted captioning as additional weak supervision .
The experimental results on the widely used Multi30 K dataset show that the proposed model significantly improves over the state- ofthe - art methods and generalizes well when images are not available at the testing time .
Introduction Neural machine translation ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ) has achieved near human-level performance ( Wu et al. , 2016 ) .
However , its effectiveness strongly relies on the availability of large-scale parallel corpora .
Unfortunately , preparing the parallel data remains a challenge as there are more than 6,500 languages in the world , and recruiting translators with bilingual or multilingual knowledge to cover all those languages is impractical .
As a result , developing methods alleviating the need of well - annotated large parallel corpora has recently attracted increasing attention in the community .
These methods fall into two broad categories .
The first type of methods use a third language as the pivot ( Firat et al. , 2016 ; Johnson et al. , 2017 ) to enable zero-resource translation .
Although the progress is encouraging , pivoting with a third language still demands bilingual knowledge for collecting large-scale parallel source-pivot and pivottarget corpora .
The second type of methods explore unsupervised approaches Artetxe et al. , 2018 ; have recently achieved impressive translation quality .
These methods rely only on monolingual data and back - translation ( Sennrich et al. , 2016 a ) .
However , as discussed in , the alignment of source-target sentences is uncertain and highly subject to proper initialization .
Using visual content for unsupervised MT Su et al. , 2019 ) is a promising solution for pivoting and alignment based on its availability and feasibility .
Abundant multimodal content in various languages are available online ( e.g. Instagram and YouTube ) .
It is also easier to recruit monolingual annotators to describe an image than to find multilingual translators to translate sentences .
Importantly , visual content is eligible to improve the alignments in the language latent spaces since the physical visual perception is similar among people speaking different languages ( e.g. similar " blue car " for a German and a French ) .
Based on these insights , we propose a novel unsupervised multimodal MT framework incorporating images as pseudo pivots promoting latent space alignment .
In addition to use features of visual objects for multimodal back - translation , we align a shared multilingual visual -semantic embedding ( VSE ) space via leveraging disjoint image-sentence pairs in different languages .
As illustrated in Figure 2 , for sentences approximately pivoted by similar images ( src-img - tgt ) , drawing embeddings of corresponding image-sentence pairs closer results in better alignments of semantically equivalent sentences in the language latent spaces .
Inspired by back - translation , we further explore another pseudo pivoting strategy , which approximates multilingual sentence pairs ( src-img - tgt ) conditioned on a real image via captioning .
Instead of using annotation of images for pivoting as in , we generate sentences in two languages pivoted on the real image , and then approximately pairing them as weak supervision for training unsupervised MT system .
This approach is analogous to a cross-modal version of back -translation .
We make the following contributions : ( 1 ) Building a unified view of employing visual content for pseudo pivoting .
( 2 ) We learn and improve the alignments in the shared multilingual multimodal embedding space for unsupervised MMT with disjoint image - text pairs in different languages .
( 3 ) Our model achieves state of the art on Multi30 K and generalizes well to the text-only scenario .
Background Neural Machine Translation Typical NMT models are based on the encoder-decoder framework with attention ( Bahdanau et al. , 2015 ) . Let x = ( x 1 , ? ? ? , x N ) denotes a source sentence and y = ( y 1 , ? ? ? , y M ) denotes a target sentence , where ( x , y ) ? ( X , Y ) .
The encoder- decoder model learns to estimate the following likelihood from the source sentence to the target sentence : p x?y ( y|x ) = M i=1 p(y i |y < i , x ) ( 1 )
When a parallel corpus is available , the maximum likelihood estimation ( MLE ) is usually adopted to optimize the ( source to target language ) NMT model by minimizing the following loss : L M T x?y = E ( x,y ) ? ( X , Y ) [ ? log p x?y ( y | x ) ]
( 2 ) Among all encoder-decoder models , the Transformer ( Vaswani et al. , 2017 ) architecture recently achieves state - of - the - art translation quality .
Instead of using recurrent or convolutional operations , it facilitates multi-head self-attention .
In this paper , we choose the Transformer as the underlying architecture for both the translation and the captioning modules .
Unsupervised Machine Translation
While conventional MT systems rely on the availability of a large parallel corpus , translation with zero-resource ( unsupervised MT ) Artetxe et al. , 2018 ; has drawn increasing research attention .
Only monolingual sentences are presented at the training and validation phase , i.e. , only x ?
X and y ?
Y are available .
Successful unsupervised MT systems share several common principles .
First , they require the pre-training step to initialize the model and establish strong monolingual language models properly .
For example , XLM ( Conneau and Lample , 2019 ) utilizes the masked language model objective in BERT ( Devlin et al. , 2019 ) . MASS ( Song et al. , 2019 ) utilizes a span-based sequence - to-sequence masking objective for language model pre-training .
Second , these systems transform the unsupervised problem into a weakly or self-supervised one by automatically generating pseudo sentence pairs via back - translation ( Sennrich et al. , 2016 a ) .
The idea behind can be analogous to the cycleconsistency objective in CycleGAN ( Zhu et al. , 2017 ) for image-image translation with unpaired data .
Specifically , let us denote by h * ( y ) = ( x 1 , ? ? ? , xN ) the sentence in the source language inferred from y ?
Y such that h * ( y ) = argmax p y?x ( x|y ) .
Similarly , let us denote by g * ( x ) = ( ?
1 , ? ? ? , ?M ) the sentence in the target language inferred from x ?
X such that g * ( x ) = argmax p x?y ( y| x ) .
Then the " pseudo " parallel sentences ( h * ( y ) , y ) and ( x , g * ( x ) ) can be further used to train two two MT models ( X ? Y and Y ? X ) by minimizing the following backtranslation loss : L BT x?y = E x?X [ ? log p y?x ( x|g * ( x ) ) ] + E y?Y [ ? log p x?y ( y|h * ( y ) ) ]
( 3 ) Although reinforcement learning - based approaches ( He et al. , 2016a ) and Gumbel-softmax reparametrization ( Maddison et al. , 2017 ) have been used to handle back - propagation thorough non-differentiable " argmax " predictions .
in this paper , we do not back - propagate through h * ( y ) and g * ( x ) to simplify the training process .
Unsupervised Multimodal Machine Translation
As illustrated in Figure 1 , our model is composed of seven modules :
Two encoder- decoder pairs for translation , two decoders for captioning , and one shared visual encoder .
In this section , we first detail our basic MMT model architecture and the unsupervised setup .
Then we introduce pseudo visual pivoting : learning multilingual VSE and pivoted captioning .
Multimodal MT Multimodal machine translation Figure 1 : The proposed model structure ( English ?
German ) .
We incorporate visual objects for unsupervised multimodal MT and improve the language latent space alignment with pseudo visual pivoting ( ?3.3 - ?3.4 ) .
complementary information source for MT .
An image z and the description in two languages form a triplet ( x , y , z ) ? ( X , Y , Z ) .
The Transformer encoder reads the source sentence and encodes it with hierarchical self-attention into h x = {h x 1 , ? ? ? , h x N } , h x i ?
R d , where d is the dimension of the embedding space .
The visual encoder encodes the image into Su et al. , 2019 ) use 2D ( K = 14? 14 ) feature maps of ImageNet pre-trained ResNet ( He et al. , 2016 b ) .
In contrast , we utilize the regional features of K salient visual objects in an image extracted by Faster-RCNN ( Ren et al. , 2015 ) and a 1 - layer MLP as the encoder to encode visual objects .
h z = {h z 1 , ? ? ? , h z K } , h z i ?
R d , K max = 36 .
Most previous work Various attention strategies for sequence - tosequence learning have been addressed in ( Libovick ?
and Helcl , 2017 ) .
Our model employs the hierarchical multi-head multimodal attention for decoding .
For decoding at time stamp i , the textual attention Attn( h y i , h x ) computes the context vector c i = j ?
j h x j via a attention - based alignment ? j = Align( h y i , h x j ) , where j ? j = 1 and h y i is the decoder state .
Essentially , the one-head attention in Transformer is implemented as c i = softmax ( Q i ( K x ) / ? d) V x where { Q , K x , V x } are the packed d-dimensional Query , Key , Value vectors , which are the mapped and packed version of {h y i , h x , h x }.
For decoding with encoded visual and textual inputs , we utilize multimodal attention to compute the context vector c i : c x i = Attn( h y i?1 , h x ) + ? v Attn( h y i?1 , h z ) ( 4 ) In practice we set ? v = 1.0 .
Our multimodal decoder models the likelihood to predict the next token as : p( y i |y < i , x , z ) = softmax ( f ( c i , y i?1 , h y i?1 ) , ( 5 ) where f ( . ) denotes the aggregated non-linear feature mapping in Transformer .
Unsupervised Learning Unsupervised multimodal MT ( Nakayama and Nishida , 2017 ; Su et al. , 2019 ) poses a new yet challenging problem .
On both the source and target sides , only non-overlapping monolingual multimodal data are presented for training and validation .
Specifically , the data available are : ( x , z x ) ?
( X , Z ) , (y , z y ) ? ( Y , Z ) , such that { x } ? {y } = ? , {z x } ? {z y } = ?.
Note that there are no parallel translation pairs available ( unsupervised ) , and the images are mutually exclusive for different languages .
For multimodal back - translation , the generated pseudo target sentence conditioned on the source sentence and image can be re-written as g * ( x , z x ) = argmax p xz?y ( y|x , z x ) , where p xz?y ( y|x , z ) = M i=1 p(y i |y < i , x , z ) .
Similar for p yz ?x ( x|y , z ) and h * ( y , z y ) .
For unsupervised multimodal MT , the multimodal back -translation objective can be extended as : L M BT x?y = E ( x,zx ) - log p yz ?x ( x|g * ( x , z x ) , z x ) + E ( y, zy ) - log p xz?y y|h * ( y , z y ) , z y ) ( 6 ) We simplify the notation of expectation for clarity .
Aligning the latent spaces of the source and target languages without supervision is challenging , as discussed in .
However , as people speak different languages biologically share similar visual systems , we envision that the shared visual space can serve as the pivot for alignment .
Unlike most previous work Su et al. , 2019 ) treating images merely as a feature , we propose two visual pivoting approaches : ( 1 ) Aligning the multilingual VSE space ; ( 2 ) Image pseudo pivoting via captioning .
As illustrated in Figure 2 , for ( 1 ) , we use images as the approximate pivots connecting real non-parallel sentences .
( src-imgtgt . )
In ( 2 ) , for each pivoting real image , we gener - a dog running in a field ein hund l?uft in einer wiese a biker with a white helmet is in midair .
ate captions in both languages to construct " pseudo " source - target sentence pairs .
( src-img - tgt ) , where the italic item is " pseudo " .
We collectively term the proposed approach pseudo visual pivoting .
Multilingual Visual-Semantic Embedding
We posit that for X , Y , Z , the two language spaces X , Y could be properly associated by respectively aligning two monolingual VSE spaces X ? Z and Y ? Z .
We leverage the contrastive objective in cross-modal retrieval ( Kiros et al. , 2014 ; Huang et al. , 2019 b ) for aligning multimodal inputs in the shared VSE space where the embeddings are close if they are semantically associated or paired .
Specifically , we generalize the fine- grained ( object- level and token - level ) , monolingual textualto-visual , and visual- to- textual attention Huang et al. , 2019 c ) into the multilingual setup .
For fine- grained image-sentence alignment , let s ij = cos( h x i , h z j ) denotes the cosine similarity between the i-th encoded token and the j-th encoded visual object .
The image-sentence similarity can be measured by averaging the cosine similarities between the visually - attend sentence embeddings and the visual embeddings of the objects .
The visually - attended sentence embeddings h zx are the weighted combination of the encoded tokens h x .
Precisely , we compute h zx j = N i=1 ? ij h x i , where j = 1 ? ? ? K and ? ij = softmax i ( s ij ) .
Let us denote by S( x , z ) = 1 2 K K j=1 cos( h zx j , h z j ) + 1 2N N i=1 cos( h xz i , h x i ) as the image-sentence similarity , the contrastive triplet loss encouraging image-sentence alignment in the VSE space can be written as : L c ( x , z ) = max x ? ? S( x , z ) + S( x , z ) + + max z ? ? S ( x , z ) + S ( x , z ) + , ( 7 ) where [ .] + is the hinge function , and x and z are the non-paired ( negative ) instances for x and z .
Intuitively , when the loss decreases , the matched images and sentences will be drawn closer down to a margin ? than the hardest non-paired ones .
Formally , we minimizing the following objective for cross-modal alignments in the two VSE spaces : L V SE x,y , z = E ( x, zx ) L c ( x , z x ) + E ( y , zy ) L c ( y , z y ) ( 8 )
Image Captioning for Pseudo Pivoting Inspired by back -translation with monolingual corpora , we propose a novel cross-modal approach to generate weakly - supervised pairs to guide language space alignment for unsupervised MMT .
Precisely , we leverage image captioning to synthesize pseudo sentence pairs ( pivoted and conditioned on the image ) for back - translation and paired - translation .
Image Captioning Image captioning models are akin to MT models besides the non-sequential visual encoder .
For example , an image- to- source captioning model estimates the likelihood as p z?x ( x|z ) = N i=1 p( x i |x < i , z ) , where z is the encoded image .
Essentially , the captioning model learns to minimize the following loss : L CAP z?x = E ( zx , x ) [ ? log p z?x ( x|z x ) ] ( 9 ) As illustrated in Figure 2 , we incorporate two captioning models Z ? X and Z ?
Y to generate additional " pseudo " parallel sentences pivoted on the image as additional weak supervision to better align language latent spaces in unsupervised MMT .
For example , with Image ?
English and Image ?
German , the generated pseudo ( English , German ) pair is then pivoted on the Image .
Learning captioning models is practical as it is easier to collect large-scale image - text pairs than translation pairs .
We pre-train these captioning models and use them to generate sentences in two languages depicting the same image , i.e. , c * x ( z x ) = argmaxp z?x ( x|z x ) and c * y ( z x ) = argmaxp z?y ( y|z x ) .
The pivoted captions then enable the following two objectives : Pivoted Captioning for Back-Translation
We utilize the synthetic multilingual captions ( i.e. , c * x ( z x ) , c * y ( z x ) from the source images and c * x ( z y ) , c * y ( z y ) from the target images ) to reversely reconstruct the synthetic captions from their translations in both directions .
Formally , we compute the following caption - based back - translation loss : x?y and L CBT x?y , we do not back - prop through the captioning step .
For optimization , we sample mini-batches and minimizing the following loss : L CBT x?y = E zx - log pyz?x c * x ( zx ) | g * ( c * x ( zx ) , L = L M BT x?y + L V SE x,y , z + L CBT x?y + L CP T x?y ( 12 ) Here we drop the weights w of each loss for clarity .
In practice , all the weights are set to 1.0 except for w CP T where we employ a decreasing learning scheduler specified in the next section .
Experiments and Results
We first describe the implementation details and the experimental setup .
Then we compare our approach with baselines with detailed analysis .
Dataset and Preprocessing
We conduct experiments on the Multi30 K dataset , the benchmark dataset for mul-timodal MT .
It contains 29 K training , 1 K validation , and 1 K testing images .
Each image has three descriptions in English / German / French , which are translations of each other .
To ensure the model never learn from parallel sentences , we randomly split Multi30 K training and validation sets in half for one language and use the complementary half for the other .
The resulting M30k - half are two corpora with non-overlapping 14,500 training and 507 validation image-sentence pairs , respectively .
For text pre-processing , we use Moses ( Koehn et al. , 2007 ) scripts for tokenization and apply the Byte Pair Encoding ( BPE ) ( Sennrich et al. , 2016 b ) from XLM .
To identify and extract features of visual objects in images , we use the Faster-RCNN ( Ren et al. , 2015 ) model in ( Anderson et al. , 2018 ) to detect up to 36 salient visual objects per image and extract their corresponding 2048 - dim regional features .
Implementation
We use Transformer as the underlying architecture for the translation and captioning modules .
Each encoder / decoder of the translator is with 6 - layer stacked Transformer network , 8 heads , 1024 hidden units , and 4096 feed - forward filter size .
The captioner is a 6 - layer Transformer decoder with the same configuration .
The visual encoder is a 1layer MLP which maps visual feature to the shared 1,024 - dim embedding space then adds the positional encoding to encode spatial locations ( normalized top- left and bottom- right coordinates ) of visual objects .
Our implementation is based on the codebase of XLM and MASS .
Experimental Details
We respectively conduct unsupervised MMT experiments on Multi30 K - half for two language pairs : English - French and English - German .
Pre-Training Pre-training is a critical step for unsupervised MT .
We follow the setup in UMMT ( Su et al. , 2019 ) for a fair comparison .
For each language , we create a text-only pre-training set by combining the shuffled first 10 million sentences of the WMT News Crawl datasets from 2007 to 2017 with 10 times of M30k - half , resulting in a text-only dataset with 10.145 million unparalleled sentences in English , French , German respectively .
For text pre-training , we leverage the script and the masked seq-to-seq objective proposed in MASS , which randomly masks a span in a sentence then encourages the model to decode and reconstruct the masked sequence as the monolingual language model pre-training .
More details can be found in the original paper .
Note that there is no fine-tuning ( back- translation ) on WMT for a fair comparison with other baselines .
For multimodal pre-training of the captioning modules , we use the out-of- domain MS - COCO ( Lin et al. , 2014 ) dataset .
We randomly split the training set into two disjoint subsets .
Each set contains 56,643 images and 283,215 sentences .
We use the translate-train strategy as in XNLI .
We leverage Google Translate to translate one set of English sentences into French and German .
We pre-train the captioning modules with Eq. 9 and fix them during fine-tuning to avoid overfitting .
Note that the captioning modules are trained on non-parallel sentences with disjoint image subsets , which implies no overlap between English - German or English - French sentences .
Fine-tuning on Multi30 K -half
We fine-tune on the training set of Multi30 K - half for 18 epochs .
We train our model with the Adam optimizer ( Kingma and Ba , 2014 ) with a linear warm - up and a learning rate varying from 10 ?7 to 10 ?5 .
We apply a linearly decreasing weight from 1.0 to 0.1 at 10 - th epoch for w CP T as we empirically observe that the generated captions are relatively too noisy to serve as good pseudo pairs in the later stage of training .
The margin ? in VSE is set to 0.1 .
Other hyper-parameters in Transformer follow the default setting in MASS .
We use 4 Titan Xp GPUs with 1,000 tokens in each mini-batch for training .
Evaluation and Model selection For evaluation , we report BLEU scores by multi-bleu.pl 1 in Moses and METEOR 2 scorea on the Multi30 K testing set .
For model selection without a parallel validation corpus , we consider the unsupervised criterion proposed in based on the BLEU scores of " round-trip " translations ( source ? target ? source and target ? source ? target ) which have been empirically shown to correlate well with the testing metrics .
Baseline Models
We compare recent unsupervised text-only and multimodal MT baselines listed in the following : ( 1 ) MUSE ) is a word-to - word MT model with pre-trained Wikipedia embeddings .
( 2 ) UNMT sets the tone of using denoising autoencoder and back-translation for unsupervised MT . ( 3 ) XLM ( Conneau and Lample , 2019 ) deploys masked language model from BERT .
( 4 ) MASS ( Song et al. , 2019 ) uses a masked seq-to-seq pre-training objective , achieves the current state - of - the - art performance in text-only unsupervised MT .
( 5 ) Game - MMT ) is a reinforcement learning - based unsupervised MMT .
( 6 ) UMMT
( Su et al. , 2019 ) use visual feature for denoising autoencoder and back -translation .
UMMT is the current state of the art in unsupervised MMT .
We either use the reported scores in the original papers or use their best scripts with their pre-trained language models publicly available for fine-tuning on Multi30 K - half .
Comparing the multimodal models trained with and without visual content ( UMMT -T vs. UMMT - Full and Ours - T vs. Ours - Full ) , our model achieves + 2.5? 3.7 improvements in BLEU while + 1.4?2.5 for UMMT .
The results imply that , even with a higher text-only baseline ( e.g. 49.5 vs. 37.2 content more effectively .
Main Results : Unsupervised MMT
Comparison with the Baseline Models In Figure 3 , we provide some qualitative results on the Multi30 K testing set .
We observe a consistent improvement of unsupervised translation quality with our full model to the text-only one .
Without parallel translation pairs as the vital supervision , the proposed pseudo visual pivoting successfully disambiguates the word semantics in the similar syntactic category and results in improved cross-lingual word alignment ; for instance , " cafe " vs. " soda " machine in the third French example , and " felsigen " ( rocky ) vs. " verschneiten " ( snowy ) in the first German example .
Ablation Studies
To quantify module-wise contribution in pseudo visual pivoting , we summarize our ablation studies in Table 2 .
Comparing the performance improvement from text-only to the model with regional visual features ( T+ V ) , the features of salient visual objects contribute + 0.6?0.9 BLEU score over a much higher text-only baseline compared to UMMT .
In pseudo visual pivoting , + VSE promotes the alignments in the monolingual VSE spaces and results in an additional + 1.3 ?
2.0 gain in BLEU .
This improvement validates our hypothesis that the visual space can effectively serve as the bridge connecting the source and target language latent spaces .
Also , synthesizing image-pivoted pseudo caption pairs effectively provides weak supervision for aligning the cross-lingual latent space in unsupervised MMT .
We observe that the pivoted captions for paired translation ( CPT ) is more effective than treating them as back -translation pairs ( CBT ) .
Utilizing generated image-pivoted captions is shown to be a promising approach for weakly supervised or unsupervised MMT .
The full model which employs VSE , CBT , and CPT achieves + 1.9?3.1 improvements compared to our multimodal baseline ( row two , visual feature only ) .
Generalizability
How does our unsupervised MMT model generalize when images are not available at the testing time ?
Table 3 shows the testing results without images .
As can be observed , our model generalizes well .
The differences are mostly less than 1.0 in BLEU .
As our model , when being tested without visual content , still outperforms other unsupervised text -only or multimodal MT models listed in Table 1 , the minor drop in BLEU implies that the improved cross-lingual latent space alignment via pseudo visual pivoting is likely to be more critical than using images as an input feature for decoding .
Luckily , such alignment is already preserved in the training phase with the proposed approach .
An interesting question is :
How much does the visual content ( as a feature ) contribute ?
As in leave- one- feature - out cross-validation , we compare T : zwei m?nner spielen gitarre im freien .
T+V : zwei m?nner spielen gitarre vor einem gro?en publikum .
GT : zwei m?nner spielen gitarre vor einem gro?en publikum .
SRC : two men playing guitar in front of a large audience .
( b) English ?
German the difference of performance between inferencing with and without images .
The larger the difference ( the subscripts in Table 3 ) implies a model better utilizes visual content .
Compared with UMMT , our model has better utilization .
We observe that the key to such difference is the VSE objective .
Our model trained without the VSE objective results in worse utilization ( smaller difference at the testing time ) , possibly because the source text-image pairs are distant in the multilingual VSE space .
Real-pivoting & Low-resource Corpora
Will our model benefit from " real " pivoting ( srcimg 1 , img 1 - tgt , overall src-img 1 - tgt ) ?
We train our models with overlapped images while leaving sentences in the source and target languages unparalleled ( use no translation pairs ) .
From the first three rows in Table 4 , the performance is improved when training with the overlapped images and their corresponding sentences .
Comparing the improvement from 0 % to 100 % of the text-only model and the full model , a larger gain is observed with the proposed pseudo visual pivoting which aligns and reduces uncertainty in the language latent spaces .
Furthermore , under the low-resource setting ( 3.0 K non-parallel data , row six and seven ) , a substantial improvement over the text-only model is still observed .
These results suggest that the proposed pseudo visual pivoting is likely to generalize to the semi-supervised and the low-resource setting , which we consider as our future work .
Supervised Case
Although the proposed pseudo visual pivoting targets unsupervised MMT , we are also interested in its performance under the fully supervised setup .
To gain insights , we conduct supervised MMT experiments by changing the back -translation objective for unsupervised MT ( Eq. 6 ) to the supervised MT objective ( Eq. 2 ) with additional visual inputs .
We benchmark with recent supervised MMT models , including Imagination ( Elliott and K?d?r , 2017 ) , LIUM -CVC ( Caglayan et al. , 2017 ) , and VAG ( Zhou et al. , 2018 ) on Multi30K .
Table 5 shows the testing results .
Our model significantly outperforms other baselines and achieves state - of - the - art performance .
Comparing to the unsupervised model trained with full Multi30 K ( Table 4 ,100 % ( 29K / 29K ) ) , the direct supervision from parallel translation pairs results in a + 6.5? 7.1 gain in BLEU .
Notably , images provide a minor improvement with full supervision from translation pairs .
This result implies that , compared to serving as a complementary feature , visual information likely contributes more to improving crosslingual alignment via pseudo visual pivoting for MMT with limited supervision .
2017 ) maximize the expected likelihood .
Our model does not rely on a third language .
Our framework is along the line of research in ( Lample et al. , 2018a , b ; Conneau and Lample , 2019 ) , which aims at learning an aligned latent space between the two languages to translate by reconstruction .
Nevertheless , we focus on the multimodal setup where the visual space is dissimilar to the language spaces with challenging asymmetric interactions between modalities .
Supervised MMT Supervised MMT is introduced in as a multi-encoder singledecoder framework with additional image inputs .
Huang et al. ( 2016 ) encode word sequences with regional visual objects while Calixto and Liu ( 2017 ) leverage global visual feature .
LIUM -CVC ( Caglayan et al. , 2017 ) uses element -wise multiplication to model the image-text interaction .
Imagination ( Elliott and K?d?r , 2017 ) and VAG ( Zhou et al. , 2018 ) learns with the auxiliary image reconstruction and source-image - target triplet alignment tasks , respectively .
While these methods achieve improvements , their advantage over the text-only models is still minor under the supervised scenario .
As analyzed in ( Caglayan et al. , 2019 ) , visual content is more critical when the textual content is limited or uncertain in MMT .
We study the more challenging unsupervised MMT .
Unsupervised MMT
To our best knowledge , three recent works have generalized MMT to the unsupervised setting .
Nakayama and Nishida ( 2017 ) learn modal - agnostic fixed length image / sentence embeddings .
In contrast , our model promotes finegrained ( object- token ) varying - length embedding , which better aligns VSE space .
Game -MMT use a captioning and a translation model maximizing the likelihood of translated captions to original sentences .
We synthesize captions for symmetric back -translation and considers no ground truth image annotation in the loop .
Empirically , it is preferred to separate real and generated captions .
UMMT
( Su et al. , 2019 ) uses
Transformers , autoencoder loss , and multimodal back -translation .
We do not use autoencoder .
Our model leverages object detection for multimodal back -translation and equips pseudo visual pivoting .
Image Captioning and VSE Our method draws inspiration from captioning and cross-modal retrieval .
Recent progress in captioning aims at using reinforcement learning to improve diversity ( Dai et al. , 2017 ) or maximize metric ( Rennie et al. , 2017 ) .
We use a vanilla MLE objective .
For learning VSE , we leverage the contrastive loss ( Kiros et al. , 2014 ) from cross-modal retrieval , which is shown more robust than maximizing canonical correlation among modalities as in ( Andrew et al. , 2013 ; Huang et al. , 2018 ) .
For encoding image and text , we generalize the cross-modality attention from SCAN to the multilingual scenario for learning a multilingual VSE space ( Gella et al. , 2017 ; Huang et al. , 2019a ) .
Conclusion
We have presented a novel approach : pseudo visual pivoting for unsupervised multimodal MT .
Beyond features , we use visual content to improve the crosslingual alignments in the shared latent space .
Precisely , our model utilizes the visual space as the approximate pivot for aligning the multilingual multimodal embedding space .
Besides , it synthesizes image-pivoted pseudo sentences in two languages and pairs them to translate by reconstruction without parallel corpora .
The experiments on Multi30 K show that the proposed model generalizes well and yields new state - of - the - art performance .
stunts on a bike a little boy is going to throw a ball on the beach a little toddler is throwing a volleyball ein kleines kleinkind wirft einen volleyball Alignment in the Multilingual VSE space Pivoted Captioning for Paired -Translation Pivoted Captioning for Back - Translation
