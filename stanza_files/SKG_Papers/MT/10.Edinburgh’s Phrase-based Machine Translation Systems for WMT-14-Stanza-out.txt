title
Edinburgh 's Phrase- based Machine Translation Systems for WMT -14
abstract
This paper describes the University of Edinburgh 's ( UEDIN ) phrase - based submissions to the translation and medical translation shared tasks of the 2014 Workshop on Statistical Machine Translation ( WMT ) .
We participated in all language pairs .
We have improved upon our 2013 system by i ) using generalized representations , specifically automatic word clusters for translations out of English , ii ) using unsupervised character - based models to translate unknown words in Russian - English and Hindi-English pairs , iii ) synthesizing Hindi data from closely - related Urdu data , and iv ) building huge language on the common crawl corpus .
Translation Task
Our baseline systems are based on the setup described in ( Durrani et al. , 2013 b ) that we used for the Eighth Workshop on Statistical Machine Translation ( Bojar et al. , 2013 ) .
The notable features of these systems are described in the following section .
The experiments that we carried out for this year 's translation task are described in the following sections .
Baseline
We trained our systems with the following settings : a maximum sentence length of 80 , growdiag -final - and symmetrization of GIZA ++ alignments , an interpolated Kneser - Ney smoothed 5 gram language model with KenLM ( Heafield , 2011 ) used at runtime , hierarchical lexicalized reordering ( Galley and Manning , 2008 ) , a lexicallydriven 5 - gram operation sequence model ( OSM ) ( Durrani et al. , 2013a ) with 4 count- based supportive features , sparse domain indicator , phrase length , and count bin features ( Blunsom and Osborne , 2008 ; Chiang et al. , 2009 ) , a distortion limit of 6 , maximum phrase-length of 5 , 100 - best translation options , Minimum Bayes Risk decoding ( Kumar and Byrne , 2004 ) , Cube Pruning ( Huang and Chiang , 2007 ) , with a stack -size of 1000 during tuning and 5000 during test and the noreordering - over-punctuation heuristic ( Koehn and Haddow , 2009 ) .
We used POS and morphological tags as additional factors in phrase translation models for German - English language pairs .
We also trained target sequence models on the in-domain subset of the parallel corpus using Kneser - Ney smoothed 7 - gram models .
We used syntactic -preordering ( Collins et al. , 2005 ) and compound splitting ( Koehn and Knight , 2003 ) for German- to - English systems .
We used trivia tokenizer for tokenizing Hindi .
The systems were tuned on a very large tuning set consisting of the test sets from 2008 - 2012 , with a total of 13,071 sentences .
We used newstest 2013 for the dev experiments .
For Russian - English pairs news- test 2012 was used for tuning and for Hindi-English pairs , we divided the newsdev 2014 into two halves , used the first half for tuning and second for dev experiments .
Using Generalized Word Representations
We explored the use of automatic word clusters in phrase - based models ( Durrani et al. , 2014a ) .
We computed the clusters with GIZA ++'s mkcls ( Och , 1999 ) on the source and target side of the parallel training corpus .
Clusters are word classes that are optimized to reduce n-gram perplexity .
By generating a cluster identifier for each output word , we are able to add an n-gram model over these identifiers as an additional scoring function .
The inclusion of such an additional factor is trivial given the factored model implementation of Moses .
The n-gram model is trained in the similar way as the regular language model .
We trained domain-specific language models separately and then linearly interpolated them using SRILM with weights optimized on the tuning set ( Schwenk and Koehn , 2008 ) .
We also trained OSM models over cluster-ids ( ? ) .
The lexically driven OSM model falls back to very small context sizes of two to three operations due to data sparsity .
Learning operation sequences over cluster- ids enables us to learn richer translation and reordering patterns that can generalize better in sparse data conditions .
Table 1 shows gains from adding target LM and OSM models over cluster-ids .
Using word clusters was found more useful translating from English -to -*.
We also trained OSM models over POS and morph tags .
For the English- to - German system we added an OSM model over [ pos , morph ] ( source : pos , target : morph ) and for the Germanto- English system we added an OSM model over [ morph , pos ] ( source : morph , target:pos ) , a configuration that was found to work best in our previous experiments ( Birch et al. , 2013 ) .
Table 2
Last year , our Russian - English systems performed badly on the human evaluation .
In comparison other participants that used transliteration did well .
We could not train a transliteration system due to unavailability of a transliteration training data .
This year we used an EM - based method to induce unsupervised transliteration models ( Durrani et al. , 2014 b ) .
We extracted transliteration pairs automatically from the word-aligned parallel data and used it to learn a transliteration system .
We then built transliteration phrase-tables for translating OOV words and used the post-decoding method ( Method 2 as described in the paper ) to translate these .
= Transliterating OOVs
Table 3 shows the number ( types ) of transliteration pairs extracted using unsupervised mining , number of OOV words ( tokens ) in each pair and the gains achieved by transliterating unknown words .
Synthesizing Hindi Data from Urdu Hindi and Urdu are closely related language pairs that share grammatical structure and have a large overlap in vocabulary .
This provides a strong motivation to transform any Urdu-English parallel data into Hindi-English by translating the Urdu part into Hindi .
We made use of the Urdu-English segment of the Indic multi-parallel corpus ( Post et al. , 2012 ) which contains roughly 87 K sentence pairs .
The Hindi-English segment of this corpus is a subset of parallel data made available for the translation task but is completely disjoint from the Urdu-English segment .
We initially trained a Urdu-to-Hindi SMT system using a very tiny EMILLE 1 corpus ( Baker et al. , 2002 ) .
But we found this system to be useless for translating the Urdu part of Indic data due to domain mismatch and huge number of OOV words ( approximately 310 K tokens ) .
To reduce sparsity we synthesized additional phrase-tables using interpolation and transliteration .
Interpolation :
We trained two phrase translation tables p( ? i | ?i ) and p( ?i | hi ) , from Urdu-English ( Indic corpus ) and Hindi-English ( Hin-dEnCorp ( Bojar et al. , 2014 ) ) bilingual corpora .
Given the phrase-table for Urdu-English p( ?i | ?i ) and the phrase-table for English - Hindi p( ?i | hi ) , we estimated a Urdu-Hindi phrase- table p( ?i | hi ) using the well -known convolution model ( Utiyama and Isahara , 2007 ; Wu and Wang , 2007 ) : p( ?i | hi ) = ? i p( ? i | ?i ) p( ? i | hi )
The number of entries in the baseline Urdu-to - Hindi phrase - table were approximately 254K .
Using interpolation we were able to build a phrasetable containing roughly 10 M phrases .
This reduced the number of OOV tokens from 310 K to approximately 50K .
Transliteration : Urdu and Hindi are written in different scripts ( Arabic and Devanagri respectively ) .
We added a transliteration component to our Urdu-to-Hindi system .
An unsupervised transliteration model is learned from the wordalignments of Urdu-Hindi parallel data .
We were able to extract around 2800 transliteration pairs .
To learn a richer transliteration model , we additionally fed the interpolated phrase -table , as described above , to the transliteration miner .
We were able to mine additional 21000 transliteration pairs and built a Urdu-Hindi character - based model from it .
The transliteration module can be used to translate the 50K OOV words but previous research ( Durrani et al. , 2010 ; Nakov and Tiedemann , 2012 ) has shown that transliteration is useful for more than just translating OOV words when translating closely related language pairs .
To fully capitalize on the large overlap in Hindi- Urdu vocabulary , we transliterated each word in the Urdu test - data into Hindi and produced a phrase-table with 100 - best transliterations .
The two synthesized ( triangulated and transliterated ) phrase - tables are then used along with the baseline Urdu-to - Hindi phrase - table in a log-linear model .
Detailed results on Urdu-to-Hindi baseline and improvements obtained from using transliteration and triangulated phrase-tables are presented in Durrani and Koehn ( 2014 ) .
Using our best Urdu-to-Hindi system , we translated the Urdu part of the multi-indic corpus to form Hindi-English parallel data .
Table 4 shows results from using the synthesized Hindi-English corpus in isolation ( Syn ) and on top of the baseline system ( B 0 + Syn ) .
The additional language model is the only difference between the constrained and unconstrained submissions ; we did not use additional parallel data .
These language models were trained on text provided by the CommonCrawl foundation , which they converted to UTF - 8 after stripping HTML .
Languages were detected using the Compact Language Detection 2 3 and , except for Hindi where we lack tools , sentences were split with the Europarl sentence splitter ( Koehn , 2005 ) .
All text was then deduplicated , minimizing the impact of boilerplate , such as social media sharing buttons .
We then tokenized and truecased the text as usual .
Statistics are shown in We built unpruned modified Kneser - Ney language models using lmplz ( Heafield et al. , 2013 ) .
Table 6 : Gains obtained by using huge language models - B 0 = Baseline , +L = Adding Huge LM
While the Hindi and Czech models are small enough to run directly , models for other languages are quite large .
We therefore created a filter that operates directly on files in KenLM trie binary format , preserving only n-grams whose words all appear in the target side vocabulary of at least one source sentence .
For example , an English language model trained on just the 2012 and 2013 crawls takes 3.5 TB without any quantization .
After filtering to the Hindi-English tuning set , the model fit in 908 GB , again without quantization .
We were then able to tune the system on a machine with 1 TB RAM .
Results are shown in Table 6 ; we did not submit to English - French because the system takes too long to tune .
Miscellaneous Hindi-English : 1 ) A large number of Hindi sentences in the Hindi-English parallel corpus were ending with a full-stop " . " , although the end-ofthe-sentence marker in Hindi is " Danda " ( | ) .
Replacing full-stops with Danda gave improvement of + 0.20 for hi-en and + 0.40 in en-hi .
2 ) Using Wiki subtitles did not give any improvement in BLEU and were in fact harmful for the en-hi direction .
Russian- English :
We tried to improve wordalignments by integrating a transliteration submodel into GIZA ++ word aligner .
The probability of a word pair is calculated as an interpolation of the transliteration probability and translation probability stored in the t-table of the different alignment models used by the GIZA ++ aligner .
This interpolation is done for all iterations of all alignment models ( See Sajjad et al . ( 2013 )
Table 8 : Hierarchical lexicalized reordering model ( Galley and Manning , 2008 ) .
Fast align :
In preliminary experiments , we compared the fast word alignment method by Dyer et al . ( 2013 ) against our traditional use of GIZA ++.
Results are quite mixed ( Table 7 ) , ranging from a gain of +.35 for Russian - English to a loss of -.19 for Czech - English .
We stayed with GIZA ++ for all of our other experiments .
Hierarchical lexicalized reordering model :
We explored the use of the hierarchical lexicalized reordering model ( Galley and Manning , 2008 ) in two variants : using the same orientations as our traditional model ( monotone , discontinuous , swap ) , and one that distinguishes the discontinuous orientations to the left and right .
Table 8 shows slight improvements with these models , so we used them in our baseline .
Threshold filtering of phrase table :
We experimented with discarding some phrase table entry due to their low probability .
We found that phrase translations with the phrase translation probability ?( f |e ) < 10 ?4 can be safely discarded with almost no change in translations .
However , discarding phrase translations with the inverse phrase translation probability ?( e|f ) < 10 ?4 is more risky , especially with morphologically rich target languages , so we kept those .
Summary
Table 9 shows cumulative gains obtained from using word classes , transliteration and big language models 4 over the baseline system .
Our German-English constrained systems were used for EU - Bridge system combination , a collaborative effort to improve the state - of - the - art in machine translation ( See Freitag et al . ( 2014 )
Medical Translation Task
For the medical translation task , the organisers supplied several medical domain corpora ( detailed on the task website ) , as well some out - of- domain patent data , and also all the data available for the constrained track of the news translation task was permitted .
In general , we attempted to use all of this data , except for the LDC Gigaword language model data ( for reasons of time ) and we divided the data into " in- domain " and " out- of- domain " corpora .
The data sets are summarised in Tables 10 and 11 .
In order to create systems for the medical translation tasks , we used phrase - based Moses with exactly the same settings as for the news translation task , including the OSM ( Durrani et al. , 2011 ) , and compound splitting Koehn and Knight ( 2003 ) for German source .
We did not use word clusters ( Section 1.2 ) , as they did not give good results on this task , but we have yet to find a reason for this .
For language model training , we decided not to build separate models on each corpus as there was Table 11 : Additional monolingual data used in the medical translation task .
Those above the line were classified as " in- domain " and the one below as " out- of- domain " .
We also used the target sides of all the parallel corpora for language modelling .
a large variation in corpus sizes .
Instead we concatenated the in-domain target sides with the indomain extra monolingual data to create training data for an in- domain language model , and similarly for the out-of- domain data .
The two language models were interpolated using SRILM , minimising perplexity on the Khresmoi summary development data .
During system development , we only had 500 sentences of development data ( SUMMARY - DEV ) from the Khresmoi project , so we decided to select further development and devtest data from the EMEA corpus , reasoning that it was fairly close in domain to SUMMARY - DEV .
We selected a tuning set ( 5000 sentence pairs , which were added to SUMMARY - DEV ) and a devtest set ( 3000 sentence pairs ) from EMEA after first de-duplicating it , and ignoring sentence pairs which were too short , or contained too many capital letters or numbers .
The EMEA contains many duplicated sentences , and we removed all sentence pairs where either side was a duplicate , reducing the size of the corpus to about 25 % of the original .
We also removed EMEA from Czeng , since otherwise it would overlap with our selected development sets .
We also experimented with modified Moore-Lewis ( Moore and Lewis , 2010 ; Axelrod et al. , 2011 ) data selection , using the EMEA corpus as the in-domain corpus ( for the language model required in MML ) and selecting from all the out-ofdomain data .
When running on the final test set ( SUMMARY - TEST ) we found that it was better to tune just on SUMMARY - DEV , even though it was much smaller than the EMEA dev set we had selected .
All but two ( cs-en , de-en ) of our submitted systems used the MML selection , because it worked better on our EMEA devtest set .
However , as can be seen from Table 12 , systems built with all the data generally perform better .
We concluded that EMEA was not a good representative of the Khresmoi data , perhaps because of domain differences , or perhaps just because of the alignment noise that appears ( from informal inspection ) to be present in EMEA .
Table 12 : Results ( cased BLEU ) on the khresmoi summary test set .
The " in " systems include all in - domain data , the " in + 20 " systems also include 20 % of the out-of - domain data and the " out " systems include all data .
The submitted systems are shown in italics , except for de-en and cs-en where we submitted a " in+ out " systems .
For de-en , this was tuned on SUMMARY - DEV plus the EMEA dev set and scored 37.31 , whilst for cs-en we included LDC Giga in the LM , and scored 36.65 .
For translating the Khresmoi queries , we used the same systems as for the summaries , except that generally we did not retune on the SUMMARY - DEV data .
We added a post-processing script to strip out extraneous stop words , which improved BLEU , but we would not expect it to matter in a real CLIR system as it would do its own stop-word removal .
Table 3 : 3 Using Unsupervised Transliteration Model - Training = Extracted Transliteration Corpus ( types ) , OOV = Out-of-vocabulary words ( tokens ) B 0 = System without Transliteration , + T r
Table 5 5 . A full description
Table 5 : 5 Size of huge language model training data
Table 9 : 9 for details ) .
Cumulative gains obtained for each language - B 0 = Baseline , B 1 = Best System from English into English Lang B0 B1 ? B0 B1 ? de 20.44 20.85 +0.41 27.24 27.44 +0.20 cs 18.84 20.03 +1.19 26.42 26.42 ?0.00 fr 30.73 30.82 +0.09 31.64 31.76 +0.12 ru 18.78 20.81 +2.03 24.45 25.21 +0.76 hi 9.27 12.83 + 3.56 14.08 15.48 +1.40
EMILLE corpus contains roughly 12000 sentences of Hindi and Urdu comparable data .
From these we were able to sentence align 7000 sentences to build an Urdu-to -Hindi system .
http://commoncrawl.org 3 https://code.google.com/p/cld2/
Cumulative gains do not include gains obtain from big language models for hi-en and en-de .
