title
Tohoku-AIP-NTT at WMT 2020 News Translation
Task
abstract
In this paper , we describe the submission of Tohoku- AIP - NTT to the WMT '20 news translation task .
We participated in this task in two language pairs and four language directions : English ?
German and English ?
Japanese .
Our system consists of techniques such as back -translation and finetuning , which are already widely adopted in translation tasks .
We attempted to develop new methods for both synthetic data filtering and reranking .
However , the methods turned out to be ineffective , and they provided us with no significant improvement over the baseline .
We analyze these negative results to provide insights for future studies .
Introduction
The joint team of Tohoku University , RIKEN AIP , and NTT ( Tohoku - AIP - NTT ) participated in the WMT '20 shared news translation task in two language pairs and four language directions : English ?
German ( En? De ) , German ?
English ( De?En ) , English ?
Japanese ( En?Ja ) , and Japanese ?
English ( Ja?En ) .
At the very beginning of this year 's shared task , we planned to employ the following two enhancements at the core of our system .
The first enhancement is the noisy synthetic data filtering ( Koehn et al. , 2018 ) to better utilize the millions of backtranslated synthetic data .
However , as we analyze in Section 5.1 , this filtering turned out to be ineffective .
The second enhancement is the reranking of n-best candidates generated a the model .
Given a collection of scores from multiple generative / translation models , our reranking module selects the best candidate .
We attempted to develop sophisticated machine learning based methods for optimizing the weight of each score .
However , we found that those methods are not as effective as the simple grid search on the BLEU score ( details in Section 3.7 and Section 5.3 ) .
Eventually , we designed our system as a combination of techniques that are already widely adopted in the shared task , such as back -translation and fine-tuning .
The overview of our system is shown in Figure 1 .
We achieved the first place in De?En on automatic evaluation and obtained strong results in other language directions .
2 Dataset and Preprocessing
Bitext For both En?De and En?Ja , we used all bitexts that are available for a constrained system .
En?De Following , we applied language identification filtering ( langid ) 1 to the bitext .
In this filtering , sentence pairs were removed if a supposedly English / German sentence is identified as a non-English / German sentence .
Then , we applied the clean-corpus -n script available in the Moses toolkit ( Koehn et al. , 2007 ) and removed sentence pairs that are either too long and / or their length ratio is too large 2 .
These two filtering processes provided us with approximately 44 M sentence pairs .
Then , we trained and applied the Moses truecaser independently for each language .
We also trained byte-pair encoding ( BPE ) ( Sennrich et al. , 2016 c ) models using the sentencepiece ( Kudo and Richardson , 2018 ) implementation .
For BPE training , we used only a subset of the parallel corpus ( Europarl , NewsCommentary , and RAPID ) to prevent extremely rare characters from contaminating the vocabulary and the subword segmentation .
En? Ja Similar to En? De , we applied langid to clean bitext , but we did not use clean-corpus -n since the Japanese text is not segmented .
Instead , we simply removed sentence pairs in which the English sentence is longer than 500 tokens .
Eventually , we obtained about 17 M sentence pairs .
We used truecaser for the English side only , because case information does not exist in the Japanese language .
We independently trained the BPE merge operation on the bitext .
We set the character coverage option 3 of sentencepiece to 1.0 and 0.9998 for English and Japanese , respectively .
Monolingual Corpus
The origins of the monolingual corpus in our system are the Europarl , NewsCommentary , and entire NewsCrawl ( 2008 - 2019 ) corpora for English and German , and the Europarl , NewsCommentary and CommonCrawl corpora for Japanese .
Similarly to bitext preprocessing in Section 2.1 , we applied langid filtering to all monolingual cor-pora .
These corpora are used for large-scale backtranslation ( Section 3.3 ) .
System Overview
Base Model and Hyperparameter
The well -known Transformer model ( Vaswani et al. , 2017 ) is our base Encoder Decoder model .
Specifically , we started with the " Transformer ( big ) " setting described by Vaswani et al . ( 2017 ) and increased the feed -forward network ( FFN ) size from 4,096 to 8,192 .
reported that this larger FFN setting slightly improves the performance ; we also confirmed it in our preliminary experiment .
Table 1 shows a list of hyperparameters for model optimization .
We employed an extremely large mini-batch size of 512,000 tokens using the delaying gradient update technique ( Bogoychev et al. , 2018 ; Ott et al. , 2018 ) .
This is because previous studies showed that a large mini-batch size leads to a faster convergence ( Ott et al. , 2018 ) and a better generalization ( Popel and Bojar , 2018 ; Bawden et al. , 2019 ; Morishita et al. , 2019 ) .
We also used a large learning rate of 0.001 to further accelerate the convergence ( Goyal et al. , 2017 ; Ott et al. , 2018 ; Liu et al. , 2019 ) .
We use the fairseq toolkit for the entire set of experiments .
Every reported BLEU score is measured using SacreBLEU ( Post , 2018 ) .
Subword Size For En? De , we used the subword size of 32,000 , which is commonly used in previous studies ( Vaswani et al. , 2017 ; . For En?Ja , we conducted a hyperparameter search for a suitable subword size ; Morishita et al. ( 2019 ) empirically showed that a small subword size ( e.g. , 4,000 ) is superior to those commonly adopted in the literature ( e.g. , 16,000 and 32,000 ) .
Given their findings , we searched for the subword size in the following range : { 4000 , 8000 , 16000 , 32000 } .
Table 2 shows that the largest subword size achieves the best performance , which is inconsistent with the result of Morishita et al . ( 2019 ) .
One explanation for this result is that Morishita et al . ( 2019 ) conducted an experiment on the ASPEC corpus , whose size ( approx . 3 M ) is much smaller than that of the bitext available for the En? Ja task .
That is , the bitext available for the En? Ja task is sufficiently large for the model to learn a meaningful representation for each subword unit that is close to the word level .
Thus , we also used the subword size of 32,000 for En?Ja .
Large-scale Back-translation
We used the back-translation technique ( Sennrich et al. , 2016 b ) to generate large-scale synthetic data .
First , we trained models on the bitext for all language pairs .
Second , for each language , we fed the monolingual corpus ( Section 2.2 ) to the model .
Here , we used the beam search of width 6 and length penalty of 1.0 .
Finally , we applied length and ratio filtering to the model outputs 4 .
The size of the synthetic data that we generated for each language direction is shown in Table 3 .
The size of the synthetic data for En?Ja , which is generated from CommonCrawl , is extremely large .
Thus , we randomly subsampled the synthetic data of En? Ja so that its size roughly matches those of De?En and Ja?En .
We searched for an effective setting for incorporating the synthetic data .
As the most straightforward starting point , we simply combined bitext and synthetic data and trained the model .
Here , we upsampled the bitext so that the model sees the bitext and synthetic data at a 1:1 ratio .
Table 4 shows the result .
Here , naively using the synthetic data ( BASE + BT ) decreased the performance of the model trained with the bitext only ( BASE ) .
Given this result , we considered the following two enhancements : Tagged Back-translation
We used the tagged back - translation technique ( Tagged - BT ) ( Caswell et al. , 2019 ) , which prepends a special tag token ( e.g. , BT ) to the source sentence of synthetic data .
This simple technique can inform the model about the origin of the given training data , i.e. , whether the sentence pair is back -translated .
Marie et al. ( 2020 ) empirically demonstrated that the model trained with such tagged data can avoid overfitting to the synthetic data .
In Table 4 , the Tagged - BT ( BASE + TAGGED - BT ) successfully improves the performance from BASE except for the newstest2019 .
We suspect that the performance does not improve on newstest2019 because it does not contain the " translationese " text , i.e. , human- generated translations , which are reported to be the main source of improvement of back - translation ( Bogoychev and Sennrich , 2019 ; Marie et al. , 2020 ) . Deeper Model
We also considered increasing the model size to take advantage of a massive amount of training data .
Specifically , we increased the number of layers l from 6 to 9 and 12 .
Table 4 shows that the performances of BASE ( l = 9 ) + TAGGED -BT and BASE ( l = 12 ) + TAGGED - BT are almost comparable .
We determined that BASE ( l = 9 ) + TAGGED -BT is the best option by considering the model performance and training efficiency regarding the GPU memory constraints .
Fine-tuning Fine-tuning the model with an in- domain news corpus is acknowledged as an extremely important technique for boosting the performance ( Sennrich et al. , 2016 b ; Junczys-Dowmunt , 2019 ; Bawden et al. , 2019 ) .
We fine- tuned our models as follows : En?De For En? De , we fine- tuned the model with a collection of newstest2008 -2018 and evaluated its performance on newstest2019 .
For En? De , we only used sentence pairs whose source sentence is originally written in English , i.e. , we never used texts with translationese on the source side for finetuning .
Similarly , for De?En , we used sentence pairs whose source sentence is originally written in German .
This way , we ensured that our model does not overfit to the translationese texts ; since new-stest2019 does not contain translationese texts ( Barrault et al. , 2019 ) , we expected that newstest2020 does not contain translationese either .
We fine- tuned the model for 200 iterations with a mini-batch size of 20,000 tokens .
During the fine-tuning , we fixed the learning rate to 1e - 06 for De?En and 1e - 05 for En?De .
We saved the model every 20 iterations and took an average of the last eight saved models for decoding .
En?Ja For fine-tuning , we used the Kyoto Free Translation Task ( KFTT ) corpus and NewsCommentary as the clean bitext and NewsCommentary as the news bitext .
We fine- tuned the models by a two -step procedure , that is , we first fine-tuned with the clean bitext for 2,000 steps .
Then we fine-tuned with the news bitext for 200 steps .
We found that the validation performance of this two -step procedure is slightly better than that of the fine-tuning with the news bitext only .
Ensemble
We used the model ensemble method to improve the performance .
First , we trained four models with different random seeds .
These models were then simultaneously used for computing the score of each candidate during the beam search decoding .
Right- to- Left Models
We used Right- to- Left ( R2L ) models for reranking the n-best candidates from Left-to- Right ( L2R ) models .
R2L models generate sentences in reverse order .
Suppose that conventional L2R models generate sentences from the beginning - of- the-sentence ( BOS ) to the end-of- the-sentence ( EOS ) ; R2L models generate from EOS to BOS .
This reranking technique was independently proposed by Liu et al . ( 2016 ) and Sennrich et al . ( 2016a ) to mitigate the search error of L2R models , which may occur around EOS .
We trained four R2L models and used their scores for reranking the n-best candidates generated by L2R models ( Section 3.5 ) .
Specifically , we computed the score of each candidate with both L2R models and R2L models .
Then , we took the sum of the two scores and obtained the final score .
We sorted this final score and then selected the candidate with the highest score .
Reranking
We also applied a reranking method based on the scores of several translation ( or generative ) models , which is closely related to one iteration of Minimum Error Rate Training ( MERT ) ( Och , 2003 ) often used in Statistical Machine Translation ( SMT ) .
The underlying idea is to find the balance of likelihood independently computed from the models .
Suppose we have a set of candidate output sentences for each input in either the validation ( training phase ) or the test ( evaluation phase ) sets .
In our case , we independently generated n-best candidates using the L2R and R2L models , and obtained 2n candidates in total for each .
Here , let C i represent the set of the obtained 2n candidates of the i-th input .
Next , P j ( e ) ? [ 0 , 1 ] denotes the score of the candidate e ?
C i obtained from the j-th model , where j ? { 1 , . . . , J}. Let w j ? [ 0 , 1 ] be a weighting factor of the j-th model , and w = ( w 1 , . . . , w J ) be the vector representation of the weighting factor .
We then obtained the most likely candidate ? i , w from C i given the i-th input and w as follows : ? i , w = argmax e?C i J j=1 w j log ( P j ( e ) ) . ( 1 ) Finally , for the parameter estimation of w , we explored w by using the following optimization problem : w = argmax w?Gw SacreBLEU ( E w ) , ( 2 ) where ?w = ( ? i , w ) I i=1 and G w represent a set of values that w j can take , namely , [ 0 , 1 ] J .
For the reranking experiment , we prepared the following generative and translation models to compute P j ( e ) .
Source-to- Target L2R and R2L Model
The Source-to - Target L2R and R2L models are the same as that used for the candidate generation ; the ensemble of four L2R models and four R2L models compute the score of each candidate .
Target- to - Source L2 R and R2L Model
The Target- to - Source ( T2S ) model translates a sequence in a reverse direction , that is , it translates a given target sequence to a source sequence .
For example , if a candidate sentence is generated by the En? De model , we use the De?En model for computing the T2S score .
Uni-directional Language Model
We used the uni-directional language model ( UniLM ) to compute the likelihood of the decoded target sequence .
To do this , we trained the Transformer - based language model for all languages on monolingual data .
We obtained two distinct scores from two normalization methods : ( 1 ) simply dividing by the target sequence length and ( 2 ) SLOR ( Pauls and Klein , 2012 ; Lau et al. , 2020 ) .
A list of hyperparameters is shown in Table 1 . Masked Language Model
We also used the pretrained masked language model ( MLM ) ( Devlin et al. , 2019 ) for computing the score .
Specifically , we trained the RoBERTa - base ( Liu et al. , 2019 ) setting available in fairseq on monolingual data .
First , we computed the unnormalized log-probabilities by the method described by Wang and Cho ( 2019 ) .
Then , we normalized the probability by ( 1 ) dividing by the sequence length and ( 2 ) PenLP ( Vaswani et al. , 2017 ; Lau et al. , 2020 ) .
A list of hyperparameters is shown in Table 1 .
Because the uni-directional language model and MLM both have two distinct variations , we used a total of six models , namely , J = 6 .
Post-processing
We converted the decoded target sequence from a sequence of subwords to tokens .
Then we applied the Moses detruecaser to English and German sequences .
We also applied language -specific postprocessing as follows : En?De
We observed that the rare tokens such as Greek letters in the source sequence are sometimes translated into UNK .
We handled UNK in the decoded sequence by copying the corresponding token from the source sequence .
We determined the corresponding token by finding the token that does not exist in one of the source -side or targetside vocabularies .
En?Ja
We did not take any special measures for UNK 5 . We replaced the English style comma " ? " and period " ? " with the Japanese style " ? " and " ? " respectively .
Ja?En
We observed that the model translates the Japanese vertical bar " ? " to UNK .
Thus , we replaced all UNK with " | " .
3.9 Post-ensemble Kobayashi ( 2018 ) proposed the method of taking the ensemble of multiple models after decoding the sequence , namely , post-ensemble ( POSTENSEMBLE ) .
The underlying idea of POSTENSEMBLE is to choose " majority - like " candidates by comparing the similarities among candidates .
He applied POSTENSEMBLE to the abstractive summarization task and reported that the performance is superior to that of the conventional ensemble .
We used POSTENSEMBLE in En?
Ja 6 . Specifically , we adopted the PostCosE variant in which the cosine similarity is used as a similarity metric .
We created 300 dim fasttext word vectors ( Bojanowski et al. , 2017 ) on the Japanese monolingual corpus .
Results Performance on the Validation Set We show the validation performance of our system in Table 5 .
We used newstest2019 and the official validation set for En?De and En?Ja , respectively , for the validation data .
The table shows the effectiveness of incorporating each technique described in Section 3 .
Each technique consistently improves the performance in most cases .
In addition , it is noteworthy that both En?De and De?En models significantly outperform the performance of the best system from last year 's shared task ( WMT '19 ) .
Performance on the Test Set We show the test set performance that we measured in the OCELoT system 7 in Table 6 .
The system provides us with the SacreBLEU score and the chrF score ( Popovi ? , 2015 ) .
We used the following models for POSTENSEM - BLE of Ja?En : ( 1 ) model ( f ) ( Table 5 ) , ( 2 ) Model ( f ) with the ensemble of eight models , in which four models are fine-tuned with the clean bitext and the other four models are fine-tuned with the news bitext , and ( 3 ) Model ( 2 ) without n-best candidates from the R2L model .
The performance of En? Ja appears significantly better than the validation performance reported in Table 5 ; this is because OCELoT computes the BLEU score with character - level segmentation , whereas we used the MeCab-based word - level segmentation 8 .
We also computed the BLEU score with the MeCab-based segmentation for reference and obtained 25.8 points .
Analysis
In this section , we introduce several negative results from our preliminary experiments .
Our attempts include the following : ( 1 ) filtering synthetic data , ( 2 ) incorporating forward - translation , and ( 3 ) developing a more sophisticated reranking method .
We also analyze the issue regarding the use of brackets in the En? Ja task .
Negative Results on Synthetic Data Filtering
We applied corpus filtering to the synthetic data created in Section 3.3 .
The goal of this filtering is to extract and utilize the " clean " subset of synthetic data that may contribute to the model performance .
For each of the sentence pairs in the synthetic data , we assigned scores that represent the likelihood of being a sentence pair ( Section 5.1.1 ) .
Then , we regarded these scores as features for classification ; we trained a model classifying clean and noisy sentence pairs ( Section 5.1.2 ) .
Finally , on the basis of the confidence scores of the classifier , we extracted the presumably clean subset of the synthetic data .
Features Pointwise HSIC
We computed the score for each sentence pair using the pointwise Hilbert - Schmidt independence criterion ( PHSIC ) ( Yokoi et al. , 2018 ) , which is a kernel - based co-occurrence measure .
Given a set of sentence pairs , PHSIC can assign a high score to a sentence pair that is consistent with the rest of the sentence pairs .
To do this , PHSIC utilizes kernel functions and calculates the sentence similarity .
Yokoi et al. ( 2018 ) applied PHSIC to machine translation corpus filtering and reported promising results .
Thus , we also employed PHSIC for synthetic data filtering .
First , we learned the parameters of the PHSIC matrix with a cosine kernel by using all sentence pairs in the bitext , which are represented as sentence embeddings .
Then , we used this trained matrix to compute the scores for the synthetic data .
We used the following two methods for computing the sentence embeddings : ( 1 ) the weighted sum of fasttext vectors ( Bojanowski et al. , 2017 ) by smoothed inverse frequency ( SIF ) weighting ( Arora et al. , 2017 ) and ( 2 ) the average of final hidden states of the pre-trained MLM .
Here , the fasttext vector is the same as the one used for post-ensemble ( Section 3.9 ) , and MLM is the one from the reranking ( Section 3.7 ) .
The word frequency for SIF weighting is calculated from the monolingual corpus .
Cross-entropy from T2S Model
We computed the word-normalized conditional cross-entropy using the T2S translation model .
For example , the synthetic data generated using the En? De model are scored using the De?En model .
Training a Classifier
We trained a linear support vector machine model that classifies clean and noisy sentence pairs .
To train the classifier , we used newstest2009 - 2019 and the official validation set as clean sentence pairs for En?De and En?Ja , respectively .
We generated the noisy sentence pairs by randomly adding the noise presented by Wang et al . ( 2018 )
After training , we classified each sentence pair in the synthetic data .
The confidence score of the classifier was used as an overall score that represents the " cleanness " ( i.e. , quality ) of the sentence pair .
Results
We investigated the effectiveness of the synthetic data filtering .
First , we sorted the synthetic data according to the score computed with the classifier ( Section 5.1.2 ) .
Then , we used the top r% of synthetic data for training .
Table 7 shows the results of synthetic data filtering with varying r.
We trained the En?De model using the BASE + TAGGED - BT setting .
The results showed that our filtering does not seem to improve the performance over the baseline ( r = 100 ) .
One of the possible reasons for this ineffectiveness is the quality of the sentence embeddings used for PHSIC .
That is , the use of fasttext and pretrained MLM might be inappropriate .
Utilizing more powerful sentence encoders such as Sentence-BERT ( Reimers and Gurevych , 2019 ) and Universal Sentence Encoder ( Cer et al. , 2018 ) is an interesting option to explore in the future ; however , the methods of acquiring such resources in the constrained setting is not trivial .
Effectiveness of Incorporating Forward-Translation Forward-translation ( Burlot and Yvon , 2018 ) is a technique similar to back - translation ; the difference is that while back - translation uses the targetside monolingual data , forward - translation uses the source -side monolingual data to generate synthetic data .
Bogoychev and Sennrich ( 2019 ) reported that forward - translation is effective for improving the translation of texts that are originally written in the source language ( i.e. , non-translationese texts ) .
To determine if we can take the best of the two techniques , namely , forward - translation and backtranslation , we combined the synthetic data and trained the model .
As described in Section 3.3 , we prepended a distinct tag to each data source : FT and BT for data generated by forward -translation and back -translation respectively .
Then , we upsampled the bitext , so that the model is fed with the bitext and synthetic data at a 1:0.5:0.5 ratio .
Table 8 shows the result .
The model in - corporating both back -translation and forwardtranslation ( BASE + TAGGED -BT + TAGGED - FT ) achieves the best result , however , the improvement was marginal .
In addition , the performance of the model with forward -translation only ( BASE + TAGGED - FT ) was worse than that of the baseline ( BASE ) in all datasets .
Given this result , we only used back -translation and kept the training procedure as simple as possible in our final system .
Negative Result on Reranking
We actually investigated several different types of reranking algorithms other than the standard grid search described in Section 3.7 .
For example , we experiment withed optimizing model weights by machine learning based methods such as those using support vector machines , XGBoost ( Chen and Guestrin , 2016 ) , and deep neural networks .
Unfortunately , none of them worked well .
In this competition , we only used the model scores for the reranking .
This setting immediately leads the overfitting to the development sets , and hard to extract meaningful generalized weights ( rules ) that also work well for unseen test data .
The development of the methods that can further and consistently improve the quality of translations is our future work for the next year .
Japanese Text and Brackets Figure 2 shows examples from the validation set of the En? Ja task .
These examples illustrate the weakness of our model , in which the named entities are often inappropriately translated .
According to the references in the figure , the named entities must be translated from alphabetical characters to katakana ( ? ) , e.g. , Cassidy Stay to ? ?.
Although our model successfully translates the named entities in most of the cases , the model also copies original alphabetical characters into the brackets .
For example , the model translates Madam Needjan to ? ?( Madam Needjan ) .
These alphabetical characters damage the BLEU score .
We can remove the extra brackets by the rule- based post-processing ; however , we find that this naive operation hurts the brevity penalty .
This extra bracket problem seems to reflect the way that the named entities are written in the En? Ja training data such as KFTT .
We should have considered special preprocessing measures in advance to alleviate this problem .
Conclusion
In this paper , we described the submission of the joint team of Tohoku , AIP , and NTT ( Tohoku - AIP - NTT ) to the WMT '20 news translation task .
We participated in the En?De and En? Ja translation .
In preliminary experiments , we attempted new techniques such as synthetic data filtering , forward - translation , and sophisticated reranking .
However , none of them was effective .
In the submission , we used several standard techniques such as back -translation and fine-tuning .
As a result , we achieved the best BLEU score on De?En and strong results in other directions .
Figure 1 : 1 Figure 1 : Overview of our system .
