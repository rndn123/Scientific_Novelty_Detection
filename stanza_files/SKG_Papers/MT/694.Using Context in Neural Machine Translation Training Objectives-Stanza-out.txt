title
Using Context in Neural Machine Translation Training Objectives
abstract
We present Neural Machine Translation ( NMT ) training using document - level metrics with batch - level documents .
Previous sequence -objective approaches to NMT training focus exclusively on sentence - level metrics like sentence BLEU which do not correspond to the desired evaluation metric , typically document BLEU .
Meanwhile research into document- level NMT training focuses on data or model architecture rather than training procedure .
We find that each of these lines of research has a clear space in it for the other , and propose merging them with a scheme that allows a document - level evaluation metric to be used in the NMT training objective .
We first sample pseudo-documents from sentence samples .
We then approximate the expected document BLEU gradient with Monte Carlo sampling for use as a cost function in Minimum Risk Training ( MRT ) .
This twolevel sampling procedure gives NMT performance gains over sequence MRT and maximum-likelihood training .
We demonstrate that training is more robust for document - level metrics than with sequence metrics .
We further demonstrate improvements on NMT with TER and Grammatical Error Correction ( GEC ) using GLEU , both metrics used at the document level for evaluations .
Introduction Neural Machine Translation ( NMT ) research has explored token - level likelihood functions ( Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) and sequence -level objectives inspired by reinforcement learning ( Ranzato et al. , 2016 ; Bahdanau et al. , 2016 ) or expected Minimum Risk Training ( MRT ) .
A typical sequence objective in these cases is based on sentence - level BLEU ( sBLEU ) ( Edunov et al. , 2018 ) .
However * Now at Google sBLEU , even if aggregated over sentences , is only an approximation of the desired metric , documentlevel BLEU .
Beyond translation , many metrics for natural language tasks do not have robust sentencelevel approximations .
A logical progression is the extension of sequence - level NMT training objectives to include context from outside the sentence .
Document - based NMT , by contrast , aims to use out - of-sentence context to improve translation .
Recent research explores lexical consistency by providing additional sentences during training ( Maruf et al. , 2019 ; Voita et al. , 2018 Voita et al. , , 2019 or inference ( Voita et al. , 2019 ; Stahlberg et al. , 2019 ) , potentially with adjustments to model architecture .
However , to the best of our knowledge , no attempt has been made to extend sequence - level neural training objectives to include document - level reward functions .
This is despite document- level BLEU being arguably the most common NMT metric , and being the function originally optimised by Minimum Error Rate Training ( MERT ) for Statistical Machine Translation ( SMT ) ( Och , 2003 ) .
We propose merging lines of research on training objectives and document-level translation .
We achieve this by presenting a document- level approach to sequence - level objectives which brings the training objective closer to the actual evaluation metric , using MRT as a representative example .
We demonstrate MRT under document- level BLEU as well as Translation Edit Rate ( TER ) ( Snover , 2006 ) , which while decomposable to sentence level is less noisy when used over documents .
We consider both pseudo-documents where sentences are assigned randomly to a mini-batch , and true document context where all sentences in the batch are from the same document .
We finally apply our scheme to supervised Grammatical Error Correction , for which using neural models is becoming increasingly popular ( Xie et al. , 2016 ; Stahlberg et al. , 2019 ) .
We show gains in GEC metrics GLEU ( Napoles et al. , 2015 ) and M2 ( Dahlmeier and Ng , 2012 ) .
Related Work Minimum Error Rate Training was introduced for phrase - based SMT with document- level BLEU ( Och , 2003 ) . extend these ideas to NMT , using expected minimum risk at the sequence level with an sBLEU cost for end-to - end NMT training .
Edunov et al. ( 2018 ) explore random and beam sampling for NMT sequence - MRT , as well as other sequence - level training losses .
Related developments in NMT include combined reinforcement - learning / cross-entropy approaches such as MIXER ( Ranzato et al. , 2016 ) , which itself has origins in the REINFORCE algorithm described by Williams ( 1992 ) .
We do not explore such approaches , although our documentsampling and document-metric schemes could in principle be extended to them .
Sequence-level MRT has seen success outside NMT .
Ayana et al. ( 2016 ) use sequence MRT for summarization , while Shannon ( 2017 ) uses a related approach for speech recognition .
MRT can be seen as a special case of neural reinforcement learning , which apply to GEC with sequence - level costs .
Closest to our approach is the work of Jean and Cho ( 2019 ) on NMT with a minibatch - context-sensitive training procedure .
However , they do not optimize on document metrics over those contexts .
They also sample contexts randomly , while we find diverse context sampling is important for the success of document - MRT .
Background
Sequence-level MRT Sentence-level MRT for NMT aims to minimize the expected loss on training data with a loss function between sampled target sentences y and gold reference sentences y * .
For NMT a common sentencelevel cost function ?(y , y * ) is 1 - sBLEU , where sBLEU is smoothed by setting initial n-gram counts to 1 ( Edunov et al. , 2018 ) .
We take N samples for each of the S sentences in a mini-batch .
We write the cost function between the s th reference in a mini-batch , y ( s ) * , and its n th sample , y ( s ) n , as ? ( s ) n = ?( y ( s ) n , y ( s ) * ) .
The risk gradient for end-to- end NMT with MRT as in , with sample - count scaling , is then : ? ? R ( ? ) = 1 N S s=1 N n=1 ? ( s ) n ? ? log P ( y ( s ) n |x ( s ) ; ? ) ( 1 )
Document- level MRT
By analogy with sequence- level MRT , we consider MRT over batches of S sentence pairs , which we treat as a pseudo-document .
In practice we experiment both with sentences chosen randomly from all training data , and with true context where all sentences per batch are from a single document .
Let X = [ x ( 1 ) , . . . , x ( S ) ] be the source document , Y = [ y ( 1 ) , . . . , y ( S ) ] be a document of candidate translations , and Y * = [ y ( 1 ) * , . . . , y ( S ) * ] be the reference translations .
Document - level metric D( Y , Y * ) , which may be non-differentiable , replaces the sequence - level metric ?(y , y ( s ) * ) .
We define the document- level risk : R ( ? ) = Y D( Y , Y * ) P ( Y |X ; ? ) Using p ? ? ? log p ? = ?p ? , and defining L( Y ) = log P ( Y |X ; ? ) for brevity : ? ? R ( ? ) = Y D( Y , Y * ) P ( Y |X ; ? ) ? ? L( Y ) = E D( Y , Y * ) ? ? L( Y ) |X ; ? ( 2 ) Using simple Monte-Carlo , after Shannon ( 2017 ) , we replace the expectation by an average taken over N sampled translation documents Y n ? P ( Y |X ; ? ) ? ? R ( ? ) ? 1 N N n=1 D( Y n , Y * ) ? ? L( Y n )
The n th sample for the s th sentence in the batchlevel document , y n , contributes the following term to the overall gradient : 1 N Y :y ( s ) =y ( s ) n D( Y , Y * ) ? ? log P ( y ( s ) n |x ( s ) ; ?)
In other words the gradient of each sample is weighted by the aggregated document - level scores for documents in which the sample appears .
Mini- batch level document sampling
To generate sample documents we first sample sentences .
Sentence sampling for NMT generates new tokens in a left-to- right manner .
In left- to- right generation each token is sampled from a distribution conditioned on previously sampled tokens , minimizing exposure bias to gold references which the model is unlikely to see at inference time ( Ranzato et al. , 2016 ) .
Sampling can be via beam search , or random sampling from the model distribution given previously sampled tokens .
Beam search produces more likely samples which may be less diverse compared to random sampling ( Edunov et al. , 2018 ) .
Here we only consider sampling during training .
While samples can be more easily generated offline with respect to fixed model parameters , such samples are not representative of the current model .
With N sample translations for each of the S sentence pairs per batch we can construct N S possible sample documents as sequences of S sentences .
Considering all possible documents is intractable unless N and S are small .
It also carries the risk that a single sentence will appear in multiple sampled documents , giving it undue weight .
Instead we propose creating N documents by first ordering samples for each sentence ( e.g. by sBLEU ) , then creating the n th sample document Y n by concatenating the n th sample from each sentence .
This gives a set of N diverse documents sampled from N S possibilities .
We expect the sampled documents to be diverse in contents , since a given sentence will only ever occur in a single document context , and diverse in score .
We refer to this scheme as ordered document sampling .
Figure 1 illustrates ordered document sampling by comparison to a scheme which randomly samples sentences to form documents .
Experiments
We report on English - German NMT .
We initialize with a baseline trained on 17.5 M sentence pairs from WMT19 news task datasets ( Barrault et al. , 2019 ) , on which we learn a 32K - merge joint BPE vocabulary ( Sennrich et al. , 2016 ) .
We validate on newstest2017 , and evaluate on newstest2018 .
We apply MRT only during fine-tuning , following previous work ( Edunov et al. , 2018 ; .
In early experiments , we found that training from scratch with discriminative objectives ( sequence - or document - based ) is ineffective .
We suspect samples produced early in training are so unlike the references that the model never receives a strong enough signal for effective training .
We fine-tune on old WMT news task test sets ( 2008 ) ( 2009 ) ( 2010 ) ( 2011 ) ( 2012 ) ( 2013 ) ( 2014 ) ( 2015 ) ( 2016 ) in two settings .
With random batches sentences from different documents are shuffled randomly into mini-batches .
In this case doc-MRT metrics are over pseudo-documents .
With document batches each batch contains only sentences from one document , and doc-MRT uses true document context .
We use the same sampling temperatures and the same risk sharpness factors for both forms of MRT for each experiment .
For Grammatical Error Correction ( GEC ) we train on sentences from NUCLE ( Dahlmeier et al. , 2013 ) and Lang-8 Learner English ( Mizumoto et al. , 2012 ) with at least one correction , a total of 660K sentences .
We evaluate on the JFLEG ( Napoles et al. , 2017 )
For all models we use a Transformer model ( Vaswani et al. , 2017 ) with the ' base ' Ten-sor2 Tensor parameters ( Vaswani et al. , 2018 ) .
We train to validation set BLEU convergence on a single GPU .
The batch size for baselines and MLE is 4096 tokens .
For MRT , where each sentence in the batch is sampled N times , we reduce batch size by N while delaying gradient updates by the same factor to keep the effective batch size constant ( Saunders et al. , 2018 ) .
At inference time we decode using beam size 4 .
All BLEU scores are for cased , detokenized output , calculated using SacreBLEU ( Post , 2018 ) .
Computation and sample count Our proposed document - MRT approach is more complex than sequence - MRT due to the additional score-aggregation and context-sampling steps .
In practice we find that the extra computation of ordering and aggregating sequence scores is negligible when compared to the computational cost of sentence sampling , required for all forms of MRT .
Our MRT experiments use N = 8 random samples per sentence unless otherwise stated .
In this we choose the highest N we can practically experiment with , since previous work finds MRT performance increasing steadily with more samples per sentence .
That we see improvements with so few samples is in contrast to previous work which finds BLEU gains only with 20 or more samples per sentence for sequence - MRT Edunov et al. , 2018 ) .
However , we find that document - MRT allows improvements with far fewer samples , perhaps because the aggregation of scores over sentences in a context increases robustness to variation in individual samples .
Relatedly , we find that add- one BLEU smoothing ( Lin and Och , 2004 ) is required for sequence - MRT as in .
However we find that doc-MRT can achieve good results without smoothing , perhaps because n-gram precisions are far less likely to be 0 when calculated over a document .
Table 2 : TER on en-de after MLE and MRT under sentence - TER ( seq - MRT ) and doc-TER ( doc - MRT ) .
MRT for NMT Lower TER is better .
MLE fine-tuning degrades the baseline .
This suggests the baseline is well - converged , as is desirable for applying MRT .
The degradation is smaller with batches containing only sentences from the same document .
We connect this to the idea that NMT batches with fewer sentence pairs have ' noisier ' estimated gradients , harming training ( Saunders et al. , 2018 ) .
We expect batches of sentences from a single document to be similar and therefore give less noisy gradient estimates .
Both seq -MRT and doc-MRT improve over the baseline with random sampling and N = 8 .
We also explore MRT at N = 4 , with batch size adjusted as described in section 3 for the same effective batch size per update , and with fewer training steps such that the model ' sees ' a similar proportion of the overall dataset .
We do not report beam sampling results as early experiments indicate beam sampling gives similarly poor results for both seq -MRT and doc-MRT .
This may be because beam search produces insufficiently diverse samples for this task ( Freitag and Al - Onaizan , 2017 ) .
Sequence -MRT gives a 0.8 BLEU gain over the baseline with both batching schemes using N = 8 samples , but starts to degrade the baseline with N = 4 samples .
With document batches and N = 8 Doc-MRT ( ordered ) outperforms seq -MRT by a further 0.4 BLEU .
With N = 4 doc-MRT ( ordered ) still achieves a 0.7 BLEU improvement over the baseline , or a 0.8 BLEU improvement over seq-MRT .
We suggest therefore that doc-MRT ( ordered ) may be a computationally more efficient alternative to seq - MRT when large sample counts are not practical .
For contrast with the ordered document sampling approach of Section 2.3 , we give results for doc-MRT ( random ) , which uses randomly sampled contexts .
This approach falls significantly behind doc-MRT ( ordered ) with either batching scheme .
Since doc-MRT ( random ) with random batches is exposed to randomness at the batch construction , sentence sampling and document sampling stages , these results are averages over 3 experimental runs , which gave fairly consistent results ( < 0.2 BLEU range ) .
In general we do find that results with random batches and random ordering are variable and sensitive to batch size and batching scheme .
We interpret these results by considering the effect on the per-sentence cost for the different schemes .
We find MRT works well when sample scores are different enough to be discriminated , but suffers if scores are too different .
This is in line with the findings of Edunov et al . ( 2018 ) that including the gold reference causes the model to assign low relative probabilities to every other sample .
Doc - MRT aggregates scores over many samples , while seq -MRT uses individual scores .
We believe this explains the stronger performance of doc-MRT for small values of N , especially for the ordered document scheme , which ensures scores are still different enough for MRT to discriminate .
Our approach can also be used with documentlevel metrics that are not intended to be used with individual sentences .
In Table 2 we demonstrate this with TER , which estimates the edit rate required to correct a set of translation hypotheses .
Document-TER MRT improves over a strong baseline , although batching scheme has less of an impact here .
Notably seq-level MRT does not improve TER over the baseline , indicating TER may be too noisy a metric for use at the sentence level .
MRT for GEC Finally , we apply our MRT approach to the GEC GLEU metric ( Napoles et al. , 2015 ) , an n-gram edit measure typically used at the document level .
Table 3 shows that document MRT fine-tuning improves GLEU over the baseline , MLE fine-tuning , and a sequence - GLEU MRT formulation .
Also notable is the change in M2 , which finds the phraselevel edit sequence achieving the highest overlap with the gold-standard ( Dahlmeier and Ng , 2012 ) .
MLE and sequence - MRT improve recall at a detriment to precision , suggesting over-generation of spurious corrections .
Document - MRT likewise improves recall , but with a precision score closer to the baseline for more balanced performance .
There is clear indication of a tension between M2 and GLEU : a small increase in GLEU under doc-MRT on CONLL leads to a large increase in M2 , while a large increase in GLEU under doc-MRT on JFLEG leads to a small decrease in M2 .
We note that our improvements on JFLEG are similar to the improvements shown by for neural reinforcement learning with a sequence - GLEU cost metric .
However , their results involve N=20 samples and 600k updates , compared to N=8 and 3 k updates with our approach .
Conclusions and future work
We present a novel approach for structured loss training with document - level objective functions .
Our approach relies on a procedure for sampling a set of diverse batch - level contexts using N-wise sample ordering .
As well as randomly selecting training data , we assess training with mini-batches consisting only of single document contexts .
While the scope of this work does not extend to sampling sentences given document context , this would be an interesting direction for future work .
We demonstrate improvements covering three document-level evaluation metrics : BLEU and TER for NMT and GLEU for GEC .
We finish by noting that the original MERT procedure developed for SMT optimised document- level BLEU and with our procedure we reintroduce this to NMT .
Figure 1 : 1 Figure 1 : Sample-ordering schemes for MRT with S = 2 sentences / batch and N = 3 samples / sentence , showing sample costs .
In sequence - MRT each sample has its own cost ( e.g. sBLEU ) .
For doc-MRT ( ordered ) , samples are ordered and sorted into N-wise ' documents ' , each with a combined cost ( e.g. document BLEU ) .
The ordered assignment enforces an extreme range of combined costs .
In doc-MRT ( random ) , samples are randomly assigned , making documents on average less diverse with less distinct scores , with a low likelihood of extreme distributions .
and CoNLL 2014 ( Ng et al. , 2014 ) sets .
For GEC experiments we use random batching only .
We compare sentence - BLEU and document - BLEU MRT to fine-tuning with Maximum Likelihood Estimation ( MLE ) .
Model Random Document batches batches Baseline 39.2 39.2 MLE 41.2 40.0 Seq-MRT 39.4 40.5 Doc-MRT ( ordered ) 39.0 38.9 Model Random batches Document batches Baseline 42.7 MLE 40.0 41.0 N = 4 N = 8 N = 4 N = 8 Seq-MRT 42.6 43.5 42.6 43.5 Doc-MRT 41.7 * 43.1 * 43.1 43.0 ( random ) Doc-MRT 43.4 43.7 43.4 43.9 ( ordered ) Table 1 : BLEU on en-de after MLE and MRT under 1?sBLEU ( seq - MRT ) and 1? doc BLEU ( doc - MRT ) .
Results indicated by * are averages over 3 runs with the same settings , which all came within 0.2 BLEU .
.In Table1 , we fine-tune an en-de baseline on documents from past news sets .
Table 3 : 3 GEC Precision , Recall , M2 , and GLEU after MLE and MRT .
MRT is under 1 ? sentence -GLEU for seq-MRT and 1?doc- GLEU for doc-MRT .
Both MRT schemes uses random batches and random sentence sampling .
Higher scores are better for all metrics .
Model JFLEG CONLL2014 P R M2 GLEU P R M2 GLEU Baseline 67.3 38.2 58.4 50.4 54.4 21.8 41.9 67.3 MLE 64.7 37.7 56.6 50.1 51.4 20.9 39.8 67.1 Seq-MRT 62.7 39.1 56.0 50.0 52.4 24.5 42.7 67.1 Doc-MRT ( ordered ) 64.4 41.0 57.8 51.4 53.2 24.6 43.2 67.5
http://www.hpc.cam.ac.uk
