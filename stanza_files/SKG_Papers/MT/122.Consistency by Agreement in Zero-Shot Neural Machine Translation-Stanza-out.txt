title
Consistency by Agreement in Zero-shot Neural Machine Translation
abstract
Generalization and reliability of multilingual translation often highly depend on the amount of available parallel data for each language pair of interest .
In this paper , we focus on zero-shot generalization - a challenging setup that tests models on translation directions they have not been optimized for at training time .
To solve the problem , we ( i ) reformulate multilingual translation as probabilistic inference , ( ii ) define the notion of zero-shot consistency and show why standard training often results in models unsuitable for zero-shot tasks , and ( iii ) introduce a consistent agreement - based training method that encourages the model to produce equivalent translations of parallel sentences in auxiliary languages .
We test our multilingual NMT models on multiple public zeroshot translation benchmarks ( IWSLT17 , UN corpus , Europarl ) and show that agreementbased learning often results in 2 - 3 BLEU zeroshot improvement over strong baselines without any loss in performance on supervised translation directions .
Introduction Machine translation ( MT ) has made remarkable advances with the advent of deep learning approaches ( Bojar et al. , 2016 ; Crego et al. , 2016 ; .
The progress was largely driven by the encoder-decoder framework ( Sutskever et al. , 2014 ; and typically supplemented with an attention mechanism Luong et al. , 2015 b ) .
Compared to the traditional phrase - based systems ( Koehn , 2009 ) , neural machine translation ( NMT ) requires large amounts of data in order to reach high performance ( Koehn and Knowles , 2017 ) .
Using NMT in a multilingual setting exacerbates the problem by the fact that given k languages *
Work done at Google .
At training time , given English - French ( En ? Fr ) and English - German ( En ? De ) parallel sentences , the model not only is trained to translate between the pair but also to agree on translations into a third language .
translating between all pairs would require O( k 2 ) parallel training corpora ( and O ( k 2 ) models ) .
In an effort to address the problem , different multilingual NMT approaches have been proposed recently .
Luong et al . ( 2015 a ) ; Firat et al. ( 2016a ) proposed to use O( k ) encoders / decoders that are then intermixed to translate between language pairs .
Johnson et al. ( 2016 ) proposed to use a single model and prepend special symbols to the source text to indicate the target language , which has later been extended to other text preprocessing approaches ( Ha et al. , 2017 ) as well as languageconditional parameter generation for encoders and decoders of a single model ( Platanios et al. , 2018 ) . Johnson et al. ( 2016 ) also show that a single multilingual system could potentially enable zeroshot translation , i.e. , it can translate between language pairs not seen in training .
For example , given 3 languages - German ( De ) , English ( En ) , and French ( Fr ) - and training parallel data only for ( De , En ) and ( En , Fr ) , at test time , the system could additionally translate between ( De , Fr ) . Zero-shot translation is an important problem .
Solving the problem could significantly improve data efficiency - a single multilingual model would be able to generalize and translate between any of the O(k 2 ) language pairs after being trained only on O( k ) parallel corpora .
However , performance on zero-shot tasks is often unstable and significantly lags behind the supervised directions .
Moreover , attempts to improve zero-shot performance by fine-tuning ( Firat et al. , 2016 b ; Sestorain et al. , 2018 ) may negatively impact other directions .
In this work , we take a different approach and aim to improve the training procedure of Johnson et al . ( 2016 ) .
First , we analyze multilingual translation problem from a probabilistic perspective and define the notion of zero-shot consistency that gives insights as to why the vanilla training method may not yield models with good zero-shot performance .
Next , we propose a novel training objective and a modified learning algorithm that achieves consistency via agreement - based learning ( Liang et al. , 2006 ( Liang et al. , , 2008 and improves zero-shot translation .
Our training procedure encourages the model to produce equivalent translations of parallel training sentences into an auxiliary language ( Figure 1 ) and is provably zero-shot consistent .
In addition , we make a simple change to the neural decoder to make the agreement losses fully differentiable .
We conduct experiments on IWSLT17 ( Mauro et al. , 2017 ) , UN corpus ( Ziemski et al. , 2016 ) , and Europarl ( Koehn , 2017 ) , carefully removing complete pivots from the training corpora .
Agreementbased learning results in up to +3 BLEU zero-shot improvement over the baseline , compares favorably ( up to + 2.4 BLEU ) to other approaches in the literature Sestorain et al. , 2018 ) , is competitive with pivoting , and does not lose in performance on supervised directions .
Related work A simple ( and yet effective ) baseline for zero-shot translation is pivoting that chain- translates , first to a pivot language , then to a target ( Cohn and Lapata , 2007 ; Wu and Wang , 2007 ; Utiyama and Isahara , 2007 ) .
Despite being a pipeline , pivoting gets better as the supervised models improve , which makes it a strong baseline in the zero-shot setting .
proposed a joint pivoting learning strategy that leads to further improvements .
Lu et al. ( 2018 ) and Arivazhagan et al . ( 2018 ) proposed different techniques to obtain " neural interlingual " representations that are passed to the decoder .
Sestorain et al. ( 2018 ) proposed another fine-tuning technique that uses dual learning ( He et al. , 2016 ) , where a language model is used to provide a signal for fine-tuning zero-shot directions .
Another family of approaches is based on distillation ( Hinton et al. , 2014 ; Kim and Rush , 2016 ) .
Along these lines , Firat et al . ( 2016 b ) proposed to fine tune a multilingual model to a specified zeroshot-direction with pseudo- parallel data and Chen et al . ( 2017 ) proposed a teacher -student framework .
While this can yield solid performance improvements , it also adds multi-staging overhead and often does not preserve performance of a single model on the supervised directions .
We note that our approach ( and agreement - based learning in general ) is somewhat similar to distillation at training time , which has been explored for large-scale single - task prediction problems ( Anil et al. , 2018 ) .
A setting harder than zero-shot is that of fully unsupervised translation ( Ravi and Knight , 2011 ; Artetxe et al. , 2017 ; in which no parallel data is available for training .
The ideas proposed in these works ( e.g. , bilingual dictionaries , backtranslation ( Sennrich et al. , 2015a ) and language models ( He et al. , 2016 ) ) are complementary to our approach , which encourages agreement among different translation directions in the zero-shot multilingual setting .
Background
We start by establishing more formal notation and briefly reviewing some background on encoderdecoder multilingual machine translation from a probabilistic perspective .
Notation Languages .
We assume that we are given a collection of k languages , L 1 , . . . , L k , that share a common vocabulary , V .
A language , L i , is defined by the marginal probability P ( x i ) it assigns to sentences ( i.e. , sequences of tokens from the vocabulary ) , denoted x i := ( x 1 , . . . , x l ) , where l is the length of the sequence .
All languages together define a joint probability distribution , P ( x 1 , . . . , x k ) , over k-tuples of equivalent sentences .
Corpora .
While each sentence may have an equivalent representation in all languages , we assume that we have access to only partial sets of equivalent sentences , which form corpora .
In this work , we consider bilingual corpora , denoted C ij , that contain pairs of sentences sampled from P ( x i , x j ) and monolingual corpora , denoted C i , that contain sentences sampled from P ( x i ) .
En Es Fr Ru Translation .
Finally , we define a translation task from language L i to L j as learning to model the conditional distribution P ( x j | x i ) .
The set of k languages along with translation tasks can be represented as a directed graph G ( V , E ) with a set of k nodes , V , that represent languages and edges , E , that indicate translation directions .
We further distinguish between two disjoint subsets of edges : ( i ) supervised edges , E s , for which we have parallel data , and ( ii ) zero-shot edges , E 0 , that correspond to zero-shot translation tasks .
Figure 2 presents an example translation graph with supervised edges ( En ? Es , En ? Fr , En ? Ru ) and zero-shot edges ( Es ? Fr , Es ? Ru , Fr ? Ru ) .
We will use this graph as our running example .
C E n E s C E n F r C EnRu
Encoder-decoder framework First , consider a purely bilingual setting , where we learn to translate from a source language , L s , to a target language , L t .
We can train a translation model by optimizing the conditional log-likelihood of the bilingual data under the model : CNNs ( Gehring et al. , 2016 ) , and attention Vaswani et al. , 2017 ) as building blocks .
The decoding distribution , P dec ? ( x t | u ) , is typically modeled autoregressively .
Multilingual neural machine translation
In the multilingual setting , we would like to learn to translate in all directions having access to only few parallel bilingual corpora .
In other words , we would like to learn a collection of models , { P ? ( x j | x i ) } i , j?E .
We can assume that models are independent and choose to learn them by maximizing the following objective : L ind ( ? ) = i , j?
Es ( x i , x j ) ?
C ij log P ? ( x j | x i ) ( 3 )
In the statistics literature , this estimation approach is called maximum composite likelihood ( Besag , 1975 ; Lindsay , 1988 ) as it composes the objective out of ( sometimes weighted ) terms that represent conditional sub-likelihoods ( in our example , P ? ( x j | x i ) ) .
Composite likelihoods are easy to construct and tractable to optimize as they do not require representing the full likelihood , which would involve integrating out variables unobserved in the data ( see Appendix A.1 ) .
Johnson et al. ( 2016 ) proposed to train a multilingual NMT systems by optimizing a composite likelihood objective ( 3 ) while representing all conditional distributions , P ? ( x j | x i ) , with a shared encoder and decoder and using language tags , l t , to distinguish between translation directions : P ( x t | x s ) = P dec ?
( x t | u st = f enc ? ( x s , l t ) )
( 4 ) This approach has numerous advantages including : ( a ) simplicity of training and the architecture ( by slightly changing the training data , we convert a bilingual NMT into a multilingual one ) , ( b ) sharing parameters of the model between different translation tasks that may lead to better and more robust representations .
Johnson et al. ( 2016 ) also show that resulting models seem to exhibit some degree of zero-shot generalization enabled by parameter sharing .
However , since we lack data for zero-shot directions , composite likelihood ( 3 ) misses the terms that correspond to the zero-shot models , and hence has no statistical guarantees for performance on zero-shot tasks .
2 4 Zero-shot generalization & consistency Multilingual MT systems can be evaluated in terms of zero-shot performance , or quality of translation along the directions they have not been optimized for ( e.g. , due to lack of data ) .
We formally define zero-shot generalization via consistency .
Definition 1 ( Expected Zero-shot Consistency )
Let E s and E 0 be supervised and zero-shot tasks , respectively .
Let ( ? ) be a non-negative loss function and M be a model with maximum expected supervised loss bounded by some ? > 0 : max ( i , j ) ?
Es E x i , x j [ ( M ) ] < ?
We call M zero-shot consistent with respect to ( ? ) if for some ?(? ) > 0 max ( i , j ) ?E 0 E x i , x j [ ( M ) ] < ?( ? ) , where ?(? ) ? 0 as ? ? 0 . In other words , we say that a machine translation system is zero-shot consistent if low error on supervised tasks implies a low error on zero-shot tasks in expectation ( i.e. , the system generalizes ) .
We also note that our notion of consistency somewhat resembles error bounds in the domain adaptation literature ( Ben - David et al. , 2010 ) .
In practice , it is attractive to have MT systems that are guaranteed to exhibit zero-shot generalization since the access to parallel data is always limited and training is computationally expensive .
While the training method of Johnson et al . ( 2016 ) does not have guarantees , we show that our proposed approach is provably zero-shot consistent .
Approach
We propose a new training objective for multilingual NMT architectures with shared encoders and decoders that avoids the limitations of pure composite likelihoods .
Our method is based on the idea of agreement - based learning initially proposed for learning consistent alignments in phrase - based statistical machine translation ( SMT ) systems ( Liang et al. , 2006 ( Liang et al. , , 2008 .
In terms of the final objective function , the method ends up being reminiscent of distillation ( Kim and Rush , 2016 ) , but suitable for joint multilingual training .
Agreement - based likelihood
To introduce agreement - based objective , we use the graph from Figure 2 that defines translation tasks between 4 languages ( En , Es , Fr , Ru ) .
In particular , consider the composite likelihood objective ( 3 ) for a pair of En ?
Fr sentences , ( x En , x Fr ) : L ind EnFr ( ? ) ( 5 ) = log [ P ? ( x Fr | x En ) P ? ( x En | x Fr ) ]
= log ? ? z Es , z Ru P ? x Fr , z Es , z Ru | x En ? z Es , z Ru P ? x En , z Es , z Ru | x Fr ? ? where we introduced latent translations into Spanish ( Es ) and Russian ( Ru ) and marginalized them out ( by virtually summing over all sequences in the corresponding languages ) .
Again , note that this objective assumes independence of En ? Fr and Fr ?
En models .
Following Liang et al. ( 2008 ) , we propose to tie together the single prime and the double prime latent variables , z Es and z Ru , to encourage agreement between P ? ( x En , z Es , z Ru | x Fr ) and P ? ( x Fr , z Es , z Ru | x En ) on the latent translations .
We interchange the sum and the product operations inside the log in ( 5 ) , denote z := ( z Es , z Ru ) to simplify notation , and arrive at the following new objective function : L agree EnFr ( ? ) := ( 6 ) log z P ? ( x Fr , z | x En ) P ? ( x En , z | x Fr ) Next , we factorize each term as : P ( x , z | y ) = P ( x | z , y ) P ? ( z | y ) Assuming P ? ( x Fr | z , x En ) ? P ? ( x Fr | x En ) , 3 the objective ( 6 ) decomposes into two terms : L agree EnFr ( ? ) ( 7 ) ? log P ? ( x Fr | x En ) + log P ? ( x En | x Fr ) composite likelihood terms + log z P ? ( z | x En ) P ? ( z | x Fr ) agreement term
We call the expression given in ( 7 ) agreementbased likelihood .
Intuitively , this objective is the likelihood of observing parallel sentences ( x En , x Fr ) and having sub-models P ? ( z | x En ) and P ? ( z | x Fr ) agree on all translations into Es and Ru at the same time .
Lower bound .
Summation in the agreement term over z ( i.e. , over possible translations into Es and Ru in our case ) is intractable .
Switching back from z to ( z Es , z Ru ) notation and using Jensen 's inequality , we lower bound it with cross-entropy : 4 log z P ? ( z | x En ) P ? ( z | x Fr ) ?
E zEs|xEn [ log P ? ( z Es | x Fr ) ] + ( 8 ) E zRu|xEn [ log P ? ( z Ru | x Fr ) ]
We can estimate the expectations in the lower bound on the agreement terms by sampling z Es ? P ? ( z Es | x En ) and z Ru ? P ? ( z Ru | x En ) .
In practice , instead of sampling we use greedy , continuous decoding ( with a fixed maximum sequence length ) that also makes z Es and z Ru differentiable with respect to parameters of the model .
Consistency by agreement
We argue that models produced by maximizing agreement - based likelihood ( 7 ) are zero-shot consistent .
Informally , consider again our running example from Figure 2 . Given a pair of parallel sentences in ( En , Fr ) , agreement loss encourages translations from En to { Es , Ru} and translations from Fr to { Es , Ru} to coincide .
Note that En ?
{ Es , Fr , Ru} are supervised directions .
Therefore , agreement ensures that translations along the zero-shot edges in the graph match supervised translations .
Formally , we state it as : Theorem 2 ( Agreement Zero-shot Consistency )
Let L 1 , L 2 , and L 3 be a collection of languages with L 1 ? L 2 and L 2 ? L 3 be supervised while L 1 ? L 3 be a zero-shot direction .
Let P ? ( x j | x i ) be sub-models represented by a multilingual MT system .
If the expected agreement - based loss , E x 1 , x 2 , x 3 [ L agree 12 ( ? ) + L agree 23 ( ? ) ] , is bounded by some ? > 0 , then , under some mild technical assumptions on the true distribution of the equivalent translations , the zero-shot cross-entropy loss is bounded as follows : E x 1 , x 3 [ ? log P ? ( x 3 | x 1 ) ] ? ?(? ) where ?(? ) ? 0 as ? ? 0 . For discussion of the assumptions and details on the proof of the bound , see Appendix A.2 .
Note that Theorem 2 is straightforward to extend from triplets of languages to arbitrary connected graphs , as given in the following corollary .
Corollary 3 Agreement - based learning yields zero shot consistent MT models ( with respect to the cross entropy loss ) for arbitrary translation graphs as long as supervised directions span the graph .
Alternative ways to ensure consistency .
Note that there are other ways to ensure zero-shot consistency , e.g. , by fine-tuning or post-processing a trained multilingual model .
For instance , pivoting through an intermediate language is also zeroshot consistent , but the proof requires stronger assumptions about the quality of the supervised source -pivot model .
5 Similarly , using model distillation ( Kim and Rush , 2016 ; Chen et al. , 2017 ) would be also provably consistent under the same assumptions as given in Theorem 2 , but for a single , pre-selected zero-shot direction .
Note that our proposed agreement - based learning framework is provably consistent for all zero-shot directions and does not require any post-processing .
For discussion of the alternative approaches and consistency proof for pivoting , see Appendix A.3 .
Agreement - based learning algorithm Having derived a new objective function ( 7 ) , we can now learn consistent multilingual NMT models using stochastic gradient method with a couple of extra tricks ( Algorithm 1 ) .
The computation graph for the agreement loss is given in Figure 3 .
Subsampling auxiliary languages .
Computing agreement over all languages for each pair of sentences at training time would be quite computationally expensive ( to agree on k translations , we would need to encode- decode the source and target sequences k times each ) .
However , since the agreement lower bound is a sum over expectations ( 8 ) , we can approximate it by subsampling : at each training step ( and for each sample in the minibatch ) , we pick an auxiliary language uniformly at random and compute stochastic approximation of the agreement lower bound ( 8 ) for that language only .
This stochastic approximation is simple , unbiased , and reduces per step computational overhead for the agreement term from O( k ) to O ( 1 ) .
6 Overview of the agreement loss computation .
Given a pair of parallel sentences , x En and x Fr , and an auxiliary language , say Es , an estimate of the lower bound on the agreement term ( 8 ) is computed as follows .
First , we concatenate
Es language tags to both x En and x Fr and encode the sequences so that both can be translated into Es ( the encoding process is depicted in Figure 3A ) .
Next , we decode each of the encoded sentences and obtain auxiliary translations , z Es ( x En ) and z Es ( x Fr ) , depicted as blue blocks in Figure 3B .
Note that we now can treat pairs ( x Fr , z Es ( x En ) ) and ( x En , z Es ( x Fr ) ) as new parallel data for En ? Es and Fr ? Es.
Finally , using these pairs , we can compute two log-probability terms ( Figure 3B ) : log P ? ( z Es ( x Fr ) | x En ) log P ? ( z Es ( x En ) | x Fr ) ( 9 ) using encoding - decoding with teacher forcing ( same way as typically done for the supervised directions ) .
Crucially , note that z Es ( x En ) corresponds to a supervised direction , En ?
Es , while z Es ( x Fr ) corresponds to zero-shot , Fr ? Es .
We want each of the components to ( i ) improve the zero-shot direction while ( ii ) minimally affecting the supervised direction .
To achieve ( i ) , we use continuous decoding , and for ( ii ) we use stop-gradientbased protection of the supervised directions .
Both techniques are described below .
Greedy continuous decoding .
In order to make z Es ( x En ) and z Es ( x Fr ) differentiable with respect to ?
( hence , continuous decoding ) , at each decoding step t , we treat the output of the RNN , h t , as the key and use dot-product attention over the embedding vocabulary , V , to construct z t Es : z t Es := softmax ( h t ) V V ( 10 )
In other words , auxiliary translations , z Es ( x En ) and z Es ( x Fr ) , are fixed length sequences of differentiable embeddings computed in a greedy fashion .
Protecting supervised directions .
Algorithm 1 scales agreement losses by a small coefficient ?.
We found experimentally that training could be sensitive to this hyperparameter since the agreement loss also affects the supervised sub-models .
For example , agreement of En ? Es ( supervised ) and Fr ? Es ( zero-shot ) may push the former towards a worse translation , especially at the beginning of training .
To stabilize training , we apply the stop gradient operator to the log probabilities and samples produced by the supervised submodels before computing the agreement terms ( 9 ) , to zero-out the corresponding gradient updates .
Experiments
We evaluate agreement - based training against baselines from the literature on three public datasets that have multi-parallel evaluation data that allows assessing zero-shot performance .
We report results in terms of the BLEU score ( Papineni et al. , 2002 ) that was computed using mteval - v13a. perl .
6 overlap between parallel corpora in the supervised directions ( up to 100K sentence pairs per direction ) .
This implicitly makes the dataset multiparallel and defeats the purpose of zero-shot evaluation ( Dabre et al. , 2017 ) .
To avoid spurious effects , we also derived IWSLT17 dataset from the original one by restricting supervised data to only En ?
{ De , Nl , It , Ro } and removing overlapping pivoting sentences .
We report results on both the official and preprocessed datasets .
Preprocessing .
To properly evaluate systems in terms of zero-shot generalization , we preprocess Europarl and IWSLT to avoid multi-lingual parallel sentences of the form source- pivot-target , where source - target is a zero-shot direction .
To do so , we follow Cheng et al . ( 2017 ) ;
Chen et al. ( 2017 ) and randomly split the overlapping pivot sentences of the original source-pivot and pivot-target corpora into two parts and merge them separately with the non-overlapping parts for each pair .
Along with each parallel training sentence , we save information about source and target tags , after which all the data is combined and shuffled .
Finally , we use a shared multilingual subword vocabulary ( Sennrich et al. , 2015 b ) on the training data ( with 32 K merge ops ) , separately for each dataset .
Data statistics are provided in Appendix A.5 .
Training and evaluation Additional details on the hyperparameters can be found in Appendix A.4 .
Models .
We use a smaller version of the GNMT architecture in all our experiments : 512 - dimensional embeddings ( separate for source and target sides ) , 2 bidirectional LSTM layers of 512 units each for encoding , and GNMT - style , 4layer , 512 - unit LSMT decoder with residual connections from the 2nd layer onward .
Training .
We trained the above model using the standard method of Johnson et al . ( 2016 ) and using our proposed agreement - based training ( Algorithm 1 ) .
In both cases , the model was optimized using Adafactor ( Shazeer and Stern , 2018 ) on a machine with 4 P100 GPUs for up to 500K steps , with early stopping on the dev set .
Evaluation .
We focus our evaluation mainly on zero-shot performance of the following methods : ( a ) Basic , which stands for directly evaluating a multilingual GNMT model after standard training ( Johnson et al. , 2016 ) . ( b) Pivot , which performs pivoting - based inference using a multilingual GNMT model ( after standard training ) ; often regarded as gold-standard .
( c ) Agree , which applies a multilingual GNMT model trained with agreement losses directly to zero-shot directions .
To ensure a fair comparison in terms of model capacity , all the techniques above use the same multilingual GNMT architecture described in the previous section .
All other results provided in the tables are as reported in the literature .
Implementation .
All our methods were implemented using TensorFlow ( Abadi et al. , 2016 ) on top of tensor2tensor library ( Vaswani et al. , 2018 ) .
Our code will be made publicly available .
9
Results on UN Corpus and Europarl UN Corpus .
Tables 1 and 2 show results on the UNCorpus datasets .
Our approach consistently outperforms Basic and Dual -0 , despite the latter being trained with additional monolingual data ( Sestorain et al. , 2018 ) .
We see that models trained with agreement perform comparably to Pivot , outperforming it in some cases , e.g. , when the target is Russian , perhaps because it is quite 9 www.cs.cmu.edu/ ?mshediva/ code / . ? Distillation ( Chen et al. , 2017 ) . different linguistically from the English pivot .
Furthermore , unlike Dual-0 , Agree maintains high performance in the supervised directions ( within 1 BLEU point compared to Basic ) , indicating that our agreement - based approach is effective as a part of a single multilingual system .
Europarl .
Table 3 shows the results on the Europarl corpus .
On this dataset , our approach consistently outperforms Basic by 2 - 3 BLEU points but lags a bit behind Pivot on average ( except on Es ?
De where it is better ) .
10 and Chen et al . ( 2017 ) have reported zero-resource results on a subset of these directions and our approach outperforms the former but not the latter on these pairs .
Note that both and Chen et al . ( 2017 ) train separate models for each language pair and the approach of Chen et al . ( 2017 ) would require training O( k 2 ) models to encompass all the pairs .
In contrast , we use a single multilingual architecture which has more limited model capacity ( although in theory , our approach is also compatible with using separate models for each direction ) .
Analysis of IWSLT17 zero-shot tasks Table 4 presents results on the original IWSLT17 task .
We note that because of the large amount of overlap and presence of many supervised translation pairs ( 16 ) the vanilla training method ( Johnson et al. , 2016 ) achieves very high zero shot performance , even outperforming Pivot .
While our approach gives small gains over these baselines , we believe the dataset 's pecularities make it not reliable for evaluating zero-shot generalization .
On the other hand , on our proposed preprocessed IWSLT17 that eliminates the overlap and reduces the number of supervised directions ( 8 ) , there is a considerable gap between the supervised and zeroshot performance of Basic .
Agree performs better than Basic and is slightly worse than Pivot .
Small data regime
To better understand the dynamics of different methods in the small data regime , we also trained all our methods on subsets of the Europarl for 200K steps and evaluated on the dev set .
The training set size varied from 50 to 450 K parallel sentences .
From Figure 4 , Basic tends to perform extremely poorly while Agree is the most robust ( also in terms of variance across zero-shot directions ) .
We see that Agree generally upper-bounds Pivot , except for the ( Es , Fr ) pair , perhaps due to fewer cascading errors along these directions .
Conclusion
In this work , we studied zero-shot generalization in the context of multilingual neural machine translation .
First , we introduced the concept of zero-shot consistency that implies generalization .
Next , we proposed a provably consistent agreement - based learning approach for zero-shot translation .
Empirical results on three datasets showed that agreementbased learning results in up to + 3 BLEU zero-shot improvement over the Johnson et al . ( 2016 ) baseline , compares favorably to other approaches in the literature Sestorain et al. , 2018 ) , is competitive with pivoting , and does not lose in performance on supervised directions .
We believe that the theory and methodology behind agreement - based learning could be useful beyond translation , especially in multi-modal settings .
For instance , it could be applied to tasks such as cross-lingual natural language inference , style-transfer ( Shen et al. , 2017 ; Fu et al. , 2017 ; Prabhumoye et al. , 2018 ) , or multilingual image or video captioning .
Another interesting future direction would be to explore different hand - engineered or learned data representations , which one could use to encourage models to agree on during training ( e.g. , make translation models agree on latent semantic parses , summaries , or potentially other data representations available at training time ) .
