title
A Simple and Effective Unified Encoder for Document -Level Machine Translation
abstract
Most of the existing models for documentlevel machine translation adopt dual-encoder structures .
The representation of the source sentences and the document- level contexts 1 are modeled with two separate encoders .
Although these models can make use of the document- level contexts , they do not fully model the interaction between the contexts and the source sentences , and can not directly adapt to the recent pre-training models ( e.g. , BERT ) which encodes multiple sentences with a single encoder .
In this work , we propose a simple and effective unified encoder that can outperform the baseline models of dualencoder models in terms of BLEU and ME- TEOR scores .
Moreover , the pre-training models can further boost the performance of our proposed model .
Introduction
Thanks to the development of the deep learning methods , the machine translation systems have achieved good performance that is even comparable with human translation in the news domain ( Hassan et al. , 2018 ) .
However , there are still some problems with machine translation in the documentlevel context ( L?ubli et al. , 2018 ) .
Therefore , more recent work ( Jean et al. , 2017 ; Wang et al. , 2017 ; Tiedemann and Scherrer , 2017 ; Maruf and Haffari , 2018 ; Bawden et al. , 2018 ; Voita et al. , 2019a ; Junczys -Dowmunt , 2019 ) is focusing on the document- level machine translation .
Most of the existing models Maruf et al. , 2019 ; Werlen et al. , 2018 ) for document - level machine translation use two encoders to model the source sentences and the document- level contexts .
Figure 1a illustrates the structure of these models .
They extend the standard 1
In this work , document - level contexts denote the surrounding sentences of the current source sentence .
Transformer model with a new context encoder , and the encoder for source sentences is conditioned on this context encoder .
However , they do not fully model the interaction between the contexts and the source sentences because the self-attention layers are performed inside each encoder separately .
Moreover , it cannot be directly adapted to the recent pre-training models ( Devlin et al. , 2019 ; Peters et al. , 2018 ; Radford et al. , 2019 ; Dong et al. , 2019 ; Song et al. , 2019 ; Lample and Conneau , 2019 ) , which encodes multiple sentences with a single encoder .
Different from the dual-encoder structure , the uni-encoder structure takes the concatenation of contexts and source sentences as the input ( as shown in Figure 1 b ) .
Therefore , when modeling the contexts , it can make full use of the interaction between the source sentences and the contexts , while the dual-encoder model fails to exploit this information .
Moreover , the uni-encoder structure is identical to the recent pre-training models ( e.g. , BERT ) .
However , the previous uni structure suffers from two problems for document - level machine translation .
First , the attention is distracted due to longer sequences .
Second , the source sentences and the contexts are modeled equally , which is contrary to the fact that the translation is more related to the current source sentences .
To address these problems , we propose a novel flat structure with a unified encoder called Flat-Transformer .
It separates the encoder of standard Transformers into two parts so that the attention can concentrate at both the global level and the local level .
At the bottom of the encoder blocks , the self-attention is applied to the whole sequence .
At the top of the blocks , it is only implemented at the position of the source sentences .
We evaluate this model on three document - level machine translation datasets .
Experiments show that it can achieve better performance than the baseline models of dual-encoder structures in terms of BLEU and METEOR scores .
Moreover , the pre-training models can further boost the performance of the proposed structure .
Flat-Transformer
In this section , we introduce our proposed flat structured model , which we denote as Flat-Transformer .
Document - Level Translation Formally , we denote X = {x 1 , x 2 , ? ? ? , x N } as the source document with N sentences , and Y = {y 1 , y 2 , ? ? ? , y M } as the target document with M sentences .
We assume that N = M because the sentence mismatches can be fixed by merging sentences with sentence alignment algorithms ( Sennrich and Volk , 2011 ) .
Therefore , we can assume that ( x i , y i ) is a parallel sentence pair .
Following , y < i can be omitted because x < i and y < i conveys the same information .
As a result , the probability can be approximated as : P ( Y | X ) ? N i=1 P ( y i |x i ; x < i ; x >i ) ( 1 ) where x i is the source sentence aligned to y i , and ( x < i , x >i ) is the document- level context used to translate y i .
Segment Embedding
The flat structure adopts a unified encoder that does not distinguish the context sentences and the source sentences .
Therefore , we introduce the segment embedding to identify these two types of inputs .
Formally , given the source input of the surrounding context c and the current sentence x , we project them into word embedding and segment embedding .
Then , we perform a concatenation operation to unify them into a single input : e = [ E ( c ) ; E( x ) ] ( 2 ) s = [ S ( c ) ; S ( x ) ] ( 3 ) where [ ; ] denotes the concatenation operation , E is the word embedding matrix , and S is the segment embedding matrix .
Finally , we add e and s as the input of the encoder .
Unified Flat Encoder Given the document context , the input sequences of Flat- Transformer are much longer than the standard Transformer , which brings additional challenges .
First , the attention is distracted , and its weights become much smaller after the normalization operation .
Second , the memory consumption and the computation cost increase , so it is difficult to enlarge the model size , which hinders the adaptation to the pre-training model .
To address this problem , we introduce a unified flat encoder .
As shown in Figure 2 , at the bottom of the encoder blocks , we apply self-attention and the feed -forward layer to the concatenated sequence of the contexts and the current sentence : h 1 = Transformer ( e + s ; ? ) ( 4 ) where ? is the parameter of the Transformer blocks .
At the top of encoder blocks , each self-attention and feed -forward layer is only implemented on the position of the current sentences : h 2 = Transformer ( h 1 [ s : t ] ; ? ) ( 5 ) where s and t are the starting and ending positions of the source sentences in the concatenation sequence .
In this way , the attention can focus more on the current sentences , while the contexts are served as the supplemental semantics for the current sentences .
It is noted that the total number of the bottom blocks and the top blocks is equal to the number of standard Transformer 's blocks , so there is no more parameter than that of the standard Transformer .
Training and Decoding
The training of Flat-Transformer is consistent with that of standard Transformer , using the cross entropy loss : L = ? n i=1 log P ( Y i | X i ) ( 6 )
At the decoding step , it translates the document sentence - by-sentence .
When translating each sentences , it predicts the target sequence with the highest probability given the current sentence x i and the surrounding contexts x < i , x >i : ?i = arg max y i ?V P ( y i |x i ; x < i ; x >i ) ( 7 )
Comparison with Existing Models Here , we summarize some significant differences compared with the existing models for documentlevel machine translation :
1 . Compared with the dual-encoder models , our model uses a unified encoder .
To combine the representation of two encoders for the decoder , these dual-encoder models should add a layer inside the encoders .
Flat- Transformer does not put any layer on top of the standard Transformer , so it is consistent with the recent pre-training models .
2 . Compared with the previous uni-encoder models , our model limits the top transformer layers to only model the source sentences .
In this way , our model has an inductive bias of modeling on more current sentences than the contexts , because the translation is more related to the current sentences .
3 . There are also some alternative approaches to limit the use of context vectors .
For example , we can limit only the top attention layers to attend to the source sentence while keeping the feed -forward layers the same .
Compared with this approach , our model does not feed the output vectors of the context encoder to the decoder , so that the decoder attention is not distracted by the contexts .
The context vectors in our model is only to help encode a better representation for current source sentences .
Dataset
Experiments
We evaluate the proposed model and several stateof - the - art models on three document - level machine translation benchmarks .
We denote the proposed model as Flat-Transformer .
Datasets
Following the previous work ( Maruf et al. , 2019 ) , we use three English - German datasets as the benchmark datasets , which are TED , News , and Europarl .
The statistic of these datasets can be found in Table 1 .
We obtain the processed datasets from Maruf et al . ( 2019 ) 2 , so that our results can be compared with theirs reported in Maruf et al . ( 2019 ) .
We use the scripts of Moses toolkit 3 to tokenize the sentences .
We also split the words into subword units ( Sennrich et al. , 2016 ) with 30 K mergeoperations .
The evaluation metrics are BLEU ( Papineni et al. , 2002 ) and Meteor ( Banerjee and Lavie , 2005 ) .
Implementation Details
The batch size is limited to 4 , 000 tokens for all models .
We set the hidden units of the multi-head component and the feed -forward layer as 512 and 1024 .
The embedding size is 512 , the number of heads is 4 , and the dropout rate ( Srivastava et al. , 2014 ) for the top encoder is 5 , while that for the bottom encoder is 1 .
When fine-tuning on the pre-training BERT , we adopt the base setting , and the hidden size , the feed -forward dimension , and the number of heads are 768 , 3072 , 12 .
To balance the accuracy and the computation cost , we use one previous sentence and one next sentence as the surrounding contexts .
We use the Adam ( Kingma and Ba , 2014 ) optimizer to train the models .
For the hyper-parameters of Adam optimizer , we set two momentum parameters ?
1 = 0.9 and ? 2 = 0.98 , and = 1 ? 10 ?8 .
The learning rate linearly increases from 0 to 5 ? 10 ?4 for the first 4 , 000 warming - up steps and then decreases proportional to the inverse square root of the update numbers .
We also apply label smoothing to the cross-entropy loss , and the smoothing rate is 0.1 .
We implement the early stopping mechanism with patience that the loss on the validation set does not fall in 10 epochs .
Baselines
We compare our models with two categories of baseline models : the dual-encoder models and the uni-encoder models .
Uni-encoder : RNNSearch ( Bahdanau et al. , 2015 ) is an RNN - based sequence - to-sequence model with the attention mechanism .
Transformer ( Vaswani et al. , 2017 ) is a popular model for machine translation , based solely on attention mechanisms .
For a fair comparison , we use the same hyper-parameters as our model's , which is described in Section 3.2 .
Dual-encoder : Zhang et al. ( 2018 ) extends the Transformer model with a new context encoder to represent the contexts .
HAN ( Werlen et al. , 2018 ) is the first to use a hierarchical attention model to capture the context in a structured and dynamic manner .
SAN ( Maruf et al. , 2019 ) proposes a new selective attention model that uses sparse attention to focus on relevant sentences in the document context .
QCN proposes a query - guided capsule networks to cluster context information into different perspectives .
Results
We compare our Flat-Transformer model with the above baselines .
Table 2 summarizes the results of these models .
It shows that our Flat-Transformer can obtain scores of 24.87/23.55/30.09 on three datasets in terms of BLEU , and 47.05/43.97/48.56 in terms of METEOR , which significantly outperforms the previous flat models ( RNNSearch and Transformer ) .
By fine-tuning on BERT , Flat-Transformer can achieve improvements of + 1.74/+0.97/+1.90 BLEU scores as well as + 1.48/+1.43/+1.20 ME-TEOR scores .
It proves that Flat-Transformer can be compatible with the pre-training BERT model .
Except for the BLEU score on the News dataset , the Flat-Transformer can significantly outperform the dual-encoder models , achieving state - of - the - art performance in terms of both BLEU and ME-TEOR scores .
On the contrary , the dual-encoder Transformer is not compatible with BERT .
It gets slightly worse performance on two datasets , mainly because the model size becomes larger to adapt the setting of BERT .
Still , BERT does not provide a good prior initialization for modeling the uni-directional relationship from contexts to source sentences .
Ablation Study
To analyze the effect of each component of Flat- Transformer , we conduct an ablation study by removing them from our models on the TED dataset .
Table 3 summarizes the results of the ablation study .
We remove the segment embedding but reserve the unified structure .
It concludes that the segment embedding contributes to an improvement of 0.51 BLEU score and 0.85 METEOR score , showing the importance of explicitly identifying the contexts and the source sentences .
After further removing the unified structure of Flat-Transformer , the model becomes a standard Transformer .
It shows that the unified structures contribute a gain of 1.08 in terms of BLEU and 2.03 in terms of METEOR .
The reason is that the unified structures encourage the model to focus more on the source sentences , while the contexts can be regarded as the semantic supplements .
Related Work
Here we summarize the recent advances in document- level neural machine translation .
Some work focuses on improving the architectures of the document machine translation models .
Tiedemann and Scherrer ( 2017 ) and Wang et al . ( 2017 ) explore possible solutions to exploit the crosssentence contexts for neural machine translation .
2018 ) and ( Maruf et al. , 2019 ) propose two different hierarchical attention models to model the contexts .
introduces a capsule network to improve these hierarchical structures .
There are also some works analyzing the contextual errors ( Voita et al. , , 2019 b Bawden et al. , 2018 ) and providing the test suites ( M?ller et al. , 2018 ) .
More recently , Voita et al. ( 2019a ) explores the approaches to incorporate the mono-lingual data to augment the document- level bi-lingual dataset .
Different from these works , this paper mainly discusses the comparison between dual-encoder models and uniencoder models and proposes a novel method to improve the uni-encoder structure .
Conclusions
In this work , we explore the solutions to improve the uni-encoder structures for document - level machine translation .
We propose a Flat- Transformer model with a unified encoder , which is simple and can model the bi-directional relationship between the contexts and the source sentences .
Besides , our Flat-Transformer is compatible with the pretraining model , yielding a better performance than both the existing uni-encoder models and the dualencoder models on two datasets .
Figure 1 : 1 Figure 1 : The overview of the dual-encoder structure and the uni-encoder structure for document - level machine translation .
extends the Transformer model with a new context encoder to represent documentlevel context .
Werlen et al . (
Table 1 : 1 Statistics of three document- level machine translation datasets .
# Sent Avg . # Sent TED 0.21M/9K/2.3K 121/96/99 News 0.24M/2K/3K 39/27/19 Europarl
1.67M/3.6K/5.1K 14/15/14
Table 2 : 2 is 0.3 .
The number of Transformer blocks Results on three document- level machine translation benchmarks ( " Dual " denotes dual-encoder , while " Uni " means uni-encoder ) .
Model TED BLEU METR BLEU METR BLEU METR News Europarl HAN ( Werlen et al. , 2018 ) 24.58 45.48 25.03 44.02 29.58 46.91 SAN ( Maruf et al. , 2019 ) 24.62 45.32 24.84 44.27 29.90 47.11 Dual QCN ( Yang et al. , 2019 ) 25.19 45.91 22.37 41.88 29.82 47.86
Transformer ( Zhang et al. , 2018 ) 24.01 45.30 22.42 42.30 29.93 48.16 + BERT 23.19 45.25 22.06 42.25 30.72 48.62 RNN ( Bahdanau et al. , 2015 ) 19.24 40.81 16.51 36.79 26.26 44.14
Uni Transformer ( Vaswani et al. , 2017 ) 23.28 Our Flat-Transformer 24.87 44.17 47.05 22.78 23.55 42.19 43.97 28.72 30.09 46.22 48.56 + BERT 26.61 48.53 24.52 45.40 31.99 49.76 TED BLEU METEOR Flat-Transformer 24.87 47.05 w/o Segment 24.36 46.20 w/o Unified 23.28 44.17
Table 3 : 3 Ablation study on the TED dataset .
https://github.com/sameenmaruf/selective-attn 3 https://github.com/moses-smt/mosesdecoder
