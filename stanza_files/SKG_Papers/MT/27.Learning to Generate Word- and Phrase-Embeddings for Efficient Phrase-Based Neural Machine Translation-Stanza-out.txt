title
Learning to Generate Word- and Phrase-Embeddings for Efficient Phrase - Based Neural Machine Translation
abstract
Neural machine translation ( NMT ) often fails in one- to- many translation , e.g. , in the translation of multi-word expressions , compounds , and collocations .
To improve the translation of phrases , phrase - based NMT systems have been proposed ; these typically combine wordbased NMT with external phrase dictionaries or with phrase tables from phrase - based statistical MT systems .
These solutions introduce a significant overhead of additional resources and computational costs .
In this paper , we introduce a phrase - based NMT model built upon continuous - output NMT , in which the decoder generates embeddings of words or phrases .
The model uses a fertility module , which guides the decoder to generate embeddings of sequences of varying lengths .
We show that our model learns to translate phrases better , performing on par with state of the art phrase - based NMT .
Since our model does not resort to softmax computation over a huge vocabulary of phrases , its training time is about 112x faster than the baseline .
Introduction Despite the successes of neural machine translation ( Wu et al. , 2016 ; Vaswani et al. , 2017 ; Ahmed et al. , 2018 ) , state of the art NMT systems are still challenged by translation of typologically divergent language pairs , especially when languages are morphologically rich ( Burlot and Yvon , 2017 ) .
One of the reasons lies in increased sparsity of word types , which leads to the demand for ( often unavailable ) significantly larger training corpora ( Koehn and Knowles , 2017 ) .
Another reason is an implicit assumption of sequence to sequence ( seq2seq ) models that input sequences are translated into a target language word - by - word or subword - by- subword ( Sennrich et al. , 2016 ) .
This is not the case for typologically divergent language pairs , for example when translat-ing into English from agglutinative languages with high rates of morphemes per word ( e.g. , Turkish and Quechua ) or languages with productive compounding processes like German or Finnish ( Matthews et al. , 2016 ) .
Another ubiquitous source of one- to-many correspondences is a translation of idiomatic phrases and multi-word expressions ( Rikters and Bojar , 2017 ) .
While outperformed by NMT overall , translation models in traditional statistical phrase - based approaches ( Koehn , 2009 , SMT ) provide an inventory of phrase translations , which can be used to address the above challenges .
To combine the benefits of NMT and phrase - based SMT , phrasebased NMT systems have been proposed ( Huang et al. , 2017 ; Lample et al. , 2018 ) which combine word - based NMT with external phrase memories ( Tang et al. , 2016 ; Dahlmann et al. , 2017 ) .
However , prior approaches to phrase - based NMT introduced a significant overhead of additional resources and computation .
We introduce a phrase - based continuous - output NMT ( PCoNMT ) model built upon continuousoutput NMT ( Kumar and Tsvetkov , 2019 ) , in which the decoder generates embeddings of words or phrases ( ?2 ) .
The model extracts phrases in the target language from one - to -many word alignments and pre-computes word and phrase embeddings which constitute the output space of our model ( ?2.2 ) .
A fertility module guides the decoder , providing the probability of generating a word or a phrase at each time step ( ?2.3 ) .
Experimental results show that the proposed model outperforms the conventional attention - based NMT systems ( Bahdanau et al. , 2014 ) by up to 4.8 BLEU , and the baseline continuous - output models by up to 1.6 BLEU , and beat the state - of- theart phrase - based NMT system in translation from German and Turkish into English .
Since our model does not resort to softmax computation over a huge vocabulary , it also maintains the computational efficiency of continuousoutput NMT , even with additional ngram embedding tables , and is faster than the state - of - the - art baseline by 112x ( ?3 ) , making our models energyefficient ( Strubell et al. , 2019 ) .
The key contributions of our work are twofold : ( 1 ) we develop a phrase - based NMT model that outperforms existing baselines and better translates phrases , while ( 2 ) maintaining the computational efficiency of NMT end-to - end approaches .
1 2 Phrase- based Continuous-output NMT 2.1
Embedding output layer Kumar and Tsvetkov ( 2019 ) introduced continuous - output machine translation ( CoNMT ) which replaces the softmax layer in the conventional seq2seq models with a continuous embeddings layer .
The model predicts the embedding of the target word instead of its probability .
It is trained to maximize the von Mises - Fisher ( vMF ) probability density of the pretrained targetlanguage embeddings given the embeddings predicted by the model at every step ; at inference time , predicted embedding is compared to the embeddings in the pre-trained embedding table , and the closest embedding is selected as an output word .
While maintaining the translation quality of traditional seq2seq approaches , CoNMT approach alleviates the computational bottleneck of the softmax layer : it is substantially more efficient to train and the models are more compact , without limiting the output vocabulary size .
Extending the CoNMT approach , we propose phrase - based continuous - output NMT ( PCoNMT ) .
As depicted in Figure 1 , we augment the original model with ( 1 ) additional embedding tables for phrases , and ( 2 ) a fertility module that guides the choice of embedding table to look - up in ( described in ?2.3 ) .
Having additional large embedding tables , which significantly increase the vocabulary size , could be a considerable overhead to a word - based model with the softmax layer .
However , since we generate embeddings in the final layer and do not resort to the softmax computation , our models maintain the computational efficiency of continuous - output models ( ?3 ) during the training time .
At inference time , the only overhead incurred by our model is getting another set of vMF scores for the phrase embedding table for each output step .
This is almost negligible compared to the computation of the entire network .
In addition to efficiency benefits , since the PCoNMT approach enables us to pre-compute embeddings of less frequent phrases and phrases that do not have a literal translation , e.g. , multiword expressions , it facilitates better translations specifically where the translation is notoriously challenging for NMT .
Output embedding tables
To construct embedding tables for target - language phrases , we first extract the list of output phrases from parallel corpora .
Following Tang et al. ( 2016 ) , in this work , we focus on one- to -many word alignments in the training corpus .
Consider as an example translation of German com- pounds to English , e.g. , Lebensqualit ? t in German is translated as quality of life .
We extract all such one- to -many word alignments from the parallel corpora using Fastalign ( Dyer et al. , 2013 ) .
There are several standard approaches to extract meaningful phrases from a monolingual corpus , such as using scores like pointwise mutual information ( PMI ) ( Mikolov et al. , 2013 ) .
However , for our model , we utilize word-alignment results to construct a phrase list since we are particularly interested in multi-word translation cases .
Note that with this approach , phrases in target - side embedding tables can be different depending on which language pair and which corpus are being used .
s 1 e 1 s 2 ? Decoder o 1 y 1 Attn s word y 1 gen ?
1 < s > Word Table Phrase Table s 1 x 1 x 3 x 2 h 3 h 2 h 1 ? ? Encoder x n h n ?
e , After extracting all noisy one- to-many alignments from the parallel corpus , we filter our phrase list in order to keep only the useful phrases and to remove potential erroneous phrases coming from alignment errors .
We filter according to the following heuristics : ( 1 ) a phrase should appear at least twice in the parallel corpus ; ( 2 ) it should not contain any punctuation ; ( 3 ) PMI of the phrase should be positive ; ( 4 ) a bigram phrase should not repeat the same word ; and ( 5 ) the phrase should not contain only stopwords .
We train embeddings for the resulting list of words and phrases as follows .
First , we preprocess the target language 's large monolingual corpus to concatenate words to match the longest phrase in the extracted phrase list .
For example , the sen-tence ' I went to a graduate school ' will be converted into ' I went to a graduate school ' if we have went to and graduate school in our phrase list .
This concatenated corpus is then used to train fastText ( Peters et al. , 2018 ) embeddings for both phrases and words simultaneously .
We use fast - Text because it encodes subword - level information which may provide a signal about each word in a phrase .
From this training , we obtain both the word - and phrase-tables , which are of the same dimension .
Fertility module
We introduce a fertility module , similar to the fertility concept in SMT ( Brown et al. , 1993 ) .
The fertility indicates how many target words each source word should produce in the output .
The SMT models keep the fertility probabilities over fertility count , typically from zero to four , and use it to produce probability over words .
We integrate this fertility concept into our PCoNMT model .
Our fertility module predicts the fertility probability ? e = [? e0 , ... , ? eN ] , where ?
ei indicates the scalar probability of the source word at position e being translated into i words .
This is predicted based on the word embedding and encoder 's output of the word : ? e = FFNN ( x e ; h e ) .
FFNN is the feed -forward neural network , and ( x e ; h e ) denotes the concatenation of x e and h e , which are embedding and encoder 's hidden state of eth source word , respectively .
The dimension of fertility vector ? , N , can be arbitrarily large , but in this paper we explore two different variants ; the first one is Fertility 4 where each dimension corresponds to zero to three words to produce respectively ( N ? { 0 , 1 , 2 , 3 } ) , and the second one is Fertility 2 which simplifies the fertility prediction into binary classification by setting N =1 as a cut-off point , i.e. , whether the model should generate a word ( N ? 1 ) or a phrase ( N > 1 ) .
Therefore , ? e becomes a four-dimensional vector of [ ?
e0 , ? e1 , ? e2 , ? e3 ] for Fertility 4 , and twodimensional vector of [ 1 n=0 ? en , ? n=2 ? en ] for Fertility 2 .
At decoding time , we combine this fertility probability of each source word and the attention to guide the decoder to generate a phrase or a word .
To get the probability of producing a word ?
d , word for timestep d , we use attention given to each source word as a weight to its fertility probability and sum over the entire source sentence : we freeze those parameters , and separately train parameters of the fertility module .
2 During the preprocessing , we extract the actual fertility value for each source word using the word-alignment model and the filtered phrase list , then set it as a gold label for the fertility prediction training .
? d, word = e a d,e ( ? e0 + ? e1 ) ( dim = 4 ) e a d,e [ ?
e ] 0 ( dim = 2 ) ? d, phrase = 1 ? ? d,
Experiments In ( 4 % ) , respectively .
Also note that following Kumar and Tsvetkov ( 2019 ) , in this paper , we only used greedy decoding .
We compared our proposed model with three baselines : ( 1 ) Attn : Standard attention - based NMT model as in Wiseman and Rush ( 2016 ) ; ( 2 ) CoNMT : RNN - based Continuous-output NMT systems ( Kumar and Tsvetkov , 2019 ) ; ( 3 ) NPMT :
The state of the art phrase - based NMT model proposed by Huang et al . ( 2017 ) .
For NPMT , we ran its released code with the same preprocessed data we are using without changing any hyperparameters they set .
3 For both De- En and Tr- En CoNMT models , we used the best hyperparameter settings reported by Kumar and Tsvetkov ( 2019 ) for De-En .
For our model , PCoNMT , we only changed the batch size from the original setting in CoNMT and chose other additional parameters based on the performance on the validation set .
Although we use recurrent architectures in this paper to make our findings comparable to prior work that uses the same setting , we believe using multi-layer self-attention mechanism ( Vaswani et al. , 2017 ) as a base of our model has further potential to improve the performance .
Even with Transformers , the conventional tokenby - token generation scheme will be still prone to mistakes in multi-word generations .
Therefore , explicitly handling the phrase generation as we propose is likely to be helpful , which we leave it as future work .
Translation quality De- En and Tr- En translation results are summarized in Tables 1 and 2 . PCoNMT significantly outperforms both the conventional attention - based model ( by >4 BLEU ) and its base CoNMT model ( by 1.6 BLEU ) , and also performs better than NPMT ( by 1.4 BLEU ) .
The fertility module is shown to be relatively more helpful in Tr- En task , while showing less impact in De- En task .
We also observed that Fertility 2 consistently generates better translations than Fertility 4 .
On the more difficult MWT subset containing multi-word phrases , PCoNMT obtains large absolute gains in BLEU , confirming their effectiveness in phrase translations .
Examples of translations are shown in Table 7 .
Computational efficiency
We report the training efficiency of models in three metrics : speed , number of training epochs till convergence , and total training time .
All results were measured on the same machine with the same batch size .
The machine was a single-node local machine with NVIDIA GTX 1080 Ti. During the training , no other process was executed except for the training for the fair comparison .
Table 3 shows that CoNMT and PCoNMT can process 28 times faster than NPMT , and converge six times faster , i.e. , reducing the entire training time by 112x .
Somewhat surprisingly , PCoNMT further accelerates the CoNMT as it can reduce the timestep needed for a sample by generating phrases .
This result proves that additional phrase embeddings of PCoNMT has little impact on computational efficiency while training .
Fertility Prediction Evaluation
The fertility prediction can have a significant impact on the translation as it guides the decoder to decide when to generate phrases and when to generate words .
We evaluate the prediction results on the test set with the gold label obtained from the word alignment model in Table 5 and Table 4 .
In both datasets , we observe that the data is highly skewed toward word - level classes as most translations are word-to - word generation .
This results in Fertility 4 not to predict N = 3 classes at all in the Tr- En dataset .
The comparison between Table 5 and Table 4 shows that the Fertility 2 has slighly higher F- 1 score than Fertility 4 in both datasets .
It implies that aggregating the classes into two made the prediction task easier for the model , which thus led to the improved translation quality shown in the previous results .
Analysis on Generated Phrases
Table 6 presents further analysis of the generated phrases .
We first see in which category of phrases our model performs well compared to the baseline , CoNMT , to know from where the improvement of our model is coming .
As for the phrase categories , we consider three categories , compound nouns ( CNs , e.g. , thought experiment ) , verb phrases ( VPs , e.g. , grow apart ) , and collocations ( COs , e.g. , at risk ) .
We randomly sampled a hundred generated phrases from the De- En test set , and manually annotate the category of phrases and whether it is the correct translation .
We also look at the output of CoNMT baseline for the same test samples , and also annotate if the sampled phrases are well translated in the CoNMT output .
The results in Table 6 show that the most frequently generated phrases are collocations ( 56 % ) followed by verb phrases ( 28 % ) and compound nouns ( 16 % ) .
Among the entire sampled phrases , 64 percent of phrases were correct in PCoNMT output while CoNMT had 50 percent of them correct .
Specifically , our model significantly outperformed the baseline in compound word generation cases while performs worse in verb phrases generation .
By looking into the instances of wrong verb phrase generation , we found that a significant amount of those errors are related to the tense of the verb .
Related Work Multi-word Expressions for NMT
There have been several studies that incorporate multi-word phrases into supervised NMT ( Tang et al. , 2016 ; Dahlmann et al. , 2017 ) .
Most approaches rely on pre-defined phrase dictionaries obtained from methods such as phrase - based Stastitical MT ( Koehn et al. , 2003 )
Recent works have also explored using an additional RNN to compute phrase generation probabilities .
Huang et al. ( 2017 ) proposed Neural Phrase MT ( NPMT ) that is built upon Sleep - WAke Network ( SWAN ) , a segmentation - based sequence modeling technique , which automatically discovers phrases given the data and appends the special symbol $ to the source and target data .
The model gets these segmented word / phrase sequences as input and keeps two levels of RNNs to encode and decode phrases .
NPMT established state of the art results for phrase - based NMT , but at a price of significant computational overhead .
The main differences between previous studies and our work are : ( 1 ) we do not rely on SMT model and adapt in an end-to - end manner only requiring some preprocessing using word- alignment models ; and ( 2 ) we use phrase embedding tables to represent phrases instead of keeping external phrase memory and its generation probability .
By using the phrase embeddings along with the continuous - output layer , we significantly reduce the computational complexity and propose an approach to overcome the phrase generation bottleneck .
Fertility in MT Fertility ( Brown et al. , 1993 ) has been a core component in phrase - based SMT models ( Koehn et al. , 2003 ) .
Fertility gives the likelihood of each source word of being translated into n words .
Fertility helps in deciding which phrases should be stored in the phrase tables .
Tu et al. ( 2016 ) revisited fertility to model coverage in NMT to address the issue of under-translation .
They used a fertility vector to express how many words should be generated per source word and a coverage vector to keep track of words translated so far .
We use a very similar concept in this work but the fertility module is introduced with a purpose to guide the decoder to switch over generating phrases and words .
Conclusion
We proposed PCoNMT , a phrase - based NMT system built upon continuous - output NMT models .
We also introduced a fertility module that guides the decoder by providing the probabilities of generating a phrase and a word by leveraging the attention mechanism .
Our experimental results showed that our model outperforms the state of the art phrase NMT systems , and also speeds up the computation by 112x .
Figure 1 : 1 Figure 1 : Phrase - based neural machine translation architectures generate word - and phrase embeddings at each step of decoding .
The PCoNMT models are guided by on the fertility prediction and the attention .
