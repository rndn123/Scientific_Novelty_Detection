title
From Speech-to - Speech Translation to Automatic Dubbing
abstract
We present enhancements to a speech-tospeech translation pipeline in order to perform automatic dubbing .
Our architecture features neural machine translation generating output of preferred length , prosodic alignment of the translation with the original speech segments , neural text - to- speech with fine tuning of the duration of each utterance , and , finally , audio rendering to enriches text - to- speech output with background noise and reverberation extracted from the original audio .
We report and discuss results of a first subjective evaluation of automatic dubbing of excerpts of TED Talks from English into Italian , which measures the perceived naturalness of automatic dubbing and the relative importance of each proposed enhancement .
Introduction Automatic dubbing can be regarded as an extension of the speech - to-speech translation ( STST ) task ( Wahlster , 2013 ) , which is generally seen as the combination of three sub-tasks : ( i ) transcribing speech to text in a source language ( ASR ) , ( ii ) translating text from a source to a target language ( MT ) and ( iii ) generating speech from text in a target language ( TTS ) .
Independently from the implementation approach ( Weiss et al. , 2017 ; Waibel , 1996 ; Vidal , 1997 ; Metze et al. , 2002 ; Nakamura et al. , 2006 ; Casacuberta et al. , 2008 ) , the main goal of STST is producing an output that reflects the linguistic content of the original sentence .
On the other hand , automatic dubbing aims to replace all speech contained in a video document with speech in a different language , so that the result sounds and looks as natural as the original .
Hence , in addition to conveying the same content of the original utterance , dubbing should also match the * * Contribution while the author was with Amazon .
original timbre , emotion , duration , prosody , background noise , and reverberation .
While STST has been addressed for long time and by several research labs ( Waibel , 1996 ; Vidal , 1997 ; Metze et al. , 2002 ; Nakamura et al. , 2006 ; Wahlster , 2013 ) , relatively less and more sparse efforts have been devoted to automatic dubbing ( Matousek et al. , 2010 ; Matousek and V?t , 2012 ; Furukawa et al. , 2016 ; ?ktem et al. , 2019 ) , although the potential demand of such technology could be huge .
In fact , multimedia content created and put online has been growing at exponential rate , in the last decade , while availability and cost of human skills for subtitling and dubbing still remains a barrier for its diffusion worldwide .
1 Professional dubbing ( Mart?nez , 2004 ) of a video file is a very labor intensive process that involves many steps : ( i ) extracting speech segments from the audio track and annotating these with speaker information ; ( ii ) transcribing the speech segments , ( iii ) translating the transcript in the target language , ( iv ) adapting the translation for timing , ( v ) casting the voice talents , ( vi ) performing the dubbing sessions , ( vii ) fine - aligning the dubbed speech segments , ( viii ) mixing the new voice tracks within the original soundtrack .
Automatic dubbing has been addressed both in monolingual cross-lingual settings .
In ( Verhelst , 1997 ) , synchronization of two speech signals with the same content was tackled with time - alignment via dynamic time warping .
In ( Hanzl ? cek et al. , 2008 ) automatic monolingual dubbing for TV users with special needs was generated from subtitles .
However , due to the poor correlation between length and timing of the subtitles , TTS output fre-quently broke the timing boundaries .
To avoid unnatural time compression of TTS 's voice when fitting its duration to the duration of the original speech , ( Matousek et al. , 2010 ) proposed phonedependent time compression and text simplification to shorten the subtitles , while ( Matousek and V?t , 2012 ) leveraged scene-change detection to relax the subtitle time boundaries .
Regarding crosslingual dubbing , lip movements synchronization was tackled in ( Furukawa et al. , 2016 ) by directly modifying the actor 's mouth motion via shuffling of the actor 's video frames .
While the method does not use any prior linguistic or phonetic knowledge , it has been only demonstrated on very simple and controlled conditions .
Finally , mostly related to our contribution is ( ? ktem et al. , 2019 ) , which discusses speech synchronization at the phrase level ( prosodic alignment ) for Englishto - Spanish automatic dubbing .
In this paper we present research work to enhance a STST pipeline in order to comply with the timing and rendering requirements posed by cross-lingual automatic dubbing of TED Talk videos .
Similarly to ( Matousek et al. , 2010 ) , we also shorten the TTS script by directly modifying the MT engine rather than via text simplification .
As in ( ? ktem et al. , 2019 ) , we synchronize phrases across languages , but follow a fluencybased rather than content - based criterion and replace generation and rescoring of hypotheses in ( ? ktem et al. , 2019 ) with a more efficient dynamic programming solution .
Moreover , we extend ( ? ktem et al. , 2019 ) by enhancing neural MT and neural TTS to improve speech synchronization , and by performing audio rendering on the dubbed speech to make it sound more real inside the video .
In the following sections , we introduce the overall architecture ( Section 2 ) and the proposed enhancements ( Sections 3 - 6 ) .
Then , we present results ( Section 7 ) of experiments evaluating the naturalness of automatic dubbing of TED Talk clips from English into Italian .
To our knowledge , this is the first work on automatic dubbing that integrates enhanced deep learning models for MT , TTS and audio rendering , and evaluates them on real-world videos .
Automatic Dubbing
With some approximation , we consider here automatic dubbing of the audio track of a video as the task of STST , i.e. ASR + MT + TTS , with the additional requirement that the output must be temporally , prosodically and acoustically close to the original audio .
We investigate an architecture ( see Figure 1 ) that enhances the STST pipeline with ( i ) enhanced MT able to generate translations of variable lengths , ( ii ) a prosodic alignment module that temporally aligns the MT output with the speech segments in the original audio , ( iii ) enhanced TTS to accurately control the duration of each produce utterance , and , finally , ( iv ) audio rendering that adds to the TTS output background noise and reverberation extracted from the original audio .
In the following , we describe each component in detail , with the exception of ASR , for which we use ( Di Gangi et al. , 2019a ) an of- the-shelf online service 2 .
Machine Translation
Our approach to control the length of MT output is inspired by target forcing in multilingual neural MT ( Johnson et al. , 2017 ; Ha et al. , 2016 ) .
We partition the training sentence pairs into three groups ( short , normal , long ) according to the target / source string - length ratio .
In practice , we select two thresholds t 1 and t 2 , and partition training data according to the length- ratio intervals [ 0 , t 1 ) , [ t 1 , t 2 ) and [ t 2 , ?] .
At training time a length token is prepended to each source sentence according to its group , in order to let the neural MT model discriminate between the groups .
At inference time , the length token is instead prepended to bias the model to generate a translation of the desired length type .
We trained a Transformer model ( Vaswani et al. , 2017 ) with output length control on web crawled and proprietary data amounting to 150 million English -Italian sentence pairs ( with no overlap with the test data ) .
The model has encoder and decoder with 6 layers , layer size of 1024 , hidden size of 4096 on feed forward layers , and 16 heads in the multi-head attention .
For the reported experiments , we trained the models with thresholds t 1 = 0.95 and t 2 = 1.05 and generated at inference time translations of the shortest type , resulting , on our test set , in an average length ratio of 0.97 .
A reason for the length exceeding the threshold could be that for part of test data the model did not learn ways to keep the output short .
A detailed account of the approach , the followed training procedure and experimental results on the same task of this paper , but using slightly different thresholds , can be found in ( Lakew et al. , 2019 ) .
The paper also shows that human evaluation conducted on the short translations resulted in a minor loss in quality with respect to the model without output length control .
Finally , as baseline MT system for our evaluation experiments we used an online service 3
Prosodic Alignment Prosodic alignment ( ? ktem et al. , 2019 ) is the problem of segmenting the target sentence to optimally match the distribution of words and pauses 4 . Let e = e 1 , e 2 , . . . , e n be a source sentence of n words which is segmented according to k breakpoints 1 ? i 1 < i 2 < . . .
i k = n , shortly denoted with i. Given a target sentence f = f 1 , f 2 , . . . , f m of m words , the goal is to find within it k corresponding breakpoints 1 ? j 1 < j 2 < . . . j k = m ( shortly denoted with j ) that maximize the probability : max j log Pr( j i , e , f ) ( 1 )
By assuming a Markovian dependency on j , i.e. : Pr(j i , e , f ) = k t=1 log Pr( j t j t?1 ; t , i , e , f ) ( 2 ) and omitting from the notation the constant terms i , e , f , we can derive the following recurrent quantity : Q( j , t ) = max j ? <j log Pr( j j ? ; t ) + Q ( j ? , t ? 1 ) ( 3 ) where Q( j , t ) denotes the log-probability of the optimal segmentation of f up to position j with t break points .
It is easy to show that the solution of ( 1 ) corresponds to Q( m , k ) and that optimal segmentation can be efficiently computed via dynamic -programming .
Let ft = f j t?1 + 1 , . . . , f jt and ?t = e i t?1 + 1 , . . . , e it indicate the t-th segments of f and e , respectively , we define the conditional probability of the t-th break point in f by : Pr( j t j t?1 , t ) ? exp 1 ? d(? t ) ? d( ft ) d( ?
t ) ?
Pr( br j t , f ) ( 4 )
The first term computes the relative match in duration between the corresponding t-th segments 5 , while the second term measure the linguistic plausibility of a placing a break after the j t in f .
For this , we simply compute the following ratio of normalized language model probabilities of text windows centered on the break point , by assuming or not the presence of a pause ( br ) in the middle : Pr( br j , f ) = Pr(f j , br , f j +1 ) 1 3 Pr(f j , br , f j +1 ) 1 3 + Pr(f j , f j +1 ) 1 2 The rational of our model is that we want to favor split points were also TTS was trained to produce pauses .
TTS was in fact trained on read speech that generally introduces pauses in correspondence of punctuation marks such as period , comma , semicolon , colon , etc .
Notice that our interest , at the moment , is to produce fluent TTS speech , not to closely match the speaking style of the original speaker .
In our implementation , we use a larger text window ( last and first two words ) , we replace words with parts - of speech , and estimate the language model with KenLM ( Heafield , 2011 ) on the training portion of the MUST - C corpus tagged with parts - of-speech using an online service 6 .
Text To Speech
Our neural TTS system consists of two modules : a Context Generation module , which generates a context sequence from the input text , and a Neural Vocoder module , which converts the context sequence into a speech waveform .
The first one is an attention - based sequence - to-sequence network ( Prateek et al. , 2019 ; ) that predicts a Mel-spectrogram given an input text .
A grapheme- to- phoneme module converts the sequence of words into a sequence of phonemes plus augmented features like punctuation marks and prosody related features derived from the text ( e.g. lexical stress ) .
For the Context Generation module , we trained speaker - dependent models on two Italian voices , male and female , with 10 and 37 hours of high quality recordings , respectively .
We use the Universal Neural Vocoder introduced in , pre-trained with 2000 utterances per each of the 74 voices from a proprietary database .
To ensure close matching of the duration of Italian TTS output with timing information extracted from the original English audio , for each utterance we re-size the generated Mel spectrogram using spline interpolation prior to running the Neural Vocoder .
We empirically observed that this method produces speech of better quality than traditional time -stretching .
6 Audio Rendering
Foreground -Background Separation
The input audio can be seen as a mixture of foreground ( speech ) and background ( everything else ) and our goal is to extract the background and add it to the dubbed speech to make it sound more real and similar to the original .
Notice that in the case of TED talks , background noise is mainly coming from the audience ( claps and laughs ) but sometime also from the speaker , e.g. when she is explaining some functioning equipment .
For the foreground - background separation task , we adapted ( Giri et al. , 2019 ; Tolooshams et al. , 2020 ) the popular U- Net ( Ronneberger et al. , 2015 ) architecture , which is described in detail in ( Jansson et al. , 2017 ) for a music-vocal separation task .
It consists of a series of down- sampling blocks , followed by one bottom convolutional layer , followed by a series of up-sampling blocks with skip connections from the down -sampling to the upsampling blocks .
Because of the down- sampling blocks , the model can compute a number of highlevel features on coarser time scales , which are concatenated with the local , high- resolution features computed from the same - level up-sampling block .
This concatenation results into multiscale features for prediction .
The model operates on a time -frequency representation ( spectrograms ) of the audio mixture and it outputs two soft ratio masks corresponding to foreground and background , respectively , which are multiplied element - wise with the mixed spectrogram , to ob - tain the final estimates of the two sources .
Finally , the estimated spectrograms go through an inverse short-term Fourier transform block to produce raw time domain signals .
The loss function used to train the model is the sum of the L 1 losses between the target and the masked input spectrograms , for the foreground and the background ( Jansson et al. , 2017 ) , respectively .
The model is trained with the Adam optimizer on mixed audio provided with foreground and background ground truths .
Training data was created from 360 hours of clean speech from Librispeech ( foreground ) and 120 hours of recording taken from audioset ( Gemmeke et al. , 2017 ) ( background ) , from which speech was filtered out using a Voice Activity Detector ( VAD ) .
Foreground and background are mixed for different signal - to-noise ratio ( SNR ) , to generate the audio mixtures .
Re-reverberation
In this step , we estimate the environment reverberation from the original audio and apply it to the dubbed audio .
Unfortunately , estimating the room impulse response ( RIR ) from a reverberated signal requires solving an ill-posed blind deconvolution problem .
Hence , instead of estimating the RIR , we do a blind estimation of the reverberation time ( RT ) , which is commonly used to assess the amount of room reverberation or its effects .
The RT is defined as the time interval in which the energy of a steady -state sound field decays 60 dB below its initial level after switching off the excitation source .
In this work we use a Maximum Likelihood Estimation ( MLE ) based RT estimate ( see details of the method in ( L?llmann et al. , 2010 ) ) .
Estimated RT is then used to generate a synthetic RIR using a publicly available RIR generator ( Habets , 2006 ) .
This synthetic RIR is finally applied to the dubbed audio .
Experimental Evaluation
We evaluated our automatic dubbing architecture ( Figure 1 ) , by running perceptual evaluations in which users are asked to grade the naturalness of video clips dubbed with three configurations ( see Table 1 ) : ( A ) speech - to-speech translation baseline , ( B ) the baseline with enhanced MT and prosodic alignment , ( C ) the former system enhanced with audio rendering .
7 ? What is the overall naturalness of automatic dubbing ? ?
How does each introduced enhancement contribute to the naturalness of automatic dubbing ?
We adopt the MUSHRA ( MUlti Stimulus test with Hidden Reference and Anchor ) methodology ( MUSHRA , 2014 ) , originally designed to evaluate audio codecs and later also TTS .
We asked listeners to evaluate the naturalness of each versions of a video clip on a 0 - 100 scale .
Figure 2 shows the user interface .
In absence of a human dubbed version of each clip , we decided to use , for calibration purposes , the clip in the original language as hidden reference .
The clip versions to evaluate are not labeled and randomly ordered .
The observer has to play each version at least once before moving forward and can leave a comment about the worse version .
In order to limit randomness introduced by ASR and TTS across the clips and by MT across vergiven its very poor quality , as also reported in ( ? ktem et al. , 2019 ) .
Other intermediate configurations were not explored to limit the workload of the subjects participating in the experiment .
sions of the same clip , we decided to run the experiments using manual speech transcripts , 8 one TTS voice per gender , and MT output by the baseline ( A ) and enhanced MT system ( B - C ) of quality judged at least acceptable by an expert .
9
With these criteria in mind , we selected 24 video clips from 6 TED Talks ( 3 female and 3 male speakers , 5 clips per talk ) from the official test set of the MUST - C corpus ( Di Gangi et al. , 2019 b ) with the following criteria : duration of around 10 - 15 seconds , only one speaker talking , at least two sentences , speaker face mostly visible .
We involved in the experiment both Italian and non Italian listeners .
We recommended all participants to disregard the content and only focus on the naturalness of the output .
Our goal is to measure both language independent and language dependent naturalness , i.e. to verify how speech in the video resembles human speech with respect to acoustics and synchronization , and how intelligible it is to native listeners .
Results
We collected a total of 657 ratings by 14 volunteers , 5 Italian and 9 non-Italian listeners , spread over the 24 clips and three testing conditions .
We conducted a statistical analysis of the data with linear mixed - effects models using the lme4 package for R ( Bates et al. , 2015 ) .
We analyzed the naturalness score ( response variable ) against the following two -level fixed effects : dubbing system A vs. B , system A vs. C , and system B vs. C .
We run separate analysis for Italian and non-Italian listeners .
In our mixed models , listeners and video clips are random effects , as they represent a tiny sample of the respective true populations ( Bates et al. , 2015 ) .
We keep models maximal , i.e. with intercepts and slopes for each random effect , end remove terms required to avoid singularities .
Each model is fitted by maximum likelihood and significance of intercepts and slopes are computed via t-test .
Table 2 summarized our results .
In the first comparison , baseline ( A ) versus the system with enhanced MT and prosody alignment ( B ) , we see that both non-Italian and Italian listeners perceive a similar naturalness of system A ( 46.81 vs. 47.22 ) .
When moving to system B , non-Italian listeners perceive a small improvement ( + 1.14 ) , although not statistically significant , while Italian speaker perceive a statistically significant degradation ( - 10.93 ) .
In the comparison between B and C ( i.e. B enhanced with audio rendering ) , we see that non-Italian listeners observe a significant increase in naturalness ( + 10.34 ) , statistically significant , while Italian listeners perceive a smaller and not statistical significant improvement ( + 1.05 ) .
The final comparison between A and C gives almost consistent results with the previous two evaluations : non-Italian listeners perceive better quality in condition C ( + 11.01 ) , while Italian listeners perceive lower quality ( - 9.60 ) .
Both variations are however not statistically significant due to the higher standard errors of the slope estimates ?C.
Notice in fact that each mixed - effects model is trained on distinct data sets and with different random effect variables .
A closer look at the random effects parameters indeed shows that for the B vs .
C comparison , the standard deviation estimate of the listener intercept is 3.70 , while for the A vs .
C one it is 11.02 .
In other words , much higher variability across user scores is observed in the A vs .
C case rather than in the B vs .
C case .
A much smaller increase is instead observed across the video-clip random intercepts , i.e. from 11.80 to 12.66 .
The comments left by the Italian listeners tell that the main problem of system B is the unnaturalness of the speaking rate , i.e. is is either too slow , too fast , or too uneven .
The distributions of the MUSHRA scores presented at the top of Figure 3 confirm our analysis .
What is more relevant , the distribution of the rank order ( bottom ) strengths our previous analysis .
Italian listeners tend to rank system A the best system ( median 1.0 ) and vary their preference between systems B and C ( both with median 2.0 ) .
In contrast , non-Italian rank system A as the worse system ( median 2.5 ) , system B as the second ( median 2.0 ) , and statistically significantly prefer system C as the best system ( median 1.0 ) .
Hence , while our preliminary evaluation found that shorter MT output can potentially enable better synchronization , the combination of MT and prosodic alignment appears to be still problematic and prone to generate unnatural speech .
In other words , while non-Italian listeners seem to value synchronicity achieved through prosodic align - ment , Italian listeners seem to prefer trading synchronicity for more fluent speech .
We think that more work is needed to get MT closer to the script adaptation ( Chaume , 2004 ) style used for dubbing , and to improve the accuracy of prosodic alignment .
The incorporation of audio rendering ( system C ) significantly improves the experience of the non-Italian listeners ( 66 in median ) respect to systems B and C .
This points out the relevance of including para-linguist aspects ( i.e. applause 's , audience laughs in jokes , etc . ) and acoustic conditions ( i.e. reverberation , ambient noise , etc. ) .
For the target ( Italian ) listeners this improvement appears instead masked by the disfluencies introduced by the prosodic alignment step .
If we try to directly measure the relative gains given by audio rendering , we see that Italian listeners score system B better than system A 27 % of the times and system C better than A 31 % of the times , which is a 15 % relative gain .
On the contrary non-Italian speakers score B better than A 52 % of the times , and C better than A 66 % of the times , which is a 27 % relative gain .
Conclusions
We have perceptually evaluated the naturalness of automatic speech dubbing after enhancing a baseline speech - to-speech translation system with the possibility to control the verbosity of the translation output , to segment and synchronize the target words with the speech - pause structure of the source utterances , and to enrich TTS speech with ambient noise and reverberation extracted from the original audio .
We tested our system with both Italian and non-Italian listeners in order to evaluate both language independent and language dependent naturalness of dubbed videos .
Results show that while we succeeded at achieving synchronization at the phrasal level , our prosodic alignment step negatively impacts on the fluency and prosody of the generated language .
The impact of these disfluencies on native listeners seems to partially mask the effect of the audio rendering with background noise and reverberation , which instead results in a major increase of naturalness for non-Italian listeners .
Future work will be devoted to better adapt machine translation to the style used in dubbing and to improve the quality of prosodic alignment , by generating more accurate sentence segmentation and by introducing more flexible synchronization .
Figure 1 : 1 Figure 1 : Speech-to-speech translation pipeline ( dotted box ) with enhancements to perform automatic dubbing ( in bold ) .
Figure 2 : 2 Figure 2 : MUSHRA perceptual evaluation interface
Figure 3 : 3 Figure 3 : Boxplots with the MUSHRA scores ( top ) and Rank Order ( bottom ) per system and mother language ( Italian vs Non-Italian ) .
Table 1 : 1 Our evaluation fo-Evaluated dubbing conditions .
System Condition R Original recording ( reference ) A Speech-to-speech translation ( baseline ) B A with Enhanced MT and Pros.
Align .
C B with Audio Rendering
Table 2 : 2 Summary of the analysis of the evaluation with mixed -effects models .
From top down : A vs. B , B vs. C , A vs. C .
For each fixed effect , we report the estimate and standard error .
Symbols ? , * , + indicate significance levels of 0.001 , 0.01 , and 0.05 , respectively .
Non Italian Italian Fixed effects Estim SE Estim .
SE A intercept 46.81 ? 4.03 47.22 ? 6.81 ?B slope + 1.14 4.02 - 10.93 * 4.70 B intercept 47.74 ? 3.21 35.19 ? 7.22 ?C slope + 10.34 + 3.53 + 1.05 2.30 A intercept 46.92 ? 4.95 45.29 ? 7.42 ?C slope + 11.01 6.51 - 9.60 4.89
Actually , there is still a divide between countries / languages where either subtitling or dubbing are the preferred translation modes ( Kilborn , 1993 ; Koolstra et al. , 2002 ) .
The reasons for this are mainly economical and historical ( Danan , 1991 ) .
Amazon Transcribe : https://aws.amazon.com/transcribe.
Amazon Translate : https://aws.amazon.com/translate.4
In this work the minimum pause interval is set to 300 ms .
Pauses are detected from the time stamps produce by forcealigning audio with the transcript ( Ochshorn and Hawkins , 2017 ) .
We approximate the duration d( ? ) of a segment with the sum of the lengths of its words .
We plan to use better approximations in the future , e.g. the number of syllables ( ? ktem et al. , 2019 ) .6 Amazon Comprehend : https://aws.amazon.com/comprehend.
Notice that after preliminary experiments , we decided to not evaluate the configuration A with Prosodic Alignment ,
We would clearly expect significant drop in dubbing quality due to the propagation of ASR errors .9
We use the scale : 1 - Not acceptable : not fluent or not correct ; 2 - Acceptable : almost fluent and almost correct ; 3 - Good : fluent and correct .
