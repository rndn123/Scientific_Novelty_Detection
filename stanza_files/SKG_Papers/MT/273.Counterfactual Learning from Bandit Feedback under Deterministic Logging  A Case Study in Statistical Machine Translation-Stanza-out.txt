title
Counterfactual Learning from Bandit Feedback under Deterministic Logging : A Case Study in Statistical Machine Translation
abstract
The goal of counterfactual learning for statistical machine translation ( SMT ) is to optimize a target SMT system from logged data that consist of user feedback to translations that were predicted by another , historic SMT system .
A challenge arises by the fact that riskaverse commercial SMT systems deterministically log the most probable translation .
The lack of sufficient exploration of the SMT output space seemingly contradicts the theoretical requirements for counterfactual learning .
We show that counterfactual learning from deterministic bandit logs is possible nevertheless by smoothing out deterministic components in learning .
This can be achieved by additive and multiplicative control variates that avoid degenerate behavior in empirical risk minimization .
Our simulation experiments show improvements of up to 2 BLEU points by counterfactual learning from deterministic bandit feedback .
Introduction Commercial SMT systems allow to record large amounts of interaction log data at no cost .
Such logs typically contain a record of the source , the translation predicted by the system , and the user feedback .
The latter can be gathered directly if explicit user quality ratings of translations are supported , or inferred indirectly from the interaction of the user with the translated content .
Indirect feedback in form user clicks on displayed ads has been shown to be a valuable feedback signal in response prediction for display advertising ( Bottou et al. , 2013 ) .
Similar to the computational advertising scenario , one could imagine a scenario where SMT systems are optimized from partial information in form of user feedback to predicted translations , instead of from manually created reference translations .
This learning scenario has been investigated in the areas of bandit learning ( Bubeck and Cesa-Bianchi , 2012 ) or reinforcement learning ( RL ) ( Sutton and Barto , 1998 ) .
Figure 1 illustrates the learning protocol using the terminology of bandit structured prediction ( Sokolov et al. , 2016 ; Kreutzer et al. , 2017 ) , where at each round , a system ( corresponding to a policy in RL terms ) makes a prediction ( also called action in RL , or pulling an arm of a bandit ) , and receives a reward , which is used to update the system .
Counterfactual learning attempts to reuse existing interaction data where the predictions have been made by a historic system different from the target system .
This enables offline or batch learning from logged data , and is important if online experiments that deploy the target system are risky and / or expensive .
Counterfactual learning tasks include policy evaluation , i.e. estimating how a target policy would have performed if it had been in control of choosing the predictions for which the rewards were logged , and policy optimization ( also called policy learning ) , i.e. optimizing parameters of a target policy given the logged data from the historic system .
Both tasks are called counterfactual , or off-policy in RL terms , since the target policy was actually not in control during logging .
Figure 2 shows the learning protocol for off-policy learning from partial feedback .
The crucial trick to obtain unbiased estimators to evaluate and to optimize the off-policy system is to correct the sampling bias of the logging policy .
This can be done by importance sampling where the estimate is corrected by the inverse propensity score ( Rosenbaum and Rubin , 1983 ) of the historical algorithm , mitigating the problem that predictions there were favored by the historical system are over-represented in the logs .
As shown by Langford et al. ( 2008 ) or Strehl et al . ( 2010 ) , a sufficient exploration of the output space by the logging system is a prerequisite for counterfactual learning .
If the logging policy acts stochastically in predicting outputs , this condition is satisfied , and inverse propensity scoring can be applied to correct the sampling bias .
However , commercial SMT systems usually try to avoid any risk and only log the most probable translation .
This effectively results in deterministic logging policies , making theory and practice of off-policy methods inapplicable to counterfactual learning in SMT .
This paper presents a case study in counterfactual learning for SMT that shows that policy optimization from deterministic bandit logs is possible despite these seemingly contradictory theoretical requirements .
We formalize our learning problem as an empirical risk minimization over logged data .
While a simple empirical risk minimizer can show degenerate behavior where the objective is minimized by avoiding or over-representing training samples , thus suffering from decreased generalization ability , we show that the use of control variates can remedy this problem .
Techniques such as doubly - robust policy evaluation and learning ( Dudik et al. , 2011 ) or weighted importance sampling ( Jiang and Li , 2016 ; Thomas and Brunskill , 2016 ) can be interpreted as additive ( Ross , 2013 ) or multiplicative control variates ( Kong , 1992 ) that serve for variance reduction in estimation .
We observe that a further effect of these techniques is that of smoothing out deterministic components by taking the whole output space into account .
Furthermore , we conjecture that while outputs are logged deterministically , the stochastic selection of inputs serves as sufficient exploration in parameter optimization over a joint feature representation over inputs and outputs .
We present experiments using simulated bandit feedback for two different SMT tasks , showing improvements of up to 2 BLEU in SMT domain adaptation from deterministically logged bandit feedback .
This result , together with a comparison to the standard case of policy learning from stochastically logged simulated bandit feedback , confirms the effectiveness our proposed techniques .
Related Work Counterfactual learning has been known under the name of off-policy learning in various fields that deal with partial feedback , namely contextual bandits ( Langford et al .
( 2008 ) ; Strehl et al . ( 2010 ) ; Dudik et al. ( 2011 ) ; Li et al. ( 2015 ) , inter alia ) , reinforcement learning ( Sutton and Barto ( 1998 ) ; Precup et al . ( 2000 ) ; Jiang and Li ( 2016 ) ; Thomas and Brunskill ( 2016 ) , inter alia ) , and structured prediction ( Swaminathan and Joachims ( 2015 a , b) , inter alia ) .
The idea behind these approaches is to first perform policy evaluation and then policy optimization , under the assumption that better evaluation leads to better optimization .
Our work puts a focus on policy optimization in an empirical risk minimization framework for deterministically logged data .
Since our experiment is a simulation study , we can compare the deterministic case to the standard scenario of policy optimization and evaluation under stochastic logging .
Variance reduction by additive control variates has implicitly been used in doubly robust techniques ( Dudik et al. , 2011 ; Jiang and Li , 2016 ) .
However , the connection to Monte Carlo techniques has not been made explicit until Thomas and Brunskill ( 2016 ) , nor has the control variate technique of optimizing the variance reduction by adjusting a linear interpolation scalar ( Ross , 2013 ) been applied in off-policy learning .
Similarly , the technique of weighted importance sampling has been used as variance reduction technique in off-policy learning ( Precup et al. , 2000 ; Jiang and Li , 2016 ; Thomas and Brunskill , 2016 ) .
The connection to multiplicative control variates ( Kong , 1992 ) has been made explicit in Swaminathan and Joachims ( 2015 b ) .
To our knowledge , our analysis of both control variate techniques from the perspective of avoiding degenerate behavior in learning from deterministically logged data is novel .
Counterfactual Learning from Deterministic Bandit Logs Problem Definition .
The problem of counterfactual learning ( in the following used in the sense of counterfactual optimization ) for bandit structured prediction can be described as follows :
Let X be a structured input space , let Y( x ) be the set of possible output structures for input x , and let ? : Y ? [ 0 , 1 ] be a reward function ( and ? = ? be the corresponding task loss function ) 1 quantifying the quality of structured outputs .
We are given a data log of triples D = {( x t , y t , ? t ) } n t=1 where outputs y t for inputs x t were generated by a logging system , and loss values ?
t were observed only at the generated data points .
In case of stochastic logging with probability ?
0 , the inverse propensity scoring approach ( Rosenbaum and Rubin , 1983 ) uses importance sampling to achieve an unbiased estimate of the expected loss under the parametric target policy ? w : RIPS ( ? w ) = 1 n n t=1 ? t ? w ( y t |x t ) ? 0 ( y t |x t ) ( 1 ) ? E p( x ) E ? 0 ( y|x ) [?( y ) ? w ( y|x ) ? 0 ( y|x ) ] = E p( x ) E ?w( y |x ) [?( y ) ] .
In case of deterministic logging , we are confined to empirical risk minimization : RDPM ( ? w ) = 1 n n t=1 ? t ? w ( y t |x t ) .
( 2 ) Equation ( 2 ) assumes deterministically logged outputs with propensity ?
0 = 1 , t = 1 , . . . , n of the historical system .
We call this objective the deterministic propensity matching ( DPM ) objective since it matches deterministic outputs of the logging system to outputs in the n-best list of the target system .
For optimization under deterministic logging , a sampling bias is unavoidable since objective ( 2 ) does not correct it by importance sampling .
Furthermore , the DPM estimator may show a degenerate behavior in learning .
This problem can be remedied by the use of control variates , as we will discuss in Section 5 .
Learning Principle : Doubly Controlled Empirical Risk Minimization .
Our first modification of Equation ( 2 ) has been originally motivated by the use of weighted importance sampling in inverse propensity scoring because of ?
RDPM = 1 n n t=1 ? t ? w ( y t |x t ) ?
log ? w ( y t |x t ) .
? RDPM +R = 1 n n t=1 [?
t ?w ( y t |x t ) ( ?
log ? w ( y t |x t ) ? n u=1 ?w ( y u |x u ) ?
log ? w ( y u |x u ) ) ] .
? R? DC = 1 n n t=1 [ ( ?
t ? ? t ) ?
w ( y t |x t ) ( ?
log ? w ( y t |x t ) ? n u=1 ?w ( y u |x u ) ?
log ? w ( y u |x u ) ) +? y?Y( xt ) ?( x t , y ) ? w ( y|x t ) ?
log ? w ( y|x t ) ] .
Table 1 : Gradients of counterfactual objectives .
its observed stability and variance reduction effects ( Precup et al. , 2000 ; Jiang and Li , 2016 ; Thomas and Brunskill , 2016 ) .
We call this objective the reweighted deterministic propensity matching ( DPM + R ) objective : RDPM +R ( ? w ) = 1 n n t=1 ? t ?w ( y t |x t ) ( 3 ) = 1 n n t=1 ? t ? w ( y t |x t ) n t=1 ? w ( y t |x t ) .
From the perspective of Monte Carlo simulation , the advantage of this modification can be explained by viewing reweighting as a multiplicative control variate ( Swaminathan and Joachims , 2015 b ) .
Let Z = ? t ? w ( y t |x t ) and W = ? w ( y t |x t ) be two random variables , then the variance of r = 1 n n t=1 Z 1 n n t=1 W can be approximately written as follows ( Kong , 1992 ) : Var ( r ) ? 1 n ( r 2 Var ( W ) + Var ( Z ) ? 2r Cov( W , Z ) ) .
This shows that a positive correlation between the variable W , representing the target model probability , and the variable Z , representing the target model scaled by the task loss function , will reduce the variance of the estimator .
Since there are exponentially many outputs to choose from for each input during logging , variance reduction is useful in counterfactual learning even in the deterministic case .
Under a stochastic logging policy , a similar modification can be done to objective ( 1 ) by reweighting the ratio ? t = ?w( yt | xt ) ? 0 ( yt| xt ) as ?t = ?t t ?t .
We will use this reweighted IPS objective , called IPS +R , in our comparison experiments that use stochastically logged data .
A further modification of Equation ( 3 ) is motivated by the incorporation of a direct reward estimation method in the inverse propensity scorer as proposed in the doubly - robust estimator ( Dudik et al. , 2011 ; Jiang and Li , 2016 ; Thomas and Brunskill , 2016 ) .
Let ?( x t , y t ) be a regression - based reward model trained on the logged data , and let ?
be a scalar that allows to optimize the estimator for minimal variance ( Ross , 2013 ) .
We define a doubly controlled empirical risk minimization objective R? DC as follows ( for ? = 1 we arrive at a similar objective called RDC ) : R? DC ( ? w ) = 1 n n t=1 ( ?
t ? ? ?t ) ?w ( y t |x t ) ( 4 ) + ? y?Y( xt ) ?( x t , y ) ? w ( y|x t ) .
From the perspective of Monte Carlo simulation , the doubly robust estimator can be seen as variance reduction via additive control variates ( Ross , 2013 ) .
Let X = ? t and Y = ?t be two random variables .
Then ? = y?Y( xt ) ?( x t , y ) ? w ( y|x t ) is the expectation 2 of Y , and Equation ( 4 ) can be rewritten as ( Ross ( 2013 ) , Chap. 9.2 ) .
Again this shows that variance of the estimator can be reduced if the variable X , representing the reward function , and the variable Y , representing the regression - based reward model , are positively correlated .
The optimal scalar parameter ?
can be derived easily by taking the derivative of variance term , leading to E ?w( x ) ( X ? ? Y ) + ? ? .
The variance of the term X ? Y is Var ( X ? Y ) = Var ( X ) +?
2 Var ( Y ) ?
2 ? Cov(X , Y ) . ? = Cov(X , Y ) Var ( Y ) . ( 5 ) In case of stochastic logging the reweighted target probability ?w ( y t |x t ) is replaced by a reweighted ratio ?t .
We will use such reweighted models of the original doubly robust model , with and without optimal ? , called DR and ?
DR , in our experiments that use stochastic logging .
Learning Algorithms .
Applying a stochastic gradient descent update rule w t+1 = w t ? ? R ( ? w ) t to the objective functions defined above leads to a variety of algorithms .
The gradients of the objectives can be derived by using the score function gradient estimator ( Fu , 2006 ) and are shown in Table 1 . Stochastic gradient descent algorithms apply to any differentiable policy ?
w , thus our methods can be applied to a variety of systems , including linear and non-linear models .
Since previous work on off-policy methods in RL and contextual bandits has been done in the area of linear classification , we start with an adaptation of off-policy methods to linear SMT models in our work .
We assume a Gibbs model ? w ( y t |x t ) = e ?( w ?( xt , yt ) ) y?Y ( xt ) e ?( w ?( xt , y ) ) , ( 6 ) based on a feature representation ? : X ? Y ?
R d , a weight vector w ?
R d , and a smoothing parameter ? ? R + , yielding the following simple derivative ? log ? w ( y t |x t ) = ? ?( x t , y t ) ? y?Y( xt ) ?( x t , y ) ?
w ( y t |x t ) .
Experiments Setup .
In our experiments , we aim to simulate the following scenario :
We assume that it is possible to divert a small fraction of the user interaction traffic for the purpose of policy evaluation and to perform stochastic logging on this small data set .
The main traffic is assumed to be logged deterministically , following a conservative regime where one - best translations are used for an SMT system that does not change frequently over time .
Since our experiments are simulation studies , we will additionally perform stochastic logging , and compare policy learning for the ( realistic ) case of deterministic logging with the ( theoretically motivated ) case of stochastic logging .
In our deterministic - based policy learning experiments , we evaluate the empirical risk minimization algorithms derived from objectives ( 3 ) ( DPM + R ) and ( 4 ) .
For the doubly controlled objective we employ two variants :
First , ? is set to 1 as in ( Dudik et al. , 2011 ) ( DC ) . Second , we calculate ? as described in Equation ( 5 ) ( ? DC ) .
The algorithms used in policy evaluation and for stochastic - based policy learning are variants of these objectives that replace ? by ? to yield estimators IPS+R , DR , and ?
DR of the expected loss .
All objectives will be employed in a domain adaptation scenario for machine translation .
A system trained on out -of- domain data will be used to collect feedback on in- domain data .
This data will serve as the logged data D in the learning experiments .
We conduct two SMT tasks with hypergraph re-decoding :
The first is German-to - English and is trained using a concatenation of the Europarl corpus ( Koehn , 2005 ) , the Common Crawl corpus 3 and the News Commentary corpus ( Koehn and Schroeder , 2007 ) .
The goal is to adapt the trained system to the domain of transcribed TED talks using the TED parallel corpus ( Tiedemann , 2012 ) .
A second task uses the French-to - English Europarl data with the goal of domain adaptation to news articles with the News Commentary corpus ( Koehn and Schroeder , 2007 ) .
We split off two parts from the TED corpus to be used as validation and test data for the learning experiments .
As validation data for the News Commentary corpus we use the splits provided at the WMT shared task , namely nc-devtest2007 as validation data and nc-test2007 as test data .
An overview of the data statistics can be seen in Table 2 .
As baseline , an out-of- domain system is built using the SCFG framework CDEC ( Dyer et al. , 2010 ) with dense features ( 10 standard features and 2 for the language model ) .
After tokenizing and lowercasing the training data , the data were word aligned using CDEC 's fast align .
A 4 - gram language model is build on the target languages for the out-of- domain data using KENLM ( Heafield et al. , 2013 ) .
For News , we additionally assume access to in- domain target language text and train another in- domain language model on that data , increasing the number of features to 14 for News .
The framework uses a standard linear Gibbs model whose distribution can be peaked using a parameter ?
( see Equation ( 6 ) ) :
Higher value of ? will shift the probability of the one- best translation closer to 1 and all others closer to 0 .
Using ? > 1 during training will promote to learn models that are optimal when outputting the one-best translation .
In our experiments , we found ? = 5 to work well on validation data .
Additionally , we tune a system using CDEC 's MERT implementation ( Och , 2003 ) on the indomain data with their references .
This fullinformation in- domain system conveys the best possible improvement using the given training data .
It can thus be seen as the oracle system for the systems which are learnt using the same input-side training data , but have only bandit feedback available to them as a learning signal .
All systems are evaluated using the corpus-level BLEU metric ( Papineni et al. , 2002 ) .
The logged data D is created by translating the in-domain training data of the corpora using the original out - of- domain systems , and logging the one- best translation .
For the stochastic experiments , the translations are sampled from the model distribution .
The feedback to the logged translation is simulated using the reference and sentence - level BLEU ( Nakov et al. , 2012 ) . Direct Reward Estimation .
When creating the logged data D , we also record the feature vectors of the translations to train the direct reward estimate that is needed for ( ? ) DC .
Using the feature vector as input and the per-sentence BLEU as the output value , we train a regressionbased random forest with 10 trees using scikitlearn ( Pedregosa et al. , 2011 ) .
To measure performance , we perform 5 - fold cross-validation and measure the macro average between estimated rewards and the true rewards from the log : | 1 n ?( x t , y t ) ?
1 n ?( x t , y t ) | .
We also report the micro average which quantifies how far off one can expect the model to be for a random sample : 1 n |?( x t , y t ) ? ?( x t , y t ) | .
The final model used in the experiments is trained on the full training data .
Cross-validation results for the regression - based direct reward model can be found in Table 3 . Policy Evaluation .
Policy evaluation aims to use the logged data D to estimate the performance of the target system ? w .
The small logged data D eval that is diverted for policy evaluation is created by translating only 10 k sentences of the in-domain training data with the out-ofdomain system and sample translations according to the model probability .
Again we record the sentence - level BLEU as the feedback .
The reference translations that also exist for those 10 k sentences are used to measure the ground truth BLEU value for translations using the fullinformation in-domain system .
The goal of evaluation is to achieve a value of IPS +R , DR , and ?
DR on D eval that are as close as possible to the ground truth BLEU value .
To be able to measure variance , we create five folds of D eval , differing in random seeds .
We report the average difference between the ground truth BLEU score and the value of the log-based policy evaluation , as well as the standard deviation in Table 4 .
We see that IPS +R underestimates the BLEU value by 7.78 on News .
DR overestimates instead . ?
DR achieves the closest estimate , overestimating the true value by less than 1 BLEU .
On TED , all policy evaluation results are overestimates .
For the DR variants the overestimation result can be explained by the random forests ' tendency to overestimate .
Optimal ?
DR can correct for this , but not always in a sufficient way .
Policy Learning .
In our learning experiments , learning starts with the weights w 0 from the outof-domain model .
As this was the system that produced the logged data D , the first iteration will have the same translations in the one- best position .
After some iterations , however , the translation that was logged may not be in the first position any more .
In this case , the n-best list is searched for the correct translation .
Due to speed reasons , the scores of the translation system are normalized to probabilities using the first 1,000 unique entries in the n-best list , rather than using the full hypergraph .
Our experiments showed that this did not impact the quality of learning .
In order for the multiplicative control variate to be effective , the learning procedure has to utilize mini-batches .
If the mini-batch size is chosen too small , the estimates of the control variates may not be reliable .
We test mini-batch sizes of 30 k and 10k examples , whereas 30k on News means that we perform batch training since the mini-batch spans the entire training set .
Minibatch size ? and early stopping point where selected by choosing the setup and iteration that achieved the highest BLEU score on the one- best translations for the validation data .
The learning rate ? was selected in the same way , whereas the possible values were 1e?4 , 1e?5 , 1e?6 or , alternatively , Adadelta ( Zeiler , 2012 ) , which sets the learning rate on a per-feature basis .
The results on both validation and test set are reported in Table 5 . Statistical significance of the outof-domain system compared to all other systems is measured using Approximate Randomization testing ( Noreen , 1989 ) .
For the deterministic case , we see that in general DPM + R shows the lowest increase but can still significantly outperform the baseline .
An explanation of why DPM +R cannot improve any further , will be addressed separately below .
DC yields improvements of up to 1.5 BLEU points , while ?
DC obtains improvements of up to 2 BLEU points over the out-of- domain baseline .
In more detail on the TED data , DC can close the gap of nearly 3 BLEU by half between the out-of- domain and the full- information indomain system .
?
DC can improve by further 0.6 BLEU which is a significant improvement at p = 0.0017 .
Also note that , while ?
DC takes more iterations to reach its best result on the validation data , ? DC already outperforms DC at the stopping iteration of DC .
At this point ?
DC is better by 0.18 BLEU on the validation set and continues to increase until its own stopping iteration .
The final results of ?
DC falls only 0.8 BLEU behind the oracle system that had references available during its learning process .
Considering the substantial difference in information that both systems had available , this is remark -
Table 5 : BLEU increases for learning , over the out-of- domain baseline on validation and test set .
Outof-domain is the baseline and starting system and in- domain is the oracle system tuned on in-domain data with references .
For the deterministic case , all results are statistically significant at p ? 0.001 with regards to the baseline .
For the stochastic case , all results are statistically significant at p ?
0.002 with regards to the baseline , except for IPS + R on the News corpus .
able .
The improvements on the News corpus show similar tendencies .
Again there is a gap of nearly 3 BLEU to close and with an improvement of 1.05 BLEU points , DC can achieve a notable result . ?
DC was able to further improve on this but not as successfully as was the case for the TED corpus .
Analyzing the actual ?
values that were calculated in both experiments allows us to gain an insight as to why this was the case : For TED , ? is on average 1.35 .
In the case of News , however , ? has a maximum value of 1.14 and thus stays quite close to 1 , which would equate to using DC .
It is thus not surprising that there is no significant difference between DC and ? DC .
Comparison to the Stochastic Case .
Even if not realistic for commercial applications of SMT , our simulation study allows us to stochastically log large amounts of data in order to compare learning from deterministic logs to the standard case .
As shown in Table 5 , the relations between algorithms and even the absolute improvements are similar for stochastic and deterministic logging .
Significance tests between each deterministic / stochastic experiment pair show a significant difference only in case of DC / DR on TED data .
However , the DR result still does not significantly outperform the best deterministic objective on TED ( ? DC ) .
The p values for all other experiment pairs lie above 0.1 .
From this we can conclude that it is indeed an acceptable practice to log deterministically .
Analysis Langford et al. ( 2008 ) show that counterfactual learning is impossible unless the logging system sufficiently explores the output space .
This condition is seemingly not satisfied if the logging systems acts according to a deterministic policy .
Furthermore , since techniques such as " exploration over time " ( Strehl et al. , 2010 ) are not applicable to commercial SMT systems that are not frequently changed over time , the case of counterfactual learning for SMT seems hopeless .
However , our experiments present evidence to the contrary .
In the following , we present an analysis that aims to explain this apparent contradiction .
Implicit Exploration .
In an experimental comparison between stochastic and deterministic logging for bandit learning in computational advertising , Chapelle and Li ( 2011 ) observed that varying contexts ( representing user and page visited ) induces enough exploration into ad selection such that learning becomes possible .
A similar implicit exploration can also be attributed to the case of SMT :
An identical input word or phrase can lead , depending on the other words and phrases in the input sentence , to different output words and phrases .
Moreover , an identical output word or phrase can appear in different output sentences .
Across the entire log , this implicitly performs the exploration on phrase translations that seems to be missing at first glance .
Smoothing by Multiplicative Control Variates .
The DPM estimator can show a degenerate behavior in that the objective can be minimized simply by setting the probability of every logged data point to 1.0 .
This over-represents logged data that received low rewards , which is undesired .
Furthermore , systems optimized with this objective cannot properly discriminate between the translations in the output space .
This can be seen as a case of translation invariance of the objective , as has been previously noted by Swaminathan and Joachims ( 2015 b ) :
Adding a small constant c to the probability of every data point in the log increases the overall value of the objective without improving the discriminative power between high - reward and low-reward translations .
DPM +R solves the degeneracy of DPM by defining a probability distribution over the logged data by reweighting via the multiplicative control variate .
After reweighting , the objective value will decrease if the probability of a low-reward translation increased , as it takes away probability mass from other , higher reward samples .
Because of this trade-off , balancing the probabilities over low-reward and high- reward samples becomes important , as desired .
Smoothing by Additive Control Variates .
Despite reweighting , DPM +R can still show a degenerate behavior by setting the probabilities of only the highest - reward samples to 1.0 , while avoiding all other logged data points .
This clearly hampers the generalization ability of the model since inputs that have been avoided in training will not receive a proper ranking of their translations .
The use of an additive control variate can solve this problem by using a reward estimate that takes the full output space into account .
The objective will now be increased if the probability of translations with high estimated reward is increased , even if they were not seen in training .
This will shift probability mass to unseen data with high estimated - reward , and thus improve the generalization ability of the model .
Conclusion
In this paper , we showed that off-policy learning from deterministic bandit logs for SMT is possible if smoothing techniques based on control variates are used .
These techniques will avoid degenerate behavior in learning and improve generalization of empirical risk minimization over logged data .
Furthermore , we showed that standard off-policy evaluation is applicable to SMT under stochastic logging policies .
To our knowledge , this is the first application of counterfactual learning to a complex structured prediction problem like SMT .
Since our objectives are agnostic of the choice of the underlying model ?
w , it is also possible to transfer our techniques to non-linear models such as neural machine translation .
This will be a desideratum for future work .
Figure 1 : 1 Figure 1 : Online learning from partial feedback .
