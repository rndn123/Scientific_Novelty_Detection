title
Using Target -side Monolingual Data for Neural Machine Translation through Multi-task Learning
abstract
The performance of Neural Machine Translation ( NMT ) models relies heavily on the availability of sufficient amounts of parallel data , and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found .
We propose to modify the decoder in a neural sequence - to-sequence model to enable multi-task learning for two strongly related tasks : target - side language modeling and translation .
The decoder predicts the next target word through two channels , a target - side language model on the lowest layer , and an attentional recurrent model which is conditioned on the source representation .
This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance .
Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only .
Introduction
In recent years , neural encoder-decoder models ( Kalchbrenner and Blunsom , 2013 ; have significantly advanced the state of the art in NMT , and now consistently outperform Statistical Machine Translation ( SMT ) ( Bojar et al. , 2016 ) .
However , their success hinges on the availability of sufficient amounts of parallel data , and contrary to the long line of research in SMT , there has only been a limited amount of work on how to effectively and efficiently make use of monolingual data which is typically amply available .
We propose a modified neural sequence - to- sequence model with atten-tion Luong et al. , 2015 b ) that uses multi-task learning on the decoder side to jointly learn two strongly related tasks : target - side language modeling and translation .
Our approach does not require any pre-translation or pre-training to learn from monolingual data and thus provides a principled way to integrate monolingual data resources into NMT training .
Related Work G?lc ? ehre et al. ( 2015 ) investigate two ways of integrating a pre-trained neural Language Model ( LM ) into a pre-trained NMT system : shallow fusion , where the LM is used at test time to rescore beam search hypothesis , requiring no additional finetuning and deep fusion , where hidden states of NMT decoder and LM are concatenated before making a prediction for the next word .
Both components are pre-trained separately and fine-tuned together .
More recently , Sennrich et al . ( 2016 ) have shown significant improvements by back - translating target - side monolingual data and using such synthetic data as additional parallel training data .
One downside of this approach is the significantly increased training time , due to training of a model in the reverse direction and translation of monolingual data .
In contrast , we propose to train NMT models from scratch on both bilingual and target -side monolingual data in a multi-task setting .
Our approach aims to exploit the signals from target - side monolingual data to learn a strong language model that supports the decoder in making translation decisions for the next word .
Our approach further relates to Zhang and Zong ( 2016 ) , who investigate multi-task learning for sequenceto-sequence models by strengthening the encoder using source -side monolingual data .
A shared encoder architecture is used to predict both , transla-tions of parallel source sentences and permutations of monolingual source sentences .
In this paper we focus on target -side monolingual data and only update encoder parameters based on existing parallel data .
In a broader context , multi-task learning has shown to be effective in the context of sequenceto-sequence models ( Luong et al. , 2015a ) , where different parts of the network can be shared across multiple tasks .
Neural Machine Translation
We briefly recap the baseline NMT model Luong et al. , 2015 b ) and highlight architectural differences of our implementation where necessary .
Given source sentence x = x 1 , ... , x n and target sentence y = y 1 , ... , y m , NMT models p( y|x ) as a target language sequence model , conditioning the probability of the target word y t on the target history y 1:t?1 and source sentence x .
Each x i and y t are integer ids given by source and target vocabulary mappings , V src , V trg , built from the training data tokens .
The target sequence is factorized as : p( y|x ; ?) = m t=1 p(y t |y 1:t?1 , x ; ? ) . ( 1 )
The model , parameterized by ? , consists of an encoder and a decoder part .
For training set P consisting of parallel sentence pairs ( x , y ) , we minimize the cross-entropy loss w.r.t ? : L ? = ( x, y ) ? P ? log p( y|x ; ? ) .
( 2 ) Encoder Given source sentence x = x 1 , ... , x n , the encoder produces a sequence of hidden states h 1 . . . h n through an Recurrent Neural Network ( RNN ) , such that : ? ?
h i = f enc ( E S x i , ? ? h i?1 ) , ( 3 ) where h 0 = 0 , x i ? { 0 , 1 } | Vsrc | is the one- hot encoding of x i , E S ? R e?|Vsrc | is a source embedding matrix with embedding size e , and f enc some non-linear function , such as the Gated Rectified Unit ( GRU ) or a Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) network .
Attentional Decoder
The decoder also consists of an RNN to predict one target word at a time through a state vector s : s t = f dec ( [ E T y t?1 ; st ?1 ] , s t?1 ) , ( 4 ) where y t?1 ? { 0 , 1 } | Vtrg | is the one- hot encoding of the previous target word , E T ? R e?| Vtrg | the target word embedding matrix , f dec an RNN , s t?1 the previous state vector , and st ?1 the sourcedependent attentional vector .
The initial decoder hidden state is a non-linear transformation of the last encoder hidden state : s 0 = tanh ( W init h n + b init ) .
The attentional vector st combines the decoder state with a context vector c t : st = tanh ( W s[s t ; c t ] ) , ( 5 ) where c t is a weighted sum of encoder hidden states : c t = n i=1 ? ti h i and brackets denote vector concatenation .
The attention vector ?
t is computed by an attention network Luong et al. , 2015 b ) : ? ti = sof tmax ( score ( s t , h i ) ) score(s , h ) = v a tanh ( W u s + W v h ) .
( 6 ) The next target word y t is predicted through a softmax layer over the attentional vector st : p(y t |y 1:t?1 , x ; ? ) = sof tmax ( W o st + b o ) ( 7 ) where W o maps st to the dimension of the target vocabulary .
Figure 1a depicts this decoder architecture .
Note that source information from c indirectly influences the states s of the decoder RNN as it takes s as one of its inputs .
Incorporating Monolingual Data
Separate Decoder LM layer
The decoder RNN ( Figure 1a ) is essentially a targetside language model , additionally conditioned on source-side sequences .
Such sequences are not available for monolingual corpora and previous work has tried to overcome this problem by either using synthetically generated source sequences or using a NULL token as the source sequence ( Sennrich et al. , 2016 ) .
As previously shown empirically , the model tends to " forget " source -side information if trained on much more monolingual than parallel data .
In our approach we explicitly define a sourceindependent network that only learns from targetside sequences ( a language model ) , and a sourcedependent network on top , that takes information from the source sequence into account ( a translation model ) through the attentional vector s.
Formally , we modify the decoder RNN of Equation 4 to operate on the outputs an LM layer , which is independent of any source-side information : s t = f dec ( [ r t ; st ?1 ] , s t?1 ) ( 8 ) r t = f lm ( E T y t?1 , r t?1 ) ( 9 ) Figure 1 b illustrates this separation graphically .
Multi-task Learning
The separation from above allows us to train the target embeddings E T and f lm parameters from monolingual data , concurrent to training the rest of the network on bilingual data .
Let us denote the source-independent parameters by ?.
We connect a second loss to f lm to predict the next target word also conditioned only on target history information ( Figure 1 c ) .
Parameters for softmax layers are shared such that predictions of the LM layer are given by : p(y t |y 1:t?1 , ? ) = sof tmax ( W o r t + b o ) .
( 10 ) Formally , for a heterogeneous data set Z = { P , M} , consisting of parallel and monolingual sentences ( x , y ) , ( y ) , we optimize the following joint loss : L ? , ? = 1 | P| ( x, y ) ? P ? log p( y|x ; ? ) +?
1 | M| y?M ? log p(y ; ? ) , ( 11 ) where the source-independent parameters ? ? ? are updated by gradients from both mono- and parallel data examples , and source-dependent parameters ? are updated only through gradients from parallel data examples .
? ?
0 is a scalar to influence the importance of the monolingual loss .
In practice , we construct mini-batches of training examples , where 50 % of the data is parallel , and 50 % of the data is monolingual and set ? = 1 . Since parts of the decoder are shared among both tasks and we optimize both loss terms concurrently , we view this approach as an instance of multi-task learning rather than transfer learning , where optimization is typically carried out sequentially .
Experiments
We conduct experiments for three different language pairs in the news domain : FR?EN , EN?DE , and CS?EN .
Data For 1 : BLEU / METEOR / TER scores on test sets for different language pairs .
For BLEU and METEOR higher is better .
For TER lower is better .
WMT2016 ( Bojar et al. , 2016 ) . For FR?EN we use newscommentary - v9 as bilingual data , NewsCrawl 2009 - 13 as monolingual data , and news development and test sets from WMT 2014 ( Bojar et al. , 2014 ) .
The number of sentences for these corpora is shown below : EN?DE 242 , 770 51 , 315 , 088 FR?EN 183 , 251 51 , 995 , 709 CS?EN 191 , 432 27,236,445 Data
Set bilingual monolingual
Experimental Setup
We tokenize all data and apply Byte Pair Encoding ( BPE ) ( Sennrich et al. , 2015 ) with 30 k merge operations learned on the joined bilingual data .
Models are evaluated in terms of BLEU ( Papineni et al. , 2002 ) , METEOR ( Lavie and Denkowski , 2009 ) and TER ( Snover et al. , 2006 ) on tokenized , cased test data .
Decoding is performed using beam search with a beam of size 5 .
We implement all models using MXNet ( Chen et al. , 2015 ) 1 . Baselines
Our baseline model consists of a 1layer bi-directional LSTM encoder with an embedding size of 512 and a hidden size of 1024 .
The 1 - layer LSTM decoder with 1024 hidden units uses an attention network with 256 hidden units .
The model is optimized using Adam ( Kingma and Ba , 2014 ) with a learning rate of 0.0003 , no weight decay and gradient clipping if the norm exceeds 1.0 .
The batch size is set to 64 and the maximum sequence length to 100 .
Dropout ( Srivastava et al. , 2014 ) of 0.3 is applied to source word embeddings and outputs of RNN cells .
We initialize all 1 Baseline systems are equivalent to an earlier version of Sockeye : https://github.com/awslabs/sockeye
RNN parameters with orthogonal matrices ( Saxe et al. , 2013 ) and the remaining parameters with the Xavier ( Glorot and Bengio , 2010 ) method .
We use early stopping with respect to perplexity on the development set .
We train each model configuration three times with different seeds and report average metrics across the three runs .
Further , we train models with synthetic parallel data generated through back - translation ( Sennrich et al. , 2016 ) .
For this , we first train a baseline model in the reverse direction and then translate a random sample of 200k sentences from the monolingual target data .
On the combined parallel and synthetic training data we train a new model with the same training hyper-parameters as the baseline .
Language Model Layer
The architecture with an additional source- independent LM layer ( + LML ) is trained with the same hyper-parameters and data as the baseline model .
The LM RNN uses a hidden size of 1024 .
The multi-task system ( + LML + MTL ) is trained on both parallel and monolingual data .
In practice , all + LML + MTL models converge before seeing the entire monolingual corpus and at about the same number of updates as the baseline .
Results
Table 1 shows results on the held - out test sets .
We observe that a separate LM layer does not significantly impact performance across all metrics .
Adding monolingual data in the described multitask setting improves translation performance by a small but consistent margin across all metrics .
Interestingly , the improvements from monolingual data are additive to the gains from ensembling of 3 models with different random seeds .
However , the use of synthetic parallel data still outperforms our approach both in single and ensemble systems .
While separating out a language model allowed us to carry out multi-task training on mixed data types , it constrains gradients from monolingual data examples to a subset of source - independent network parameters ( ? ) .
In contrast , synthetic data always affects all network parameters ( ? ) and has a positive effect despite source sequences being noisy .
We speculate that training from synthetic source data may also act as a model regularizer .
Conclusion
We proposed a way to directly integrate target -side monolingual data into NMT through multi-task learning .
Our approach avoids costly pre-training processes and jointly trains on bilingual and monolingual data from scratch .
While initial results show only moderate improvements over the baseline and fall short against using synthetic parallel data , we believe there is value in pursuing this line of research further to simplify training procedures .
Figure 1 : 1 Figure 1 : Illustration of the proposed decoder architecture .
( a) Baseline model with a single - layer decoder RNN and attention ( b ) Addition of a source - independent LM layer that feeds into the source- dependent decoder ( c ) Multi-task setting next- word prediction from both layers ; green softmax layers are shared .
