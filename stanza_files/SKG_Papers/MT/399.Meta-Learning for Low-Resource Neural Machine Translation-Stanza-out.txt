title
Meta-Learning for Low-Resource Neural Machine Translation
abstract
In this paper , we propose to extend the recently introduced model - agnostic meta-learning algorithm ( MAML , Finn et al. , 2017 ) for lowresource neural machine translation ( NMT ) .
We frame low-resource translation as a metalearning problem , and we learn to adapt to low-resource languages based on multilingual high- resource language tasks .
We use the universal lexical representation ( Gu et al. , 2018 b ) to overcome the input-output mismatch across different languages .
We evaluate the proposed meta-learning strategy using eighteen European languages ( Bg , Cs , Da , De , El , Es , Et , Fr , Hu , It , Lt , Nl , Pl , Pt , Sk , Sl , Sv and Ru ) as source tasks and five diverse languages ( Ro , Lv , Fi , Tr and Ko ) as target tasks .
We show that the proposed approach significantly outperforms the multilingual , transfer learning based approach ( Zoph et al. , 2016 ) and enables us to train a competitive NMT system with only a fraction of training examples .
For instance , the proposed approach can achieve as high as 22.04 BLEU on Romanian - English WMT ' 16 by seeing only 16,000 translated words ( ?
600 parallel sentences ) .
Introduction
Despite the massive success brought by neural machine translation ( NMT , Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) , it has been noticed that the vanilla NMT often lags behind conventional machine translation systems , such as statistical phrase - based translation systems ( PBMT , Koehn et al. , 2003 ) , for low-resource language pairs ( see , e.g. , Koehn and Knowles , 2017 ) .
In the past few years , various approaches have been proposed to address this issue .
The first attempts at tackling this problem exploited the availability of monolingual corpora ( Gulcehre * Equal contribution .
et al. , 2015 ; Sennrich et al. , 2015 ; Zhang and Zong , 2016 ) .
It was later followed by approaches based on multilingual translation , in which the goal was to exploit knowledge from high- resource language pairs by training a single NMT system on a mix of high- resource and low-resource language pairs ( Firat et al. , 2016 a , b ; Lee et al. , 2016 ; Johnson et al. , 2016 ; Ha et al. , 2016 b ) .
Its variant , transfer learning , was also proposed by Zoph et al . ( 2016 ) , in which an NMT system is pretrained on a high- resource language pair before being finetuned on a target low-resource language pair .
In this paper , we follow up on these latest approaches based on multilingual NMT and propose a meta-learning algorithm for low-resource neural machine translation .
We start by arguing that the recently proposed model- agnostic meta-learning algorithm ( MAML , Finn et al. , 2017 ) could be applied to low-resource machine translation by viewing language pairs as separate tasks .
This view enables us to use MAML to find the initialization of model parameters that facilitate fast adaptation for a new language pair with a minimal amount of training examples ( ?3 ) .
Furthermore , the vanilla MAML however cannot handle tasks with mismatched input and output .
We overcome this limitation by incorporating the universal lexical representation ( Gu et al. , 2018 b ) and adapting it for the meta-learning scenario ( ?3.3 ) .
We extensively evaluate the effectiveness and generalizing ability of the proposed meta-learning algorithm on low-resource neural machine translation .
We utilize 17 languages from Europarl and Russian from WMT as the source tasks and test the meta-learned parameter initialization against five target languages ( Ro , Lv , Fi , Tr and Ko ) , in all cases translating to English .
Our experiments using only up to 160k tokens in each of the target task reveal that the proposed meta-learning approach outperforms the multilingual translation approach across all the target language pairs , and the gap grows as the number of training examples decreases .
Background Neural Machine Translation ( NMT ) Given a source sentence X = {x 1 , ... , x T 0 } , a neural machine translation model factors the distribution over possible output sentences Y = {y 1 , ... , y T } into a chain of conditional probabilities with a leftto-right causal structure : p( Y | X ; ? ) = T +1 Y t=1 p(y t |y 0:t 1 , x 1:T 0 ; ? ) , ( 1 ) where special tokens y 0 ( hbosi ) and y T + 1 ( heosi ) are used to represent the beginning and the end of a target sentence .
These conditional probabilities are parameterized using a neural network .
Typically , an encoder-decoder architecture ( Sutskever et al. , 2014 ; Cho et al. , 2014 ; Bahdanau et al. , 2015 ) with a RNN - based decoder is used .
More recently , architectures without any recurrent structures ( Gehring et al. , 2017 ; Vaswani et al. , 2017 ) have been proposed and shown to speed up training while achieving state - of - the - art performance .
Low Resource Translation NMT is known to easily over -fit and result in an inferior performance when the training data is limited ( Koehn and Knowles , 2017 ) .
In general , there are two ways for handling the problem of low resource translation : ( 1 ) utilizing the resource of unlabeled monolingual data , and ( 2 ) sharing the knowledge between low - and high- resource language pairs .
Many research efforts have been spent on incorporating the monolingual corpora into machine translation , such as multi-task learning ( Gulcehre et al. , 2015 ; Zhang and Zong , 2016 ) , back - translation ( Sennrich et al. , 2015 ) , dual learning ( He et al. , 2016 ) and unsupervised machine translation with monolingual corpora only for both sides ( Artetxe et al. , 2017 b ; Lample et al. , 2017 ; . For the second approach , prior researches have worked on methods to exploit the knowledge of auxiliary translations , or even auxiliary tasks .
For instance , Cheng et al . ( 2016 ) ; ; Lee et al. ( 2017 ) ; investigate the use of a pivot to build a translation path between two languages even without any directed resource .
The pivot can be a third language or even an image in multimodal domains .
When pivots are not easy to obtain , Firat et al . ( 2016 a ) ; Lee et al . ( 2016 ) ; Johnson et al. ( 2016 ) have shown that the structure of NMT is suitable for multilingual machine translation .
Gu et al. ( 2018 b ) also showed that such a multilingual NMT system could improve the performance of low resource translation by using a universal lexical representation to share embedding information across languages .
All the previous work for multilingual NMT assume the joint training of multiple high- resource languages naturally results in a universal space ( for both the input representation and the model ) which , however , is not necessarily true , especially for very low resource cases .
Meta Learning
In the machine learning community , meta-learning , or learning - to - learn , has recently received interests .
Meta-learning tries to solve the problem of " fast adaptation on new training data . "
One of the most successful applications of meta-learning has been on few-shot ( or oneshot ) learning ( Lake et al. , 2015 ) , where a neural network is trained to readily learn to classify inputs based on only one or a few training examples .
There are two categories of meta-learning : 1 . learning a meta-policy for updating model parameters ( see , e.g. , Andrychowicz et al. , 2016 ; Ha et al. , 2016a ; Mishra et al. , 2017 ) 2 . learning a good parameter initialization for fast adaptation ( see , e.g. , Finn et al. , 2017 ; Vinyals et al. , 2016 ; Snell et al. , 2017 ) .
In this paper , we propose to use a meta-learning algorithm for low-resource neural machine translation based on the second category .
More specifically , we extend the idea of model- agnostic metalearning ( MAML , Finn et al. , 2017 ) in the multilingual scenario .
Meta Learning for Low-Resource Neural Machine Translation
The underlying idea of MAML is to use a set of source tasks T 1 , . . . , T K to find the initialization of parameters ?
0 from which learning a target task T 0 would require only a small number of training examples .
In the context of machine translation , this amounts to using many high- resource language pairs to find good initial parameters and training a new translation model on a low-resource language starting from the found initial parame -
For each episode , one task ( language pair ) is sampled for meta-learning .
The boxes and arrows in blue are mainly involved in language -specific learning ( ?3.1 ) , and those in purple in meta-learning ( ?3.2 ) .
ters .
This process can be understood as ? ? = Learn( T 0 ; MetaLearn( T 1 , . . . , T K ) ) .
That is , we meta-learn the initialization from auxiliary tasks and continue to learn the target task .
We refer the proposed meta-learning method for NMT to MetaNMT .
See Fig. 1 for the overall illustration .
Learn : language-specific learning Given any initial parameters ?
0 ( which can be either random or meta-learned ) , the prior distribution of the parameters of a desired NMT model can be defined as an isotropic Guassian : ? i ? N (? 0 i , 1 / ) , where 1 / is a variance .
With this prior distribution , we formulate the language -specific learning process Learn( D T ; ? 0 ) as maximizing the logposterior of the model parameters given data D T : Learn( D T ; ? 0 ) = arg max ? L D T ( ? ) = arg max ?
X ( X , Y ) 2D
T log p( Y | X , ? ) k? ? 0 k 2 , where we assume p( X| ? ) to be uniform .
The first term above corresponds to the maximum likelihood criterion often used for training a usual NMT system .
The second term discourages the newly learned model from deviating too much from the initial parameters , alleviating the issue of overfitting when there is not enough training data .
In practice , we solve the problem above by maximizing the first term with gradient - based optimization and early - stopping after only a few update steps .
Thus , in the low-resource scenario , finding a good initialization ?
0 strongly correlates the final performance of the resulting model .
MetaLearn
We find the initialization ?
0 by repeatedly simulating low-resource translation scenarios using auxiliary , high- resource language pairs .
Following Finn et al. ( 2017 ) , we achieve this goal by defining the meta-objective function as L ( ? )
=E k E D T k , D 0 T k ( 2 ) 2 6 4 X ( X , Y ) 2D 0 T k log p( Y | X ; Learn ( D T k ; ? ) )
3 7 5 , where k ? U ( { 1 , . . . , K} ) refers to one metalearning episode , and D T , D 0 T follow the uniform distribution over T 's data .
We maximize the meta-objective function using stochastic approximation ( Robbins and Monro , 1951 ) with gradient descent .
For each episode , we uniformly sample one source task at random , T k .
We then sample two subsets of training examples independently from the chosen task , D T k and D 0 T k .
We use the former to simulate languagespecific learning and the latter to evaluate its outcome .
Assuming a single gradient step is taken only the with learning rate ? , the simulation is : ? 0 k = Learn( D T k ; ? ) = ? ?r ? L D T k ( ? ) .
Once the simulation of learning is done , we evaluate the updated parameters ?
0 k on D 0 T k ,
The gradient computed from this evaluation , which we refer to as meta-gradient , is used to update the ? ? ? 0 X k r ? L D 0 T k ( ? 0 k ) , where ?
0 is the meta learning rate .
Unlike a usual learning scenario , the resulting model ?
0 from this meta-learning procedure is not necessarily a good model on its own .
It is however a good starting point for training a good model using only a few steps of learning .
In the context of machine translation , this procedure can be understood as finding the initialization of a neural machine translation system that could quickly adapt to a new language pair by simulating such a fast adaptation scenario using many high- resource language pairs .
Meta-Gradient
We use the following approximation property H( x ) v ? r( x + ?v ) r( x ) ? to approximate the meta-gradient : 1 r ? L D 0 (? 0 ) = r ? 0 L D 0 ( ? 0 ) r ? ( ? ?r ? L D ( ? ) ) = r ? 0 L D 0 (? 0 ) ?r ? 0 L D 0 (? 0 ) H ? ( L D ( ? ) ) ? r ? 0 L D 0 (? 0 ) ? ? ? r ? L D ( ? ) ? r ? L D ( ? ) ? , where ? is a small constant and ? = ? + ?r ? 0 L D 0 ( ? 0 ) .
In practice , we find that it is also possible to ignore the second-order term , ending up with the following simplified update rule : r ? L D 0 ( ? 0 ) ? r ? 0 L D 0 ( ? 0 ) . ( 3 ) 1 We omit the subscript k for simplicity .
Related Work : Multilingual Transfer Learning
The proposed MetaNMT differs from the existing framework of multilingual translation ( Lee et al. , 2016 ; Johnson et al. , 2016 ; Gu et al. , 2018 b ) or transfer learning ( Zoph et al. , 2016 ) .
The latter can be thought of as solving the following problem : max ? L multi ( ? ) = E k 2 4 X ( X , Y ) 2D k log p( Y | X ; ?) 3 5 , where D k is the training set of the k-th task , or language pair .
The target low-resource language pair could either be a part of joint training or be trained separately starting from the solution ?
0 found from solving the above problem .
The major difference between the proposed MetaNMT and these multilingual transfer approaches is that the latter do not consider how learning happens with the target , low-resource language pair .
The former explicitly incorporates the learning process within the framework by simulating it repeatedly in Eq. ( 2 ) .
As we will see later in the experiments , this results in a substantial gap in the final performance on the low-resource task .
Illustration In Fig. 2 , we contrast transfer learning , multilingual learning and meta-learning using three source language pairs ( Fr- En , Es - En and Pt-En ) and two target pairs ( Ro - En and Lv-En ) .
Transfer learning trains an NMT system specifically for a source language pair ( Es - En ) and finetunes the system for each target language pair ( Ro - En , Lv-En ) .
Multilingual learning often trains a single NMT system that can handle many different language pairs ( Fr- En , Pt-En , Es -En ) , which may or may not include the target pairs ( Ro - En , Lv-En ) .
If not , it finetunes the system for each target pair , similarly to transfer learning .
Both of these however aim at directly solving the source tasks .
On the other hand , meta-learning trains the NMT system to be useful for fine-tuning on various tasks including the source and target tasks .
This is done by repeatedly simulating the learning process on low-resource languages using many high- resource language pairs ( Fr- En , Pt-En , Es -En ) .
Unified Lexical Representation I/O mismatch across language pairs
One major challenge that limits applying meta-learning for low resource machine translation is that the approach outlined above assumes the input and output spaces are shared across all the source and target tasks .
This , however , does not apply to machine translation in general due to the vocabulary mismatch across different languages .
In multilingual translation , this issue has been tackled by using a vocabulary of sub-words ( Sennrich et al. , 2015 ) or characters ( Lee et al. , 2016 ) shared across multiple languages .
This surface - level sharing is however limited , as it cannot be applied to languages exhibiting distinct orthography ( e.g. , Indo-Euroepan languages vs. Korean . )
Universal Lexical Representation ( ULR )
We tackle this issue by dynamically building a vocabulary specific to each language using a keyvalue memory network ( Miller et al. , 2016 ; Gulcehre et al. , 2018 ) , as was done successfully for low-resource machine translation recently by Gu et al . ( 2018 b ) .
We start with multilingual word embedding matrices ?
k query 2 R | V k |?d pretrained on large monolingual corpora , where V k is the vocabulary of the k-th language .
These embedding vectors can be obtained with small dictionaries of seed word pairs ( Artetxe et al. , 2017a ; Smith et al. , 2017 ) or in a fully unsupervised manner ( Zhang et al. , 2017 ; Conneau et al. , 2018 ) .
We take one of these languages k 0 to build universal lexical representation consisting of a universal embedding matrix ?
u 2 R M ?d and a corresponding key matrix ? key 2 R M ?d , where M < | V 0 k |. Both ?
k query and ? key are fixed during meta-learning .
We then compute the language-specific embedding of token x from the language k as the convex sum of the universal embedding vectors by ? 0 [ x ] = M X i=1 ? i ? u [ i ] , where ? i / exp 1 ? ? key [ i ] > A ? k query [ x ] and ? is set to 0.05 .
This approach allows us to handle languages with different vocabularies using a fixed number of shared parameters ( ?
u , ? key and A. ) tuning on a small corpus which contains a limited set of unique tokens in the target language , as it could adversely influence the other tokens ' embedding vectors .
We thus estimate the change to each embedding vector induced by languagespecific learning by a separate parameter ? k [ x ] : Learning ? k [ x ] = ? 0 [ x ] + ? k [ x ] .
During language -specific learning , the ULR ? 0 [ x ] is held constant , while only ? k [ x ] is updated , starting from an all- zero vector .
On the other hand , we hold ? k [ x ] 's constant while updating ?
u and A during the meta-learning stage .
Experimental Settings
Dataset Target Tasks
We show the effectiveness of the proposed meta-learning method for low resource NMT with extremely limited training examples on five diverse target languages : Romanian ( Ro ) from WMT '16 , 2 Latvian ( Lv ) , Finnish ( Fi ) , Turkish ( Tr ) from WMT ' 17 , 3 and Korean ( Ko ) from Korean Parallel Dataset .
4
We use the officially provided train , dev and test splits for all these languages .
The statistics of these languages are presented in Table 1 .
We simulate the low-resource translation scenarios by randomly sub-sampling the training set with different sizes .
Source Tasks
We use the following languages from Europarl 5 : Bulgarian ( Bg ) , Czech ( Cs ) , Danish ( Da ) , German ( De ) , Greek ( El ) , Spanish ( Es ) , Estonian ( Et ) , French ( Fr ) , Hungarian ( Hu ) , Italian ( It ) , Lithuanian ( Lt ) , Dutch ( Nl ) , Polish ( Pl ) , Portuguese ( Pt ) , Slovak ( Sk ) , Slovene ( Sl ) and Swedish ( Sv ) , in addition to Russian ( Ru ) 6 to learn the intilization for fine-tuning .
In our experiments , different combinations of source tasks are explored to see the effects from the source tasks .
Validation
We pick either Ro - En or Lv-En as a validation set for meta-learning and test the generalization capability on the remaining target tasks .
This allows us to study the strict form of metalearning , in which target tasks are unknown during both training and model selection .
Preprocessing and ULR Initialization
As described in ?3.3 , we initialize the query embedding vectors ?
k query of all the languages .
For each language , we use the monolingual corpora built from Wikipedia 7 and the parallel corpus .
The concatenated corpus is first tokenized and segmented using byte-pair encoding ( BPE , Sennrich et al. , 2016 ) , resulting in 40 , 000 subwords for each language .
We then estimate word vectors using fast - Text ( Bojanowski et al. , 2016 ) and align them across all the languages in an unsupervised way using MUSE ( Conneau et al. , 2018 ) to get multilingual word vectors .
We use the multilingual word vectors of the 20,000 most frequent words in English to form the universal embedding matrix ? u .
Model and Learning Model
We utilize the recently proposed Transformer ( Vaswani et al. , 2017 ) as an underlying NMT system .
We implement Transformer in this paper based on ( Gu et al. , 2018a ) 8 and modify it to use the universal lexical representation from ?3.3 .
We use the default set of hyperparameters ( d model = d hidden = 512 , n layer = 6 , n head = 8 , n batch = 4000 , t warmup = 16000 ) for all the language pairs and across all the experimental settings .
We refer the readers to ( Vaswani et al. , 2017 ; Gu et al. , 2018a ) for the details of the model .
However , since the proposed metalearning method is model- agnostic , it can be easily extended to any other NMT architectures , e.g. RNN - based sequence - to-sequence models with attention ( Bahdanau et al. , 2015 ) . Learning
We meta-learn using various sets of source languages to investigate the effect of source task choice .
For each episode , by default , we use a single gradient step of language -specific learning with Adam ( Kingma and Ba , 2014 ) per computing the meta-gradient , which is computed by the first-order approximation in Eq. ( 3 ) .
For each target task , we sample training examples to form a low-resource task .
We build tasks of 4 k , 16 k , 40 k and 160k English tokens for each language .
We randomly sample the training set five times for each experiment and report the average score and its standard deviation .
Each fine- tuning is done on a training set , early - stopped on a validation set and evaluated on a test set .
In default without notation , datasets of 16 k tokens are used .
Fine-tuning Strategies
The transformer consists of three modules ; embedding , encoder and decoder .
We update all three modules during metalearning , but during fine-tuning , we can selectively tune only a subset of these modules .
Following ( Zoph et al. , 2016 ) , we consider three fine-tuning strategies ; ( 1 ) fine - tuning all the modules ( all ) , ( 2 ) fine - tuning the embedding and encoder , but freezing the parameters of the decoder ( emb+enc ) and ( 3 ) fine -tuning the embedding only ( emb ) .
Results vs. Multilingual Transfer Learning
We metalearn the initial models on all the source tasks using either Ro - En or Lv-En as a validation task .
We also train the initial models to be multilingual translation systems .
We fine- tune them using the four target tasks ( Ro - En , Lv-En , Fi-En and Tr-En ; 16 k tokens each ) and compare the proposed meta-learning strategy and the multilingual , transfer learning strategy .
As presented in Fig. 3 , the proposed learning approach significantly outperforms the multilingual , transfer learning strategy across all the target tasks regardless of which target task was used for early stopping .
We also notice that the emb+enc strategy is most effective for both meta-learning and transfer learning approaches .
With the proposed meta-learning and emb+enc fine-tuning , the final NMT systems trained using only a fraction of all available training examples achieve 2/3 ( Ro - En ) and 1/2 ( Lv-En , Fi-En and Tr-En ) of the BLEU score achieved by the models trained with full training sets .
vs. Statistical Machine Translation
We also test the same Ro - En datasets with 16 , 000 target tokens using the default setting of Phrase - based MT ( Moses ) with the dev set for adjusting the parameters and the test set for calculating the final performance .
We obtain 4.79 ( ?0.234 ) BLEU point , which is higher than the standard NMT performance ( 0 BLEU ) .
It is however still lower than both the multi-NMT and meta- NMT .
Impact of Validation Tasks Similarly to training any other neural network , meta-learning still requires early - stopping to avoid overfitting to a specific set of source tasks .
In doing so , we observe that the choice of a validation task has nonnegligible impact on the final performance .
For instance , as shown in Fig. 3 , Fi-En benefits more when Ro - En is used for validation , while the opposite happens with Tr-En .
The relationship between the task similarity and the impact of a validation task must be investigated further in the future .
Training Set Size
We vary the size of the target task 's training set and compare the proposed meta-learning strategy and multilingual , transfer learning strategy .
We use the emb+enc fine-tuning on Ro- En and Fi-En .
Fig.
4 demonstrates that the meta-learning approach is more robust to the drop in the size of the target task 's training set .
The gap between the meta-learning and transfer learning grows as the size shrinks , confirming the effectiveness of the proposed approach on extremely lowresource language pairs .
Figure 5 : The learning curves of BLEU scores on the validation task ( Ro - En ) .
Impact of Source Tasks In Table 2 , we present the results on all five target tasks obtained while varying the source task set .
We first see that it is always beneficial to use more source tasks .
Although the impact of adding more source tasks varies from one language to another , there is up to 2 ?
improvement going from one source task to 18 source tasks ( Lv- En , Fi-En , Tr- En and Ko-En ) .
The same trend can be observed even without any fine-tuning ( i.e. , unsupervised translation , ( Lample et al. , 2017 ; Artetxe et al. , 2017 b ) ) .
In addition , the choice of source languages has different implications for different target languages .
For instance , Ro - En benefits more from { Es , Fr , It , Pt} than from { De , Ru} , while the opposite effect is observed with all the other target tasks .
Training Curves
The benefit of meta-learning over multilingual translation is clearly demonstrated when we look at the training curves in Fig.
5 .
With the multilingual , transfer learning ap- proach , we observe that training rapidly saturates and eventually degrades , as the model overfits to the source tasks .
MetaNMT on the other hand continues to improve and never degrades , as the metaobjective ensures that the model is adequate for fine- tuning on target tasks rather than for solving the source tasks .
Sample Translations
We present some sample translations from the tested models in Table 3 .
Inspecting these examples provides the insight into the proposed meta-learning algorithm .
For instance , we observe that the meta-learned model without any fine-tuning produces a word- by - word translation in the first example ( Tr -En ) , which is due to the successful use of the universal lexcial representation and the meta-learned initialization .
The system however cannot reorder tokens from Turkish to English , as it has not seen any training example of Tr-En .
After seeing around 600 sentence pairs ( 16 K English tokens ) , the model rapidly learns to correctly reorder tokens to form a better translation .
A similar phenomenon is observed in the Ko- En example .
These cases could be found across different language pairs .
Conclusion
In this paper , we proposed a meta-learning algorithm for low-resource neural machine translation that exploits the availability of high- resource languages pairs .
We based the proposed algorithm on the recently proposed model - agnostic metalearning and adapted it to work with multiple languages that do not share a common vocabulary using the technique of universal lexcal representation , resulting in MetaNMT .
Our extensive evaluation , using 18 high- resource source tasks and 5 low-resource target tasks , has shown that the proposed MetaNMT significantly outperforms the existing approach of multilingual , transfer learning in low-resource neural machine translation across all the language pairs considered .
The proposed approach opens new opportunities for neural machine translation .
First , it is a principled framework for incorporating various extra sources of data , such as source - and targetside monolingual corpora .
Second , it is a generic framework that can easily accommodate existing and future neural machine translation systems .
Figure 1 : 1 Figure1 : The graphical illustration of the training process of the proposed MetaNMT .
For each episode , one task ( language pair ) is sampled for meta-learning .
The boxes and arrows in blue are mainly involved in language -specific learning ( ?3.1 ) , and those in purple in meta-learning ( ?3.2 ) .
