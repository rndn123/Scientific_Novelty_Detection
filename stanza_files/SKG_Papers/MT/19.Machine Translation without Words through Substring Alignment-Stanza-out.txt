title
Machine Translation without Words through Substring Alignment
abstract
In this paper , we demonstrate that accurate machine translation is possible without the concept of " words , " treating MT as a problem of transformation between character strings .
We achieve this result by applying phrasal inversion transduction grammar alignment techniques to character strings to train a character - based translation model , and using this in the phrase - based MT framework .
We also propose a look - ahead parsing algorithm and substring - informed prior probabilities to achieve more effective and efficient alignment .
In an evaluation , we demonstrate that character - based translation can achieve results that compare to word - based systems while effectively translating unknown and uncommon words over several language pairs .
Introduction
Traditionally , the task of statistical machine translation ( SMT ) is defined as translating a source sentence f J 1 = {f 1 , . . . , f J } to a target sentence e I 1 = {e 1 , . . . , e I } , where each element of f J 1 and e I 1 is assumed to be a word in the source and target languages .
However , the definition of a " word " is often problematic .
The most obvious example of this lies in languages that do not separate words with white space such as Chinese , Japanese , or Thai , in which the choice of a segmentation standard has a large effect on translation accuracy .
Even for languages with explicit word
The first author is now affiliated with the Nara Institute of Science and Technology .
boundaries , all machine translation systems perform at least some precursory form of tokenization , splitting punctuation and words to prevent the sparsity that would occur if punctuated and non-punctuated words were treated as different entities .
Sparsity also manifests itself in other forms , including the large vocabularies produced by morphological productivity , word compounding , numbers , and proper names .
A myriad of methods have been proposed to handle each of these phenomena individually , including morphological analysis , stemming , compound breaking , number regularization , optimizing word segmentation , and transliteration , which we outline in more detail in Section 2 .
These difficulties occur because we are translating sequences of words as our basic unit .
On the other hand , Vilar et al . ( 2007 ) examine the possibility of instead treating each sentence as sequences of characters to be translated .
This method is attractive , as it is theoretically able to handle all sparsity phenomena in a single unified framework , but has only been shown feasible between similar language pairs such as Spanish - Catalan ( Vilar et al. , 2007 ) , Swedish -Norwegian ( Tiedemann , 2009 ) , and Thai-Lao ( Sornlertlamvanich et al. , 2008 ) , which have a strong co-occurrence between single characters .
As Vilar et al. ( 2007 ) state and we confirm , accurate translations cannot be achieved when applying traditional translation techniques to character - based translation for less similar language pairs .
In this paper , we propose improvements to the alignment process tailored to character - based machine translation , and demonstrate that it is , in fact , possible to achieve translation accuracies that ap- proach those of traditional word - based systems using only character strings .
We draw upon recent advances in many - to - many alignment , which allows for the automatic choice of the length of units to be aligned .
As these units may be at the character , subword , word , or multi-word phrase level , we conjecture that this will allow for better character alignments than one - to -many alignment techniques , and will allow for better translation of uncommon words than traditional word - based models by breaking down words into their component parts .
We also propose two improvements to the manyto-many alignment method of .
One barrier to applying many - to - many alignment models to character strings is training cost .
In the inversion transduction grammar ( ITG ) framework ( Wu , 1997 ) , which is widely used in many - to - many alignment , search is cumbersome for longer sentences , a problem that is further exacerbated when using characters instead of words as the basic unit .
As a step towards overcoming this difficulty , we increase the efficiency of the beam-search technique of Saers et al . ( 2009 ) by augmenting it with look - ahead probabilities in the spirit of A* search .
Secondly , we describe a method to seed the search process using counts of all substring pairs in the corpus to bias the phrase alignment model .
We do this by defining prior probabilities based on these substring counts within the Bayesian phrasal ITG framework .
An evaluation on four language pairs with differing morphological properties shows that for distant language pairs , character - based SMT can achieve translation accuracy comparable to word - based systems .
In addition , we perform ablation studies , showing that these results were not possible without the proposed enhancements to the model .
Finally , we perform a qualitative analysis , which finds that character - based translation can handle unsegmented text , conjugation , and proper names in a unified framework with no additional processing .
Related Work on Data Sparsity in SMT
As traditional SMT systems treat all words as single tokens without considering their internal structure , major problems of data sparsity occur for less frequent tokens .
In fact , it has been shown that there is a direct negative correlation between vocabulary size ( and thus sparsity ) of a language and translation accuracy ( Koehn , 2005 ) .
Sparsity causes trouble for alignment models , both in the form of incorrectly aligned uncommon words , and in the form of garbage collection , where uncommon words in one language are incorrectly aligned to large segments of the sentence in the other language ( Och and Ney , 2003 ) .
Unknown words are also a problem during the translation process , and the default approach is to map them as - is into the target sentence .
This is a major problem in agglutinative languages such as Finnish or compounding languages such as German .
Previous works have attempted to handle morphology , decompounding and regularization through lemmatization , morphological analysis , or unsupervised techniques ( Nie?en and Ney , 2000 ; Brown , 2002 ; Lee , 2004 ; Goldwater and McClosky , 2005 ; Talbot and Osborne , 2006 ; Mermer and Ak?n , 2010 ; Macherey et al. , 2011 ) .
It has also been noted that it is more difficult to translate into morphologically rich languages , and methods for modeling target -side morphology have attracted interest in recent years ( Bojar , 2007 ; Subotin , 2011 ) .
Another source of data sparsity that occurs in all languages is proper names , which have been handled by using cognates or transliteration to improve translation ( Knight and Graehl , 1998 ; Kondrak et al. , 2003 ; Finch and Sumita , 2007 ) , and more sophisticated methods for named entity translation that combine translation and transliteration have also been proposed ( Al - Onaizan and Knight , 2002 ) .
Choosing word units is also essential for creating good translation results for languages that do not explicitly mark word boundaries , such as Chinese , Japanese , and Thai .
A number of works have dealt with this word segmentation problem in translation , mainly focusing on Chinese- to - English translation ( Bai et al. , 2008 ; Zhang et al. , 2008 b ; Chung and Gildea , 2009 ; Nguyen et al. , 2010 ) , although these works generally assume that a word segmentation exists in one language ( English ) and attempt to optimize the word segmentation in the other language ( Chinese ) .
We have enumerated these related works to demonstrate the myriad of data sparsity problems and proposed solutions .
Character - based translation has the potential to handle all of the phenomena in the previously mentioned research in a single unified framework , requiring no language specific tools such as morphological analyzers or word segmenters .
However , while the approach is attractive conceptually , previous research has only been shown effective for closely related language pairs ( Vilar et al. , 2007 ; Tiedemann , 2009 ; Sornlertlamvanich et al. , 2008 ) .
In this work , we propose effective alignment techniques that allow character - based translation to achieve accurate translation results for both close and distant language pairs .
Alignment Methods SMT systems are generally constructed from a parallel corpus consisting of target language sentences E and source language sentences F .
The first step of training is to find alignments A for the words in each sentence pair .
We represent our target and source sentences as e I 1 and f J 1 . e i and f j represent single elements of the target and source sentences respectively .
These may be words in word - based alignment models or single characters in character - based alignment models .
1
We define our alignment as a K 1 , where each element is a span a k = s , t , u , v indicating that the target string e s , . . . , e t and source string f u , . . . , f v are aligned to each-other .
One-to - Many Alignment
The most well -known and widely - used models for bitext alignment are for one - to - many alignment , including the IBM models ( Brown et al. , 1993 ) and HMM alignment model ( Vogel et al. , 1996 ) .
These models are by nature directional , attempting to find the alignments that maximize the conditional probability of the target sentence P ( e I 1 |f J 1 , a K 1 ) .
For computational reasons , the IBM models are restricted to aligning each word on the target side to a single word on the source side .
In the formalism presented above , this means that each e i must be included in at most one span , and for each span u = v. Traditionally , these models are run in both directions and combined using heuristics to create many - to - many alignments ( Koehn et al. , 2003 ) .
However , in order for one - to - many alignment methods to be effective , each f j must contain enough information to allow for effective alignment with its corresponding elements in e I 1 .
While this is often the case in word - based models , for characterbased models this assumption breaks down , as there is often no clear correspondence between characters .
Many- to- Many Alignment
On the other hand , in recent years , there have been advances in many - to - many alignment techniques that are able to align multi-element chunks on both sides of the translation ( Marcu and Wong , 2002 ; DeNero et al. , 2008 ; Blunsom et al. , 2009 ; . Many - to-many methods can be expected to achieve superior results on character - based alignment , as the aligner can use information about substrings , which may correspond to letters , morphemes , words , or short phrases .
Here , we focus on the model presented by , which uses Bayesian inference in the phrasal inversion transduction grammar ( ITG , Wu ( 1997 ) ) framework .
ITGs are a variety of synchronous context free grammar ( SCFG ) that allows for many - to - many alignment to be achieved in polynomial time through the process of biparsing , which we explain more in the following section .
Phrasal ITGs are ITGs that allow for non-terminals that can emit phrase pairs with multiple elements on both the source and target sides .
It should be noted that there are other many - to - many alignment methods that have been used for simultaneously discovering morphological boundaries over multiple languages ( Snyder and Barzilay , 2008 ; Naradowsky and Toutanova , 2011 ) , but these have generally been applied to single words or short phrases , and it is not immediately clear that they will scale to aligning full sentences .
Look - Ahead Biparsing
In this work , we experiment with the alignment method of , which can achieve competitive accuracy with a much smaller phrase table than traditional methods .
This is important in the character - based translation context , as we would like to use phrases that contain large numbers of characters without creating a phrase table so large that it cannot be used in actual decoding .
In this framework , training is performed using sentence -
Lightly and darkly shaded spans will be trimmed when the beam is log ( P ) ? ?3 and log ( P ) ? ?6 respectively .
wise block sampling , acquiring a sample for each sentence by first performing bottom - up biparsing to create a chart of probabilities , then performing topdown sampling of a new tree based on the probabilities in this chart .
An example of a chart used in this parsing can be found in Figure 1 ( a ) .
Within each cell of the chart spanning e t s and f v u is an " inside " probability I ( a s,t , u , v ) .
This probability is the combination of the generative probability of each phrase pair P t ( e t s , f v u ) as well as the sum the probabilities over all shorter spans in straight and inverted order 2 I ( a s,t , u , v ) = P t ( e t s , f v u ) + s?S?t u?U ?v P x ( str ) I ( a s,S , u , U ) I ( a S,t , U , v ) + s?S?t u?U ?v P x ( inv ) I ( a s,S , U , v ) I ( a S , t , u , U ) where P x ( str ) and P x ( inv ) are the probability of straight and inverted ITG productions .
While the exact calculation of these probabilities can be performed in O(n 6 ) time , where n is the length of the sentence , this is impractical for all but the shortest sentences .
Thus it is necessary to use methods to reduce the search space such as beamsearch based chart parsing ( Saers et al. , 2009 ) or slice sampling ( Blunsom and Cohn , 2010 ) .
3
In this section we propose the use of a look - ahead probability to increase the efficiency of this chart parsing .
Taking the example of Saers et al . ( 2009 ) , spans are pushed onto a different queue based on their size , and queues are processed in ascending order of size .
Agendas can further be trimmed based on a histogram beam ( Saers et al. , 2009 ) or probability beam compared to the best hypothesis ?.
In other words , we have a queue discipline based on the inside probability , and all spans a k where I ( a k ) < cI ( ? ) are pruned .
c is a constant describing the width of the beam , and a smaller constant probability will indicate a wider beam .
This method is insensitive to the existence of competing hypotheses when performing pruning .
Figure 1 ( a ) provides an example of why it is unwise to ignore competing hypotheses during beam pruning .
Particularly , the alignment " les / 1960s " competes with the high- probability alignment " les / the , " so intuitively should be a good candidate for pruning .
However its probability is only slightly higher than " ann ?es /1960s , " which has no competing hypotheses and thus should not be trimmed .
In order to take into account competing hypotheses , we can use for our queue discipline not only the inside probability I ( a k ) , but also the outside probability O( a k ) , the probability of generating all spans other than a k , as in A* search for CFGs ( Klein and Manning , 2003 ) , and tic-tac - toe pruning for wordbased ITGs ( Zhang and Gildea , 2005 ) .
As the calculation of the actual outside probability O( a k ) is just as expensive as parsing itself , it is necessary to approximate this with heuristic function O * that can be calculated efficiently .
Here we propose a heuristic function that is designed specifically for phrasal ITGs and is computable with worst - case complexity of n 2 , compared with the n 3 amortized time of the tic-tac - toe pruning algorithm described by ( Zhang et al. , 2008a ) .
During the calculation of the phrase generation probabilities P t , we save the best inside probability I * for each monolingual span .
I * e ( s , t ) = max {? = s , t , ? , ? ; s=s , t=t } P t ( ? )
I * f ( u , v ) = max {? = s , t , ? , ? ;?=u , ?=v} P t ( ? )
For each language independently , we calculate forward probabilities ? and backward probabilities ?.
For Backwards probabilities and probabilities over f can be defined similarly .
These probabilities are calculated for e and f independently , and can be calculated in n 2 time by processing each ?
in ascending order , and each ? in descending order in a fashion similar to that of the forward - backward algorithm .
Finally , for any span , we define the outside heuristic as the minimum of the two independent look - ahead probabilities over each language O * ( a s,t , u, v ) = min (?
e ( s ) * ? e ( t ) , ? f ( u ) * ? f ( v ) ) .
Looking again at Figure 1 ( b ) , it can be seen that the relative probability difference between the highest probability span " les / the " and the spans " ann ?es/1960s " and " 60/1960s " decreases , allowing for tighter beam pruning without losing these good hypotheses .
In contrast , the relative probability of " les / 1960s " remains low as it is in conflict with a high - probability alignment , allowing it to be discarded .
Substring Prior Probabilities
While the Bayesian phrasal ITG framework uses the previously mentioned phrase distribution P t during search , it also allows for definition of a phrase pair prior probability P prior ( e t s , f v u ) , which can efficiently seed the search process with a bias towards phrase pairs that satisfy certain properties .
In this section , we overview an existing method used to calculate these prior probabilities , and also propose a new way to calculate priors based on substring cooccurrence statistics .
Word - based Priors Previous research on many - to -many translation has used IBM model 1 probabilities to bias phrasal alignments so that phrases whose member words are good translations are also aligned .
As a representative of this existing method , we adopt a base measure similar to that used by DeNero et al .
( 2008
P pois is the Poisson distribution with the average length parameter ? , which we set to 0.01 .
P m1 is the word- based ( or character - based ) Model 1 probability , which can be efficiently calculated using the dynamic programming algorithm described by Brown et al . ( 1993 ) .
However , for reasons previously stated in Section 3 , these methods are less satisfactory when performing character - based alignment , as the amount of information contained in a character does not allow for proper alignment .
Substring Co-occurrence Priors Instead , we propose a method for using raw substring co-occurrence statistics to bias alignments towards substrings that often co-occur in the entire training corpus .
This is similar to the method of Cromieres ( 2006 ) , but instead of using these cooccurrence statistics as a heuristic alignment criterion , we incorporate them as a prior probability in a statistical model that can take into account mutual exclusivity of overlapping substrings in a sentence .
We define this prior probability using three counts over substrings c ( e ) , c( f ) , and c(e , f ) . c( e ) and c( f ) count the total number of sentences in which the substrings e and f occur respectively .
c( e , f ) is a count of the total number of sentences in which the substring e occurs on the target side , and f occurs on the source side .
We perform the calculation of these statistics using enhanced suffix arrays , a data structure that can efficiently calculate all substrings in a corpus ( Abouelhoda et al. , 2004 ) . 4
While suffix arrays allow for efficient calculation of these statistics , storing all co-occurrence counts c( e , f ) is an unrealistic memory burden for larger corpora .
In order to reduce the amount of memory used , we discount every count by a constant d , which we set to 5 .
This has a dual effect of reducing the amount of memory needed to hold co-occurrence counts by removing values for which c( e , f ) < d , as well as preventing over-fitting of the training data .
In addition , we heuristically prune values for which the conditional probabilities P ( e|f ) or P ( f |e ) are less than some fixed value , which we set to 0.1 for the reported experiments .
To determine how to combine c ( e ) , c( f ) , and c(e , f ) into prior probabilities , we performed preliminary experiments testing methods proposed by previous research including plain co-occurrence counts , the Dice coefficient , and ?-squared statistics ( Cromieres , 2006 ) , as well as a new method of defining substring pair probabilities to be proportional to bidirectional conditional probabilities P cooc ( e , f ) = P cooc ( e|f ) P cooc ( f |e ) / Z = c( e , f ) ? d c( f ) ? d c( e , f ) ? d c ( e ) ?
d / Z for all substring pairs where c(e , f ) > d and where Z is a normalization term equal to Z = {e , f ; c ( e , f ) >d}
P cooc ( e|f ) P cooc ( f |e ) .
The experiments showed that the bidirectional conditional probability method gave significantly better results than all other methods , so we adopt this for the remainder of our experiments .
It should be noted that as we are using discounting , many substring pairs will be given zero probability according to P cooc .
As the prior is only supposed to bias the model towards good solutions and not explicitly rule out any possibilities , we linearly interpolate the co-occurrence probability with the one-to- many Model 1 probability , which will give at least some probability mass to all substring pairs P prior ( e , f ) = ? P cooc ( e , f ) + ( 1 ? ?) P m1 ( e , f ) .
We put a Dirichlet prior ( ? = 1 ) on the interpolation coefficient ? and learn it during training .
Experiments
In order to test the effectiveness of character - based translation , we performed experiments over a variety of language pairs and experimental settings .
Experimental Setup
We use a combination of four languages with English , using freely available data .
We selected French-English , German-English , Finnish -English data from EuroParl ( Koehn , 2005 ) , with development and test sets designated for the 2005 ACL shared task on machine translation .
5 We also did experiments with Japanese -English Wikipedia articles from the Kyoto Free Translation Task ( Neubig , 2011 ) using the designated training and tuning sets , and reporting results on the test set .
These languages were chosen as they have a variety of interesting characteristics .
French has some inflection , but among the test languages has the strongest oneto- one correspondence with English , and is generally considered easy to translate .
German has many compound words , which must be broken apart to translate properly into English .
Finnish is an agglutinative language with extremely rich morphology , resulting in long words and the largest vocabulary of the languages in EuroParl .
Japanese does not have any clear word boundaries , and uses logographic characters , which contain more information than phonetic characters .
With regards to data preparation , the EuroParl data was pre-tokenized , so we simply used the tokenized data as - is for the training and evaluation of all models .
For word - based translation in the Kyoto task , training was performed using the provided tokenization scripts .
For character - based translation , no tokenization was performed , using the original text for both training and decoding .
For both tasks , we selected as training data all sentences for which both de-en fi-en fr-en ja-en GIZA - word 24.58 / 64 . 28 / 30.43 ITG models for word and character - based translation , with bold numbers indicating a statistically insignificant difference from the best system according to the bootstrap resampling method at p = 0.05 ( Koehn , 2004 ) . source and target were 100 characters or less , 6 the total size of which is shown in Table 1 .
In characterbased translation , white spaces between words were treated as any other character and not given any special treatment .
Evaluation was performed on tokenized and lower - cased data .
For alignment , we use the GIZA ++ implementation of one- to - many alignment 7 and the pialign implementation of the phrasal ITG models 8 modified with the proposed improvements .
For GIZA + + , we used the default settings for word - based alignment , but used the HMM model for character - based alignment to allow for alignment of longer sentences .
For pialign , default settings were used except for character - based ITG alignment , which used a probability beam of 10 ?4 instead 10 ?10 . 9 For decoding , we use the Moses decoder , 10 using the default settings except for the stack size , which we set to 1000 instead of 200 .
Minimum error rate training was performed to maximize word - based BLEU score for all systems .
11
For language models , word - based translation uses a word 5 - gram model , and characterbased translation uses a character 12 - gram model , both smoothed using interpolated Kneser - Ney .
6 100 characters is an average of 18.8 English words 7 http://code.google.com/p/giza-pp/ 8 http://phontron.com/pialign/ 9
Improvement by using a beam larger than 10 ?4 was marginal , especially with co-occurrence prior probabilities .
10 http://statmt.org/moses/ 11
We chose this set-up to minimize the effect of tuning criterion on our experiments , although it does indicate that we must have access to tokenized data for the development set .
Quantitative Evaluation
Table 2 presents a quantitative analysis of the translation results for each of the proposed methods .
As previous research has shown that it is more difficult to translate into morphologically rich languages than into English ( Koehn , 2005 ) , we perform experiments translating in both directions for all language pairs .
We evaluate translation quality using BLEU score ( Papineni et al. , 2002 ) , both on the word and character level ( with n = 4 ) , as well as METEOR ( Denkowski and Lavie , 2011 ) on the word level .
It can be seen that character - based translation with all of the proposed alignment improvements greatly exceeds character - based translation using one - to - many alignment , confirming that substringbased information is necessary for accurate alignments .
When compared with word- based translation , character - based translation achieves better , comparable , or inferior results on character - based BLEU , comparable or inferior results on METEOR , and inferior results on word - based BLEU .
The differences between the evaluation metrics are due to the fact that character - based translation often gets words mostly correct other than one or two letters .
These are given partial credit by character - based BLEU ( and to a lesser extent METEOR ) , but marked entirely wrong by word - based BLEU .
Interestingly , for translation into English , character - based translation achieves higher accuracy compared to word - based translation on Japanese and Finnish input , followed by German , fi-en ja-en ITG - word 2.851 2.085 ITG - char 2.826 2.154 and finally French .
This confirms that characterbased translation is performing well on languages that have long words or ambiguous boundaries , and less well on language pairs with relatively strong one - to - one correspondence between words .
Qualitative Evaluation
In addition , we performed a subjective evaluation of Japanese -English and Finnish - English translations .
Two raters evaluated 100 sentences each , assigning a score of 0 - 5 based on how well the translation conveys the information contained in the reference .
We focus on shorter sentences of 8- 16 English words to ease rating and interpretation .
Table 3 shows that the results are comparable , with no significant difference in average scores for either language pair .
Table 4 shows a breakdown of the sentences for which character - based translation received a score of at 2 + points more than word-based .
It can be seen that character - based translation is properly handling sparsity phenomena .
On the other hand , word - based translation was generally stronger with reordering and lexical choice of more common words .
Effect of Alignment Method
In this section , we compare the translation accuracies for character - based translation using the phrasal ITG model with and without the proposed improvements of substring co-occurrence priors and lookahead parsing as described in Sections 4 and 5.2 . fi-en en-fi ja-en en-ja ITG + cooc + look 28 .
Table 5 : METEOR scores for alignment with and without look - ahead and co-occurrence priors .
Figure 5 shows METEOR scores 12 for experiments translating Japanese and Finnish .
It can be seen that the co-occurrence prior gives gains in all cases , indicating that substring statistics are effectively seeding the ITG aligner .
The introduced lookahead probabilities improve accuracy significantly when substring co-occurrence counts are not used , and slightly when co-occurrence counts are used .
More importantly , they allow for more aggressive beam pruning , increasing sampling speed from 1.3 sent / s to 2.5 sent / s for Finnish , and 6.8 sent / s to 11.6 sent / s for Japanese .
Conclusion and Future Directions
This paper demonstrated that character - based translation can act as a unified framework for handling difficult problems in translation : morphology , compound words , transliteration , and segmentation .
One future challenge includes scaling training up to longer sentences , which can likely be achieved through methods such as the heuristic span pruning of Haghighi et al . ( 2009 ) or sentence splitting of Vilar et al . ( 2007 ) .
Monolingual data could also be used to improve estimates of our substring - based prior .
In addition , error analysis showed that wordbased translation performed better than characterbased translation on reordering and lexical choice , indicating that improved decoding ( or pre-ordering ) and language modeling tailored to character - based translation will likely greatly improve accuracy .
Finally , we plan to explore the middle ground between word- based and character based translation , allowing for the flexibility of character - based translation , while using word boundary information to increase efficiency and accuracy .
Figure 1 : 1 Figure 1 : ( a ) A chart with inside probabilities in boxes and forward / backward probabilities marking the surrounding arrows .
( b) Spans with corresponding lookaheads added , and the minimum probability underlined .
Lightly and darkly shaded spans will be trimmed when the beam is log ( P ) ? ?3 and log ( P ) ? ?6 respectively .
