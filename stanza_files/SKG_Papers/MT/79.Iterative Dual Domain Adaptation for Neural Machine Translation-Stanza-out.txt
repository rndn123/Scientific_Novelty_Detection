title
Iterative Dual Domain Adaptation for Neural Machine Translation
abstract
Previous studies on the domain adaptation for neural machine translation ( NMT ) mainly focus on the one - pass transferring out -ofdomain translation knowledge to in- domain NMT model .
In this paper , we argue that such a strategy fails to fully extract the domainshared translation knowledge , and repeatedly utilizing corpora of different domains can lead to better distillation of domain-shared translation knowledge .
To this end , we propose an iterative dual domain adaptation framework for NMT .
Specifically , we first pretrain in- domain and out-of- domain NMT models using their own training corpora respectively , and then iteratively perform bidirectional translation knowledge transfer ( from indomain to out - of - domain and then vice versa ) based on knowledge distillation until the indomain NMT model convergences .
Furthermore , we extend the proposed framework to the scenario of multiple out - of- domain training corpora , where the above-mentioned transfer is performed sequentially between the indomain and each out - of- domain NMT models in the ascending order of their domain similarities .
Empirical results on Chinese-English and English - German translation tasks demonstrate the effectiveness of our framework .
Introduction Currently , neural machine translation ( NMT ) has become dominant in the community of machine translation due to its excellent performance ( Bahdanau et al. , 2015 ; Wu et al. , 2016 ; Vaswani et al. , 2017 ) .
With the development of NMT , prevailing NMT models become more and more complex with large numbers of parameters , which often require abundant corpora for effective training .
However , for translation tasks in most domains , domain-specific parallel sentences are of -ten scarce .
If we only use domain-specific data to train the NMT model for such a domain , the performance of resulting model is usually unsatisfying .
Therefore , NMT for low-resource domains becomes a challenge in its research and applications .
To deal with this issue , many researchers have conducted studies on the domain adaptation for NMT , which can be classified into two general categories .
One is to transfer the rich-resource domain ( out - of- domain ) translation knowledge to benefit the low-resource ( in- domain ) NMT model .
The other is to use the mixed - domain training corpus to construct a unified NMT model for all domains .
Here , we mainly focus on the first type of research , of which typical methods include finetuning ( Luong and Manning , 2015 ; Zoph et al. , 2016 ; Servan et al. , 2016 ) , mixed fine-tuning ( Chu et al. , 2017 ) , cost weighting , data selection ( Wang et al. , 2017 a , b ; Zhang et al. , 2019a ) and so on .
The underlying assumption of these approaches is that in- domain and out-ofdomain NMT models share the same parameter space or prior distributions , and the useful out -ofdomain translation knowledge can be completely transferred to in- domain NMT model in a onepass manner .
However , it is difficult to achieve this goal due to domain differences .
Particularly , when the domain difference is significant , such conventional brute-force transfer may be unsuccessful , facing the similar issue as the domain adaptation for other tasks ( Pan and Yang , 2010 ) .
In this paper , to tackle the above problem , we argue that corpora of different domains should be repeatedly utilized to fully distill domain-shared translation knowledge .
To this end , we propose a novel Iterative Dual Domain Adaptation ( IDDA ) framework for NMT .
Under this framework , we first train in- domain and out -of- domain NMT models using their own training corpora re-spectively , and then iteratively perform bidirectional translation knowledge transfer ( from indomain to out - of - domain and then vice versa ) .
In this way , both in - domain and out -of- domain NMT models are expected to constantly reinforce each other , which is likely to achieve better NMT domain adaptation .
Particularly , we employ a knowledge distillation ( Hinton et al. , 2015 ; Kim and Rush , 2016 ) based approach to transfer translation knowledge .
During this process , the targetdomain NMT model is first initialized with the source-domain NMT model , and then trained to fit its own training data and match the output of its previous best model simultaneously .
By doing so , the previously transferred translation knowledge can be effectively retained for better NMT domain adaptation .
Finally , we further extend the proposed framework to the scenario of multiple out - of- domain training corpora , where the abovementioned bidirectional knowledge transfer is performed sequentially between the in-domain and each out - of- domain NMT models in the ascending order of their domain similarities .
The contributions of this work are summarized as follows : ?
We propose an iterative dual domain adaptation framework for NMT , which is applicable to many conventional domain transfer approaches , such as fine-tune , mixed fine-tune .
Compared with previous approaches , our framework is able to better exploit domainshared translation knowledge for NMT domain adaptation .
Related Work
Our work is obviously related to the research on transferring the out-of- domain translation knowledge into the in-domain NMT model .
In this aspect , fine-tuning ( Luong and Manning , 2015 ; Zoph et al. , 2016 ; Servan et al. , 2016 ) is the most popular approach , where the NMT model is first trained using the out-of- domain training corpus , and then fine-tuned on the in-domain training corpus .
To avoid overfitting , Chu et al . ( 2017 ) blended in - domain with out - of- domain corpora to fine - tune the pre-trained model , and Freitag and Al - Onaizan ( 2016 ) combined the fine-tuned model with the baseline via ensemble method .
Meanwhile , applying data weighting into NMT domain adaptation has attracted much attention .
Wang et al. ( 2017a ) and Wang et al . ( 2017 b ) proposed several sentence and domain weighting methods with a dynamic weight learning strategy .
Zhang et al. ( 2019a ) ranked unlabeled domain training samples based on their similarity to in- domain data , and then adopts a probabilistic curriculum learning strategy during training .
applied the sentence - level cost weighting to refine the training of NMT model .
Recently , Vilar ( 2018 ) introduced a weight to each hidden unit of out -of- domain model .
Chu and Wang ( 2018 ) gave a comprehensive survey of the dominant domain adaptation techniques for NMT .
Gu et al. ( 2019 ) not only maintained a private encoder and a private decoder for each domain , but also introduced a common encoder and a common decoder shared by all domains .
Significantly different from the above methods , along with the studies of dual learning for NMT ( He et al. , 2016 ; Zhang et al. , 2019 b ) , we iteratively perform bidirectional translation knowledge transfer between in - domain and out -of- domain training corpora .
To the best of our knowledge , our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation .
Furthermore , we extend our framework to the scenario of multiple out - of- domain corpora .
Particularly , we introduce knowledge distillation into the domain adaptation for NMT and experimental results demonstrate its effectiveness , echoing its successful applications on many tasks , such as speech recognition ( Hinton et al. , 2015 ) and natural language processing ( Kim and Rush , 2016 ; Tan et al. , 2019 ) .
Besides , our work is also related to the studies ? ( 0 ) in ? TrainModel ( D in ) , ? ( 0 ) out ?
TrainModel ( D out ) 4 : ? * in ? ? ( 0 ) in , ? * out ? ? ( 0 ) out 5 : for k = 1 , 2 , ... , K do 6 : ? ( k ) out ? TransferModel ( ? ( k?1 ) in , D out , ? * out ) 7 : if EvalModel ( D v out , ? ( k ) out ) > EvalModel ( D v out , ? * out ) 8 : ? * out ? ? ( k ) out 9 : end if 10 : ? ( k ) in ? TransferModel ( ? ( k ) out , D in , ? * in ) 11 : if EvalModel ( D v in , ? ( k ) in ) > EvalModel ( D v in , ? * in ) 12 : ? * in ? ? ( k ) in 13 : end if 14 : end for of multi-domain NMT , which focus on building a unified NMT model trained on the mixed - domain training corpus for translation tasks in all domains ( Kobus et al. , 2016 ; Tars and Fishel , 2018 ; Farajian et al. , 2017 ; Pryzant et al. , 2017 ; Sajjad et al. , 2017 ; Bapna and Firat , 2019 ) .
Although our framework is also able to refine outof-domain NMT model , it is still significantly different from multi-domain NMT , since only the performance of in- domain NMT model is considered .
Finally , note that similar to our work , Tan et al . ( 2019 ) introduced knowledge distillation into multilingual NMT .
However , our work is still different from ( Tan et al. , 2019 ) in the following aspects : ( 1 ) Tan et al . ( 2019 ) mainly focused on constructing a unified NMT model for multi-lingual translation task , while we aim at how to effectively transfer out - of- domain translation knowledge to indomain NMT model ; ( 2 ) Our translation knowledge transfer is bidirectional , while the procedure of knowledge distillation in ( Tan et al. , 2019 ) is unidirectional ; ( 3 )
When using knowledge distil-lation under our framework , we iteratively update teacher models for better domain adaptation .
In contrast , all language -specific teacher NMT models in ( Tan et al. , 2019 ) remain fixed .
Iterative Dual Domain Adaptation Framework
In this section , we first detailedly describe our proposed framework for conventional one - to - one NMT domain adaptation , and then extend this framework to the scenario of multiple out-ofdomain corpora ( many-to-one ) .
One-to-one Domain Adaptation
As shown in Figure 1
To better describe our framework , we summarize the training procedure of our framework L ( 0 ) in = ( x, y ) ?
D in ?logP ( y|x ; ? ( 0 ) in ) , ( 1 ) L ( 0 ) out = ( x, y ) ?
Dout ?logP ( y|x ; ? ( 0 ) out ) . ( 2 ) Then , we iteratively perform bidirectional translation knowledge transfer to update both in - domain and out -of- domain NMT models , until the maximal iteration number K is reached ( Lines 5 - 14 ) .
More specifically , at the k-th iteration , we first transfer the translation knowledge of the previous in - domain NMT model ?
Obviously , during the above procedure , one of important steps is how to transfer the translation knowledge from one domain-specific NMT model to the other one .
However , if we directly employ conventional domain transfer approaches , such as fine-tuning , as the iterative dual domain adaptation proceeds , the previously learned translation knowledge tends to be ignored .
To deal with this issue , we introduce knowledge distillation ( Kim and Rush , 2016 ) to conduct the translation knowledge transfer .
Specifically , during the transfer process from ? ( k ) out to ? ( k ) in , we first initialize ? ( k ) in with parameters of ? ( k ) out , and then train ? ( k ) in not only to match the references of D in , but also to be consistent with probability outputs of the previous best in - domain NMT model ? * in , which is considered as the teacher model .
To this end , we define the loss function as L ( k ) in = ( x, y ) ?
D in [? ( 1 ? ? ) ? logP ( y|x ; ? ( k ) in ) + ? ? KL ( P ( y|x ; ? ( k ) in ) | | P ( y|x ; ? * in ) ) ] , ( 3 ) where ? is the coefficient used to trade off these two loss terms , and it can be tuned on the development set .
Notably , when ?=0 , only the term of likelihood function affects the model training , and thus our transfer approach degenerate into finetuning at each iteration .
In this way , we enable in - domain NMT model ? out .
Similarly , we employ the above method to transfer translation knowledge from ? ( k? 1 ) in to ? ( k ) out using out - of- domain corpus D out and the previous best out - of- domain model ? * out .
Due to the space limitation , we omit the specific description of this procedure .
Many- to-one Domain Adaptation Usually , in practical applications , there exist multiple available out -of- domain training corpora simultaneously .
As shown in Figure 2 ( a ) , previous studies usually mix them into one out - of- domain corpus , which is applicable for the conventional one - to - one NMT domain adaptation .
However , various out - of- domain corpora are semantically related to in-domain corpus to different degrees , and thus intuitively , it is difficult to adequately play their roles without distinguishing them .
To address this issue , we extend the proposed framework to many - to- one NMT domain adaptation .
Our extended framework is illustrated in Figure 2 ( b ) .
Given an in- domain corpus and N out - of- domain corpora , we first measure the semantic distance between each out - of- domain corpus and the in-domain corpus using the proxy Adistance dA = 2 ( 1 ? 2 ) ( Ganin et al. , 2015 ; Pryzant et al. , 2017 ) , where the is the generalization error of a linear bag-of- words SVM classifier trained to discriminate between the two domains .
Then , we determine the transfer order of these out-ofdomain NMT models as {?
out 1 , ? out 2 , ...? out N } , according to distances of their own training corpora to the in-domain corpus in a decreasing order .
The reason behind this step is the translation knowledge of previously transferred out -ofdomain NMT models will be partially forgotten during the continuous transfer .
By setting transfer order according to their dA values in a decreasing order , we enable the in-domain NMT model to fully preserve the translation knowledge transferred from the most relevant out - of- domain NMT model .
Finally , we sequentially perform bidirectional knowledge transfer between the in-domain and each out - of- domain models , where this process will be repeated for K iterations .
Experiments
To verify the effectiveness of our framework , we first conducted one - to - one domain adaptation experiments on Chinese - English translation , where we further investigated impacts of various factors on our framework .
Then , we carried out two -toone domain adaptation experiments on English - German translation , so as to demonstrate the generality of our framework on different language pairs and multiple out - of- domain corpora .
Setup Datasets .
In the Chinese-English translation task , our in- domain training corpus is from IWSLT2015 dataset consisting of 210K TED
Talk sentence pairs , and the out-of- domain training corpus contains 1.12 M LDC sentence pairs related to News domain .
For these two domains , we chose IWSLT dev2010 and NIST 2002 dataset as development sets .
Finally , we used IWSLT tst2010 , tst2011 and tst2012 as in- domain test sets .
Particularly , in order to verify whether our framework can enable NMT models of two domains to benefit each other , we also tested the performance of outdomain NMT model on NIST 2003 NIST , 2004 NIST , 2005 NIST , 2006 datasets .
For the English - German translation task , our training corpora totally include one in- domain dataset : 200K TED
Talk sentence pairs provided by IWSLT2015 , and two out -of- domain datasets : 500 K sentence pairs ( News topic ) extracted from WMT2014 corpus , and 500K sentence pairs ( Medical topic ) that are sampled from OPUS EMEA corpus 2 .
As for development sets , we chose IWSLT tst2012 , WMT tst2012 and 1 K sampled sentence pairs of OPUS EMEA corpus , respectively .
In addition , IWSLT tst2013 , tst2014 were used as in- domain test sets , WMT news - test2014 ( News topic ) and 1 K sampled sentence pairs of OPUS EMEA corpus were used as two out-ofdomain test sets .
We first employed Stanford Segmenter 3 to conduct word segmentation on Chinese sentences and MOSES script 4 to tokenize English and German sentences .
Then , we limited the length of sentences to 50 words in the training stage .
Besides , we employed Byte Pair Encoding ( Sennrich et al. , 2016 ) to split words into subwords and set the vocabulary size for both Chinese -English and English - German as 32,000 .
We evaluated the translation quality with BLEU scores ( Papineni et al. , 2002 ) as calculated by multi-bleu.
perl script .
Settings .
We chose Transformer ( Vaswani et al. , 2017 ) as our NMT model , which exhibits excellent performance due to its flexibility in parallel computation and long- range dependency modeling .
We followed Vaswani et al . ( 2017 ) to set the configurations .
The dimensionality of all input and output layers is 512 , and that of FFN layer is 2048 .
We employed 8 parallel attention heads in both encoder and decoder .
Parameter optimization was performed using stochastic gradient descent , where Adam ( Kingma and Ba , 2015 ) was used to automatically adjust the learning rate of each parameter .
We batched sentence pairs by approximated length , and limited input and output tokens per batch to 25000 tokens .
As for decoding we employed beam search algorithm and set the beam size as 4 .
Besides , we set the distillation coefficient ? as 0.4 . Contrast Models .
We compared our framework with the following models , namely : ?
Single A reimplemented Transformer only trained on a single domain-specific ( in / out ) training corpus . ?
Mix A reimplemented Transformer trained on the mix of in- domain and out-of- domain training corpora .
?
Fine-tuning ( FT ) ( Luong and Manning , 2015 ) .
It first trains the NMT model on outof-domain training corpus and then fine - tunes it using in - domain training corpus .
? Mixed Fine-tuning ( MFT ) ( Chu et al. , 2017 ) .
It also first trains the NMT model on out - of- domain training corpus , and then finetunes it using both out - of- domain and oversampling in - domain training corpora .
? Knowledge Distillation ( KD ) ( Kim and Rush , 2016 ) .
Using this method , we first train a out-of- domain and an in- domain NMT models using their own training corpus , respectively .
Then , we use the in-domain training corpus to fine - tune the out-of- domain NMT model , supervised by the in-domain NMT model .
Besides , we reported the performance of some recently proposed multi-domain NMT models .
? Domain Control ( DC ) ( Kobus et al. , 2016 ) .
It is also based on the mix-domain NMT model .
However , it adds an additional domain tag to each source sentence , incorporating domain information into source annotations .
? Discriminative Mixing ( DM ) ( Pryzant et al. , 2017 ) .
It jointly trains NMT with domain classification via multitask learning .
Please note that it performs the best among three approaches proposed by Pryzant et al. , ( 2017 ) . ? Word-level Domain Context Discrimination ( WDCD ) .
It discriminates the source-side word- level domain specific and domain-shared contexts for multi- domain NMT by jointly modeling NMT and domain classifications .
Results on Chinese-English Translation
Effect of Iteration Number K
The iteration number K is a crucial hyperparameter that directly determines the amount of the transferred translation knowledge under our framework .
Therefore , we first inspected its impacts on the development sets .
To this end , we varied K from 0 to 7 with an increment of 1 in each step , where our framework degrades to Single when K=0 .
Figure 3 provides the experimental results using different Ks .
We can observe that both IDDA ( ? =0 ) and IDDA achieve the best performance at the 3 - th iteration , respectively .
Therefore , we directly used K=3 in all subsequent experiments .
Overall Performance
Table 1 shows the overall experimental results .
On all test sets , our framework significantly outperforms other contrast models .
Furthermore , we reach the following conclusions : First , on the in- domain test sets , both IDDA ( ? =0 ) and IDDA surpass Single , Mix , FT , MFT and KD , most of which are commonly used in the domain adaptation for NMT .
This confirms the difficulty in completely one - pass transferring the useful out - of- domain translation knowledge to the in-domain NMT model .
Moreover , the indomain NMT model benefits from multiple - pass knowledge transfers under our framework .
Second , compared with DC , DM and WDCD that are proposed for multi-domain NMT , both IDDA ( ? =0 ) and IDDA still exhibit better performance on the in- domain test sets .
The underlying reason is that these multi-domain models discriminate domain-specific and domain-shared information in encoder , however , their shared decoder are inadequate to effectively preserve domain-related text style and idioms .
In contrast , our framework is adept at preserving these information since we construct an individual NMT model for each domain .
Third , IDDA achieves better performance than IDDA ( ? =0 ) , demonstrating the importance of retaining previously learned translation knowledge .
Surprisingly , IDDA significantly outperforms IDDA ( ? =0 ) on out -of- domain data sets .
We conjecture that during the process of knowledge distillation , by assigning non-zero probabilities to multiple words , the output distribution of teacher model is more smooth , leading to smaller variance in gradients ( Hinton et al. , 2015 ) .
Consequently , the out-of- domain NMT model becomes more robust by iteratively absorbing the translation knowledge from the best out - of- domain model .
Finally , note that even on the out-of- domain test sets , IDDA still has better performance than all listed contrast models in the subsequent experimental analyses .
This result demonstrates the advantage of dual domain adaptation under our framework .
According to the reported performance of our framework shown in Table 1 , we only considered IDDA in all subsequent experiments .
Besides , we only chose MFT , KD , and WDCD as typical contrast models .
This is because KD is the basic domain adaption approach of our framework , MFT and WDCD are the best domain adaptation method and multi-domain NMT model for comparison , respectively .
Results on Source Sentences with Different Lengths Following previous work ( Bahdanau et al. , 2015 ) , we divided IWSLT test sets into different groups based on the lengths of source sentences and then investigated the performance of various models .
Figure 4 illustrates the results .
We observe that our framework also achieves the best performance in all groups , although the performances of all models degrade with the increase of the length of source sentences .
Effect of Out-of-domain Corpus Size
In this group of experiments , we investigated the impacts of out - of- domain corpus size on our proposed framework .
Specifically , we inspected the results of our framework using different sizes of out -of- domain corpora : 50K , 200 K and 1.12 M , respectively Figure 5 shows the comparison results on the average BLEU scores of all IWSLT test sets .
No matter how large out - of- domain data is used , IDDA always achieves better performance than other contrast models , demonstrating the effectiveness and generality of our framework .
Specially , IDDA with 200 K out - of- domain corpus is comparable to KD with 1.12 M corpus .
From this result , we confirm again that our framework is able to better exploit the complementary information between domains than KD .
Effects of Dual Domain Adaptation and Updating Teacher Models
Two highlights of our framework consist of the usage of bidirectional translation knowledge transfer and continuous updating teacher models ? * out and ? * in ( See Line 6 , 10 of Algorithm 1 ) .
To inspect their effects on our framework , we compared our framework with its two variants : ( 1 ) IDDAunidir , where we only iteratively transfer out -ofdomain translation knowledge to the in- domain NMT model ; ( 2 ) IDDA - fixTea , where teacher models are fixed as the initial out - of- domain and in- domain NMT models , respectively .
The results are displayed in Table 2 .
We can see that our framework exhibits better performance than its two variants , which demonstrates that dual domain adaptation enables NMT models of two domains to benefit from each other , and updating teacher models is more helpful to retain useful translation knowledge .
example provides the insight into the advantage of our proposed framework to some extent .
Specifically , we observe that MFT , KD , WDCD are unable to correctly understand the meaning of " zh?l? x?ngz?u de l?ngzh?ngl ? i d?ngw ? " and thus generate incorrect or incomplete translations , while IDDA successfully corrects these errors by gradually absorbing transferred translation knowledge .
Case Study
Results on English - German Translation
Overall Performance
We first calculated the distance between the indomain and each out-of- domain corpora : dA ( Ted Talk , News ) = 0.92 and dA ( Ted Talk , Medical ) = 1.92 .
Obviously , the News domain is more relevant to TED
Talk domain than Medical domain , and thus we determined the final transfer order as {?
out medical , ? outnews } for this task .
Then , as implemented in the previous Chinese - English experiments , we determined the optimal K=2 on the development set .
Table 4 shows experimental results .
Similar to the previously reported experiment results , our framework still obtains the best performance among all models , which verifies the effectiveness of our framework on many - to - one domain adaptation for NMT .
As described above , we have two careful designs for many - to- one NMT domain adaptation : ( 1 ) We distinguish different out - of- domain corpora , and then iteratively perform bidirectional translation knowledge transfer between in - domain and each out - of- domain NMT models .
( 2 ) We determine the transfer order according to the seman - tic distance between each out - of- domain and indomain training corpora .
Here , we carried out two groups of experiments to investigate their impacts on our framework .
In the first group of experiments , we first combined all out - of- domain training corpora into a mixed corpus , and then applied our framework to establish the in-domain NMT model .
In the second group of experiments , we employed our framework in different transfer orders to perform domain adaptation .
Table 5 shows the final experimental results , which are in line with our expectations and verify the validity of our designs .
Conclusion
In this paper , we have proposed an iterative dual domain adaptation framework for NMT , which continuously fully exploits the mutual complementarity between in- domain and out-domain corpora for translation knowledge transfer .
Experimental results and in- depth analyses on translation tasks of two language pairs strongly demonstrate the effectiveness of our framework .
In the future , we plan to extend our framework to multi-domain NMT .
Besides , how to leverage monolingual sentences of different domains to refine our proposed framework .
Finally , we will apply our framework into other translation models ( Bahdanau et al. , 2015 ; Song et al. , 2019 ) , so as to verify the generality of our framework . ( a ) , previous studies mainly focus on the one - pass translation knowledge transfer from one out-of- domain NMT model to the in-domain NMT model .
Unlike these studies , we propose to conduct iterative dual domain adaptation for NMT , of which framework is illustrated in Figure1 ( b ) .
