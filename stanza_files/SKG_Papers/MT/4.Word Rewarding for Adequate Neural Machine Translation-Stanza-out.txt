title
Word Rewarding for Adequate Neural Machine Translation
abstract
To improve the translation adequacy in neural machine translation ( NMT ) , we propose a rewarding model with target word prediction using bilingual dictionaries inspired by the success of decoder constraints in statistical machine translation .
In particular , the model first predicts a set of target words promising for translation ; then boosts the probabilities of the predicted words to give them better chances to be output .
Our rewarding model minimally interacts with the decoder so that it can be easily applied to the decoder of an existing NMT system .
Extensive evaluation under both resource- rich and resource - poor settings shows that ( 1 ) BLEU score improves more than 10 points with oracle prediction , ( 2 ) BLEU score improves about 1.0 point with target word prediction using bilingual dictionaries created either manually or automatically , ( 3 ) hyper- parameters of our model are relatively easy to optimize , and ( 4 ) undergeneration problem can be alleviated in exchange for increasing over- generated words .
Introduction Neural machine translation ( NMT ) [ 1 , 2 , 3 ] has dramatically improved machine translation quality compared to statistical machine translation ( SMT ) .
However , current NMT systems still suffer from the adequacy problem due to inappropriate lexical choice , under-generation , and over-generation [ 4 ] .
In SMT , bilingual dictionaries have been used to improve adequacy in translation as decoder constraints .
Typical example is the XML markup function implemented on MOSES [ 5 ] .
Inspired by the decoding constraints for SMT , we propose a rewarding model using bilingual dictionaries to address the adequacy problem in NMT .
Our model rewards target words that are promising to be used in correct translations by boosting their probabilities to be output by a decoder .
It predicts such target words using bilingual dictionaries that are created manually or automatically .
By applying byte pair encoding ( BPE ) [ 6 ] to dictionaries , our model can benefit from both BPE and dictionaries .
While previous studies incorporate bilingual dictionaries into NMT for translation of rare words [ 7 , 8 ] and domainspecific words [ 9 ] , we do so to improve the adequacy of NMT .
Hence , dictionaries are made use of translating not only specific types of words but also all words .
In addition , these are methodologically different ; our model simply biases the trained decoder while previous models change the inside NMT architectures and require training of the entire systems .
Due to this design , our model is easy to add to trained NMT systems and compatible with BPE .
Extensive evaluation on Japanese-to-English and English-to - Japanese translation has been conducted using two datasets ; IWSLT ( TED Talk ) [ 10 ] , spoken language domain with a small set of bilingual sentences ( 223 k ) , and ASPEC [ 11 ] , a scientific domain with a large set of bilingual sentences ( 3M ) .
We refer to the former as a resource -poor domain and the latter as a resource - rich domain , hereafter .
The results show that the rewarding model with oracle prediction of target words , where all and only target words in references are predicted , BLEU score improves more than 10 points on average in both of the resource -poor and resource - rich domains .
When using bilingual dictionaries created manually or automatically in the rewarding model to predict target words , BLEU scores improve about 1.0 point on average in both domains .
Detailed analysis of our model reveals that it is relatively insensitive to settings of its hyper-parameters and easy to optimize .
In addition , it is shown that our model decreases the number of under-generated words while tends to increase the number of over- generated words .
Neural Machine Translation
The encoder- decoder model with attention [ 3 , 12 ] is one of the most popular architectures in NMT .
It takes an input sentence X = {x 1 , ... , x n } and generates its translation Y = {y 1 , ... , y m } as : p( Y | X ; ?) = m ? j=1 p(y j |y <j , X ; ? ) , where ? is a set of parameters and y <j = {y 1 , ? ? ? , y j?1 }. Given a parallel corpus C = { ( X , Y ) } , the training objective minimizes the cross-entropy loss with regard to ? : L ? = ? ( X , Y ) ?C ? log p( Y | X ) .
s 0 s j+1 y j ! " #- 1 ! " # ! " # + 1 ? ? ? ? Encoder Decoder x i- 1 x i x i+ 1 s j ? ? x 1 x 2 x n Word Prediction Rewarding Model Softmax layer D f2e ? y j-1 y j $ % " i- 1 % " i % " i+1 Figure 1 : Rewarding model at decoding step j : predicted target words D f 2e are rewarded to have better chances to be output at each decoding time step .
Note that the attention model is omitted for clarity .
The model consists of three parts , namely , an encoder , a decoder , and an attention model .
The encoder has an embedding layer and an recurrent neural network ( RNN ) layer .
The former converts words into their continuous space representations .
Taking these embeddings , the RNN layer then computes a state that represents the input sequence till the current time step .
Specifically , we use the bi-directional long shortterm memory ( LSTM ) [ 13 ] that encodes the source sentence by forward and backward directions .
At time step i , the state is represented by concatenating the forward hidden state ? i and the backward one hi as h i = [ ?i ; hi ] .
In this manner , X can be represented as h = {h 1 , ... , h n }.
The decoder remembers all the history of translation and its softmax layer computes the posterior probability p(y j |y <j , X ) of a word y j to output as translation .
In order to focus on specific parts of the input sentence necessary for translation , the attention model is incorporated .
We use the global attention mechanism proposed in [ 12 ] .
Rewarding Model
On top of a decoder , our model rewards predicted words so that they have better chances to be output as translations as shown in Figure 1 .
Specifically , it first predicts a set of target words D f 2e that are promising to be used in translations using bilingual dictionaries .
Then , our model rewards a target word if it is contained in D f 2e by adding weight to the posterior probability : Q(y j |y <j , X ) = log p(y j |y <j , X ) + ?r yj , ( 1 ) where ? is the weight of reward that will be tuned using a development set .
This means that our model boosts the probabilities of predicted words that might have been slipped away during beam search in the conventional decoder .
In [ 14 ] , a similar rewarding model is proposed , but rewards are based on remaining sequence lengths .
We use a simple binary rewarding in this paper : r yj = { 1 ( y j ? D f 2e ) , 0 ( otherwise ) .
( 2 ) We also tried to model the rewarding function using lexical translation probabilities that can be estimated for automatically created dictionaries .
However , preliminary experiments empirically showed that this simple form of rewarding worked best .
This may be because these probabilities are modeled in completely different ways , i.e. , p(y j |y <j , X ) in Equation ( 1 ) is conditioned on the entire source sentence while lexical translation probabilities are conditioned on source words .
Further investigation is our future work .
Finally , a target word is output as : y j = arg max yj Q(y j |y <j , X ) .
Accurate prediction of D f 2e is crucial for our rewarding model .
In the next section , we discuss practical implementations to obtain D f 2e from dictionaries .
Target Word Prediction with Dictionaries
In this study , we look up bilingual dictionaries created manually or automatically as word prediction , which allows to make our model minimally interact with the original NMT system .
We will consider a sophisticated prediction model using an information in the encoder in future [ 15 ] .
Prediction with Manually Created Dictionary
Thanks to the accumulated efforts by the academia and industry , bilingual dictionaries have been manually created for language pairs of English and Japanese .
Such manual bilingual dictionaries provide reliable translation knowledge , although their coverage is limited .
One disadvantage of manual dictionaries is that conjugation and derivative forms are generally not provided in such dictionaries .
As a simple way to predict the target word set , we look up source words in a manual bilingual dictionary .
Prediction with Automatically Created Dictionary Previous studies have proposed methods to automatically construct bilingual dictionaries .
Especially , word alignment techniques for SMT [ 16 , 5 ] allow us to construct a dictionary directly from a parallel corpus .
Similar alignment may be possible using the attention model in NMT , however , reliability is not assured because the attention model is rather soft as a constraint [ 17 , 18 ] .
The biggest advantage of using word alignment for dictionary construction is that the domain of the dictionary matches that of translation targets .
In addition , conjugations are available in the dictionary .
A disadvantage is that alignment errors may decrease the quality of the dictionary .
We apply the GIZA ++ toolkit 1 that is an implementation of the IBM alignment models [ 16 ] on a parallel corpus to automatically create a bilingual dictionary .
To control the precision and recall of target word prediction , we introduce a threshold ? , which is tuned on development data .
Target words with lower translation probability than ? are discarded .
Exact and Partial Matching with BPE Conducting translation on sub-words is effective to address the unknown word problem [ 19 ] .
We apply BPE [ 6 ] to dictionaries for word prediction to make our rewarding model compatible to BPE - based NMT .
For both the dictionary entries and source sentences , we first apply a BPE model trained on a parallel corpus and then match the entries in dictionaries and source sentences .
We use two types of matching methods between an input sentence and dictionary entries : exact match and partial match .
The former is precision-oriented and the latter is recall-oriented .
After applying BPE , a dictionary headword ( lemma ) consists of multiple sub-words ; a lemma w is denoted as w = w 1 , . . . , w k .
Exact match regards w as matched to a source sentence X if and only if : w 1 , . . . , w k ? X , s.t. , for ?i ? { 1 , . . . , k ?
1 } , w i = x j ? w i+1 = x j +1 .
On the other hand , partial match regards w as matched to X if w i ?
X for ?w i ? w .
In both matching methods , translations of w are added to the target word set as predictions .
Obviously , target word predictions by partial match subsumes those by exact match .
Experiment Settings
To investigate the effects of our model , we conducted Japanese - to - English and English - to - Japanese translation experiments on resource-poor and resource - rich domains .
Translation Tasks
The resource-poor task used the IWSLT 2017 Japanese - English task from the WIT project [ 10 ] .
The IWSLT task provides 223 k parallel sentences for training .
We used the dev 2010 and test 2010 sets for development and testing , containing 871 and 1 , 549 sentences , respectively .
The resource- rich task used the Japanese - English paper excerpt corpus ( ASPEC ) 2 [ 11 ] , which is one subtask of the workshop on Asian translation ( WAT ) 3 [ 20 ] .
For training , we used the first 2 M parallel sentence pairs among the entire 3 M pairs sentences following [ 21 ] , because the remaining 1 M sentences were noisy .
The ASPEC task provides 1 , 790 , and 1 , 812 sentences for development and testing , respectively .
We conducted both Japanese-to-English and Englishto - Japanese translation experiments on these two tasks , referred to as IWSLT - JE , IWSLT - EJ , ASPEC - JE , and ASPEC - EJ for short , hereafter .
NMT and Rewarding Model
We used the mlpnlp - nmt system 4 that is an LSTM based encoder-decoder NMT model with attention , which achieved the best translation performance in human evaluations for both the ASPEC - JE and ASPEC - EJ tasks at WAT 2017 [ 20 ] .
5
We implemented our rewarding model on top of the mlpnlpnmt system ( our implementation will be public upon acceptance of the paper ) .
We followed the hyper-parameter settings of [ 21 ] .
The sizes of the source and target side embeddings , the LSTM hidden states , the attention hidden states were all set to 512 .
We used 2 - layer LSTMs for both the encoder and decoder with beam size of 5 .
Stochastic gradient descent was used as the learning algorithm , with an initial learning rate of 1.0 , gradient clipping of 5.0 , and a dropout rate of 30 % for the inter-layer dropout .
The mini batch size was 128 .
The training epochs for IWSLT - JE , IWSLT - EJ , ASPEC - JE , and ASPEC - EJ were all set to 20 , and we chose the model with the best development BLEU score among all the epochs as the baseline systems .
6
For the rewarding models , ? in Equation ( 1 ) was tuned on the development sets from 0.1 to 1.0 by 0.1 interval .
The threshold ? that prunes the automatically constructed dictionaries in Section 4.2 was tuned on 0 , 0.0001 , 0.001 , 0.01 and 0.1 .
We selected the best combination among all combinations of ? and ? on the development set for each model .
We investigate the upper-bound performance of our rewarding model using oracle target word prediction .
On this oracle model , predicted target words are all and only words in a reference translation , i.e. , precision and recall of prediction are both 100 % .
The best weight of ? was searched from 0.1 increasing the value by 0.1 until we observed a decrease in BLEU scores .
As preprocessing for the parallel corpora and bilingual dictionaries , we segmented Japanese sentences / entries using MeCab , 7 and tokenized and truecased the English sen - tences / entries with the truecase .
perl script in Moses 8 for both translation tasks .
We further split the words into subwords using joint BPE [ 6 ] with 32 , 000 merge operations .
The vocabulary sizes of the IWSLT - JE task were 21 , 534 and 18 , 022 , respectively .
The vocabulary sizes of ASPEC - JE task were 28 , 852 and 22 , 340 , respectively .
Bilingual Dictionaries
As the manual dictionary , we used EDR , 9 which is the publicly available English and Japanese bilingual dictionary .
10
The numbers of English-to - Japanese and Japaneseto - English entry pairs are 676 k and 1 , 052k , respectively .
In EDR , only lemmas are provided and thus inflected forms of English verbs are unavailable .
To address this issue , inflected forms of the EDR lemmas are extracted from the English dictionary of XTAG project , 11 which is used as the English morphological analysis dictionary for TreeTagger .
12
All the possible inflected forms are added into our dictionary .
For dictionary look - up , a source sentence is first lemmatized and matched with the dictionary .
We used MeCab for Japanese and TreeTagger for English to lemmatize words .
To automatically construct bilingual dictionaries , 13 we used the GIZA ++ toolkit on the training corpus in both English - to - Japanese and Japanese - to - English directions .
14
We applied the " grow-diag-final - and " heuristic and obtained lexical translation probabilities using Moses .
We then prune translation pairs with low probabilities by ?.
Results
We first investigate the effect of ?
using the development sets on both the oracle target word sets and our word prediction methods .
Next , we evaluate the translation quality on the test sets using the optimized ?.
Finally , we conduct detailed analysis of translation results by our rewarding model .
Throughout the section , the BLEU - 4 score was used as the evaluation metric , which was computed using the multibleu .
perl script in Moses on tokenized and truecased English and word-segmented Japanese sentences , respectively .
The significance tests were performed using the bootstrap resampling [ 22 ] at p < 0.01 .
Effects of ?
Figure 2 shows the BLEU scores by the oracle word rewarding on the development sets of the IWSLT - JE , IWSLT - EJ , ASPEC - JE , and ASPEC - EJ tasks .
The BLEU scores significantly improved according to the ?.
The best settings of ? improves 6.00 , 8.25 , 9.80 , and 11.77 BLEU scores on the IWSLT - JE , IWSLT - EJ , ASPEC - JE , and ASPEC - EJ tasks from each baseline system , respectively .
Figure 3 shows the BLEU scores with respect to the ? and precision / recall of word prediction on our model with word prediction using manually or automatically created dictionaries .
EDR indicates the models predicting target words using EDR .
XTAG indicates the models using EDR extended with XTAG , which are only for the Japanese - to - English direction .
GIZA indicates the models that predict target words using automatically constructed dictionary by GIZA ++.
The suffixes e and p in the legends indicate exact match and partial match , respectively .
The results show that BLEU scores depend on precision and recall of target word prediction by different dictionaries .
The weights of ? that achieved the best BLEU scores varied from 0.1 to 1.0 .
Notice that these weights are much smaller than the oracle prediction , which are 0.5 , 0.4 , 0.4 , and 0.5 for IWSLT - JE , IWSLT - EJ , ASPEC - JE , and ASPEC - EJ on GIZA partial - match , respectively .
This is because predicted words are less reliable and too much rewarding degrades the trans - 3 : BLEU scores by our rewarding models with word prediction using bilingual dictionaries when changing the ? on the development sets .
The gentle convex curves of BLEU scores show that the weight of ? is tunable by a simple grid search .
lation quality .
The gentle convex curves of BLEU scores also show that ? is easily tunable using a simple grid search .
Word Prediction and Translation Results
Table 1 shows the comparison of BLEU scores on the test sets of the baseline and the rewarding models .
We also report the results that use a merged dictionary .
We chose the XTAG partial and GIZA partial for Japanese- to - English , EDR partial and GIZA partial for English - to - Japanese for merging because of their individual good performance .
We tuned the ? for merged dictionary using the development set .
We can see that compared to the baselines , most of our methods significantly improve BLEU scores .
Overall , a word prediction method with high recall shows a larger improvement in BLEU score as consistently shown by comparing exact matching v.s. partial matching , as well as comparing EDR v.s. XTAG , EDR or XTAG v.s. GIZA , and GIZA v.s. merged dictionary .
However , there is still a gap between rewarding by our target word prediction and rewarding by oracle prediction .
Our GIZA and merged dictionary models achieve a high recall of about 90 % but a very low precision of 0.1 % .
Improving the precision for word prediction while keeping a recall high is our future work .
The baselines on ASPEC -JE and ASPEC - EJ are our reproduction of the state - of - the - art at WAT competition as single models , which are reported as achieved 27.62 and 39.71 BLEU scores in the paper .
Compared to these scores , our rewarding model improved 0.67 and 0.36 points , respectively .
Under and Over Generation
We investigated the rate of under-generation and overgeneration that are the major adequacy problems in NMT [ 23 ] using Translation Edit Rate ( TER ) [ 24 ] .
TER aligns a reference and translation result .
We counted the number of Deletion and Insertion regarding these are caused by under and over generation , respectively .
This is an approximation to detect under and over generations , but we consider it is useful as an automatic and handy evaluation metric .
Table 2 shows the average numbers of under and over generations per sentence .
The under-generation decreases on all the rewarding models in exchange of increasing overgeneration .
The rewarding model with oracle target word prediction reduces under generation about 1.2 word on average .
This result shows that our rewarding model is also effective for alleviating the under-generation problem .
The over- generation can be reduced by adding global constraint to the rewarding model , which prohibits rewarding the same predicted target .
This is our future work .
Example translations of the baseline and our rewarding model ( GIZA partial match ) are shown in the following .
The phrase of " congenital immunity " and " cancer of " were successfully translated by our model .
Source IL -1 2 ? ? ? ? ? ( ? ? ) ? ? ? ? ? ? ? ? ?
Reference biological response of the resistance ( congenital immunity ) to cancer of IL - 12 was also examined .
Baseline the biological response of the resistance to IL - 12 is also discussed .
Our Model the biological response of the resistance ( congenital immunity ) to the cancer of IL - 12 is also discussed .
Related Work
Our rewarding model can be viewed as a constraint on the decoder to output desired target words .
There have been studies that aim to output predetermined words or phrases in neural language generation .
For this purpose , the grid beam search in NMT is proposed [ 25 ] and the SMT lattice is combined into NMT [ 26 ] .
In neural conversation generation , Wen et al . ( 2015 ) input a vector representing which information should be generated to an encoder [ 27 ] , and a decoder is designed to explicitly control generation of emotional words [ 28 ] .
Compared with these previous studies , one benefit of our rewarding model is that the predicted words are used as soft constraints on outputs with minimal interaction to the decoder .
The most relevant study from the methodological point of view is [ 14 ] that also proposes a rewarding model in a decoder of NMT to improve the translation quality in general , such as remaining sequence lengths to output .
We focus on the adequacy problem in NMT and combine word pre-diction with bilingual dictionaries .
Some studies tackle the adequacy problem in NMT , but they require an independent SMT system [ 29 , 30 ] or modification of the decoder [ 31 ] .
Different from these , ours is simple and a cost-effective solution for the adequacy problem .
The under and over-generation problems have been recognized not only in NMT , but in other applications that use the encoder-decoder model for natural language generation .
Different solutions have been proposed .
First , a coverage vector is introduced in NMT [ 23 , 32 , 33 ] that tracks which source words have been translated by the attention mechanism .
A sparse and constrained attention has been proposed [ 34 ] , while word prediction , which are also used to reduce computational cost of softmax function at the decoder [ 35 , 36 ] , has been proposed to solve the undergeneration problem .
The decoder in [ 37 ] encourages to output predicted target words by initializing the decoder through word prediction , and the model in [ 38 ] predicts target words and their expected frequencies to resolve the under and over generation problems in NMT - based summarization .
Conclusion
We proposed a rewarding model with word prediction to boost the translation probabilities of the predicted target words that should be in correct translations .
Our model allows incorporating bilingual dictionaries on a BPE - based NMT system .
Extensive evaluation on both resource-poor and resource - rich domains showed its effectiveness .
As future work , first , we plan to improve the precision of word prediction preserving the recall at high .
Second , we plan to improve our rewarding model to effectively incorporate translation probabilities and extend the model to reward not only words but also phrases .
We will also consider a global constraint by predicting not only target words but their frequencies , and adjust rewards when a word has been used in translation .
Finally , more experiments on datasets of various domains and language pairs will be conducted to investigate the generality of our approach .
Figure 2 : 2 Figure 2 : BLEU scores by the oracle rewarding model when changing the ? on the development set .
BLEU scores dramatically improved on ASPEC task ; 9.8 and 11.8 point improvements on ASPEC - JE and EJ , respectively .
