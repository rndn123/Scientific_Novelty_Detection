title
Combining Domain Adaptation Approaches for Medical Text Translation
abstract
This paper explores a number of simple and effective techniques to adapt statistical machine translation ( SMT ) systems in the medical domain .
Comparative experiments are conducted on large corpora for six language pairs .
We not only compare each adapted system with the baseline , but also combine them to further improve the domain-specific systems .
Finally , we attend the WMT2014 medical summary sentence translation constrained task and our systems achieve the best BLEU scores for Czech-English , English - German , French - English language pairs and the second best BLEU scores for reminding pairs .
Introduction
This paper presents the experiments conducted by the NLP 2 CT Laboratory at the University of Macau for WMT2014 medical sentence translation task on six language pairs : Czech-English ( cs-en ) , French -English ( fr-en ) , German-English ( de-en ) and the reverse direction pairs , i.e. , en-cs , en-fr and en-de .
By comparing the medical text with common text , we discovered some interesting phenomena in medical genre .
We apply domain-specific techniques in data pre-processing , language model adaptation , translation model adaptation , numeric and hyphenated words translation .
Compared to the baseline systems ( detailed in Section 2 & 3 ) , the results of each method show reasonable gains .
We combine individual approach to further improve the performance of our systems .
To validate the robustness and language - independency of individual and combined systems , we conduct experiments on the official training data ( detailed in Section 3 ) in all six language pairs .
We anticipate the numeric comparison ( BLEU scores ) on these individual and combined domain adaptation approaches that could be valuable for others on building a real-life domain-specific system .
The reminder of this paper is organized as follows .
In Section 2 , we detail the configurations of our experiments as well as the baseline systems .
Section 3 presents the specific preprocessing for medical data .
In Section 4 and 5 , we describe the language model ( LM ) and translation model ( TM ) adaptation , respectively .
Besides , the techniques for numeric and hyphenated words translation are reported in Section 6 and 7 .
Finally , the performance of design systems and the official results are reported in Section 8 .
Experimental Setup
All available training data from both WMT2014 standard translation task 1 ( general- domain data ) and medical translation task 2 ( in- domain data ) are used in this study .
The official medical summary development sets ( dev ) are used for tuning and evaluating all the comparative systems .
The official medical summary test sets ( test ) are only used in our final submitted systems .
The experiments were carried out with the Moses 1.0 3 .
The translation and the re-ordering model utilizes the " growdiag -final " symmetrized word - to - word alignments created with MGIZA + + 4 ( Och and Ney , 2003 ; Gao and Vogel , 2008 ) and the training scripts from Moses .
A 5 - gram LM was trained using the SRILM toolkit 5 ( Stolcke et al. , 2002 ) , exploiting improved modified Kneser - Ney smoothing , and quantizing both probabilities and back -off weights .
For the log-linear model training , we take the minimum-error-rate training ( MERT ) method as described in ( Och , 2003 ) .
Task Oriented Pre-processing
A careful pre-processing on training data is significant for building a real- life SMT system .
In addition to the general data preparing steps used for constructing the baseline system , we introduce some extra steps to pre-process the training data .
The first step is to remove the duplicate sentences .
In data-driven methods , the more frequent a term occurs , the higher probability it biases .
Duplicate data may lead to unpredicted behavior during the decoding .
Therefore , we keep only the distinct sentences in monolingual corpus .
By taking into account multiple translations in parallel corpus , we remove the duplicate sentence pairs .
The second concern in preprocessing is symbol normalization .
Due to the nature of medical genre , symbols such as numbers and punctuations are commonly -used to present chemical formula , measuring unit , terminology and expression .
Fig. 1 shows the examples of this case .
These symbols are more frequent in medical article than that in the common texts .
Besides , the punctuations of apostrophe and single quotation are interchangeably used in French text , e.g. " l'effet de l'inhibition " .
We unify it by replacing with the apostrophe .
In addition , we observe that some monolingual training subsets ( e.g. , Gene Regulation Event Corpus ) contain sentences of more than 3,000 words in length .
To avoid the long sentences from harming the truecase model , we split them into sentences with a sentence splitter 6 ( Rune et al. , 2007 ) that is optimized for biomedical texts .
On the other hand , we consider the target system is intended for summary translation , the sentences tend to be short in length .
For instance , the average sentence lengths in development sets of cs , fr , de and en are around 15 , 21 , 17 and 18 , respectively .
We remove sentence pairs which are more than 80 words at length .
In order to that our experiments are reproducible , we give the detailed statistics of task oriented pre-processed training data in Table 2 .
To validate the effectiveness of the preprocessing , we compare the SMT systems trained on original data 7 ( Baseline1 ) and taskoriented - processed data ( Baseline2 ) , respectively .
Table 1 shows the results of the baseline systems .
We found all the Baseline2 systems outperform the Baseline1 models , showing that the systems can benefit from using the processed data .
For cs-en and en-cs pairs , the BLEU scores improve quite a lot .
For other language pairs , the translation quality improves slightly .
By analyzing the Baseline2 results ( in Table 1 ) and the statistics of training corpora ( in Table 2 ) , we can further elaborate and explain the results .
The en-cs system performs poorly , because of the short average length of training sentences , as well as the limited size of in-domain parallel and monolingual corpora .
On the other hand , the fren system achieves the best translation score , as we have sufficient training data .
The translation quality of cs-en , en-fr , fr-en and de-en pairs is much higher than those in the other pairs .
Hence , Baseline2 will be used in the subsequent comparisons with the proposed systems described in Section 4 , 5 , 6 and 7 .
Lang .
Pair
Language Model Adaptation
The use of LMs ( trained on large data ) during decoding is aided by more efficient storage and inference ( Heafield , 2011 ) .
Therefore , we not 2 : Statistics summary of corpora after pre-processing .
only use the in- domain training data , but also the selected pseudo in-domain data 8 from generaldomain corpus to enhance the LMs ( Toral , 2013 ; Rubino et al. , 2013 ; Duh et al. , 2013 ) .
Firstly , each sentence s in general- domain monolingual corpus is scored using the cross-entropy difference method in ( Moore and Lewis , 2010 ) , which is calculated as follows : ( ) ( ) ( ) I G score s H s H s ? ? ( 1 ) where H( s ) is the length - normalized crossentropy .
I and G are the in-domain and generaldomain corpora , respectively .
G is a random subset ( same size as the I ) of the general - domain corpus .
Then top N percentages of ranked data sentences are selected as a pseudo in-domain subset to train an additional LM .
Finally , we linearly interpolate the additional LM with indomain LM .
We use the top N % of ranked results , where N={0 , 25 , 50 , 75 , 100 } percentages of sentences out of the general corpus .
Table 3 shows the absolute BLEU points for Baseline2 ( N=0 ) , while the LM adapted systems are listed with values relative to the Baseline2 .
The results indicate that LM adaptation can gain a reasonable improvement if the LMs are trained on more relevant data for each pair , instead of using the whole training data .
For different systems , their BLEU 8 Axelrod et al . ( 2011 ) names the selected data as pseudo indomain data .
We adopt both terminologies in this paper .
scores peak at different values of N .
It gives the best results for cs-en , en-fr and de-en pairs when N=25 , en-cs and en-de pairs when N=50 , and fren pair when N=75 .
Among them , en-cs and enfr achieve the highest BLEU scores .
The reason is that their original monolingual ( in- domain ) data for training the LMs are not sufficient .
When introducing the extra pseudo in- domain data , the systems improve the translation quality by around 2 BLEU points .
While for cs-en , fr-en and de-en pairs , the gains are small .
However , it can still achieve a significant improvement of 0.60 up to 1.12 BLEU points .
Lang. N=0 N=25 N=50 N=75 N=100
Translation Model Adaptation
As shown in Table 2 , general- domain parallel corpora are around 1 to 7 times larger than the in-domain ones .
We suspect if general- domain corpus is broad enough to cover some in-domain sentences .
To observe the domain-specificity of general- domain corpus , we firstly evaluate systems trained on general - domain corpora .
In Ta-ble 4 , we show the BLEU scores of generaldomain systems 9 on translating the medical sentences .
The BLEU scores of the compared systems are relative to the Baseline2 and the size of the used general - domain corpus is relative to the corresponding in -domain one .
For en-cs , cs-en , en-fr and fr-en pairs , the general - domain parallel corpora we used are 6 times larger than the original ones and we obtain the improved BLEU scores by 1.72 up to 3.96 points .
While for en-de and de-en pairs , the performance drops sharply due to the limited training corpus we used .
Hence we can draw a conclusion : the generaldomain corpus is able to aid the domain-specific translation task if the general - domain data is large and broad enough in content .
Taking into account the performance of general - domain system , we explore various data selection methods to derive the pseudo in-domain sentence pairs from general - domain parallel corpus for enhancing the TMs ( Wang et al. , 2013 ; Wang et al. , 2014 ) .
Firstly , sentence pair in corresponding general - domain corpora is scored by the modified Moore - Lewis ( Axelrod et al. , 2011 ) , which is calculated as follows :
Lang ? ? g g ( ) ( ) ( ) ( ) ( ) I src G src I t t G t t score s H s H s H s H s ? ? ? ? ? ? ? ? ? ? ? ? ( 2 ) which is similar to Eq. ( 1 ) and the only difference is that it considers the both the source ( src ) and target ( tgt ) sides of parallel corpora .
Then top N percentage of ranked sentence pairs are selected as a pseudo in-domain subset to train an individual translation model .
The additional model is log-linearly interpolated with the indomain model ( Baseline2 ) using the multidecoding method described in ( Koehn and Schroeder , 2007 ) .
Similar to LM adaptation , we use the top N % of ranked results , where N={0 , 25 , 50 , 75 , 100 } percentages of sentences out of the general cor-9 General - domain systems are trained only on generadomain training corpora ( i.e. , parallel , monolingual ) .
pus .
Table 5 shows the absolute BLEU points for Baseline2 ( N=0 ) , while for the TM adapted systems we show the values relative to the Base-line2 .
For different systems , their BLEU peak at different N. For en-fr and en-de pairs , it gives the best translation results at N=25 .
Regarding cs-en and fr-en pairs , the optimal performance is peaked at N=50 .
While the best results for de-en and en-cs pairs are N=75 and N=100 respectively .
Besides , performance of TM adapted system heavily depends on the size and ( domain ) broadness of the general- domain data .
For example , the improvements of en-de and de-en systems are slight due to the small general - domain corpora .
While the quality of other systems improve about 3 BLEU points , because of their large and broad general - domain corpora .
Lang. N=0 N=25 N=50 N=75 N=100 en-cs 17.57 +0.84 +1.53 + 1.74 + 2.55 cs-en 31.29 +2.03 +3.12 +3.12 +2.24 en-fr 38.36 + 3.87 + 3.66 + 3.53 + 2.88 fr-en 44.36 + 1.29 +3.36 + 1.84 + 1.65 en-de 18.01 +0.02 -0.13 -0.07 0 de-en 32.50 -0.12 +0.06 +0.31 + 0.24 Table 5 : BLEU scores of TM adapted systems .
Numeric Adaptation
As stated in Section 3 , numeric occurs frequently in medical texts .
However , numeric expression in dates , time , measuring unit , chemical formula are often sparse , which may lead to OOV problems in phrasal translation and reordering .
Replacing the sparse numbers with placeholders may produce more reliable statistics for the MT models .
Moses has support using placeholders in training and decoding .
Firstly , we replace all the numbers in monolingual and parallel training corpus with a common symbol ( a sample phrase is illustrated in Fig. 2 ) .
Models are then trained on these processed data .
We use the XML markup translation method for decoding .
Original : Vitamin D 1,25 - OH Replaced : Vitamin D @num@ , @num@-OH
Table 6 shows the results on this number adaptation approach as well as the improvements compared to the Baseline2 .
The method improves the Baseline2 systems by 0.23 to 0.40 BLEU scores .
Although the scores increase slightly , we still believe this adaptation method is significant for medical domain .
The WMT2014 medical task only focuses on the summary of medical text , which may contain fewer chemical expression in compared with the full article .
As the used of numerical instances increases , placeholder may play a more important role in domain adaptation .
Lang
Hyphenated Word Adaptation Medical texts prefer a kind of compound words , hyphenated words , which is composed of more than one word .
For instance , " slow- growing " and " easy - to - use " are composed of words and linked with hyphens .
These hyphenated words occur quite frequently in medical texts .
We analyze the development sets of cs , fr , en and de respectively , and observe that there are approximately 3.2 % , 11.6 % , 12.4 % and 19.2 % of sentences that contain one or more hyphenated words .
The high ratio of such compound words results in Out - Of - Vocabulary words ( OOV ) 10 , and harms the phrasal translation and reordering .
However , a number of those hyphenated words still have chance to be translated , although it is not precisely , when they are tokenized into individual words .
To resolve this problem , we present an alternative -translation method in decoding .
Table 7 shows the proposed algorithm .
Algorithm
In the implementation , we apply XML markup to record the translation ( terminology ) for each compound word .
During the decoding , a hyphenated word delimited with markup will be replaced with its corresponding translation .
Table 8 shows the BLEU scores of adapted systems applied to hyphenated translation .
This method is effective for most language pairs .
While the translation systems for en-cs and cs-en do not benefit from this adaptation , because the hyphenated words ratio in the en and cs dev are asymmetric .
Thus , we only apply this method for en-fr , fr-en , de-en and en-de pairs .
Lang
Final Results and Conclusions
According to the performance of each individual domain adaptation approach , we combined the corresponding models for each language pair .
In Table 10 , we show the BLEU scores and its increments ( compared to the Baseline2 ) of combined systems in the second column .
The official test set is converted into the recased and detokenized SGML format .
The official results of our submissions are given in the last column of Table 9 .
This paper presents a set of experiments conducted on all available training data for six language pairs .
We explored various domain adaptation approaches for adapting medical transla-tion systems .
Compared with other methods , language model adaptation and translation model adaptation are more effective .
Other adapted techniques are still necessary and important for building a real-life system .
Although all individual methods are not fully additive , combining them together can further boost the performance of the overall domain-specific system .
We believe these empirical approaches could be valuable for SMT development .
Lang .
Pair
BLEU of Figure 1 . 1 Figure 1 . Examples of the segments with symbols in medical texts .
Figure 2 . 2 Figure 2 . Examples of placeholders .
Baseline1 Baseline2 Diff. en-cs 12.92 17.57 +4.65 cs-en 20.85 31.29 +10.44 en-fr 38.31 38.36 + 0.05 fr-en 44.27 44.36 +0.09 en-de 17.81 18.01 +0.20 de-en 32.34 32.50 + 0.16
Table 1 : BLEU scores of two baseline systems trained on original and processed corpora for different language pairs .
Table 3 : 3 BLEU scores of LM adapted systems .
en-cs 17.57 + 1.66 +2.08 +1.72 +2.04 cs-en 31.29 +0.94 +0.60 +0.66 +0.47 en-fr 38.36 + 1.82 +1.66 + 1.60 +0.08 fr-en 44.36 +0.91 +1.09 +1.12 +0.92 en-de 18.01 +0.57 + 1.02 -4.48 - 4.54 de-en 32.50 +0.60 +0.50 +0.56 +0.38
. Pair BLEU Diff. Corpus en-cs cs-en 21.53 + 3.96 + 601.89 % 33.01 + 1.72 en- fr fr-en 41.57 +3.21 +693.59 % 47.33 + 2.97 en-de de-en 16.54 - 1.47 + 13.63 % 27.35 - 5.15
Table 4 : 4 The BLEU scores of systems trained on general- domain corpora .
. Pair BLEU ( Dev ) Diff. en-cs 17.80 +0.23 cs-en 31.52 +0.23 en-fr 38.72 + 0.36 fr-en 44.69 +0.33 en-de 18.41 + 0.40 de-en 32.88 + 0.38
Table 6 : BLEU scores of numeric adapted sys- tems .
. Pair BLEU ( Dev ) Diff. en-cs 16.84 -0.73 cs-en 31.23 -0.06 en-fr 39.12 +0.76 fr-en 45.02 +0.66 en-de 18.64 +0.63 de-en 33.01 +0.51
Table 8 : 8 BLEU scores of hyphenated word adapted systems .
Table 9 : 9 BLEU scores of the submitted systems for the medical translation task .
Com- Official bined systems BLEU en-cs 23.66 ( + 6.09 ) 22.60 cs-en 38.05 ( + 6.76 ) 37.60 en-fr 42.30 ( + 3.94 ) 41.20 fr-en 48.25 ( + 3.89 ) 47.10 en-de 21.14 ( + 3.13 ) 20.90 de-en 36.03 ( + 3.53 ) 35.70
http://www.statmt.org/wmt14/translation-task.html.
2 http://www.statmt.org/wmt14/medical-task/.
3 http://www.statmt.org/moses/.
4 http://www.kyloo.net/software/doku.php/mgiza:overview.
http://www.speech.sri.com/projects/srilm/.
6 http://www.nactem.ac.uk/y-matsu/geniass/.
Data are processed according to Moses baseline tutorial : http://www.statmt.org/moses/?n=Moses.Baseline.
