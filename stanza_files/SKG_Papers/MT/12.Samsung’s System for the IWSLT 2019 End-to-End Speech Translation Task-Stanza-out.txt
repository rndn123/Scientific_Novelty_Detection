title
Samsung 's System for the IWSLT 2019 End-to - End Speech Translation Task
abstract
This paper describes the submission to IWSLT 2019 Endto - End speech translation task by Samsung R&D Institute , Poland .
We decided to focus on end-to- end English to German TED lectures translation and did not provide any submission for other speech tasks .
We used a slightly altered Transformer [ 1 ] architecture with standard convolutional layer preparing the audio input to Transformer encoder .
Additionally , we propose an audio segmentation algorithm maximizing BLEU score on tst2015 test set .
Introduction
This paper describes the submission to IWSLT 2019 Endto-End Speech Translation task by Samsung R&D Institute , Poland .
System architecture and data preparation techniques were designed before IWSLT 2019 data were released .
We have been using LibriSpeech corpus [ 2 ] to develop these techniques .
We evaluated our models on TED 2010 test set .
After IWSLT 2019 training data was published we gathered successful techniques to train a systems for this competition .
These techniques were used in all three models that will be presented here .
Document structure is as follows .
Firstly we describe data preparation and augmentation .
Then we provide system specification and training procedure used in our experiments .
We describe data segmentation algorithm used to segment test sets TED 2015 and TED 2019 .
We show results of our experiments with monolingual data and simple recurrent unit ( SRU ) [ 3 ] .
Finally we conclude our results .
Training Data
To train our system we used only IWSLT 2019 permissible audio corpora - iwslt-corpus , TEDLIUM2 [ 4 ] and MUST -C corpus [ 5 ] .
Data preparation process started with data filtering .
Then we generated synthetic target sentences with textto-text machine translation model .
We augmented audio input with sox 1 . Finally we included monolingual text data with empty audio input .
1 SoX - Sound eXchange sox.sourceforge.net v14.4.1
Data filtration
We trained English ASR system that was used to filter iwsltcorpus and TEDLIUM2 corpora .
We removed cases where WER score exceeded 75 % when comparing ASR output and English reference .
We decided that MUST - C corpus does not need filtration .
Additionally we filtered iwslt-corpus with regard to quality of translation using statistical dictionarybased methods .
Size of the corpora before and after filtration is shown in Table 1 Training data for these systems has been prepared with our in -house data preparation pipeline .
We also used synthetic translations as an alternative translation in iwslt-corpus when augmenting it .
To diversify target data as much as possible , for each example created in augmentation process , we generated 4 translations , 2 per each MT model .
Such a technique was described in Jia et al . [ 6 ] .
Number of training examples with synthetic data are shown in Table 2 .
Data Augmentation
We augmented the data by processing the audio files with three sox 's effects : tempo , speed and echo .
We sampled the parameters with uniform distribution within ranges presented in Table 3 .
For each file we repeated the process four times .
As a result we had five times larger audio corpus with synthetic Corpora Ref.
MT
Monolingual data
Similarly to pipeline systems , quality of End-to - end speech translation system depends on accuracy of extracted audio features from the source input as well as on quality of target generation .
E2E system can be improved by either introducing more variation of input speech or more variation of targets .
In Transformer decoder architecture the target selfattention layer works as a language model - depending on the previous decoded symbols it predicts the next one without looking at encoder output .
This led us to believe we could add monolingual target data to the training corpus .
To such monolingual data we attached an empty audio input to train just the self-attention part of the decoder .
To choose this monolingual data we randomly selected 15 million sentences from Paracrawl and Europarl and trained a language model on these sentences .
Next we trained a language model on TED corpora [ 7 ] and used cross-entropy difference scor-ing [ 8 ] to choose 2 million sentences closest to TED talks .
BLEU scores of this model can be found in Table 7 .
Our test on a non-agumented data showed significant improvement of translation quality ( 15.23 vs 16.74 BLEU ) , however in the end , it was not the case when data was agumented .
To our best knowledge such an approach has never been described in literature before .
E2E Speech Translation System
In this section we will describe architecture of end-to - end spoken language translation system .
ASR Transformer for SLT
As a baseline system we used Transformer architecture and hparams transformer librispeech v2 for automatic speech recognition implemented in TensorFlow .
The targets , however were translation instead of transcripts .
The Transformer has hidden layer of size 384 , convolutional ( kernel size 9 ) feed forward layer of size 1536 , 2 - head self-attention , 6 encoder layers and 4 decoder layers .
Audio data is turned into log mel spectrogram with frame size of 25 ms , frame step 10 ms and 80 filters .
To log mel spectrograms we apply 2D 3x3 convolution twice with stride 2x2 and 128 filters and then 3x80 convolution to reduce the spectrogram to a vector , exactly like in the case of ASR .
Dense Feed Forward Layer in Decoder
We also proposed a change to the baseline ASR architecture : use dense feed forward layer of the same size in the decoder layer instead of convolution .
The rationale behind it being the fact that standard text Transformer uses such feed forward layer for generating translation .
For output of the decoder we use a standard representation used in text to text translation - subword data tokenizations with dictionary of size 32k .
Dual learning : ASR and SLT tasks Additionally we introduced a second decoder with ASR task , making it a multitask setup similar to [ 9 ] .
A separate dictionary of size 32 k was used for this task .
In such a setup loss is calculated with two targets - one in English and one in German .
Two decoders with different weights are simultaneously trained on these targets .
An experiment on nonaugmented data showed almost 2 BLEU increase ( 15.23 vs 17.15 on tst2010 ) compared to the same model trained on a single task .
SRU Recurrent layers in Encoder
The sequences processed by the Transformer encoder are at least 4 times longer in the case of speech translation than in the case of text translation .
We tried to contract these sequences further with convolutions to be able to use deeper and still fast encoder .
Unfortunately , this resulted in a signif-icant reduction of BLEU .
The best BLEU score was achieved after introducing 4 layers of Simple recurrent unit and then kernel 3 , stride 2 convolution applied twice before the encoder .
Number of encoder layers were increased to 8 and and embedding size to 512 without loosing speed of the decoding .
Spectrogram masking
To augment data even more we implemented spectrogram masking technique described in Park et al . [ 10 ]
This technique involves masking the spectrogram for a range of frequencies and period of time .
In our implementation we chose to introduce three such masks for frequency .
The width of frequency range is selected randomly between values 5 and 10 .
This means that out of 80 filters 15 to 30 are masked .
In time we chose one mask for every 300 time steps .
Again , length of such mask is random between 10 and 20 time steps .
Training process : Adam Multistep optimizer
We trained our primary model on 4 GTX 1080 Ti GPUs for about a week , which resulted in 800k steps .
SRU model was trained slightly longer - for 1 million steps .
The model trained on additional monolingual data was trained for 3 million steps .
Instead of using standard Adam optimizer , we used Adam Multistep optimizer updating weights every 32 batches .
As a result effective batch size is increased .
Without Multistep optimizer , models did not learn at all , possibly because batch size in this case is just a few utterances per card .
Our early experiments on LibriSpeech data showed the best performance for multistep value of 32 , for higher values the model trained much more slowly .
In the case of all trainings 10 % dropout was applied .
Model averaging
For a final validation we averaged last 7 checkpoints of the training .
Averaging checkpoints almost always resulted in higher BLEU scores .
We experimented with continuation of training after averaging but it did not give any better results .
Segmentation
This year 's IWSLT formula allows the submissions to be based on a custom audio segmentation , which directs a part of the research effort towards finding the optimal method of splitting the input of a end-to - end model .
The considered segmentation methods can be described as time - and featurebased .
The time - based algorithms split the audio file in consecutive windows of constant or varying size and are completely ignoring the content of the audio , while the featurebased solutions extract and analyze specially designed traits of the input to enhance the division process .
To acquire a simple yet effective segmentation algorithm , the method of choice was based on a silence periods between utterances .
The reasoning behind such a selection is quite in - Figure 1 : Dependency of BLEU on maximal segment lenght duration tuitive .
In general , the speakers tend to make longer pauses between separate sentences than between the words in a single sentence .
To incorporate this observation , the designed method , further called DIV , utilizes the divide- and - conquer approach .
Firstly we segment the audio with Audacity tool 2
Parameter tst2010 Max silence 26 - db Silence min duration 0.2s Label starting point 0.2s Label ending point 0.3s Table 5 : Audacity parameters for silence and speech recognition to detect speech and salience periods .
The method recursively splits the audio file and later its parts into 2 recordings at the point where the utterances are separated by the longest silence period .
The algorithm finishes when no further splits are possible , that is when the lengths of all created parts are not longer than the user-specified threshold or contain a single utterance .
To find the optimal value of the threshold parameter , multiple values were tested using the IWSLT 2015 dataset .
The duration of 11 seconds was found to be the most beneficial yielding the BLEU score of 16.32 .
An analysis of different segmentation algorithms and the parameter tuning of a selected method allowed to observe the significant influence of a segmentation method on the produced translation .
For comparison , the use of pre-processed segmentation resulted in a BLEU score of 12.38 while the sub-optimally parameterized DIV algorithm scored 15.12 BLEU , see Figure 1 .
Table 7 shows comparison of the results for models trained on full permissible audio corpora .
Primary system is a ASR Transformer with dense feed -forward layer in decoder , spectrogram masking , trained with a dual ASR task and averaged snapshots .
SRU system is the same as above but recurrent layers in encoder were added .
Finally Mono system is the same as Primary but trained with additional monolingual data .
Additionally we compare the results to our general purpose ASR + MT pipeline system trained on unconstrained data .
Evaluation
Model
Conclusions
We presented three end-to - end speech translation models .
Our primary model achieved quality comparable to pipeline production systems Table 7 .
Unfortunately , introducing monolingual data to the training did not result in higher BLEU score on TED test sets .
However this model might be better on other domains .
We also showed an alternative architecture with slightly lower quality than primary system .
This alternative architecture could be further improved in order to achieve higher decoding speed .
Based on these results we conclude E2E models will challenge pipeline systems in the near future .
Table 1 : 1 . Corpora Orig. size Filtered Length iwslt-corpus ( ASR ) 171121 158737 224h + trans .
quality
158737 126817 188h TEDLIUM2 92973 90715 197h MUST -C 229703 229703 400h 2.2 .
Synthetic target data TEDLIUM2 corpus did not provide any German translations , therefore we generated synthetic targets using two Trans - former Big MT systems trained with different hyperparame - ters on WMT data - Paracrawl , Europarl and OpenSubtitles .
Size ( number of audio utterances ) of the training corpora before and after filtration .
Iwslt-corpus ( ASR ) is corpus filtered by ASR only .
The last column is total audio length after filtration .
Table 3 : 3 Sox parameters value ranges used in processing of audio data .
Echo effect is parametrised by two values .
- 1 MT -2 iwslt-corpus 126817 2x158737 2x158737 TEDLIUM2 0 3x90715 3x90715 MUST -C 229703 0 0 Table 2 : Size ( number of text lines ) of the training corpora with synthetic data .
For each model two or three best beam results have been used .
Option Min value Max value tempo 0.85 1.3 speed 0.95 1.05 echo delay 20 200 echo decay 5 20 Corpora Orig. & Augm .
Length iwslt-corpus 761765 1084h TEDLIUM2 544290 1182h MUST -C 918812 1600h Total 2224867 3866h translations for roughly half of the audio files .
The range of speed option is very small because we did not want our model to train on an unnaturally sounding samples .
The rationale behind using echo option is the fact that many TED lectures have a significant echo .
Final number of training audio examples is shown in Table 4 .
Table 4 : 4 Size of the training audio corpora with data augmentation .
Number of distinct audio and text pairs .
Table 6 6 presents experiment performed on the LibriSpeech training data set evaluated on TED 2010 .
In the case of these trainings data augmentation was applied only once .
Model tst2010 ASR
Transformer for SLT baseline 12.76 + Model averaging 12.99 + Dense FF in Decoder 13.20 + Spectogram masking 13.74 + Data augmentation ( speed x1 ) 14.85 + 8 head attention 15.31 + Data augmentation ( speed + echo x1 ) 15.56
Table 6 : 6 BLEU scores for models trained on LibriSpeech data .
Each subsequent model includes all the previous techniques in the table .
Table 7 : 7 BLEU scores for our three models .
SRU model is an alternative architecture with simple recurrent unit and Mono model was trained with additional monolingual data .
tst2010 tst2015 tst2019 Primary 25.81 21.29 19.96 SRU 25.70 19.08 18.83 Mono 23.99 20.81 19.36 ASR +MT 23.58 19.96 -
Audacity audacityteam.org v2.3.2
