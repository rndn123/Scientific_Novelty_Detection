title
Coverage Embedding Models for Neural Machine Translation
abstract
In this paper , we enhance the attention - based neural machine translation ( NMT ) by adding explicit coverage embedding models to alleviate issues of repeating and dropping translations in NMT .
For each source word , our model starts with a full coverage embedding vector to track the coverage status , and then keeps updating it with neural networks as the translation goes .
Experiments on the large-scale Chinese- to - English task show that our enhanced model improves the translation quality significantly on various test sets over the strong large vocabulary NMT system .
Introduction Neural machine translation ( NMT ) has gained popularity in recent years ( e.g. Jean et al. , 2015 ; Luong et al. , 2015 ; Mi et al. , 2016 b ; ) , especially for the attentionbased models of .
The attention at each time step shows which source word the model should focus on to predict the next target word .
However , the attention in each step only looks at the previous hidden state and the previous target word , there is no history or coverage information typically for each source word .
As a result , this kind of model suffers from issues of repeating or dropping translations .
The traditional statistical machine translation ( SMT ) systems ( e.g. ( Koehn , 2004 ) ) address the above issues by employing a source side " coverage vector " for each sentence to indicate explicitly which words have been translated , which parts have not yet .
A coverage vector starts with all zeros , meaning no word has been translated .
If a source word at position j got translated , the coverage vector sets position j as 1 , and they wo n't use this source word in future translation .
This mechanism avoids the repeating or dropping translation problems .
However , it is not easy to adapt the " coverage vector " to NMT directly , as attentions are soft probabilities , not 0 or 1 .
And SMT approaches handle one-tomany fertilities by using phrases or hiero rules ( predict several words in one step ) , while NMT systems only predict one word at each step .
In order to alleviate all those issues , we borrow the basic idea of " coverage vector " , and introduce a coverage embedding vector for each source word .
We keep updating those embedding vectors at each translation step , and use those vectors to track the coverage information .
Here is a brief description of our approach .
At the beginning of translation , we start from a full coverage embedding vector for each source word .
This is different from the " coverage vector " in SMT in following two aspects : ? each source word has its own coverage embedding vector , instead of 0 or 1 , a scalar , in SMT , ? we start with a full embedding vector for each word , instead of 0 in SMT .
After we predict a translation word y t at time step t , we need to update each coverage embedding vector accordingly based on the attentions in the current step .
Our motivation is that if we observe a very high attention over x i in this step , there is a high chance that x i and y t are translation equivalent .
So the embedding vector of x i should come to empty ( a zero vector ) in a one- to - one translation case , or subtract the embedding of y t for the one-to -many translation case .
An empty coverage embedding of a word x i indicates this word is translated , and we can not translate x i again in future .
Empirically , we model this procedure by using neural networks ( gated recurrent unit ( GRU ) or direct subtraction ) .
Large-scale experiments over Chinese-to - English on various test sets show that our method improves the translation quality significantly over the large vocabulary NMT system ( Section 5 ) .
s t 1 s t ?
o t y 1 ? ? y | V y | ? ?
H t = l X i=1 ( ? ti ? h i ) l X i=1 ( ? ti ? ! h i ) x 1 x l h 1 h l ! h l ! h 1 ? ? ? x 2 ! h 2 h 2 x 1 x l h 1 h l ! h l ! h 1 ? ? ? h j ! h j x j ? ? ? y ?
t 1 y ? t s t c t 1 , x j ? t , j = exp( e t , j ) P l i=1 exp( e t , i ) A t , j e t , j e t,l e t,1 ? ? h i and ? ? h i are bi-directional encoder states .
?
t , j is the attention probability at time t , position j .
H t is the weighted sum of encoding states .
s t is a hidden state .
o t is an output state .
Another one layer neural network projects o t to the target output vocabulary , and conducts softmax to predict the probability distribution over the output vocabulary .
The attention model ( in right gray box ) is a two layer feedforward neural network , A t , j is an intermediate state , then another layer converts it into a real number e t , j , the final attention probability at position j is ?
t , j . We plug coverage embedding models into NMT model by adding an input c t?1 , xj to A t , j ( the red dotted line ) .
?
t,1 ? t,2 ? t,l
Neural Machine Translation
As shown in Figure 1 , attention - based neural machine translation is an encoder-decoder network .
the encoder employs a bidirectional recurrent neural network to encode the source sentence x = ( x 1 , ... , x l ) , where l is the sentence length , into a sequence of hidden states h = ( h 1 , ... , h l ) , each h i is a concatenation of a left - to -right ? ? h i and a right- to- left ? ? h i , h i = ? ? h i ? ? h i = ? ? f ( x i , ? ? h i +1 ) ? ? f ( x i , ? ? h i?1 ) , where ? ? f and ? ? f are two GRUs .
Given the encoded h , the decoder predicts the target translation by maximizing the conditional log-probability of the correct translation y * = ( y * 1 , ...y * m ) , where m is the sentence length .
At each time t , the probability of each word y t from a target vocabulary V y is : p( y t |h , y * t?1 ..y * 1 ) = g( s t , y * t?1 ) , ( 1 ) where g is a two layer feed - forward neural network ( o t is a intermediate state ) over the embedding of the previous word y * t?1 , and the hidden state s t .
The s t is computed as : s t = q(s t?1 , y * t?1 , H t ) ( 2 ) H t = l i=1 ( ?
t , i ? ? ? h i ) l i=1 ( ? t , i ? ? ? h i ) , ( 3 ) where q is a GRU , H t is a weighted sum of h , the weights , ? , are computed with a two layer feedforward neural network r : ?
t , i = exp{r(s t?1 , h i , y * t?1 ) } l k=1 exp{r(s t?1 , h k , y * t?1 ) } ( 4 )
Coverage Embedding Models
Our basic idea is to introduce a coverage embedding for each source word , and keep updating this embedding at each time step .
Thus , the coverage embedding for a sentence is a matrix , instead of a vector in SMT .
As different words have different fertilities ( one - to- one , one - to- many , or one- to- zero ) , similar to word embeddings , each source word has its own coverage embedding vector .
For simplicity , the number of coverage embedding vectors is the same as the source word vocabulary size .
At the beginning of our translation , our coverage embedding matrix ( c 0 , x 1 , c 0 , x 2 , ...c 0 , x l ) is initialized with the coverage embedding vectors of all the source words .
Then we update them with neural networks ( a GRU ( Section 3.1.1 ) or a subtraction ( Section 3.1.2 ) ) , l l 1 , l y t 1 ? ? ? ? y t ? ? ? t 1 , j ? t , j c t 2 , x 1 c t 2 , x j c t 2 , x l c t 1 , x l c t 1 , x j c t 1 , x 1 c t , x 1 c t , x j c t, x l Figure 2 : The coverage embedding model with a GRU at time step t ?
1 and t. c 0,1 to c 0,l are initialized with the word coverage embedding matrix until we translation all the source words .
In the middle of translation , some coverage embeddings should be close to zero , which indicate those words are covered or translated , and can not be translated in future steps .
Thus , in the end of translation , the embedding matrix should be close to zero , which means all the words are covered .
In the following part , we first show two updating methods , then we list the NMT objective that takes into account the embedding models .
Updating Methods
Updating with a GRU Figure 2 shows the updating method with a GRU .
Then , at time step t , we feed y t and ?
t , j to the coverage model ( shown in Figure 2 ) , z t , j = ?( W zy y t + W z? ? t , j + U z c t?1 , x j ) r = ?( W ry y t + W r? ? t , j U r c t?1 , x j ) ct , x j = tanh ( W y t + W ? ? t , j + r t , j ? U c t?1 , x j ) c t , x j = z t , j ? c t?1 , x j + ( 1 ? z t , j ) ? ct , x j , where , z t is the update gate , r t is the reset gate , ct is the new memory content , and c t is the final memory .
The matrix W zy , W z? , U z , W ry , W r? , U r , W y , W ? and U are shared across different position j. ? is a pointwise operation .
Updating as Subtraction
Another updating method is to subtract the embedding of y t directly from the coverage embedding c t , x j with a weight ?
t , j as c t, x j = c t?1 , x j ? ? t , j ? ( W y?c y t ) , ( 5 ) where W y?c is a matrix that coverts word embedding of y t to the same size of our coverage embedding vector c .
Objectives
We integrate our coverage embedding models into the attention NMT by adding c t?1 , x j to the first layer of the attention model ( shown in the red dotted line in Figure 1 ) .
Hopefully , if y t is partial translation of x j with a probability ?
t , j , we only remove partial information of c t?1 , x j .
In this way , we enable coverage embedding c 0 , x j to encode fertility information of x j .
As we have mentioned , in the end of translation , we want all the coverage embedding vectors to be close to zero .
So we also minimize the absolute values of embedding matrixes as ?
* = arg max ?
N n=1 m t=1 log p(y * n t |x n , y * n t?1 ..y * n 1 ) ? ? l i=1 ||c m , x i || , ( 6 ) where ? is the coefficient of our coverage model .
As suggested by Mi et al . ( 2016 a ) , we can also use some supervised alignments in our training .
Then , we know exactly when each c t , x j should become close to zero after step t.
Thus , we redefine Equation 6 as : ? * = arg max ?
N n=1 m t=1 log p(y * n t |x n , y * n t?1 ..y * n 1 ) ? ? l i=1 ( m j=ax i ||c j , x i || ) , ( 7 ) where a x i is the maximum index on the target sentence x i can be aligned to .
Related Work
There are several parallel and independent related work ( Tu et al. , 2016 ; Feng et al. , 2016 ; Cohn et al. , 2016 ) .
Tu et al. ( 2016 ) is the most relevant one .
In their paper , they also employ a GRU to model the coverage vector .
One main difference is that our model introduces a specific coverage embedding vector for each source word , in contrast , their work initializes the word coverage vector with a scalar with a uniform distribution .
Another difference lays in the fertility part , Tu et al . ( 2016 ) add an accumulate operation and a fertility function to simulate the process of one- to -many alignments .
In our approach , we add fertility information directly to coverage embeddings , as each source word has its own embedding .
The last difference is that our baseline system ( Mi et al. , 2016 b ) is an extension of the large vocabulary NMT of Jean et al . ( 2015 ) with candidate list decoding and UNK replacement , a much stronger baseline system .
Cohn et al. ( 2016 ) augment the attention model with well -known features in traditional SMT , including positional bias , Markov conditioning , fertility and agreement over translation directions .
This work is orthogonal to our work .
Experiments
Data Preparation
We run our experiments on Chinese to English task .
We train our machine translation systems on two training sets .
The first training corpus consists of approximately 5 million sentences available within the DARPA BOLT Chinese -English task .
The second training corpus adds HK Law , HK Hansard and UN data , the total number of training sentence pairs is 11 million .
The Chinese text is segmented with a segmenter trained on CTB data using conditional random fields ( CRF ) .
Our development set is the concatenation of several tuning sets ( GALE Dev , P1R6 Dev , and Dev 12 ) released under the DARPA GALE program .
The development set is 4491 sentences in total .
Our test sets are NIST MT06 , MT08 news , and MT08 web .
For all NMT systems , the full vocabulary sizes for thr two training sets are 300k and 500k respectively .
The coverage embedding vector size is 100 .
In the training procedure , we use AdaDelta ( Zeiler , 2012 ) to update model parameters with a mini-batch size 80 .
Following Mi et al. ( 2016 b ) , the output vocabulary for each mini-batch or sentence is a sub-set of the full vocabulary .
For each source sentence , the sentence - level target vocabularies are union of top 2 k most frequent target words and the top 10 candidates of the word- to- word / phrase translation tables learned from ' fast align ' ( Dyer et al. , 2013 ) .
The maximum length of a source phrase is 4 .
In the training time , we add the reference in order to make the translation reachable .
Following Jean et al. ( 2015 ) ,
We dump the align-ments , attentions , for each sentence , and replace UNKs with the word-to - word translation model or the aligned source word .
Our traditional SMT system is a hybrid syntaxbased tree-to-string model ( Zhao and Al-onaizan , 2008 ) , a simplified version of Liu et al . ( 2009 ) and Cmejrek et al . ( 2013 ) .
We parse the Chinese side with Berkeley parser , and align the bilingual sentences with GIZA ++.
Then we extract Hiero and tree-to-string rules on the training set .
Our two 5 gram language models are trained on the English side of the parallel corpus , and on monolingual corpora ( around 10 billion words from Gigaword ( LDC2011T07 ) ) , respectively .
As suggestion by Zhang ( 2016 ) , NMT systems can achieve better results with the help of those monolingual corpora .
We tune our system with PRO ( Hopkins and May , 2011 ) to minimize ( TER - BLEU ) / 2 on the development set .
Translation Results
Table 1 shows the results of all systems on 5 million training set .
The traditional syntax - based system achieves 9.45 , 12.90 , and 17.72 on MT06 , MT08 News , and MT08 Web sets respectively , and 13.36 on average in terms of ( TER - BLEU ) / 2 .
The largevocabulary NMT ( LVNMT ) , our baseline , achieves an average ( TER - BLEU ) / 2 score of 15.74 , which is about 2 points worse than the hybrid system .
We test four different settings for our coverage embedding models : ?
U GRU : updating with a GRU ; ?
U Sub : updating as a subtraction ; ? U GRU + U Sub : combination of two methods ( do not share coverage embedding vectors ) ; ? + Obj. : U GRU + U Sub plus an additional objective in Equation 6 1 . U GRU improves the translation quality by 1.3 points on average over LVNMT .
And U GRU + U Sub achieves the best average score of 13.14 , which is about 2.6 points better than LVNMT .
All the improvements of our coverage embedding models over LVNMT are statistically significant with the signtest of Collins et al . ( 2005 ) .
We believe that we need to explore more hyper-parameters of + Obj . in order to get even better results over U GRU + U Sub .
Table 2 shows the results of 11 million systems , LVNMT achieves an average ( TER - BLEU ) / 2 of 13.27 , which is about 2.5 points better than 5 million LVNMT .
The result of our U GRU coverage model gives almost 1 point gain over LVNMT .
Those results suggest that the more training data we use , the stronger the baseline system becomes , and the harder to get improvements .
In order to get a reasonable or strong NMT system , we have to conduct experiments over a large-scale training set .
Alignment Results
Table 3 shows the F1 scores on the alignment test set ( 447 hand aligned sentences ) .
The MaxEnt model is trained on 67 k hand - aligned data , and achieves an F1 score of 75.96 .
For NMT systems , we dump alignment matrixes , then , for each target word we only add the highest probability link if it is higher than 0.2 .
Results show that our best coverage model , U GRU + U Sub , improves the F1 score by 2.2 points over the sorce of LVNMT .
We also check the repetition statistics of NMT outputs .
We simply compute the number of repeated
Conclusion
In this paper , we propose simple , yet effective , coverage embedding models for attention - based NMT .
Our model learns a special coverage embedding vector for each source word to start with , and keeps updating those coverage embeddings with neural networks as the translation goes .
Experiments on the large-scale Chinese- to - English task show significant improvements over the strong LVNMT system .
Figure 1 : 1 Figure 1 : The architecture of attention - based NMT .
The source sentence is x = ( x 1 , ... , x l ) with length l , the translation is y * = ( y * 1 , ... , y * m ) with length m.?
?
h i and ? ? h i are bi-directional encoder states .
?
t , j is the attention probability at time t , position j .
H t is the weighted sum of encoding states .
s t is a hidden state .
o t is an output state .
Another one layer neural network projects o t to the target output vocabulary , and conducts softmax to predict the probability distribution over the output vocabulary .
The attention model ( in right gray box ) is a two layer feedforward neural network , A t , j is an intermediate state , then another layer converts it into a real number e t , j , the final attention probability at position j is ?
t , j . We plug coverage embedding models into NMT model by adding an input c t?1 , xj to A t , j ( the red dotted line ) .
