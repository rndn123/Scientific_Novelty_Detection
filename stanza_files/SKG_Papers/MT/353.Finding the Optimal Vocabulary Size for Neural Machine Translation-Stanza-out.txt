title
Finding the Optimal Vocabulary Size for Neural Machine Translation
abstract
We cast neural machine translation ( NMT ) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components .
Classifiers are known to perform better with balanced class distributions during training .
Since the Zipfian nature of languages causes imbalanced classes , we explore its effect on NMT .
We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes , and reveal an explanation for why certain vocabulary sizes are better than others .
1
Introduction Natural language processing ( NLP ) tasks such as sentiment analysis ( Maas et al. , 2011 ; Zhang et al. , 2015 ) and spam detection are modeled as classification tasks , where instances are independently labeled .
Tasks such as part- of-speech tagging ( Zeman et al. , 2017 ) and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) are examples of structured classification tasks , where instance classification is decomposed into a sequence of per-token contextualized labels .
We can similarly cast neural machine translation ( NMT ) , an example of a natural language generation ( NLG ) task , as a form of structured classification , where an instance label ( a translation ) is generated as a sequence of contextualized labels , here by an autoregressor ( see Section 2 ) .
Since the parameters of modern machine learning ( ML ) classification models are estimated from training data , whatever biases exist in the training data will affect model performance .
Among those biases , class imbalance is a topic of our interest .
Class imbalance is said to exist when one or more classes are not of approximately equal frequency in data .
The effect of class imbalance has been extensively studied in several domains where classifiers are used ( see Section 6.3 ) .
With neural networks , the imbalanced learning problem is mostly targeted to computer vision tasks ; NLP tasks are under-explored ( Johnson and Khoshgoftaar , 2019 ) .
Word types in natural language models resemble a Zipfian distribution , i.e. in any natural language corpus , we observe that a type 's rank is roughly inversely proportional to its frequency .
Thus , a few types are extremely frequent , while most of the rest lie on the long tail of infrequency .
Zipfian distributions cause two problems in classifier - based NLG systems : 1 . Unseen Vocabulary :
Any hidden data set may contain types not seen in the finite set used for training .
A sequence drawn from a Zipfian distribution is likely to have a large number of rare types , and these are likely to have not been seen in training .
Imbalanced Classes :
There are a few extremely frequent types and many infrequent types , causing an extreme imbalance .
Such an imbalance , in other domains where classifiers are used , has been known to cause undesired biases and severe performance degradation ( Johnson and Khoshgoftaar , 2019 ) .
The use of subwords , that is , decomposition of word types into pieces , such as the widely used Byte Pair Encoding ( BPE ) ( Sennrich et al. , 2016 ) addresses the open-ended vocabulary problem by ultimately allowing a word to be represented as a sequence of characters if necessary .
BPE has a single hyperparameter named merge operations that governs the vocabulary size .
The effect of this hyperparameter is not well understood .
In practice , it is either chosen arbitrarily or via trial - and - error ( Salesky et al. , 2018 ) .
Regarding the problem of imbalanced classes , Steedman ( 2008 ) states that " the machine learning techniques that we rely on are actually very bad at inducing systems for which the crucial information is in rare events . "
However , to the best of our knowledge , this problem has not yet been directly addressed in the NLG setting .
In this work , we attempt to find answers to these questions : ' What value of BPE vocabulary size is best for NMT ? ' , and more crucially an explanation for ' Why that value ?'.
As we will see , the answers and explanations for those are an immediate consequence of a broader question , namely ' What is the impact of Zipfian imbalance on classifier - based NLG ? '
The contributions of this paper are as follows :
We offer a simplified view of NMT architectures by re-envisioning them as two high- level components : a classifier and an autoregressor ( Section 2 ) .
We describe some of the desired settings for the classifier ( Section 2.1 ) and autoregressor ( Section 2.2 ) components .
In Section 2.3 , we describe how vocabulary size choice relates to the desired settings for the two components .
Our experimental setup is described in Section 3 , followed by an analysis of results in Section 4 that offers an explanation with evidence for why some vocabulary sizes are better than others .
Section 5 uncovers the impact of class imbalance , particularly frequency based discrimination on classes .
2 Section 6 provides an overview of related work , and in Section 7 we recommend a heuristic for choosing the BPE hyperparameter .
Classifier based NLG Machine translation is commonly defined as the task of transforming sequences from the form x = x 1 x 2 x 3 ...x m to y = y 1 y 2 y 3 ...y n , where x is in source language X and y is in target language Y .
There are many variations of NMT architectures ( Section 6.1 ) , however , all share the common objective of maximizing Q n t=1 P ( y t |y <t , x 1 : m ) for pairs ( x 1 : m , y 1:n ) sampled from a parallel dataset .
NMT architectures are commonly viewed as encoderdecoder networks .
We instead re-envision the NMT architecture as two higher level components : an autoregressor ( R ) and a token classifier ( C ) , as shown in Figure 1 . Autoregressor R , ( Box et al. , 2015 ) being the most complex component of the NMT model , has many implementations based on various neu - ral network architectures : recurrent neural networks ( RNN ) such as long short - term memory ( LSTM ) and gated recurrent unit ( GRU ) , convolutional neural networks ( CNN ) , and Transformer ( Section 6.1 ) .
At time step t , R transforms the input context y <t , x 1 : m into hidden state vector h t = R(y <t , x 1 : m ) .
Classifier C is the same across all architectures .
It maps h t to a distribution P ( y j | h t ) 8y j 2 V Y , where V Y is the vocabulary of Y .
In machine learning , input to classifiers such as C is generally described as features that are either hand -engineered or automatically extracted .
In our high- level view of NMT architectures , R is a neural network that serves as an automatic feature extractor for C .
Balanced Classes for Token Classifier Untreated , class imbalance leads to bias based on class frequencies .
Specifically , classification learning algorithms focus on frequent classes while paying relatively less importance to infrequent classes .
Frequency - based bias leads to poor recall of infrequent classes ( Johnson and Khoshgoftaar , 2019 ) .
When a model is used in a domain mismatch scenario , i.e. where test and training set distributions do not match , model performance generally degrades .
It is not surprising that frequency - biased classifiers show particular degradation in domain mismatch scenarios , as types that were infrequent in the training distribution and were ignored by the learning algorithm may appear with high frequency in the new domain .
Koehn and Knowles ( 2017 ) showed empirical evidence of poor generalization of NMT to out - of- domain datasets .
In other classification tasks , where each instance is classified independently , methods such as upsampling infrequent classes and down - sampling frequent classes are used .
In NMT , since classification is done within the context of sequences , it is possible to accomplish the objective of balancing by altering sequence lengths .
This can be done by choosing the level of subword segmentation ( Sennrich et al. , 2016 ) . Quantification of Zipfian Imbalance :
We use two statistics to quantify the imbalance of a training distribution :
The first statistic relies on a measure of Divergence ( D ) from a balanced ( uniform ) distribution .
We use a simplified version of Earth Mover Distance , in which the total cost for moving a probability mass between any two classes is the sum of the total mass moved .
Since any mass moved out of one class is moved into another , we divide the total per-class mass moves in half to avoid double counting .
Therefore , the imbalance measure D on K class distributions where p i is the observed probability of class i in the training data is computed as : D = 1 2 K X i=1 |p i 1 K | ; 0 ? D ? 1 A lower value of D is the desired setting for C , since the lower value results from a balanced class distribution .
When classes are balanced , they have approximately equal frequencies ; C is thus less likely to make errors due to class bias .
The second statistic is Frequency at 95th % Class Rank ( F 95 % ) , defined as the least frequency in the 95 th percentile of most frequent classes .
More generally , F P % is a simple way of quantifying the minimum number of training examples for at least theP th percentile of classes .
The bottom ( 1 P ) percentile of classes are overlooked to avoid the noise that is inherent in the real-world natural - language datasets .
A higher value for F 95 % is the desired setting for C , as a higher value indicates the presence of many training examples per class , and ML methods are known to perform better when there are many examples for each class .
Shorter Sequences for Autoregressor
Every autoregressive model is an approximation ; some may be better than others , but no model is perfect .
The total error accumulated grows in proportion to the length of the sequence .
These accumulated errors alter the prediction of subsequent tokens in the sequence .
Even though beam search attempts to mitigate this , it does not completely resolve it .
These challenges with respect to long sentences and beam size are examined by Koehn and Knowles ( 2017 ) .
We summarize sequence lengths using Mean Sequence Length , ? , computed trivially as the arithmetic mean of the lengths of target language sequences after encoding them : ? = 1 N P N i=1 |y ( i ) | where y ( i ) is the ith sequence in the training corpus of N sequences .
Since shorter sequences have relatively fewer places where an imperfectly approximated autoregressor model can make errors , a smaller ? is a desired setting for R .
Choosing the Vocabulary Size Systematically BPE ( Sennrich et al. , 2016 ) is a greedy iterative algorithm often used to segment a vocabulary into useful subwords .
The algorithm starts with characters as its initial vocabulary .
In each iteration , it greedily selects the most frequent type bigram in the training corpus , and replaces the sequence with a newly created compound type .
Once the subword vocabulary is learned , it can be applied to a corpus by greedily segmenting words with the longest available subword type .
These operations have an effect on D , F 95 % , and ?.
Effect of BPE on ? : BPE expands rare words into two or more subwords , lengthening a sequence ( and raising ? ) relative to simple white -space segmentation .
BPE merges frequent - character sequences into one subword piece , shortening a sequence ( and lowering ? ) relative to character segmentation .
Hence , the sequence length of BPE segmentation lies in between the sequence lengths obtained by white -space and character-only segmentation methods ( Morishita et al. , 2018 ) .
Effect of BPE on F 95 % and D : Whether BPE is viewed as a merging of frequent subwords into a relatively less frequent compound , or a splitting of rare words into relatively frequent subwords , BPE alters the class distribution by moving the probability mass of classes .
Hence , by altering the class distribution , BPE also alters both F 95 % and D .
The BPE hyperparameter controls the amount of probability mass moved between subwords and compounds .
Figure 2 shows the relation between number of BPE merges ( i.e. the BPE hyperparameter ) , and both D and ?.
When few BPE merge operations are performed , we observe the lowest value of D , which is a desired setting for C , but at the same point ? is large and undesired for R ( Section 2 ) .
When a large number of BPE merges are performed , the effect is reversed , i.e. we observe that D is large and unfavorable to C while ? is small and favorable to R .
In the following sections we describe our experiments and analysis to locate the optimal number of BPE merges that achieves the right trade - off for both C and R .
Experimental Setup Our NMT experiments use the base Transformer model ( Vaswani et al. , 2017 ) on four different target languages at various training data sizes , described in the following subsections .
Datasets
We use the following four language pairs for our analysis : English !
German , German !English , English ! Hindi , and English !
Lithuanian .
To analyze the impact of different training data sizes , we randomly sub-select smaller training corpora for English $ German and English !
Hindi languages .
Statistics regarding the corpora used for validation , testing , and training are in Table 1 .
The datasets for English $ German , and English !
Lithuanian are retrieved from the News Translation task of WMT2019 ( Barrault et al. , 2019 ) .
3 For English !
Hindi , we use the IIT Bombay Hindi-English parallel corpus v1.5 ( Kunchukuttan et al. , 3 http://www.statmt.org/wmt19/translation-task.html 2018 ) .
English , German , and Lithuanian sentences are tokenized using SACREMOSES .
4 Hindi sentences are tokenized using INDICNLPLIBRARY .
5
The training datasets are trivially cleaned : we exclude sentences with length in excess of five times the length of their parallel counterparts .
Since the vocabulary is a crucial part of this analysis , we exclude all sentence pairs containing URLs .
Hyperparameters
Our model is a 6 layer Transformer encoderdecoder that has 8 attention heads , 512 hidden vector units , and a feed forward intermediate size of 2048 , with GELU activation .
We use label smoothing at 0.1 , and a dropout rate of 0.1 .
We use the Adam optimizer ( Kingma and Ba , 2015 ) with a controlled learning rate that warms up for 16 K steps followed by the decay rate recommended for training Transformer models ( Popel and Bojar , 2018 ) .
To improve performance at different data sizes we set the mini-batch size to 6 K tokens for the 30K - sentence datasets , 12 K tokens for 0.5 Msentence datasets , and 24 K for the remaining larger datasets ( Popel and Bojar , 2018 ) .
All models are trained until no improvement in validation loss is observed , with a patience of 10 validations , each done at 1,000 update steps apart .
Our model is implemented using PyTorch and run on NVIDIA P100 and V100 GPUs .
To reduce padding tokens per batch , mini-batches are made of sentences having similar lengths ( Vaswani et al. , 2017 ) .
We trim longer sequences to a maximum of 512 tokens after BPE .
To decode , we average the last 10 checkpoints , and use a beam size of 4 with length penalty of 0.6 , similar to Vaswani et al . ( 2017 ) .
Since the vocabulary size hyperparameter is the focus of this analysis , we use a range of vocabulary sizes that include character vocabulary and BPE operations that yield vocabulary sizes between 500 and 64 K types .
A common practice , as seen in Vaswani et al . ( 2017 ) 's setup , is to jointly learn BPE for both source and target languages , which facilitates three - way weight sharing between the encoder 's input , the decoder 's input , and the output ( i.e. classifier 's class ) embeddings ( Press and Wolf , 2017 ) .
However , to facilitate fine- grained analysis of vocabulary sizes and their effect on class imbalance , our models separately learn source and target vocabularies ; weight sharing between the encoder 's combined in Figure 4 .
All the reported BLEU scores are obtained using SACREBLEU ( Post , 2018 ) . 6
We make the following observations : smaller vocabulary such as characters have not produced the best BLEU for any of our language pairs or dataset sizes .
A vocabulary of 32 K or larger is unlikely to produce optimal results unless the data set is large e.g. the 4.5 M DE$EN sets .
The BLEU curves as a function of vocabulary sizes have a shape resembling a hill .
The position of the peak of the hill seems to shift towards a larger vocabulary when the datasets are large .
However , there is a lot of variance in the position of the peak : one extreme is at 500 types on 0.5 M EN !
HI , and the other extreme is at 64 K types in 4.5 M DE!EN .
Although Figures 3 and 4 indicate where the optimal vocabulary size is for these chosen language pairs and datasets , the question of why the peak is where it is remains unanswered .
We visualize ? , D , and F 95 % in Figure 5 to answer that question , and report these observations :
1 . Small vocabularies have a relatively larger F 95 % ( favorable to classifier ) , yet they are suboptimal .
We reason that this is due to the presence of a larger ? , which is unfavorable to the autoregressor .
2 . Larger vocabularies such as 32 K and beyond have a smaller ?
which favors the autoregressor , yet rarely achieved the best BLEU .
We reason this is due to the presence of a lower F 95 % and a higher D being unfavorable to the classifier .
Since the larger datasets have many training examples for each class , as indicated by a generally larger F 95 % , we conclude that bigger vocabularies tend to yield optimal results compared to smaller datasets in the same language .
3 . On small ( 30K ) to medium ( 1.3 M ) data sizes , the vocabulary size of 8 K seems to find a good trade - off between ? and D , as well as between ? and F 95 % .
There is a simple heuristic to locate the peak : the near-optimal vocabulary size is where sentence length ? is small , while F 95 % is approximately 100 or higher .
BLEU scores are often lower at larger vocabulary sizes - where ? is ( favorably ) low but D is ( unfavorably ) high ( Figure 5 ) .
This calls for a further investigation that is reported in the following section .
Measuring Classifier Bias Due to Imbalance
In a typical classification setting with imbalanced classes , the classifier learns an undesired bias based on frequencies .
A balanced class distribution debiases in this regard , leading to improvement in the precision of frequent classes as well as recall of infrequent classes .
However , BLEU focuses only on the precision of classes ; except for adding a global brevity penalty , it is ignorant of the poor recall of infrequent classes .
Therefore , the BLEU scores shown in Figures 3a , 3 b and 4 capture only a part of the improvements and biases .
In this section we perform a detailed analysis of the impact of class balancing by considering both precision and recall of classes .
We accomplish this in two stages :
First , we define a method to measure the bias of the model for classes based on their frequencies .
Second , we track the bias in relation to vocabulary size and class imbalance , and report DE!EN , as it has many data points .
Frequency Based Bias
We measure frequency bias using the Pearson correlation coefficient , ? , between class rank and class performance , where for performance measures we use precision and recall .
Classes are ranked based on descending order of frequencies in the training data encoded with the same encoding schemes used for reported NMT experiments .
With this setup , the class with rank 1 , say F 1 , is the one with the highest frequency , rank 2 is the next highest , and so on .
More generally , F k is an index in the class rank list which has an inverse relation to class frequencies .
We define precision as P k for class k similar to the unigram precision in BLEU and extend its definition to the unigram recall as R k ( See Appendix A for detail ) .
The Pearson correlation coefficients between class rank and precision ( ? F , P ) , and class rank and recall ( ?
F , R ) are reported in Figure 6 .
In datasets where D is high , the performance of classifier correlates with class rank .
Such correlations are undesired for a classifier .
Analysis of Class Frequency Bias
An ideal classifier is one that does not discriminate classes based on their frequencies , i.e. one that exhibits no correlation between ?
F , P , and ?
F , R .
However , we see in Figure 6 Figure 6 shows a trend that frequency based bias measured by correlation coefficient is lower in settings that have lower D.
However , since D is nonzero , there still exists non-zero correlation between propose several variations that became essential components of many future models .
RNN modules , either LSTM ( Hochreiter and Schmidhuber , 1997 ) or GRU ( Cho et al. , 2014a )
There are only a few models that perform nonautoregressive NMT ( Libovick ?
and Helcl , 2018 ; Gu et al. , 2018 ) .
These are focused on improving the speed of inference ; generation quality is currently sub-par compared to autoregressive models .
These non-autoregressive models can also be viewed as token classifiers with a different kind of feature extractor , whose strengths and limitations are yet to be theoretically understood .
BPE Subwords Sennrich et al. ( 2016 ) introduce BPE as a simplified way to solve out - of- vocabulary ( OOV ) words without having to use a back - off dictionary for OOV words .
They note that BPE improves the translation of not only the OOV words , but also some rare invocabulary words .
The analysis by Morishita et al . ( 2018 ) is different than ours in that they view various vocabulary sizes as hierarchical features that are used in addition to a fixed vocabulary .
Salesky et al. ( 2018 ) offer an efficient way to search BPE vocabulary size for NMT .
Kudo ( 2018 ) use BPE as a regularization technique by introducing sampling based randomness to the BPE segmentation .
To the best of our knowledge , no previous work exists that analyzes BPE 's effect on class imbalance .
Class Imbalance
The class imbalance problem has been extensively studied in classical ML ( Japkowicz and Stephen , 2002 ) .
In the medical domain Mazurowski et al. ( 2008 ) find that classifier performance deteriorates with even modest imbalance in the training data .
Untreated class imbalance has been known to deteriorate the performance of image segmentation .
Sudre et al. ( 2017 ) investigate the sensitivity of various loss functions .
Johnson and Khoshgoftaar ( 2019 ) survey imbalance learning and report that the effort is mostly targeted to computer vision tasks .
Buda et al. ( 2018 ) provide a definition and quantification method for two types of class imbalance : step imbalance and linear imbalance .
Since the imbalance in Zipfian distribution of classes is neither single -stepped nor linear , we use a divergence based measure to quantify the imbalance .
Conclusion Envisioning NMT as a token classifier with an autoregressor helps in analysing its weaknesses .
Our analysis provides an explanation of why text generation using BPE vocabulary is more effective compared to word and character vocabularies , and why certain BPE hyperparameters are better than others .
We show that the number of BPE merges is not an arbitrary hyperparameter , and that it can be tuned to address the class imbalance and sequence length problems .
Figure 1 : 1 Figure 1 : The NMT model re-envisioned as a token classifier with an autoregressive feature extractor .
