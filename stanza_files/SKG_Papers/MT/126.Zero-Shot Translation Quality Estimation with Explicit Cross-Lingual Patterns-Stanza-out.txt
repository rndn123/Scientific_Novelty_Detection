title
Zero - Shot Translation Quality Estimation with Explicit Cross-Lingual Patterns
abstract
This paper describes our submission of the WMT 2020 Shared Task on Sentence Level Direct Assessment , Quality Estimation ( QE ) .
In this study , we empirically reveal the mismatching issue when directly adopting BERTScore ( Zhang et al. , 2020 ) to QE .
Specifically , there exist lots of mismatching errors between source sentence and translated candidate sentence with token pairwise similarity .
In response to this issue , we propose to expose explicit cross lingual patterns , e.g. word alignments and generation score , to our proposed zero-shot models .
Experiments show that our proposed QE model with explicit cross-lingual patterns could alleviate the mismatching issue , thereby improving the performance .
Encouragingly , our zero-shot QE method could achieve comparable performance with supervised QE method , and even outperforms the supervised counterpart on 2 out of 6 directions .
We expect our work could shed light on the zero-shot QE model improvement .
Introduction Translation quality estimation ( QE ) ( Blatz et al. , 2004 ; Specia et al. , 2018 Specia et al. , , 2020 aims to predict the quality of translation hypothesis without golden-standard human references , setting it apart from reference - based translation metrics .
Existing reference - based evaluation metrics , e.g. BLEU ( Papineni et al. , 2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , NIST ( Doddington , 2002 ) , ROUGE ( Lin , 2004 ) , TER ( Snover et al. , 2006 ) , are commonly used in language generation tasks including translation , summarization , and captioning but all heavily rely on the quality of given references .
Recently , ( Edunov et al. , 2020 ) show that reference - based automatic evaluation metrics , e.g. , BLEU , are not always reliable because the human translated references are translationese ( Koppel and Example of mismatching error , Russian ?
English .
On the left , token " ? " is mismatched to " The " with the maximal probability ( within the red rectangle ) only .
On the right , guided by our proposed cross-lingual patterns , " ? " is correctly matched to the token " named " with the maximal probability ( within the green rectangle . )
Ordan , 2011 ; Graham et al. , 2019 ) .
Thus , an automatic method with no access to any references , i.e. , QE , is highly appreciated .
? ##? ? ? ##? ##? ? ? ##? ##? ##? ? ? ##? ##? ? ##? ##?
In this paper , we mainly focus on sentence level QE metrics , where existing studies categorize it into two classes : 1 ) supervised QE with human assessment as supervision signal : a feature extractor stacked with an estimator ( Yankovskaya et al. , 2019 ; Wang et al. , 2016 b ; Fan et al. , 2019 ) ; 2 ) unsupervised QE without human assessment , which normally based on the pre-trained word embeddings , for example , YISI ( Lo , 2019 ) and BERTScore ( Zhang et al. , 2020 ) .
Our work follows the latter , where we adopt BERTScore ( Zhang et al. , 2020 ) without extra fine-tuning .
In particular , we implement our approach upon the pre-trained multilingual BERT ( Devlin et al. , 2019 ) and XLM ( Conneau and Lample , 2019 ) .
We first empirically reveal the mismatching issue when directly adopting BERTScore ( Zhang et al. , 2020 ) to QE task .
Specifically , there exist lots of mismatching errors between source tokens and translated candidate tokens when performing greedy matching with pairwise similarity .
Figure 1 shows an example of the mismatching error , where the Russian token " ? " is mismatched to the English token " The " due to lacking of proper guidance .
To alleviate this issue , we design two explicit cross-lingual patterns to augment the BERTScore as a QE metric : ? CROSS -LINGUAL ALIGNMENT MASKING : we design an alignment masking strategy to provide the pairwise similarity matrix with extra guidance .
The alignment is derived from GIZA ++ ( Och and Ney , 2003 )
Methods
BERTScore as Backbone
A pre-trained multilingual model generates contextual embeddings of both source sentence and translated candidate sentence , such that this pair of sentences in different language can be mapped to the same continuous feature space .
Given a source sentence x = x 1 , . . . , x k , the model generates a sequence of vectors x 1 , . . . , x k while the candidate ? = ?1 , . . . , ?l is mapped to ?1 , . . . , ?l . Different from the reference - based BERTScore , where they compute the pairwise similarity between reference sentence and translated candidate sentence , we calculate the pairwise similarity between the source and translated candidate with dotproduction , i.e. , x i ?j .
We adopt greedy matching to force each source token to be matched to the most similar target token in the translated candidate sentence .
The QE function based on BERTScore backbone therefore can be formulated as : R BERT = 1 | x| x i ?x max ?j ? x i ?j , P BERT = 1 |?| ?j ? max x i ?x x i ?j , F BERT = 2 P BERT ? R BERT P BERT + R BERT . ( 1 ) where R BERT , P BERT and F BERT are inherited from Zhang et al . ( 2020 ) , representing Recall rate , Precision rate and F-score , respectively .
Alignment Masking Strategy
With aforementioned QE function , we can follow Zhang et al . ( 2020 ) to obtain the distance between the source sentence and translated candidate sentence via directly adding up the maximum similarity score of each token pair .
However , because there exist lots of mismatching errors ( as shown in Figure 1 ) , above sentence - level similarity calculation may be sub-optimal .
Moreover , Zhang et al. ( 2020 ) 's calculation is suitable for monolingual scenario , which may be insensitive for cross-lingual computation .
Thus , we propose to augment our QE metric with more cross-lingual signals .
Inspired by Ding et al . ( 2020 ) , where they show it 's possible to augment cross-lingual modeling by leveraging cross-lingual explicit knowledge .
we therefore employ word alignment knowledge from external models , e.g. , GIZA ++ 1 , as additional information .
Alignment masking Both BERT ( Devlin et al. , 2019 ) and XLM ( Conneau and Lample , 2019 ) utilize BPE tokenization .
It should be noted that in this paper , by word alignment we mean alignment of BPE tokenized word and subword units .
Given a tokenized source sentence x and candidate sentence ? , alignment ( Och and Ney , 2003 ) is defined as a subset of the Cartesian product of position , A ? {( i , j ) : i = 1 , . . . , k ; j = 1 , . . . , l}.
Alignment results represented by M is defined as : M = 1 ( i , j ) ? A 0 ? a ? 1 otherwise ( 2 ) M is a penalty function over the similarity of unaligned tokens .
It 's a mask like matrix to assign a penalty weight a 2 to the similarity of unaligned tokens while keeping that of aligned ones unchanged , as illustrated in Figure 2 .
Thus , greedy matching is performed on a renewed similarity matrix , which is defined as the average of x i ?j and masked x i ?j by word alignment .
For example , R BERT # Metrics en-de en-zh ro-en et- en ne-en si-en ru-en ( x i ?j + M?x i ?j ) ( 3 ) which can be characterized as balancing our proposed extra explicit cross-lingual patterns , i.e. , word alignment .
Generation Score
In additional to token similarity score , we introduce force-decoding perplexity of each target token as a cross-lingual generation score .
For better coordination and considering our cross-lingual setting , we use the same pre-trained cross-lingual model , e.g. multilingual BERT , for both token embedding extraction and masked language model ( MLM ) perplexity generation .
This cross-lingual generation score is added as : F BERT ( ppl ) = ( 1 ? ? ) * F BERT + ? * ppl MLM ( 4 ) where the ?
can be seen as a variable that regulates the interpolation ratio between F BERT and our proposed ppl MLM , making the generation score after combination more wisely .
The effect of ? will be discussed in the experiments .
Experimental Results
Data Main experiments were conducted on the WMT20 QE Shared Task , Sentence-level Direct Assessment language pairs .
The task contains 7 directions , including : ? English ?
German ( en-de ) ? English ?
Chinese ( en-zh ) ? Romanian ?
English ( ro-en ) ? Estonian ?
English ( et-en ) ? Nepalese ?
English ( ne-en ) ? Sinhala?English ( si-en ) ? Russian ?
English ( ru-en )
Each of them consists of 7 K training data , 1 K validation data and 1 K test data .
Setup Based on our proposed QE metric in Section 2.1 , we conduct the validataion and main experiments with two pre-trained cross-lingual model : bertbase - multilingual -cased 3 ( 12 - layer , 768- hidden , 12 - heads , trained on 104 languages ) and xlmmlm - 100- 1280 4 ( 16 - layer , 1280 - hidden , 16 - heads , on 100 languages ) for both contextual embedding representation and generation score .
The 9th layer of multilingual BERT and the 11th of XLM are used to generate contextual embedding representations .
Furthermore , we obtain bidirectional word alignment of all the training , validation and test dataset with GIZA ++.
Notably , this work is a zero-shot approach that does n't involve training on Direct Assessment ( DA ) scores , which makes our method suitable for real industry scenarios .
Ablation Study
In order to maximize the advantages of our proposed method for zero-shot translation QE , we conducted extensive ablation studies .
We report the results of ablation studies on the validation dataset .
Effect of ?
We conduct ablation studies to empirically decide the value of of ? in Equation 4 when introducing generation scores .
We observe positive effect of proper weighted additional generation score on en-zh , ro-en , et-en , ne-en , si-en .
As illustrated in Figure 3 , considering the average performance , we pick ? = 0.01 from [ 0 , 0.03 ] .
Effect of different pretrained models
We also investigated the effect to deploy our proposed fixed cross-lingual patterns on different state - of - the - art large scale pre-trained models , e.g. , XLM ( Conneau and Lample , 2019 ) ( xlm-mlm -100 -1280 ) , BERT ( Zhang et al. , 2020 ) ( bert-base-multilingualcased ) .
Table 3 lists a comparison of multilingual BERT and XLM in terms of the Pearson correlations with Direct Assessment ( DA ) scores .
As seen , multilingual BERT outperforms XLM on almost all language pairs , excepting for si-en .
One possible reason is that multilingual BERT is not pre-trained on Sinhala corpus while XLM does .
In this end , we generate our final submission with XLM in sien direction , and with multilingual BERT in other directions .
Main Results
In the main experiments , we evaluate the agreement of our approach with Direct Assessment ( DA ) scores on validation dataset , as DA scores of the test set are not available at this point .
Baseline results , which are evaluated on test set though , are also listed for general comparison .
As shown in Table 1 , our method could achieve improvements on 4 out of 6 directions , including en-zh , ro-en , et-en and ne-en .
Particularly , combination of two strategies , i.e. , CROSS -LINGUAL ALIGNMENT and CROSS -LINGUAL GENERATION SCORE , could achieve better performance on en-zh , ro-en and et-en directions .
Besides Pearson correlations , we also calculated Kendall correlations for all language pairs .
As seen in are same as Pearson correlations , validating the effectiveness of our proposed methods .
Official Evaluations
The official automatic evaluation results of our submissions for WMT 2020 are presented in Table 4 . We participated QE ( Sentence - Level Direct Assessment ) in following language pairs : en-de , en-zh , ro-en , ne-en , si-en , ru-en , except for et-en .
From the official evaluation results ( Specia et al. , 2020 ) in terms of absolute Pearson Correlation , our submission achieves higher performance than supervised baseline ( Kepler et al. , 2019 ) in ne-en and si-en ( As shown in Table 4 ) .
Encouragingly , our proposed zero-shot QE metric could achieve comparable performance with supervised QE method , and even outperforms the supervised counterpart on 2 out of 6 directions .
Related Work MT evaluation
Taking sentence - level evaluation as an example , reference - based metrics describe to which extend a candidate sentence is similar to a reference one ( Sellam et al. , 2020 ) . BLEU ( Papineni et al. , 2002 ) , METEOR ( Banerjee and Lavie , 2005 ) , NIST ( Doddington , 2002 ) , ROUGE ( Lin , 2004 ) measure such similarity through n-gram matching , which is restricted to the exact form of sentences .
TER ( Snover et al. , 2006 ) and CHAR - ACTER ( Wang et al. , 2016 b ) use edit distance at word or character level to indicate the distance between candidate and reference .
Different from these metrics that are restricted to the exact form of sentences , recent dominated neural model metrics learn to evaluate with human assessment as supervision signal , such as BEER ( Stanojevi ? and Sima'an , 2014 ) and RUSE ( Shimanaka et al. , 2018 ) , or oth-ers as YiSi ( Lo , 2019 ) and BERTScore ( Zhang et al. , 2020 ) , evaluate with pre-trained word embedding , without using human assessment .
Incorporating Explicit Knowledge Several approaches have incorporated pre-defined or learned features into neural networks .
Tai et al. ( 2015 ) demonstrate that incorporating structured semantic information could enhance the representations .
feed the encoder cell combined embeddings of linguistic features including lemmas , subword tags , etc .
Ding et al. ( 2017 ) leverage the domain knowledge to perform data selection to improve the machine translation models .
Ding and Tao ( 2019 ) incorporate the structure patterns of sentences , i.e. , syntax , into the Transformer network to enhance seq2seq modeling performance .
Raganato et al. ( 2020 ) utilize the pre-defined fixed patterns to replace the attention weights and show promising results .
Inspired by above works , we propose to augment zero-shot QE model with crosslingual patterns .
Conclusion and Future Work
In this work , we revealed a mismatching issue in zero-shot QE modeling .
To alleviate it , we introduced two explicit cross-lingual patterns based on BERTScore backbone .
Extensive experiments indicated that our proposed patterns , without finetuning , the QE model can be improved marginally .
Notably , our zero-shot QE method outperforms supervised QE model on 2 out of 6 directions , shedding light on zero-shot QE researches .
In the future , we plan to explore more strategies for incorporating various auxiliary information and better in - domain fine-tuning ( Gururangan et al. , 2020 ) or introduce an non-autoregressive refiner ( Wu et al. , 2020 ) to address our revealed mismatching issue .
Also , it will be interesting to apply QE metrics on document - level machine translations with considering the dropped pronoun ( Wang et al. , 2016a ( Wang et al. , , 2018 . Weiyue Wang , Jan-Thorsten Peter , Hendrik Rosendahl , and Hermann Ney .
2016 b . Character : Translation edit rate on character level .
In WMT .
Di Wu , Liang Ding , Fan Lu , and J. Xie. 2020 .
Slotrefine : A fast non-autoregressive model for joint intent detection and slot filling .
In EMNLP .
E Yankovskaya , A T?ttar , M Fishel Volume 3 Shared Task Papers , Day , and 2019 .
2019 .
Quality estimation and translation metrics via pre-trained word and sentence embeddings .
In WMT .
Tianyi Zhang , Varsha Kishore , Felix Wu , Kilian Q Weinberger , and Yoav Artzi . 2020 .
Bertscore : Evaluating text generation with bert .
In ICLR .
Figure 1 : Example of mismatching error , Russian ?
English .
On the left , token " ? " is mismatched to " The " with the maximal probability ( within the red rectangle ) only .
On the right , guided by our proposed cross-lingual patterns , " ? " is correctly matched to the token " named " with the maximal probability ( within the green rectangle . )
