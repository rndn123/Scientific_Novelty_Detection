title
Learning to Transform and Select Elementary Trees for Improved Syntax - based Machine Translations
abstract
We propose a novel technique of learning how to transform the source parse trees to improve the translation qualities of syntax - based translation models using synchronous context-free grammars .
We transform the source tree phrasal structure into a set of simpler structures , expose such decisions to the decoding process , and find the least expensive transformation operation to better model word reordering .
In particular , we integrate synchronous binarizations , verb regrouping , removal of redundant parse nodes , and incorporate a few important features such as translation boundaries .
We learn the structural preferences from the data in a generative framework .
The syntax - based translation system integrating the proposed techniques outperforms the best Arabic- English unconstrained system in NIST -08 evaluations by 1.3 absolute BLEU , which is statistically significant .
Introduction
Most syntax - based machine translation models with synchronous context free grammar ( SCFG ) have been relying on the off- the-shelf monolingual parse structures to learn the translation equivalences for string - to- tree , tree - to-string or tree - to - tree grammars .
However , stateof - the- art monolingual parsers are not necessarily well suited for machine translation in terms of both labels and chunks / brackets .
For instance , in Arabic-to - English translation , we find only 45.5 % of Arabic NP - SBJ structures are mapped to the English NP - SBJ with machine alignment and parse trees , and only 60.1 % of NP - SBJs are mapped with human alignment and parse trees as in ?
2 . The chunking is of more concern ; at best only 57.4 % source chunking decisions are translated contiguously on the target side .
To translate the rest of the chunks one has to frequently break the original structures .
The main issue lies in the strong assumption behind SCFG - style nonterminals - each nonterminal ( or variable ) assumes a source chunk should be rewritten into a contiguous chunk in the target .
Without integrating techniques to modify the parse structures , the SCFGs are not to be effective even for translating NP - SBJ in linguistically distant language - pairs such as Arabic- English .
Such problems have been noted in previous literature .
Zollmann and Venugopal ( 2006 ) and Marcu et al . ( 2006 ) used broken syntactic fragments to augment their grammars to increase the rule coverage ; while we learn optimal tree fragments transformed from the original ones via a generative framework , they enumerate the fragments available from the original trees without learning process .
introduced parse forests to blur the chunking decisions to a certain degree , to expand search space and reduce parsing errors from 1 - best trees ; others tried to use the parse trees as soft constraints on top of unlabeled grammar such as Hiero ( Marton and Resnik , 2008 ; Chiang , 2010 ; Shen et al. , 2010 ) without sufficiently leveraging rich tree context .
Recent works tried more complex approaches to integrate both parsing and decoding in one single search space as in ( Liu and Liu , 2010 ) , at the cost of huge search space .
In ( Zhang et al. , 2009 ) , combinations of tree forest and tree-sequence ( Zhang et al. , 2008 ) based approaches were carried out by adding pseudo nodes and hyper edges into the forest .
Overall , the forest- based translation can reduce the risks from upstream parsing errors and expand the search space , but it cannot sufficiently address the syntactic divergences between various language - pairs .
The tree sequence approach adds pseudo nodes and hyper edges to the forest , which makes the forest even denser and harder for navigation and search .
As trees thrive in the search space , especially with the pseudo nodes and edges being added to the already dense forest , it is becoming harder to wade through the deep forest for the best derivation path out .
We propose to simplify suitable subtrees to a reasonable level , at which the correct reordering can be easily identified .
The transformed structure should be frequent enough to have rich statistics for learning a model .
Instead of creating pseudo nodes and edges and make the forest dense , we transform a tree with a few simple operators ; only meaningful frontier nodes , context nodes and edges are kept to induce the correct reordering ; such operations also enable the model to share the statistics among all similar subtrees .
On the basis of our study on investigating the language divergence between Arabic - English with human aligned and parsed data , we integrate several simple statistical operations , to transform parse trees adaptively to serve the translation purpose better .
For each source span in the given sentence , a subgraph , corresponding to an elementary tree ( in Eqn. 1 ) , is proposed for PSCFG translation ; we apply a few operators to transform the subgraph into some frequent subgraphs seen in the whole training data , and thus introduce alternative similar translational equivalences to explain the same source span with enriched statistics and features .
For instance , if we regroup two adjacent nodes IV and NP - SBJ in the tree , we can obtain the correct reordering pattern for verb-subject order , which is not easily available otherwise .
By finding a set of similar elementary trees derived from the original elementary trees , statistics can be shared for robust learning .
We also investigate the features using the context beyond the phrasal subtree .
This is to further disambiguate the transformed subgraphs so that informative neighboring nodes and edges can influence the reordering preferences for each of the transformed trees .
For instance , at the beginning and end of a sentence , we do not expect dramatic long distance reordering to happen ; or under SBAR context , the clause may prefer monotonic reordering for verb and subject .
Such boundary features were treated as hard constraints in previous literature in terms of re-labeling ( Huang and Knight , 2006 ) or re-structuring ( Wang et al. , 2010 ) .
The boundary cases were not addressed in the previous literature for trees , and here we include them in our feature sets for learning a MaxEnt model to predict the transformations .
We integrate the neighboring context of the subgraph in our transformation preference predictions , and this improve translation qualities further .
The rest of the paper is organized as follows : in section 2 , we analyze the projectable structures using human aligned and parsed data , to identify the problems for SCFG in general ; in section 3 , our proposed approach is explained in detail , including the statistical operators using a MaxEnt model ; in section 4 , we illustrate the integration of the proposed approach in our decoder ; in section 5 , we present experimental results ; in section 6 , we conclude with discussions and future work .
The Projectable Structures
A context- free style nonterminal in PSCFG rules means the source span governed by the nonterminal should be translated into a contiguous target chunk .
A " projectable " phrase -structure means that it is translated into a contiguous span on the target side , and thus can be generalized into a nonterminal in our PSCFG rule .
We carried out a controlled study on the projectable structures using human annotated parse trees and word alignment for 5 k Arabic- English sentence - pairs .
In are real translation boundaries that can be explained by a nonterminal in PSCFG rule .
Even for human parse and alignment , the unlabeled F-measures are still as low as 57.39 % .
Such statistics show that we should not blindly learn tree- to-string grammar ; additional transformations to manipulate the bracketing boundaries and labels accordingly have to be implemented to guarantee the reliability of source - tree based syntax translation grammars .
The transformations could be as simple as merging two adjacent nonterminals into one bracket to accommodate non-contiguity on the target side , or lexicalizing those words which have fork-style , many - to - many alignment , or unaligned content words to enable the rest of the span to be generalized into nonterminals .
We illustrate several cases using the tree in Figure 1 .
In Figure 1 , several non-projectable nodes were illus-trated : the deleted nonterminals PRON ( + hA ) , the manyto-many alignment for IV ( ttAlf ) PREP ( mn ) , fork-style alignment for NOUN ( Azmp ) .
Intuitively , it would be good to glue the nodes NOUN ( Al$ rq ) ADJ ( AlAwsT ) under the node of NP , because it is more frequent for moving ADJ before NOUN in our training data .
It should be easier to model the swapping of ( NOUN ADJ ) using the tree ( NP NOUN , ADJ ) instead of the original bigger tree of ( NP - SBJ Azmp , NOUN , ADJ ) with one lexicalized node .
Approaches in tree-sequence based grammar ( Zhang et al. , 2009 ) tried to address the bracketing problem by using arbitrary pseudo nodes to weave a new " tree " back into the forest for further grammar extractions .
Such approach may improve grammar coverage , but the pseudo node labels would be arguably a worse choice to split the already sparse data .
Some of the interior nodes connecting the frontier nodes might be very informative for modeling reordering .
Also , due to the introduced pseudo nodes , it would need exponentially many nonterminals to keep track of the matching tree-structures for translations .
The created pseudo node could easily block the informative neighbor nodes associated with the subgraph which could change the reordering nature .
For instance , IV and NP - SBJ tends to swap at the beginning of a sentence , but it may prefer monotone if they share a common parent of SBAR for a subclause .
In this case , it is unnecessary to create a pseudo node " IV + SBJ " to block useful factors .
We propose to navigate through the forest , via simplifying trees by grouping the nodes , cutting the branches , and attaching connected neighboring informative nodes to further disambiguate the derivation path .
We apply explicit translation motivated operators , on a given monolingual elementary tree , to transform it into similar but simpler trees , and expose such statistical preferences to the decoding process to select the best rewriting rule from the enriched grammar rule sets , for generating target strings .
Elementary Trees to String Grammar
We propose to use variations of an elementary tree , which is a connected subgraph fitted in the original monolingual parse tree .
The subgraph is connected so that the frontiers ( two or more ) are connected by their immediate common parent .
Let ? be a source elementary tree : ? =< ; v f , v i , E > , ( 1 ) where v f is a set of frontier nodes which contain nonterminals or words ; v i are the interior nodes with source labels / symbols ; E is the set of edges connecting the nodes v = v f +v i into a connected subgraph fitted in the source parse tree ; is the immediate common parent of the frontier nodes v f .
Our proposed grammar rule is formulated as follows : < ? ; ? ; ? ; m ; t > , ( 2 ) where ? is the target string , containing the terminals and / or nonterminals in a target language ; ? is the oneto-one alignment of the nonterminals between ? and ? ; t contains possible sequence of transform operations ( to be explained later in this section ) associated with each rule ; m is a function of enumerating the neighborhood of the source elementary tree ? , and certain tree context ( nodes and edges ) can be used to further disambiguate the reordering or the given lexical choices .
The interior nodes of ?.v i , however , are not necessarily informative for the reordering decisions , like the unary nodes WHNP , VP , and PP - CLR in Figure 1 ; while the frontier nodes ?.v f are the ones directly executing the reordering decisions .
We can selectively cut off the interior nodes , which have no or only weak causal relations to the reordering decisions .
This will enable the frequency or derived probabilities for executing the reordering to be more focused .
We call such transformation operators t.
We specified a few operators for transforming an elementary tree ? , including flattening tree operators such as removing interior nodes in v i , or grouping the children via binarizations .
Let 's use the trigram " Alty ttAlf mn " in Figure 1 as an example , the immediate common parent for the span is SBAR : ?. = SBAR ; the interior nodes are ?.v i = { WHNP VP S PP - CLR } ; the frontier nodes are ?.v f = ( x : PRON x:IV x :PREP ) .
The edges ?.E ( as highlighted in Figure 1 ) connect ?.v i and ?.v f into a subgraph for the given source ngram .
For any source span , we look up one elementary tree ?
covering the span , then we select an operator t ?
T , to explore a set of similar elementary trees t( ? , m ) = {?
} as simplified alternatives for translating that source tree ( span ) ? into an optimal target string ? * accordingly .
Our generative model is summarized in Eqn. 3 : ? * = arg max t?T ;? ? t( ? , m ) p a ( ? |? ) ?
p b ( ? | t , ? , m ) ? p c ( t| ? , m ) . ( 3 ) In our generative scheme , for a given elementary tree ? , we sample an operator ( or a combination of operations ) t with the probability of p c ( t| ? ) ; with operation t , we transform ?
into a set of simplified versions ? ? t( ? , m ) with the probability of p b ( ? | t , ? ) ; finally we select the transformed version ? to generate the target string ? with a probability of p a ( ? |? ) .
Note here , ? and ? share the same immediate common parent , but not necessarily the frontier , or interior , or even neighbors .
The frontier nodes can be merged , lexicalized , or even deleted in the tree-to-string rule associated with ? , as long as the alignment for the nonterminals are book -kept in the derivations .
To simplify the model , one can choose the operator t to be only one level , and the model using a single operator t is to be deterministic .
Thus , the final set of models to learn are p a ( ? |? ) for rule alignment , and the preference model p b ( ? | t , ? , m ) , and the operator proposal model p c ( t| ? , m ) , which in our case is a maximum entropy model - the key model in our proposed approach in this paper for transforming the original elementary tree into similar trees for evaluating the reordering probabilities .
Eqn. 3 significantly enriches reordering powers for syntax - based machine translation .
This is because it uses all similar set of elementary trees to generate the best target strings .
In the next section , we 'll first define the operators conceptually , and then explain how we learn each of the models .
Model p a ( ? |? )
A log linear model is applied here to approximate p a ( ? |? ) ? exp ( ?f f ) via weighted combination ( ? ) of feature functions f f ( ? , ? ) , including relative frequencies in both directions , and IBM Model - 1 scores in both directions as ? and ? have lexical items within them .
We also employed a few binary features listed in the following table . ? is observed less than 2 times ( ? , ? ) deletes a src content word ( ? , ? ) deletes a src function word ( ? , ? ) over generates a tgt content word ( ? , ? ) over generates a tgt function word 3.2 Model p b ( ? | t , ? , m ) p b ( ? | t , ? , m ) is our preference model .
For instance using the operator t of cutting an unary interior node in ?.v i , if ?.v i has more than one unary interior node , like the SBAR tree in Figure 1 , having three unary interior node : WHNP , VP and PP - CLR , p b ( ? | t , ? , m ) specifies which one should have more probabilities to be cut .
In our case , to make model simple , we simply choose histogram / frequency for modeling the choices .
Model p c ( t| ? , m ) p c ( t| ? , m ) is our operator proposal model .
It ranks the operators which are valid to be applied for the given source tree ?
together with its neighborhood m .
Here , in our approach , we applied a Maximum Entropy model , which is also employed to train our Arabic parser : p c ( t| ? , m ) ? exp ? ? f f ( t , ? , m ) .
The feature sets we use here are almost the same set we used to train our Arabic parser ; the only difference is the future space here is operator categories , and we check bag-of-nodes for interior nodes and frontier nodes .
The key feature categories we used are listed as in the
t : Tree Transformation Function Obvious systematic linguistic divergences between language - pairs could be handled by some simple operators such as using binarization to re-group contiguously aligned children .
Here , we start from the human aligned and parsed data as used in section 2 to explore potential useful operators .
Binarizations
One of the simplest way for transforming a tree is via binarization .
Monolingual binarization chooses to re-group children into smaller subtree with a suitable label for the newly created root .
We choose a function mapping to select the top-frequent label as the root for the grouped children ; if such label is not found we simply use the label of the immediate common parent for ?.
In decoding time , we need to select trees from all possible binarizations , while in the training time , we restrict the choices allowed with the alignment constraint , that every grouped children should be aligned contiguously on the target side .
Our goal is to simulate the synchronous binarization as much as we can .
In this paper , we applied the four basic operators for binarizing a tree : left-most , right -most and additionally head - out left and head - out right for more than three children .
Two examples are given in Table 4 , in which we used LDC style representation for the trees .
With the proper binarization , the structure becomes rich in sub-structures which allow certain reordering to happen more likely than others .
For instance for the subtree ( VP PV NP - SBJ ) , one would apply stronger statistics from training data to support the swap of NP - SBJ and PV for translation .
Regrouping verbs
Verbs are keys for reordering especially for Araic- English with VSO translated into SVO .
However , if the verb and its relevant arguments for reordering are at different levels in the tree , the reordering is difficult to model as more interior nodes combinations will distract the distributions and make the model less focused .
We provide the following two operations specific for verb in VP trees as in Table 5 .
Removing interior nodes and edges
For reordering patterns , keeping the deep tree structure might not be the best choice .
Sometimes it is not even Binarization Operations Examples right-most ( NP X noun X adj 1 X adj 2 ) ? ( NP X noun ( ADJP X adj 1 X adj 2 ) ) left-most ( VP X pv X NP - SBJ X SBAR ) ? ( VP ( VP X pv X NP - SBJ ) X SBAR ) ( V P 1 X v ( V P 2 Y ) ) ? ( V P 1 ( V P 2 X v Y ) ) regroup verb and remove the top level VP ( R ( V P 1 X v ( R 2 Y ) ) ) ? ( R ( R 2 X v Y ) )
Table 5 : Operators for manipulating the trees possible due to the many - to -many alignment , insertions and deletions of terminals .
So , we introduce the operators to remove the interior nodes ?.v i selectively ; this way , we can flatten the tree , remove irrelevant nodes and edges , and can use more frequent observations of simplified structures to capture the reordering patterns .
We use two operators as shown in Table 6 .
The second operator deletes all the interior nodes , labels and edges ; thus reordering will become a Hiero-alike ( Chiang , 2007 ) unlabeled rule , and additionally a special glue rule : X 1 X 2 ? X 1 X 2 .
This operator is necessary , we need a scheme to automatically back off to the meaningful glue or Hiero-alike rules , which may lead to a cheaper derivation path for constructing a partial hypothesis , at the decoding time .
As shown in Table 1 , NP brackets has only 35.56 % of time to be translated contiguously as an NP in machine aligned & parsed data .
The NP tree in Figure 2 happens to be an " inside-out " style alignment , and context free grammar such as ITG ( Wu , 1997 ) can not explain this structure well without necessary lexicalization .
Actually , the Arabic tokens of " dfE Aly AlAnf j Ar " form a combination and is turned into English word " ignite " in an idiomatic way .
With lexicalization , a Hiero style rule " dfE X Aly AlAnfj Ar ? to ignite X " is potentially a better alternative for translating the NP tree .
Our operators allow us to back off to such Hiero-style rules to construct derivations , which share the immediate common parent NP , as defined for the elementary tree , for the given source span .
m : Neighboring Function
For a given elementary tree , we use function m to check the context beyond the subgraph .
This includes looking the nodes and edges connected to the subgraph .
Similar to the features used in ( Dyer et al. , 2009 ) , we check the following three cases .
Sentence boundaries
When the tree ?
frontier sets contain the left-most token , right - most token , or both sides , we will add to the neighboring nodes the corresponding decoration tags L ( left ) , R ( right ) , and B ( both ) , respectively .
These decorations are important especially when the reordering patterns for the same trees are depending on the context .
For instance , at the beginning or end of a sentence , we do not expect dramatic reordering - moving a token too far away in the middle of the sentences .
SBAR / IP / PP / FRAG boundaries
We check siblings of the root for ?
for a few special labels , including SBAR , IP , PP , and FRAG .
These labels indicate a partial sentence or clause , and the reordering patterns may get different distributions due to the position relative to these nodes .
For instance , the PV and SBJ nodes under SBAR tends to have more monotone preference for word reordering ( Carpuat et al. , 2010 ) .
We mark the boundaries with position markers such as L-PP , to indicate having a left sibling PP , R-IP for having a right sibling IP , and C-SBAR to indicate the elementary tree is a child of SBAR .
These labels are selected mainly based on our linguistic intuitions and errors in our translation system .
A data-driven approach might be more promising for identifying useful markups w.r.t specific reordering patterns .
Translation boundaries
In the Figure 2 , there are two special nodes under NP : NP * and PP * .
These two nodes are aligned in a " insideout " fashion , and none of them can be generalized into a nonterminal to be rewritten in a PSCFG rule .
In other words , the phrasal brackets induced from NP * and PP * operators for removing nodes / edges Examples remove unary nodes Xiong et al . ( 2010 ) , defined translation boundaries on phrase - decoder style derivation trees due to the nature of their shift- reduce algorithm , which is a special case in our model .
( R X t1 ( R 1 ( R2 X t2 ) ) ) ? ( R X t1 ( R2 X t2 ) ) ) remove all labels ( R ( R 1 X t1 ( R 2 X t2 ) ) ) ? ( R X t2 X t1 )
Decoding Decoding using the proposed elementary tree to string grammar naturally resembles bottom up chart parsing algorithms .
The key difference is at the grammar querying step .
Given a grammar G , and the input source parse tree ?
from a monolingual parser , we first construct the elementary tree for a source span , and then retrieve all the relevant subgraphs seen in the given grammar through the proposed operators .
This step is called populating , using the proposed operators to find all relevant elementary trees ?
which may have contributed to explain the source span , and put them in the corresponding cells in the chart .
There would have been exponential number of relevant elementary trees to search if we do not have any restrictions in the populating step ; we restrict the maximum number of interior nodes |?.v
i | to be 3 , and the size of frontier nodes |?.v f | to be less than 6 ; additional pruning for less frequent elementary trees is carried out .
After populating the elementary trees , we construct the partial hypotheses bottom up , by rewriting the frontier nodes of each elementary tree with the probabilities ( costs ) for ? ? ? * as in Eqn. 3 . Our decoder ( Zhao and Al - Onaizan , 2008 ) is a template - based chart decoder in C ++ .
It generalizes over the dotted - product operator in Earley style parser , to allow us to leverage many operators t ?
T as above-mentioned , such as binarizations , at different levels for constructing partial hypothesis .
Experiments
In our experiments , we built our system using most of the parallel training data available to us : 250M
Arabic running tokens , corresponding to the " unconstrained " condi-tion in NIST - MT08 .
We chose the testsets of newswire and weblog genres from MT08 and DEV10 1 .
In particular , we choose MT08 to enable the comparison of our results to the reported results in NIST evaluations .
Our training and test data is summarized in Table 5 .
For testings , we have 129,908 tokens in our testsets .
For language models ( LM ) , we used 6 - gram LM trained with 10.3 billion English tokens , and also a shrinkage - based LM ( Chen , 2009 ) -" ModelM " ( Chen and Chu , 2010 ; Emami et al. , 2010 ) with 150 word-clusters learnt from 2.1 million tokens .
From the parallel data , we extract phrase pairs ( blocks ) and elementary trees to string grammar in various configurations : basic tree - to-string rules ( Tr2str ) , elementary tree-to-string rules with boundaries t( elm2str + m ) , and with both t and m ( elm2str + t + m ) .
This is to evaluate the operators ' effects at different levels for decoding .
To learn our MaxEnt models defined in ? 3.3 , we collect the events during extracting elm2str grammar in training time , and learn the model using improved iterative scaling .
We use the same training data as that used in training our Arabic parser .
There are 16 thousand human parse trees with human alignment ; additional 1 thousand human parse and aligned sent-pairs are used as unseen test set to verify our MaxEnt models and parsers .
For our Arabic parser , we have a labeled F-measure of 78.4 % , and POS tag accuracy 94.9 % .
In particular , we 'll evaluate model p c ( t| ? , m ) in Eqn. 3 for predicting the translation boundaries in ? 3.5.3 for projectable spans as detailed in ? 5.1 .
Our decoder ( Zhao and Al - Onaizan , 2008 ) supports grammars including monotone , ITG , Hiero , tree-tostring , string - to - tree , and several mixtures of them ( Lee et al. , 2010 ) .
We used 19 feature functions , mainly from those used in phrase - based decoder like Moses ( Koehn et al. , 2007 ) , including two language models ( one for a 6 - gram LM , one for ModelM , one brevity penalty , IBM Model - 1 ( Brown et al. , 1993 ) style alignment probabilities in both directions , relative frequency in both directions , word / rule counts , content / function word mismatch , together with features on tr2str rule probabilities .
We use BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) to evaluate translation qualities .
Our baseline used basic elementary tree to string grammar without any manipulations and boundary markers in the model , We expose the statistical decisions in Eqn. 3 as the rule probability as one of the 19 dimensions , and use Simplex Downhill algorithm with Armijo line search ( Zhao and Chen , 2009 ) to optimize the weight vector for decoding .
The algorithm moves all dimensions at the same time , and empirically achieved more stable results than MER ( Och , 2003 ) in many of our experiments .
Predicting Projectable Structures
The projectable structure is important for our proposed elementary tree to string grammar ( elm2str ) .
When a span is predicted not to be a translation boundary , we want the decoder to prefer alternative derivations outside of the immediate elementary tree , or more aggressive manipulation of the trees , such as deleting interior nodes , to explore unlabeled grammar such as Hiero style rules , with proper costs .
We test separately on predicting the projectable structures , like predicting function tags in ? 3.5.3 , for each node in syntactic parse tree .
We use one thousand test sentences with two conditions : human parses and machine parses .
There are totally 40,674 nodes excluding the sentence - level node .
The results are shown in Table 8 .
It showed our Max - Ent model is very accurate using human trees : 94.5 % of accuracy , and about 84.7 % of accuracy for using the machine parsed trees .
Our accuracies are higher compared with the 71 + % accuracies reported in ( Xiong et al. , 2010 ) for their phrasal decoder .
Setups Accuracy Human Parses 94.5 % Machine Parses 84.7 %
We zoom in the translation boundaries for MT08 - NW , in which we studied a few important frequent labels including VP and NP - SBJ as in Table 9 .
According to our MaxEnt model , 20 % of times we should discourage a VP tree to be translated contiguously ; such VP trees have an average span length of 16.9 tokens in MT08 - NW .
Similar statistics are 15.9 % for S-tree with an average span of 13.8 tokens .
Labels
Integrating t and m
We carried out a series of experiments to explore the impacts using t and m for elm2str grammar .
We start from transforming the trees via simple operator t( ? ) , and then expand the function with more tree context to include the neighboring functions : t( ? , m ) . over the baseline from + 0.18 ( via right - most binarization ) to + 0.52 ( via head- out- right ) BLEU points .
When we combine all binarizations ( abz ) , we did not see additive gains over the best individual case - hrbz .
Because during our decoding time , we do not frequently see large number of children ( maximum at 6 ) , and for smaller trees ( with three or four children ) , these operators will largely generate same transformed trees , and that explains the differences from these individual binarization are small .
For other languages , these binarization choices might give larger differences .
Additionally , regrouping the verbs is marginally helpful for BLEU and TER .
Upon close examinations , we found it is usually beneficial to group verb ( PV or IV ) with its neighboring nodes for expressing phrases like " have to do " and " will not only " .
Deleting the interior nodes helps on shrinking the trees , so that we can translate it with more statistics and confidences .
It helps more on TER than BLEU for MT08 -NW .
Setups
Experiments in Table 11 extends
Table 10 with neighboring function to further disambiguate the reordering rule using the tree context .
Besides the translation boundary , the reordering decisions should be different with regard to the positions of the elementary tree relative to the sentence .
At the sentence - beginning one might expect more for monotone decoding , while in the middle of the sentence , one might expect more reorderings .
Table 11 shows when we add such boundary markups in our rules , an improvement of 0.33 BLEU points were obtained ( 56.46 v.s. 56.13 ) on top of the already improved setups .
A close check up showed that the sentence - begin / end markups significantly reduced the leading " and " ( from Arabic word w# ) in the decoding output .
Also , the verb subject order under SBAR seems to be more like monotone with a leading pronoun , rather than the general strong reordering of moving verb after subject .
Overall , our results showed that such boundary conditions are helpful for executing the correct reorderings .
We conclude the investigation with full function t( ? , m ) , which leads to a BLEUr4n4 of 56.87 ( cased BLEUr4n4 c 55.16 ) , a significant improvement of 1.77 BLEU point over a already strong baseline .
We apply the setups for several other NW and WEB datasets to further verify the improvement .
Shown in Table 12 , we apply separately the operators for t and m first , then combine them as the final results .
Varied improvements were observed for different genres .
On DEV10 - NW , we observed 1.29 BLEU points improvement , and about 0.63 and 0.98 improved BLEU points for MT08 - WB and DEV10 - WB , respectively .
The improvements for newwire are statistically significant .
The improvements for weblog are , however , only marginally better .
One possible reason is the parser quality for web genre is reliable , as our training data is all in newswire .
Regarding to the individual operators proposed in this paper , we observed consistent improvements of applying them across all the datasets .
The generative model in Eqn. 3 leverages the operators further by selecting the best transformed tree form for executing the reorderings .
A Translation Example
To illustrate the advantages of the proposed grammar , we use a testing case with long distance word reordering and the source side parse trees .
We compare the translation from a strong phrasal decoder ( DTM2 ) ( Ittycheriah and Roukos , 2007 ) , which is one of the top systems in NIST -08 evaluation for Arabic- English .
The translations from both decoders with the same training data ( LM + TM ) are in Table 13 .
The highlighted parts in Figure 3 show that , the rules on partial trees are effectively selected and applied for capturing long-distance word reordering , which is otherwise rather difficult to get correct in a phrasal system even with a MaxEnt reordering model .
Discussions and Conclusions
We proposed a framework to learn models to predict how to transform an elementary tree into its simplified forms for better executing the word reorderings .
Two types of operators were explored , including ( a ) transforming the trees via binarizations , grouping or deleting interior nodes to change the structures ; and ( b ) neighboring boundary context to further disambiguate the reordering decisions .
Significant improvements were observed on top of a strong baseline system , and consistent improvements were observed across genres ; we achieved a cased BLEU of 55.16 for MT08 - NW , which is significantly better than the officially reported results in NIST MT08 Arabic- English evaluations .
Phrasal Decoder prince abdul rahman bin abdul aziz , deputy minister of defense former saudi said in a press statement that he was optimistic about the kingdom 's ability to find a solution to the problem . Elm2Str + t( ? , m ) former saudi deputy defense minister prince abdul rahman bin abdul aziz said in a press statement that he was optimistic of the kingdom 's ability to find a solution to the problem .
Table 13 : A translation example , comparing with phrasal decoder .
Figure 3 : A testing case : illustrating the derivations from chart decoder .
The left panel is source parse tree for the Arabic sentence - the input to our decoder ; the right panel is the English translation together with the simplified derivation tree and alignment from our decoder output .
Each " X " is a nonterminal in the grammar rule ; a " Block " means a phrase pair is applied to rewrite a nonterminal ; " Glue " and " Hiero " means the unlabeled rules were chosen to explain the span as explained in ? 3.4.3 ; " Tree " means a labeled rule is applied for the span .
For instance , for the source span [ 1,10 ] , a rule is applied on a partial tree with PV and NP - SBJ ; for the span [ 18,23 ] , a rule is backed off to an unlabeled rule ( Hiero-alike ) ; for the span [ 21 , 22 ] , it is another partial tree of NPs .
Within the proposed framework , we also presented several special cases including the translation boundaries for nonterminals in SCFG for translation .
We achieved a high accuracy of 84.7 % for predicting such boundaries using MaxEnt model on machine parse trees .
Future works aim at transforming such non-projectable trees into projectable form ( Eisner , 2003 ) , driven by translation rules from aligned data ( Burkett et al. , 2010 ) , and informative features form both the source 3 and the target sides ( Shen et al. , 2008 ) to enable the system to leverage more Figure 1 : 1 Figure1 : Non-projectable structures in an SBAR tree with human parses and alignment ; there are non-projectable structures : the deleted nonterminals PRON ( + hA ) , the many - tomany alignment for IV ( ttAlf ) PREP ( mn ) , fork-style alignment for NOUN ( Azmp ) .
Figure 2 : 2 Figure2 : A NP tree with an " inside-out " alignment .
The nodes " NP * " and " PP * " are not suitable for generalizing into NTs used in PSCFG rules .
bn EbdAlEzyz nA}b wzyr AldfAE AlsEwdy AlsAbq fy tSryH SHAfy An +h mtfA}l b# qdrp Almmlkp Ely AyjAd Hl l# Alm$klp .
Table 1 1 , the unlabeled F-measures with machine alignment and parse trees show that , for only 48.71 % of the time , the boundaries introduced by the source parses
Table 2 : 2 Additional 5 Binary Features for pa (? |? )
Table 3 . 3
The headtable used in our training is manually built for Arabic .
bag-of-nodes ?.v i bag-of-nodes and ngram of ?.v f chunk - level features : left-child , right - child , etc. lexical features : unigram and bigram pos features : unigram and bigram contextual features : surrounding words
Table 3 : 3 Feature Features for learning pc ( t| ? , m )
Table 4 : 4 Operators for binarizing the trees Operators for regroup verbs Examples regroup verb
Table 6 : 6 Operators for simplifying the trees are not translation boundaries , and to avoid translation errors we should identify them by applying a PSCFG rule on top of them .
During training , we label nodes with translation boundaries , as one additional function tag ; during decoding , we employ the MaxEnt model to predict the translation boundary label probability for each span associated with a subgraph ? , and discourage deriva - tions accordingly for using nonterminals over the non- translation boundary span .
The translation boundaries over elementary trees have much richer representation power .
The previous works as in
Table 7 : 7 Training and test data ; using all training parallel training data for 4 test sets and we achieved a BLEUr4n4 55.01 for MT08 - NW , or a cased BLEU of 53.31 , which is close to the best officially reported result 53.85 for unconstrained systems .
2
Table 8 : 8 Accuracies of predicting projectable structures
Table 9 : 9
The predicted projectable structures in MT08 - NW Using the predicted projectable structures for elm2str grammar , together with the probability defined in Eqn. 3 as additional cost , the translation results in Table11 show it helps BLEU by 0.29 BLEU points ( 56.13 v.s. 55.84 ) .
The boundary decisions penalize the derivation paths using nonterminals for non-projectable spans for partial hypothesis construction .
total NonProj Percent Avg.len VP * 4479 920 20.5 % 16.9 NP * 14164 825 5.8 % 8.12 S* 3123 495 15.9 % 13.8 NP - SBJ * 1284 53 4.12 % 11.9 Setups TER BLEUr4n4 Baseline 39.87 55.01 right-binz ( rbz ) 39.10 55.19 left-binz ( lbz ) 39.67 55.31 Head-out-left ( hlbz ) 39.56 55.50 Head-out-right ( hrbz ) 39.52 55.53 + all binzation ( abz ) 39.42 55.60 + regroup-verb 39.29 55.72 + deleting interior nodes ?.v i 38.98 55.84
Table 10 : 10 TER and BLEU for MT08 - NW , using only t( ? )
Table 11 : 11 TER and BLEU for MT08 - NW , using t( ? , m ) .
TER BLEUr4n4 Baseline w/ t 38.98 55.84 + TM Boundaries 38.89 56.13 + SENT
Bound all t( ? , m) 38.63 38.61 56.46 56.87
Table 10 focus on testing operators especially binarizations for transforming the trees .
In Table 10 , the four possible binarization methods all improve Data MT08-NW MT08-WB Dev10-NW Dev10-WB Tr2Str elm2str + t 55.01 55.84 39.19 39.43 37.33 38.02 41.77 42.70 elm2str + m elm2str + t( ? , m) 55.57 56.87 39.60 39.82 37.67 38.62 42.54 42.75
Table 12 : BLEU scores on various test sets ; comparing elementary tree-to-string grammar ( tr2str ) , transformation of the trees ( elm2str + t ) , using the neighboring function for boundaries ( elm2str + m ) , and combination of all together ( elm2str + t( ? , m ) ) .
MT08 - NW and MT08 - WB have four references ; Dev10 - WB has three references , and Dev10 - NW has one reference .
BLEUn4 were reported .
DEV10 are unseen testsets used in our GALE project .
It was selected from recently released LDC data LDC2010E43.v3 .
See link : http://www.itl.nist.gov/iad/mig/tests/mt/2008/doc/mt08 official results v0.html
The BLEU score on MT08 - NW has been improved to 57.55 since the acceptance of this paper , using the proposed technique but with our GALE P5 data pipeline and setups .
isomorphic trees , and avoid potential detour errors .
We are exploring the incremental decoding framework , like ( Huang and Mi , 2010 ) , to improve pruning and speed .
