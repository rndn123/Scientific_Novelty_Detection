title
Neural Machine Translation with Source-Side Latent Graph Parsing
abstract
This paper presents a novel neural machine translation model which jointly learns translation and source-side latent graph representations of sentences .
Unlike existing pipelined approaches using syntactic parsers , our end-to - end model learns a latent graph parser as part of the encoder of an attention - based neural machine translation model , and thus the parser is optimized according to the translation objective .
In experiments , we first show that our model compares favorably with state - of - the - art sequential and pipelined syntax - based NMT models .
We also show that the performance of our model can be further improved by pretraining it with a small amount of treebank annotations .
Our final ensemble model significantly outperforms the previous best models on the standard Englishto - Japanese translation dataset .
Introduction Neural Machine Translation ( NMT ) is an active area of research due to its outstanding empirical results ( Bahdanau et al. , 2015 ; Sutskever et al. , 2014 ) .
Most of the existing NMT models treat each sentence as a sequence of tokens , but recent studies suggest that syntactic information can help improve translation accuracy ( Eriguchi et al. , 2016 b ( Eriguchi et al. , , 2017
Stahlberg et al. , 2016 ) .
The existing syntax - based NMT models employ a syntactic parser trained by supervised learning in advance , and hence the parser is not adapted to the translation tasks .
An alternative approach for leveraging syntactic structure in a language processing task is to jointly learn syntactic trees of the sentences
All the calculated electronic band structures are metallic . along with the target task ( Socher et al. , 2011 ; Yogatama et al. , 2017 ) .
Motivated by the promising results of recent joint learning approaches , we present a novel NMT model that can learn a task - specific latent graph structure for each source -side sentence .
The graph structure is similar to the dependency structure of the sentence , but it can have cycles and is learned specifically for the translation task .
Unlike the aforementioned approach of learning single syntactic trees , our latent graphs are composed of " soft " connections , i.e. , the edges have realvalued weights ( Figure 1 ) .
Our model consists of two parts : one is a task - independent parsing component , which we call a latent graph parser , and the other is an attention - based NMT model .
The latent parser can be independently pre-trained with human-annotated treebanks and is then adapted to the translation task .
In experiments , we demonstrate that our model can be effectively pre-trained by the treebank annotations , outperforming a state - of - the - art sequential counterpart and a pipelined syntax - based model .
Our final ensemble model outperforms the previous best results by a large margin on the WAT English - to - Japanese dataset .
Latent Graph Parser
We model the latent graph parser based on dependency parsing .
In dependency parsing , a sentence is represented as a tree structure where each node corresponds to a word in the sentence and a unique root node ( ROOT ) is added .
Given a sentence of length N , the parent node H w i ?
{w 1 , . . . , w N , ROOT } ( H w i = w i ) of each word w i ( 1 ? i ? N ) is called its head .
The sentence is thus represented as a set of tuples ( w i , H w i , w i ) , where w i is a dependency label .
In this paper , we remove the constraint of using the tree structure and represent a sentence as a set of tuples ( w i , p( H w i |w i ) , p( w i |w i ) ) , where p( H w i |w i ) is the probability distribution of w i 's parent nodes , and p( w i |w i ) is the probability distribution of the dependency labels .
For example , p( H w i = w j |w i ) is the probability that w j is the parent node of w i .
Here , we assume that a special token EOS is appended to the end of the sentence , and we treat the EOS token as ROOT .
This approach is similar to that of graph - based dependency parsing ( McDonald et al. , 2005 ) in that a sentence is represented with a set of weighted arcs between the words .
To obtain the latent graph representation of the sentence , we use a dependency parsing model based on multi-task learning proposed by Hashimoto et al . ( 2017 ) .
Word Representation
The i-th input word w i is represented with the concatenation of its d 1 - dimensional word embedding v dp ( w i ) ?
R d 1 and its character n-gram embed - ding c( w i ) ?
R d 1 : x( w i ) = [ v dp ( w i ) ; c( w i ) ] .
c( w i ) is computed as the average of the embeddings of the character n-grams in w i .
POS Tagging Layer
Our latent graph parser builds upon multilayer bi-directional Recurrent Neural Networks ( RNNs ) with Long Short - Term Memory ( LSTM ) units ( Graves and Schmidhuber , 2005 ) .
In the first layer , POS tagging is handled by computing a hidden state h ( 1 ) i = [ ? ? h ( 1 ) i ; ? ? h ( 1 ) i ] ?
R 2d 1 for w i , where ? ? h ( 1 ) i = LSTM ( ? ? h ( 1 ) i?1 , x( w i ) ) ?
R d 1 and ? ? h ( 1 ) i = LSTM ( ? ? h ( 1 ) i + 1 , x( w i ) ) ?
R d 1 are hidden states of the forward and backward LSTMs , respectively .
h ( 1 ) i is then fed into a softmax classifier to predict a probability distribution p ( 1 ) i ? R C ( 1 ) for word - level tags , where C ( 1 ) is the number of POS classes .
The model parameters of this layer can be learned not only by human-annotated data , but also by backpropagation from higher layers , which are described in the next section .
Dependency Parsing Layer Dependency parsing is performed in the second layer .
A hidden state h ( 2 ) i ?
R 2d 1 is computed by ? ? h ( 2 ) i = LSTM ( ? ? h ( 2 ) i?1 , [ x ( w i ) ; y( w i ) ; ? ? h ( 1 ) i ] ) and ? ? h ( 2 ) i = LSTM ( ? ? h ( 2 ) i + 1 , [ x ( w i ) ; y( w i ) ; ? ? h ( 1 ) i ] ) , where y(w i ) = W ( 1 ) p ( 1 ) i ?
R d 2 is the POS information output from the first layer , and W ( 1 ) ? R d 2 ?C ( 1 ) is a weight matrix .
Then , ( soft ) edges of our latent graph representation are obtained by computing the probabilities p( H w i = w j | w i ) = exp ( m ( i , j ) ) k =i exp ( m ( i , k ) ) , ( 1 ) where m( i , k ) = h ( 2 ) T k W dp h ( 2 ) i ( 1 ? k ?
N + 1 , k = i ) is a scoring function with a weight matrix W dp ? R 2d 1 ?2d 1 . While the models of Hashimoto et al . ( 2017 ) , , and Dozat and Manning ( 2017 ) learn the model parameters of their parsing models only by humanannotated data , we allow the model parameters to be learned by the translation task .
Next , [ h i ; z( H w i ) ] is fed into a softmax classifier to predict the probability distribution p( w i |w i ) , where z( H w i ) ?
R 2d 1 is the weighted average of the hidden states of the parent nodes : j =i p( H w i = w j | w i ) h ( 2 ) j .
This results in the latent graph representation ( w i , p( H w i |w i ) , p( w i |w i ) ) of the input sentence .
NMT with Latent Graph Parser
The latent graph representation described in Section 2 can be used for any sentence - level tasks , and here we apply it to an Attention - based NMT ( ANMT ) model .
We modify the encoder and the decoder in the ANMT model to learn the latent graph representation .
Encoder with Dependency Composition
The ANMT model first encodes the information about the input sentence and then generates a sentence in another language .
The encoder represents the word w i with a word embedding v enc ( w i ) ? R d 3 .
It should be noted that v enc ( w i ) is different from v dp ( w i ) because each component is separately modeled .
The encoder then takes the word embedding v enc ( w i ) and the hidden state h ( 2 ) i as the input to a uni-directional LSMT : h ( enc ) i = LSTM ( h ( enc ) i?1 , [ v enc ( w i ) ; h ( 2 ) i ] ) , ( 2 ) where h ( enc ) i ?
R d 3 is the hidden state corresponding to w i .
That is , the encoder of our model is a three - layer LSTM network , where the first two layers are bi-directional .
In the sequential LSTMs , relationships between words in distant positions are not explicitly considered .
In our model , we explicitly incorporate such relationships into the encoder by defining a dependency composition function : dep( w i ) = tanh ( W dep [ h enc i ; h( H w i ) ; p( w i |w i ) ] ) , ( 3 ) where h( H w i ) = j =i p( H w i = w j |w i ) h ( enc ) j is the weighted average of the hidden states of the parent nodes .
Note on character n-gram embeddings
In NMT models , sub-word units are widely used to address rare or unknown word problems .
In our model , the character n-gram embeddings are fed through the latent graph parsing component .
To the best of our knowledge , the character n-gram embeddings have never been used in NMT models .
Wieting et al. ( 2016 ) , Bojanowski et al. ( 2017 ) , and Hashimoto et al . ( 2017 ) have reported that the character n-gram embeddings are useful in improving several NLP tasks by better handling unknown words .
Decoder with Attention Mechanism
The decoder of our model is a single - layer LSTM network , and the initial state is set with h ( enc ) N + 1 and its corresponding memory cell .
Given the t-th hidden state h ( dec ) t ?
R d 3 , the decoder predicts the t-th word in the target language using an attention mechanism .
The attention mechanism in computes the weighted average of the hidden states h ( enc ) i of the encoder : s( i , t ) = exp ( h ( dec ) t ?h ( enc ) i ) N +1 j=1 exp ( h ( dec ) t ?h ( enc ) j ) , ( 4 ) a t = N + 1 i=1 s( i , t ) h ( enc ) i , ( 5 ) where s( i , t ) is a scoring function which specifies how much each source -side hidden state contributes to the word prediction .
In addition , like the attention mechanism over constituency tree nodes ( Eriguchi et al. , 2016 b ) , our model uses attention to the dependency composition vectors : s ( i , t ) = exp ( h ( dec ) t ?dep ( w i ) ) N j=1 exp ( h ( dec ) t ?dep ( w j ) ) , ( 6 ) a t = N i=1 s ( i , t ) dep ( w i ) , ( 7 )
To predict the target word , a hidden state h( dec ) t ?
R d 3 is then computed as follows : h( dec ) t = tanh ( W [ h ( dec ) t ; a t ; a t ] ) , ( 8 ) where W ? R d 3 ?3d 3 is a weight matrix . h( dec ) t is fed into a softmax classifier to predict a target word distribution . h( dec ) t is also used in the transition of the decoder LSTMs along with a word embedding v dec ( w t ) ?
R d 3 of the target word w t : h ( dec ) t+1 = LSTM ( h ( dec ) t , [ v dec ( w t ) ; h( dec ) t ] ) , ( 9 ) where the use of h( dec ) t is called input feeding proposed by .
The overall model parameters , including those of the latent graph parser , are jointly learned by minimizing the negative log-likelihood of the prediction probabilities of the target words in the training data .
To speed up the training , we use BlackOut sampling ( Ji et al. , 2016 ) .
By this joint learning using Equation ( 3 ) and ( 7 ) , the latent graph representations are automatically learned according to the target task .
Implementation Tips Inspired by Zoph et al. ( 2016 ) , we further speed up BlackOut sampling by sharing noise samples across words in the same sentences .
This technique has proven to be effective in RNN language modeling , and we have found that it is also effective in the NMT model .
We have also found it effective to share the model parameters of the target word embeddings and the softmax weight matrix for word prediction ( Inan et al. , 2016 ; Press and Wolf , 2017 ) .
Also , we have found that a parameter averaging technique ( Hashimoto et al. , 2013 ) is helpful in improving translation accuracy .
Translation
At test time , we use a novel beam search algorithm which combines statistics of sentence lengths ( Eriguchi et al. , 2016 b ) and length normalization ( Cho et al. , 2014 ) .
During the beam search step , we use the following scoring function for a generated word sequence y = ( y 1 , y 2 , . . . , y Ly ) given a source word sequence x = ( x 1 , x 2 , . . . , x Lx ) : 1 L y ? ?
Ly i=1 log p(y i |x , y 1:i?1 ) + log p( L y | L x ) ? ? , ( 10 ) where p( L y | L x ) is the probability that sentences of length L y are generated given source -side sentences of length L x .
The statistics are taken by using the training data in advance .
In our experiments , we have empirically found that this beam search algorithm helps the NMT models to avoid generating translation sentences that are too short .
Experimental Settings
Data
We used an English-to - Japanese translation task of the Asian Scientific Paper Excerpt Corpus ( AS - PEC ) ( Nakazawa et al. , 2016 b ) used in the Workshop on Asian Translation ( WAT ) , since it has been shown that syntactic information is useful in English - to - Japanese translation ( Eriguchi et al. , 2016 b ; Neubig et al. , 2015 ) .
We followed the data preprocessing instruction for the English- to - Japanese task in Eriguchi et al . ( 2016 b ) .
The English sentences were tokenized by the tokenizer in the Enju parser ( Miyao and Tsujii , 2008 ) , and the Japanese sentences were segmented by the KyTea tool 1 . Among the first 1,500,000 translation pairs in the training data , we selected 1,346,946 pairs where the maximum sentence length is 50 .
In what follows , we call this dataset the large training dataset .
We further selected the first 20,000 and 100,000 pairs to construct the small and medium training datasets , respectively .
The development data include 1,790 pairs , and the test data 1,812 pairs .
For the small and medium datasets , we built the vocabulary with words whose minimum frequency is two , and for the large dataset , we used words whose minimum frequency is three for English and five for Japanese .
As a result , the vocabulary of the target language was 8,593 for the small dataset , 23,532 for the medium dataset , and 65,680 for the large dataset .
A special token UNK was used to replace words which were not included in the vocabularies .
The character ngrams ( n = 2 , 3 , 4 ) were also constructed from each training dataset with the same frequency settings .
Parameter Optimization and Translation
We turned hyper-parameters of the model using development data .
We set ( d 1 , d 2 ) = ( 100 , 50 ) for the latent graph parser .
The word and character n-gram embeddings of the latent graph parser 1 http://www.phontron.com/kytea/.
were initialized with the pre-trained embeddings in Hashimoto et al . ( 2017 ) .
2
The weight matrices in the latent graph parser were initialized with uniform random values in [?
? 6 ? row+col , + ? 6 ? row+col ] , where row and col are the number of rows and columns of the matrices , respectively .
All the bias vectors and the weight matrices in the softmax layers were initialized with zeros , and the bias vectors of the forget gates in the LSTMs were initialized by ones ( Jozefowicz et al. , 2015 ) .
We et al. , 2016 ) , the number of the negative samples was set to 2,000 for the small and medium training datasets , and 2,500 for the large training dataset .
The mini-batch size was set to 128 , and the momentum rate was set to 0.75 for the small and medium training datasets and 0.70 for the large training dataset .
A gradient clipping technique was used with a clipping value of 1.0 .
The initial learning rate was set to 1.0 , and the learning rate was halved when translation accuracy decreased .
We used the BLEU scores obtained by greedy translation as the translation accuracy and checked it at every half epoch of the model training .
We saved the model parameters at every half epoch and used the saved model parameters for the parameter averaging technique .
For regularization , we used L2 - norm regularization with a coefficient of 10 ?6 and applied dropout ( Hinton et al. , 2012 ) to Equation ( 8 ) with a dropout rate of 0.2 .
The beam size for the beam search algorithm was 12 for the small and medium training datasets , and 50 for the large training dataset .
We used BLEU ( Papineni et al. , 2002 ) , RIBES ( Isozaki et al. , 2010 ) , and perplexity scores as our evaluation metrics .
Note that lower perplexity scores indicate better accuracy .
Pre-Training of Latent Graph Parser
The latent graph parser in our model can be optionally pre-trained by using human annotations for dependency parsing .
In this paper we used the widely - used Wall Street Journal ( WSJ ) training data to jointly train the POS tagging and dependency parsing components .
We used the standard training split ( Section 0 - 18 ) for POS tagging .
We followed Chen and Manning ( 2014 ) to generate the training data ( Section 2 - 21 ) for dependency parsing .
From each training dataset , we selected the first K sentences to pre-train our model .
The training dataset for POS tagging includes 38,219 sentences , and that for dependency parsing includes 39,832 sentences .
The parser including the POS tagger was first trained for 10 epochs in advance according to the multi-task learning procedure of Hashimoto et al . ( 2017 ) , and then the overall NMT model was trained .
When pre-training the POS tagging and dependency parsing components , we did not apply dropout to the model and did not fine - tune the word and character n-gram embeddings to avoid strong overfitting .
Model Configurations LGP -NMT is our proposed model that learns the Latent Graph Parsing for NMT .
LGP - NMT + is constructed by pre-training the latent parser in LGP - NMT as described in Section 4.3 .
SEQ is constructed by removing the dependency composition in Equation ( 3 ) , forming a sequential NMT model with the multi-layer encoder .
DEP is constructed by using pre-trained dependency relations rather than learning them .
That is , p( H w i = w j |w i ) is fixed to 1.0 such that w j is the head of w i .
The dependency labels are also given by the parser which was trained by using all the training samples for parsing and tagging .
UNI is constructed by fixing p( H w i = w j |w i ) to 1 N for all the words in the same sentence .
That is , the uniform probability distributions are used for equally connecting all the words .
Results on Small and Medium Datasets
We first show our translation results using the small and medium training datasets .
We report averaged scores with standard deviations across five different runs of the model training .
Small Training Dataset
Table 1 shows the results of using the small training dataset .
LGP - NMT performs worse than SEQ and UNI , which shows that the small training dataset is not enough to learn useful latent graph structures from scratch .
However , LGP - NMT + ( K = 10,000 ) outperforms SEQ and UNI , and the standard deviations are the smallest .
Therefore , the results suggest that pre-training the parsing and tagging components can improve the translation accuracy of our proposed model .
We can also see that DEP performs the worst .
This is not surprising because previous studies , e.g. , Li et al . ( 2015 ) , have reported that using syntactic structures do not always outperform competitive sequential models in several NLP tasks .
Now that we have observed the effectiveness of pre-training our model , one question arises naturally : how many training samples for parsing and tagging are necessary for improving the translation accuracy ?
Table 2 shows the results of using different numbers of training samples for parsing and tagging .
The results of K= 0 and K= 10,000 correspond to those of LGP - NMT and LGP - NMT + in Table 1 , respectively .
We can see that using the small amount of the training samples performs better than using all the training samples .
3
One possible reason is that the domains of the translation dataset and the parsing ( tagging ) dataset are considerably different .
The parsing and tagging datasets come from WSJ , whereas the translation dataset comes from abstract text of scientific papers in a wide range of domains , such as biomedicine and computer science .
These results suggest that our model can be improved by a small amount of parsing and tagging datasets in different domains .
Considering the recent universal dependency project 4 which covers more than 50 languages , our model has the potential of being applied to a variety of language pairs .
Medium Training Dataset
Table 3 shows the results of using the medium training dataset .
In contrast with using the small training dataset , LGP - NMT is slightly better than SEQ .
LGP - NMT significantly outperforms UNI , which shows that our adaptive learning is more effective than using the uniform graph weights .
By pre-training our model , LGP - NMT + significantly outperforms SEQ in terms of the BLEU score .
Again , DEP performs the worst among all the models .
By using our beam search strategy , the Brevity Penalty ( BP ) values of our translation results are equal to or close to 1.0 , which is important when evaluating the translation results using the BLEU scores .
A BP value ranges from 0.0 to 1.0 , and larger values mean that the translated sentences have relevant lengths compared with the reference translations .
As a result , our BLEU evaluation results are affected only by the word n-gram precision scores .
BLEU scores are sensitive to the BP values , and thus our beam search strategy leads to more solid evaluation for NMT models .
Results on Large Dataset Table 4 shows the BLEU and RIBES scores on the development data achieved with the large training dataset .
Here we focus on our models and SEQ because UNI and DEP consistently perform worse than the other models as shown in Table 1 and 3 .
The averaging technique and attentionbased unknown word replacement ( Jean et al. , 2015 ; Hashimoto et al. , 2016 )
Again , we see that the translation scores of our model can be further improved by pre-training the model .
Table 5 shows our results on the test data , and the previous best results summarized in Nakazawa et al . ( 2016 a ) and the WAT website 5 are also shown .
Our proposed models , LGP - NMT and LGP - NMT + , outperform not only SEQ but also all of the previous best results .
Notice also that our implementation of the sequential model ( SEQ ) provides a very strong baseline , the performance of which is already comparable to the previous state of the art , even without using ensemble techniques .
The confidence interval ( p ? 0.05 ) of the RIBES score of LGP - NMT + estimated by bootstrap resampling ( Noreen , 1989 ) is ( 82.27 , 83.37 ) , and thus the RIBES score of LGP - NMT + is significantly better than that of SEQ , which shows that our latent parser can be effectively pre-trained with the human-annotated treebank .
The sequential NMT model in Cromieres et al. ( 2016 ) and the tree-to-sequence NMT model in Eriguchi et al . ( 2016 b ) rely on ensemble techniques while our results mentioned above are obtained using single models .
Moreover , our model is more compact 6 than the previous best NMT model in Cromieres et al . ( 2016 ) .
By applying the ensemble technique to LGP - NMT , LGP - NMT + ,
As a result , it was found that a path which crosses a sphere obliquely existed .
Reference : ? LGP-NMT : ? LGP -NMT + : ?
( As a result , it was found that a path which obliquely crosses a sphere existed .)
Google trans : ? SEQ : ? ( As a result , it was found that a path which crosses a sphere existed obliquely .)
The androgen controls negatively ImRNA . and SEQ , the BLEU and RIBES scores are further improved , and both of the scores are significantly better than the previous best scores .
Selectional Preference
In the translation example ( 1 ) in Figure 2 , we see that the adverb " obliquely " is interpreted differently across the systems .
As in the reference translation , " obliquely " is a modifier of the verb " crosses " .
Our models correctly capture the relationship between the two words , whereas Google Translation and SEQ treat " obliquely " as a modifier of the verb " existed " .
This error is not a surprise since the verb " existed " is located closer to " obliquely " than the verb " crosses " .
A possible reason for the correct interpretation by our models is that they can better capture long-distance dependencies and are less susceptible to surface word distances .
This is an indication of our models ' ability of capturing domain-specific selectional preference that cannot be captured by purely sequential 7 These English sentences were created by manual simplification of sentences in the development data .
8
The translations were obtained at https : //translate .
google.com in Feb. and Mar. 2017 .
models .
It should be noted that simply using standard treebank - based parsers does not necessarily address this error , because our pre-trained dependency parser interprets that " obliquely " is a modifier of the verb " existed " .
Analysis on Translation Examples Adverb or Adjective
The translation example ( 2 ) in Figure 2 shows another example where the adverb " negatively " is interpreted as an adverb or an adjective .
As in the reference translation , " negatively " is a modifier of the verb " controls " .
Only LGP - NMT + correctly captures the adverb-verb relationship , whereas " negatively " is interpreted as the adjective " negative " to modify the noun " ImRNA " in the translation results from Google Translation and LGP - NMT .
SEQ interprets " negatively " as both an adverb and an adjective , which leads to the repeated translations .
This error suggests that the state - of- the - art NMT models are strongly affected by the word order .
By contrast , the pre-training strategy effectively embeds the information about the POS tags and the dependency relations into our model .
Analysis on Learned Latent Graphs Without Pre-Training
We inspected the latent graphs learned by LGP - NMT .
Figure 1 shows an example of the learned latent graph obtained for a sentence taken from the development data of the translation task .
It has long-range dependencies and cycles as well as ordinary left-to- right dependencies .
We have observed that the punctuation mark " . " is often pointed to by other words with large weights .
This is primarily because the hidden state corresponding to the mark in each sentence has rich information about the sentence .
To measure the correlation between the latent graphs and human-defined dependencies , we parsed the sentences on the development data of the WSJ corpus and converted the graphs into dependency trees by Eisner 's algorithm ( Eisner , 1996 ) .
For evaluation , we followed Chen and Manning ( 2014 ) and measured Unlabeled Attachment Score ( UAS ) .
The UAS is 24.52 % , which shows that the implicitly - learned latent graphs are partially consistent with the human-defined syntactic structures .
Similar trends have been reported by Yogatama et al . ( 2017 ) in the case of binary constituency parsing .
We checked the most dominant gold dependency labels which were assigned for the dependencies detected by LGP - NMT .
The labels whose ratio is more than 3 % are All the calculated electronic band structures are metallic .
nn , amod , prep , pobj , dobj , nsubj , num , det , advmod , and poss .
We see that dependencies between words in distant positions , such as subject - verb-object relations , can be captured .
With Pre-Training
We also inspected the pretrained latent graphs .
Figure 3 -( a ) shows the dependency structure output by the pre-trained latent parser for the same sentence in Figure 1 .
This is an ordinary dependency tree , and the head selection is almost deterministic ; that is , for each word , the largest weight of the head selection is close to 1.0 .
By contrast , the weight values are more evenly distributed in the case of LGP - NMT as shown in Figure 1 .
After the overall NMT model training , the latent parser is adapted to the translation task , and Figure 3 -( b ) shows the adapted latent graph .
Again , we can see that the adapted weight values are also distributed and different from the original pre-trained weight values , which suggests that human-defined syntax is not always optimal for the target task .
The UAS of the pre-trained dependency trees is 92.52 % 9 , and that of the adapted latent graphs is 18.94 % .
Surprisingly , the resulting UAS ( 18.94 % ) is lower than the UAS of our model without pretraining ( 24.52 % ) .
However , in terms of the translation accuracy , our model with pre-training is better than that without pre-training .
These results suggest that human-annotated treebanks can provide useful prior knowledge to guide the overall model training by pre-training , but the resulting sentence structures adapted to the target task do not need to highly correlate with the treebanks .
9
The UAS is significantly lower than the reported score in Hashimoto et al . ( 2017 ) .
The reason is described in Section 4.3 .
Related Work
While initial studies on NMT treat each sentence as a sequence of words ( Bahdanau et al. , 2015 ; Sutskever et al. , 2014 ) , researchers have recently started investigating into the use of syntactic structures in NMT models ( Bastings et al. , 2017 ; Chen et al. , 2017 ; Eriguchi et al. , 2016a Eriguchi et al. , , b , 2017 Li et al. , 2017 ; Stahlberg et al. , 2016 ; Yang et al. , 2017 ) .
In particular , Eriguchi et al . ( 2016 b ) introduced a tree-to-sequence NMT model by building a tree-structured encoder on top of a standard sequential encoder , which motivated the use of the dependency composition vectors in our proposed model .
Prior to the advent of NMT , the syntactic structures had been successfully used in statistical machine translation systems ( Neubig and Duh , 2014 ; Yamada and Knight , 2001 ) .
These syntax - based approaches are pipelined ; a syntactic parser is first trained by supervised learning using a treebank such as the WSJ dataset , and then the parser is used to automatically extract syntactic information for machine translation .
They rely on the output from the parser , and therefore parsing errors are propagated through the whole systems .
By contrast , our model allows the parser to be adapted to the translation task , thereby providing a first step towards addressing ambiguous syntactic and semantic problems , such as domain-specific selectional preference and PP attachments , in a task - oriented fashion .
Our model learns latent graph structures in a source-side language .
Eriguchi et al. ( 2017 ) have proposed a model which learns to parse and translate by using automatically - parsed data .
Thus , it is also an interesting direction to learn latent structures in a target-side language .
As for the learning of latent syntactic structure , there are several studies on learning task - oriented syntactic structures .
Yogatama et al. ( 2017 ) used a reinforcement learning method on shift-reduce action sequences to learn task - oriented binary constituency trees .
They have shown that the learned trees do not necessarily highly correlate with the human-annotated treebanks , which is consistent with our experimental results .
Socher et al. ( 2011 ) used a recursive autoencoder model to greedily construct a binary constituency tree for each sentence .
The autoencoder objective works as a regularization term for sentiment classification tasks .
Prior to these deep learning approaches , Wu ( 1997 ) presented a method for bilingual parsing .
One of the characteristics of our model is directly using the soft connections of the graph edges with the real-valued weights , whereas all of the above-mentioned methods use one best structure for each sentence .
Our model is based on dependency structures , and it is a promising future direction to jointly learn dependency and constituency structures in a task - oriented fashion .
Finally , more related to our model , Kim et al . ( 2017 ) applied their structured attention networks to a Natural Language Inference ( NLI ) task for learning dependency - like structures .
They showed that pre-training their model by a parsing dataset did not improve accuracy on the NLI task .
By contrast , our experiments show that such a parsing dataset can be effectively used to improve translation accuracy by varying the size of the dataset and by avoiding strong overfitting .
Moreover , our translation examples show the concrete benefit of learning task - oriented latent graph structures .
Conclusion and Future Work
We have presented an end-to- end NMT model by jointly learning translation and source-side latent graph representations .
By pre-training our model using treebank annotations , our model significantly outperforms both a pipelined syntax - based model and a state - of - the - art sequential model .
On English- to - Japanese translation , our model outperforms the previous best models by a large margin .
In future work , we investigate the effectiveness of our approach in different types of target tasks .
Figure 1 : 1 Figure 1 : An example of the learned latent graphs .
Edges with a small weight are omitted .
