title
Machine Translation Evaluation using Recurrent Neural Networks
abstract
This paper presents our metric ( UoW - LSTM ) submitted in the WMT - 15 metrics task .
Many state - of- the- art Machine Translation ( MT ) evaluation metrics are complex , involve extensive external resources ( e.g. for paraphrasing ) and require tuning to achieve the best results .
We use a metric based on dense vector spaces and Long Short Term Memory ( LSTM ) networks , which are types of Recurrent Neural Networks ( RNNs ) .
For WMT - 15 our new metric is the best performing metric overall according to Spearman and Pearson ( Pre-TrueSkill ) and second best according to Pearson ( TrueSkill ) system level correlation .
Introduction
Deep learning approaches have turned out to be successful in many NLP applications such as paraphrasing ( Mikolov et al. , 2013 b ; Socher et al. , 2011 ) , sentiment analysis ( Socher et al. , 2013 b ) , parsing ( Socher et al. , 2013a ) and machine translation ( Mikolov et al. , 2013a ) .
While dense vector space representations such as those obtained through Deep Neural Networks ( DNNs ) or Recurrent Neural Networks ( RNNs ) are able to capture semantic similarity for words ( Mikolov et al. , 2013 b ) , segments ( Socher et al. , 2011 ) and documents ( Le and Mikolov , 2014 ) naturally , traditional measures can only achieve this using resources like WordNet and paraphrase databases .
This paper presents a novel , efficient and compact MT evaluation measure based on RNNs .
Our metric ( Gupta et al. , 2015 ) is simple in the sense that it does not require much machinery and resources apart from the dense word vectors .
This cannot be said of most of the state - of - the - art MT evaluation metrics , which tend to be complex and require extensive feature engineering .
Our metric is based on RNNs and particularly on Tree Long Short Term Memory ( Tree - LSTM ) networks ( Tai et al. , 2015 ) . LSTM is a sequence learning technique which uses a memory cell to preserve a state over a long period of time .
This enables distributed representations of sentences using distributed representations of words .
Tree-LSTM ( Tai et al. , 2015 ) is a recent approach , which is an extension of the simple LSTM framework ( Hochreiter and Schmidhuber , 1997 ; Zaremba and Sutskever , 2014 ) .
Related Work Many metrics have been proposed for MT evaluation .
Earlier popular metrics are based on ngram counts ( e.g. BLEU ( Papineni et al. , 2002 ) and NIST ( Doddington , 2002 ) ) or word error rate .
Other popular metrics like METEOR ( Denkowski and Lavie , 2014 ) and TERp ( Snover et al. , 2008 ) also use external resources like WordNet and paraphrase databases .
However , system-level correlation with human judgements for these metrics remains below 0.90 Pearson correlation coefficient ( as per WMT - 14 results , BLEU-0.888 , NIST -0.867 , METEOR-0.829 , TER -0.826 , WER -0.821 ) .
Recent best performing metrics in the WMT - 14 metric shared task ( Mach?cek and Bojar , 2014 ) used a combination of different metrics .
The top performing system DiskoTK - Party - Tuned ( Joty et al. , 2014 ) in the WMT - 14 task uses five different discourse metrics and twelve different metrics from the ASIYA MT evaluation toolkit ( Gim?nez and M?rquez , 2010 ) .
The metric computes the number of common sub-trees between a reference and a translation using a convolution tree kernel ( Collins and Duffy , 2001 ) .
The basic version of the metric does not perform well but in combination with the other 12 metrics from the ASIYA toolkit obtained the best results for the WMT - 14 metric shared task .
Another top performing metric LAYERED ( Gautam and Bhattacharyya , 2014 ) , uses linear interpolation of different metrics .
LAY - ERED uses BLEU and TER to capture lexical similarity , Hamming score and Kendall Tau Distance ( Birch and Osborne , 2011 ) to identify syntactic similarity , and dependency parsing ( De Marneffe et al. , 2006 ) and the Universal Networking Language 1 for semantic similarity .
For our participation in the WMT - 15 task , we used our metric ReVal ( Gupta et al. , 2015 ) .
ReVal metric is based on dense vector spaces and Tree Long Short Term Memory networks .
This metric achieved state of the art results for the WMT - 14 dataset .
The metric including training data is available at https://github.com/rohitguptacs/ReVal.
LSTMs and Tree-LSTMs Recurrent Neural Networks allow processing of arbitrary length sequences , but early RNNs had the problem of vanishing and exploding gradients ( Bengio et al. , 1994 ) . RNNs with LSTM ( Hochreiter and Schmidhuber , 1997 ) tackle this problem by introducing a memory cell composed of a unit called constant error carousel ( CEC ) with multiplicative input and output gate units .
Input gates protect against irrelevant inputs and output gates against current irrelevant memory contents .
This architecture is capable of capturing important pieces of information seen in a bigger context .
Tree -LSTM is an extension of simple LSTM .
A typical LSTM processes the information sequentially whereas Tree-LSTM architectures enable sentence representation through a syntactic structure .
Equation ( 1 ) represents the composition of a hidden state vector for an LSTM architecture .
For a simple LSTM , c t represents the memory cell and o t the output gate at time step t in a sequence .
For Tree -LSTM , c t represents the memory cell and o t represents the output gate corresponding to node t in a tree .
The structural processing of Tree-LSTM makes it more favourable for representing sentences .
For example , dependency tree structure captures syntactic features and model parameters capture the importance of words ( content vs. function words ) .
h t = o t ? tanh c t ( 1 ) 1 http://www.undl.org/unlsys/unl/unl2005/UW.htm
Figure 1 shows simple LSTM and Tree-LSTM architectures .
. . x 2 . . y 2 . . y 1 . x 1 . . y 3 . x 3 . . y 4 . x 4 . . y 5 . x 5 . . x 1 . y 1 . . x 2 . y 2 . . x 3 . y 3 . .
Evaluation Metric
We used the ReVal ( Gupta et al. , 2015 ) metric for this task .
This metric represents both the reference ( h ref ) and the translation ( h tra ) using a dependency Tree-LSTM ( Tai et al. , 2015 ) and predicts the similarity score ? based on a neural network which considers both distance and angle between h ref and h tra : h ?
= h ref ?
h tra h + = |h ref ?
h tra | h s = ? ( W ( ? ) h ? + W ( +) h + + b ( h ) ) p? = softmax ( W ( p ) h s + b ( p ) ) ? = r T p? ( 2 ) where , ? is a sigmoid function , p? is the estimated probability distribution vector and r T = [ 1 2 ... K ] .
The cost function J ( ? ) is defined over probability distributions p and p? using regularised Kullback - Leibler ( KL ) divergence .
J ( ? ) = 1 n n ? i=1 KL ( p ( i ) || p ( i ) ? ) + ? 2 ||?|| 2 2 ( 3 ) In Equation 3 , i represents the index of each training pair , n is the number of training pairs and p is the sparse target distribution such that y = r T p is defined as follows : for 1 ? j ? K . Where , y ? [ 1 , K ] is the similarity score of a training pair .
For example , for y = 2.7 , p T = [ 0 0.3 0.7 0 0 ] .
In our case , the similarity score y is a value between 1 and 5 .
p j = ? ? ? ? ? y ? ?y? , j = ?y? + 1 ?y ? ? y + 1 , j = ?y?
0 otherwise
To compute our training data we automatically convert the human rankings of the WMT - 13 evaluation data into similarity scores between the reference and the translation .
These translationreference pairs labelled with similarity scores are used for training .
We also augment the WMT - 13 data with 4500 pairs from the SICK training set ( Marelli et al. , 2014 ) , resulting in a training dataset of 14059 pairs in total .
The metric uses Glove word vectors ( Pennington et al. , 2014 ) and the simple LSTM , the dependency Tree-LSTM and neural network implementations by Tai et al . ( 2015 ) .
Training is performed using a mini batch size of 25 with learning rate 0.05 and regularization strength 0.0001 .
The memory dimension is 300 , hidden dimension is 100 and compositional parameters are 541,800 .
Training is performed for 10 epochs .
System level scores are computed by aggregating and normalising the segment level scores .
Full details can be found in ( Gupta et al. , 2015 ) . 2
Results
The results for WMT - 15 are presented in Table 1 and Table 2 .
Table 1 shows system-level Pearson correlation ( TrueSkill ) ( see ( Bojar et al. , 2013 ) for difference between TrueSkill and Pre-TrueSkill systemranking approaches ) obtained on different language pairs as well as average ( PAvg ) over all language pairs .
The second last column shows average Pearson correlation ( Pre-TrueSkill ) .
The last column shows average Spearman correlation ( SAvg ) .
The 95 % confidence level scores are obtained using bootstrap resampling as used in the WMT - 2015 metric task evaluation .
Table 2 shows results on segment - wise Kendall tau correlation .
The first section of Table 1 and Table 2 shows the results of our ReVal metric as UoW - LSTM , the second section shows the other four top performing metrics and the third section shows baseline metrics ( BLEU , TER and WER for system- level and SENTBLEU for segment level ) .
Table 1 shows that our metric obtains the best results overall for both Pearson ( Pre-TrueSkill ) and Spearman system-level correlation and second best overall using Pearson ( TrueSkill ) correlation .
Table 2 shows that while improving over SENT - BLEU our metric does not obtain high segment level scores .
Conclusion and Future Work Our dense-vector-space - based ReVal metric is simple , elegant and fully competitive with the best of the current complex alternative approaches that involve system combination , extensive external resources , feature engineering and tuning .
In future work we will investigate the difference between system and segment level evaluation scores .
Figure 1 : 1 Figure 1 : Tree-LSTM ( left ) and simple LSTM ( right )
