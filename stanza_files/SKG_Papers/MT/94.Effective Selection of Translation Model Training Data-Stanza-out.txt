title
Effective Selection of Translation Model Training Data
abstract
Data selection has been demonstrated to be an effective approach to addressing the lack of high-quality bitext for statistical machine translation in the domain of interest .
Most current data selection methods solely use language models trained on a small scale in- domain data to select domain-relevant sentence pairs from general - domain parallel corpus .
By contrast , we argue that the relevance between a sentence pair and target domain can be better evaluated by the combination of language model and translation model .
In this paper , we study and experiment with novel methods that apply translation models into domain-relevant data selection .
The results show that our methods outperform previous methods .
When the selected sentence pairs are evaluated on an end-to- end MT task , our methods can increase the translation performance by 3 BLEU points . *
Introduction Statistical machine translation depends heavily on large scale parallel corpora .
The corpora are necessary priori knowledge for training effective translation model .
However , domain-specific machine translation has few parallel corpora for translation model training in the domain of interest .
For this , an effective approach is to automatically select and expand domain-specific sentence pairs from large scale general - domain parallel corpus .
The approach is named Data Selection .
Current data selection methods mostly use language models trained on small scale indomain data to measure domain relevance and select domain-relevant parallel sentence pairs to expand training corpora .
Related work in literature has proven that the expanded corpora can substantially improve the performance of ma -*
Corresponding author chine translation ( Duh et al. , 2010 ; .
However , the methods are still far from satisfactory for real application for the following reasons : ?
There is n't ready - made domain-specific parallel bitext .
So it 's necessary for data selection to have significant capability in mining parallel bitext in those assorted free texts .
But the existing methods seldom ensure parallelism in the target domain while selecting domain-relevant bitext .
?
Available domain-relevant bitext needs keep high domain-relevance at both the sides of source and target language .
But it 's difficult for current method to maintain two -sided domain- relevance when we aim at enhancing parallelism of bitext .
In a word , current data selection methods ca n't well maintain both parallelism and domainrelevance of bitext .
To overcome the problem , we first propose the method combining translation model with language model in data selection .
The language model measures the domainspecific generation probability of sentences , being used to select domain-relevant sentences at both sides of source and target language .
Meanwhile , the translation model measures the translation probability of sentence pair , being used to verify the parallelism of the selected domainrelevant bitext .
Related Work
The existing data selection methods are mostly based on language model .
Yasuda et al. ( 2008 ) and Foster et al . ( 2010 ) ranked the sentence pairs in the general- domain corpus according to the perplexity scores of sentences , which are computed with respect to in- domain language models .
Axelrod et al. ( 2011 ) improved the perplexitybased approach and proposed bilingual crossentropy difference as a ranking function with inand general - domain language models .
Duh et al. ( 2013 ) employed the method of ( Axelrod et al. , 2011 ) and further explored neural language model for data selection rather than the conventional n-gram language model .
Although previous works in data selection ( Duh et al. , 2013 ; Axelrod et al. , 2011 ; Foster et al. , 2010 ; Yasuda et al. , 2008 ) have gained good performance , the methods which only adopt language models to score the sentence pairs are sub-optimal .
The reason is that a sentence pair contains a source language sentence and a target language sentence , while the existing methods are incapable of evaluating the mutual translation probability of sentence pair in the target domain .
Thus , we propose novel methods which are based on translation model and language model for data selection .
Training Data Selection Methods
We present three data selection methods for ranking and selecting domain-relevant sentence pairs from general - domain corpus , with an eye towards improving domain-specific translation model performance .
These methods are based on language model and translation model , which are trained on small in - domain parallel data .
Data Selection with Translation Model Translation model is a key component in statistical machine translation .
It is commonly used to translate the source language sentence into the target language sentence .
However , in this paper , we adopt the translation model to evaluate the translation probability of sentence pair and develop a simple but effective variant of translation model to rank the sentence pairs in the generaldomain corpus .
The formulations are detailed as below : ( ) ( ) ? ? ( ) ( 1 ) ? ( ) ( 2 ) Where ( ) is the translation model , which is IBM Model 1 in this paper , it represents the translation probability of target language sentence conditioned on source language sentence . and are the number of words in sentence and respectively . ( ) is the translation probability of word conditioned on word and is estimated from the small in - domain parallel data .
The parameter is a constant and is assigned with the value of 1.0. is the lengthnormalized IBM Model 1 , which is used to score general - domain sentence pairs .
The sentence pair with higher score is more likely to be generated by in- domain translation model , thus , it is more relevant to the in-domain corpus and will be remained to expand the training data .
Data Selection by Combining Translation and Language model
As described in section 1 , the existing data selection methods which only adopt language model to score sentence pairs are unable to measure the mutual translation probability of sentence pairs .
To solve the problem , we develop the second data selection method , which is based on the combination of translation model and language model .
Our method and ranking function are formulated as follows : ( ) ( ) ( ) ( 3 ) ? ( ) ? ( ) ( 4 ) Where ( ) is a joint probability of sentence and according to the translation model ( ) and language model ( ) , whose parameters are estimated from the small in - domain text .
is the improved ranking function and used to score the sentence pairs with the length - normalized translation model ( ) and language model ( ) .
The sentence pair with higher score is more similar to in- domain corpus , and will be picked out .
Data Selection by Bidirectionally Combining Translation and Language Models
As presented in subsection 3.2 , the method combines translation model and language model to rank the sentence pairs in the general - domain corpus .
However , it does not evaluate the inverse translation probability of sentence pair and the probability of target language sentence .
Thus , we take bidirectional scores into account and simply sum the scores in both directions .
? ( ) ? ( ) ? ( ) ? ( ) ( 5 ) Again , the sentence pairs with higher scores are presumed to be better and will be selected to incorporate into the domain-specific training data .
This approach makes full use of two translation models and two language models for sentence pairs ranking .
Experiments
Corpora
We conduct our experiments on the Spoken Language Translation English - to - Chinese task .
System settings
We use the NiuTrans 2 toolkit which adopts GIZA ++ ( Och and Ney , 2003 ) and MERT to train and tune the machine translation system .
As NiuTrans integrates the mainstream translation engine , we select hierarchical phrasebased engine ( Chiang , 2007 ) to extract the translation rules and carry out our experiments .
Moreover , in the decoding process , we use the NiuTrans decoder to produce the best outputs , and score them with the widely used NIST mt-eval131a 3 tool .
This tool scores the outputs in several criterions , while the case-insensitive BLEU - 4 ( Papineni et al. , 2002 ) is used as the evaluation for the machine translation system .
Translation and Language models
Our work relies on the use of in ( Stolcke , 2002 ) to train the in- domain 4 - gram language model with interpolated modified Kneser - Ney discounting ( Chen and Goodman , 1998 ) .
The language model is only used to score the general - domain sentences .
Meanwhile , we use the language model training scripts integrated in the NiuTrans toolkit to train another 4 - gram language model , which is used in MT tuning and decoding .
Additionally , we adopt GIZA ++ to get the word alignment of in-domain parallel data and form the word translation probability table .
This table will be used to compute the translation probability of general - domain sentence pairs .
Baseline Systems
As described above , by using the NiuTrans toolkit , we have built two baseline systems to fulfill " 863 " SLT task in our experiments .
The In-domain baseline trained on spoken language corpus has 1.05 million rules in its hierarchicalphrase table .
While , the General- domain baseline trained on 16 million sentence pairs has a hierarchical phrase table containing 1.7 billion translation rules .
These two baseline systems are equipped with the same language model which is trained on large-scale monolingual target language corpus .
The BLEU scores of the Indomain and General-domain baseline system are listed in Table 2 .
The results show that General- domain system trained on a larger amount of bilingual resources outperforms the system trained on the in-domain corpus by over 12 BLEU points .
The reason is that large scale parallel corpus maintains more bilingual knowledge and language phenomenon , while small in- domain corpus encounters data sparse problem , which degrades the translation performance .
However , the performance of General- domain baseline can be improved further .
We use our three methods to refine the generaldomain corpus and improve the translation performance in the domain of interest .
Thus , we build several contrasting systems trained on refined training data selected by the following different methods .
? Bidirectional TM +LM : Data selection by bidirectionally combining translation and language models ( equal weight ) .
Results of Training Data Selection
We adopt five methods for extracting domainrelevant parallel data from general - domain corpus .
Using the scoring methods , we rank the sentence pairs of the general - domain corpus and select only the top N = { 50k , 100k , 200k , 400k , 600k , 800k , 1000 k } sentence pairs as refined training data .
New MT systems are then trained on these small refined training data .
Figure 1 shows the performances of systems trained on selected corpora from the general- domain corpus .
The horizontal coordinate represents the number of selected sentence pairs and vertical coordinate is the BLEU scores of MT systems .
From Figure 1 , we conclude that these five data selection methods are effective for domainspecific translation .
When top 600k sentence pairs are picked out from general - domain corpus to train machine translation systems , the systems perform higher than the General - domain baseline trained on 16 million parallel data .
The results indicate that more training data for translation model is not always better .
When the domainspecific bilingual resources are deficient , the domain-relevant sentence pairs will play an important role in improving the translation performance .
Additionally , it turns out that our methods ( TM , TM +LM and Bidirectional TM + LM ) are indeed more effective in selecting domainrelevant sentence pairs .
In the end-to- end SMT evaluation , TM selects top 600k sentence pairs of general - domain corpus , but increases the translation performance by 2.7 BLEU points .
Meanwhile , the TM +LM and Bidirectional TM +LM have gained 3.66 and 3.56 BLEU point improvements compared against the generaldomain baseline system .
Compared with the mainstream methods ( Ngram and Neural net ) , our methods increase translation performance by nearly 3 BLEU points , when the top 600k sentence pairs are picked out .
Although , in the figure 1 , our three methods are not performing better than the existing methods in all cases , their overall performances are relatively higher .
We therefore believe that combining in- domain translation model and language model to score the sentence pairs is well - suited for domainrelevant sentence pair selection .
Furthermore , we observe that the overall performance of our methods is gradually improved .
This is because our methods are combining more statistical characteristics of in-domain data in ranking and selecting sentence pairs .
The results have proven the effectiveness of our methods again .
Conclusion
We present three novel methods for translation model training data selection , which are based on the translation model and language model .
Compared with the methods which only employ language model for data selection , we observe that our methods are able to select high-quality domain-relevant sentence pairs and improve the translation performance by nearly 3 BLEU points .
In addition , our methods make full use of the limited in - domain data and are easily implemented .
In the future , we are interested in applying ?
Ngram : Data selection by 4 - gram LMs with Kneser - Ney smoothing .
( Axelrod et al. , 2011 ) Neural net : Data selection by Recurrent Neural LM , with the RNNLM Tookit .
( Duh et al. , 2013 ) ? Translation Model ( TM ) : Data selection with translation model : IBM Model 1 . ? Translation model and Language Model ( TM + LM ) : Data selection by combining 4 gram LMs with Kneser - Ney smoothing and IBM model 1 ( equal weight ) .
