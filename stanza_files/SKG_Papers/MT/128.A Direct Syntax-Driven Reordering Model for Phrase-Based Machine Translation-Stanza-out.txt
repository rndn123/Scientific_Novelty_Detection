title
A Direct Syntax -Driven Reordering Model for Phrase -Based Machine Translation
abstract
This paper presents a direct word reordering model with novel syntax - based features for statistical machine translation .
Reordering models address the problem of reordering source language into the word order of the target language .
IBM
Models 3 through 5 have reordering components that use surface word information but very little context information to determine the traversal order of the source sentence .
Since the late 1990s , phrase - based machine translation solves much of the local reorderings by using phrasal translations .
The problem of longdistance reordering has become a central research topic in modeling distortions .
We present a syntax driven maximum entropy reordering model that directly predicts the source traversal order and is able to model arbitrarily long distance word movement .
We show that this model significantly improves machine translation quality .
Introduction
Machine translation reordering models model the problem of the word order when translating a source language into a target language .
For example in Spanish and Arabic , adjectives often come after the nouns they modify whereas in English modifying adjectives usually precede the nouns .
When translating Spanish or Arabic into English , the position of the adjectives need to be properly reordered to be placed before the nouns to make fluent English .
In this paper , we present a word reordering model that models the word reordering process in translation .
The paper is organized as follows .
?2 outlines previous approaches to reordering .
?3 details our model and its training and decoding process .
?4 discusses experiments to evaluate the model and ?5 presents machine translation results .
?6 is discussion and conclusion .
Previous Work
The word reordering problem has been one of the major problems in statistical machine translation ( SMT ) .
Since exploring all possible reorderings of a source sentence is an NP - complete problem ( Knight 1999 ) , SMT systems limit words to be reordered within a window of length k. IBM Models 3 through 5 ( Brown et.al. 1993 ) model reorderings based on surface word information .
For example , Model 4 attempts to assign target - language positions to source - language words by modeling d ( j | i , l , m ) where j is the target - language position , i is the source - language position , l and m are respectively source and target sentence lengths .
These models are not effective in modeling reorderings because they do n't have enough context and lack structural information .
Phrase - based SMT systems such as ( Koehn et.al. 2003 ) move from using words as translation units to using phrases .
One of the advantages of phrasebased SMT systems is that local reorderings are inherent in the phrase translations .
However , phrase - based SMT systems capture reordering instances and not reordering phenomena .
For example , if the Arabic phrase " the car red " and its English translation " the red car ' is seen in the training data , phrase - based SMT is able to produce the correct English for the Arabic ' the car red ' .
However it will not be able to produce ' the blue car ' for the Arabic ' the car blue ' if the training data does not contain this phrase pair .
Phrases do not capture the phenomenon that Arabic adjectives and nouns need to be reordered .
Another problem with phrase - based SMT is the problem of longrange reorderings .
Recent work on reordering has been focusing on capturing general reordering phenomena ( as opposed to instances ) and on solving long- range reordering problems .
( Al-onaizan et.al. 2006 ) proposes 3 distortion models , the inbound , outbound , and pair models .
They together model the likelihood of translating a source word at position i given that the source word at position j has just been translated .
These models perform better than n-gram based language models but are limited in their use of only the surface strings .
Instead of directly modeling the distance of word movement , phrasal level reordering models model how to move phrases , also called orientations .
Orientations typically apply to adjacent phrases .
Two adjacent phrases can be either placed monotonically ( sometimes called straight ) or swapped ( non-monotonically or inverted ) .
Early orientation models do not use lexical contents such as ( Zens et. al. , 2004 ) .
More recently , ( Xiong et.al. 2006 ; Zens 2006 ; Och et. al , 2004 ; Tillmann , 2004 ; Kumar et al. , 2005 , Ni et al. , 2009 all presented models that use lexical features from the phrases to predict their orientations .
These models are very powerful in predicting local phrase placements .
More recently ( Galley et.al. 2008 ) introduced a hierarchical orientation model that captures some non-local phrase reorderings by a shift reduce algorithm .
Because of the heavy use of lexical features , these models tend to suffer from data sparseness problems .
Another limitation is that these models are restricted to reorderings with no gaps and phrases that are adjacent .
We present a probabilistic reordering model that models directly the source translation sequence and explicitly assigns probabilities to the reorderings of the source input with no restrictions on gap , length or adjacency .
This is different from the approaches of pre-order such as ( Xia and McCord 2004 ; Collins et.al. 2005 ; Kanthak et. al. 2005 ; . Although our model can be used to produce top N pre-ordered source , the experiments reported here do not use the model in the pre-order mode .
Instead , the reordering model is used to generate a reorder lattice which encodes many reorderings and their costs ( negative log probability ) .
This reorder lattice is independent of the translation decoder .
In principle , any decoder can use this lattice for its reordering needs .
We have integrated the reorder lattice into a phrasebased .
The experiments reported here are from the phrase - based decoder .
We present the reordering model based on maximum entropy models .
We then describe the syntactic features in the context of Chinese to English translation .
Maximum Entropy Reordering Model
The model takes a source sequence of length n : ] , ... , [ 2 1 n s s s S = and models its translation or visit order according to the target language : ] , ... , [ 2 1 n v v v V = where v j is the source position for target position j .
For example , if the 2 nd source word is to be translated first , then v 1 = 2 .
We find V such that ) 2 ( ) , | ( max ) 1 ( ) | ( max arg 1 1 ... 1 } { ? = ? ? = n j j j V v v S v p S V p ?
In equation ( 1 ) { ?
} is the set of possible visit orders .
We want to find a visit order V such that the probability p( V |S ) is maximized .
Equation ( 2 ) is a component - wise decomposition of ( 1 ) .
Let ) ... , ( 1 1 ? = = j j v v S h and v f
We use the maximum entropy model to estimate equation ( 2 ) : ? = k k k h f h Z h f p ) 3 ( ) ) , ( exp ( ) ( 1 ) | ( ? ? where Z( h ) is the normalization constant ) 4 ( ) , ( exp ) ( ? ? = f k k k h f h Z ? ?
In equation ( 3 ) , ? k ( f , h ) are binary - valued features .
During training , instead of exploring all possible permutations , samples are drawn given the correct path only .
Feature Overview
Most of our features ? k ( f , h ) are syntax - based .
They examine how each parse node is reordered during translation .
We also have a few non-syntax features that inspect the surface words and part-ofspeech tags .
They complement syntax features by capturing lexical dependencies and guarding against parsing errors .
Instead of directly model - ing the absolute source position v j , we model the jump from the last source position v j-1 .
All features share two common components : j ( for jump ) , and cov ( for coverage ) .
Jumps are bucketed and capped at 4 to prevent data sparsity .
Coverage is an integer indicating the visiting status of the words between the jump .
Coverage is 0 if none of the words was visited prior to this step , 1 if all were visited , and 2 if some but not all were visited .
( j , cov ) are present in all features and are removed from the descriptions below .
A couple of features use a variation of Jump and Coverage .
These will be described in the feature description .
Parse- based Syntax Features
We use the sentence pair in Figure 1 . as a working example when describing the features .
Shown in the figure are a Chinese - English parallel sentence pair , the word alignments between them , and the Chinese parse tree .
The parse tree is simplified .
Some details such as part- of-speech tags are omitted and denoted by triangles .
The first step is to determine the source visit sequence from the word alignment , also shown at the bottom of Fig-ure 1 .
If a target is aligned to more than one source , we assume the visit order is left to right .
In Figure 1 , source words 2 and 8 are aligned to the English ' at ' and we define the visit sequence to be 8 following 2 .
Chinese and English differ in the positioning of the modifiers .
In English , non-adjectival modifiers follow the object they modify .
This is most prominent in the use of relative clauses and prepositional phrases .
Chinese in contrast is a premodification language where modifiers whether adjectival , clausal or prepositional typically precede the object they modify .
In Figure 1 . , the Chinese prepositional phrase PP ( in lightly shaded box in the parse tree ) spanning range [ 2,8 ] precedes the verb phrase VP2 at positions [ 9 , 10 ] .
These two phrases are swapped in English as shown by the two lightly shaded boxes in the alignment grid .
The relative clause CP ( in dark shaded box in the parse tree ) in Chinese spanning range [ 3 , 6 ] precedes the noun phrase NP3 at position 7 whereas these two phrases are again swapped in English .
The phenomenon for the reordering model to capture is that node VP1 's two children PP and VP2 ( lightly shaded ) need to be swapped regardless of how long the PP phrase is .
This is also true for node NP2 whose two children CP and NP3 ( dark shaded ) need to be reversed .
Parse - based features model how to reorder the constituents in the parse by learning how to walk the parse nodes .
For every non-unary node in the parse we learn such features as which of its child is visited first and for subsequent visits how to jump from one child to another .
For the treelet VP1 PP VP2 in Figure 1 , we learn to visit the child VP2 first , then PP .
We now define the notion of ' node visit ' .
When a source word s i is visited at step j , we find its path to root from the leaf node denoted as PathToRoot i .
We say all the nodes contained in PathToRoot i are being visited at that step .
Parse - based features are applied to every qualifying node in PathToRoot i .
Unary extensions do not qualify and are ignored .
Since part- of-speech tags are unary branches , parse- based features apply from the lowest - level labels .
Another condition depends on the jump and is discussed in section ?3.4 .
All our features are encoded by a vector of integers and are denoted as ? ( ? ) in this paper .
We now describe the features .
First Child Features
The first-child feature applies when a node is visited for the first time .
The feature learns which of the node 's child to visit first .
This feature learns such phenomena as translating the main verb first under a VP or translating the main NP first under an NP .
The feature is defined as ?
( currentLabel , parentLabel , nthNode , j , cov ) where currentLabel = label of the current parse node parentLabel = label of the parent node nthNode = an integer indicating the nth occurrence of the current node
In Figure 1 , when source word 9 is visited at step 2 , its PathToRoot is computed which is [ VP2 , VP1 , IP1 ] .
The first-child feature applied to VP2 is ?( VP2 , VP1 , 1 , 4 , 1 ) since currentLabel = VP2 ; parentLabel = VP1 ; nthChild = 1 : VP2 is the 1 st VP among its parent 's children j = 4 : actual jump from 1 is 8 and is capped .
cov = 0 : words in between the jump [ 1 , 9 ] are not yet visited at this step .
The semantics of this feature is that when a VP node is visited , the first VP child under it is visited first .
This feature learns to visit the first VP first which is usually the head VP no matter where it is positioned or how many modifiers precede it .
Node Jump Features
This feature applies on all subsequent visits to the parse node .
This feature models how to jump from one sibling to another sibling .
This feature has these components : ?( currentLabel , parentLabel , fromLable , nodeJump , cov ) where fromLabel = the node label where the jump is from nodeJump = node distance from that node
This feature effectively captures syntactic reorderings by looking at the node jump instead of surface distance jump .
In our example , a node-jump feature for jumping from source 10 to 2 at step 4 at VP1 level is ?( PP , VP1 , VP2 , - 1 , 2 ) where currentLabel = PP where source word 2 is under parentLabel = VP1 fromLabel = VP2 where source word 10 is under nodeJump = - 1 since the jump is from VP2 to PP cov = 2 because in between [ 2,10 ] word 9 has been visited and other words have not .
This feature captures the necessary information for the ' PP VP ' reorderings regardless of how long the PP or VP phrase is .
Jump Over Sibling Features
To make a correct jump from one sibling to the other , siblings that are jumped over should also be considered .
For example in Chinese , while jumping over a PP to cover a VP is a good jump , jumping over an ADVP to cover a VP may not be because adverbs in both Chinese and English often precede the verb they modify .
The jump-oversibling features help distinguish these cases .
This feature 's components are ?( currentLabel , parent - Label , jumpOverSibling , sibling Cov , j ) where jum- pOverSibling is the label of the sibling that is jumped over and siblingCov is the coverage status of that sibling .
This feature applies to every sibling that is jumped over .
At step 2 where the jump is from source 1 to 9 , this feature at VP1 level is ?( VP2 , VP1 , PP , 0 , 4 ) because PP is a sibling of VP2 and is jumped over , PP is not covered at this step , and the jump is capped to be 4 .
Back Jump Sibling Features
For every forward jump of length greater than 1 , there is a backward jump to cover those words that were skipped .
In these situations we want to know how far we can move forward before we must jump backward .
The back - jump-sibling feature applies when the jump is backward ( distance is negative ) and inspects the sibling to the right .
It generates ?( currentLabel , rightSiblingCov , j ) .
When jumping from 10 to 2 at step 4 , this feature is ?( PP , 1 , - 4 ) where - 4 is the jump and currentLabel = PP where source word 2 is under rightSiblingCoverage = 1 since VP2 has been completed visited at this time .
This feature learns to go back to PP when its right sibling ( VP2 ) is completed .
Broken Features
Translations do not always respect the constituent boundaries defined by the source parse tree .
Consider the fragment in Figure 2 . After the VV under VP2 is translated ( " account for " ) , a transition is made to translate the ADVP ( " approximately " ) leaving VP2 partially translated .
We say that the node VP2 is broken at this step .
This type of feature has been shown to be useful for machine translation ( Marton & Resnik 2008 ) .
Here , broken features model the context under which a node is broken by observing the feature ?( curTag , prevTag , parentLabel , j , cov ) .
For the transition of source word 2 to source word 1 in Figure 2 , a broken feature applies at VP2 : ?( AD , VV , VP2 , - 1 , 1 ) .
This feature learns that a VP can be broken when making a jump from a verb ( VV ) to an adverb ( AD ) .
Non-Parse Features
Non-parse features do not use or use less finegrained information from the parse tree .
Barrier Features
Barrier features model the intuition that certain words such as punctuation should not move freely .
This phenomenon has been observed and shown to be helpful in ( Xiong et. al. , 2008 ) .
We call these words barrier words .
Barrier features are ?( barri-erWord , cov , j ) .
All punctuations are barrier words .
Number of Zero Islands Features
Although word reorderings can involve words far apart , certain jump patterns are highly unlikely .
For example , the coverage pattern ' 1010101010 ' where every other source word is translated would be very improbable .
Let the right most covered source word be the frontier .
For every jump , the number- of-zero-islands feature computes the number of uncovered source islands to the left of the frontier .
Additionally it takes into account the number of parse nodes in between .
This feature is defined as ?( numZeroIslands , j , num-ParseNodesInBetween ) .
The number of parse nodes is the number of maximum spanning nodes in between the jump .
The jump at step 2 from source 1 to 9 triggers this number-zero-island feature ?( 1 , 4 , 1 ) .
The source coverage status at step 2 is 10000000100 because the first source word has been visited and the current visit is source 9 .
All words in between have not been visited .
There is 1 contiguous sequence of 0's between the first ' 1 ' and the last ' 1 ' , hence the numZeroIslands = 1 .
There is one parse node PP that spans all the source words from 2 to 8 , therefore the last argument to the feature is 1 .
If instead , the transition was from source 1 to 8 , then there would be 2 maximum spanning parse nodes for source [ 2 , 7 ] which are nodes P and NP2 .
The feature would be ?( 1 , 4 , 2 ) .
This feature discourages scattered jumps that leave lots of zero islands and jump over lots of parse nodes .
Training Training the maximum entropy reordering model needs word alignments and source-side parses .
We use hand alignments from LDC .
The training data statistics are shown in Table 1 . We use the ( Levy and Manning 2003 )
The first column shows alignment type from source ( S ) to target ( T ) .
1 - 1 means one source word aligns to one target word .
m- 1 means many source words align to one target and vice versa .
? means unaligned source words .
After the source visit sequence is decided , features are generated .
Note that the height of the tree is not uniform for all the words .
To preserve the structure and also alleviate the depth problem , we use the lowest- level-common-ancestor approach .
For every jump , we generate features bottom up until we reach the node that is the common ancestor of the origin and the destination of the jump .
In Figure 1 there is a jump from source 7 to 6 at step 7 .
The lowest- level- common-ancestor for source 6 and 7 is the node NP2 and features are generated up to the level of NP2 .
Features on this training data are shown in the second column in Table 5 .
The MaxEnt model on this data is efficiently trained at 15 minutes per iteration ( 24 sentences / sec or 471 words / sec ) .
Experiments
Reorder Evaluation
To evaluate how accurate the reordering model is , we first compute its prediction accuracy .
We choose the first 100 sentences from NIST MT03 as our test set for this evaluation .
We manually word align them to the first set of reference using LDC annotation guidelines version 1.0 of April 2006 .
An average of 73 % of the training sentences contain unaligned source words and over 87 % of the test sentences contain unaligned source words .
The unaligned source words are mostly function words .
Because the visit sequence of unaligned source words are determined not by truth but by heuristics ( Table 2 ) , they pose a problem in evaluation .
We thus evaluate the model by measuring the accuracy of its decision conditioned on true history .
We measure performance on the model 's top - N choices for N = 1,2 , and 3 .
Results are in Table 3 ' Lexical ' errors are those that rise from lexical choice of source words .
For example , an " ADVP VP " structure would normally be visited monotonically .
However , in case of Chinese phrase ' so do ' , they should be swapped .
More than a third of the errors are of this nature .
Errors in the Refer-ence category are those that are marked wrong because of the particular English reference .
The proposed reorderings are correct but they do n't match the reference reorderings .
Another 30 % of the errors are due to parsing errors .
The Model errors are due to two sources .
One is the depth problem mentioned above .
Local statistics for some very deep treelets overwhelm the global statistics and local jumps win over the long jumps in these cases .
Another problem is the data sparseness .
For example , the model has learned to reorder the ' PP VP ' structure but there is not much data for ' PP ADVP VP ' .
The model fails to jump over PP into ADVP .
Feature Utility
We conduct ablation studies to see the utilities of each feature .
We take the best feature set which gives the performance in Table 3 and takes away one feature type at a time .
The results are in Table 5 .
The first row keeps all the features .
The Subtract column shows performance after subtracting each feature while keeping all the other features .
The Add column shows performance of adding the feature .
Using just first-child features gets 75.97 % .
Adding node-jump features moves the accuracy to 78.40 % and so on .
Features # Features Subtract
Add
Translation Experiments
Reorder Lattice Generation
The reordering model is used to generate reorder lattices which are used by machine translation decoders .
Reorder lattices have been frequently used in decoding in works such as , Kumar et.al.
2005 , Hildebrand et.al. 2008 , to name just a few .
The main difference here is that our lattices encode probabilities from the reordering model and are not used to preorder the source .
The lattice contains reorderings and their cost ( negative log probability ) .
Figure 4 shows a reorder lattice example .
Nodes are lattice states .
Arcs store source word positions to be visited ( translated ) and their cost and they are delimited by comma in the figure .
Lower cost indicates better choice .
Figure 4 is much simplified for readability .
It shows only the best path ( highlighted ) and a few neighboring arcs .
For example , it shows source words 1 , 2 , and 8 are the top 3 choices at step 1 .
Position 1 is the best choice with the lowest cost of 0.302 and so on .
The first part of the reference ( true ) path is indicated by the alignment which is source sequence 1 , 8 , 9 , and 2 .
We see that this matches the lattice 's top - 1 choice .
Lattice generation takes source sentence and source parse as input .
The lattice generation process makes use of a beam search algorithm .
Every node in the lattice generates top - N next possible positions and the rest is pruned away .
A coverage vector is maintained on each path to ensure each source word is visited exactly once .
A wide beam width explores many source positions at any step and results in a bushy lattice .
This is needed for machine translation because the parses are errorful .
The structures that are hard for MT to reorder are also hard for parsers to parse .
Labels critical to reordering such as CP are among the least accurate labels .
Overall parsing accuracy is 83.63 % but CP accuracy is 73.11 % .
We need a wide beam to include more long jumps to compensate the parsing errors .
Machine Translation
We run MT experiments on NIST Chinese- English test sets MT03 - 05 .
We compare the performance of using distance - based reordering and using maximum entropy reordering lattices .
The decoder is a log-linear phrase based decoder .
Translation models are trained from HMM alignments .
A smoothed 5 - gram English LM is built on the English Gigaword corpus and English side of the Chinese - English parallel corpora .
In the experiments , lexicalized distance - based reordering allows up to 9 words to be jumped over .
MT performance is measured by BLEUr4n4 ( Papineni et.al. 2001 ) .
The test set statistics and experiment results are show in Table 6 . Decoding with MaxEnt reorder lattices shows significant improvement for all conditions .
Conclusions
We present a direct syntax - based reordering model that captures source structural information .
The model is capable of handling reorderings of arbitrary length .
Long-range reorderings are essential in translation between languages with great word order differences such as Chinese-English and Arabic- English .
We have shown that phrase based SMT can benefit significantly from such a reordering model .
The current model is not regularized and feature selection by thresholding the feature counts is quite primitive .
Regularizing the model will prevent overfitting , especially given the small training data set .
Regularization will also make the ablation study more meaningful .
The reordering model presented here aims at capturing structural differences between source and target languages .
It does not have enough lexical features to deal with lexical idiosyncrasies .
We see from Table 4 that 34 % of the errors are due to source lexical choices which indicates the weakness of the current lexical features .
Regularization of the model might also make a difference with the lexical features .
Reordering and word choice in translation are not independent of each other .
We have shown some initial success with a separate reordering model .
In the future , we will build joint models on reordering and translation .
This approach will also address some of the reordering problems due to source lexical idiosyncrasies .
Figure 1 . A Chinese-English Parallel Sentence with Chinese Parse
Figure Figure 2 . A ' Broken '
Tree
Figure 4 . 4 Figure 4 . A lattice example
Figures 5 5 Figures 5 shows an example from MT output with word alignments to the Chinese input .
The MaxEnt reordering model correctly reorders two source modifiers at source positions 8 and 22 .
The Skip9 output reorders locally whereas the MaxEnt lattice output shows much more complex reorderings .
Figure 5 . MT comparisonOur initial attempt at adding lexical pair jump features ?( fromWord , toWord , j ) has not proved useful .
It hurt accuracy by 3 % ( from 80 % to 77 % ) .
We see from Table4that 34 % of the errors are due to source lexical choices which indicates the weakness of the current lexical features .
Regularization of the model might also make a difference with the lexical features .
Reordering and word choice in translation are not independent of each other .
We have shown some initial success with a separate reordering model .
In the future , we will build joint models on reordering and translation .
This approach will also address some of the reordering problems due to source lexical idiosyncrasies .
parser on Chinese .
Data # Sentences # Words LDC2006E93 10,408 230,764 LDC2008E57 11,463 194,024 Table 1 . Training Data
From the word alignments we first determine the source visit sequence .
Table 2 details how the visit sequence is determined in various cases .
Alignment Type S-T Visit Sequence 1 - 1 Left to right from target m-1 Left to right from source 1 - m
Left most target link ?
Attaches left Table 2 .
Determining visit sequence
Table 3 . 3 .
The table also shows the accuracy of no reordering in the Monotone column .
Reordering model performance Figure 3 plots accuracy vs .
MaxEnt training iteration .
Accuracy starts low at 74.7 % and reaches is highest at iteration 8 and fluctuates around 80.5 % thereafter .
Top -N Accuracy Monotone 1 80.56 % 65.39 % 2 90.66 % - 3 93.05 % - 81 80 79 78 77 76 75 74 73 72 71 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Figure 3 . Accuracy vs. MaxEnt Training Iteration
We analyze 50 errors from the top - 1 run .
The er- rors are categorized and shown in Table 4 . Error Category Percentage Lexical 34 % Parse 30 % Model 20 % Reference 16 %
Table 4 . 4 Error Analysis
Table 5 . 5 Ablation study on features - 80.56 % - First Child 7,559 79.87 % 75.97 % Node Jump 6,334 79.52 % 78.40 % JumpOver Sib. 2,403 80.52 % 79.00 % BackJump 602 80.48 % 79.05 % Broken 15,183 80.30 % 79.13 % Barrier 158 80.26 % 79.22 % NumZ Islands 200 79.52 % 80.56 %
