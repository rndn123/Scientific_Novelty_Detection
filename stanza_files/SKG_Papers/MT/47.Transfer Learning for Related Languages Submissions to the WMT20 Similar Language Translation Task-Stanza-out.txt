title
Transfer Learning for Related Languages : Submissions to the WMT20 Similar Language Translation Task
abstract
In this paper , we describe IIT Delhi 's submissions to the WMT 2020 task on Similar Language Translation for four language directions : Hindi ? Marathi and Spanish ?
Portuguese .
We try out three different model settings for the translation task and select our primary and contrastive submissions on the basis of performance of these three models .
For our best submissions , we fine - tune the mBART model ( Liu et al. , 2020 ) on the parallel data provided for the task .
The pre-training is done using self-supervised objectives on a large amount of monolingual data for many languages .
Overall , our models are ranked in the top four of all systems for the submitted language pairs , with first rank in Spanish ?
Portuguese .
Introduction Machine Translation ( MT ) is currently tackled using rule- based methods ( RBMT ) ( Charoenpornsawat et al. , 2002 ) , phrase - based statistical methods ( SMT ) ( Koehn et al. , 2003 ) and neural methods ( NMT ) ( Cho et al. , 2014 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ; Vaswani et al. , 2017 ) . NMT has achieved high translation quality for several language pairs ( Bojar et al. , 2018 ; Barrault et al. , 2019 ) , but this level of performance usually requires large amounts of aligned data in the order of millions of sentence pairs .
For low and medium resource languages , SMT performs better than NMT ( Koehn and Knowles , 2017 ; Sennrich and Zhang , 2019 ) .
SMT also shows better performance when there is a domain mismatch between the train and test datasets , which is typical of low and medium resource language pairs .
In these settings , NMT performance can be boosted by leveraging additional monolingual data to enforce various types of constraints or increasing the training data using back - translation .
These methods can be particularly helpful if the source and target languages in MT are closely related and share language structure and alphabet .
Recently , pre-training methods for sequence - to-sequence ( seq2seq ) models have been introduced like MASS ( Song et al. , 2019a ) , XLM ( Conneau and Lample , 2019 ) , BART ( Lewis et al. , 2019 ) , and mBART ( Liu et al. , 2020 ) .
These methods show significant gains in downstream tasks like NMT , summarization , natural language inference ( NLI ) , etc .
In this paper , we focus on the transfer learning capabilities in NMT for the task of translation between related languages where parallel data is scarce .
IIT Delhi participated in the WMT 2020 Shared task on Similar Language Translation for four language directions : Hindi ( hi ) ? Marathi ( mr ) and Spanish ( es ) ? Portuguese ( pt ) .
The first language pair is low resource and second is medium resource in terms of the parallel data available for the task .
Refer to Table 2 for the classification .
We fine- tuned the pre-trained mBART model ( Liu et al. , 2020 ) on the parallel data provided for the task .
mBART gives better performance than SMT models even when the parallel data is very limited .
mBART is pre-trained on 25 languages , which contain Hindi and Spanish , but not Marathi and Portuguese .
mBART is able to leverage transfer learning capabilities even for those languages that are originally not present during the pre-training phase .
The fine- tuned mBART architecture forms our best submissions for both language pairs : hi ? mr and es ? pt .
The rankings obtained by us in each of the language directions are listed in The results and analysis are detailed in Section 5 .
We finally conclude in Section 6 .
Background SMT is tackled by building a phrase table from the aligned parallel data .
The target side translation is then generated by matching the most appropriate phrases in the source sentence conditioned on the target side language model along with a reordering model ( Koehn et al. , 2003 ) . NMT is modeled using Encoder - Decoder models ( Cho et al. , 2014 ; Sutskever et al. , 2014 ; Bahdanau et al. , 2015 ) , with the Transformer model ( Vaswani et al. , 2017 ) achieving state - of - the - art on many MT problems .
But these models ' reliance on large aligned parallel data for the source and target languages makes them unsuitable for low / medium resource language pairs ( Koehn and Knowles , 2017 ) .
Some of the previous works in these settings to improve NMT performance are described below :
Multilingual NMT
Instead of using only two languages ( source and target ) for training an NMT model , using multiple languages has been shown to help in low resource scenarios .
For example , it might be the case that a certain pair of languages have very little parallel data between them , but there exists a third language with abundant parallel data with the original two languages .
This third language acts as a pivot and helps in improving NMT between the two languages ( Aharoni et al. , 2019 ; Gu et al. , 2018 ; Liu et al. , 2020 ; Zhang et al. , 2020 ) .
Back-Translation Back-Translation Hoang et al. , 2018 ) increases the amount of training data by using monolingual corpus along with partially - trained NMT models on the limited parallel data .
Pseudo- parallel corpus for each direction is first obtained by generating the translations of the monolingual data for each language using the partially - trained MT models on the limited parallel data .
Using these pseudoparallel corpora , the partially - trained NMT models are then trained further for some number of steps .
In this way , millions of pseudo- parallel sentence pairs can be generated to improve NMT models because of the abundance of monolingual data .
Another version of using back -translation is the copying mechanism .
Currey et al. ( 2017 ) proposes to copy the target side monolingual data on the source side to create additional data without modifying the training regimen for NMT .
This helps the model to generate fluent translations .
Pre-trained Language Models For NMT , the first step is the random initialization of model weights in both the encoder and decoder .
Instead of random initialization , NMT models can be initialized by pre-training parts of the model ( Conneau and Lample , 2019 ; Edunov et al. , 2019 ) , or pre-training the complete seq2seq model ( Ramachandran et al. , 2017 ; Song et al. , 2019 b ; Liu et al. , 2020 ) .
These pre-training methods leverage different kinds of masking techniques and the pretraining objective is to predict these masked tokens , similar to BERT ( Devlin et al. , 2019 ) .
Denoising auto-encoding can also be used where a sentence is corrupted by various noising techniques and the pre-training objective is to generate the original uncorrupted sentence as in BART ( Lewis et al. , 2019 ) and mBART ( Liu et al. , 2020 ) .
Incorporating Linguistic Information in NMT
There also have been works to improve low / medium resource NMT by adding linguistic information either using data augmentation ( Currey and Heafield , 2019 ) , subword embedding augmentation , or architectural changes ( Eriguchi et al. , 2017 ) .
This helps the model to not only learn the alignment between source and target language spaces , but also syntax structure like dependency parse , part of speech , etc .
This helps in making the target side translations more fluent and conforming to the structure of the language .
We do not explore this direction in this paper .
System Overview
We experimented with three different settings for hi ? mr as listed below .
SMT
This phrase - based system leverages both monolingual and parallel data provided for the task .
We use Moses ( Koehn et al. , 2007 ) for training the SMT systems .
NMT ( Transformer )
For this , we used the standard Transformer large architecture from Vaswani et al . ( 2017 ) for training on the parallel data provided for the task .
NMT ( mBART ) mBART ( Liu et al. , 2020 ) is a large Transformer pre-trained on monolingual data for 25 languages .
The pre-training objective for mBART is seq2seq de-noising for natural text as in BART ( Lewis et al. , 2019 ) . mBART provides a general- purpose pre-trained Transformer for any downstream task .
It has been shown to give significant improvements over the random initialization for NMT and is the current state - of - the - art for many low resource language pairs .
Implementation Details mBART uses a shared subword vocabulary of 250 K tokens for all the 25 languages present in the pre-training .
We use the same vocabulary for Marathi and Portuguese also , even though they were not used during the pre-training phase .
Marathi shares its subword vocabulary with languages like Hindi and Nepali in mBART , and Portuguese shares with Spanish , Italian and other European languages present in mBART .
The percentage of unknown tokens [ UNK ] in Marathi and Portuguese parallel datasets is less than 0.003 % when using the shared mBART vocabulary .
Additionally , the mBART architecture requires language specific token at the end of each input sequence to provide the language specific context for the decoder .
Since Marathi and Portuguese were not present during the pre-training phase , we use the token corresponding to the second most related language present in mBART pre-training for specifying the context at the time of decoding in each case .
For Marathi , we used the Nepali language token and for Portuguese , we used the Italian language token .
We could not use Spanish language token for Portuguese because we are doing translations to and from Spanish .
Experiments
We use hi ? mr and es ?
pt language pairs for our experiments .
Datasets & Preprocessing Because of the constrained nature of the shared task , we only use the parallel data provided for this task .
We removed the empty instances for both language pairs ( < 2000 instances ) .
For es ?
pt , we do not use ' WikiTitles v2 ' part of the parallel data for training because of very short sentences in the dataset .
The cleaned parallel dataset statistics are provided in Table 2 . Preprocessing
We use sentence piece tokenization ( Kudo and Richardson , 2018 ) for generating the source and target sequences for the NMT architectures .
For the standard Transformer , we train a sentence piece model using 40 K subword tokens for hi ? mr .
For mBART , we use Liu et al . ( 2020 ) 's pre-trained 1 sentence piece model comprising of 250 K subword tokens as the vocabulary .
For the SMT model on hi ? mr , we also use the monolingual data provided for this task .
We extract 5 Million monolingual sentences each for Hindi and Marathi after deduplication and use this set for training the language models .
We use Moses ( Koehn et al. , 2007 ) NMT ( mBART )
For this , we use 12 Transformer encoder and decoder layers , with total number of model parameters ?611 Million .
We use the pretrained mBART for initializing the model weights .
We follow the recommendations of Liu et al . ( 2020 ) for the hyperparameter settings .
We stop the training after 25 K gradient updates for the model .
These updates take ?35 hours on 4 Nvidia V100 ( 32 GB ) GPUs .
Evaluation
We use case- insensitive BLEU scores ( Papineni et al. , 2002 ) calculated using sacreBLEU 2 ( Post , 2018 ) .
These scores are calculated on the validation set to decide our primary and contrastive submissions .
For evaluating performance on the test set , the organizers use BLEU , TER ( Snover et al. , 2006 ) , and RIBES ( Isozaki et al. , 2010 ) .
Results and Analysis Results
Table 3 shows our results on the test set for our primary and contrastive submissions .
We observed the performance of our three model settings on the validation set , and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi ? mr .
Similarly , the mBART model forms our primary submission for es ?
pt .
Table 4 lists our final results on this shared task .
We also list the BLEU scores for the submission that got first rank in each of the language directions .
Since the test sets were hidden at the time of submission , we do not report our numbers on the standard Transformer architecture .
Analysis
Even though Marathi and Portuguese are not present during the pre-training phase of mBART , fine - tuning on these languages provides significant boosts over SMT and standard Transformer .
This shows that some level of language independent multilingual embeddings are present in the pre-trained model weights which can be exploited for the transfer task .
Discussion and Conclusion
We have participated in the Similar Language Translation task on four language directions .
We have shown that pre-trained models can help in low and medium resource NMT .
Our best system uses the pre-trained mBART model ( Liu et al. , 2020 ) and fine-tunes on the parallel data provided for the specific translation task .
Our results demonstrate that pre-training can help even when the language used for fine - tuning is not present during pre-training .
One direction of future work is to add linguistic information during the pre-training phase to get more fluent translations .
When this information is not available directly ( especially for low resource languages ) , pre-training on a related high resource language with syntax information can help low resource languages also .
by the DARPA Explainable Artificial Intelligence ( XAI ) Program with number N66001-17-2-4032 , Visvesvaraya Young Faculty Fellowships by Govt. of India and IBM SUR awards .
Any opinions , findings , conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies , either expressed or implied , of the funding agencies .
Table 1 1 . Our findings are in
Table 2 : 2 Dataset statistics .
First is low resource pair ( # train < 1 Million ) and second is medium resource ( 1 Million < # train < 10 Million ) .
Model hi - mr ? ? SMT 18.74 14.91 mBART 24.53 15.14
Table 3 : 3 BLEU scores on Hindi ?
Marathi on the test set for our primary and contrastive submissions .
Table 4 : 4 for all tokenization / detokenization scripts .
Hindi - Marathi and Spanish - Portuguese BLEU scores on the test dataset of the Similar Language Translation Task .
Our submission scores are bolded when they match the first ranked submission .
Submission hi - mr es - pt ? ? ? ? IIT Delhi ( ours ) 24.53 15.14 32.84 32.69 Rank 1 24.53 18.26 33.82 32.69 4.2 Model Architectures & Training SMT
We generate a phrase table for the SMT model using the code provided by Lample et al . ( 2018 ) .
We used Moses ( Koehn et al. , 2007 ) and Giza ++ with standard settings to train the SMT model in both directions .
NMT ( Transformer )
We use the large Trans - former from Vaswani et al . ( 2017 ) with 8 encoder and decoder layers and replicate all the parameters from Ott et al . ( 2018 ) .
The number of parameters in the model are approximately 248 Million and it takes ? 26 hours on 4 Nvidia V100 ( 32 GB ) GPUs .
https://github.com/pytorch/fairseq/ blob/master/examples/mbart/README.md
Signature : BLEU + case.mixed + numrefs .1 + smooth .exp + tok.13a + version .1.3.1
http://supercomputing.iitd.ac.in/
