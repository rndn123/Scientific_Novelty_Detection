title
Machine Translation Human Evaluation : an investigation of evaluation based on Post-Editing and its relation with Direct Assessment
abstract
In this paper we present an analysis of the two most prominent methodologies used for the human evaluation of MT quality , namely evaluation based on Post-Editing ( PE ) and evaluation based on Direct Assessment ( DA ) .
To this purpose , we exploit a publicly available large dataset containing both types of evaluations .
We first focus on PE and investigate how sensitive TER - based evaluation is to the type and number of references used .
Then , we carry out a comparative analysis of PE and DA to investigate the extent to which the evaluation results obtained by methodologies addressing different human perspectives are similar .
This comparison sheds light not only on PE but also on the so-called reference bias related to monolingual DA .
Also , we analyze if and how the two methodologies can complement each other 's weaknesses .
Introduction
The evaluation of machine translation ( MT ) is of crucial importance and has a long research history .
Both human and automatic evaluation have been explored extensively within the MT community , in the effort to find more and more suitable , efficient and reliable methods and metrics .
Automatic metrics play a central role in the progress of the field and the improvement of MT quality over time .
However , they represent a proxy for human evaluation which - despite being costly and time - consuming - is to be considered primary .
Among the various human evaluation methods that have been devised and tested along the years , currently two approaches have become well - established standards in the field , namely evaluation based on Post-Editing ( PE ) and evaluation based on Direct Assessment ( DA ) .
In the PE - based evaluation , the MT outputs are postedited , i.e. manually corrected , according to the source sentence ( bilingual PE ) or to an existing reference translation ( 2 ) Work conducted while this author was at FBK .
( monolingual PE ) .
The original MT outputs are then evaluated against their post-edited versions through TER - based automatic metrics [ 1 ] .
Relying on the post-edit instead of an independently created reference translation ensures that only true errors in the MT output are counted , and not those differences due to linguistic variation , which are accounted for by post-editors .
PE has become the standard evaluation metric for the yearly evaluation campaign of the International Workshop of Spoken Language Translation since 2013 ( IWSLT - 2013 ) and is described in detail in [ 2 ] .
The DA - based evaluation [ 3 ] consists of collecting human assessments of translation quality for single MT systems .
Assessors see a candidate translation and a corresponding translation hint ( e.g. the source text , a reference translation , or multimodal content ) and are asked to assign a quality score from 0 to 100 .
DA has become the standard evaluation metric for the yearly Conference on Machine Translation ( WMT ) in 2017 [ 4 ] .
Following the findings of WMT17 , the main focus for DA is on semantic transfer ( which corresponds to adequacy ) while syntactic transfer ( or fluency ) has turned out to be less relevant .
Traditionally , in the DA task MT quality is assessed according to a reference translation , without access to the source text .
This is called referencebased DA ( DA - ref ) .
A problematic issue with DA - ref is its inherent dependence on reference translations , which can lead to reference bias , both in the form of giving an implicit boost to candidate translations which are very similar ( e.g. , in syntax or lexical choice ) to the corresponding reference text , or by penalizing good translations because of translation errors affecting the reference itself .
To address the reference bias , source - based DA ( DA -src ) can be used , where translation quality is assessed directly according to the source text .
DA - src has been tested on a large scale for the first time in the IWSLT 2017 evaluation campaign [ 5 ] .
DA and PE are different and complementary methodologies , not only from the point of view of their design but also concerning their practical usage .
First , the two evaluation methods address different human perspectives .
Indeed , while DA focuses on the generic assessment of overall translation quality , PE - based evaluation reflects a real application scenario - the integration of MT in Computer -Assisted Translation ( CAT ) tools - and directly measures the utility of a given MT output to translators .
Furthermore , while DA is based only on human annotators , in PE an automatic component ( i.e. TER ) is applied to quantify the errors of the MT output .
Finally , in terms of data collection DA is less costly then PE and thus more viable when used within the research scenario ; however PE has the double advantage of ( i ) producing a set of additional reference translations , and ( ii ) being particularly suitable for performing fine - grained analyses of the MT systems , since it produces a set of edits pointing to specific translation errors [ 6 , 7 , 8 ] .
Given the importance of human evaluation for MT improvement and the specific features of these two most prominent frameworks , we present an empirical analysis of these different methodologies as a contribution to their better understanding .
The analysis is conducted on the publicly available Human Evaluation dataset created as part of the IWSLT 2017 evaluation campaign [ 5 ] .
The dataset covers two language directions , namely Dutch-to - German and Romanian - to -Italian .
For each direction , it includes DA -src , DA -ref , and PE human evaluation data for nine different state - of - the art neural MT systems on the same 603 segments .
DA evaluation was performed by linguists , while professional translators carried out the bilingual PE task .
Besides making our study possible , the size , variety and high quality of this three - way evaluation dataset ensure sound empirical analyses and generalizable outcomes .
The main investigations presented in the paper are : ?
New analyses on PE data .
The availability of multiple post-edits allows us to investigate how sensitive TERbased evaluation is to the type ( external versus postedit ) and number of references used , both in terms of reliability and informativeness of the evaluation ; ?
New comparative analysis of PE and DA .
Related Work Human Evaluation has always received a lot of attention in the field of MT and many methodologies have been devised and tested in different scenarios .
The same holds for the two methods addressed in this paper .
PE - based evaluation was the focus of various studies [ 1 , 9 , 6 ] and was commonly employed in large-scale evaluation campaigns , such as IWSLT [ 2 , 10 , 11 , 12 , 5 ] and the MT Quality Estimation Task at WMT - 2015 [ 13 ] .
Also research on DA has been very active since its introduction as method for human evaluation of MT [ 3 , 14 ] .
Large-scale evaluations were carried out through DA - ref [ 4 ] and , more recently , also through DA - src [ 5 , 15 ] .
As specifically regards the impact of different numbers and types of post-edits in PE - based evaluation , a study on multiple references was presented in [ 16 ] , but it did not target PE - based evaluation .
Concerning the issue of reference bias in DA - ref evaluation , it was examined in detail in [ 17 ] , [ 18 ] , and [ 19 ] .
To this aim , [ 17 ] compares directly DA - src and DA - ref but on a very small dataset , not comparable to the one used in our investigation .
As regards the comparative analysis of DA and PE , correlation results between DA - ref and HTER for 9 language directions are presented in [ 19 ] .
However , the evaluation data differs in many respects , making results not comparable .
First , the dataset used in this paper includes both DA - ref and DA - src .
Furthermore , PE data is made of multiple bilingual post-edits created by professional translators native in the target language and working in their professional CAT environment .
On the contrary , the post-edits used to calculate HTER in [ 19 ] were created through monolingual postediting , probably based on the same reference used to collect DA - ref judgments .
Evaluation Data
To perform our investigations on DA and PE we relied on the Human Evaluation dataset created as part of the IWSLT 2017 evaluation campaign [ 5 ] .
The resource is publicly available at the WIT 3 website [ 20 ] , where all IWSLT data and tools are released by the organizers of the campaign .
1
The dataset is based on TED talks 2 and includes 603 sentences ( around 10,000 source words ) , corresponding to the first half of ten different TED talks .
It covers two language pairs , namely Dutch - German ( N lDe ) and Romanian -Italian ( RoIt ) which - belonging to two distinct families ( West- Germanic and Romance , respectively ) - show rather different characteristics .
For each language direction , evaluation data were collected for nine different state - of - the - art neural MT systems : three standard bilingual systems ( i.e. a different system is created for each language direction ) and six multilingual systems ( i.e. one single system for multiple language directions ) , out of which three in the zero-shot condition ( i.e. tested on language pairs that are not present in the training data ) .
Furthermore , systems differ also for their architecture , since some of them implement Recurrent Neural Networks , while others are based on the Transformer model [ 21 ] .
3
The MT systems were evaluated on all the 603 dataset sentences according to PE , source - based DA , and referencebased DA .
Details on human evaluation data are given in the following .
Post- Editing data
This evaluation was carried out through bilingual postediting : the outputs of the nine MT systems on the 603 test sentences were assigned to nine professional translators to be manually corrected directly according to the source sentence .
To ensure the soundness of the evaluation and cope with translators ' variability , an equal number of outputs from each MT system was assigned randomly to each translator , in such a way that each translator had to post-edit all the sentences in the test set but only once .
The resulting PE data used in this study consists of nine new reference translations for each sentence of the test set .
Each one of these references represents the targeted reference of the system output from which it was derived , while the post-edits of the other systems are available for evaluation as additional references .
All details about data preparation and post-editing can be found in [ 2 , 5 ] .
In addition to the PE data , an external - independently created - reference was also available , for a total of ten references for each of the 603 sentences in the dataset .
Direct Assessment data Both DA -src and DA - ref data were collected for all the MT system outputs on all the 603 test sentences employing bilingual linguists .
To ensure the reliability of the human assessments , part of the collected data was used for quality control .
Based on artificially degraded translation outputwhich should be scored worse than the corresponding candidate translation - it is possible to identify users who randomly assign scores without paying attention to the presented data and , thus , work unreliably .
Only annotations from reliable annotators were used to compute the final system evaluation .
Furthermore , as annotators may have different annotation behaviour , the collected scores ( at least two for each sentence ) were standardized into z scores , which capture the number of standard deviations a score is different from ( i.e. better or worse than ) the respective annotator 's mean score .
Then , z scores were averaged at segment and system level to determine the overall MT system quality as observed by all annotators .
Analysis of PE - based evaluation
As described in Section 1 , evaluation via post-editing is based on TER , which measures the amount of editing that a human would have to perform to change an automatic translation so that it exactly matches a given reference translation .
Since TER is an automatic metric that works on exact word matching , it is unable to distinguish differences between MT output and reference due to normal linguistic variation from those due to real MT errors .
For this reason the reference translations used in TERbased evaluation ( as in all automatic evaluations ) play a central role in determining its reliability and informativeness .
It is widely accepted that the most suitable reference to evaluate an MT system is its corresponding post-edit ( targeted reference ) , since it is derived from that specific system and thus should differ from the MT output only with respect to the parts of it that are incorrect .
External references are at the other hand of the spectrum , since they are manually generated by translating the source text from scratch , independently from any MT system output .
A particular case of reference is the post-edit of an actual system output which is not the one under evaluation .
In this case the reference represents one of the many possible translation options and can indeed differ from the evaluated MT output due to linguistic variation .
However , being created starting from an MT output , it is possible that its peculiar features make it more suitable to MT evaluation .
This type of reference is particularly interesting since it can be easily gathered , being a natural by -product of professional translation in the CAT framework .
Finally , the usage of multiple references has often been investigated as a way to address the issue of acceptable linguistic variation , under the assumption that the more references the highest the reliability of the evaluation .
In this section we exploited the PE data - i.e. one external reference and nine post-edits created from the nine evaluated MT systems - to carry out different analyses aimed at understanding if and how TER - based evaluation is sensitive to the type and number of references used .
Depending on the reference ( s ) used in the analysis , we relied on different variants of TER , namely : ( i ) Humantargeted TER ( HTER ) , where TER is computed between the machine translation and its post-edited version ( targeted reference ) ; ( ii ) Multiple reference TER ( mTER ) , where TER is computed against the closest reference - i.e. the one which minimizes the number of edits - among all the available ones .
We empirically analyzed the impact of references in the evaluation from two different angles : ( i ) for each evaluated MT system , we investigated the specific contribution of each of the nine available post-edits to the mTER score of the system ; ( ii ) for each language pair , we calculated how overall MT system performance ( i.e. TER score ) varies depending on the type and number of references used .
Figure 1 shows an example of the distribution of the identity of systems which originated the post-edits that were chosen as closest reference translation in the computation of mTER .
Four NlDe systems are presented in the figure , among which three were post-edited ( BL.lab1 , SD.lab2 , ZS.lab3 ) and one was not ( SD.lab4 ) , and is shown for comparison purposes .
The same behaviour of the NlDe systems presented in the figure was observed also for the other NlDe systems as well as for the RoIt direction .
As expected , the peak occurs in correspondence of the post-edit of the system under evaluation .
Looking at the cor- responding column , we note however that the targeted reference is the closest to the MT output only for around onethird of the test set ( orange-coloured ) , while for another third there is at least one equivalent post-edit from another system ( blue-coloured ) .
Interestingly enough , for the remaining third of the test set , the closest reference is a post-edit from another system .
Looking at the columns of the post-edits originated from the other 8 MT systems , we see that for a non-negligible number of test sentences these references represent the closest translation ( orange ) .
This is particularly relevant when confronted to the results of the external reference translation , which is not shown in the figure since it was never picked as the closest reference translation .
It is also worthwhile to note that the post-edits of other MT systems created by the same Lab - which are expected to have similar outputs - are not chosen as closest references significantly more often than the post-edits of other Labs ' systems .
This suggests that the advantage of the post-edits of other systems does not rely in the similarity of the MT systems but more generally in the fact that the reference translation is derived from an MT output .
From the point of view of the number of references used in the evaluation , we understand from Figure 1 that a certain degree of variability is present also in the targeted translation - since for one - third of the test set it does not ensure the lowest edit distance with the MT output .
We can thus confirm that - even when a targeted reference is available - mTER guarantees the highest reliability of the evaluation .
Finally , the rightmost part of Figure 1 presents results for a system ( SD.lab4 ) for which no post-edit was created .
We can see that the closest references are equally distributed among all the available references , further confirming the importance of having multiple references .
The same conclusions can be drawn by analyzing the overall performances of the MT systems when using different reference translations .
For each language direction , Figure 2 shows the impact that each of the ten references at our disposal has on TER , averaged across systems .
The vertical bars provide the TER score computed using a single reference , be it one of the external post-edits , the targeted post-edit , or the external reference ; for each system , the PEs are considered in reverse order with respect to their overall score , that is from the farthest to the closest to the system output , which invariably is the targeted PE ; the external reference is presented as the last ; the red line represents the mTER computed on an incremental set of references .
The low TER results obtained using a single non-targeted post-edit are quite interesting .
Indeed evaluating a system against a post-edit created for another system is more sound than using an external reference .
This is particularly relevant in a real application scenario where obtaining a post-edit of a system is easy and inexpensive .
On the same line , considering the mTER cumulative score , it is interesting to see that the same HTER results obtained with the targeted reference ( trgPE , dark green bar ) can be achieved using seven external post-edits for the NlDe direction and six for the RoIt direction .
For completeness , Table 1 gives the exact figures of the most relevant information contained in Figure 2 , namely mTER using all 9 available post-edits , HTER , and TER over the external reference .
Indeed we can observe a considerable TER reduction when using all collected post-edits with respect to both the HTER obtained using the targeted post-edit and the TER obtained using the independent reference .
This reduction clearly confirms that exploiting all the available reference translations allows to produce a score which is not only more reliable but also more informative about the real performance of the systems .
mTER
Comparative analysis of DA - based and PE - based evaluation
As introduced in Section 1 , the DA - based and PE - based evaluation tasks focus on different aspects of automatic translation : general quality for the reader and usefulness for translator , respectively .
To investigate the extent to which PE and DA lead to similar results , for each evaluated system we calculated the Pearson correlation between PE - based scores and DA - based scores for each sentence in the test set .
The correlation results obtained for each system were then averaged through the Fisher transformations suggested in [ 25 ] .
Table 2 presents the average correlation results .
Correlations are calculated for both DA - src and DA - ref and for all the metrics investigated for PE - based evaluation , namely mTER , HTER and TER .
As expected , correlation is good , that is , in general segments judged as poor by DA annotators ( low DA scores ) also need substantial post-editing ( high PE scores ) or vice-versa .
Results slightly vary across language directions , but the same trends can be observed .
First , the highest correlation is found between DA - src and mTER , confirming that these are the two most highly reliable human evaluation measures .
As regards PE , mTER correlates better than HTER with DA , showing once again the importance of having multiple references .
As regards DA , correlation with PE is considerably Given the correlation results obtained , we carried out a further analysis to investigate whether having both evaluations can help improving the evaluation quality , i.e. whether the two methodologies can complement each other 's weaknesses .
Figure 3 shows the scatter plot of the correlation between mTER and DA - src for one of the investigated RoIt systems ( r=-0.5812 ) .
Conflicting evaluations appear in the lower -left and upper-right quadrants of the scatter plot .
The first quadrant includes segments which resulted good according to PE evaluation ( low PE scores ) but were judged as poor by DA annotators ( low DA scores ) ; the second includes segments which needed substantial post-editing ( high PE scores ) but were judged as good by DA annotators ( high DA scores ) .
Conflicting evaluation cases are particularly relevant since PE is known to be more informative ( see Section 1 ) , but DA could identify issues that PE - based evaluation cannot spot .
We manually inspected a sample of the sentences with conflicting evaluations and we found some interesting patterns .
Examples are provided in Table 3 .
When PE scores are low ( i.e. few edits are needed to correct the MT output ) but the translation is bad according to DA , typically the sentence contains few but crucial errors , which make it difficult to understand the meaning of the sentence ( see Example 1 in the table ) .
In these cases , the conflict is not solvable since from the point of view of DA - which is focused on adequacy - the MT output is rightfully not good , while from the point of view of the translator who has access to the source sentence , the MT output is indeed useful to speed - up translation .
In the opposite situation , i.e. high PE scores but good translation according to DA , we have two main causes for
Conclusions
In order to shed light on the properties , strengths and weaknesses of human evaluation it is crucial to rely on high quality datasets .
The specific characteristics of the IWSLT - 17 Human Evaluation dataset used in this investigation - size , variety and high quality of the three - way human evaluation - ensured sound empirical analyses and generalizable outcomes .
The main findings of this paper are summarized in the following .
Analysis on PE evaluation data : ? the targeted reference is the closest to the MT output only for one - third of the test sentences .
Thus , mTER guarantees the highest reliability of the evaluation over HTER ; ? evaluating a system against a post-edit created for another system is more sound than using an external reference , independently from the similarity of the two MT systems ; ? the same results obtained with the targeted reference ( HTER ) can be achieved using six / seven external postedits ( mTER ) , not including the targeted reference .
Comparative analysis of DA and PE : ? the highest correlation is found between DA - src and mTER , confirming that these are the two most highly reliable human evaluation measures ; ? correlation with PE is considerably stronger for DAsrc than DA - ref .
This indicates that the so-called reference - bias affects not only automatic metrics but also DA - based human evaluation ; ? conflicting evaluations between DA - src and mTER exist .
In some cases DA - src can help mitigate the weakness of PE which depends on its automatic component .
In other cases conflicts are caused by inherent differences due to the fact that the two evaluation methods address different human perspectives .
To conclude , we are planning to extend our research on both the analyses presented in this paper .
First , we will further verify and generalize the results obtained on PE data by carrying out the analyses on other publicly available IWSLT datasets , which include multiple post-edits for other language directions such as English - German , English - French , and Vietnamese - English .
Second , we will compare more deeply how DA - ref and DA - src behave on the same data .
Finally , we will perform the manual analysis also on NlDe data .
Figure 1 : 1 Figure 1 : Frequency distribution of the closest PE selected in the computation of mTER of four NlDe systems .
For each system that originated the PE , in orange the number of sentences for which that PE was the closest translation to the MT under investigation , in blue the number of sentences where the PE was the closest together with at least another PE .
Figure 2 : 2 Figure 2 : TERs on single references ( green bars ) and mTER on increasing number of references ( red line ) .
Figure 3 : 3 Figure 3 : ZS.lab3
RoIt system : scatter plot of source - based DA standardized scores and mTER scores .
Table 1 : 1 HTER 9 PE refs tgt PE 1 ext ref TER NlDe 23.80 29.96 66.10 RoIt 23.64 31.25 61.56 % TERs computed on different ( set of ) references .
Table 2 : 2 Average ( DA , PE ) correlations across systems .
avg ( r) NlDe mTER HTER TER RoIt mTER HTER TER zDA src -0.5466 -0.4796 -0.1918 -0.5294 -0.4306 -0.2137 ref -0.4491 -0.4100 -0.3579 -0.4524 -0.3882 -0.3570 mTER DA -src ( abs ) 1 . SRC
Nu are flapsuri , balamale , eleroane , actuatoare sau alte suprafee de control , doar o simpl ?
elice .
It has no flaps , no hinges , no ailerons , no actuators , no other control surfaces , just a simple propeller .
MT
Non ha fiori , balconi , elenchi , attuatori o altre superfici di controllo , solo una semplice elica .
It has no flowers , no balconies , no lists , no actuators , no other control surfaces , just a simple propeller .
14.43 % 28 PE
Non ha flaps , cerniere , alettoni , attuatori o altre superfici di controllo , solo una semplice elica .
2 . SRC
Prietenele mele , feministe convinse , au fost s , ocate .
My [ female ] friends , committed feminist , were aghast MT I miei amici , femministe convinti , sono rimasti scioccati .
My [ male ] friends , committed feminist , were aghast .
47.87 % 88 PE
Le mie amiche , femministe convinte , sono rimasti scioccate .
Table 3 : 3 RoIt language direction .
Examples of conflicting DA - PE evaluation . conflicts .
First , we found very short or long sentences which are indeed good translations but the mTER score was not correct due to tokenization ( and consequently alignment ) problems .
These cases highlight the main weakness of PE - based evaluation , namely the fact that it relies on automatic metrics to compute the edit distance .
The other type of conflict ( see Example 2 in the table ) regards those segments that have to be heavily post-edited for amending errors which do not alter the overall comprehension , like in chains of morphological errors .
In these cases , the MT errors affect more fluency than adequacy , to which DA - based assessment is less sensitive .
Proceedings of the 15 th International Workshop on Spoken Language Translation Bruges , Belgium , October 29-30 , 2018
https://wit3.fbk.eu/show.php?release=2017-02&page=subjeval&texthead=Evaluation%20Data 2 www.ted.com 3
All details about the MT systems can be found in [ 22 , 23 , 24 ] .
