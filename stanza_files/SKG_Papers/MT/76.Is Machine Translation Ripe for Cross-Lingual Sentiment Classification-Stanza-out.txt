title
Is Machine Translation Ripe for Cross-lingual Sentiment Classification ?
abstract
Recent advances in Machine Translation ( MT ) have brought forth a new paradigm for building NLP applications in low-resource scenarios .
To build a sentiment classifier for a language with no labeled resources , one can translate labeled data from another language , then train a classifier on the translated text .
This can be viewed as a domain adaptation problem , where labeled translations and test data have some mismatch .
Various prior work have achieved positive results using this approach .
In this opinion piece , we take a step back and make some general statements about crosslingual adaptation problems .
First , we claim that domain mismatch is not caused by MT errors , and accuracy degradation will occur even in the case of perfect MT .
Second , we argue that the cross-lingual adaptation problem is qualitatively different from other ( monolingual ) adaptation problems in NLP ; thus new adaptation algorithms ought to be considered .
This paper will describe a series of carefullydesigned experiments that led us to these conclusions .
Summary Question
1 : If MT gave perfect translations ( semantically ) , do we still have a domain adaptation challenge in cross-lingual sentiment classification ?
Answer : Yes .
The reason is that while many translations of a word may be valid , the MT system might have a systematic bias .
For example , the word " awesome " might be prevalent in English reviews , but in translated reviews , the word " excellent " is generated instead .
From the perspective of MT , this translation is correct and preserves sentiment polarity .
But from the perspective of a classifier , there is a domain mismatch due to differences in word distributions .
Question
2 : Can we apply standard adaptation algorithms developed for other ( monolingual ) adaptation problems to cross-lingual adaptation ?
Answer : No .
It appears that the interaction between target unlabeled data and source data can be rather unexpected in the case of cross-lingual adaptation .
We do not know the reason , but our experiments show that the accuracy of adaptation algorithms in cross-lingual scenarios have much higher variance than monolingual scenarios .
The goal of this opinion piece is to argue the need to better understand the characteristics of domain adaptation in cross-lingual problems .
We invite the reader to disagree with our conclusion ( that the true barrier to good performance is not insufficient MT quality , but inappropriate domain adaptation methods ) .
Here we present a series of experiments that led us to this conclusion .
First we describe the experiment design ( ?2 ) and baselines ( ?3 ) , before answering Question 1 ( ?4 ) and Question 2 ( ?5 ) .
Experiment Design
The cross-lingual setup is this : we have labeled data from source domain S and wish to build a sentiment classifier for target domain T . Domain mismatch can arise from language differences ( e.g. English vs. translated text ) or market differences ( e.g. DVD vs. Book reviews ) .
Our experiments will involve fixing T to a common testset and varying S .
This allows us to experiment with different settings for adaptation .
We use the Amazon review dataset of Prettenhofer ( 2010 ) 1 , due to its wide range of languages ( English [ EN ] , Japanese [ JP ] , French [ FR ] , German [ DE ] ) and markets ( music , DVD , books ) .
Unlike Prettenhofer ( 2010 ) , we reverse the direction of cross-lingual adaptation and consider English as target .
English is not a low-resource language , but this setting allows for more comparisons .
Each source dataset has 2000 reviews , equally balanced between positive and negative .
The target has 2000 test samples , large unlabeled data ( 25 k , 30k , 50 k samples respectively for Music , DVD , and Books ) , and an additional 2000 labeled data reserved for oracle experiments .
Texts in JP , FR , and DE are translated word- by- word into English with Google Translate .
2
We perform three sets of experiments , shown in Table 1 .
Table 2 lists all the results ; we will interpret them in the following sections .
How much performance degradation occurs in cross-lingual adaptation ?
First , we need to quantify the accuracy degradation under different source data , without consideration of domain adaptation methods .
So we train a SVM classifier on labeled source data 3 , and directly apply it on test data .
The oracle setting , which has no domain-mismatch ( e.g. train on Music - EN , test on Music - EN ) , achieves an average test accuracy of ( 81.6 + 80.9 + 80.0 ) /3 = 80.8 % 4 . Aver-age cross-lingual accuracies are : 69.4 % ( JP ) , 75.6 % ( FR ) , 77.0 % ( DE ) , so degradations compared to oracle are : - 11 % ( JP ) , - 5 % ( FR ) , - 4 % ( DE ) .
5 Crossmarket degradations are around - 6 % 6 . Observation
1 : Degradations due to market and language mismatch are comparable in several cases ( e.g. MUSIC -DE and DVD -EN perform similarly for target MUSIC - EN ) .
Observation
2 : The ranking of source language by decreasing accuracy is DE > FR > JP .
Does this mean JP - EN is a more difficult language pair for MT ?
The next section will show that this is not necessarily the case .
Certainly , the domain mismatch for JP is larger than DE , but this could be due to phenomenon other than MT errors .
4
Where exactly is the domain mismatch ?
Theory of Domain Adaptation
We analyze domain adaptation by the concepts of labeling and instance mismatch ( Jiang and Zhai , 2007 ) .
Let p t ( x , y ) = p t ( y| x ) p t ( x ) be the target distribution of samples x ( e.g. unigram feature vector ) and labels y ( positive / negative ) .
Let p s ( x , y ) = p s ( y| x ) p s ( x ) be the corresponding source distribution .
We assume that one ( or both ) of the following distributions differ between source and target : ?
Instance mismatch : p s ( x ) = p t ( x ) .
?
Labeling mismatch : p s ( y|x ) = p t ( y| x ) .
Instance mismatch implies that the input feature vectors have different distribution ( e.g. one dataset uses the word " excellent " often , while the other uses the word " awesome " ) .
This degrades performance because classifiers trained on " excellent " might not know how to classify texts with the word " awesome . "
The solution is to tie together these features ( Blitzer et al. , 2006 ) or re-weight the input distribution ( Sugiyama et al. , 2008 ) .
Under some assumptions ( i.e. covariate shift ) , oracle accuracy can be achieved theoretically ( Shimodaira , 2000 ) .
Labeling mismatch implies the same input has different labels in different domains .
For example , the JP word meaning " excellent " may be mistranslated as " bad " in English .
Then , positive JP reviews will be associated with the word " bad " : p s (y = + 1| x = bad ) will be high , whereas the true conditional distribution should have high p t ( y = ?1| x = bad ) instead .
There are several cases for labeling mismatch , depending on how the polarity changes ( Table 3 ) .
The solution is to filter out these noisy samples ( Jiang and Zhai , 2007 ) or optimize loosely - linked objectives through shared parameters or Bayesian priors ( Finkel and Manning , 2009 ) .
Which mismatch is responsible for accuracy degradations in cross-lingual adaptation ?
?
Instance mismatch : Systematic MT bias generates word distributions different from naturallyoccurring English .
( Translation may be valid . ) ?
Label mismatch : MT error mis-translates a word into something with different polarity .
Conclusion from ?4 .2 and ?4.3 : Instance mismatch occurs often ; MT error appears minimal .
