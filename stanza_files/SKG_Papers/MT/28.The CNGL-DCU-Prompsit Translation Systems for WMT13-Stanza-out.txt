title
The CNGL-DCU -Prompsit Translation Systems for WMT13
abstract
This paper presents the experiments conducted by the Machine Translation group at DCU and Prompsit Language Engineering for the WMT13 translation task .
Three language pairs are considered : Spanish -English and French - English in both directions and German-English in that direction .
For the Spanish - English pair , the use of linguistic information to select parallel data is investigated .
For the French - English pair , the usefulness of the small indomain parallel corpus is evaluated , compared to an out-of- domain parallel data sub-sampling method .
Finally , for the German- English system , we describe our work in addressing the long distance reordering problem and a system combination strategy .
Introduction
This paper presents the experiments conducted by the Machine Translation group at DCU 1 and Prompsit Language Engineering 2 for the WMT13 translation task on three language pairs : Spanish -English , French -English and German-English .
For these language pairs , the language and translation models are built using different approaches and datasets , thus presented in this paper in separate sections .
In Section 2 , the systems built for the Spanish - English pair in both directions are described .
We investigate the use of linguistic information to select parallel data .
In Section 3 , we present the systems built for the French - English pair in both di-rections .
The usefulness of the small in- domain parallel corpus is evaluated , compared to an outof-domain parallel data sub-sampling method .
In Section 4 , for the German- English system , aiming at exploring the long distance reordering problem , we first describe our efforts in a dependency treeto-string approach , before combining different hierarchical systems with a phrase - based system and show a significant improvement over three baseline systems .
Spanish-English
This section describes the experimental setup for the Spanish - English language pair .
Setting
Our setup uses the MOSES toolkit , version 1.0 ( Koehn et al. , 2007 ) .
We use a pipeline with the phrase - based decoder with standard parameters , unless noted otherwise .
The decoder uses cube pruning ( - cube-pruning-pop-limit 2000 -s 2000 ) , MBR ( - mbr-size 800 - mbr-scale 1 ) and monotone at punctuation reordering .
Individual language models ( LMs ) , 5 - gram and smoothed using a simplified version of the improved Kneser - Ney method ( Chen and Goodman , 1996 ) , are built for each monolingual corpus using IRSTLM 5.80.01 ( Federico et al. , 2008 ) .
These LMs are then interpolated with IRSTLM using the test set of WMT11 as the development set .
Finally , the interpolated LMs are merged into one LM preserving the weights using SRILM ( Stolcke , 2002 ) .
We use all the parallel corpora available for this language pair : Europarl ( EU ) , News Commentary ( NC ) , United Nations ( UN ) and Common Crawl ( CC ) .
Regarding monolingual corpora , we use the freely available monolingual corpora ( Eu-roparl , News Commentary , News 2007 - 2012 as well as the target side of several parallel corpora : Common Crawl , United Nations and 10 9 French - English corpus ( only for English as target language ) .
Both the parallel and monolingual data are tokenised and truecased using scripts from the MOSES toolkit .
Data selection
The main contribution in our participation regards the selection of parallel data .
We follow the perplexity - based approach to filter monolingual data ( Moore and Lewis , 2010 ) extended to filter parallel data ( Axelrod et al. , 2011 ) .
In our case , we do not measure perplexity only on word forms but also using different types of linguistic information ( lemmas and named entities ) ( Toral , 2013 ) .
We build LMs for the source and target sides of the domain-specific corpus ( in our case NC ) and for a random subset of the non-domainspecific corpus ( EU , UN and CC ) of the same size ( number of sentences ) of the domain-specific corpus .
Each parallel sentence s in the non-domainspecific corpus is then scored according to equation 1 where P P Isl ( s ) is the perplexity of s in the source side according to the domain-specific LM and P P Osl ( s ) is the perplexity of s in the source side according to the non-domain-specific LM .
P P Itl ( s ) and P P O tl ( s ) contain the corresponding values for the target side .
score ( s ) = 1 2 ? ( P P I sl ( s ) ? P P O sl ( s ) ) +( P P I tl ( s ) ? P P O tl ( s ) ) ( 1 ) Table 1 shows the results obtained using four models : word forms ( forms ) , forms and named entities ( forms+nes ) , lemmas ( lem ) and lemmas and named entities ( lem+nes ) .
Details on these methods can be found in Toral ( 2013 ) .
For each corpus we selected two subsets ( see in bold in Table 1 ) , the one for which one method obtained the best perplexity ( top 5 % of EU using forms , 2 % of UN using lemmas and 50 % of CC using forms and named entities ) and a bigger one used to compare the performance in SMT ( top 14 % of EU using lemmas and named entities ( lem+nes ) , top 12 % of UN using forms and named entities and the whole CC ) .
These subsets are used as training data in our systems .
As we can see in the table , the use of linguistic information allows to obtain subsets with lower perplexity than using solely word forms , e.g. 1057.7 ( lem+nes ) versus 1104.8 ( forms ) for 14 % of EU .
The only exception to this is the subset that comprises the top 5 % of EU , where perplexity using word forms ( 957.9 ) is the lowest one .
The advantage of data selection is clear .
The second system , although smaller in size compared to the first ( 1.4 M sentence pairs versus 2.1M ) , takes its training from a more varied set of data , and its performance is over one absolute BLEU point higher .
When comparing the two systems that rely on data selection , one might expect the one that uses data with lower perplexity ( small ) to perform better .
However , this is not the case , the third system ( big ) performing around half an absolute BLEU point higher than the second ( small ) .
This hints at the fact that perplexity alone is not an optimal metric for data selection , but size should also be considered .
Note that the size of system 3 's phrase table is more than double that of system 2 .
French -English
This section describe the particularities of the MT systems built for the French - English language pair in both directions .
The goal of the experimental setup presented here is to evaluate the gain of adding small in- domain parallel data into a translation system built on a sub-sample of the out-ofdomain parallel data .
Data Pre-processing
All the available parallel and monolingual data for the French - English language pair , including the last versions of LDC Gigaword corpora , are normalised and special characters are escaped using the scripts provided by the shared task organisers .
Then , the corpora are tokenised and for each language a true-case model is built on the concatenation of all the data after removing duplicated sentences , using the scripts included in MOSES distribution .
The corpora are then true-cased before being used to build the language and the translation models .
Language Model
To build our final language models , we first build LMs on each corpus individually .
All the monolingual corpora are considered , as well as the source or target side of the parallel corpora if the data are not already in the monolingual data .
We build modified Kneser - Ney discounted 5 - gram LMs using the SRILM toolkit for each corpus and separate the LMs in three groups : one in- domain ( containing news - commentary and news crawl corpora ) , another out-of- domain ( containing Common Crawl , Europarl , UN and 10 9 corpora ) , and the last one with LDC Gigaword LMs ( the data are kept separated by news source , as distributed by LDC ) .
The LMs in each group are linearly interpolated based on their perplexities obtained on the concatenation of all the development sets from previous WMT translation tasks .
The same development corpus is used to linearly interpolate the in-domain and LDC LMs .
We finally obtain two LMs , one containing out - of- domain data which is only used to filter parallel data , and another one containing in -domain data which is used to filter parallel data , tuning the translation model weights and at decoding time .
Details about the number of n-grams in each language model are presented in
Translation Model
Two phrase - based translation models are built using MGIZA ++ ( Gao and Vogel , 2008 ) and MOSES 3 , with the default alignment heuristic ( grow-diag-final ) and bidirectional reordering models .
The first translation model is in- domain , built with the news -commentary corpus .
The second one is built on a sample of all the other parallel corpora available for the French - English language pair .
Both corpora are cleaned using the script provided with Moses , keeping the sentences with a length below 80 words .
For the second translation model , we used the modified Moore - Lewis method based on the four LMs ( two per language ) presented in section 3.2 .
The sum of the source and target perplexity difference is computed for each sentence pair of the corpus .
We set an acceptance threshold to keep a limited amount of sentence pairs .
The kept sample finally contains ?
3.7 M sentence pairs to train the translation model .
Statistics about this data sample and the news -commentary corpus are presented in Table 4 .
The test set of WMT12 translation task is used to optimise the weights for the two translation models with the MERT algorithm .
For this tuning step , the limit of target phrases loaded per source phrase is set to 50 .
We also use a reordering constraint around punctuation marks .
The same parameters are used during the decoding of the test set .
Results
The two translation models presented in Section 3.3 allow us to design three translation systems : one using only the in-domain model , one using only the model built on the sub-sample of the out-of- domain data , and one using both models by giving two decoding paths to Moses .
For this latter system , the MERT algorithm is also used to optimise the translation model weights .
Results obtained on the WMT13 test set , measured with the official automatic metrics , are presented in Table 5 .
The submitted system is the one built on the sub-sample of the out-of- domain parallel data .
This system was chosen during the tuning step because it reached the highest BLEU scores on the development corpus , slightly above the combination of the two translation models .
For both FR -EN and EN - FR tasks , the best results are reached by the system built on the subsample taken from the out-of- domain parallel data .
Using only News - Commentary to build a translation model leads to acceptable BLEU scores , with regards to the size of the training corpus .
When the sub-sample of the out-of- domain parallel data is used to build the translation model , adding a model built on News - Commentary does not improve the results .
The difference between these two systems in terms of BLEU score ( both cased sensitive and insensitive ) indicates that similar results can be achieved , however it appears that the amount of sentence pairs in the sample is large enough to limit the impact of the small in - domain corpus parallel .
Further experiments are still required to determine the minimum sample size needed to outperform both the in-domain system and the combination of the two translation models .
News
German-English
In this section we describe our work on German to English subtask .
Firstly we describe the Dependency tree to string method which we tried but unfortunately failed due to short of time .
Secondly we discuss the baseline system and the preprocessing we performed .
Thirdly a system combination method is described .
Dependency Tree to String Method
Our original plan was to address the long distance reordering problem in German-English translation .
We use Xie's Dependency tree to string method ( Xie et al. , 2011 ) which obtains good results on Chinese to English translation and exhibits good performance at long distance reordering as our decoder .
We use Stanford dependency parser 4 to parse the English side of the data and Mate-Tool 5 for the German side .
The first set of experiments did not lead to encouraging results and due to insufficient time , we decide to switch to other decoders , based on statistical phrase - based and hierarchical approaches .
Baseline System
In this section we describe the three baseline system we used as well as the preprocessing technologies and the experiments set up .
Preprocessing and Corpus
We first use the normalisation scripts provided by WMT2013 to normalise both English and German side .
Then we escape special characters on both sides .
We use Stanford tokeniser for English and OpenNLP tokeniser 6 for German .
Then we train a true-case model using with Europarl and News - Commentary corpora , and true-case all the corpus we used .
The parallel corpus is filtered with the standard cleaning scripts provided with MOSES .
We split the German compound words with jWordSplitter 7 .
All the corpus provided for the shared task are used for training our translation models , while WMT2011 and WMT2012 test sets are used to tune the models parameters .
For the LM , we use all the monolingual data provided , including LDC Gigaword .
Each LM is trained with the SRILM toolkit , before interpolating all the LMs according to their weights obtained by minimizing the perplexity on the tuning set ( WMT2011 and WMT2012 test sets ) .
As SRILM can only interpolate 10 LMs , we first interpolate a LM with Europarl , News Commentary , News Crawl ( 2007 - 2012 , each year individually , 6 separate parts ) , then we interpolate a new LM with this interpolated LM and LDC Gigawords ( we kept the Gigaword subsets separated according to the news sources as distributed by LDC , which leads to 7 corpus ) .
Three baseline systems
We use the data set up described by the former subsection and build up three baseline systems , namely PB MOSES ( phrase - based ) , Hiero MOSES ( hierarchical ) and CDEC ( Dyer et al. , 2010 ) .
The motivation of choosing Hierarchical Models is to address the German-English 's long reorder problem .
We want to test the performance of CDEC and Hiero MOSES and choose the best .
PB MOSES is used as our benchmark .
The three results obtained on the development and test sets for the three baseline system and the system combination are shown in the Table 6 From the Table 6 we can see that on development set , CDEC performs the best , and its much better than MOSES 's two decoder , but on test set , Hiero MOSES and CDEC performs as well as each other , and they both performs better than PB Model .
System Combination
We also use a word-level combination strategy ( Rosti et al. , 2007 ) to combine the three translation hypotheses .
To combine these systems , we first use the Minimum Bayes - Risk ( MBR ) ( Kumar and Byrne , 2004 ) decoder to obtain the 5 best hypothesis as the alignment reference for the Confusion Network ( CN ) ( Mangu et al. , 2000 ) .
We then use IHMM ( He et al. , 2008 ) to choose the backbone build the CN and finally search for and generate the best translation .
We tune the system parameters on development set with Simple -Simplex algorithm .
The parameters for system weights are set equal .
Other parameters like language model , length penalty and combination coefficient are chosen when we see a good improvement on development set .
Conclusion
This paper presented a set of experiments conducted on Spanish -English , French -English and German- English language pairs .
For the Spanish - English pair , we have explored the use of linguistic information to select parallel data and use this as the training for SMT .
However , the comparison of the performance obtained using this method and the purely statistical one ( i.e. perplexity on word forms ) remains to be carried out .
Another open question regards the optimal size of the selected data .
As we have seen , minimum perplexity alone cannot be considered an optimal metric since using a larger set , even if it has higher perplexity , allowed us to obtain notably higher BLEU scores .
The question is then how to decide the optimal size of parallel data to select .
For the French - English language pair , we investigated the usefulness of the small in - domain parallel data compared to out - of- domain parallel data sub-sampling .
We show that with a sample containing ?
3.7 M sentence pairs extracted from the out-of- domain parallel data , it is not necessary to use the small domain-specific parallel data .
Further experiments are still required to determine the minimum sample size needed to outperform both the in-domain system and the combination of the two translation models .
Finally , for the German- English language pair , we presents our exploitation of long ordering problem .
We compared two hierarchical models with one phrase - based model , and we also use a system combination strategy to further improve the translation systems performance .
Table 1 : 1 Perplexities in data selection 2.3 ResultsTable2 presents the results obtained .
Note that these were obtained during development and thus the systems are tuned on WMT 's 2011 test set and tested on WMT 's 2012 test set .
All the systems share the same LM .
The first system ( no selection ) is trained with the whole NC and EU .
The second ( small ) and third ( big ) systems use as training data the whole NC and subsets of EU ( 5 % and 14 % , respectively ) , UN ( 2 % and 12 % , respectively ) and CC ( 50 % and 100 % , respectively ) , as shown in Table1 .
corpus size EU 5 % 14 % 1104.8 forms forms+nes 957.9 987.2 1058.7 1111.6 lem lem+nes 974.3 1005.5 1057.7 UN 2 % 877.1 969.6 866.6 962.2 12 % 1203.2 1130.9 1183.8 1131.6 CC 50 % 573.0 547.2 574.5 546.4 100 % 560.1 560.1 560.1 560.1
