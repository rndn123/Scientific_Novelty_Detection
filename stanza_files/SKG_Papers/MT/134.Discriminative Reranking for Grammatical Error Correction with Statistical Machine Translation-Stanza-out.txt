title
Discriminative Reranking for Grammatical Error Correction with Statistical Machine Translation
abstract
Research on grammatical error correction has received considerable attention .
For dealing with all types of errors , grammatical error correction methods that employ statistical machine translation ( SMT ) have been proposed in recent years .
An SMT system generates candidates with scores for all candidates and selects the sentence with the highest score as the correction result .
However , the 1 - best result of an SMT system is not always the best result .
Thus , we propose a reranking approach for grammatical error correction .
The reranking approach is used to re-score N-best results of the SMT and reorder the results .
Our experiments show that our reranking system using parts of speech and syntactic features improves performance and achieves state - of - theart quality , with an F 0.5 score of 40.0 .
Introduction Research on assisting second language learners has received considerable attention , especially regarding grammatical error correction of essays written by English as a Second Language ( ESL ) learners .
To address all types of errors , grammatical error correction methods that use statistical machine translation ( SMT ) have been proposed ( Brockett et al. , 2006 ; Mizumoto et al. , 2012 ; Buys and van der Merwe , 2013 ; Yuan and Felice , 2013 ; Felice et al. , 2014 ; Junczys -Dowmunt and Grundkiewicz , 2014 ) . SMTbased error correction systems have achieved rankings first and third in the CoNLL2014 Shared Task .
SMT systems generate many candidates of translation .
SMT systems generate scored candidates and select a sentence having the highest score as the translation result .
However , the 1 - best result of SMT system is not always the best result because the scoring is conducted only with local features .
In other words , N-best ( N > 1 ) results may be better than the 1 - best result .
Reranking approaches have been devised to solve the scoring problem .
Reranking is a method that re-scores N-best candidates of SMT and reorders the candidates by score .
Figure 1 shows a flow of reranking .
First , N- best results are obtained by a grammatical error correction system using SMT for a learner sentence ( A in Figure 1 ) .
A reranking sys-tem then re-scores the N-best results and reorders them ( B in Figure 1 ) .
In this study , we apply a discriminative reranking method to the task of grammatical error correction .
Syntactic information is not considered in the phrase - based SMT .
We show that using syntactic features in the reranking system can improve error correction performance .
Although reranking using only surface features is not effective for grammatical error correction , reranking using syntactic features improves the F 0.5 score .
Related Work for Reranking Reranking approaches have been proposed for common SMT tasks Carter and Monz , 2011 ; Li and Khudanpur , 2008 ; . first used a perceptron-like algorithm for reranking of common SMT tasks .
However , they used only a few features .
Li and Khudanpur ( 2008 ) proposed a reranking approach that uses a large-scale discriminative Ngram language model for common SMT tasks .
They extended the reranking method for automatic speech recognition ( Roark et al. , 2007 ) to SMT tasks .
The approach of Carter and Monz ( 2011 ) was similar to that of Li and Khudanpur ( 2008 ) , but they used additional syntactic features ( e.g. part of speech ( POS ) , parse tree ) for reranking of common SMT tasks .
The reranking approach has been used in grammatical error correction based on phrase - based SMT ( Felice et al. , 2014 ) .
However , their method uses only language model scores .
In the reranking step , the system can consider not only surface but also syntactic features such as those in the approach of Carter and Monz ( 2011 ) .
We use syntactic features in our reranking system .
Heafield et al. ( 2009 ) proposed a system combination method for machine translation that is similar to that of reranking .
System combination is a method that merges the outputs of multiple systems to produce an output that is better than each individual system .
applied this system combination to grammatical error correction .
They combined pipeline systems based on classification approaches and SMT systems .
Classifier - based systems use syntactic features as POS and dependency for error correction .
However , syntactic information 3 Why is Reranking Necessary ?
Grammatical error correction using SMT has the same problem as that of common SMT task : the 1best correction by the system is not always the best .
To prove this , we conducted a grammatical error correction experiment using SMT and calculated N-best oracle scores .
The oracle scores are calculated by selecting the correction candidate with the highest score from the N-best results for each sentence .
Table 1 shows oracle scores of a baseline grammatical error correction system using SMT 1 .
Althogh the F 0.5 score of the 1 - best output was 37.9 , the F 0.5 of the 10 - best oracle score was 64.3 .
The higher the value of N-best , the higher is the oracle score .
This result reveals that the 1 - best correction by a grammatical error correction system using SMT is not always the best .
Advantage of Reranking
Two advantages exist for using a reranking approach for grammatical error correction .
The first is that a reranking system can use POS and syntactic features unlike phrase - based SMT .
With some errors , the relation between distant words must be considered ( e.g. , article relation between a and dolls in the phrase a big Snoopy dolls ) .
The second advantage is that POS taggers and parsers can analyze error-corrected candidates more properly than they analyze erroneous sentences , which enables more accurate features to be obtained .
Thus , the fact that taggers for N-best corrected results work much better than for learner original sentences is promising .
Proposed Method
In this section , we explain our discriminative reranking method and features of reranking for grammati -
Discriminative Reranking Method
In this study , we use a discriminative reranking algorithm using perceptron which successfully exploits syntactic features for N-best reranking for common translation tasks ( Carter and Monz , 2011 ) .
Figure 2 shows the standard perceptron algorithm for reranking .
In this figure , T is the number of iterations for perceptron learning and N is the number of learner original sentences in the training corpus .
In addition , GEN ( x ) is the N-best list generated by a grammatical error correction system using SMT for an input sentence and ORACLE ( x i ) determines the best correction for each of the N-best lists according to the F 0.5 score .
Moreover w is the weight vector for features and ? is the feature vector for candidate sentences .
When selecting the sentence with the highest score from candidate sentences ( line 5 ) , if the selected sentence matches oracle sentence , then the algorithm proceeds to next sentence .
Otherwise , the weight vector is updated .
The disadvantage of perceptron is instability when training data are not linearly separable .
As a solution to this problem , an averaged perceptron algorithm was proposed ( Freund and Schapire , 1999 ) .
In this algorithm , weight vector w avg is defined as : w avg = 1 T T ? t=1 1 N N ? i=1 w i t ( 1 )
To select the best correction from N-best candidates , we use the following formula : S( z ) = ? ? 0 ( z ) + w ? ? ( z ) ( 2 ) where ? 0 ( z ) is the score calculated by the SMT system for each translation hypothesis .
This score is weighted by ? . Using ? 0 ( z ) as a feature in the perceptron algorithm is possible , but this may lead to 1 : w ? 0 2 : for t = 1 to T do 3 : for i = 1 to N do 4 : y i ? ORACLE ( x i ) 5 : z i ? argmax x?GEN ( x i ) ? ( z ) ? w 6 : if z i ?
= y i then 7 : w ? w + ? ( y i ) ? ? ( z i ) 8 : end if 9 : end for 10 : end for 11 : return w under-training ( Sutton et al. , 2006 ) .
We select the value for ? with the highest F 0.5 score by changing ? from 0 to 100 in 0.1 increments on the development data .
Features of Discriminative Reranking for Grammatical Error Correction
In this study , we use the features used in Carter and Monz ( 2011 ) as well as our new features of POS and dependency .
We use the features extracted from the following sequences : POS tag , shallow parse tag , and shallow parse tag plus POS tag sequences ( Carter and Monz , 2011 ) .
From these sequences , features are extracted based on the following three definitions : 1 . ( t i?2 t i?1 t i ) , ( t i?1 t i ) , ( t i w i ) 2 . ( t i?2 t i?1 w i ) 3 . ( t i?2 w i?2 t i?1 w i?1 t i w i ) , ( t i?2 t i?1 w i?1 t i w i ) , ( t t?1 w i?1 t i w i ) , ( t i?1 t i w i )
Here , w i is a word at position i and t i is a tag ( POS or shallow parse tag ) at position i .
Table 2 shows our new features .
For the " POS - function N-gram " feature , if words are con -
Experiments of Reranking
We conducted experiments on grammatical error correction to observe the effect of discriminative reranking and our syntactic features .
Experimental Settings
We used phrase - based SMT which many previous studies used for grammatical error correction for a baseline system .
We used cicada 0.3.5 2 for the machine translation tool and KenLM 3 as the language modeling tool .
We used ZMERT 4 as the parameter tuning tool and implemented the averaged perceptron for reranking .
The translation model was trained on the Lang-8 Learner Corpora v2.0 .
We extracted English essays that were written by ESL learners and cleaned noise with the method proposed in ( Mizumoto et al. , 2011 ) .
From the results , we obtained 1,069,127 sentence pairs .
We used a 5 - gram language model built on the " Associated Press Worldstream English Service " from English Gigaword corpus and NU - CLE 3.2 ( Dahlmeier et al. , 2013 ) .
We used these two language models as separate feature functions in the SMT system .
For training data of reranking , Lang-8 Learner Corpora was split into 10 parts and each part was corrected by a grammatical error correction system trained on the other nine parts .
We selected 10 as N for N-best reranking .
PukWaC corpus ( Baroni et al. , 2009 ) was used for constructing our " Web dependency N-gram " feature .
We use Stanford Parser 3.2.0 5 as a dependency parser .
CoNLL - 2013 test set were split into 700 sentences for parameter tuning of SMT and 681 sentences for tuning parameter beta .
CoNLL - 2014 test set , 1,312 sentences were used for evaluation .
We used M2 Scorer as an evaluation tool .
This scorer calculates precision , recall , and F 0.5 scores .
We used F 0.5 as a tuning metric .
In addition , we used GLEU ( Napoles et al. , 2015 ) as evaluation metrics .
Experimental Results and Discussion Table 3 shows the experimental results .
We used the 1 - best result of the SMT correction system and reranking by probability of the large N-gram language model ( Felice et al. , 2014 ) as baseline systems .
In addition , we compared the systems that are ranked first ( CAMB ) and second ( CUUI ) ( Felice et al. , 2014 ; Rozovskaya et al. , 2014 ) in CoNLL2014 Shared Task .
The discriminative reranking system with our features achieved the best F 0.5 score .
The difference between the results of baseline and reranking using our features was statistically significant ( p < 0.01 ) .
Because a large N-gram language model was adopted for reranking , recall increased considerably but precision declined .
This result is extremely similar to that of the CAMB system , which is an SMT - based error correction system that reranks by using a large N-gram language model .
When we compare the reranking system using our features to CUUI , our system is better in all metrics .
When we use the discriminative reranking with our features , both precision and recall increase .
In the experimental results of system combination , recall increases but precision declines with respect to original SMT results .
In addition , precision increases but recall declines with respect to pipeline results .
The reranking that employed all features generated a lower F 0.5 score than when only our features were used .
One reason for this is that the roles of features overlap .
These experiments revealed that reranking is effective in grammatical error correction tasks and that POS and syntactic features are important .
Conclusion
We proposed a reranking approach to grammatical error correction using phrase - based SMT .
Our system achieved F 0.5 score of 40.0 ( an increase of 2.1 points from that of the baseline system ) on the CoNLL2014 Shared Task test set .
We showed that POS and dependency features are effective for the reranking of grammatical error correction .
In future work , we will use the adaptive regularization of weight vectors ( AROW ) algorithm ( Crammer et al. , 2009 ) instead of the averaged perceptron .
In addition , we will apply the pairwise approach to ranking ( Herbrich et al. , 1999 ) used in information retrieval to rerank of grammatical error correction .
Figure 1 : 1 Figure 1 : Flow of reranking .
