title
Using Syntactic Head Information in Hierarchical Phrase -Based Translation
abstract
Chiang 's hierarchical phrase- based ( HPB ) translation model advances the state - of - the - art in statistical machine translation by expanding conventional phrases to hierarchical phrases - phrases that contain sub-phrases .
However , the original HPB model is prone to overgeneration due to lack of linguistic knowledge : the grammar may suggest more derivations than appropriate , many of which may lead to ungrammatical translations .
On the other hand , limitations of glue grammar rules in the original HPB model may actually prevent systems from considering some reasonable derivations .
This paper presents a simple but effective translation model , called the Head-Driven HPB ( HD - HPB ) model , which incorporates head information in translation rules to better capture syntax - driven information in a derivation .
In addition , unlike the original glue rules , the HD - HPB model allows improved reordering between any two neighboring non-terminals to explore a larger reordering search space .
An extensive set of experiments on Chinese - English translation on four NIST MT test sets , using both a small and a large training set , show that our HD - HPB model consistently and statistically significantly outperforms Chiang 's model as well as a source side SAMT - style model .
Introduction Chiang 's hierarchical phrase- based ( HPB ) translation model utilizes synchronous context free grammar ( SCFG ) for translation derivation ( Chiang , 2005 ; Chiang , 2007 ) and has been widely adopted in statistical machine translation ( SMT ) .
Typically , such models define two types of translation rules : hierarchical ( translation ) rules which consist of both terminals and non-terminals , and glue ( grammar ) rules which combine translated phrases in a monotone fashion .
However , due to lack of linguistic knowledge , Chiang 's HPB model contains only one type of non-terminal symbol X , often making it difficult to select the most appropriate translation rules .
1
One important research question is therefore how to refine the non-terminal category X using linguistically motivated information : Zollmann and Venugopal ( 2006 ) ( SAMT ) e.g. use ( partial ) syntactic categories derived from CFG trees while Zollmann and Vogel ( 2011 ) use word tags , generated by either POS analysis or unsupervised word class induction .
Almaghout et al . ( 2011 ) employ CCGbased supertags .
Mylonakis and Sima'an ( 2011 ) use linguistic information of various granularities such as Phrase - Pair , Constituent , Concatenation of Constituents , and Partial Constituents , where applicable .
By contrast , and inspired by previous work in parsing ( Charniak , 2000 ; Collins , 2003 ) , our Head-Driven HPB ( HD - HPB ) model is based on the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment , as in HPB .
We identify heads using linguistically motivated dependency parsing , and use head information to refine X. Furthermore , Chiang 's HPB model suffers from limited phrase reordering by combining translated zuotian chuxi huiyi attended a meeting yesterday phrases in a monotonic way with glue rules .
In addition , once a glue rule is adopted , it requires all rules above it to be glue rules .
For example , given a Chinese-English sentence pair ( ?
?/zuotian 1 ?/chuxi 2 ?/huiyi 3 , Attended 2 a 3 meeting 3 yesterday 1 ) , a correct translation is impossible via HPB derivations in Figure 1 . For the derivation in Figure 1 ( a ) , swap reordering in the glue rule ( i.e. , S 1 ? S 2 X 2 , X 2 S 2 ) is disallowed and , even if such a swap reordering is available , it lacks useful information for rule selection .
For the derivation in Figure 1 ( b ) , the combination of two non-terminals ( i.e. , X 2 ? X 3 X 4 , X 3 X 4 ) is disallowed to form a new non-terminal which in turn is a sub-phrase of a hierarchical rule .
These limitations prevent traditional HPB systems from even considering some reasonable derivations .
X 4 X 3 X 3 X 4 X 2 X 1 X 2 S 2 X 1 S 1 S 1 X 1 To tackle the problem of glue rules , He ( 2010 ) extended the HPB model by using bracketing transduction grammar ( Wu , 1996 ) instead of the monotone glue rules , and trained an extra classifier for glue rules to predict reorderings of neighboring phrases .
By contrast , our HD - HPB model refines the nonterminal symbol X with syntactic head information and provides flexible reordering rules , including swap , which can mix freely with hierarchical translation rules for better interleaving of translation and reordering in translation derivations .
Different from the soft constraint modeling adopted in ( Chan et al. , 2007 ; Marton and Resnik , 2008 ; Shen et al. , 2009 ; He et al. , 2010 ; Huang et al. , 2010 ; Gao et al. , 2011 ) , our approach encodes syntactic information in translation rules .
However , the two approaches are not mutually exclusive , as we could also include a set of syntax - driven features into our translation model .
Our approach maintains the advantages of Chiang 's HPB model while at the same time incorporating head information and flexible reordering in a derivation in a natural way .
Experiments on Chinese-English translation using four NIST MT test sets show that our HD - HPB model significantly outperforms Chiang 's HPB as well as a SAMT - style refined version of HPB .
The paper is structured as follows : Section 2 describes the synchronous context-free grammar ( SCFG ) in our HD - HPB translation model .
Section 3 presents our model and features , followed by the decoding algorithm in Section 4 .
We report experimental results in Section 5 .
Finally we conclude in Section 6 .
Head -Driven HPB Translation Model Like Chiang ( 2005 ) and Chiang ( 2007 ) , our HD - HPB translation model adopts a synchronous context free grammar , a rewriting system which generates source and target side string pairs simultaneously using a context-free grammar .
In particular , each synchronous rule rewrites a non-terminal into a pair of strings , s and t , where s ( or t ) contains terminals and non-terminals from the source ( or target ) language and there is a one- to - one correspondence between the non-terminal symbols on both sides .
A good and informative inventory of non-terminal symbols is always important , especially for a successful SCFG - based translation model .
Instead of collapsing all non-terminals in the source language into a single symbol X as in Chiang ( 2007 ) , ideally non-terminals should capture important information of the word sequences they cover to be able to properly discriminate between similar and different word sequences during translation .
This motivates our approach to provide syntax -enriched non-terminal symbols .
Given a word sequence f i j from position i to position j , we refine the non-terminal symbol X to reflect some of the internal syntactic structure of the word sequence covered by X .
A correct translation rule selection therefore not only maps terminals into terminals , but is both constrained and guided by syntactic information in the non-terminals .
At the same time , it is not clear whether an " ideal " approach that captures a full syntactic analysis of the string fragment covered by a non-terminal is feasible : the diversity of syntactic structures could make training impossible and lead to serious data sparseness issues .
As a compromise , given a word sequence f i j , we first find heads and then concatenate the POS tags of these heads as f i j 's non-terminal symbol .
2
Our approach is guided by the intuition that linguistic heads provide important information about a constituent or distributionally defined fragment , as in HPB .
Specifically , we adopt dependency structure to derive heads , which are defined as : Definition 1 .
For word sequence f i j , word f k ( i ? k ? j ) is regarded as a head if it is dominated by a word outside of this sequence .
Note that this definition ( i ) allows for a word sequence to have one or more heads ( largely due to the fact that a word sequence is not necessarily linguistically constrained ) and ( ii ) ensures that heads are always the highest heads in the sequence from a dependency structure perspective .
For example , the word sequence ouzhou baguo lianming in Figure 2 has two heads ( i.e. , baguo and lianming , ouzhou is not a head of this sequence since its headword baguo falls within this sequence ) and the non-terminal corresponding to the sequence is thus labeled as NN - AD .
It is worth noting that in this paper we only refine non-terminal X on the source side to headinformed ones , while still using X on the target side .
In our HD - HPB model , the SCFG is defined as a tuple ? , N , ? , ? , , where ? is a set of source language terminals , N is a set of non-terminals categorizing terminals in ? , ? is a set of target language terminals , ? is a set of non-terminals categorizing terminals in ? , and is a set of translation rules .
A rule ? in is in the form of P s ? s , P t ? t , ? , where : ? P s ?
N and P t ? ? ; ? s ? (? ? N ) + and t ? (? ? ? ) + ? ? is a bijection between non-terminals in s and t.
According to the occurrence of terminals in s and t , we group the rules in the HD - HPB model into two categories : head - driven hierarchical rules ( HD - HRs ) and non-terminal reordering rules ( NRRs ) , where the former have at least one terminal on both source and target sides and the later have no terminals .
For rule extraction , we first identify initial phrase pairs on word-aligned sentence pairs by using the same criterion as most phrase - based translation models ( Och and Ney , 2004 ) and Chiang 's HPB model ( Chiang , 2005 ; Chiang , 2007 ) .
We extract HD - HRs and NRRs based on initial phrase pairs , respectively .
HD - HRs : Head-Driven Hierarchical Rules
As mentioned , a HD - HR has at least one terminal on both source and target sides .
This is the same as the hierarchical rules defined in Chiang 's HPB model ( Chiang , 2007 ) , except that we use head POSinformed non-terminal symbols in the source language .
We look for initial phrase pairs that contain other phrases and then replace sub-phrases with their corresponding non-terminal symbols .
Given the word alignment as shown in Figure 2 , Table 1 demonstrates the difference between hierarchical rules in Chiang ( 2007 ) and HD - HRs defined here .
dui yi celie 2 , support America 's 1 stand 2 against Iraq X?X 1 dui yi X 2 , X 1 X 2 against Iraq VV?VV -NR 1 dui yi NN 2 , X?X 1 X 2 against Iraq Table 1 : Comparison of hierarchical rules in Chiang ( 2007 ) and HD - HRs .
Indexed underlines indicate sub-phrases and corresponding non-terminal symbols .
The non-terminals in HD - HRs ( e.g. , NN , VV , VV - NR ) capture the head ( s ) POS tags of the corresponding word sequence in the source language .
Similar to Chiang 's HPB model , our HD - HPB model will result in a large number of rules causing problems in decoding .
To alleviate these problems , we filter our HD - HRs according to the same constraints as described in Chiang ( 2007 ) .
Moreover , we discard rules that have non-terminals with more than four heads .
NRRs : Non-terminal Reordering Rules NRRs are translation rules without terminals .
Given an initial phrase pair f i j , e i * j * , we check all other initial phrase pairs f k l , e k * l * which satisfy k = j + 1 ( i.e. , phrase f k l is located immediately to the right of f i j in the source language ) .
For their target side translations , there are four possible positional relationships : monotone , discontinuous monotone , swap , and discontinuous swap .
In order to differentiate non-terminals from those in the target language ( i.e. , X ) , we use Y as a variable for non-terminals in the source language , and obtain four types of NRRs : ? Monotone Y ? Y 1 Y 2 , X ? X 1 X 2 ; ? Discontinuous monotone Y ? Y 1 Y 2 , X ? X 1 . . . X 2 ; ? Swap Y ? Y 1 Y 2 , X ? X 2 X 1 ; ? Discontinuous swap Y ? Y 1 Y 2 , X ? X 2 . . . X 1 . For example in Figure 2 , the NRR for initial phrase pairs zhichi meiguo , support America 's and dui yi celie , stand against Iraq would be V V ? V V -N R 1 N N 2 , X ? X 1 X 2 . Merging two neighboring non-terminals into a single non-terminal , NRRs enable the translation model to explore a wider search space .
During training , we extract four types of NRRs and calculate probabilities for each type .
To speed up decoding , we currently ( i ) only use monotone and swap NRRs and ( ii ) limit the number of non-terminals in a NRR to 2 .
Log-linear Model and Features Following Och and Ney ( 2002 ) , we depart from the traditional noisy - channel approach and use a general log-linear model .
Let d be a derivation from sentence f in the source language to sentence e in the target language .
The probability of d is defined as : P ( d ) ? i ? i ( d ) ? i ( 1 ) where ?
i are features defined on derivations and ?
i are feature weights .
In particular , we use a feature set analogous to the default feature set of Chiang ( 2007 ) , which includes : ? P hd-hr ( t|s ) and P hd-hr ( s|t ) , translation probabilities for HD - HRs ; ? P lex ( t|s ) and P lex ( s|t ) , lexical translation probabilities for HD - HRs ; ?
P ty hd-hr = exp ( ? 1 ) , rule penalty for HD - HRs ; ? P nrr ( t | s ) , translation probability for NRRs ; ?
P ty nrr = exp ( ? 1 ) , rule penalty for NRRs ; ? P lm ( e ) , language model ; ?
P ty word ( e ) = exp ( ?|e| ) , word penalty .
Algorithm 1 : Decoding Algorithm Input : Sentence f 1 n in the source language Dependency structure of f 1 n HD - HR rule set HDHR NRR rule set NRR Initial phrase length K Output : Best derivation d * 1 . set chart [ i , j] =NIL ( 1 ? i ? j ? n ) ; 2 . for l from 1 to n do 3 . for all i , j such that j ?
i = l do 4 . if l ?
K do 5 . for all derivations d derived from HDHR spanning from i to j do 6 . add d into chart [ i , j] 7 . for all derivations d derived from NRR spanning from i to j do 8 . add d into chart [ i , j] 9 . set d * as the top derivation of chart [ 1 , n] 10.return d *
It is worth pointing out that we define translation probabilities for NRRs only for the direction from source language to target language , although translation probabilities for HD - HRs are defined for both directions .
This is mostly due to the fact that a NRR excludes terminals and has only two options on the target side ( i.e. , either X ? X 1 X 2 or X ? X 2 X 1 ) .
Decoding
Our decoder is based on CKY - style chart parsing with beam search .
Given an input sentence f , it finds a sentence e in the target language derived from the best derivation d * among all possible derivations D : d * = arg max d?D P ( D ) ( 2 ) Algorithm 1 presents the decoding process .
Given a source sentence , it searches for the best derivation bottom - up .
For a source span [ i , j ] , it applies both types of HD - HRs and NRRs .
However , HD - HRs are only applied to generate derivations spanning no more than K words - the initial phrase length limit used in training to extract HD - HRswhile NRRs are applied to derivations spanning any length .
Unlike in Chiang ( 2007 ) , it is possible for a non-terminal generated by a NRR to be included afterwards by a HD - HR or another NRR .
Similar to Chiang ( 2007 ) in generating k-best derivations from i to j , we make use of cube pruning ( Huang and Chiang , 2005 ) with an integrated language model for each derivation .
Experiments
We evaluate the performance of our HD - HPB model and compare it with our implementation of Chiang 's HPB model ( Chiang , 2007 ) , a source- side SAMTstyle refined version of HPB ( SAMT - HPB ) , and the Moses implementation of HPB .
For fair comparison , we adopt the same parameter settings for HD - HPB , HPB and SAMT - HPB systems , including initial phrase length ( as 10 ) in training , the maximum number of non-terminals ( as 2 ) in translation rules , maximum number of non-terminals plus terminals ( as 5 ) on the source , prohibition of non-terminals to be adjacent on the source , beam threshold ? ( as 10 ?5 ) ( to discard derivations with a score worse than ? times the best score in the same chart cell ) , beam size b ( as 200 ) ( i.e. each chart cell contains at most b derivations ) .
For Moses HPB , we use " grow-diagfinal - and " to obtain symmetric word alignments , 10 for the maximum phrase length , and the recommended default values for all other parameters .
Experimental Settings
To examine the efficacy of our approach on training datasets of different scales , we first train translation models on a small -sized corpus , and then scale to a larger one .
We use the 2002 NIST MT evaluation test data ( 878 sentence pairs ) as the development data , and the , 2004 , 2005 , 2006 - news NIST MT evaluation test data ( 919 , 1788 , 1082 sentence pairs , respectively ) as the test data .
To find heads , we parse the source sentences with the Berkeley Parser 3 ( Petrov and Klein , 2007 ) trained on Chinese TreeBank 6.0 and use the Penn2 Malt toolkit 4 to obtain dependency structures .
We obtain the word alignments by running GIZA ++ ( Och and Ney , 2000 ) on the corpus in both directions , applying " grow-diag-final - and " refinement ( Koehn et al. , 2003 ) .
We use the SRI language modeling toolkit to train a 5 - gram language model on the Xinhua portion of the Gigaword corpus and standard MERT ( Och , 2003 ) to tune the feature weights on the development data .
For evaluation , the NIST BLEU script ( version 12 ) with the default settings is used to calculate the NIST and the BLEU scores , which measures caseinsensitive matching of n-grams with n up to 4 .
To test whether a performance difference is statistically significant , we conduct significance tests following the paired bootstrap approach ( Koehn , 2004 ) .
In this paper , '* * ' and '* ' denote p-values less than 0.01 and in- between [ 0.01 , 0.05 ) , respectively .
Results on Small Data
To test the HD - HPB models , we firstly carried out experiments using the FBIS corpus as training data , which contains ?240 K sentence pairs .
Table 2 lists the rule table sizes .
The full rule table size ( including HD - HRs and NRRs ) of our HD - HPB model is about 1.5 times that of Chiang's , largely due to refining the non-terminal symbol X in Chiang 's model into head - informed in our model .
It is also unsurprising , that the test set-filtered rule table size of our model is only about 0.8 times that of Chiang's : this is due to the fact that some of the refined translation rule patterns required by the test set are unattested in the training data .
Furthermore , the rule table size of NRRs is much smaller than that of HD - HRs since a NRR contains only two non-terminals .
Table 3 lists the translation performance with NIST and BLEU scores .
Note that our re-implementation of Chiang 's original HPB model performs on a par with Moses HPB .
Table 3 shows that our HD - HPB model significantly outperforms Chiang 's HPB model with an average improvement of 1.32 in BLEU and 0.16 in NIST ( and similar improvements over Moses HPB ) .
Although HD - HPB has small size of phrase tables compared to HPB , it still consumes more time in decoding ( e.g. , 15.1 vs. 11.0 ) , mostly due to the flexible reordering of NRRs .
Results on Large Data
We also conduct experiments on larger training data with ?1.5 M sentence pairs from the LDC dataset .
5 Table 4 lists the rule table sizes and Table 5 presents translation performance with NIST and BLEU scores .
It shows that our HD - HPB model consistently outperforms Chiang 's HPB model with an average improvement of 1.91 in BLEU and 0.35 in NIST ( similar for Moses HPB ) .
Compared to the improvement achieved on the small data , it is encouraging to see that our HD - HPB model benefits more from larger training data with little adverse effect on decoding time which increases only slightly from 15.1 to 16.6 seconds per sentence .
Comparison with SAMT -HPB Comparing the performance of SAMT - HPB with regular HPB in Table 3 and Table 5 , it is interesting to see that in general the SAMT - style approach leads to a deterioration of translation performance for the small training set ( e.g. , 30.09 for SAMT - HPB vs. 30.64 for HPB ) while it comes into its own for the large training set ( e.g. , 33.54 for SAMT - HPB vs. 32.95 for HPB ) , indicating that the SAMT - style approach is more prone to data sparseness than HPB ( or , indeed , HD - HPB ) .
Comparing the performance of SAMT - HPB with HD - HPB , shows that our head- driven non-terminal refining approach consistently outperforms the SAMT - style approach on an extensive set of experiments ( for each test set p < 0.01 ) , indicating that head information is more effective than ( partial ) CFG categories .
To make the comparison fair , it is important to note that our implementation of source- side SAMT - HPB includes the same sophisticated non-terminal re-ordering NRR rules as HD - HPB ( Section 2.2 ) .
Thus the performance differences reported here are not due to different reordering capabilities , but to the discriminative impact of the head information in HD - HPB over SAMT - style annotation .
Taking lianming zhichi in Figure 2 as an example , HD - HPB labels the span VV , as lianming is dominated by zhichi , effecively ignoring lianming in the translation rule , while the SAMT label is ADVP : AD +VV 6 which is more susceptible to data sparsity ( Table 2 and Table 4 ) .
In addition , SAMT resorts to X if a text span fails to satisify pre-defined categories .
Examining initial phrases extracted from the SAMT training data shows that 28 % of them are labeled as X. Finally , for Chinese syntactic analy - sis , dependency structure is more reliable than constituency structure .
Moreover , SAMT - HPB takes more time in decoding than HD - HPB due to larger phrase tables .
Discussion
Individual Contribution of HD - HRs and NRRs Examining translation output shows that on average each sentence employs 16.6/5.2 HD - HRs / NRRs in our HD - HPB model , compared to 15.9/3.6 hierarchical rules / glue rules in Chiang 's model , providing further indication of the importance of NRRs in translation .
In order to separate out the individual contributions of the novel HD - HRs and NRRs , we carry out an additional experiment ( HD - HR + Glue ) using HD - HRs with monotonic glue rules only ( adjusted to refined rule labels , but effectively switching off the extra reordering power of full NRRs ) both on the small and the large datasets , with interesting results : Table 3 ( HD - HR + Glue ) shows that for the small training set most of the improvement of our full HD - HPB model comes from the NRRs , as RR + Glue performs on the same level as Chiang 's original and Moses HPB ( the differences are not statistically significant ) , perhaps indicating sparseness for the refined HD - HRs given the small training set .
Table 5 shows that for the large training set , HD - HRs come into their own : on average more than half of the improvement over HPB ( Chiang and Moses ) comes from the refined HD - HRs , the rest from NRRs .
It is not surprising that compared to the others HD - HR + Glue takes much less time in decoding .
This is due to the fact that 1 ) compared to HPB , the refined translation rule patterns on the source side have fewer entries in phrase table ; 2 ) compared to HD- HPB , HD - HR + Glue switches off the extra reordering of NRRs .
The decoding time for HD - HPB and HD - HR + Glue suggests that NRRs are more than doubling the time required to decode .
Different Head Label Sets
Examining initial phrases extracted from the large size training data shows that there are 63 K types of refined non-terminals with respect to 33 types of POS tags .
Considering the sparseness in translation rules caused by this comparatively detained POS tag set , we carry out an experiment with a reduced set of non-terminal types by using a less granular POS tag set ( C- HPB ) .
Moreover , due to the fact that concatenation of POS tags of heads mostly captures internal structure of a text span , it is interesting to examine the effect of other syntactic labels , in particular dependency labels , to try to better capture the impact of the external context on the text span .
To this end , we replace the POS tag of head with its incoming dependency label ( DL - HPB ) , or the combination of ( the original fine- grained ) POS tag and its dependency label ( POS - DL - HPB ) .
For C-HPB we use the coarse POS tag set obtained by grouping the 33 types of Chinese POS tags into 11 types following Xia ( 2000 ) .
For example , we generalize all verbal tags ( e.g. , VA , VC , VE , and VV ) and all nominal tags ( e.g. , NR , NT , and NN ) into Verb and Noun , respectively .
We use the dependency labels in Penn2 Malt which defines 9 types of dependency labels for Chinese , including AMOD , DEP , NMOD , P , PMOD , ROOT , SBAR , VC , and VMOD .
7 Table 6 shows the results trained on large data .
Although the number of non-terminal types decreased sharply from 63 K to 3 K , using the coarse POS tag set in C-HPB surprisingly lowers the performance with 1.1 BLEU scores on average ( e.g. , 33.75 vs. 34.86 ) , indicating that grouping POS tags using simple linguistic rules is inappropriate for HD - HPB .
We still believe that this initial negative finding should be supplemented by future work on groupping POS tags using machine learning techniques considering contextual information .
Table 6 also shows that replacing POS tags of heads with their dependency labels ( DL - HPB ) substantially lowers the average performance from 34.86 on BLEU score to 32.54 , probably due to the very coarse granularity of the dependency labels used .
In addition , replacing non-terminal label with more refined tags ( e.g. , combination of original POS tag and dependency label ) also lowers translation performance ( POS - DL - HPB ) .
Further experiments with more fine- grained dependency labels are required .
Table 6 : BLEU ( % ) scores of models trained on large data .
Encoding Full Dependency Relations in Translation Rule Xie et al. ( 2011 ) present a dependency - to-string translation model with a complete dependency structure on the source side and a moderate average improvement of 0.46 BLEU over the HPB baseline .
By contrast , in our HD - HPB approach , dependency information is used to identify heads in the strings covered by non-terminals in HD - HR rules , and to refine non-terminal labels accordingly , with an average improvement of 1.91 in BLEU over the HPB baseline ( when trained on the large data ) .
This raises the question whether and to what extent complete ( unlabeled ) dependency information between the string and the heads in head - labeled non-terminal parts of the source side of SCFGs in HD - HPB can further improve results .
Given the source side of a translation rule ( either HD - HR or NRR ) , say P s ? s 1 . . . s m ( where each s i is either a terminal or a head POS in a refined non-terminal ) , in a further set of experiments we keep the full unlabeled dependency relations be-tween s 1 . . . s m so as to capture contextual syntactic information in translation rules .
For example , on the source side of Figure 3 ( b ) where VV - NR maps into words zhichi and meiguo while NN maps into word celie , we keep the full unlabeled dependency relations among words { zhichi , meiguo , dui , yi , celie} .
HD -DEP-HPB ( Table 6 ) augments translation rules in HD - HPB with full dependency relations on the source side .
This further boosts the performance by 0.35 BLEU scores on average over HD - HPB and outperforms the HPB baseline by 2.26 BLEU scores on average .
Error Analysis
We carried out a manual error analysis comparing the outputs of our HD - HPB system with those of Chiang 's ( both trained on the large data ) .
We observe that improved BLEU score often correspond to better topological ordering of phrases in the hierarchical structure of the source side , with a direct impact on which words in a source sentence should be translated first , and which later .
As ungrammatical translations are often due to inappropriate topological orderings of phrases in the hierarchical structure , guiding the translation through appropriate topological ordering should improve translation quality .
To give an example , consider the following input sentence from the 04 NIST MT test data and its two translation results : ? Input : ? 0 ? 1 ? 2 ? 3 ? 4 ? 5 ? ? 6 ? 7 ? 8 ? 9 ? HPB : chinese delegation to us dollar purchase of more high technology equipment ?
HD - HPB : chinese delegation went to the united states to buy more us high - tech equipment Figure 4 demonstrates the topological orderings in the two hierarchical structures .
In addition to disfluency and some grammar errors ( e.g. , a main verb is missing ) , the basic HPB system also makes mistakes in reordering ( e.g. , ? 4 ? 5 ? 6 translated as dollar purchase of more ) .
The poor translation quality , unsurprisingly , is caused by inappropriate topological ordering ( Figure 4 ( a ) ) .
By comparison , the topological ordering reflected in the hierarchical structure of our HD - HPB model better respects syntactic structure ( Figure 4 ( b ) ) .
Let ? 0 ? 1 ? 2 ? 3 ? 4 ? 5 ? 6 ? 7 ? 8 ? 9 X [ 4 - 4 ] X [ 6 - 6 ] X [ 4 - 6 ] X [ 3 - 7 ] X [ 3 - 8 ] X [ 2 - 9 ] X [ 1 - 9 ] X [ 0 - 9 ] S [ 0 - 9 ] ( a ) .
Topological orderings of phrases in Chiang 's HPB . ( b ) .
Improved topological orderings of phrases in HD - HPB .
1 . S [ 0 - 9 ] ? X [ 0 - 9 ] , X [ 0 - 9 ] 2 . X [ 0 - 9 ] ? ? [ 0 - 0 ] X [ 1 - 9 ] , chinese X [ 1 - 9 ] 3 . X [ 1 - 9 ] ? ? [ 1 - 1 ] X [ 2 - 9 ] , delegation X [ 2 - 9 ] 4 . X [ 2 - 9 ] ? ? [ 2 - 2 ] X [ 3 - 8 ] ? [ 9 - 9 ] , to X [ 3 - 8 ] equipment 5 . X [ 3 - 8 ] ? X [ 3 - 7 ] ? , X [ 3 - 7 ] technology 6 . X [ 3 - 7 ] ? ? [ 3 - 3 ] X [ 4 - 6 ] ? [ 7 - 7 ] , us X [ 4 - 6 ] high 7 . X [ 4 - 6 ] ? X [ 4 - 4 ] ? [ 5 - 5 ] X [ 6 - 6 ] , X [ 6 - 6 ] X [ 4 - 4 ] of more 8 . X [ 4 - 4 ] ? ? [ 4 - 4 ] , purchase 9 . X [ 6 - 6 ] ? ? [ 6 - 6 ] , dollar 1 . VV [ 0 - 9 ] ? NN [ 0 - 1 ] VV [ 2 - 9 ] , X ? X [ 0 - 1 ] X [ 2 - 9 ] 2 . NN [ 0 - 1 ] ? ?[ 0 -0 ] NN [ 1 - 1 ] , X ? chinese X [ 1 - 1 ] 3 . NN [ 1 - 1 ] ? ? [ 1 - 1 ] , X ? delegation 4 . VV [ 2- 9 ] ? ? [ 2 - 2 ] ? [ 3 - 3 ] VV [ 4 - 9 ] , X ? went to the united states to X [ 4 - 9 ]
5 . VV us refer to the HD - HPB hierarchical structure on the source side as translation parse tree and to the treebank - based parser derived tree as syntactic parse tree from which we obtain unlabeled dependency structure .
Examining the translation parse trees of our HD - HPB model shows that phrases with 1/2/3/4 heads account for 64.9%/23.1%/8.8%/3.2 % , respectively .
Compared to 37.9 % of the phrases in the translation parse trees of the HPB model , 43.2 % of the phrases of our HD - HPB model correspond to a linguistically motivated constituent in the syntactic parse tree with exactly the same text span .
In sum , therefore , instead of simply enforcing hard linguistic constraints imposed by a full syntactic parse structure , our model opts for a successful mix of linguistically motivated and combinatorial ( matching subphrases in HPB ) constraints .
Conclusion
In this paper , we present a head-driven hierarchical phrase - based translation model , which adopts head information ( derived through unlabeled dependency analysis ) in the definition of non-terminals to better differentiate among translation rules .
In addition , improved and better integrated reordering rules allow better reordering between consecutive non-terminals through exploration of a larger search space in the derivation .
Our model maintains the strengths of Chiang 's HPB model while at the same time it addresses the over-generation problem caused by using a uniform non-terminal symbol .
Experimental results on Chinese-English translation across a wide range of training and test sets demonstrate significant and consistent improvements of our HD - HPB model over Chiang 's HPB model as well as over a source side version of the SAMT - style model .
Currently , we only consider head information in a word sequence .
In the future work , we will exploit more syntactic and semantic information to systematically and automatically define the inventory of non-terminals ( in source and target ) .
For example , for a non-terminal symbol VV , we believe it will benefit translation if we use fine- grained dependency labels ( subject , object etc. ) used to link it to its governing head elsewhere in the translation rule .
Figure 1 : 1 Figure 1 : Example of derivations disallowed in Chiang 's HPB model .
The rules with dotted lines are not covered in Chiang 's model .
