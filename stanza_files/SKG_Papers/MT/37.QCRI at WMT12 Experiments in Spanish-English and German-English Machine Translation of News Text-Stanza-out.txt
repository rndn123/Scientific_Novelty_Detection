title
QCRI at WMT12 : Experiments in Spanish -English and German-English Machine Translation of News Text
abstract
We describe the systems developed by the team of the Qatar Computing Research Institute for the WMT12 Shared Translation Task .
We used a phrase - based statistical machine translation model with several non-standard settings , most notably tuning data selection and phrase table combination .
The evaluation results show that we rank second in BLEU and TER for Spanish - English , and in the top tier for German - English .
Introduction
The team of the Qatar Computing Research Institute ( QCRI ) participated in the Shared Translation Task of WMT12 for two language pairs : 1 Spanish -English and German-English .
We used the state- ofthe - art phrase - based model ( Koehn et al. , 2003 ) for statistical machine translation ( SMT ) with several non-standard settings , e.g. , data selection and phrase table combination .
The evaluation results show that we rank second in BLEU ( Papineni et al. , 2002 ) and TER ( Snover et al. , 2006 ) for Spanish - English , and in the top tier for German- English .
In Section 2 , we describe the parameters of our baseline system and the non-standard settings we experimented with .
In Section 3 , we discuss our primary and secondary submissions for the two language pairs .
Finally , in Section 4 , we provide a short summary .
1 The WMT12 organizers invited systems translating between English and four other European languages , in both directions : French , Spanish , German , and Czech .
However , we only participated in Spanish ?
English and German ?
English .
System Description Below , in Section 2.1 , we first describe our initial configuration ; then , we discuss our incremental improvements .
We explored several non-standard settings and extensions and we evaluated their impact with respect to different baselines .
These baselines are denoted in the tables below by a # number that corresponds to systems in Figures 1 for Spanish - English and in Figure 2 for German - English .
We report case insensitive BLEU calculated on the news2011 testing data using the NIST scoring tool v.11b .
Initial Configuration
Our baseline system can be summarized as follows : ?
Training : News Commentary + Europarl training bi-texts ; ?
Tuning : news2010 ; ? Testing : news2011 ; ? Tokenization : splitting words containing a dash , e.g. , first-order becomes first @-@ order ; ?
Maximum sentence length : 100 tokens ; ?
Truecasing : convert sentence-initial words to their most frequent case in the training dataset ; ?
Word alignments : directed IBM model 4 ( Brown et al. , 1993 ) alignments in both directions , then grow-diag-final - and heuristics ; ?
Maximum phrase length : 7 tokens ; ?
Phrase table scores : forward & reverse phrase translation probabilities , forward & reverse lexical translation probabilities , phrase penalty ; ?
Language model : 5 - gram , trained on the target side of the two training bi-texts ; ?
Reordering : lexicalized , msd-bidirectional - fe ; ?
Detokenization : reconnecting words that were split around dashes ; ?
Model parameter optimization : minimum error rate training ( MERT ) , optimizing BLEU .
Phrase Tables
We experimented with two non-standard settings : Smoothing .
The four standard scores associated with each phrase pair in the phrase table ( forward & reverse phrase translation probabilities , forward & reverse lexical translation probabilities ) are normally used unsmoothed .
We also experimented with Good-Turing and Kneser - Ney smoothing ( Chen and Goodman , 1999 ) .
As Table 1 shows , the latter works a bit better for both Spanish -English and German- English .
es-en de-en Baseline ( es : # 3 , de : # 4 )
Phrase table combination .
We built two phrase tables , one for News Commentary + Europarl and an additional one for the UN bi-text .
We then merged them , 2 adding additional features to each entry in the merged phrase table : F 1 , F 2 , and F 3 .
The value of F 1 / F 2 is 1 if the phrase pair came from the first / second phrase table , and 0.5 otherwise , while F 3 is 1 if the phrase pair was in both tables , and 0.5 otherwise .
We optimized the weights for all features , including the additional ones , using MERT .
3 Table 2 shows that this improves by + 0.42 BLEU points .
2
In theory , we should also re-normalize the conditional probabilities ( forward / reverse phrase translation probability , and forward / reverse lexicalized phrase translation probability ) since they may not sum to one anymore .
In practice , this is not that important since the log-linear phrase - based SMT model does not require that the phrase table features be probabilities ( e.g. , F1 , F2 , F3 , and the phrase penalty are not probabilities ) ; moreover , we have extra features whose impact is bigger .
3
This is similar but different from ( Nakov , 2008 ) : when a phrase pair appeared in both tables , they only kept the entry from the first table , while we keep the entries from both tables .
es-en Baseline ( es : # 7 ) 30.94 Merging ( 1 ) News +EP with ( 2 ) UN 31.36
Language Models
We built the language models ( LM ) for our systems using a probabilistic 5 - gram model with Kneser - Ney ( KN ) smoothing .
We experimented with LMs trained on different training datasets .
We used the SRILM toolkit ( Stolcke , 2002 ) for training the language models , and the KenLM toolkit ( Heafield and Lavie , 2010 ) for binarizing the resulting ARPA models for faster loading with the Moses decoder ( Koehn et al. , 2007 ) .
Using WMT12 Corpora Only
We trained 5 - gram LMs on datasets provided by the task organizers .
The results are presented in Table 3 .
The first line reports the baseline BLEU scores using a language model trained on the target side of the News Commentary + Europarl training bi-texts .
The second line shows the results when using an interpolation ( minimizing the perplexity on the news2010 tuning dataset ) of different language models , trained on the following corpora : ? the monolingual News Commentary corpus plus the English sides of all training News Commentary v.7 bi-texts ( for French - English , Spanish - English , German- English , and Czech-English ) , with duplicate sentences removed ( 5.5 M word tokens ; one LM ) ; ? the News Crawl 2007 - 2011 corpora , ( 1213 M word tokens ; separate LM for each of these five years ) ; ? the Europarl v.7 monolingual corpus ( 60 M word tokens ; one LM ) ; ? the English side of the Spanish - English UN bitext ( 360 M word tokens ; one LM ) .
The last line in Table 3 shows the results when using an additional 5 - gram LM in the interpolation , one trained on the English side of the 10 9 French - English bi-text ( 662 M word tokens ) .
We can see that using these interpolations yields very sizable improvements of 1.7- 2.5 BLEU points over the baseline .
However , while the impact of adding the 10 9 bi-text to the interpolation is clearly visible for Spanish -English ( + 0.47 BLEU ) , it is almost negligible for German-English ( + 0.06 BLEU ) .
Corpora es-en de-en Baseline ( es : # 1 , de :# 2 ) 27.34 20.01 News + EP + UN ( interp . ) 29.36 21.66 News + EP + UN + 10 9 ( interp . ) 29.83 21.72 Table 3 : LMs using the provided corpora only .
Using Gigaword
In addition to the WMT12 data , we used the LDC Gigaword v.5 corpus .
We divided the corpus into reasonably - sized chunks of text of about 2 GB per chunk , and we built a separate intermediate language model for each chunk .
Then , we interpolated these language models , minimizing the perplexity on the news2010 development set as with the previous LMs .
We experimented with two different strategies for creating the chunks by segmenting the corpus according to ( a ) data source , e.g. , AFP , Xinhua , etc. , and ( b ) year of release .
We thus compared the advantages of interpolating epoch-consistent LMs vs. source-coherent LMs .
We trained individual LMs for each of the segments and we added them to a pool .
Finally , we selected the ten most relevant ones from this pool based on their perplexity on the news2010 devset , and we interpolated them .
The results are shown in Table 4 .
The first line shows the baseline , which uses an interpolation of the nine LMs from the previous subsection .
The following two lines show the results when using an LM trained on Gigaword only .
We can see that for Spanish - English , interpolation by year performs better , while for German - English , it is better to use the by-source chunks .
However , the following two lines show that when we translate with two LMs , one built from the WMT12 data only and one built using Gigaword data only , interpolation by year is preferable for Gigaword for both language pairs .
For our submitted systems , we used the LMs shown in bold in Table 4 : we used a single LM for Spanish -English and two LMs for German - English .
Parameter Tuning and Data Selection Parameter tuning is a very important step in SMT .
The standard procedure consists of performing a series of iterations of MERT to choose the model parameters that maximize the translation quality on a development set , e.g. , as measured by BLEU .
While the procedure is widely adopted , it is also recognized that the selection of an appropriate development set is important since it biases the parameters towards specific types of translations .
This is illustrated in Table 5 , which shows BLEU on the news2011 testset when using different development sets for MERT .
Devset es-en news2008 29.47 news2009 29.14 news2010 29.61
To address this problem , we performed a selection of development data using an n-gram- based similarity ranking .
The selection was performed over a pool of candidate sentences drawn from the news2008 , news2009 , and news2010 tuning datasets .
The similarity metric was defined as follows : sim(f , g ) = 2 match ( f , g ) * lenpen(f , g ) ( 1 ) where 2 match represents the number of bi-gram matches between sentences f and g , and lenpen is a length penalty to discourage unbalanced matches .
We penalized the length difference using an inverted -squared sigmoid function : lenpen(f , g ) = 3 ? 4 * sig |f | ? | g| ? 2 ( 2 ) where |.| denotes the length of a sentence in number of words , ? controls the maximal tolerance to differences , and sig is the sigmoid function .
To generate a suitable development set , we averaged the similarity scores of candidate sentences w.r.t. to the target testset .
For instance : s f = 1 | G | g?G sim(f , g ) ( 3 ) where G is the set of the test sentences .
Finally , we selected a pool of candidates f from news 2008 , news2009 and news2011 to generate a 2000 - best tuning set .
The results when using each of the above penalty functions are presented on Table 6 .
The average length of the source-side sentences in our selected sentence pairs was smaller than in our baseline , the news2011 development dataset .
This means that our selected source-side sentences tended to be shorter than in the baseline .
Moreover , the standard deviation of the sentence lengths was smaller for our samples as well , which means that there were fewer long sentences ; this is good since long sentences can take very long to translate .
As a result , we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets .
Decoding and Hypothesis Reranking
We experimented with two decoding settings : ( 1 ) monotone at punctuation reordering ( Tillmann and Ney , 2003 ) , and ( 2 ) minimum Bayes risk decoding ( Kumar and Byrne , 2004 ) .
The results are shown in Table 7 .
We can see that both yield improvements in BLEU , even if small .
System Combination
As the final step in our translation system , we performed hypothesis re-combination of the output of several of our systems using the Multi-Engine MT system ( MEMT ) ( Heafield and Lavie , 2010
The results for the actual news2012 testset are shown in Table 8 : the system combination results are our primary submission .
We can see that system combination yielded 0.4 BLEU points of improvement for Spanish -English and 0.2-0.3 BLEU points for German- English .
Our Submissions
Here we briefly describe the cumulative improvements when applying the above modifications to our baseline system , leading to our official submissions for the WMT12 Shared Translation Task .
Spanish -English
The development of our final Spanish - English system involved several incremental improvements , which have been described above and which are summarized in Figure 1 .
We started with a baseline system ( see Section 2.1 ) , which scored 27.34 BLEU points .
From there , using a large interpolated language model trained on the provided data ( see Section 2.3.1 ) yielded + 2.49 BLEU points of improvement .
Monotone- at- punctuation decoding contributed an additional improvement of + 0.15 , smoothing the phrase table using Kneser - Ney boosted the score by + 0.18 , and using minimum Bayes risk decoding added another + 0.15 BLEU points .
Changing the language model to one trained on Gigaword v.5 and interpolated by year yielded + 0.37 additional points of improvement .
Another + 0.26 points came from tuning data selection .
Finally , using the UN data in a merged phrase table ( see Section 2.2 ) yielded another + 0.42 BLEU points .
Overall , we achieve a total improvement over our initial baseline of about 4 BLEU points .
German-English Figure 2 shows a similar sequence of improvements for our German-English system .
We started with a baseline ( see Section 2.1 ) that scored 19.79 BLEU points .
Next , we performed compound splitting for the German side of the training , the development and the testing bi-texts , which yielded + 0.22 BLEU points of improvement .
Using a large interpolated language model trained on the provided corpora ( see Section 2.3.1 ) added another + 1.71 .
Monotone -atpunctuation decoding contributed + 0.31 , smoothing the phrase table using Kneser - Ney boosted the score by + 0.27 , and using minimum Bayes risk decoding added another + 0.18 BLEU points .
Finally , adding a second language model trained on the Gigaword v.5 corpus interpolated by year yielded + 0.23 additional BLEU points .
Overall , we achieved about 3 BLEU points of total improvement over our initial baseline .
Final Submissions
For both language pairs , our primary submission was a combination of the output of several of our best systems shown in Figures 1 and 2 , which use different experimental settings ; our secondary submission was our best individual system , i.e. , the right-most one in Figures 1 and 2 .
The official BLEU scores , both cased and lowercased , for our primary and secondary submissions , as evaluated on the news2012 dataset , are shown in Table 8 . For Spanish - English , we achieved the second highest BLEU and TER scores , while for German - English we were ranked in the top tier .
Conclusion
We have described the primary and the secondary systems developed by the team of the Qatar Computing Research Institute for Spanish -English and German-English machine translation of news text for the WMT12 Shared Translation Task .
We experimented with phrase - based SMT , exploring a number of non-standard settings , most notably tuning data selection and phrase table combination , which we described and evaluated in a cumulative fashion .
The automatic evaluation metrics , 4 have ranked our system second for Spanish - English and in the top tier for German- English .
We plan to continue our work on data selection for phrase table and the language model training , in addition to data selection for tuning .
selection ( ? = 10 ) 30.90
