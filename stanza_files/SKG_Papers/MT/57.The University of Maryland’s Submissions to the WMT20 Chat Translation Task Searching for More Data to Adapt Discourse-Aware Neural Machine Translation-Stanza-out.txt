title
The University of Maryland 's Submissions to the WMT20 Chat Translation Task : Searching for More Data to Adapt Discourse -Aware Neural Machine Translation
abstract
This paper describes the University of Maryland 's submissions to the WMT20 Shared Task on Chat Translation .
We focus on translating agent-side utterances from English to German .
We started from an off-the-shelf BPEbased standard transformer model trained with WMT17 news and fine-tuned it with the provided in - domain training data .
In addition , we augment the training set with its best matches in the WMT19 news dataset .
Our primary submission uses a standard Transformer , while our contrastive submissions use multi-encoder Transformers to attend to previous utterances .
Our primary submission achieves 56.7 BLEU on the agent side ( en?de ) , outperforming a baseline system provided by the task organizers by more than 13 BLEU points .
Moreover , according to an evaluation on a set of carefullydesigned examples , the multi-encoder architecture is able to generate more coherent translations .
Introduction Recent advances have made MT a widespread tool for asynchronous consumption of text .
The dream of dissolving language barriers , however , will not be fulfilled until MT enables two or more people carry on a synchronous conversation , each speaking their native languages .
Building translation systems that enable seamless conversations between an English-speaking customer support agent and a German-speaking customer is the goal of WMT20 's shared task of chat translation ( Farajian et al. , 2020 ) .
In participation of this shared task , we focused on the agent side , translating English utterances into German .
Our methods are inspired by Voita et al . ( 2018 ) and Bawden et al . ( 2018 ) , explicitly leveraging broader context to address coreference and cohesion to improve translation quality .
* Equal contribution .
We compare architectures of a standard transformer with a single encoder and a multi-encoder one with an additional transformer encoder to incorporate information from the previous utterance .
In the case of blind testing or production use , since customer target utterances ( English ) will not be given , a separate de?en model was trained and used to back - translate customer utterances .
Additionally , given the limited training pairs , we experiment with augmenting our dataset .
We selected a subset of WMT19 en-de news data that were similar to the chat training data , which we then added to the training data .
The subset was constructed using a full- text search engine loaded with the entire en-de WMT19 news data , which iterated through each chat training example , querying for the two closest matches with both the source and target as search strings .
Our primary system , denoted PRIMARY , is a single-encoder pretrained transformer fine-tuned on WMT20 Chat data .
The first contrastive system , denoted CONTRASTIVE1 , is a multi-encoder transformer that pre-warms , using WMT19 news data , the weights of an additional encoder after loading the pretrained transformer .
The second contrastive system , denoted CONTRASTIVE2 , is a multi-encoder transformer that fine- tunes the pretrained transformer on a combination of WMT19 news data and WMT20 chat data .
Related Work
One of the main challenges for translating discourse arises from ambiguities of sentences when they are taken out of context , as MT models often do ( Yamashita et al. , 2009 ) .
Especially in dialogue , sentences tend to reference entities in previous sentences , which necessitates using cross-sentential information to translate a given sentence .
Individual words can be translated in different ways , significantly varying the meaning of the resulting sentence in a larger context ( Gao et al. , 2015 ) .
In addition , dialogue in the customer support domain is a distinctive and spontaneous category of text , with colloquialisms , errors and minimal revisions .
All of these deviations can accumulate error throughout the course of a conversation .
Dialogue Translation : Specific interests in translating dialogues can be found as early as Lee and Kim ( 1997 ) 's work on Korean - English dialogue translation based on syntactic patterns and n-grams .
Though their model parses sentences into speech acts instead of generating full-sentence translations , they have pointed out the importance of context ( previous sentence ) in interpreting the current sentence properly .
The most relevant recent work is ( Maruf et al. , 2018 ) , in which contexts for both source-side and target - side are utilized as additional generation conditions for the decoder in their NMT model .
Several variants of the model architecture and the attention mechanism are explored .
However , their experiments are conducted on Europarl and OpenSubtitles .
The former is formal language and the latter scripted conversations of movies and TV .
Here , in contrast , chat data is informal unscripted real-world language .
Context - Aware Machine Translation :
Chat translation can be regarded as a special case of context - aware translation .
Jean et al. ( 2017 ) extends the vanilla attention - based neural MT model ( Bahdanau et al. , 2015 ) by conditioning the decoder on the previous sentence via attention over its words .
Wang et al. ( 2017 ) propose a crosssentence context - aware model .
They integrate the historical representation into NMT with two strategies : a warm-start of encoder and decoder states , and an auxiliary context source for updating decoder .
Bawden et al. ( 2018 ) use multi-encoder NMT models to exploit context from the previous source and target sentence .
Voita et al. ( 2018 ) propose a context - aware model based on the Transformer .
Their model controls the flow of information from the extended context and improves on pronoun translation .
NMT Facilitated with Retrieved Translations :
There is a line of NMT research inspired by example - based translation systems that aims to generate better translations by retrieving and referencing additional translation pairs .
Gu et al. ( 2018 ) utilize an off-the-shelf search engine to retrieve training sentence pairs whose source side is similar to a given source sentence and incorporate them as additional input to the decoder .
Zhang et al. ( 2018 ) use the retrieved examples at prediction time to upweight outputs whose constituents match retrieved n-gram translation candidates .
In a similar vein , but at training time rather than prediction time , we use a retrieval system to select similar examples from a larger dataset to augment the smaller in - domain training set .
3 Data Preparation
Preprocessing
We used the Moses toolkit ( Koehn et al. , 2007 ) to preprocess our data .
The training corpus was tokenized and cleaned .
After that , we applied byte pair encoding ( BPE ) ( Sennrich et al. , 2016 ) on the data with the BPE model learned on the data of the pretrained model ( Section 4.1.1 ) .
Following the pre-trained model , we use its shared vocabulary for both target and source sides .
The size of the vocabulary , which is the union of English and German tokens , is 36,628 .
Retrieval -Based Training Data Augmentation
There were only 13.85 k utterances in the provided parallel WMT20 Chat training data .
Given the limited data , we start off with a model pretrained on the WMT17 en-de news data , and additionally augment our training data with a filtered set of 4.75 k lines of WMT19 en-de news data .
We adopted Elasticsearch 1 to build a fast full - text search engine on the entire WMT19 en-de news set , and then iterated through each ( source , reference ) pair in the Chat training data .
With each pair , we used the search engine to find the top two matches with the current source and target as search strings .
We truncated this set to 4.75 k training samples to limit the possibility of overwhelming our fine-tuning set and denote it Chat-Similar News .
This technique brings the total training set to 18.6 K parallel utterances .
Experiments
We conducted varied experiments in the English - German direction .
We included the English reference of the customer utterances as training data for the scope of these experiments , even though this would not be available in a production setting .
This was a strategy to provide more training pairs to our models , knowing that the English references for customers is natural language , according to task organizers .
Systems Overview
We base all our systems off the Transformer architecture .
Our implementation is based on the JoeyNMT toolkit ( ? ) .
We kept hyperparameters common throughout .
We experimented with the following settings : 1 . Trained a standard single-encoder Transformer model .
2 . Introduced a second encoder into our NMT architecture to process the preceding sentence , using context-target attention along with source-target attention to compute the final encoder hidden state , on a combination of Chat-Similar News and Chat .
3 . As in item 2 , introduced a second encoder and pre-warmed that encoder 's weights on Chat-Similar News , before fine-tuning on Chat .
Off - the-shelf Pretrained Model
We found that an existing model trained on a different domain can generalize to this smaller dataset .
We downloaded model weights for WMT17 en-de , provided by JoeyNMT Transformer 2 .
This model was able to adapt to the chat domain , so the pretrained model was used for all experimental settings .
Common Hyperparameters
We kept hyperparameters consistent across models we tested , with some exceptions to account for slight differences in architecture .
All models had embedding and hidden layers with 512 units , and feed-forward layers with 2048 units .
A dropout rate of 0.1 was used on both the encoder and decoder layers .
Training was performed with the Adam optimizer and in minibatches of 2048 tokens , with cross-entropy loss , an initial learning rate of 0.0002 , and a patience of 8 validation cycles .
All models were trained for a maximum of 65 epochs .
The checkpoint with lowest validation perplexity is selected as the final model .
For all validation cycles , greedy decoding is adopted .
For testing , we used beam search decoding with a beam width of 5 .
Single-encoder Implementation : PRIMARY
We trained a discourse-agnostic Transformer model with self-attention .
This model had 6 layers for the both the encoder and decoder , each with 8 attention heads .
A single-encoder implementation fine-tuned only on the Chat data was used to produce the primary submission results .
We selected this model due to its slightly higher Chat validation BLEU ( Table 1 ) .
It also achieves the highest test BLEU but with only minor differences with the contrastive systems .
However , the gaps between it and the two contrastive multi-encoder implementations are not wide as can be seen .
Multi-encoder Implementation
Two context - aware models that are partial extensions of that described in Voita et al . ( 2018 ) were produced for the contrastive submissions .
Voita et al. ( 2018 ) 's context - aware model encodes a source sentence and a context sentence independently and applies a gating function to produce a context - aware representation of the source sentence .
We explored this combination idea by implementing a trainable gating function , ? la ( Voita et al. , 2018 ) , that takes the independently encoded source-side context and independently encoded source -side sentence as inputs to generate a representation for the decoder .
Each layer retained 8 attention heads .
We used 6 layers in each encoder .
The total number of trainable parameters can be seen in Table 2 .
Incremental Domain Adaptation : CONTRASTIVE1
This system has two steps : we pre-warm the context encoder of a multi-encoder implementation by fine-tuning on Chat-Similar News and validating on a subset of the Chat training data .
Same-time Training of Chat-Similar News : CONTRASTIVE2
This system fine-tunes on the multi-encoder architecture with the combined Chat-Similar News and Chat training data in one shot .
We see only a 0.2 validation BLEU point improvement here over the multi-encoder fine -tuned only with Chat ( Vanilla Chat in Table 1 ) .
Evaluation
Official Evaluation BLEU scores on the development and test sets , and official human evaluation results are shown in Table 1 .
The PRIMARY system achieves the best validation and test BLEU .
While CONTRASTIVE1 has a slightly higher validation BLEU , it turns out that CONTRASTIVE2 performs better at test time , showing that the same - time training technique may be less prone to overfitting .
Coherence Evaluation with Hand-crafted Examples
The official evaluation results seem to suggest the context- aware multi-encoder architecture ( contrastive systems ) is not superior to the standard Transformer which has no access to contextual information .
We manually examined the training data , and noted that between two people interacting with each other on the phone or through their computer screens , there are not many indirect pronouns , possibly because there is no associated real - life gesturing necessitating expressions such as " that one " or " those ones " .
Seemingly , in the provided datasets , the need to be clear over the phone / internet means key words are often repeated for clarity , especially on the agent side ( " I would like to order a pizza " ; " how can I help you with ordering a pizza " ) .
Inspired by ( Bawden et al. , 2018 ) , we carefully evaluate performance of the systems on a hand-crafted dataset consisting of coreference and cohesion test instances .
Example instances can be seen in Tables 4 and 5 respectively .
A contributor fluent in both English and German produced two versions of a dataset of 103 sourcetarget pairs 3 based loosely off the provided validation set , following the spirit of ( Bawden et al. , 2018 ) , in which a current utterance will require the previous utterance in order to make a disambiguating translation in the current .
One version has the reference translation set to the correct coreference or cohesion resolution , while the other version can be a potentially correct translation viewing the source sentence in isolation but is incorrect with the additional context .
The source side remains unchanged in both versions .
We benchmarked each of our submitted models by producing hypotheses using each model given the source sentence , and then computing BLEU scores on the reference from both versions of this dataset .
In Table 3 , we show the results of each model against the two versions .
We used greedy decoding to generate the hypotheses .
We observe that the contrastive multi-encoder systems , though performing worse in BLEU than the single- encoder primary system on the provided validation dataset , actually score higher in the specifically crafted correct coreference / cohesion dataset .
By contrast , PRIMARY scores higher for the incorrect coreference / cohesion dataset .
Furthermore , the difference in BLEU points between the correct and incorrect Yes , that 's more than generous .
Correct reference Ja , das ist mehr als grosszuegig .
Incorrect reference Ja , das ist mehr als wohlwollend .
Table 5 : Example of sentence requiring lexical disambiguation .
Given the context of giving a " tip " , a system should bias the translation of " generous " more towards " grosszuegig " ( someone is free with money ) and away from " wohlwollend " ( more in the altruistic , do-gooder sense ) , which is inappropriate here .
coherence datasets is more significant in the contrastive systems , suggesting that the contrastive models are recovering more of the correct coreference and cohesion , as opposed to retrieving vocabulary words in other areas of the reference .
Discussion
Our results largely agree with those of ( Voita et al. , 2018 ) , chiefly that combining knowledge from a previous " context " sentence can improve the model 's ability to improve translation quality when measured against sentences whose translations require anaphora considerations .
To accommodate this , we produced one set of sentences which require coreference and cohesion resolution , and one set of sentences that have invalid resolution .
We found that each submitted system scored worse on the invalid set compared to the valid set , but the difference was more staggering ( Table 3 ) in the context - aware contrastive systems , lending evidence that these models are able to resolve this type of anaphora .
Our work and submission to the shared task can be viewed with several caveats in mind , which may explain the sub-optimal performance of the contrastive systems compared to the primary system .
First , we used hyperparameters consistent with a context-agnostic pretrained model in order to have a fair comparison for evaluation and because these presumably have been well - tuned for the original model .
It may be the case that different hyperparameters would work better for this particular data and the slightly larger architectures used for the contrastive submissions .
It would be worth strategizing with better hyperparameter optimization .
Secondly , we use the provided target sides of the de?en direction to provide context to our en-de data as if it were back - translated .
Since both the agent and customer sides of this datasets were actually produced in English ( the latter being translated with human-corrected machine translation ) , these additional utterances are likely higher quality than we would get from back - translating in a real test setting .
Conclusions
In this paper , we discussed our methods for training and submitting the outputs of three models for the WMT20 shared task of chat translation .
Each system was based off a transformer model pretrained on WMT17 en-de news to provide better fluency .
Our best system achieves a test BLEU of 56.7 , improving over the provided baseline by more than 13 BLEU points , and less than 4 points behind the best shared task submission .
Though we were unable to show that a context - aware model produced better translation quality than the context-agnostic model on the given dataset , our coherence evaluations indicated that it can produce better translations when measured against references needing context for coreference and cohesion resolution .
This was validated both in terms of BLEU and by model scoring of references .
Table 1 : 1 Agent-side ( en?de ) performance of submitted systems on the official development and test sets of the WMT20 chat translation task .
BASELINE was the best performing model in the WMT19 News task , PRIMARY is our primary submission , and WMT20 - CHAT - BEST produced the best Agent-side outputs , according to human evaluation .
We then on Chat , we achieve a 1.79 validation BLEU point improvement .
Compared to CONTRASTIVE2 , a model that fine-tunes on a mixture of Chat and Chat-Similar News in one step , we achieve a 0.59 validation BLEU point improvement .
Table 3 : 3 Agent-side ( en?de ) performance of submitted systems on our coherence dataset .
Context utterance Nein .
Ich weiss nicht wo sie ist .
( No , I do not know where it is . )
Source utterance
It 's 200 meters north of City Center .
Correct reference Sie ist zweihundert Meter nordlich vom Stadtzentrum .
Incorrect reference
Es ist zweihundert Meter nordlich vom Stadtzentrum .
Table 4 : 4 Example of sentence requiring anaphoric pronoun resolution .
A better translation should bias to the correct pronoun based on context as ' sie ' and not as ' er ' or 'es ' ( for masculine and neuter nouns respectively ) .
Context utterance Ist 30 % in Ordnung als Trinkgeld ?
( Is 30 % alright as tip ? )
Source utterance
https://www.elastic.co/
https://www.cl.uni-heidelberg.
de/statnlpgroup/joeynmt/wmt_ende_ transformer.tar.gz
https://github.com/SongChujun/ joeynmt/blob/master/chatnmt/coher / manual_coher.json
