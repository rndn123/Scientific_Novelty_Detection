title
EU - BRIDGE MT : Combined Machine Translation
abstract
This paper describes one of the collaborative efforts within EU - BRIDGE to further advance the state of the art in machine translation between two European language pairs , German ?
English and English ?
German .
Three research institutes involved in the EU - BRIDGE project combined their individual machine translation systems and participated with a joint setup in the shared translation task of the evaluation campaign at the ACL 2014 Eighth Workshop on Statistical Machine Translation ( WMT 2014 ) .
We combined up to nine different machine translation engines via system combination .
RWTH Aachen University , the University of Edinburgh , and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input .
We devoted special attention to building syntax - based systems and combining them with the phrasebased ones .
The joint setups yield empirical gains of up to 1.6 points in BLEU and 1.0 points in TER on the WMT news - test2013 test set compared to the best single systems .
Introduction EU - BRIDGE 1 is a European research project which is aimed at developing innovative speech translation technology .
This paper describes a 1 http://www.eu-bridge.eu joint WMT submission of three EU - BRIDGE project partners .
RWTH Aachen University ( RWTH ) , the University of Edinburgh ( UEDIN ) and Karlsruhe Institute of Technology ( KIT ) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach .
As distinguished from our EU - BRIDGE joint submission to the IWSLT 2013 evaluation campaign ( Freitag et al. , 2013 ) , we particularly focused on translation of news text ( instead of talks ) for WMT .
Besides , we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase - based engines .
We built combined system setups for translation from German to English as well as from English to German .
This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU - BRIDGE submission to the WMT 2014 translation task .
The remainder of the paper is structured as follows :
We first describe the individual systems by RWTH Aachen University ( Section 2 ) , the University of Edinburgh ( Section 3 ) , and Karlsruhe Institute of Technology ( Section 4 ) .
We then present the techniques for machine translation system combination in Section 5 .
Experimental results are given in Section 6 .
We finally conclude the paper with Section 7 .
RWTH Aachen University RWTH ( Peitz et al. , 2014 ) employs both the phrase - based ( RWTH scss ) and the hierarchical ( RWTH hiero ) decoder implemented in RWTH 's publicly available translation toolkit Jane ( Vilar et al. , 2010 ; Wuebker et al. , 2012 ) .
The model weights of all systems have been tuned with standard Minimum Error Rate Training ( Och , 2003 ) on a concatenation of the newstest2011 and news - test2012 sets .
RWTH used BLEU as optimization objective .
Both for language model estimation and querying at decoding , the KenLM toolkit ( Heafield et al. , 2013 ) is used .
All RWTH systems include the standard set of models provided by Jane .
Both systems have been augmented with a hierarchical orientation model ( Galley and Manning , 2008 ; and a cluster language model .
The phrasebased system ( RWTH scss ) has been further improved by maximum expected BLEU training similar to ( He and Deng , 2012 ) .
The latter has been performed on a selection from the News Commentary , Europarl and Common Crawl corpora based on language and translation model cross-entropies ( Mansour et al. , 2011 ) .
University of Edinburgh UEDIN contributed phrase- based and syntaxbased systems to both the German ?
English and the English ?
German joint submission .
Phrase - based Systems UEDIN 's phrase - based systems ( Durrani et al. , 2014 ) have been trained using the Moses toolkit , replicating the settings described in ( Durrani et al. , 2013 b ) .
The features include : a maximum sentence length of 80 , growdiag -final - and symmetrization of GIZA ++ alignments , an interpolated Kneser - Ney smoothed 5 gram language model with KenLM ( Heafield , 2011 ) used at runtime , a lexically - driven 5 - gram operation sequence model ( OSM ) ( Durrani et al. , 2013a ) , msd-bidirectional - fe lexicalized reordering , sparse lexical and domain features ( Hasler et al. , 2012 ) , a distortion limit of 6 , a maximum phrase length of 5 , 100 - best translation options , Minimum Bayes Risk decoding ( Kumar and Byrne , 2004 ) , cube pruning ( Huang and Chiang , 2007 ) , with a stack size of 1000 during tuning and 5000 during testing and the no-reordering - overpunctuation heuristic .
UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using Kneser - Ney smoothed 7 - gram models as additional factors in phrase translation models .
UEDIN has furthermore built OSM mod-els over POS and morph sequences following Durrani et al . ( 2013 c ) .
The English ?
German system additionally comprises a target- side LM over automatically built word classes ( Birch et al. , 2013 ) . UEDIN has applied syntactic prereordering ( Collins et al. , 2005 ) and compound splitting ( Koehn and Knight , 2003 ) of the source side for the German ?
English system .
The systems have been tuned on a very large tuning set consisting of the test sets from 2008 - 2012 , with a total of 13,071 sentences .
UEDIN used news - test2013 as held - out test set .
On top of UEDIN phrase - based 1 system , UEDIN phrase - based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs .
Furthermore , it learns OSM models over POS , morph and word classes .
Syntax - based Systems UEDIN 's syntax - based systems ( Williams et al. , 2014 ) follow the GHKM syntax approach as proposed by Galley , Hopkins , Knight , and Marcu ( Galley et al. , 2004 ) .
The open source Moses implementation has been employed to extract GHKM rules ( Williams and Koehn , 2012 ) .
Composed rules ( Galley et al. , 2006 ) are extracted in addition to minimal rules , but only up to the following limits : at most twenty tree nodes per rule , a maximum depth of five , and a maximum size of five .
Singleton hierarchical rules are dropped .
The features for the syntax - based systems comprise Good- Turing - smoothed phrase translation probabilities , lexical translation probabilities in both directions , word and phrase penalty , a rule rareness penalty , a monolingual PCFG probability , and a 5 - gram language model .
UEDIN has used the SRILM toolkit ( Stolcke , 2002 ) to train the language model and relies on KenLM for language model scoring during decoding .
Model weights are optimized to maximize BLEU .
2000 sentences from the newstest2008 -2012 sets have been selected as a development set .
The selected sentences obtained high sentence - level BLEU scores when being translated with a baseline phrasebased system , and each contain less than 30 words for more rapid tuning .
Decoding for the syntaxbased systems is carried out with cube pruning using Moses ' hierarchical decoder ( Hoang et al. , 2009 ) . UEDIN 's German ?
English syntax - based setup is a string - to - tree system with compound splitting on the German source - language side and syntactic annotation from the Berkeley Parser ( Petrov et al. , 2006 ) on the English target - language side .
For English ?
German , UEDIN has trained various string - to- tree GHKM syntax systems which differ with respect to the syntactic annotation .
A tree-to-string system and a string - to-string system ( with rules that are not syntactically decorated ) have been trained as well .
The English ?
German UEDIN GHKM system names in Table 3 denote : UEDIN GHKM S2T ( ParZu ) :
A string - to - tree system trained with target -side syntactic annotation obtained with ParZu ( Sennrich et al. , 2013 ) .
It uses a modified syntactic label set , target - side compound splitting , and additional syntactic constraints .
UEDIN GHKM S2T ( BitPar ) :
A string - to - tree system trained with target -side syntactic annotation obtained with BitPar ( Schmid , 2004 ) .
UEDIN GHKM S2T ( Stanford ) : A string - totree system trained with target-side syntactic annotation obtained with the German Stanford Parser ( Rafferty and Manning , 2008a ) .
UEDIN GHKM S2T ( Berkeley ) :
A string -totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser ( Petrov and Klein , 2007 ; Petrov and Klein , 2008 ) .
UEDIN GHKM T2S ( Berkeley ) :
A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser ( Petrov et al. , 2006 ) .
UEDIN GHKM S2S ( Berkeley ) : A string -tostring system .
The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser , but we strip off the syntactic labels .
The final grammar contains rules with a single generic nonterminal instead of syntactic ones , plus rules that have been added from plain phrase - based extraction .
Karlsruhe Institute of Technology
The KIT translations ( Herrmann et al. , 2014 ) are generated by an in-house phrase - based translations system ( Vogel , 2003 ) .
The provided News Commentary , Europarl , and Common Crawl parallel corpora are used for training the translation model .
The monolingual part of those parallel corpora , the News Shuffle corpus for both directions and additionally the Gigaword corpus for German ?
English are used as monolingual training data for the different language models .
Optimization is done with Minimum Error Rate Training as described in ( Venugopal et al. , 2005 ) , using newstest2012 and newstest2013 as development and test data respectively .
Compound splitting ( Koehn and Knight , 2003 ) is performed on the source side of the corpus for German ?
English translation before training .
In order to improve the quality of the web-crawled Common Crawl corpus , noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al . ( 2011 ) .
The word alignment for German ?
English is generated using the GIZA ++ toolkit ( Och and Ney , 2003 ) .
For English ?
German , KIT uses discriminative word alignment ( Niehues and Vogel , 2008 ) .
Phrase extraction and scoring is done using the Moses toolkit .
Phrase pair probabilities are computed using modified Kneser - Ney smoothing as in ( Foster et al. , 2006 ) .
In both systems KIT applies short - range reorderings ( Rottmann and Vogel , 2007 ) and longrange reorderings ( Niehues and Kolss , 2009 ) based on POS tags ( Schmid , 1994 ) to perform source sentence reordering according to the target language word order .
The long- range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model .
In addition , a tree-based reordering model trained on syntactic parse trees ( Rafferty and Manning , 2008 b ; Klein and Manning , 2003 ) as well as a lexicalized reordering model are applied .
Language models are trained with the SRILM toolkit ( Stolcke , 2002 ) and use modified Kneser - Ney smoothing .
Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm ( Och , 1999 ) .
The English ?
German system comprises language models based on fine- grained part-ofspeech tags ( Schmid and Laws , 2008 ) .
In addition , a bilingual language model is used as well as a discriminative word lexicon ( Mauser et al. , 2009 ) using source context to guide the word choices in the target sentence .
In total , the English ?
German system uses the following language models : two 4 - gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately , two 5 - gram POS - based language models trained on the same data as the word - based language models , and a 4 - gram cluster - based language model trained on 1,000 MKCLS word classes .
The German ?
English system uses a 4 - gram word - based language model trained on all monolingual data and an additional language model trained on automatically selected data ( Moore and Lewis , 2010 ) .
Again , a 4 - gram cluster - based language model trained on 1000 MKCLS word classes is applied .
System Combination System combination is used to produce consensus translations from multiple hypotheses which are outputs of different translation engines .
The consensus translations can be better in terms of translation quality than any of the individual hypotheses .
To combine the engines of the project partners for the EU - BRIDGE joint setups , we apply a system combination implementation that has been developed at RWTH Aachen University .
The implementation of RWTH 's approach to machine translation system combination is described in .
This approach includes an enhanced alignment and reordering framework .
Alignments between the system outputs are learned using METEOR ( Banerjee and Lavie , 2005 ) .
A confusion network is then built using one of the hypotheses as " primary " hypothesis .
We do not make a hard decision on which of the hypotheses to use for that , but instead combine all possible confusion networks into a single lattice .
Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models , e.g. a special n-gram language model which is learned on the input hypotheses .
Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm .
The translation with the best total score within the lattice is selected as consensus translation .
Results
In this section , we present our experimental results on the two translation tasks , German ?
English and English ?
German .
The weights of the in-dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012 .
System combination weights are either optimized on newstest2011 or newstest2012 .
We kept newstest2013 as an unseen test set which has not been used for tuning the system combination or any of the individual systems .
German ?
English
The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1 . KIT , UEDIN and RWTH are each providing one individual phrasebased system output .
RWTH ( hiero ) and UEDIN ( GHKM ) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model .
The pairwise difference of the single system performances is up to 1.3 points in BLEU and 2.5 points in TER .
For German ?
English , our system combination parameters are optimized on newstest2012 .
System combination gives us a gain of 1.6 points in BLEU and 1.0 points in TER for newstest2013 compared to the best single system .
In Table 2 the pairwise BLEU scores for all individual systems as well as for the system combination output are given .
The pairwise BLEU score of both RWTH systems ( taking one as hypothesis and the other one as reference ) is the highest for all pairs of individual system outputs .
A high BLEU score means similar hypotheses .
The syntax - based system of UEDIN and RWTH scss differ mostly , which can be observed from the fact of the lowest pairwise BLEU score .
Furthermore , we can see that better performing individual systems have higher BLEU scores when evaluating against the system combination output .
In Figure 1 system combination output is compared to the best single system KIT .
We distribute the sentence - level BLEU scores of all sentences of newstest2013 .
To allow for sentence - wise evaluation , all bi- , tri- , and four -gram counts are initialized with 1 instead of 0 .
Many sentences have been improved by system combination .
Nevertheless , some sentences fall off in quality compared to the individual system output of KIT .
English ?
German
The results of all English ?
German system setups are given in Table 3 : Results for the English ?
German translation task .
The system combination is tuned on news - test2011 , newstest2013 is used as held - out test set for all individual systems and system combination .
Bold font indicates system combination results that are significantly ( Bisani and Ney , 2004 ) better than the best single system with p < 0.05 .
Italic font indicates system combination results that are significantly better than the best single system with p < 0.1. tributing individual systems .
KIT is providing a phrase - based system output , UEDIN is providing two phrase - based system outputs and six syntaxbased ones ( GHKM ) .
For English ?
German , our system combination parameters are optimized on newstest2011 .
Combining all nine different system outputs yields an improvement of 0.5 points in BLEU and 1.7 points in TER over the best single system performance .
In As for the German ?
English translation direction , the best performing individual system outputs are also having the highest BLEU scores when evaluated against the final system combination output .
In Figure 2 system combination output is compared to the best single system pbt 2 .
We distribute the sentence - level BLEU scores of all sentences of newstest2013 .
Many sentences have been improved by system combination .
But there is still room for improvement as some sentences are still better in terms of sentence - level BLEU in the individual best system pbt 2 .
Conclusion
We achieved significantly better translation performance with gains of up to + 1.6 points in BLEU and - 1 . based , hierarchical phrase - based , and syntaxbased .
For English ?
German , we included six different syntax - based systems , which were combined to our final combined translation .
The automatic scores of all submitted system outputs for the actual 2014 evaluation set are presented on the WMT submission page .
2
Our joint submission is the best submission in terms of BLEU and TER for both translation directions German ?
English and English ?
German without adding any new data .
Figure 1 : 1 Figure 1 : Sentence distribution for the German ?
English newstest2013 test set comparing system combination output against the best individual system .
