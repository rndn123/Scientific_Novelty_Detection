title
Phrase Training Based Adaptation for Statistical Machine Translation
abstract
We present a novel approach for translation model ( TM ) adaptation using phrase training .
The proposed adaptation procedure is initialized with a standard general - domain TM , which is then used to perform phrase training on a smaller in - domain set .
This way , we bias the probabilities of the general TM towards the in-domain distribution .
Experimental results on two different lectures translation tasks show significant improvements of the adapted systems over the general ones .
Additionally , we compare our results to mixture modeling , where we report gains when using the suggested phrase training adaptation method .
Introduction
The task of domain-adaptation attempts to exploit data mainly drawn from one domain ( e.g. news , parliamentary discussion ) to maximize the performance on the test domain ( e.g. lectures , web forums ) .
In this work , we focus on translation model ( TM ) adaptation .
A prominent approach in recent work is weighting at different levels of granularity .
Foster and Kuhn ( 2007 ) perform weighting at the corpus level , where different corpora receive different weights and are then combined using mixture modeling .
A finer grained weighting is that of Matsoukas et al . ( 2009 ) , who weight each sentence in the bitexts using features of meta-information and optimize a mapping from the feature vectors to weights using a translation quality measure .
In this work , we propose to perform TM adaptation using phrase training .
We start from a generaldomain phrase table and adapt the probabilities by training on an in-domain data .
Thus , we achieve direct phrase probabilities adaptation as opposed to weighting .
Foster et al. ( 2010 ) perform weighting at the phrase level , assigning each phrase pair a weight according to its relevance to the test domain .
They compare phrase weighting to a " flat " model , where the weight directly approximates the phrase probability .
In their experiments , the weighting method performs better than the flat model , therefore , they conclude that retaining the original relative frequency probabilities of the TM is important for good performance .
The " flat " model of Foster et al . ( 2010 ) is similar to our work .
We differ in the following points : ( i ) we use the same procedure to perform the phrase training based adaptation and the search thus avoiding inconsistencies between the two ; ( ii ) we do not directly interpolate the original statistics with the new ones , but use a training procedure to manipulate the original statistics .
We perform experiments on the publicly available IWSLT TED task , on both Arabic-to -English and Germanto - English lectures translation tracks .
We compare our suggested phrase training adaptation method to a variety of baselines and show its effectiveness .
Finally , we experiment with mixture modeling based adaptation .
We compare mixture modeling to our adaptation method , and apply our method within a mixture modeling framework .
In Section 2 , we present the phrase training method and explain how it is utilized for adaptation .
Experimental setup including corpora statistics and the SMT system are described in Section 3 .
Section 4 summarizes the phrase training adaptation results ending with a comparison to mixture modeling .
Phrase Training
The standard phrase extraction procedure in SMT consists of two phases : ( i ) word- alignment training ( e.g. , IBM alignment models ) , ( ii ) heuristic phrase extraction and relative frequency based phrase translation probability estimation .
In this work , we utilize phrase training for the task of adaptation .
We use the forced alignment ( FA ) method ( Wuebker et al. , 2010 ) to perform the phrase alignment training and probability estimation .
We perform phrase training by running a normal SMT decoder on the training data and constrain the translation to the given target instance .
Using n-best possible phrase segmentation for each training instance , the phrase probabilities are re-estimated over the output .
Leaving - one- out is used during the forced alignment procedure phase to avoid over-fitting ( Wuebker et al. , 2010 ) .
In the standard phrase training procedure , we are given a training set y , from which an initial heuristics - based phrase table p 0 y is generated .
FA training is then done over the training set y using the phrases and probabilities in p 0 y ( possibly updated by the leaving - one- out method ) .
Finally , re-estimation of the phrase probabilities is done over the decoder output , generating the FA phrase table p 1 .
We explain next how to utilize FA training for adaptation .
Adaptation
In this work , we utilize phrase training for the task of adaptation .
The main idea is to generate the initial phrase table required for FA using a general - domain training data y , thus resulting in p 0 y , and perform the FA training over y IN , the in- domain training data ( instead of y in the standard procedure ) .
This way , we bias the probabilities of p 0 y towards the indomain distribution .
We denote this new procedure by Y'-FA-IN .
This differs from the standard IN - FA - IN by that we have more phrase pairs to use for FA .
Thus , we obtain phrase pairs relevant to IN in addition to " general " phrase pairs which were not extracted from IN , perhaps due to faulty word alignments .
The probabilities of the general phrase table will be tailored towards IN .
In practice , we usually have in - domain IN and other-domain OD data .
We denote by ALL the concatenation of IN and OD .
To adapt the ALL phrase table , we perform the FA procedure ALL -FA-IN .
We also utilize leaving - one - out to avoid over-fitting .
Another procedure we experimented with is adapting the OD phrase table using FA over IN , without leaving - one- out .
We denote it by OD -FA 0 -IN .
In this FA scenario , we do not use leaving - oneout as IN is not contained in OD , therefore , overfitting will not occur .
By this procedure , we train phrases from OD that are relevant for both OD and IN , while the probabilities will be tailored to IN .
In this case , we do not expect improvements over the IN based phrase table , but , improvements over OD and reduction in the phrase table size .
We compare our suggested FA based adaptation to the standard FA procedure .
3 Experimental Setup
Training Corpora
To evaluate the introduced methods experimentally , we use the IWSLT 2011 TED Arabic-to-English and German-to - English translation tasks .
The IWSLT 2011 evaluation campaign focuses on the translation of TED talks , a collection of lectures on a variety of topics ranging from science to culture .
For Arabic-to - English , the bilingual data consists of roughly 100K sentences of in- domain TED talks data and 8 M sentences of " other " - domain United Nations ( UN ) data .
For the German-to - English task , the data consists of 130K TED sentences and 2.1 M sentences of " other " - domain data assembled from the news -commentary and the europarl corpora .
For language model training purposes , we use an additional 1.4 billion words ( supplied as part of the campaign monolingual training data ) .
The bilingual training and test data for the Arabicto-English and German-to - English tasks are summarized in Table 1 1 .
The English data was tokenized and lowercased while the Arabic data was tokenized and segmented using MADA v3.1 ( Roth et al. , 2008 ) with the ATB scheme .
The German source is decompounded ( Koehn and Knight , 2003 ) and part- of-speech - based long- range verb reordering rules ( Popovi ? and Ney , 2006 ) are applied .
From Table 1 , we note that using the general data considerably reduces the number of out - of- vocabulary ( OOV ) words .
This comes with the price of increasing the size of the training data by a factor of more than 20 .
A simple concatenation of the corpora might mask the phrase probabilities obtained from the in-domain corpus , causing a deterioration in performance .
One way to avoid this contamination is by filtering the general corpus , but this discards phrase translations completely from the phrase model .
A more principled way is by adapting the phrase probabilities of the full system to the domain being tackled .
We perform this by phrase training the full phrase table over the in-domain training set .
Translation System
The baseline system is built using the open-source SMT toolkit Jane 2.0 , which provides a state- ofthe - art phrase - based SMT system ( Wuebker et al. , 2012a ) .
In addition to the phrase based decoder , Jane 2.0 implements the forced alignment procedure used in this work for the purpose of adaptation .
We use the standard set of models with phrase translation probabilities for source - to-target and target - tosource directions , smoothing with lexical weights , a word and phrase penalty , distance - based reordering and an n-gram target language model .
The SMT systems are tuned on the dev ( dev2010 ) development set with minimum error rate training ( Och , 2003 ) us-ing BLEU ( Papineni et al. , 2002 ) accuracy measure as the optimization criterion .
We test the performance of our system on the test ( tst2010 ) and eval ( tst2011 ) sets using the BLEU and translation edit rate ( TER ) ( Snover et al. , 2006 ) measures .
We use TER as an additional measure to verify the consistency of our improvements and avoid over-tuning .
The Arabic-English results are case sensitive while the German-English results are case insensitive .
Results For
The results of the various experiments over both Arabic-English and German-English tasks are summarized in Table 2 .
The usefulness of the OD data differs between the Arabic-to -English and the German-to - English translation tasks .
For Arabic-to - English , the OD system is 2.5 % -4.3 % BLEU worse than the IN system , whereas for the German- to - English task the differences between IN and OD are smaller and range from 0.9 % to 1.6 % BLEU .
The inferior performance of the OD system can be related to noisy data or bigger discrepancy between the OD data domain distribution and the IN distribution .
The ALL system performs according to the usefulness of the OD training set , where for Arabicto - English we observe deterioration in performance for all test sets and up - to - 0.9 % BLEU on the test set .
On the other hand , for German- to - English , the ALL system is improving over IN where the biggest improvement is observed on the eval set with + 0.9 % BLEU improvement .
The standard FA procedure achieves mixed results , where IN - FA deteriorates the results over the IN counterpart for Arabic- English , while improving for German- English .
ALL - FA performs comparably to the ALL system on both tasks , while reducing the phrase table size considerably .
The OD - FA system deteriorates the results in comparison to the OD system in most cases , which is expected as training over the OD set fits the phrase model on the OD domain , making it perform worse on IN .
( Wuebker et al. , 2012 b ) also report mixed results with FA training .
The FA adaptation results are summarized in the last block of the experiments .
The OD -FA 0 -IN improves over the OD system , which means that the training procedure was able to modify the OD probabilities to perform well on the IN data .
On the German-to- English task , the OD - FA 0 -IN performs comparably to the IN system , whereas for Arabicto-English OD - FA 0 -IN was able to close around half of the gap between OD and IN .
The FA adapted ALL system ( ALL - FA - IN ) performs best in our experiments , improving on both BLEU and TER measures .
In comparison to the best heuristics system ( IN for Arabic- English and ALL for German- English ) , + 0.4 % BLEU and - 0.6 % TER improvements are observed on the eval set for Arabic- English .
For German- English , the biggest improvements are observed on TER with - 0.8 % on test and - 0.5 % on eval .
The results suggest that ALL -FA - IN is able to learn more useful phrases than the IN system and adjust the ALL phrase probabilities towards the in-domain distribution .
Mixture Modeling
In this section , we compare our method to mixture modeling based adaptation , in addition to applying mixture modeling on top of our method .
We focus on linear interpolation ( Foster and Kuhn , 2007 ) of the in- domain ( IN ) and other -domain phrase tables , where we vary the latter between the heuristically extracted phrase table ( OD ) and the FA adapted one ( OD - FA 0 - IN ) .
The interpolation weight is uniform for the interpolated phrase tables ( 0.5 ) .
The results of mixture modeling are summarized in Table 3 .
In this table , we include the best heuristics based system ( Heuristics best ) from Table 2 as a reference system .
The results on the eval set are omitted as they show similar tendencies to the test set results .
Linear interpolation of IN and OD ( IN , OD ) is performing well in our experiments , with big improvements over the dev set , + 1.0 % BLEU for Arabic- to - English and + 0.4 % BLEU for German-to - English .
On the test set , we observe smaller improvements .
Interpolating IN with the phrase training adapted system OD - FA 0 -IN ( IN , OD - FA 0 - IN ) achieves additional gains over the IN , OD system , the biggest are observed on TER for German- to - English , with - 0.4 % and - 0.5 % improvements on the dev and test sets correspondingly .
Comparing heuristics based interpolation ( IN , OD ) to our best phrase training adapted system ( ALL - FA - IN ) shows mixed results .
For Arabic- to - English , the systems are comparable , while for the German- to - English test set , IN , OD is + 0.2 % BLEU better and + 0.8 % TER worse than ALL -FA-IN .
We hypothesize that for Arabic-to - English interpolation is important due to the larger size of the OD data , where it could reduce the masking of the IN training data by the much larger OD data .
Nevertheless , as mentioned previously , using phrase training adapted phrase table in a mixture setup consistently improves over using heuristically extracted tables .
Conclusions
In this work , we propose a phrase training procedure for adaptation .
The phrase training is implemented using the FA method .
First , we extract a standard phrase table using the whole available training data .
Using this table , we initialize the FA procedure and perform training on the in-domain set .
Experiments are done on the Arabic-to-English and German-to - English TED lectures translation tasks .
We show that the suggested procedure is improving over unadapted baselines .
On the Arabicto- English task , the FA adapted system is + 0.9 % BLEU better than the full unadapted counterpart on both test sets .
Unlike the Arabic-to - English setup , the German-to - English OD data is helpful and produces a strong unadapted baseline in concatenation with IN .
In this case , the FA adapted system achieves BLEU improvements mainly on the development set with + 0.6 % BLEU , on the test and eval sets , improvements of - 0.8 % and - 0.6 % TER are observed correspondingly .
As a side effect of the FA training process , the size of the adapted phrase table is less than 10 % of the size of the full table .
Finally , we experimented with mixture modeling where improvements are observed over the unadapted baselines .
The results show that using our phrase training adapted OD table yields better performance than using the heuristically extracted OD in a mixture framework .
Table 1 : 1 IWSLT 2011 TED bilingual corpora statistics : the number of tokens is given for the source side .
OOV / X denotes the number of OOV words in relation to corpus X ( the percentage is given in parentheses ) .
IN is the TED in- domain data , OD denotes other-domain data , ALL denotes the concatenation of IN and OD .
Set Sen Tok OOV / IN OOV / ALL German-to- English IN 130K 2.5M OD 2.1M 55 M dev 883 20 K 398 ( 2.0 % ) 215 ( 1.1 % ) test 1565 32 K 483 ( 1.5 % ) 227 ( 0.7 % ) eval 1436 27 K 490 ( 1.8 % ) 271 ( 1.0 % ) Arabic-to-English IN 90K 1.6M OD 7.9M 228 M dev 934 19 K 408 ( 2.2 % ) 184 ( 1.0 % ) test 1664 31 K 495 ( 1.6 % ) 228 ( 0.8 % ) eval 1450 27 K 513 ( 1.9 % ) 163 ( 0.6 % )
Table 3 : 3 TED 2011 mixture modeling results .
Heuristics best is the best heuristics based system , IN for Arabic-English and ALL for German- English .
X , Y denotes linear interpolation between X and Y phrase tables .
System dev test BLEU TER BLEU TER Arabic-to- English Heuristics best 27.2 54.1 25.3 57.1 IN , OD 28.2 53.1 25.5 56.8 IN , OD-FA 0 -IN 28.4 52.9 25.7 56.5 German-to- English Heuristics best 31.2 48.3 29.5 50.5 IN , OD 31.6 48.2 29.9 50.5 IN , OD-FA 0 -IN 31.8 47.8 30.0 50.0
For a list of the IWSLT TED 2011 training corpora , see http://www.iwslt2011.org/doku.php?
id=06 _evaluation
