title
Shallow Local Multi Bottom-up Tree Transducers in Statistical Machine Translation
abstract
We present a new translation model integrating the shallow local multi bottomup tree transducer .
We perform a largescale empirical evaluation of our obtained system , which demonstrates that we significantly beat a realistic tree - to- tree baseline on the WMT 2009 English ?
German translation task .
As an additional contribution we make the developed software and complete tool - chain publicly available for further experimentation .
Introduction Besides phrase - based machine translation systems ( Koehn et al. , 2003 ) , syntax - based systems have become widely used because of their ability to handle non-local reordering .
Those systems use synchronous context-free grammars ( Chiang , 2007 ) , synchronous tree substitution grammars ( Eisner , 2003 ) or even more powerful formalisms like synchronous tree-sequence substitution grammars ( Sun et al. , 2009 ) .
However , those systems use linguistic syntactic annotation at different levels .
For example , the systems proposed by Wu ( 1997 ) and Chiang ( 2007 ) use no linguistic information and are syntactic in a structural sense only .
Huang et al. ( 2006 ) and Liu et al . ( 2006 ) use syntactic annotations on the source language side and show significant improvements in translation quality .
Using syntax exclusively on the target language side has also been successfully tried by Galley et al . ( 2004 ) and Galley et al . ( 2006 ) .
Nowadays , open-source toolkits such as Moses ( Koehn et al. , 2007 ) offer syntax - based components ( Hoang et al. , 2009 ) , which allow experiments without expert knowledge .
The improvements observed for systems using syntactic annotation on either the source or the target language side naturally led to experiments with models that use syntactic annotations on both sides .
However , as noted by Lavie et al . ( 2008 ) , Liu et al. ( 2009 ) , and Chiang ( 2010 ) , the integration of syntactic information on both sides tends to decrease translation quality because the systems become too restrictive .
Several strategies such as ( i ) using parse forests instead of single parses ( Liu et al. , 2009 ) or ( ii ) soft syntactic constraints ( Chiang , 2010 ) have been developed to alleviate this problem .
Another successful approach has been to switch to more powerful formalisms , which allow the extraction of more general rules .
A particularly powerful model is the non-contiguous version of synchronous tree-sequence substitution grammars ( STSSG ) of Zhang et al . ( 2008a ) , Zhang et al. ( 2008 b ) , and Sun et al . ( 2009 ) , which allows sequences of trees on both sides of the rules [ see also ( Raoult , 1997 ) ] .
The multi bottom - up tree transducer ( MBOT ) of Arnold and Dauchet ( 1982 ) and Lilin ( 1978 ) offers a middle ground between traditional syntax - based models and STSSG .
Roughly speaking , an MBOT is an STSSG , in which all the discontinuities must occur on the target language side ( Maletti , 2011 ) .
This restriction yields many algorithmic advantages over both the traditional models as well as STSSG as demonstrated by Maletti ( 2010 ) .
Formally , they are expressive enough to express all sensible translations ( Maletti , 2012 ) 1 . Figure 2 displays sample rules of the MBOT variant , called MBOT , that we use ( in a graphical representation of the trees and the alignment ) .
In this contribution , we report on our novel statistical machine translation system that uses an MBOT - based translation model .
The theoretical foundations of MBOT and their integration into our translation model are presented in Sections 2 and 3 .
In order to empirically evaluate the MBOT model , we implemented a machine trans - 1 A translation is sensible if it is of linear size increase and can be computed by some ( potentially copying ) top-down tree transducer .
lation system that we are going to make available to the public .
We implemented MBOT inside the syntax - based component of the Moses open source toolkit .
Section 4 presents the most important algorithms of our MBOT decoder .
We evaluate our new system on the WMT 2009 shared translation task English ? German .
The translation quality is automatically measured using BLEU scores , and we confirm the findings by providing linguistic evidence ( see Section 5 ) .
Note that in contrast to several previous approaches , we perform large scale experiments by training systems with approx .
1.5 million parallel sentences .
Theoretical Model
In this section , we present the theoretical generative model used in our approach to syntax - based machine translation .
Essentially , it is the local multi bottom - up tree transducer of Maletti ( 2011 ) with the restriction that all rules must be shallow , which means that the left - hand side of each rule has height at most 2 ( see Figure 2 for shallow rules and Figure 4 for rules including non-shallow rules ) .
The rules extracted from the training example of Figure 3 are displayed in Figure 4 .
Those extracted rules are forcibly made shallow by removing internal nodes .
The application of those rules is illustrated in Figures 5 and 6 .
For those that want to understand the inner workings , we recall the principal model in full detail in the rest of this section .
Since we utilize syntactic parse trees , let us introduce trees first .
Given an alphabet ?
of labels , the set T ? of all ?- trees is the smallest set T such that ?( t 1 , . . . , t k ) ?
T for all ? ? ? , integer k ? 0 , and t 1 , . . . , t k ?
T .
Intuitively , a tree t consists of a labeled root node ? followed by a sequence t 1 , . . . , t k of its children .
A tree t ?
T ? is shallow if t = ?( t 1 , . . . , t k ) with ? ? ? and t 1 , . . . , t k ? ?.
To address a node inside a tree , we use its position , which is a word consisting of positive integers .
Roughly speaking , the root of a tree is addressed with the position ? ( the empty word ) .
The position iw with i ?
N addresses the position w in the i th direct child of the root .
In this way , each node in the tree is assigned a unique position .
We illustrate this notion in Figure 1 . Formally , the positions pos ( t ) ?
N * of a tree t = ?( t 1 , . . . , t k ) are inductively defined by pos ( t ) = {?} ? pos ( k ) ( t 1 , . . . , t k ) , where pos ( k ) ( t 1 , . . . , t k ) = 1 ?i? k
{ iw | w ? pos( t i ) } . Let t ? T ? and w ? pos ( t ) .
The label of t at position w is t ( w ) , and the subtree rooted at position w is t| w .
These notions are also illustrated in Figure 1 . A position w ? pos ( t ) is a leaf ( in t ) if w1 / ? pos ( t ) .
In other words , leaves do not have any children .
Given a subset N ? ? , we let leaf N ( t ) = {w ? pos ( t ) | t( w ) ?
N , w leaf in t} be the set of all leaves labeled by elements of N .
When N is the set of nonterminals , we call them leaf nonterminals .
We extend this notion to sequences t 1 , . . . , t k ?
T ? by leaf ( k ) N ( t 1 , . . . , t k ) = 1 ?i? k { iw | w ? leaf N ( t i ) }.
Let w 1 , . . . , w n ? pos ( t ) be ( pairwise prefixincomparable ) positions and t 1 , . . . , t n ?
T ? . Then t[ w i ? t i ] 1 ?i? n denotes the tree that is obtained from t by replacing ( in parallel ) the subtrees at w i by t i for every 1 ? i ? n. Now we are ready to introduce our model , which is a minor variation of the local multi bottom - up tree transducer of Maletti ( 2011 ) . Let ? and ? be the input and output symbols , respectively , and let N ? ? ? ? be the set of nonterminal symbols .
Essentially , the model works on pairs t , ( u 1 , . . . , u k ) consisting of an input tree t ?
T ? and a sequence u 1 , . . . , u k ?
T ? of output trees .
Such pairs are pre-translations of rank k .
The pretranslation t , ( u 1 , . . . , u k ) is shallow if all trees t , u 1 , . . . , u k in it are shallow .
Together with a pre-translation we typically have to store an alignment .
Given a pre-translation t , ( u 1 , . . . , u k ) of rank k and 1 ? i ? k , we call u i the i th translation of t.
An alignment for this pre-translation is an injective mapping ? : leaf ( k ) N ( u 1 , . . . , u k ) ? leaf N ( t ) ?
N such that if ( w , j ) ? ran ( ? ) , then also ( w , i ) ? ran ( ? ) for all 1 ? j ? i. 2
In other words , if an alignment requests the i th translation , then it should also request all previous translations .
Definition 1
A shallow local multi bottom - up tree transducer ( MBOT ) is a finite set R of rules together with a mapping c : R ?
R such that every rule , written t ? ? ( u 1 , . . . , u k ) , contains a shallow pre-translation t , ( u 1 , . . . , u k ) and an alignment ? for it .
The components t , ( u 1 , . . . , u k ) , ? , and c ( ? ) are called the left - hand side , the right - hand side , the alignment , and the weight of the rule ? = t ? ? ( u 1 , . . . , u k ) .
Figure 2 shows two example MBOT rules ( without weights ) .
Overall , the rules of an MBOT are similar to the rules of an SCFG ( synchronous context- free grammar ) , but our right - hand sides contain a sequence of trees instead of just a single tree .
In addition , the alignments in an SCFG rule are bijective between leaf nonterminals , whereas our model permits multiple alignments to a single leaf nonterminal in the left - hand side ( see Figure 2 ) .
Our MBOT rules are obtained automatically from data like that in Figure 3 .
Thus , we ( word ) align the bilingual text and parse it in both the source and the target language .
In this manner we obtain sentence pairs like the one shown in Figure 3 .
To these sentence pairs we apply the rule extraction method of Maletti ( 2011 ) .
The rules extracted from the sentence pair of Figure 3 are shown in Figure 4 . Note that these rules are not necessarily shallow ( the last two rules are not ) .
Thus , we post-process the extracted rules and make them shallow .
The shallow rules corresponding to the non-shallow rules of Figure 4 are shown in Figure 2 . Next , we define how to combine rules to form derivations .
In contrast to most other models , we only introduce a derivation semantics that does not collapse multiple derivations for the same input-output pair .
3
We need one final notion .
Let ? = t ? ? ( u 1 , . . . , u k ) be a rule and w ? leaf N ( t ) be a leaf nonterminal ( occurrence ) in the left - hand side .
The w-rank rk ( ? , w ) of the rule ? is rk ( ? , w ) = max { i ? N | ( w , i ) ? ran ( ? ) } .
For example , for the lower rule ? in Figure 2 we have rk ( ? , 1 ) = 1 , rk ( ? , 2 ) = 2 , and rk ( ? , 3 ) = 1 . Definition 2
The set ? ( R , c ) of weighted pretranslations of an MBOT ( R , c ) is the smallest set T subject to the following restriction :
If there exist ? c = c( ? ) ? w?leaf N ( t ) c w , and ?
a rule ? = t ? ? ( u 1 , . . . , u k ) ? R , ? a weighted pre-translation t w , c w , ( u w 1 , . . . , u w kw ) ?
T for every w ? leaf N ( t ) with - rk ( ? , w ) = k w , 4 -t ( w ) = t w ( ? ) , 5 and - for every iw ? leaf ( k ) N ( u 1 , . . . , u k ) , 6 u i ( w ) = u v j ( ? ) with ?( iw ) = ( v , j ) , then t , c , ( u 1 , . . . , u k ) ?
T is a weighted pre- translation , where ? t = t[ w ? t w | w ? leaf N ( t ) ] , ? u i = u i [ iw ? u v j | ?( iw ) = ( v , j ) ] for every 1 ? i ? k. Rules that do not contain any nonterminal leaves are automatically weighted pre-translations with their associated rule weight .
Otherwise , each nonterminal leaf w in the left - hand side of a rule ?
must be replaced by the input tree t w of a pretranslation t w , c w , ( u w 1 , . . . , u w kw ) , whose root is labeled by the same nonterminal .
In addition , the rank rk ( ? , w ) of the replaced nonterminal should match the number k w of components in the selected weighted pre-translation .
Finally , the nonterminals in the right - hand side that are aligned to w should be replaced by the translation that the alignment requests , provided that the nonterminal matches with the root symbol of the requested translation .
The weight of the new pre-translation is obtained simply by multiplying the rule weight and the weights of the selected weighted pretranslations .
The overall process is illustrated in Figures 5 and 6 .
Translation Model Given a source language sentence e , our translation model aims to find the best corresponding target language translation ? ; 7 i.e. , ? = arg max g p( g |e ) .
We estimate the probability p( g |e ) through a loglinear combination of component models with parameters ?
m scored on the pre-translations t , ( u ) such that the leaves of t concatenated read e. 8 p( g |e ) ?
7 m=1 h m t , ( u ) ?m
Our model uses the following features h m ( t , ( u 1 , . . . , u k ) ) for a general pre-translation ? = t , ( u 1 , . . . , u k ) : 7
Our main translation direction is English to German .
8 Actually , t must embed in the parse tree of e ; see Section 4 . ( 1 )
The forward translation weight using the rule weights as described in Section 2 ( 2 )
The indirect translation weight using the rule weights as described in Section 2 ( 3 ) Lexical translation weight source ? target ( 4 ) Lexical translation weight target ? source ( 5 ) Target side language model ( 6 ) Number of words in the target sentences ( 7 ) Number of rules used in the pre-translation ( 8 ) Number of target side sequences ; here k times the number of sequences used in the pretranslations that constructed ?
( gap penalty )
The rule weights required for ( 1 ) are relative frequencies normalized over all rules with the same left - hand side .
In the same fashion the rule weights required for ( 2 ) are relative frequencies normalized over all rules with the same righthand side .
Additionally , rules that were extracted at most 10 times are discounted by multiplying the rule weight by 10 ?2 .
The lexical weights for ( 2 ) and ( 3 ) are obtained by multiplying the word translations w( g i |e j ) [ respectively , w( e j |g i ) ] of lexically aligned words ( g i , e j ) accross ( possibly discontiguous ) target side sequences .
9
Whenever a source word e j is aligned to multiple target words , we average over the word translations .
10 h 3 ( t , ( u 1 , . . . , u k ) ) = lexical item e occurs in t average { w ( g | e ) | g aligned to e}
The computation of the language model estimates for ( 6 ) is adapted to score partial translations consisting of discontiguous units .
We explain the details in Section 4 .
Finally , the count c of target sequences obtained in ( 7 ) is actually used as a score 100 1 ? c .
This discourages rules with many target sequences .
Decoding
We implemented our model in the syntax - based component of the Moses open-source toolkit by Koehn et al . ( 2007 ) and Hoang et al . ( 2009 ) .
The standard Moses syntax - based decoder only handles SCFG rules ; i.e , rules with contiguous components on the source and the target language side .
Roughly speaking , SCFG rules are MBOT rules with exactly one output tree .
We thus had to extend the system to support our MBOT rules , in which arbitrarily many output trees are allowed .
The standard Moses syntax - based decoder uses a CYK + chart parsing algorithm , in which each source sentence is parsed and contiguous spans are processed in a bottom - up fashion .
A rule is applicable 11 if the left - hand side of it matches the nonterminal assigned to the full span by the parser and the ( non - ) terminal assigned to each subspan .
12
In order to speed up the decoding , cube pruning ( Chiang , 2007 ) is applied to each chart cell in order to select the most likely hypotheses for subspans .
The language model ( LM ) scoring is directly integrated into the cube pruning algorithm .
Thus , LM estimates are available for all considered hypotheses .
To accommodate MBOT rules , we had to modify the Moses syntax - based decoder in several ways .
First , the rule representation itself is adjusted to allow sequences of shallow output trees on the target side .
Naturally , we also had to adjust hypothesis expansion and , most importantly , language model scoring inside the cube pruning algorithm .
An overview of the modified pruning procedure is given in Algorithm 1 .
The most important modifications are hidden in lines 5 and 8 .
The expansion in Line 5 involves matching all nonterminal leaves in the rule as defined in Definition 2 , which includes matching all leaf nonterminals in all ( discontiguous ) output trees .
Because the output trees can remain discontiguous after hypothesis creation , LM scoring has to be done individually over all output trees .
Algorithm 2 describes our LM scoring in detail .
In it we use k strings w 1 , . . . , w k to collect the lexical information from the k output com-11 Note that our notion of applicable rules differs from the default in Moses .
12
Theoretically , this allows that the decoder ignores unary parser nonterminals , which could also disappear when we make our rules shallow ; e.g. , the parse tree left in the pretranslation of Figure 5 can be matched by a rule with lefthand side NP ( Official , forecasts ) .
Algorithm 1 Cube pruning with MBOT rules
Data structures : r[ i , j ] : list of rules matching span e [ i . . . j] h [ i , j ] : hypotheses covering span e [ i . . . j] c [ i , j ] : cube of hypotheses covering span e [ i . .
. j] 1 : for all MBOT rules ? covering span e [ i . . . j] do 2 : Insert ? into r[ i , j] 3 : Sort r[ i , j] 4 : for all ( l ? ? r) ? r[ i , j ] do 5 : Create h[ i , j ] by expanding all nonterminals in l with best scoring hypotheses for subspans 6 : Add h [ i , j] to c [ i , j] 7 : for all hypotheses h ? c [ i , j ] do 8 : Estimate LM score for h // see Algorithm 2 9 : Estimate remaining feature scores 10 : Sort c [ i , j] 11 : Retrieve first ? elements from c [ i , j ] // we use ? = 10 3 ponents ( u 1 , . . . , u k ) of a rule .
These strings can later be rearranged in any order , so we LM - score all of them separately .
Roughly speaking , we obtain w i by traversing u i depth-first left-to- right .
If we meet a lexical element ( terminal ) , then we add it to the end of w i .
On the other hand , if we meet a nonterminal , then we have to consult the best pre-translation ? = t , ( u 1 , . . . , u k ) , which will contribute the subtree at this position .
Suppose that u j will be substituted into the nonterminal in question .
Then we first LM - score the pretranslation ? to obtain the string w j corresponding to u j .
This string w j is then appended to w i .
Once all the strings are built , we score them using our 4 - gram LM .
The overall LM score for the pretranslation is obtained by multiplying the scores for w 1 , . . . , w k .
Clearly , this treats w 1 , . . . , w k as k separate strings , although they eventually will be combined into a single string .
Whenever such a concatenation happens , our LM scoring will automatically compute n-gram LM scores based on the concatenation , which in particular means that the LM scores get more accurate for larger spans .
Finally , in the final rule only one component is allowed , which yields that the LM indeed scores the complete output sentence .
Figure 7 illustrates our LM scoring for a pretranslation involving a rule with two ( discontiguous ) target sequences ( the construction of the pretranslation is illustrated in Figure 6 ) .
When processing the rule rooted at S , an LM estimate is computed by expanding all nonterminal leaves .
In our case , these are NP , VAFIN , PP , and VVPP .
However , the nodes VAFIN and VVPP are assembled from a ( discontiguous ) tree sequence .
This means that those units have been considered as in - Algorithm 2 LM scoring Data structures : -( u1 , . . . , u k ) : right - hand side of a rule -( w1 , . . . , w k ) : k strings all initially empty 1 : score = 1 2 : for all 1 ? i ? k do 3 : for all leaves in ui ( in lexicographic order ) do 4 : if is a terminal then 5 : Append to wi 6 : else 7 : LM score the best hypothesis for the subspan 8 : Expand wi by the corresponding w j 9 : score = score ? LM ( wi ) dependent until now .
So far , the LM scorer could only score their associated unigrams .
However , we also have their associated strings w 1 and w 2 , which can now be used .
Since VAFIN and VVPP now become parts of a single tree , we can perform LM scoring normally .
Assembling the string we obtain Offizielle Prognosen sind von nur 3 % ausgegangen which is scored by the LM .
Thus , we first score the 4 - grams " Offizielle Prognosen sind von " , then " Prognosen sind von nur " , etc .
Experiments
Setup
The baseline system for our experiments is the syntax - based component of the Moses opensource toolkit of Koehn et al . ( 2007 ) and Hoang et al . ( 2009 ) .
We use linguistic syntactic annotation on both the source and the target language side ( tree-to-tree ) .
Our contrastive system is the MBOT - based translation system presented here .
We provide the system with a set of SCFG as well as MBOT rules .
We do not impose any maximal span restriction on either system .
The compared systems are evaluated on the English-to - German 13 news translation task of WMT 2009 ( Callison - Burch et al. , 2009 .
For both systems , the used training data is from the 4th version of the Europarl Corpus ( Koehn , 2005 ) and the News Commentary corpus .
Both translation models were trained with approximately 1.5 million bilingual sentences after length- ratio filtering .
The word alignments were generated by GIZA ++ ( Och and Ney , 2003 ) with the growdiag -final - and heuristic English side of the bilingual data was parsed using the Charniak parser of Charniak and Johnson ( 2005 ) , and the German side was parsed using BitPar ( Schmid , 2004 ) without the function and morphological annotations .
Our German 4 gram language model was trained on the German sentences in the training data augmented by the Stuttgart SdeWaC corpus ( Web-as- Corpus Consortium , 2008 ) , whose generation is detailed in ( Baroni et al. , 2009 ) .
The weights ?
m in the log-linear model were trained using minimum error rate training ( Och , 2003 ) with the News 2009 development set .
Both systems use glue-rules , which allow them to concatenate partial translations without performing any reordering .
Results
We measured the overall translation quality with the help of 4 - gram BLEU ( Papineni et al. , 2002 ) , which was computed on tokenized and lowercased data for both systems .
The results of our evaluation are reported in Table 1 .
For comparison , we also report the results obtained by a system that utilizes parses only on the source side ( Moses tree-to-string ) with its standard features .
We can observe from Table 1 that our MBOTbased system outperforms the baseline .
We obtain a BLEU score of 13.06 , which is a gain of 0.46 BLEU points over the baseline .
This improvement is statistically significant at confidence p < 0.05 , which we computed using the pairwise bootstrap resampling technique of Koehn ( 2004 ) .
Our system is also better than the Moses tree-tostring system .
However this improvement ( 0.34 ) is not statistically significant .
In the next section , we confirm the result of the automatic evaluation through a manual examination of some translations generated by our system and the baseline .
In items .
The label non-term stands for rules containing at least one leaf nonterminal .
The results show that approx .
6 % of all rules used by our MBOTsystem have discontiguous target sides .
Furthermore , the reported numbers show that the system also uses rules in which lexical items are combined with nonterminals .
Finally ,
Table 3 presents the number of rules with k target side components used during decoding .
Linguistic Analysis
In this section we present linguistic evidence supporting the fact that the MBOT - based system significantly outperforms the baseline .
All examples are taken from the translation of the test set used for automatic evaluation .
We show that when system generates better translations , this is directly related to the use of MBOT rules .
Figures 8 and 9 show the ability of our system to correctly reorder multiple segments in the source sentence where the baseline translates those segments sequentially .
An analysis of the generated derivations shows that our system produces the correct translation by taking advantage of rules with discontiguous units on target language side .
The rules used in the presented derivations are displayed in Figures 10 and 11 .
In the first example ( Figure 8 ) , we begin by translating " ( ( smuggle ) VB ( eight projectiles ) NP ( into the kingdom ) PP ) VP " into the discontiguous sequence composed of ( i ) " ( acht geschosse ) NP " ; ( ii ) " ( in das k?nigreich ) PP " and ( iii ) " ( schmuggeln ) VP " .
In a second step we assemble all sequences in a rule with contiguous target language side and , at the same time , insert the word " ( zu ) PTKZU " between " ( in das k?nigreich ) PP " and " ( schmuggeln ) VP " .
The second example ( Figure 9 ) illustrates a more complex reordering .
First , we trans - late " ( ( again ) ADV commented on ( the problem of global warming ) NP ) VP " into the discontiguous sequence composed of ( i ) " ( das problem der globalen erw?rmung ) NP " ; ( ii ) " ( wieder ) ADV " and ( iii ) " ( kommentiert ) VPP " .
In a second step , we translate the auxiliary " ( has ) VBZ " by inserting " ( hat ) VAFIN " into the sequence .
We thus obtain , for the input segment " ( ( has ) VBZ ( again ) ADV commented on ( the problem of global warming ) NP ) VP " , the sequence ( i ) " ( das problem der globalen erw?rmung ) NP " ; ( ii ) " ( hat ) VAFIN " ; ( iii ) " ( wieder ) ADV " ; ( iv ) " ( kommentiert ) VVPP " .
In a last step , the constituent " ( president v?clav klaus ) NP " is inserted between the discontiguous units " ( hat ) VAFIN " and " ( wieder ) ADV " to form the contiguous sequence " ( ( das problem der globalen erw?rmung ) NP ( hat ) VAFIN ( pr?sident v?clav klaus ) NP ( wieder ) ADV ( kommentiert ) VVPP ) TOP " .
Figures 12 and 13 show examples where our system generates complex words in the target language out of a simple source language word .
Again , an analysis of the generated derivation shows that MBOT takes advantage of rules having several target side components .
Examples of such rules are given in Figure 14 .
Through its ability to use these discontiguous rules , our system correctly translates into reflexive or particle verbs such as " konzentriert sich " ( for the English " focuses " ) or " besteht darauf " ( for the English " insist " ) .
Another phenomenon well handled by our system are relative pronouns .
Pronouns such as " that " or " whose " are systematically translated into both both , " , " and " dass " or " , " and " deren " ( Figure 12 ) .
Conclusion and Future Work
We demonstrated that our MBOT - based machine translation system beats a standard tree - to - tree system ( Moses tree-to- tree ) on the WMT 2009 translation task English ? German .
To achieve this we implemented the formal model as described in Section 2 inside the Moses machine translation toolkit .
Several modifications were necessary to obtain a working system .
We publicly release all our developed software and our complete tool - chain to allow independent experiments and evaluation .
This includes our MBOT decoder Besides the automatic evaluation , we also performed a small manual analysis of obtained translations and show- cased some examples ( see Section 5.3 ) .
We argue that our MBOT approach can adequately handle discontiguous phrases , which occur frequently in German .
Other languages that exhibit such phenomena include Czech , Dutch , Russian , and Polish .
Thus , we hope that our system can also successfully be applied for other language pairs , which we plan to pursue as well .
In other future work , we want to investigate full backwards application of MBOT rules , which would be more suitable for the converse translation direction German ?
English .
The current independent LM scoring of components has some negative side-effects that we plan to circumvent with the use of lazy LM scoring .
Figure 1 : 1 Figure 1 : Example tree t with indicated positions .
We have t ( 21 ) = VBD and t| 221 is the subtree marked in red .
