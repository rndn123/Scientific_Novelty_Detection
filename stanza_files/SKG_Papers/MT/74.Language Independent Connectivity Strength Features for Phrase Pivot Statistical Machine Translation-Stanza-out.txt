title
Language Independent Connectivity Strength Features for Phrase Pivot Statistical Machine Translation
abstract
An important challenge to statistical machine translation ( SMT ) is the lack of parallel data for many language pairs .
One common solution is to pivot through a third language for which there exist parallel corpora with the source and target languages .
Although pivoting is a robust technique , it introduces some low quality translations .
In this paper , we present two language - independent features to improve the quality of phrase-pivot based SMT .
The features , source connectivity strength and target connectivity strength reflect the quality of projected alignments between the source and target phrases in the pivot phrase table .
We show positive results ( 0.6 BLEU points ) on Persian - Arabic SMT as a case study .
Introduction
One of the main issues in statistical machine translation ( SMT ) is the scarcity of parallel data for many language pairs especially when the source and target languages are morphologically rich .
A common SMT solution to the lack of parallel data is to pivot the translation through a third language ( called pivot or bridge language ) for which there exist abundant parallel corpora with the source and target languages .
The literature covers many pivoting techniques .
One of the best performing techniques , phrase pivoting ( Utiyama and Isahara , 2007 ) , builds an induced new phrase table between the source and target .
One of the main issues of this technique is that the size of the newly created pivot phrase table is very large ( Utiyama and Isahara , 2007 ) .
Moreover , many of the produced phrase pairs are of low quality which affects the translation choices during decoding and the overall translation quality .
In this paper , we introduce language independent features to determine the quality of the pivot phrase pairs between source and target .
We show positive results ( 0.6 BLEU points ) on Persian - Arabic SMT .
Next , we briefly discuss some related work .
We then review two common pivoting strategies and how we use them in Section 3 .
This is followed by our approach to using connectivity strength features in Section 4 .
We present our experimental results in Section 5 .
Related Work Many researchers have investigated the use of pivoting ( or bridging ) approaches to solve the data scarcity issue ( Utiyama and Isahara , 2007 ; Wu and Wang , 2009 ; Khalilov et al. , 2008 ; Bertoldi et al. , 2008 ; Habash and Hu , 2009 ) .
The main idea is to introduce a pivot language , for which there exist large source-pivot and pivot-target bilingual corpora .
Pivoting has been explored for closely related languages ( Haji ?
et al. , 2000 ) as well as unrelated languages ( Koehn et al. , 2009 ; Habash and Hu , 2009 ) .
Many different pivot strategies have been presented in the literature .
The following three are perhaps the most common .
The first strategy is the sentence translation technique in which we first translate the source sentence to the pivot language , and then translate the pivot language sentence to the target language ( Khalilov et al. , 2008 ) .
The second strategy is based on phrase pivoting ( Utiyama and Isahara , 2007 ; Cohn and Lapata , 2007 ; Wu and Wang , 2009 ) .
In phrase pivoting , a new source - target phrase table ( translation model ) is induced from source-pivot and pivottarget phrase tables .
Lexical weights and translation probabilities are computed from the two translation models .
The third strategy is to create a synthetic sourcetarget corpus by translating the pivot side of source-pivot corpus to the target language using an existing pivot-target model ( Bertoldi et al. , 2008 ) .
In this paper , we build on the phrase pivoting approach , which has been shown to be the best with comparable settings ( Utiyama and Isahara , 2007 ) .
We extend phrase table scores with two other features that are language independent .
Since both Persian and Arabic are morphologically rich , we should mention that there has been a lot of work on translation to and from morphologically rich languages ( Yeniterzi and Oflazer , 2010 ; Elming and Habash , 2009 ; El Kholy and Habash , 2010a ; Habash and Sadat , 2006 ; Kathol and Zheng , 2008 ) .
Most of these efforts are focused on syntactic and morphological processing to improve the quality of translation .
To our knowledge , there has n't been a lot of work on Persian and Arabic as a language pair .
The only effort that we are aware of is based on improving the reordering models for Persian - Arabic SMT ( Matusov and K?pr? , 2010 ) .
Pivoting Strategies
In this section , we review the two pivoting strategies that are our baselines .
We also discuss how we overcome the large expansion of source-totarget phrase pairs in the process of creating a pivot phrase table .
Sentence Pivoting
In sentence pivoting , English is used as an interface between two separate phrase - based MT systems ; Persian - English direct system and English -Arabic direct system .
Given a Persian sentence , we first translate the Persian sentence from Persian to English , and then from English to Arabic .
Phrase Pivoting
In phrase pivoting ( sometimes called triangulation or phrase table multiplication ) , we train a Persian - to - Arabic and an English - Arabic translation models , such as those used in the sentence pivoting technique .
Based on these two models , we induce a new Persian - Arabic translation model .
Since we build our models on top of Moses phrase - based SMT ( Koehn et al. , 2007 ) , we need to provide the same set of phrase translation probability distributions .
1 We follow Utiyama and Isahara ( 2007 ) in computing the probability distributions .
The following are the set of equations used to compute the lexical probabilities ( ? ) and the phrase probabilities ( p w ) ?( f | a ) = e ?( f |e ) ?( e|a ) ?( a|f ) = e ?( a|e ) ? ( e|f ) p w ( f |a ) = e p w ( f |e ) p w ( e|a ) p w ( a| f ) = e p w ( a| e ) p w ( e|f ) where f is the Persian source phrase .
e is the English pivot phrase that is common in both Persian - English translation model and English -Arabic translation model .
a is the Arabic target phrase .
We also build a Persian - Arabic reordering table using the same technique but we compute the reordering weights in a similar manner to Henriquez et al . ( 2010 ) .
As discussed earlier , the induced Persian - Arabic phrase and reordering tables are very large .
Table 1 shows the amount of parallel corpora used to train the Persian - English and the English -Arabic and the equivalent phrase table sizes compared to the induced Persian - Arabic phrase table .
2
We introduce a basic filtering technique discussed next to address this issue and present some baseline experiments to test its performance in Section 5.3 .
Filtering for Phrase Pivoting
The main idea of the filtering process is to select the top [ n ]
English candidate phrases for each Persian phrase from the Persian - English phrase table and similarly select the top [ n ]
Arabic target phrases for each English phrase from the English - Arabic phrase table and then perform the pivoting process described earlier to create a pivoted
We compare the different pivoting strategies and various filtering thresholds in Section 5.3 .
Approach
One of the main challenges in phrase pivoting is the very large size of the induced phrase table .
It becomes even more challenging if either the source or target language is morphologically rich .
The number of translation candidates ( fanout ) increases due to ambiguity and richness ( discussed in more details in Section 5.2 ) which in return increases the number of combinations between source and target phrases .
Since the only criteria of matching between the source and target phrase is through a pivot phrase , many of the induced phrase pairs are of low quality .
These phrase pairs unnecessarily increase the search space and hurt the overall quality of translation .
To solve this problem , we introduce two language - independent features which are added to the log linear space of features in order to determine the quality of the pivot phrase pairs .
We call these features connectivity strength features .
Connectivity Strength Features provide two scores , Source Connectivity Strength ( SCS ) and Target Connectivity Strength ( TCS ) .
These two scores are similar to precision and recall metrics .
They depend on the number of alignment links between words in the source phrase to words of the target phrase .
SCS and TSC are defined in equations 1 and 2 where S = { i : 1 ? i ?
S} is the set of source words in a given phrase pair in the pivot phrase table and T = { j : 1 ? j ?
T } is the set of the equivalent target words .
The word alignment between S and T is defined as A = { ( i , j ) : i ?
S and j ?
T }. SCS = | A| | S| ( 1 ) T CS = | A| | T | ( 2 ) We get the alignment links by projecting the alignments of source-pivot to the pivot-target phrase pairs used in pivoting .
If the source- target phrase pair are connected through more than one pivot phrase , we take the union of the alignments .
In contrast to the aggregated values represented in the lexical weights and the phrase probabilities , connectivity strength features provide additional information by counting the actual links between the source and target phrases .
They provide an independent and direct approach to measure how good or bad a given phrase pair are connected .
Figure 1 and 2 are two examples ( one good , one bad ) Persian - Arabic phrase pairs in a pivot phrase table induced by pivoting through English .
3
In the first example , each Persian word is aligned to an Arabic word .
The meaning is preserved in both phrases which is reflected in the SCS and TCS scores .
In the second example , only one Persian word in aligned to one Arabic word in the equivalent phrase and the two phrases conveys two different meanings .
The English phrase is not a good translation for either , which leads to this bad pairing .
This is reflected in the SCS and TCS scores .
Experiments
In this section , we present a set of baseline experiments including a simple filtering technique to overcome the huge expansion of the pivot phrase table .
Then we present our results in using connectivity strength features to improve Persian - Arabic pivot translation quality .
Persian : " A?tmAd " myAn " dw " k? wr " " " " " ' ? %$# " ? " ? ) ( " ? " ? " ?.- , ? ' " " " " " " " " " " " " " " ' trust " between " the" two " countries ' " English : " trust " between " the " two " countries " Arabic : " " Al ?q ? " byn" Aldwltyn " " " " " " ' ?210 / ? " 34 " ?25 ? 2$ 3 ? ' " " " " " " " " " " " " " " ' the " trust " between " the" two " countries ' " Persian : " Ay jAd " cnd " ?rkt" m?trk " " " " " ' ?$# " ? " &'( " ) *+ , " ?0 / .+? ' " " " " " " " " " " " " " " ' Establish " few " joint " companies ' " English : " joint " ventures " Arabic : " " b?D " ? rkAt " AlmqAwlAt " fy " Albld " " ' 123 " ? , +5 " ? " ?98 " ?6 ? " :; " ?>=<&? ' " " " " " " " " " " " " " " ' Some " construcBon " companies " in " the " country ' "
Experimental Setup
In our pivoting experiments , we build two SMT models .
One model to translate from Persian to English and another model to translate from English to Arabic .
The English -Arabic parallel corpus is about 2.8 M sentences ( ? 60 M words ) available from LDC 4 and GALE 5 constrained data .
We use an in-house Persian - English parallel corpus of about 170K sentences and 4 M words .
Word alignment is done using GIZA ++ ( Och and Ney , 2003 ) .
For Arabic language modeling , we use 200M words from the Arabic Gigaword Corpus ( Graff , 2007 ) together with the Arabic side of our training data .
We use 5 - grams for all language models ( LMs ) implemented using the SRILM toolkit ( Stolcke , 2002 ) .
For English language modeling , we use English Gigaword Corpus with 5 - gram LM using the KenLM toolkit ( Heafield , 2011 ) .
All experiments are conducted using the Moses phrase - based SMT system ( Koehn et al. , 2007 ) .
We use MERT ( Och , 2003 ) for decoding weight optimization .
For Persian - English translation model , weights are optimized using a set 1000 sentences randomly sampled from the parallel corpus while the English - Arabic translation model weights are optimized using a set of 500 sentences from the 2004 NIST MT evaluation test set ( MT04 ) .
The optimized weights are used for ranking and filtering ( discussed in Section 3.3 ) .
We use a maximum phrase length of size 8 across all models .
We report results on an inhouse Persian - Arabic evaluation set of 536 sentences with three references .
We evaluate using BLEU -4 ( Papineni et al. , 2002 ) and METEOR ( Lavie and Agarwal , 2007 ) .
Linguistic Preprocessing
In this section we present our motivation and choice for preprocessing Arabic , Persian , English data .
Both Arabic and Persian are morphologically complex languages but they belong to two different language families .
They both express richness and linguistic complexities in different ways .
One aspect of Arabic 's complexity is its various attachable clitics and numerous morphological features ( Habash , 2010 ) . which include conjunction proclitics , e.g. , + w+ ' and ' , particle proclitics , e.g. , + l+ ' to / for ' , the definite article + Al + ' the ' , and the class of pronominal enclitics , e.g. , + + hm ' their / them ' .
Beyond these clitics , Arabic words inflect for person ( PER ) , gender ( GEN ) , number ( NUM ) , aspect ( ASP ) , mood ( MOD ) , voice ( VOX ) , state ( STT ) and case ( CAS ) .
This morphological richness leads to thousands of inflected forms per lemma and a high degree of ambiguity : about 12 analyses per word , typically corresponding to two lemmas on average ( Habash , 2010 )
We follow El Kholy and Habash ( 2010a ) and use the PATB tokenization scheme ( Maamouri et al. , 2004 ) in our experiments .
which separates all clitics except for the determiner clitic Al + ( DET )
We use MADA v3.1 ( Habash and Rambow , 2005 ; to tokenize the Arabic text .
We only evaluate on detokenized and orthographically correct ( enriched ) output following the work of El Kholy and Habash ( 2010 b ) .
Persian on the other hand has a relatively simple nominal system .
There is no case system and words do not inflect with gender except for a few animate Arabic loanwords .
Unlike Arabic , Persian shows only two values for number , just singular and plural ( no dual ) , which are usually marked by either the suffix + + hA and sometimes + + An , or one of the Arabic plural markers .
Persian also possess a closed set of few broken plurals loaned from Arabic .
Moreover , unlike Arabic which expresses definiteness , Persian expresses indefiniteness with an enclitic article + +y 'a / an ' which does n't have separate forms for singular and plural .
When a noun is modified by one or more adjective , the indefinite article is attached to the last adjective .
Persian adjectives are similar to English in expressing comparative and superlative constructions just by adding suffixes + + tar ' + er ' and + + taryn ' + est ' respectively .
Verbal morphology is very complex in Persian .
Each verb has a past and present root and many verbs have attached prefix that is regarded part of the root .
A verb in Persian inflects for 14 different tense , mood , aspect , person , number and voice combination values ( Rasooli et al. , 2013 ) .
We use Perstem ( Jadidinejad et al. , 2010 ) for segmenting Persian text .
English , our pivot language , is quite different from both Arabic and Persian .
English is poor in morphology and barely inflects for number and tense , and for person in a limited context .
English preprocessing simply includes down- casing , sepa-rating punctuation and splitting off " 's " .
Baseline Evaluation
We compare the performance of sentence pivoting against phrase pivoting with different filtering thresholds .
The results are presented in Table 2 .
In general , the phrase pivoting outperforms the sentence pivoting even when we use a small filtering threshold of size 100 .
Moreover , the higher the threshold the better the performance but with a diminishing gain .
We use the best performing setup across the rest of the experiments .
Pivot
Connectivity Strength Features Evaluation
In this experiment , we test the performance of adding the connectivity strength features ( + Conn ) to the best performing phrase pivoting model ( Phrase Pivot F1K ) .
The results in Table 3 show that we get a nice improvement of ?0.6/0.3 ( BLEU / METEOR ) points by adding the connectivity strength features .
The differences in BLEU scores between this setup and all other systems are statistically significant above the 95 % level .
Statistical significance is computed using paired bootstrap resampling ( Koehn , 2004 ) .
Model
Conclusion and Future Work
We presented an experiment showing the effect of using two language independent features , source connectivity score and target connectivity score , to improve the quality of pivot- based SMT .
We showed that these features help improving the overall translation quality .
In the future , we plan to explore other features , e.g. , the number of the pivot phases used in connecting the source and target phrase pair and the similarity between these pivot phrases .
We also plan to explore language specific features which could be extracted from some seed parallel data , e.g. , syntactic and morphological compatibility of the source and target phrase pairs .
Figure 1 : 1 Figure 1 : An example of strongly connected Persian - Arabic phrase pair through English .
All Persian words are connected to one or more Arabic words .
SCS = 1.0 and TCS =1.0 .
