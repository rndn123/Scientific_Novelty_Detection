title
Unsupervised Translation Sense Clustering
abstract
We propose an unsupervised method for clustering the translations of a word , such that the translations in each cluster share a common semantic sense .
Words are assigned to clusters based on their usage distribution in large monolingual and parallel corpora using the soft K-Means algorithm .
In addition to describing our approach , we formalize the task of translation sense clustering and describe a procedure that leverages WordNet for evaluation .
By comparing our induced clusters to reference clusters generated from WordNet , we demonstrate that our method effectively identifies sense- based translation clusters and benefits from both monolingual and parallel corpora .
Finally , we describe a method for annotating clusters with usage examples .
Introduction
The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing .
Probabilistic word alignment models can induce bilexical distributions over target - language translations of source - language words ( Brown et al. , 1993 ) .
However , word - to - word correspondences do not capture the full structure of a bilingual lexicon .
Consider the example bilingual dictionary entry in Figure 1 ; in addition to enumerating the translations of a word , the dictionary author has grouped those translations into three sense clusters .
Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers .
Colocar [ co?lo?car ? ] , va. 1 . To arrange , to put in due place or order .
2 . To place , to put in any place , rank condition or office , to provide a place or employment .
3 . To collocate , to locate , to lay .
Figure 1 : This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses ( Vel ? zquez de la Cadena et al. , 1965 ) .
This paper formalizes the task of clustering a set of translations by sense , as might appear in a published bilingual dictionary , and proposes an unsupervised method for inducing such clusters .
We also show how to add usage examples for the translation sense clusters , hence providing complete structure to a bilingual dictionary .
The input to this task is a set of source words and a set of target translations for each source word .
Our proposed method clusters these translations in two steps .
First , we induce a global clustering of the entire target vocabulary using the soft K-Means algorithm , which identifies groups of words that appear in similar contexts ( in a monolingual corpus ) and are translated in similar ways ( in a parallel corpus ) .
Second , we derive clusters over the translations of each source word by projecting the global clusters .
We evaluate these clusters by comparing them to reference clusters with the overlapping BCubed metric ( Amigo et al. , 2009 ) .
We propose a clustering criterion that allows us to derive reference clusters from the synonym groups of WordNet R ( Miller , 1995 ) .
1 Our experiments using Spanish -English and Japanese - English datasets demonstrate that the automatically generated clusters produced by our method are substantially more similar to the WordNet - based reference clusters than naive baselines .
Moreover , we show that bilingual features collected from parallel corpora improve clustering accuracy over monolingual distributional similarity features alone .
Finally , we present a method for annotating clusters with usage examples , which enrich our automatically generated bilingual dictionary entries .
Task Description
We consider a three -step pipeline for generating structured bilingual dictionary entries automatically .
( 1 ) The first step is to identify a set of high-quality target - side translations for source lexical items .
In our experiments , we ask bilingual human annotators to create these translation sets .
2
We restrict our present study to word-level translations , disallowing multi-word phrases , in order to leverage existing lexical resources for evaluation .
( 2 ) The second step is to cluster translations of each word according to common word senses .
This clustering task is the primary focus of the paper , and we formalize it in this section .
( 3 )
The final step annotates clusters with usage examples to enrich the structure of the output .
Section 7 describes a method of identifying clusterspecific usage examples .
In the task of translation sense clustering , the second step , we assume a fixed set of source lexical items of interest S , each with a single part of speech 3 , and for each s ?
S a set T s of target translations .
Moreover , we assume that each target word t ?
T s has a set of senses in common with s .
These senses may also be shared among different target words .
That is , each target word may have multiple senses and each sense may be expressed by multiple words .
Given a translation set T s , we define a cluster G ?
T s to be a correct sense cluster if it is both coherent and complete . ?
A sense cluster G is coherent if and only if there exists some sense B shared by all of the target words in G. ?
A sense cluster G is complete if and only if , for every sense B shared by all words in G , there is no other word in T s but not in G that also shares that sense .
The full set of correct clusters for a set of translations consists of all sense clusters that are both coherent and complete .
The example translation set for the Spanish word colocar in Figure 2 is shown with four correct sense clusters .
For descriptive purposes , these clusters are annotated by WordNet senses and bilingual usage examples .
However , the task we have defined does not require the WordNet sense or usage example to be identified : we must only produce the correct sense clusters within a set of translations .
In fact , a cluster may correspond to more than one sense .
Our definition of correct sense clusters has several appealing properties .
First , we do not attempt to enumerate all senses of the source word .
Sense Notation T s : The set of target - language translations ( given ) D t : The set of synsets in which t appears ( given ) C : A synset ; a set of target - language words B : A source-specific synset ; a subset of T s B : A set of source-specific synsets G : A set of correct sense clusters for T s The Cluster Projection Algorithm : B ? C ? T s : C ? t?Ts D t G ? ? for B ? B do if B ?
B such that B ?
B then add B to G return G Figure 3 : The Cluster Projection ( CP ) algorithm projects language - level synsets ( C ) to source -specific synsets ( B ) and then filters the set of synsets for redundant subsets to produce the complete set of source-specific synsets that are both coherent and complete ( G ) .
distinctions are only made when they affect crosslingual lexical choice .
If a source word has many fine- grained senses but translates in the same way regardless of the sense intended , then there is only one correct sense cluster for that translation .
Second , no correct sense cluster can be a superset of another , because the subset would violate the completeness condition .
This criterion encourages larger clusters that are easier to interpret , as their unifying senses can be identified as the intersection of senses of the translations in the cluster .
Third , the correct clusters need not form a partition of the input translations .
It is common in published bilingual dictionaries for a translation to appear in multiple sense clusters .
In our example , the polysemous English verbs place and put appear in multiple clusters .
Generating Reference Clusters
To construct a reference set for the translation sense clustering task , we first collected English translations of Spanish and Japanese nouns , verbs , and adverbs .
Translation sets were curated by human annotators to keep only high-quality singleword translations .
Rather than gathering reference clusters via an additional annotation effort , we leverage WordNet , a large database of English lexical semantics ( Miller , 1995 ) .
WordNet groups words into sets of cogni- tive synonyms called synsets , each expressing a distinct concept .
We use WordNet version 2.1 , which has wide coverage of nouns , verbs , and adverbs , but sparser coverage of adjectives and prepositions .
4 Reference clusters for the set of translations T s of some source word s are generated algorithmically from WordNet synsets via the Cluster Projection ( CP ) algorithm defined in Figure 3 .
An input to the CP algorithm is the translation set T s of some source word s.
Also , each translation t ?
T s belongs to some set of synsets D t , where each synset C ?
D t contains target - language words that may or may not be translations of s.
First , the CP algorithm constructs a source-specific synset B for each C , which contains only translations of s.
Second , it identifies all correct sense clusters G that are both coherent and complete with respect to the sourcespecific senses B . A sense cluster must correspond to some synset B ?
B to be coherent , and it must not have a proper superset in B to be complete .
5 Figure 4 illustrates the CP algorithm for the translations of the Spanish source word colocar that appear in our input dataset .
Clustering with K-Means
In this section , we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora .
Our method is language independent .
Distributed Soft K-Means Clustering
As a first step , we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features .
Several methods exist for this task , such as the K-Means algorithm ( MacQueen , 1967 ) , the Brown algorithm ( Brown et al. , 1992 ) and the exchange algorithm ( Kneser and Ney , 1993 ; Martin et al. , 1998 ; Uszkoreit and Brants , 2008 ) .
We use a distributed implementation of the " soft " K-Means clustering algorithm described in Lin and Wu ( 2009 ) .
Given a feature vector for each element ( a word type ) and the number of desired clusters K , the K-Means algorithm proceeds as follows :
1 . Select K elements as the initial centroids for K clusters .
repeat 2 .
Assign each element to the top M clusters with the nearest centroid , according to a similarity function in feature space .
3 . Recompute each cluster 's centroid by averaging the feature vectors of the elements in that cluster .
until convergence
Monolingual Features Following Lin and Wu ( 2009 ) , each word to be clustered is represented as a feature vector describing the distributional context of that word .
In our setup , the 5 One possible shortcoming of our approach to constructing reference sets for translation sense clustering is that a cluster may correspond to a sense that is not shared by the original source word used to generate the translation set .
All translations must share some sense with the source word , but they may not share all senses with the source word .
It is possible that two translations are synonymous in a sense that is not shared by the source .
However , we did not observe this problem in practice .
context of a word w consists of the words immediately to the left and right of w .
The context feature vector of w is constructed by first aggregating the frequency counts of each word f in the context of each w .
We then compute point-wise mutual information ( PMI ) features from the frequency counts : PMI ( w , f ) = log c( w , f ) c ( w ) c ( f ) where w is a word , f is a neighboring word , and c ( ? ) is the count of a word or word pair in the corpus .
6 A feature vector for w contains a PMI feature for each word type f ( with relative position left or right ) for all words that appears a sufficient number of times as a neighbor of w .
The similarity of two feature vectors is the cosine of the angle between the vectors .
We follow Lin and Wu ( 2009 ) in applying various thresholds during K-Means , such as a frequency threshold for the initial vocabulary , a totalcount threshold for the feature vectors , and a threshold for PMI scores .
Bilingual Features
In addition to the features described in Lin and Wu ( 2009 ) , we introduce features from a bilingual parallel corpus that encode reverse-translation information from the source - language ( Spanish or Japanese in our experiments ) .
We have two types of bilingual features : unigram features capture source-side reverse-translations of w , while bigram features capture both the reverse-translations and source -side neighboring context words to the left and right .
Features are expressed again as PMI computed from frequency counts of aligned phrase pairs in a parallel corpus .
For example , one unigram feature for place would be the PMI computed from the number of times that place was in the target side of a phrase pair whose source side was the unigram lugar .
Similarly , a bigram feature for place would be the PMI computed from the number of times that place was in the target side of a phrase pair whose source side was the bigram lugar de .
These features characterize the way in which a word is translated , an indication of its meaning .
Predicting Translation Clusters
As a result of soft K-Means clustering , each word in the target - language vocabulary is assigned to a list of up to M clusters .
To predict the sense clusters for a set of translations of a source word , we apply the CP algorithm ( Figure 3 ) , treating the K-Means clusters as synsets ( D t ) .
Related Work
To our knowledge , the translation sense clustering task has not been explored previously .
( Diab and Resnik , 2002 ; Kaji , 2003 ; Ng et al. , 2003 ; Tufis et al. , 2004 ; Apidianaki , 2009 ) relates to our work in that these approaches discover word senses automatically through clustering , even using multilingual parallel corpora .
However , our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in - context uses of a single word .
The underlying notion of " sense " is shared across these tasks , but the way in which we use and evaluate induced senses is novel .
Experiments
The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference sense clusters derived from WordNet .
Datasets
We conduct experiments using two bilingual datasets : Spanish -to-English ( S?E ) and Japaneseto-English ( J?E ) .
Table 1 shows , for each dataset , the number of source words and the total number of target words in their translation sets .
The datasets Monolingual features for K- Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text .
Bilingual features were computed from 0.78 ( S?E ) and 1.04 ( J?E ) billion tokens of parallel text , primarily extracted from the Web using automated parallel document identification ( Uszkoreit et al. , 2010 ) .
Word alignments were induced from the HMMbased alignment model ( Vogel et al. , 1996 ) , initialized with the bilexical parameters of IBM Model 1 ( Brown et al. , 1993 ) .
Both models were trained using 2 iterations of the expectation maximization algorithm .
Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation ( Koehn et al. , 2003 ) .
Clustering Evaluation Metrics
The quality of text clustering algorithms can be evaluated using a wide set of metrics .
For evaluation by set matching , the popular measures are Purity ( Zhao and Karypis , 2001 ) and Inverse Purity and their harmonic mean ( F measure , see Van Rijsbergen ( 1974 ) ) .
For evaluation by counting pairs , the popular metrics are the Rand Statistic and Jaccard Coefficient ( Halkidi et al. , 2001 ; Meila , 2003 ) .
Metrics based on entropy include Cluster Entropy ( Steinbach et al. , 2000 ) , Class Entropy ( Bakus et al. , 2002 ) , VI -measure ( Meila , 2003 ) , Q 0 ( Dom , 2001 ) , V-measure ( Rosenberg and Hirschberg , 2007 ) and Mutual Information ( Xu et al. , 2003 ) .
Lastly , there exist the BCubed metrics ( Bagga and Baldwin , 1998 ) , a family of metrics that decompose the clus-tering evaluation by estimating precision and recall for each item in the distribution .
Amigo et al. ( 2009 ) compares the various clustering metrics mentioned above and their properties .
They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families .
Their analysis shows that of the wide range of metrics , only BCubed satisfies those constraints .
After defining each constraint below , we briefly describe its relevance to the translation sense clustering task .
Homogeneity :
In a cluster , we should not mix items belonging to different categories .
Relevance :
All words in a proposed cluster should share some common WordNet sense .
Completeness :
Items belonging to the same category should be grouped in the same cluster .
Relevance :
All words that share some common WordNet sense should appear in the same cluster .
Rag Bag : Introducing disorder into a disordered cluster is less harmful than introducing disorder into a clean cluster .
Relevance :
We prefer to maximize the number of error-free clusters , because these are most easily interpreted and therefore most useful .
Cluster Size vs .
Quantity : A small error in a big cluster is preferable to a large number of small errors in small clusters .
Relevance :
We prefer to minimize the total number of erroneous clusters in a dictionary .
Amigo et al. ( 2009 ) also show that BCubed extends cleanly to settings with overlapping clusters , where an element can simultaneously belong to more than one cluster .
For these reasons , we focus on BCubed for cluster similarity evaluation .
7
The BCubed metric for scoring overlapping clusters is computed from the pair-wise precision and recall between pairs of items : where e and e are two items , L ( e ) is the set of reference clusters for e and C( e ) is the set of predicted 7
An evaluation using purity and inverse purity ( extended to overlapping clusters ) has been omitted for space , but leads to the same conclusions as the evaluation using BCubed .
P(e , clusters for e ( i.e. , clusters to which e belongs ) .
Note that P(e , e ) is defined only when e and e share some predicted cluster , and R(e , e ) when e and e share some reference cluster .
The BCubed precision associated to one item is its averaged pair-wise precision over other items sharing some of its predicted clusters , and likewise for recall 8 ; and the overall BCubed precision ( or recall ) is the averaged precision ( or recall ) of all items : P B3 = Avg e [ Avg
Results
Figure 5 shows the F ? - score for various ?
values : F ? = ( 1 + ? 2 ) ? P B3 ? R B3 ? 2 ? P B3 + R B3
This graph gives us a trade- off between precision and recall ( ?
= 0 is exact precision and ? ? ? tends to exact recall ) .
9 Each curve in Figure 5 represents a particular clustering method .
We include three naive baselines : ewnc : Each word in its own cluster aw1c :
All words in one cluster Random : Each target word is assigned M random cluster id 's in the range 1 to K , then translation sets are clustered with the CP algorithm .
The curves for K- Means clustering include one condition with monolingual features alone and two curves that include bilingual features as well .
10
The bilingual curves correspond to two different feature sets : the first includes only unigram features ( t1 ) , while the second includes both unigram and bigram features ( t1t2 ) .
Each point on an F ? curve in Figure 5 ( including the baseline curves ) represents a maximum over two An application that focuses on synonym discovery would favor recall , while an application portraying highly granular sense distinctions would favor precision .
Clustering accuracy improves over the baselines with monolingual features alone , and it improves further with the addition of bilingual features , for a wide range of ? values .
Our unsupervised approach with bilingual features achieves up to 6 - 8 % absolute improvement over the random baseline , and is particularly effective for recall - weighted metrics .
11
As an example , in a S?E experiment with a K-Means setting of K = 4096 : M = 3 , the overall F 1.5 score increases from 80.58 % to 86.12 % upon adding bilingual features .
Table 2 shows two examples from that experiment for which bilingual features improve the output clusters .
The parameter values we use in our experiments are K ? { 2 3 , 2 4 , . . . , 2 12 } and M ? { 1 , 2 , 3 , 4 , 5 } .
To provide additional detail , Figure 6 shows the BCubed precision and recall for each induced clustering , as the values of K and M vary , for Japanese - English .
12
Each point in this scatter plot represents a clustering methodology and a particular value for K and M . Soft K-Means with bilingual features provides the strongest performance across a broad range of cluster parameters .
Evaluation Details Certain special cases needed to be addressed in order to complete this evaluation .
Target words not in WordNet : Words that did not have any synset in WordNet were each assigned to a singleton reference cluster .
13
The S?E dataset has only 2 out of 225 target types missing in WordNet and the J?E dataset has only 55 out of 1351 target types missing .
Target words not clustered by K-Means : The K-Means algorithm applies various thresholds during different parts of the process .
As a result , there are some target word types that are not assigned any cluster at the end of the algorithm .
For example , in the J?E experiment with K = 4096 and with bilingual ( t1 only ) features , only 49 out of 1351 target - types are not assigned any cluster by K-Means .
These unclustered words were each assigned to a singleton cluster in post-processing .
Identifying Usage Examples
We now briefly consider the task of automatically extracting usage examples for each predicted cluster .
We identify these examples among the extracted phrase pairs of a parallel corpus .
Let P s be the set of source phrases containing source word s , and let A t be the set of source phrases that align to target phrases containing target word t.
For a source word s and target sense cluster G , we identify source phrases that contain s and translate to all words in G .
That is , we collect the set of phrases P s ? t?G A t .
We use the same parallel corpus as we used to compute bilingual features .
For example , if we consider the cluster [ place , position , put ] for the Spanish word colocar , then we find Spanish phrases that contain colocar and also align to English phrases containing place , position , and put somewhere in the parallel corpus .
Sample usage examples extracted by this approach appear in Figure 7 .
We have not performed a quantitative evaluation of these extracted examples , although qualitatively we have found that the technique surfaces useful phrases .
We look forward to future research that further explores this important sub-task of automatically generating bilingual dictionaries .
Conclusion
We presented the task of translation sense clustering , a critical second step to follow translation extraction in a pipeline for generating well -structured bilingual dictionaries automatically .
We introduced a method of projecting language - level clusters into clusters for specific translation sets using the CP algorithm .
We used this technique both for constructing reference clusters , via WordNet synsets , and constructing pre- dicted clusters from the output of a vocabulary - level clustering algorithm .
Our experiments demonstrated that the soft K-Means clustering algorithm , trained using distributional features from very large monolingual and bilingual corpora , recovered a substantial portion of the structure of reference clusters , as measured by the BCubed clustering metric .
The addition of bilingual features improved clustering results over monolingual features alone ; these features could prove useful for other clustering tasks as well .
Finally , we annotated our clusters with usage examples .
In future work , we hope to combine our clustering method with a system for automatically generating translation sets .
In doing so , we will develop a system that can automatically induce highquality , human-readable bilingual dictionaries from large corpora using unsupervised learning methods .
Figure 2 : 2 Figure 2 : Correct sense clusters for the translations of Spanish verb s = colocar , assuming that it has translation set Ts = { collocate , invest , locate , place , position , put} .
Only the sense clusters are outputs of the translation sense clustering task ; the additional columns are presented for clarity .
