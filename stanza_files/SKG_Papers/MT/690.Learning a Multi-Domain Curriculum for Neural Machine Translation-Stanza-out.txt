title
Learning a Multi-Domain Curriculum for Neural Machine Translation
abstract
Most data selection research in machine translation focuses on improving a single domain .
We perform data selection for multiple domains at once .
This is achieved by carefully introducing instance - level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches .
Both the choice of features and the use of curriculum are crucial for balancing and improving all domains , including out-ofdomain .
In large-scale experiments , the multidomain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training .
Introduction
In machine translation ( MT ) , data selection , e.g. , ( Moore and Lewis , 2010 ; Axelrod et al. , 2011 ) , has remained as a fundamental and important research topic .
It has played a crucial role in domain adaptation by selecting domain-matching training examples , or data cleaning ( aka denoising ) by selecting high-quality examples .
So far , the most extensively studied scenario assumes a single domain to improve .
It becomes both technically challenging and practically appealing to build a large-scale multidomain neural machine translation ( NMT ) model that performs simultaneously well on multiple domains at once .
This requires addressing research challenges such as catastrophic forgetting ( Goodfellow et al. , 2014 ) at scale and data balancing .
Such a model can easily find potential use cases , i.e. , as a solid general service , for downstream transfer learning , for better deployment efficiency , or for transfer learning across datasets .
Unfortunately , existing single-domain dataselection methods do not work well for multiple domains .
For example , improving the translation accuracy of one domain will often hurt that of another ( van der Wees et al. , 2017 ; Britz et al. , 2017 ) , and improving model generalization across all domains by clean-data selection may not promise optimization of a particular domain .
Multiple aspects need to be considered for training a multi-domain model .
This paper presents a dynamic data selection method to multi-domain NMT .
Things we do differently from previous work in mixing data are the choice of instance - level features and the employment of a multi-domain curriculum that is additionally able to denoise .
These are crucial for mixing and improving all domains , including outof-domain .
We experiment with large datasets at different noise levels and show that the resulting models meet our requirements .
Related Work
In MT , research that is most relevant to our work is data selection and data mixing , both being concerned with how to sample examples to train an MT model , usually for domain adaptation .
Table 1 categorizes previous research by two aspects and shows where our work stands .
These two aspects are : 1 . Is the method concerned with a single domain or multiple domains ?
2 . Does the method use data statically or dynamically ?
Static data selection for a single domain .
Moore and Lewis ( 2010 ) select in - domain data for n-gram language model ( LM ) training .
It is later generalized by Axelrod et al . ( 2011 ) to select parallel data for training MT models .
; use classifiers to select domain data .
Clean-data selection ( Koehn et al. , 2019 Junczys - Dowmunt , 2018 ) reduces harmful data noise to improve translation quality across domains .
All these works select a data subset for a single " domain " 1 and treat the selected data as a static / flat distribution .
Dynamic data selection for a single domain .
Static selection has two shortcomings : it discards data and it treats all examples equally after selection .
When data is scarce , any data could be helpful , even if it is out of domain or noisy 2 . Dynamic data selection is introduced to " sort " data from least in - domain to most in- domain .
Training NMT models on data sorted this way effectively takes advantage of transfer learning .
Curriculum learning ( CL ) ( Bengio et al. , 2009 ) has been used as a formulation for dynamic data selection .
Domain curricula ( van der Wees et al. , 2017 ; Zhang et al. , 2019 ) are used for domain adaptation .
Model stacking ( Sajjad et al. , 2017 ; Freitag and Al - Onaizan , 2016 ) is a practical idea to build domain models .
CL is also used for denoising Wang et al. , 2018 a , b ) , and for faster convergence and improved general quality ( Zhang et al. , 2018 ; Platanios et al. , 2019 ) .
Wang et al. ( 2018a ) introduce a curriculum for training efficiency .
In addition to data sorting / curriculum , instance / loss weighting ( Wang et al. , 2017 ; Wang et al. , 2019 b ) has been used as an alternative .
CL for NMT represents the SOTA data-selection method , but most existing works target at a single " domain " , be it a specific domain or the " denoising domain " .
Static data mixing for multiple domains .
When mixing data from multiple domains , a fundamental challenge is to address catastrophic forgetting ( Goodfellow et al. , 2014 ) - training an NMT model to focus on one domain can likely hurt another ( van der Wees et al. , 2017 ; Britz et al. , 2017 ) . Britz et al. ( 2017 ) learn domain-discerning ( or - invariant ) network representation with a domain discriminator network for NMT .
The methods , however , require that domain labels are available in data .
Tars and Fishel ( 2018 ) Automatic data balancing for multi-domains .
( Wang et al. , 2020 ) automatically learn to weight ( flat ) data streams of multi-languages ( or " domains " ) .
We perform dynamic data selection and regularization through a mulit-domain curriculum .
Automatic curriculum learning .
Our work falls under automatic curriculum construction ( Graves et al. , 2017 ) and is directly inspired by Tsvetkov et al . ( 2016 ) , who learn to weight and combine instance - level features to form a curriculum for an embedding learning task , through Bayesian Optimization .
A similar idea ( Ruder and Plank , 2017 ) is used to improve other NLP tasks .
Here , we use the idea for NMT to construct a multi-domain data selection scheme with various selection scores at our disposal .
The problem we study is connected to the more general multiobjective optimization problem .
Duh ( 2018 ) uses Bandit learning to tune hyper-parameters such as the number of network layers for NMT .
More related work .
Previously , catastrophic forgetting has mostly been studied in the continued - training setup ( Saunders et al. , 2019 ; , to refer to the degrading performance on the out- of- domain task when a model is fine-tuned on in- domain data .
This setup is a popular topic in general machine learning research ( Aljundi et al. , 2019 ) . study domain adaptation by freezing subnetworks .
Our work instead addresses forgetting in the data- balancing scenario for multi-domains .
We use curriculum to generalize fine-tuning .
3 Curriculum Learning for NMT S1 S2 S3 S3 S2 S1 S1 S3 S2 S2 S1 S3 ( 1 ) ( 2 ) ( 3 ) ( 5 ) S2 S1 S3 ( 4 ) W1 ? W2 ? W3 W1 ? W2 ? W3 ? ? 1/3 1/3 1/3 ? ? ? ? 1/2 1/2 0.0 ? ? ? ? 1.0 0.0 0.0 ? ? ? ? 1/3 1/3 1/3 ? ? ? ? 1/2 1/2 0.0 ? ? ? ? 0.0 1.0 0.0 ? ? ( 1 ) ( 2 ) Table 2 : Curriculum examples characterized by reweighting , Wt (x , y ) , over three steps , to stochastically order data to benefit a final domain .
Strikethrough discards examples . ( 1 ) corresponds to data order Figure 1 ( 2 ) . ( 2 ) corresponds to data order Figure 1 ( 5 ) .
In NMT , CL is used to implement dynamic data selection .
First , a scoring function ( Section 4.3 ) is employed to measure the usefulness of an example to a domain and sort data .
Then mini- batch sampling , e.g. , ( Kocmi and Bojar , 2017 ) , is designed to realize the weighting W t , to dynamically evolve the training criteria Q t towards in- domain .
Figure 1 ( 1 ) -( 4 ) illustrates the basic idea of the curriculum we use .
( 1 ) shows three sentence pairs , S 1 , S 2 , S 3 , each having three scores , respectively representing usefulness to three domains .
A greydomain training curriculum , for example , relies on the data order in ( 2 ) , gradually discards least useful examples according to W t ( x , y ) ( Eq. 1 ) in Table 2 ( 1 ) :
At step 1 , the learner uniformly samples from all examples ( W 1 ) , producing model m 1 .
In step 2 , the least- in- domain S 3 is discarded ( strikethrough ) by W 2 so we sample from subset { S 1 , S 2 } uniformly to reach m 2 .
We repeat this until reaching the final model M .
In this process , sampling is uniform in each step , but in - domain examples ( e.g. , S 1 ) are reused more over steps .
Similarly , we can construct the dark- domain curriculum in Figure 1 ( 3 ) and the white - domain ( 4 ) .
Our Approach : Learning a Multi-Domain Curriculum
General Idea
The challenges in multi-domain / - task data selection lie in addressing catastrophic forgetting and data balancing .
In Figure 1 , while curriculum ( 2 ) moves a model to the grey-domain direction , this direction may not necessarily be positively consistent with the dark domain ( Figure 1 ( 3 ) ) , causing dropped dark-domain performance .
Ideally , a training example that introduces the least forgetting across all domains would have gradients that move the model in a common direction towards all domains .
While this may not be easily feasible by selecting a single example , we would like the intuition to work in a data batch on average .
Therefore , our idea is to carefully introduce D curriculum finetune NMT f 1 ( x , y ) fN ( x , y ) . . . model v1 vN . . . P1 PK . . . eval : optimizer f ( x , y ) = V ? F ( x , y ) C( V ) V = Figure 2 : Learning a multi-domain curriculum .
per-example data-selection scores ( called features ) to measure " domain sharing " , intelligently weight them to balance the domains of interest , and dynamically schedule examples to trade - off between regularization and domain adaptation .
A method to realize the above idea has the following properties :
1 . Features of an example reflect its relevance to domains .
2 . Feature weights are jointly learned / optimized based on end model performance .
3 . Training is dynamic , by gradually focusing on multi-domain relevant and noise-reduced data batches .
Furthermore , a viable multi-domain curriculum meets the following performance requirements : ( i ) It improves the baseline model across all domains .
( ii )
It simultaneously reaches ( or outperforms ) the peak performance of individual singledomain curricula .
Above requires improvement over out - of- domain , too .
The Framework Formally , for a sentence pair ( x , y ) , let f n ( x , y ) ?
R be its n-th feature that specifies how ( x , y ) is useful to a domain .
Suppose we are interested in K domains and each example has N features .
For instance , each sentence pair of S1 , S2 , S3 in Figure 1 ( 1 ) has three features ( N = 3 ) , each for one domain ( K = 3 ) .
4
We represent ( x , y ) 's features using a feature vector F ( x , y ) = 4 But N does not necessarily equal K because we can introduce multiple features for one domain or a single feature for multiple domains . [ f 0 ( x , y ) , ... , f N ?1 ( x , y ) ] .
Given a weight vector V = [ v 0 , ... , v N ?1 ] for all sentence pairs , we compute an aggregated score f ( x , y ) = V ? F ( x , y ) ( 4 ) for each sentence pair and sort the entire data in increasing order .
We then construct a curriculum C( V ) to fine- tune a warmed - up model , evaluate its performance and propose a next weight vector .
After several iterations / trials , the optimal weight vector V * is the one with the best end performance : V * = arg max V P ( C ( V ) ) ( 5 ) Figure 2 shows the framework .
For the process to be practical and scalable , C fine- tunes a warmedup model for a small number of steps .
The learned V * can then eventually be used for retraining a final model from scratch .
Instance -Level Features
We design the following types of features for each training example and instantiate them in Experiments ( Section 5 ) .
NMT domain features ( q Z ) compute , for a pair ( x , y ) , the cross-entropy difference between two NMT models : q Z ( x , y ) = log P ( y|x ; ? Z ) ?log P ( y|x ; ? base ) |y | ( 6 ) P ( y|x ; ? base ) is a baseline model with parameters ?
base trained on the background parallel corpus , P ( y|x ; ? Z ) is a Z-domain model with ?
Z by finetuning ?
base on a small , Z-domain parallel corpus D Z with trusted quality and |y | is the length of y. q Z discerns both noise and domain Z ( Wang et al. , 2019a ) .
Each domain Z has its own D Z . Importantly , Grangier ( 2019 ) shows that , under the Taylor approximation ( Abramowitz and Stegun , 1964 ) , q Z approximates the dot product between gradient , g( x , y ; ? base ) , of training example ( x , y ) and gradient , g( D Z , ? base ) , of seed data D Z . 5
Thus an example with positive q Z likely 5
That is , according to Grangier ( 2019 ) : qZ ( x , y ) ? | y | = log P ( y|x ; ?Z ) ? log P ( y|x ; ? base ) ? ? g( x , y ; ? base ) g( DZ , ? base ) ( 7 ) when ? base and ?Z are close , which is the case for finetuning : ?Z = ? base + ? g( DZ , ? base ) . moves a model towards domain Z. For multiple domains , Z 1 , ... , Z K , selecting a batch of examples with q Z k 's all being positive would move a model towards a common direction shared across multiple domains , which alleviates forgetting .
The Z-domain feature q Z ( x , y ) can be easily generalized into a single multi-domain feature , q Z , for a set of domains Z : q Z ( x , y ) = log P ( y|x ; ? Z ) ?log P ( y|x ; ? base ) |y | ( 8 ) by simply concatenating all the seed parallel corpus D Z from the constituent domains into D Z and use it to fine - tune the baseline ? base into ?
Z . A benefit of q Z is scalability : using a single feature value to approximate ( x , y ) 's gradient consistency with the multiple domains at once .
Simple concatenation means , however , domain balancing is not optimized as in Eq. 5 . NLM domain features ( d Z ) ( Moore and Lewis , 2010 ; van der Wees et al. , 2017 ) compute Zdomain relevance of sentence x with neural language models ( NLM ) , like q Z : d Z ( x ) = log P ( x ; ? Z ) ? log P ( x ; ? base ) | x | ( 9 ) where P ( x ; ? base ) is an NLM with parameters ?
base trained on the x half of the background parallel data , and P ( x ; ? Z ) is obtained by fine-tuning P ( x ; ? base ) on Z-domain monolingual data .
Although d Z may not necessarily reflect the translation gradient of an example under an NMT model , it effectively assesses the Z-domain relevance and , furthermore , allows us to include additional larger amounts of in-domain monolingual data .
We do not use its bilingual version ( Axelrod et al. , 2011 ) , but choose to consider only the source side , for simplicity .
Cross-lingual embedding similarity feature ( emb ) computes the cosine similarity of a sentence pair in a cross-lingual embedding space .
The embedding model is trained to produce similar representations exclusively for true bilingual sentence pairs , following Yang et al . ( 2019 ) .
BERT quality feature ( BERT ) represents quality scores from a fine-tuned multilingual BERT model ( Devlin et al. , 2018 ) .
We fine- tune a pre-trained BERT model 6 on a supervised dataset with positive and negative translation pairs .
6
We use the public cased 12 layers multilingual model : multi_cased_L-12_H-768_A-12
Algorithm 1 : Bayesian optimization
These features compensate each other by capturing the information in a sentence pair from different aspects : NLM features capture domain .
NMT features additionally discern noise .
BERT and emb are introduced for denoising , by transfering the strength of the data they are trained on .
All these features are from previous research and here we integrate them to solve a generalized problem .
Performance Metric P Eq. 5 evaluates the end performance P ( C ( V ) ) of a multi-domain curriculum candidate .
We simply combine the validation sets from multi-domains into a single validation set to report the perplexity of the last model checkpoint , after training the model on C( V ) .
The best multi-domain curriculum minimizes model 's perplexity ( or maximizes its negative per Eq. 5 ) on the mixed validation set .
We experiment with different mixing ratios .
Curriculum Optimization
We solve Eq. 5 with Bayesian Optimization ( BayesOpt ) ( Shahriari et al. , 2016 ) as the optimizer in Figure 2 . BayesOpt is derivative - free and can optimize expensive black - box functions , with no assumption of the form of P .
It has recently become popular for training expensive machinelearning models in the " AutoML " paradigm .
It consists of a surrogate model for approximating P ( C ( V ) ) and an acquisition function for deciding the next sample to evaluate .
The surrogate model evaluates C( V ) without running the actual NMT training , by the Gaussian process ( GP ) priors over functions that express assumptions about P .
The acquisition function depends on previous trials , as well as the GP hyper-parameters .
The Expected Improvement ( EI ) criterion ( Srinivas et al. , 2010 ) is usually used as acquisition function .
Algo - rithm 1 depicts how BayesOpt works in our setup .
We use Vizier ( Golovin et al. , 2017 ) for Batched Gaussian Process Bandit , but open-source implementations of BayesOpt are easily available .
7 .
Curriculum Construction
We pre-compute all features for each sentence pair ( x , y ) in training data and turn its features into a single score f ( x , y ) by Eq. 4 , given a weight vector .
We then construct a curriculum by instantiating its re-weighting W t ( x , y ) ( Eq. 1 ) .
To that end , we define a Boolean , dynamic data selection function ? f ? ( x , y ; t ) to check , at step t , if ( x , y ) ?
D belongs to the top ?( t ) - ratio examples in training data D sorted in increasing order of f ( x , y ) , ( 0 < ? ? 1 ) .
So ? f ? is a mask .
Suppose n ( t ) examples are selected by ? f ? ( x , y ; t ) , the re-weighting will then be W t ( x , y ) = 1/n( t ) ? ? f ? ( x , y ; t ) .
( 10 ) Filtered examples have zero weights and selected ones are uniformly weighted .
We set ?( t ) = ( 1/2 ) t/H to decay / tighten over time 8 , controlled by the hyper-parameter H. During training , ? f ? ( x , y ; t ) progressively selects higher f ( x , y ) scoring examples .
In implementation , we integrate ? f ? ( x , y ; t ) in the data feeder to pass only selected examples to the downstream model trainer ; we also normalize f ( x , y ) offline to directly compare to ?( t ) online to decide filtering .
As an example , the W t ( x , y ) for the multi-domain curriculum order in Figure 1 ( 5 ) can look like Table 2 ( 2 ) .
Experiments
Setup Data and domains .
We experiment with two English ?
French training datasets : the noisy ParaCrawl data 9 ( 290 million sentence pairs ) and the WMT14 training data ( 38 million pairs ) .
We use SentencePiece model ( Kudo , 2018 ) for subword segmentation with a source-target shared vocabulary of 32,000 subword units .
We evaluate our method with three " domains " : two specific domains , news and TED subtitles , and outof-domain .
News domain uses the WMT14 news testset ( N14 ) for testing , and WMT12 - 13 for validation in early stopping ( Prechelt , 1997 ) .
The TED domain uses the IWSLT15 testset ( T15 ) for testing , and the IWSLT14 testset for validation .
Out - of-domain performance is measured by two additional testsets , patent testset ( PA ) ( 2000 sentences ) 10 and WMT15 news discussion testset ( D15 ) .
We report SacreBLEU 11 ( Post , 2018 ) . 2019 ) with a 3 - layer transformer ( Vaswani et al. , 2017 ) ( more details in Appendix A ) .
For the BERT feature , we sample positive pairs from the same data to train the cross-lingual embedding model .
The negatives are generated using the cross-lingual embedding model , via 10 - nearest neighbor retrieval in the embedding space , excluding the true translation .
We pick the nearest neighbor to form a hard negative pair with the English sentence , and a random neighbor to form another negative pair .
We sample 600k positive pairs and produce 1.8 M pairs in total .
Model .
We use LSTM NMT
( Wu et al. , 2016 ) as our models , but with the Adam optimizer ( Kingma and Ba , 2015 ) .
The batch size is 10k averaged over 8 length - buckets ( with synchronous training ) .
NLM / NMT features uses 512 dimensions by 3 layers - NLM shares the same architecture as NMT by using dummy source sentences ( Sennrich et al. , 2016 ) .
The final models are of 1024 dimensions by 8 layers , trained for 55 k max steps .
Training on WMT data uses a dropout probability of 0.2 .
Transformer results are in Appendix B. Curriculum optimization .
In Eq. 5 ( Section 4.5 ) , we launch 30 trials ( candidate curricula ) .
BayesOpt spends 25 trials in exploration and the last 5 in exploitation .
Each trial trains for 2 k steps 12 by fine-tuning a warmed - up model with the candidate curriculum .
The curriculum decays ( ? ( t ) ) from 100 % and plateaus at 20 % at step 2k .
We simply and heuristically set a range of [ 0.0 , 1.0 ] for all feature weights .
We do n't normalize feature values when weighting them .
Results
We evaluate if the multi-domain curriculum meets requirements ( i ) and ( ii ) in Section 4.1 .
Compared to no curriculum
We compare : ?
B : baseline that does not use curriculum learning .
?
C 6 - feats : multi-domain curriculum with 6 features , d N , d T , q N , q T , BERT , emb , weights learned by BayesOpt .
Table 3 shows C 6 - feats improves B on all testsets , especially on noisy ParaCrawl - requirement ( i ) is met .
It is important to note that our WMT baseline ( W1 ) matches Wu et al . ( 2016 ) on N14 , as shown by re-computed tokenized BLEU ( italics ) .
Compared to single-domain curricula
We examine the following individual curricula , by training NMT models with each , respectively : ?
C d N , uses news NLM feature d N ( Eq. 9 ) . ?
C d T , uses TED subtitle NLM feature d T . ? C q N , uses news NMT feature q N ( Eq. 6 ) . ?
C q T , uses TED NMT feature q T . ?
C BERT , uses BERT quality feature . ?
C emb , uses cross-lingual embedding feature .
12 2 k is empirically chosen to be practical .
We use a number of fine-tuning trials in Eq. 5 .
NMT training is expensive so we do n't want a trial to tune for many steps .
NMT is very adaptive on domain data , so each trial does not need many steps .
We find no significant difference among 1k , 2 k , 6 k .
In Table 4 , frame boxes mark the best BLEUs ( P* or W* ) per column , across P3 - P7 or W3 - W7 .
The last column shows averaged BLEU over all testsets .
Bold font indicates C 6 - feats matches or improves W* .
As shown , C 6 - feats matches or slightly outperforms the per-domain curricula across testsets .
Therefore , C 6 - feats meets requirement ( ii ) .
Curriculum
Ablation Studies
Features Strengths and weaknesses of a feature .
?
C 4 - feats , multi-domain curriculum that excludes BERT and emb and uses 4 features .
Table 5 shows BERT and emb features in C 6 - feats improve C 4 - feats with ParaCrawl , adding to the intuition that they have a denoising effect .
Learned feature weights .
Figure 3 shows BayesOpt learns to weight features adaptively in C 6 - feats on ParaCrawl ( grey ) and WMT ( white ) , respectively .
ParaCrawl is very noisy thus noise non-discerning features d N and d T do not have a chance to help , but their weights become stronger on the cleaner WMT training data .
It is surprising that BERT feature is still useful to the WMT training .
We hypothesize this may suggest BERT feature have additional strength to just denoising , or that data noise could be subtle and exist in cleaner data .
BayesOpt vs. random search
We compare BayesOpt ( BO ) and Random Search ( RS ) ( Bergstra and Bengio , 2012 ) to solve Eq. 5 , as well as uniform weighting ( Uniform ) .
In Table 6 , all improve baselines , especially on ParaCrawl ( P ) .
RS does surprisingly well on ParaCrawl , but BayesOpt appears better overall .
13
Mixing validation sets Eq. 5 evaluates P using the concatenated validation set ( Section 4.4 ) .
Table 7 shows that the newsvs-TED mixing ratios can affect the per-domain BLEUs .
For example , on ParaCrawl , when news sentences are absent from the validation set , N14 drops by 0.7 BLEU ( P8 vs. P13 ) .
We use the four feats as in C 4 - feats in this examination .
Dynamic data balancing
We simulate dynamic data selection with a random sample of 2000 pairs from the WMT data and annotate each pair by human raters with 0 ( nonsense ) - 4 ( perfect ) quality scale ( following Wang et al . ( 2018 b ) ) .
We sort the pairs by f ( x , y ) ( Eq. 4 ) .
A threshold selects a subset of pairs , for which we average the respective NMT feature values as the domain relevance .
Figure 4 shows that the multi-domain curriculum ( C 6 - feats ) learns to dynamically increase quality and multi-domain relevance .
Therefore , our idea ( Section 4.1 ) works as intended .
Furthermore , training seems to gradually increase quality or domain in different speeds , determined by Eq. 5 .
Weighting loss vs. curriculum
With the learned weights , we compute a weight for each example to sort data to form a curriculum .
Alternatively , we could weight the cross-entropy loss for that sentence during training ( Wang et al. , 2017 ; . sentence loss , in particular on noisy training data , confirming previous findings ( van der Wees et al. , 2017 ) .
In - domain fine- tuning C q N and C q T each use a small in - domain parallel dataset , but we can simply fine - tune the final models on either dataset ( + N , +T ) or their concatenation ( + N+T ) .
Table 9 shows that C 6 - feats can be further improved by in- domain fine-tuning 14 and that both C 6 - feats and its fine-tuning still improve the fine-tuned baselines , in particular on ParaCrawl .
Discussion : Feature Dependency
One potential issue with using multiple perdomain features ( q Z ( x , y ) 's in Eq. 6 ) is scores are not shared across domains and linear weighting may not capture feature dependency .
For example , we need two NMT features if there are two domains .
We replace the two NMT features , q N and q T , in 4 - feats with a single two -domain feature q Z={N , T } ( Eq. 8 ) , but with the two corresponding NLM features unchanged ( so the new experiment has 3 features ) .
Table 10 shows multi-domain feature contributes slightly better than linear combination of per-domain features ( P19 vs. P8 ) .
The per-domain features , however , have the advantage of efficient feature weighting .
In case of many features , learning to compress them seems to be an interesting future investigation . : multi-dom . 36.6 38.6 46.8 35.9 39.5 Table 10 : Multi-domain / task feature ( Eq. 8 ) seems to contribute slightly better than linear combination of multiple perdomain features ( Eq. 6 ) .
Conclusion Existing curriculum learning research in NMT focuses on a single domain .
We present a multidomain curriculum learning method .
We carefully introduce instance -level features and learn a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches .
End -to - end experiments and ablation studies on large datasets at different noise levels show that the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training , on in- domain and out-ofdomain testsets .
data ( 200 k > 10k ) .
Therefore , by " noisy data can be helpful " , we refer to data regularization ( using more data ) and to transfer learning ( fine-tuning ) to exploit both data quantity and quality , the idea behind dynamic data selection .
Figure 1 : 1 Figure 1 : Data order in single-domain curricula and a potential multi-domain curriculum . ( 1 ) A toy training dataset of 3 examples .
Each example has three scores , representing relevance to three domains , grey / dark / white domains , respectively .
The higher the bar the more relevant .
( 2 ) Grey -domain order .
( 3 ) Dark-domain order .
( 4 ) White - domain order . ( 5 ) A potential multi-domain data order .
