title
A Beam-Search Decoder for Normalization of Social Media Text with Application to Machine Translation
abstract
Social media texts are written in an informal style , which hinders other natural language processing ( NLP ) applications such as machine translation .
Text normalization is thus important for processing of social media text .
Previous work mostly focused on normalizing words by replacing an informal word with its formal form .
In this paper , to further improve other downstream NLP applications , we argue that other normalization operations should also be performed , e.g. , missing word recovery and punctuation correction .
A novel beam-search decoder is proposed to effectively integrate various normalization operations .
Empirical results show that our system obtains statistically significant improvements over two strong baselines in both normalization and translation tasks , for both Chinese and English .
Introduction Social media texts include SMS ( Short Message Service ) messages , Twitter messages , Facebook updates , etc .
They are different from formal texts due to their significant informal characteristics , so they always pose difficulties for applications such as machine translation ( MT ) ( Aw et al. , 2005 ) and named entity recognition , because of a lack of training data containing informal texts .
Thus , the applications always suffer from a substantial performance drop when evaluated on social media texts .
For example , Ritter et al. ( 2011 ) reported a drop from 90 % to 76 % on part- of-speech tagging , and Foster et al . ( 2011 ) found a drop of 20 % in dependency parsing .
Creating training data of social media texts specifically for a text processing task is time - consuming .
For example , to create parallel Chinese - English training texts for translation of social media texts , it takes three minutes on average to translate an informally written social media text of eleven words from Chinese into English .
On the other hand , it takes thirty seconds to normalize the same message , a six -fold increase in speed .
After training a text normalization system to normalize social media texts , we can use an existing statistical machine translation ( SMT ) system trained on normal texts ( non-social media texts ) to carry out translation .
So we argue that normalization followed by regular translation is a more practical approach .
Thus , text normalization is important for social media text processing .
Most previous work on normalization of social media text focused on word substitution ( Beaufort et al. , 2010 ; Gouws et al. , 2011 ; Han and Baldwin , 2011 ; Liu et al. , 2012 ) .
However , we argue that some other normalization operations besides word substitution are also critical for subsequent natural language processing ( NLP ) applications , such as missing word recovery ( e.g. , zero pronouns ) and punctuation correction .
In this paper , we propose a novel beam-search decoder for normalization of social media text for MT .
Our decoder can effectively integrate different normalization operations together .
In contrast to previous work , some of our normalization operations are specifically designed for MT , e.g. , missing word recovery based on conditional random fields ( CRF ) ( Lafferty et al. , 2001 ) and punctuation correction based on dynamic conditional random fields ( DCRF ) ( Sutton et al. , 2004 ) .
To the best of our knowledge , our work is the first to perform missing word recovery and punctuation correction for normalization of social media text , and also the first to perform message - level normalization of Chinese social media text .
We investigate the effects on translating social media text after addressing various characteristics of informal social media text through normalization .
To show the applicability of our normalization approach for different languages , we experiment with two languages , Chinese and English .
We achieved statistically significant improvements over two strong baselines : an improvement of 9.98 % / 7.35 % in BLEU scores for normalization of Chinese / English social media text , and an improvement of 1.38 % / 1.35 % in BLEU scores for translation of Chinese / English social media text .
We created two corpora : a Chinese corpus containing 1,000 Weibo 1 messages with their normalizations and English translations ; and another similar English corpus containing 2,000 SMS messages from the NUS SMS corpus ( How and Kan , 2005 ) .
As far as we know , our corpora are the first publicly available Chinese / English corpora for normalization and translation of social media text 2 . 2 Related Work Zhu et al. ( 2007 ) performed text normalization of informally written email messages using CRF ( Lafferty et al. , 2001 ) .
Due to its importance , normalization of social media text has been extensively studied recently .
Aw et al. ( 2005 ) proposed a noisy channel model consisting of different operations : substitution of non-standard acronyms , deletion of flavor words , and insertion of auxiliary verbs and subject pronouns .
Choudhury et al. ( 2007 ) used hidden Markov model to perform word-level normalization .
Kobus et al. ( 2008 ) combined MT and automatic speech recognition ( ASR ) to better normalize French SMS message .
Cook and Stevenson ( 2009 ) used an unsupervised noisy channel model considering different word formation processes .
Han and Baldwin ( 2011 ) normalized informal words using morphophonemic similarity .
Pennell and Liu ( 2011 ) only dealt with SMS abbreviations .
Xue et al. ( 2011 ) normalized social media texts incorporating orthographic , phonetic , contextual , and acronym factors .
Liu et al. ( 2012 ) designed a system combining different human perspectives to perform word-level normalization .
Oliva et al. ( 2013 ) normalized Spanish SMS messages using a normalization and a phonetic dictionary .
For normalization of Chinese social media text , Xia et al . ( 2005 ) investigated informal phrase detection , and Li and Yarowsky ( 2008 ) mined informal -formal phrase pairs from Web corpora .
All the above work focused on normalizing words .
In contrast , our work also performs other normalization operations such as missing word recovery and punctuation correction , to further improve machine translation .
Previously , Aw et al. ( 2006 ) adopted phrase - based MT to perform SMS normalization , and required a relatively large number of manually normalized SMS messages .
In contrast , our approach performs beam search at the sentence level , and does not require large training data .
We evaluate the success of social media text normalization in the context of machine translation , so research on machine translation of social media text is relevant to our work .
However , there is not much comparative evaluation of social media text translation other than the Haitian Creole to English SMS translation task in the 2011 Workshop on Statistical Machine Translation ( WMT 2011 ) ( Callison - Burch et al. , 2011 ) .
However , the setup of the WMT 2011 task is different from ours , in that the task provided parallel training data of SMS texts and their translations .
As such , text normalization is not necessary in that task .
For example , the best reported system in that task ( Costa-juss ? and Banchs , 2011 ) did not perform SMS message normalization .
In speech to speech translation ( Paul , 2009 ; Nakov et al. , 2009 ) , the input texts contain wrongly transcribed words due to errors in automatic speech recognition , whereas social media texts contain abbreviations , new words , etc .
Although the input texts in both cases deviate from normal texts , the exact deviations are different .
Similarly , 200 English SMS messages were randomly selected from the NUS SMS corpus ( How and Kan , 2005 ) .
The informal characteristics of these messages are shown in the second half of Table 1 .
We found that our English messages contain more informal words than Chinese messages .
English words are shortened in three ways : ( 1 ) using a shorter word form with similar pronunciation ; ( 2 ) abbreviating a formal word ; and ( 3 ) using only a prefix of a formal word .
Other informal characteristics include : ( 1 ) informal punctuation conventions in-cluding omitted and misused punctuation ; ( 2 ) redundant interjections ; ( 3 ) quotation - related problems , e.g. , omitted quotation marks ; ( 4 ) " be " omission ; ( 5 ) tokenization problems ; and ( 6 ) informally written time expressions .
Methods
As can be seen in Section 3 , social media texts of different languages exhibit different informal characteristics .
For example , English messages have more informal words than Chinese messages , while punctuation problems are more prevalent for Chinese messages .
Also , fixing different types of informal characteristics often depends on each other .
For example , to be able to correct punctuation , it helps that the surrounding words are already correctly normalized .
On the other hand , with punctuation already corrected , it will be easier to normalize the surrounding words .
In this section , we first present our punctuation correction method based on a DCRF model , and then present missing word recovery based on a CRF model .
Next , we present a novel beam-search decoder for normalization of social media text , which can effectively integrate different normalization operations , including statistical and rule- based normalization .
Finally , details of text normalization for Chinese and English are presented .
Punctuation Correction
In normalization of social media text , punctuation correction is also important besides word normalization , as the subsequent NLP applications are typically trained on formal texts with correct punctuation .
We define punctuation correction as correcting punctuation in sentences which may have no or unreliable punctuation .
The task performs three punctuation operations : insertion , deletion , and substitution .
To our knowledge , no previous work has been done on punctuation correction for normalization of social media text .
In ASR , punctuation prediction only inserts punctuation symbols into ASR output that has no punctuation ( Kim and Woodland , 2001 ; Huang and Zweig , 2002 ) , but without punctuation deletion or substitution .
Lu and Ng ( 2010 ) argued that punctuation prediction should be jointly per-formed with sentence boundary detection , so they modeled punctuation prediction using a two -layer DCRF model ( Sutton et al. , 2004 ) .
We also believe that punctuation correction is closely related to sentence boundary detection .
Thus , we propose a two -layer DCRF model for punctuation correction .
Layer 1 gives the actual punctuation tags None , Comma , Period , Question - Mark , and Exclamatory - Mark .
Layer 2 gives the sentence boundary , including tags Declarative -Begin , Declarative -In , Question - Begin , Question - In , Exclamatory - Begin , and Exclamatory - In , indicating whether the current word is at the beginning of ( or inside ) a declarative , question , or exclamatory sentence .
We use word n-grams ( n = 1 , 2 , 3 ) and punctuation symbols within 5 words before and after the current word as binary features in the DCRF model .
As an example , Table 2 shows the tags and features for the word " where " in the message " where | .|? i| can | not | see | you | !|! " , where the punctuation symbols after the vertical bars are the corrected symbols .
Tags Content Layer 1 Question - Mark Layer 2 Question - Begin Features Content unigram < s >@- 1 where@0 i@1 can@2 not@3 see@4 you@5 bigram < s >+ where@ - 1 where + i@0 i+can@1 can+ not@2 not + see@3 see + you@4 you +</ s>@5 trigram < s >+ where + i@ - 1 where + i+ can@0 i+can+ not@1 can+ not + see@2 not + see + you@3 see +you +</s>@4 punctuation .@0 !@5 Table 2 : An example of tags and features used in punctuation correction .
Due to the lack of informal training texts with corrected punctuation , we train our punctuation correction model on formal texts with synthetically created punctuation errors .
We randomly add , delete , and substitute punctuation symbols in formal texts with equal probabilities .
Specifically , for s ? { , .?!} , P ( none |s ) = P ( , |s ) = P ( .|s ) = P ( ?|s ) = P ( !|s ) = 0.2 denotes the probability of replacing a punctuation symbol s ( replacing s by none denotes deletion ) ; and for a real word ( not a punctuation symbol ) w , P ( none | w ) = P ( , |w ) = P ( .|w ) = P ( ?|w ) = P ( !|w ) = 0.2 denotes the probability of inserting a punctuation symbol after w ( inserting none after w denotes no insertion ) .
Missing Word Recovery
As shown in Section 3 , some words are often omitted in social media texts , e.g. , the pronoun " ? [ I ] " in Chinese and be in English .
To fix this problem , we propose a CRF model to recover such missing words .
We explain the CRF model using be in English .
The CRF model has five tags : None , BE , IS , ARE , and AM .
In an input sentence , every token ( including words , punctuation symbols , and a special start- of-sentence placeholder ) will be assigned a tag , denoting the insertion of the form of be after the token .
We use the same n-gram features as our punctuation correction model , but exclude the punctuation features .
The model is trained on synthetically created training texts in which be has been randomly deleted with probability 0.5 .
A Decoder for Text Normalization
When designing our text normalization system , we aim for a general framework that can be applied to text normalization across different languages with minimal effort .
This is a challenging task , since social media texts in different languages exhibit different informal characteristics , as illustrated in Section 3 .
Motivated by the beam-search decoders for SMT ( Koehn et al. , 2007 ) , ASR ( Young et al. , 2002 ) , and grammatical error correction ( Dahlmeier and Ng , 2012 ) , we propose a novel beam-search decoder for normalization of social media text .
Given an input message , the normalization decoder searches for its best normalization , i.e. , the best hypothesis , by iteratively performing two subtasks : ( 1 ) producing new sentence - level hypotheses from hypotheses in the current stack , carried out by hypothesis producers ; and ( 2 ) evaluating the new hypotheses to retain good ones , carried out by feature functions .
Each hypothesis is the result of applying successive normalization operations on the initial input message , where each normalization operation is carried out by one hypothesis producer that deals with one aspect of the informal characteristics of social media text .
The hypotheses are grouped into stacks , where stack i stores all hypotheses obtained by applying i hypothesis producers on the input message .
The beam-search algorithm is shown in Algorithm 1 , and Figure 1 shows an example search tree for an English message .
We give the details of the hypothesis producers for Chinese and English social media texts in the next two subsections .
A number of the hypothesis producers detect and deal with informal words w present in a hypothesis by relying on bigram counts of w in a large corpus of formal texts .
Specifically , a word w in a hypothesis . . . w ?1 ww 1 . . . is considered an informal word if both bigrams w ?1 w and ww 1 occur infrequently ( ? 5 ) in the formal corpus .
Given a hypothesis message h , the feature functions include a language model score ( the normalized sentence probability of h ) , an informal word count penalty ( the number of informal words detected in h ) , and count feature functions .
Each count feature function gives the count of the modifications made by a hypothesis producer .
The feature func-tions are used by the decoder to distinguish good hypotheses from bad ones .
All feature functions are combined using a linear model to obtain the score for a hypothesis h : score ( h ) = i ? i f i ( h ) , ( 1 ) where f i is the i-th feature function with weight ? i .
The weights of the feature functions are tuned using the pairwise ranking optimization algorithm ( Hopkins and May , 2011 ) on the development set .
Text Normalization for Chinese
Taking into account the informal characteristics of Chinese social media text in Section 3 , we design the following hypothesis producers for Chinese text normalization : Dictionary :
We have manually assembled a dictionary of 703 informal -formal word pairs from the Internet .
The word pairs are used to produce new hypotheses .
For example , given a hypothesis " ?
?[ magical horse ] ? ?[ time ] " , if the dictionary contains the word pair " ( ? ? ,
? ?[ what ] ) " , the Dictionary hypothesis producer generates a new hypothesis " ? [ what ] ?[ time ] " .
Punctuation : A punctuation correction model ( Section 4.1 ) is adopted to correct punctuation in the current hypothesis , e.g. , it may normalize " ? ?[ what ] ?[ time ] " into " ? ? ? " .
Pronunciation :
We use Chinese Pinyin to model the pronunciation similarity of words .
To accomplish this , we pair some Pinyin initials that sound similar into a group .
The groups of paired Pinyin initials are ( c , ch ) , ( s , sh ) , and ( z , zh ) .
For example , given the hypothesis " ? [ Beijing ] ?[ tube ] ?[ come ] " , the Pinyin of the informal word " ? ? " is " t ong z i " .
The Pinyin of the formal word " ? [ comrade ] " is " t ong zh i " .
Since the similar sounding Pinyin initials z and zh are paired in a group , a new hypothesis " ?
?[ Beijing ] ? ?[ comrade ] ?[ come ] " can be produced .
In practice , this hypothesis producer can propose many spurious candidates w for an informal word w .
As such , after we replace w by w in the hypothesis , we require that some 4 - gram containing w and its surrounding words in the hypothesis appears in a formal corpus .
We call this filtering process contextual filtering .
Pronoun :
With the method of Section 4.2 , a CRF model is trained to recover the missing pronoun " ? [ I ] " .
Interjection :
If a word w in a pre-defined list of frequent redundant interjections appears at the end of a sentence , we produce a new hypothesis by removing w , e.g. , from " ? [ ok ] ?[ oh ] " to " ? ?[ ok ] " .
Resegmentation :
This hypothesis producer fixes word segmentation problems .
If an informal word is a concatenation of two constituent informal words w 1 and w 2 in our normalization dictionary , the informal word will be segmented into two words w 1 and w 2 .
As a result , the Dictionary hypothesis producer can subsequently normalize w 1 and w 2 .
Text Normalization for English Similar to Chinese text normalization , we also create the Dictionary , Punctuation , and Interjection hypothesis producers for English text normalization .
We also add the following English-specific hypothesis producers : Pronunciation :
This hypothesis producer uses pronunciation similarity to find formal candidates for a given informal word .
It considers a word as a sequence of letters and convert it into a sequence of phones using phrase - based SMT trained on the CMU pronouncing dictionary ( Weide , 1998 ) .
Similar sounding phones are paired together in a group : ( ah , ao ) , ( ow , uw ) , and ( s , z ) .
To illustrate , in the hypothesis " wat is it " , the informal word " wat " maps to the phone sequence " w ao t " .
Since the formal word " what " maps to the phone sequence " w ah t " and the phones ah and ao are paired in a group , the new hypothesis " what is it " is generated .
Be : We train a CRF model to recover missing words be , as described in Section 4.2 .
Retokenization :
This hypothesis producer fixes tokenization problems .
More precisely , given an informal word which is not a URL or email address and contains a period , it splits the informal word at the period .
For example , " how r u.where r u " is normalized to " how r u . where r u " .
Prefix :
This hypothesis producer generates a formal word w for an informal word w if w is a prefix of w .
To avoid spurious candidates , we only generate w if | w| ? 3 and |w | ? | w| ?
4 . Quotation :
If an informal word ends with a letter in ( m , s , t ) and if the word produced by inserting a quotation mark before the letter is a formal word , a new hypothesis with the quotation mark inserted is produced .
This hypothesis producer thus generates " i'm " from " im " , " she 's " from " shes " , " is n't " from " isnt " , etc .
Abbreviation : Letters denoting the vowels in a formal word are often deleted to form an informal word .
This hypothesis producer generates a formal word w from an informal word w if w can be obtained from w by adding missing vowels .
To avoid spurious candidates , we only consider w where | w| ?
2 . Time : If a number can be a potential time expression and appears after " at " or before " am " or " pm " , a new hypothesis is produced by changing the number into a time expression , e.g. , " 1130 am " is normalized to " 11 : 30 am " .
Since the Pronunciation , Prefix , and Abbreviation hypothesis producers can propose spurious candidates for an informal word , we also use contextual filtering to further filter the candidates for these hypothesis producers .
Experiments
Evaluation Corpora
As previous work ( Choudhury et al. , 2007 ; Han and Baldwin , 2011 ; Liu et al. , 2012 ) mostly focused on word normalization , no data is available with corrected punctuation and recovered missing words .
We thus create the following two corpora ( Table 3 ) : Chinese-English corpus
We crawled 1,000 messages from Weibo which were first normalized into formal Chinese and then translated into formal English .
The first half of the corpus serves as our development set to tune our text normalization decoder for Chinese , while the second half serves as the test set to evaluate text normalization for Chinese and Chinese-English MT .
English - Chinese corpus
From the NUS English SMS corpus ( How and Kan , 2005 ) , we randomly selected 2,000 messages .
The messages were first normalized into formal English and then translated into formal Chinese .
Similar to the Chinese- English corpus , the first half of the corpus serves as our development set while the second half serves as the test set .
The formal corpus used ( as described in Section 4 ) is the concatenation of two Chinese - English spoken parallel corpora : the IWSLT 2009 corpus ( Paul , 2009 ) and another spoken text corpus collected at the Harbin Institute of Technology 3 .
The language model used for Chinese ( English ) text normalization is the Chinese ( English ) side of the formal corpus and the LDC Chinese ( English ) Gigaword corpus .
Corpus
To evaluate the effect of text normalization on MT , we build phrase - based MT systems using Moses ( Koehn et al. , 2007 ) , with word alignments generated by GIZA ++ ( Och and Ney , 2003 ) .
The MT training data contains the above formal corpus and some LDC 4 parallel corpora ( LDC2000T46 , LDC2002E18 , LDC2003E14 , LDC2004E12 , LDC2005T06 , LDC2005T10 , LDC2007T23 , LDC2008T06 , LDC2008T08 , LDC2008T18 , LDC2009T02 , LDC2009T06 , LDC2009T15 , LDC2010T03 ) .
In total , 214M/192M
English / Chinese tokens are used to train our MT systems .
The language model of the Chinese-English ( English - Chinese ) MT system is the English ( Chinese ) side of the FBIS corpus ( LDC2003E14 ) and the English ( Chinese ) Gigaword corpus .
Our MT systems are tuned on the manually normalized messages of our development sets .
Following ( Aw et al. , 2006 ; Oliva et al. , 2013 ) , we use BLEU scores ( Papineni et al. , 2002 ) to evaluate text normalization .
We also use BLEU scores to evaluate MT quality .
We use the sign test to determine statistical significance , for both text normalization and translation .
Baselines
We compare our text normalization decoder against three baseline methods for performing text normalization .
We then send the respective normalized texts to the same MT system to evaluate the effect of text normalization on MT .
The simplest baseline for text normalization is one that does no text normalization .
The raw text ( un-normalized ) is simply passed on to the MT system for translation .
We call this baseline ORIGINAL .
The second baseline , LATTICE , is to use a lattice to normalize text .
For each input message , a lattice is generated in which each informal word is augmented with its formal candidates taken from the same normalization dictionary ( downloaded from Internet ) used in our text normalization decoder .
The lattice is then decoded by the same language model used in our text normalization decoder to generate the normalized text ( Stolcke , 2002 ) .
Another possible way of using lattice is to directly feed the lattice to the MT system ( Eidelman et al. , 2011 ) , but since in this paper , we assume that the MT system can only translate plain text , we leave this as future work .
The third baseline , PBMT , is a competitive baseline that performs text normalization via phrasebased MT , as proposed in Aw et al . ( 2006 ) .
Moses ( Koehn et al. , 2007 ) is used to perform text normalization , by " translating " un-normalized text to normalized text .
The training data used is the same development set used in our text normalization decoder .
The normalized text is then sent to our MT system for translation .
This method was also used in the SMS translation task of WMT 2011 by ( Stymne , 2011 ) .
In the tables showing experimental results , normalization and translation BLEU scores that are significantly higher than ( p < 0.01 ) the LATTICE or PBMT baseline are in bold or underlined , respectively .
Chinese-English Experimental Results
The Chinese-English normalization and translation results are shown in Table 4 .
The first group of experiments is the three baselines , and the second group is an oracle experiment using manually normalized messages as the output of text normaliza - tion which indicates the theoretical upper bounds of perfect normalization .
In the normalization experiments , the ORIGINAL baseline gets a BLEU score of 61.01 % , and the LATTICE baseline greatly improves the ORIGINAL baseline by 13.51 % , which shows that the dictionary collected from the Internet is highly effective in text normalization .
The PBMT baseline further improves the BLEU score by 2.25 % .
In the corresponding MT experiments , as the normalization BLEU scores increase , the MT BLEU scores also increase .
The third group is the isolated experiments , i.e. , each experiment only uses one hypothesis producer .
As expected , the individual hypothesis producers alone do not work well except the Dictionary hypothesis producer .
One interesting discovery is that the Dictionary hypothesis producer outperforms the LATTICE baseline , which shows that our normalization decoder can utilize the dictionary more effectively , probably because of the additional features used in our normalization decoder such as the informal word penalty .
The Resegmentation hypothesis producer alone worsens the BLEU scores , since it can only split informal words , and is designed to work together with other hypothesis producers to normalize words .
The last group is the combined experiments .
We add each hypothesis producer in the order of its normalization effectiveness in the isolated experiments .
Adding the Punctuation hypothesis producer greatly improves the BLEU scores of both normalization and translation , which confirms the importance of punctuation correction .
The Pronoun and Interjection hypothesis producers also contribute some improvements .
Finally , Resegmentation significantly improves the normalization / translation BLEU scores by 1.42%/0.35 % .
Compared with the isolated experiments , the combined experiments show that our normalization decoder can effectively integrate different hypothesis producers to achieve better performance for both text normalization and translation .
Overall , in the Chinese text normalization experiments , our normalization decoder outperforms the best baseline PBMT by 9.98 % in BLEU score .
In the Chinese- English MT experiments , the normalized texts output by our normalization decoder lead to improved translation quality compared to normalization by the PBMT baseline , by 1.38 % in BLEU score .
English - Chinese Experimental Results
The English - Chinese normalization and translation results are shown in Table 5 , with the same experimental setup as in the Chinese -English experiments .
The text normalization BLEU score of the ORIG -INAL baseline is much lower in English compared to Chinese , since the English texts contain more informal words .
Again , the individual hypothesis producers alone do not work well , except the Dictionary hypothesis producer .
The Retokenization hypothesis producer greatly improves the normalization / translation BLEU scores by 2.37%/0.86 % .
The Punctuation hypothesis producer helps less for English compared to Chinese , suggesting that our Chinese texts contain noisier punctuation .
Overall , we achieved similar improvements in English text normalization and English - Chinese translation , and the improvements in BLEU scores are 7.35 % and 1.35 % respectively .
Further Analysis
The effect of contextual filtering .
To measure the effect of contextual filtering proposed in Section 4.4 , we ran our normalization decoder without contextual filtering .
We obtained BLEU scores of 65.05%/22.38 % in the English - Chinese experiments , which were lower than 66.54% /22.81 % ob- tained with contextual filtering .
This shows the beneficial effect of contextual filtering .
Decoding speed .
The decoding speed of our text normalization decoder was 0.2 seconds per message on our test sets , using a 2.27 GHz Intel Xeon CPU with 32 GB memory .
The effect of text normalization decoder on MT .
We manually analyzed the effect of our text normalization decoder on MT .
For example , given the un-normalized English test message " yeah must sign up , im in lt25 " , our English - Chinese MT system translated it into " ?[ yeah ] ?[ must ] ? ?[ sign up ] ? im ?[ in ] lt25 "
On the other hand , our normalization decoder normalized it into " yeah must sign up , i 'm in lt25 . " which was then translated into " ?
? ? , ? ? lt25 ? " by our MT system .
This example shows that our text normalization decoder uses word normalization and punctuation correction to improve translation .
Conclusion
This paper presents a novel beam-search decoder for normalization of social media text .
Our decoder for text normalization effectively integrates multiple normalization operations .
In our experiments , we achieved statistically significant improve-ments over two strong baselines : an improvement of 9.98 % / 7.35 % in BLEU scores for normalization of Chinese / English social media text , and an improvement of 1.38 % / 1.35 % in BLEU scores for translation of Chinese / English social media text .
Future work can investigate how to more tightly integrate our beam-search decoder for text normalization with a standard MT decoder , e.g. , by using a lattice or an n-best list .
Figure 1 : 1 Abbreviation : whr= > where
