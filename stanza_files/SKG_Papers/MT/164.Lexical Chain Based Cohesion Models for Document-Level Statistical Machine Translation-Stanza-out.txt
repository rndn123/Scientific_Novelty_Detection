title
Lexical Chain Based Cohesion Models for Document -Level Statistical Machine Translation
abstract
Lexical chains provide a representation of the lexical cohesion structure of a text .
In this paper , we propose two lexical chain based cohesion models to incorporate lexical cohesion into document - level statistical machine translation : 1 ) a count cohesion model that rewards a hypothesis whenever a chain word occurs in the hypothesis , 2 ) and a probability cohesion model that further takes chain word translation probabilities into account .
We compute lexical chains for each source document to be translated and generate target lexical chains based on the computed source chains via maximum entropy classifiers .
We then use the generated target chains to provide constraints for word selection in document - level machine translation through the two proposed lexical chain based cohesion models .
We verify the effectiveness of the two models using a hierarchical phrase - based translation system .
Experiments on large-scale training data show that they can substantially improve translation quality in terms of BLEU and that the probability cohesion model outperforms previous models based on lexical cohesion devices .
Introduction Given a source document , traditionally most statistical machine translation ( SMT ) systems translate the document sentence by sentence .
In such a translation scheme , sentences are translated independent of any other sentences .
However , a text is normally written cohesively , in which sentences are connected *
Corresponding author to each other via syntactic and lexical devices .
This linguistic phenomenon is called as textual cohesion ( Halliday and Hasan , 1976 ) .
Cohesion is a surface - level property of wellformed texts .
It deals with five categories of relationships between text units , namely co-reference , ellipsis , substitution , conjunction and lexical cohesion that is realized via semantically related words .
The former four cohesion relations can be grouped as grammatical cohesion .
Generally speaking , grammatical cohesion is less common and harder to identify than lexical cohesion ( Barzilay and Elhadad , 1997 ) .
As most SMT systems translate a text in a sentence - by-sentence fashion , they tend to build less lexical cohesion than human translators ( Wong and Kit , 2012 ) .
We therefore study lexical cohesion for document- level translation .
We use lexical chains ( Morris and Hirst , 1991 ) to capture lexical cohesion in a text .
Lexical chains are connected graphs that represent the lexical cohesion structure of a text .
They have been successfully used for information retrieval ( Stairmand , 1996 ) , document summarization ( Barzilay and Elhadad , 1997 ) and so on .
In this paper , we investigate how lexical chains can be used to incorporate lexical cohesion into document - level translation .
Our basic assumption is that the lexical chains of a target document are direct correspondences of the lexical chains of its counterpart source document .
This assumption is reasonable as the target document translation should be faithful to the source document in terms of both text meaning and structure .
Based on this assumption , we propose a framework to incorporate lexical cohesion into target document translation via lexical chains , which works as follows .
?
Compute lexical chains for each source document that is to be translated ; ?
Project the computed source lexical chains onto the corresponding target document by translating source chain words into target chain words using maximum entropy classifiers ; ?
Incorporate lexical cohesion into the target document translation via cohesion models built on the projected target lexical chains .
We build two lexical chain based cohesion models .
The first model is a count model that rewards a hypothesis whenever a word in the projected target lexical chains occur in the hypothesis .
As a source chain word may be translated into many different target words , we further extend the count model to a second cohesion model : a probability model that takes chain word translation probabilities into account .
We test the two lexical chain based cohesion models on a hierarchical phrase - based SMT system that is trained with large-scale Chinese - English bilingual data .
Experiment results show that our lexical chain based cohesion models can achieve substantial improvements over the baseline .
Furthermore , the probability cohesion model is better than the count model and it also outperforms previous cohesion models based on lexical cohesion devices .
To the best of our knowledge , this is the first attempt to explore lexical chains for statistical machine translation .
The remainder of this paper is organized as follows .
Section 2 discusses related work and highlights the differences between our method and previous work .
Section 3 briefly introduces lexical chains and algorithms that compute lexical chains .
Section 4 elaborates the proposed lexical chain based framework , including details on source lexical chain computation , target lexical chain generation and the two lexical chain based cohesion models .
Section 5 presents our large-scale experiments and results .
Finally , we conclude with future directions in Section 6 .
Related Work Recent years have witnessed growing research interests in document-level statistical machine translation .
Such research efforts can be roughly divided into two groups : 1 ) general document- level machine translation that does not explore or explores very little linguistic discourse information ; 2 ) linguistically - motivated document- level machine translation that incorporates discourse information such as cohesion and coherence into SMT .
Recent studies ( Guillou , 2013 ; Beigman Klebanov and Flor , 2013 ) show that this discourse information is very important for document - level machine translation .
General Document - Level Machine Translation Tiedemann ( 2010 ) propose cache- based language and translation models for document - level machine translation .
These models are built on recently translated sentences .
Following this cache- based approach , Gong et al . ( 2011 ) further introduce two additional caches .
They use a static cache to store bilingual phrases extracted from documents in training data that are similar to the document being translated .
They also adopt a topic cache with target language topic words .
Xiao et al. ( 2011 ) study the translation consistency issue in document - level machine translation .
They use a hard constraint to consistently translate ambiguous source words into the most frequent translation options .
Ture et al. ( 2012 ) soften this consistency constraint by integrating three counting features into decoder .
Using Lexical Cohesion Devices in Document - Level SMT
Lexical cohesion devices are semantically related words , including word repetition , synonyms / near-synonyms , hyponyms and so on .
They are also the cohesion - building elements in lexical chains .
Wong and Kit ( 2012 ) use lexical cohesion device based metrics to improve machine translation evaluation at the document level .
These metrics measure the proportion of content words that are used as lexical cohesion devices in machine - generated translations .
Hardmeier et al. ( 2012 ) propose a documentwide phrase - based decoder and integrate a semantic language model into the decoder .
They argue that their semantic language model can capture lexical cohesion by exploring n-grams that cross sentence boundaries .
Most recently integrate three categories of lexical cohesion devices into document - level machine translation .
They define three cohesion models based on lexical cohesion devices : a direct reward model , a conditional probability model and a mutual information trigger model .
The latter two models measure the strength of lexical cohesion relation between two lexical items .
They are incorporated into SMT to calculate how appropriately lexical cohesion devices are used in document translation .
As lexical chains capture lexical cohesion relations among sequences of related words rather than those only between two words , experiments in Section 5 show that our lexical chain based probability cohesion model is better than the lexical cohesion device based trigger model , which is the best among the three cohesion models proposed by .
Modeling Coherence in Document - Level SMT
In discourse analysis , cohesion is often studied together with coherence which is another dimension of the linguistic structure of a text ( Barzilay and Elhadad , 1997 ) .
Cohesion is related to the surface structure of a text while coherence is concerned with the underlying meaning connectedness in a text ( Vasconcellos , 1989 ) .
Compared with cohesion , coherence is not easy to be detected .
Even so , various models have been proposed to explore coherence for document summarization and generation ( Barzilay and Lapata , 2008 ; Louis and Nenkova , 2012 ) .
Following this line , integrate a topic-based coherence model into document - level machine translation , where coherence is defined as a continuous sentence topic transition .
Our lexical chain based cohesion models are also related to previous work on using word and phrase sense disambiguation for lexical choice in SMT ( Carpuat and Wu , 2007 b ; Carpuat and Wu , 2007a ; Chan et al. , 2007 ) .
The difference is that we use document -wide lexical chains to build our cohesion models rather than sentence - level context features .
In our framework , lexical choice is performed to make the selected words consistent with the lexical cohesion structure of a document .
Carpuat ( 2009 ) explores the principle of one sense per discourse ( Gale et al. , 1992 ) in the context of SMT and imposes the constraint of one translation per discourse on document translation .
We also use the one sense per discourse principle to perform word sense disambiguation on the source side in our lexical chaining algorithm ( See Section 4.1 ) .
Background : Lexical Chain and Chain Computation Lexical chains are sequences of semantically related words ( Morris and Hirst , 1991 ) .
They represent the lexical cohesion structure of a text .
Figure 2 displays six lexical chains computed from the Chinese news article shown in Figure 1 . Words in these lexical chains have lexical cohesion relations such as repetition , synonym , which may range over the entire text .
For example , in the lexical chain LC 1 of Figure 2 , the same word " dWgu_ " ( Germany ) repeats 9 times .
In the lexical chain LC 3 , the two words " z`ngcSi " ( president ) and " zhdx [ " ( chairman ) are synonym words .
Generally , a text can have many different lexical chains , each of which represents a thread of cohesion through the text .
Several lexical chaining algorithms have been proposed to compute lexical chains from texts .
Normally they need an ontology to obtain semantic relations between words .
Word sense disambiguation ( WSD ) is also used to determine the sense of each word in a text .
Generally a lexical chain computation algorithm completes the following three subtasks : ?
Building a representation of a text with a set of candidate words and assigning semantic relations between the candidate words according to the ontology ; ?
Choosing the right sense for each candidate word via WSD ; ?
Building chains over the semantically related and disambiguated candidate words .
These three sub-tasks can be done separately or simultaneously .
Morris and Hirst ( Morris and Hirst , 1991 ) define the first lexical chain computation algorithm that adopts a greedy strategy to immediately disambiguate a word at its first occurrence .
This algorithm runs in linear time but suffers from inaccurate disambiguation .
Barzilay and Elhadad ( Barzilay and Elhadad , 1997 ) accuracy by processing all possible combinations of word senses in a text to disambiguate words .
Unfortunately , their algorithm runs slowly in quadratic time .
Galley and Mckeown ( 2003 ) present an algorithm that are better than the former two algorithms both in terms of running efficiency and WSD accuracy .
They separate the WSD sub-task from the task of lexical chain building and impose a " one sense per discourse " constraint in the WSD step .
Translating Documents Using Lexical Chains
In this section , we describe how we incorporate lexical cohesion into document - level machine translation using lexical chains .
We divide the lexical chain based document- level machine translation process into three steps : ( 1 ) computing lexical chains for source documents with a source language ontology , ( 2 ) generating target lexical chains from the computed source lexical chains , and finally ( 3 ) incorporating lexical cohesion encoded in the generated target lexical chains into document - level translation via lexical chain based cohesion models .
The remainder of this section will elaborate these three steps .
Source Lexical Chains Computation
We follow the chain computation algorithm introduced by Galley and McKeown ( 2003 ) to build lexical chains on source ( Chinese ) documents .
In the algorithm , the chaining process includes three steps : choosing candidate words to build a disambiguation graph ( Galley and McKeown , 2003 ) for each document , disambiguating the candidate words and finally building lexical chains over the disambiguated candidate words .
The disambiguation graph can be considered as a representation of all possible interpretations of its corresponding text .
In the graph , nodes are candidate words with different senses and edges between word senses are weighted according to their semantic relations , such as synonym , hypernym and so on .
We use an extended version of a Chinese thesaurus Tongyici Cilin ( Cilin for short ) to define word senses and semantic relations between senses .
The ex- tended Cilin contains 77,343 Chinese words , which are organized in a hierarchical structure containing 5 levels as shown in Figure 3 .
In the 5th level , each node represents an atomic concept which consists of a set of synonyms .
These atomic concepts are just like synsets in WordNet .
We use them to represent senses of words in the disambiguation graph .
We select nouns , verbs , abbreviations and idioms as candidate words for the disambiguation graph .
These words are identified by a Chinese part-tospeech tagger LTP ( Che et al. , 2010 ) in a preprocessing step .
In order to build the disambiguation graph , we first build an array indexed by the atomic concepts of Cilin , then insert a copy of each candidate word into its all concept ( sense ) entries in the array .
After that , we create all semantic links among senses of different candidate words in the disambiguation graph following Galley and McKeown ( 2003 ) .
In the second step , we use the principle of one sense per discourse to perform WSD for each candidate word in the disambiguation graph .
We sum the weights of all semantic links under the different senses of the candidate word in question .
The sense with the highest sum of weights is considered as the most probable sense for this word .
We then assign this sense to all occurrences of the word in the document by adopting the constraint of one sense per discourse .
Once all candidate words are disambiguated , we can build lexical chains over these words by removing all semantic links that connect those unselected word senses .
The six lexical chains shown in Figure 2 are computed from the Chinese document in Figure 1 exactly following the algorithm of Galley and McKeown ( 2003 ) .
The only difference is that we use Cilin rather than WordNet as the ontology .
Target Lexical Chains Generation
Since a faithful target document translation should follow the same cohesion structure as that in its corresponding source document , we generate target lexical chains from the computed source lexical chains .
Given a source lexical chain LC s = {s j i } where the ith chain word s j i is from the jth sentence of the source document D s , we generate a target lexical chain LC t = {t j i } using maximum entropy ( Max - Ent ) classifiers .
Particularly , we translate a word s j i in the source lexical chain into a target word t j i in the target lexical chain using a corresponding Max - Ent classifier as follows 1 . P ( t j i | C( s j i ) ) = exp ( k ? k f k ( t j i , C(s j i ) ) ) t exp ( k ? k f k ( t , C(s j i ) ) ) ( 1 ) where f k are binary features , ? k are weights of these features , and C(s j i ) is the surrounding context of chain word s j i .
We train one MaxEnt classifier per unique source chain word .
For each classifier , we define two groups of binary features : 1 ) the preceding and succeeding two words of s j i in the jth sentence ( { w ?2 , w ?1 , s j i , w + 1 , w +2 } ) ; 2 ) the preceding and succeeding one word of s j i in the lexical chain LC s ( { s p i?1 , s j i , s q i +1 } ) .
All features are in the following binary form .
f ( t j i , C(s j i ) ) = 1 , if t j i = ? and C(s j i ) .?
= ? 0 , else ( 2 ) where the symbol ? is a placeholder for a possible target word , the symbol ?
indicates a contextual element for the chain word s j i ( e.g. , the preceding word in the jth sentence or the succeeding word in the lexical chain LC s ) , and the symbol ? represents the value of ?.
Given a source document D s and its N lexical chains { LC k s } N k=1 computed from the document as described in Section 4.1 , we can generate the N target lexical chains { LC k t } N k=1 using our MaxEnt classifiers .
Each target word t j i in the target lexical chain LC k t is the translation of its corresponding source word s j i in the source lexical chain LC k s with the highest probability P ( t j i | C( s j i ) ) according to Eq. ( 1 ) .
As we know , the MaxEnt classifier can generate multiple translations for each source word .
In order to incorporate these multiple chain word translations , we can generate a super target lexical chain LC t from a source lexical chain LC s , where is a pre-defined threshold used to select multiple translations .
For example , given a source lexical chain LC s = { a , b , c} , we can have the corresponding super target lexical chain LC t = { { a 1 t , a 2 t ...} , {b 1 t , b 2 t ...} , {c 1 t , c 2 t . ..}} , where x i t is the translation of x with a translation probability P ( x i t | C ( x ) ) ? according to Eq. ( 1 ) .
Integrating multiple translations for each source chain word , we can reduce the error propagation of the MaxEnt classifier to some extent .
Our experiments also confirm that the super target lexical chains with multiple translation options for each chain word are better than the target lexical chains with only one translation per chain word .
Therefore we build our cohesion models based on the super target lexical chains , which will be described in the next section .
Lexical Chain Based Cohesion Models
Once we generate the super target lexical chains { LC k t } N k=1 for the target document D t , we can use them to provide constraints for the target document translation .
Our key interest is to make the target document translation T Dt as cohesive as possible .
We therefore propose lexical chain based cohesion models to measure the cohesion of the target document translation .
The basic idea is to reward a translation hypothesis if a word from the super target lexical chains occurs in the hypothesis .
According to the difference in the reward strategy , we have two cohesion models : a count cohesion model and a probability cohesion model .
Count Cohesion Model M c ( T Dt , { LC k t } N k=1 ) :
This model rewards a translation hypothesis of the jth sentence in the document whenever a lexical chain word t j i occurs in the hypothesis .
The model maintains a counter and accumulates the counter when necessary .
It is factorized into the sentence cohesion metric M c ( T j , { LC k t } N k=1 ) , where T j is the translation of the jth sentence in the target document .
M c ( T j , { LC k t } N k=1 ) is formulated as follows .
M c ( T j , { LC k t } N k=1 ) = w?T j t j i ?C e ?( w , t j i ) ( 3 ) where C represents { LC k t } N k=1 , and the ? function is defined as follows .
?( w , t j i ) = 1 , if t j i = w 0 , otherwise ( 4 ) Probability Cohesion Model M p ( T Dt , { LC k t } N k=1 ) :
This model rewards a translation hypothesis according to the translation probability of a chain word that occurs in the hypothesis .
The translation probability is computed by Eq. ( 1 ) .
The model is also factorized into the sentence cohesion metric M p ( T j , { LC k t } N k=1 ) which is formulated as follows .
M p ( T j , { LC k t } M k=1 ) = w?T j t j i ?C e ?( w , t j i ) ? P ( t j i | C( s j i ) ) ( 5 ) where P ( t j i | C( s j i ) is the translation probability computed according to Eq. ( 1 ) .
Decoding
The proposed lexical chain based cohesion models are integrated into the log-linear translation framework of SMT as a cohesion feature .
Before translating a source document , we compute lexical chains for the source document as described in Section 4.1 .
We then generate the super target lexical chains .
In order to efficiently calculate our lexical chain based cohesion models , we reorganize words in the super target lexical chains into vectors .
We associate each source sentence S j a vector to store target lexical chain words that are to occur in the corresponding target sentence T j .
Although we still translate a source document sentence by sentence , we capture the global cohesion structure of the document via lexical chains and use the lexical chain based cohesion models to constrain word selection in document translation .
Figure 4 shows the architecture of an SMT system with the lexical chain based cohesion model .
Experiments
In this section , we conducted a series of experiments to validate the effectiveness of the proposed lexical chain based cohesion models for Chinese - to - English document - level machine translation .
We used a hierarchical phrased - based SMT system ( Chiang , 2007 ) trained on large-scale data .
In particular , we aim at : ?
Measuring the impact of the threshold on the probability cohesion model and selecting the best threshold on a development test set .
?
Investigating the effect of the two lexical - chain based cohesion models .
?
Comparing our lexical chain based cohesion models against the previous lexical cohesion device based models .
Setup
We collected our bilingual training data from LDC , which includes the corpus LDC2002E18 , LDC2003E07 , LDC2003E14 , LDC2004E12 , LDC2004T07 , LDC2004T08 ( Only Hong Kong News ) , LDC2005T06 and LDC2005T10 .
The collected bilingual training data contains 3.8 M sentence pairs with 96.9M Chinese words and 109.5 M English words .
We trained a 4 - gram language model on the Xinhua portion of the English Gigaword corpus ( 306 million words ) via the SRILM toolkit ( Stolcke , 2002 )
In order to build the lexical chain based cohesion models , we selected corpora with document boundaries explicitly provided from the bilingual training data together with the whole Hong Kong parallel text corpus as the cohesion model training data 2 .
We show the statistics of these selected corpora in Table 1 .
They contain 103,236 documents and 2.80 M sentences .
Averagely , each document consists of 28.4 sentences .
From the source documents of the selected corpora , we extract 3.52 M lexical chains .
On average , there are 35.72 lexical chains per document and 14.81 words per lexical chain .
We used the off-the-shelf MaxEnt toolkit 3 to train one MaxEnt classifier per unique source lexical chain word ( 61,121 different source chain words in total ) .
We performed 100 iterations of the L-BFGS algorithm implemented in the training toolkit for each chain word with both Gaussian prior and event cutoff set to 1 to avoid overfitting .
After event cutoff , we have an average of 17.75 different classes ( target translations ) per source chain word .
We used the NIST MT05 as the tuning set for the minimum error rate training ( MERT ) [ Och , 2003 ] et al. , 2002 ) as our evaluation metric .
As MERT is normally instable , we ran the tuning process three times for all our experiments and presented the average BLEU scores on the three MERT runs as suggested by Clark et al ( 2011 ) .
Setting the Threshold
As the two lexical chain based cohesion models are built on the super target lexical chains that are associated with a parameter , we need to tune the threshold parameter on the development test set NIST MT06 .
We conducted a group of experiments using the probability cohesion model defined in Eq. ( 5 ) to find the best threshold .
Experiment results are shown in Table 2 .
If we set the threshold too small ( e.g. , 0.05 ) , the super target lexical chains may contain too many noisy words that are not the translations of source lexical chain words , which may jeopardise the quality of the super target lexical chains .
The cohesion model built on these noisy super target lexical chains may select incorrect words rather than the proper lexical chain words .
On the other hand , if we set the threshold too large ( e.g. , 0.3 or 0.4 ) , we may take the risk of not selecting the appropriate chain word translations into the super target lexical chains .
It seems that the best threshold is 0.1 as we obtained the highest BLEU score 31.64 on the NIST MT06 with this threshold .
Therefore we set the threshold to 0.1 in all experiments thereafter .
Effect of the Count and Probability Cohesion Model After we found the best threshold , we carried out experiments to test the effect of the two lexical chain based cohesion models : the count and probability cohesion model .
baseline system that does not integrate any lexical chain information .
We also compared the count cohesion model ( LexChainCount ( top1 ) ) built on the target lexical chains where each target chain word is the best translation of its corresponding source lexical chain word according to Eq. ( 1 ) .
Experiment results are shown in Table 3 . From Table 3 , we can observe that ?
Our lexical chain based cohesion models are able to substantially improve the translation quality in terms of BLEU score .
We achieve an average improvement of up to 1.21 BLEU points over the baseline on the two test sets MT06 and MT08 .
?
The count cohesion model built on the super target lexical chains is better than that based on the target lexical chains only with top one translations ( 27.07 vs. 26.99 ) .
This shows the advantage of the super target lexical chains { LC k t } N k=1 over the standard target lexical chians { LC k t } N k=1 . ?
Finally , the probability cohesion model is much better than the count cohesion model ( 28.09 vs. 27.07 ) .
This suggests that we should take into account chain word translation probabilities when we reward hypotheses where target lexical chain words occur .
Lexical Chains vs. Lexical Cohesion Devices
As we have mentioned in Section 2 , lexical cohesion devices can be also used to build lexical cohesion models to capture lexical cohesion relations in a text .
We therefore want to compare our lexical chain based cohesion models with the lexical cohesion device based cohesion models .
We re-implemented the mutual information trigger model that is the best lexical cohesion model based on lexical cohesion devices among the three models proposed by .
The mutual information trigger model measures the association strength of two lexical cohesion items x and y in a lexical cohesion relation xRy .
In the model , it is required that x occurs in a sentence preceding the sentence where y occurs and that the two items have a lexical cohesion relation such as word repetition , synonym .
The model treats x as the trigger and y as the triggered item .
The mutual information between the trigger x and the triggered item y estimates how possible y will occur given x is mentioned in a text .
System
The comparison results are reported in Table 4 .
Our lexical chain based probability cohesion model outperforms the lexical cohesion device based trigger model by 0.36 BLEU points .
The reason for this superiority of our cohesion model over the trigger model may be that the former model captures lexical cohesion relations among sequences of words through lexical chains while the latter model captures lexical cohesion relations only between two related words .
Conclusions
We have presented two lexical chain based cohesion models that incorporate the lexical cohesion structure of a text into document - level machine translation .
We project the lexical chains of a source document to the corresponding target document by translating each word in each source lexical chain into their counterparts via MaxEnt classifiers .
The projected target lexical chains provide a representation of the lexical cohesion structure of the target document that is to be generated .
We build two cohesion models based on the projected target lexical chains : a count model that rewards a hypothesis according to the time of occurrence of target lexi-cal chain words in the hypothesis and a probability model that further takes translation probabilities into account when rewarding hypotheses .
These two cohesion models are used to constrain word selection for document translation so that the generated document is consistent with the projected lexical cohesion structure .
We have integrated the two proposed cohesion models into a hierarchical phrase - based SMT system .
Experiment results on large-scale data validate that ?
The lexical chain based cohesion models are able to substantially improve translation quality in terms of BLEU .
?
The probability cohesion model is better than the count cohesion model .
?
The lexical chain based probability cohesion model is better than the previous mutual information trigger model that adopts lexical cohesion devices to capture lexical cohesion relations between two related words .
As we mentioned in Section 2 , cohesion is closely connected to coherence .
It provides a surface indicator for coherence identification ( Barzilay and Elhadad , 1997 ) .
In the future , we would like to use lexical chains to identify coherence and incorporate both cohesion and coherence into document - level machine translation .
Figure 2 : 2 Figure 2 : Six lexical chains from the example in Figure 1 .
Figure 3 : 3 Figure 3 : The architecture of the extended Cilin .
For simplicity , we only draw a binary tree to represent the hierarchical structure of Cilin .
This does n't mean that each semantic class at level i has only two sub-classes at level i +
1 . Actually , they have multiple sub-classes .
Figure 4 : 4 Figure 4 : Architecture of an SMT system with the lexical chain based cohesion model .
significantly improve WSD dWgu_ diUnx ]n g^ngsZ z`ngcSi su`ma c[ zh [ dWgu_ diUnx ] n g^ngsZ xuRnbe , qiSn jiRnsh ] hu ] zhdx [ qZsh [ Yr su ] de xZlYXr jiRng dRnrYn gRi g^ngsZ de l[nsh [ z`ngcSi , wWiqZ lie gY yuY , zh[ d Uo su`ma de j]rYn rWnxuTn jiVrYn wWizh\ " ( fTxZnshY b^Sng diUn ) dWgu_ diUnx ]n g^ngsZ z`ngcSi su`ma j ZntiRn c[ qe tR de zh[ we , tR shu^ , y_uyc tR xiTnrSn bc zUi shaudUo dWgu_ diUnx ] n g^ngsZ jiRnsh ] hu ] de ch^ngfYn x]nrYn , c [ zh [ sh ] tR wWiyZ de xuTnzW " t_uzZrWn huRny [ ng zhYxiUng xuRnbe , dWgu_ diUnx ] n g^ngsZ de gdpiUo yZnc\ zUi fTlSnkYfc gdpiUo jiRoy ] sh] chTng shUng zhTng bTifYnzhZsh [ yZ y\shUng " su`ma zUi dWgu_ diUnx ] n g^ngsZ b^Sng z`ngbe zhUokRi jiRnsh ] hu ] tYbiW hu ] y ] zh^ng fRbiTo yZ xiUng shVngm [ ng , tR shu^ : 7 w`y \ yUoqic jiRnsh ] hu ] jiXchc w`de zh[ we " 8 y_uyc liTng gY yuY hau jiRng jdx [ ng dUxuTn , dUn liSnhWzhYngfd z Ui m [ n diUo zh^ng shVngwUng luahau , dWgu_ z`ngl\ shZ ruadW s]hb xZwUng zUi gdjiU xiUcua zh ] xZn dZ sh [ , dWgu_ diUnx ] n g^ngsZ shebTiwUn m[ng xiTo gdd^ng de zZjZn b]ng wYi xiRoshZ , Wr zhZch [ tR " dWgu_ diUnx ] n gdjiU haulSi hu [ wXn , y\ sh[ yZdiTnyZbR ^uyuSn zua sh^u , shUngzhTng bTifYnzhZbRdiTnwds ] " dWgu_ cSizhYngbe huRny [ ng su`ma c [zh [ de juWd ]ng " Figure 1 : An example of a Chinese news article ( written in pinyin ) .
LC 1 : { dWgu_ , dWgu_ , b^ , dWgu_ , dWgu_ , dWgu_ , dWgu_ , b^ , dWgu_ , dWgu_ , dWgu_ } LC 2 :{ jiRnsh ] hu ] , fTxZnshY , jiRnsh ] hu ] , z`ngbe , jiRnsh ] hu ] , jiRnsh ] hu ] } LC 3 : { z`ngcSi , zhdx [ , z`ngcSi , z`ngl\ } LC 4 : { c [ zh [ , c [ qe , c [zh [ , c [ zh [ }
LC 5 : { zhTng , xiUcua , shUngzhTng } LC 6 : { xuRnbe , xuRnbe , fRbiTo }
with Kneser - Ney smoothing .
Training MT05 MT06 MT08 # Doc 103,236 100 79 109 # Sent 2.80M 1,082 1,664 1,357 # Chain 3.52 M 1700 2172 1693 # AvgC 35.72 17 27.49 15.53 # AvgW 14.81 5.89 6.89 5.63
Table 1 : Statistics of the training , development and test sets , which show the number of documents ( # Doc ) and sentences ( # Sent ) , the number of lexical chains extracted from the source documents ( # Chain ) , the average number of lexical chains per document ( # AvgC ) and the average number of words per lexical chain ( # AvgW ) .
We compared them against the System MT06 MT08 Avg Baseline 30.43 23.32 26.88 LexChainCount ( top 1 ) 30.46 23.52 26.99 LexChainCount 30.79 23.34 27.07 LexChainProb 31.64 24.54 28.09
Table 3 : Effects of the lexical chain based count and probability cohesion models .
LexChainCount : the count model defined in Eq. ( 3 ) .
LexChainProb : the probability model defined in Eq. ( 5 ) .
Table 4 : 4
The lexical chain based probability cohesion model ( LexChainProb ) vs. the lexical cohesion device based trigger model ( LexDeviceTrigger ) .
We collect training instances from word-aligned bilingual data to train the MaxEnt classifier .
The training data includes LDC2003E14 , LDC2004T07 , LDC2005T06 , LDC2005T10 and LDC2004T08 ( Hong Kong Hansards / Laws / News ) .3 Available at : http://homepages.inf.ed.ac.uk/lzhang10/ maxent toolkit.html
