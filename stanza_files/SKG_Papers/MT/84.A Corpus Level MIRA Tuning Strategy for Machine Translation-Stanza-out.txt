title
A Corpus Level MIRA Tuning Strategy for Machine Translation
abstract
MIRA based tuning methods have been widely used in statistical machine translation ( SMT ) system with a large number of features .
Since the corpus-level BLEU is not decomposable , these MIRA approaches usually define a variety of heuristic -driven sentencelevel BLEUs in their model losses .
Instead , we present a new MIRA method , which employs an exact corpus-level BLEU to compute the model loss .
Our method is simpler in implementation .
Experiments on Chinese-to - English translation show its effectiveness over two state - of- the - art MIRA implementations .
Introduction Margin infused relaxed algorithm ( MIRA ) has been widely adopted for the parameter optimization in SMT with a large feature size ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; Chiang et al. , 2009 ; Chiang , 2012 ; Eidelman , 2012 ; Cherry and Foster , 2012 ) .
Since BLEU is defined on the corpus , and not decomposed into sentences , most MIRA approaches consider a variety of sentence - level BLEUs for the model losses , many of which are heuristic -driven ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ; Chiang et al. , 2009 ; Chiang , 2012 ; Cherry and Foster , 2012 ) .
The sentence - level BLEU appearing in the objective is generally based on a pseudo-document , which may not precisely reflect the corpus-level BLEU .
We believe that this mismatch could potentially harm the performance .
To avoid the sentence BLEU , the work in ( Haddow et al. , 2011 ) proposed to process sentences in small batches .
The authors adopted a Gibbs sampling ( Arun et al. , 2009 ) technique to search the hope and fear hypotheses , and they did not compare with MIRA .
Watanabe ( 2012 ) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent .
Both approaches introduced additional complexities over baseline MIRA approaches .
In contrast , we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses .
We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured hinge loss defined on them .
The experiments show that our method consistently outperforms two state - of- the- art MIRAs in Chinese - to - English translation tasks with a moderate margin .
Margin Infused Relaxed Algorithm
We optimize the model parameters based on N-best lists .
Our development ( dev ) set is a set of triples {( f i , e i , r i ) }
M i=1 , where f i is a source - language sentence , corresponded by a list of target - language hypotheses e i = {e ij } N ( f i ) j=1 , with a number of references r i . h( e ij ) is a feature vector .
Generally , most decoders return a top - 1 candidate as the translation result , such that ?i ( w ) = arg max j w ? h( e ij ) , where w are the model parameters .
In this paper , we aim at optimizing the BLEU score ( Papineni et al. , 2002 ) .
MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure .
For example in ( Crammer et al. , 2006 ; Chiang et al. , 2008 ) , MIRA is performed after an input sentence are decoded , and the next sentence is decoded with the updated parameters .
The objective for each sentence i is , min w 1 2 ||w ? w || 2 + C ? l i ( w ) ( 1 ) l i ( w ) = max eij { b( e * i ) ? b( e ij ) ?w ? [ h( e * i ) ? h( e ij ) ] } ( 2 ) where e * i ?
e i is a hope candidate , w is the parameter vector from the last sentence .
Since MIRA defines its objective only based on the current sentence , b( ? ) is a sentence - level BLEU .
Most MIRA algorithms need a deliberate definition of b( ? ) , since BLEU cannot be decomposed into sentences .
The types of the sentence BLEU calculation includes : ( a ) a smoothed version of BLEU for e ij ( Liang et al. , 2006 ) , ( b ) fit e ij into a pseudodocument considering the history ( Chiang et al. , 2008 ; Chiang , 2012 ) , ( c ) use e ij to replace the corresponding hypothesis in the oracles ( Watanabe et al. , 2007 ) .
The sentence - level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU .
3 Corpus-level MIRA
Algorithm
We propose a batch tuning strategy , corpus-level MIRA ( c- MIRA ) , in which an objective is not built upon a hinge loss of a single sentence , but upon that of the entire corpus .
The online MIRAs are difficult to parallelize .
Therefore , similar to the batch MIRA in ( Cherry and Foster , 2012 ) , we conduct the batch tuning by repeating the following steps : ( a) Decode source sentences ( in parallel ) and obtain {e i } M i=1 , ( b) Merge {e i } M i=1 with the one from the previous iteration , ( c ) Invoke Algorithm 1 .
We define E = ( e E,1 , e E,2 , ... , e E , M ) as a corpus hypothesis , with H ( E ) = 1 M M i=1 h( e E , i ) .
e E , i is the hypothesis of the source sentence f i covered by E .
E is corresponded to a corpus-level BLEU , which we ultimately want to optimize .
Following MIRA formulated in ( Crammer et al. , 2006 ; Chiang et al. , 2008 ) , c-MIRA repeatedly optimizes , min w 1 2 ||w ? w || 2 + C ? l corpus ( w ) ( 3 ) l corpus ( w ) = max E { B ( E * ) ? B ( E ) ?w ? [ H ( E * ) ? H ( E ) ] } ( 4 ) where B ( ? ) is a corpus-level BLEU .
E * is a hope hypothesis .
E ? L , where L is the hypothesis space of the entire corpus , and | L | = |e 1 | ? ? ? |e M |. Algorithm 1 Corpus-Level MIRA Require : {( f i , e i , r i ) }
M i=1 , w 0 , C 1 : for t = 1 ? ? ?
T do 2 : E * = { } , E = {} Initialize the hope and fear 3 : for i = 1 ? ? ?
M do 4 : e E * , i = arg max eij [ w t?1 ? h( e ij ) + b ( e ij ) ]
5 : e E , i = arg max eij [ w t?1 ? h( e ij ) ? b ( e ij ) ]
6 : E * ? E * + {e E * , i } Build the hope 7 : E ? E + {e E , i } Build the fear 8 : end for 9 : B = B( E * ) ? B( E ) the BLEU difference 10 : H = H( E ) ? H( E * ) the feature difference 11 : ? = min C , B+wt?1 ? H || H || 2 12 : w t = w t?1 ? ? ? H 13 : wt = 1 t + 1 t t=0 w t 14 : end for 15 : return wt with the optimal BLEU on the dev set .
c- MIRA can be regarded as a standard MIRA , in which there is only one single triple ( F , L , R ) , where F and R are the source and reference of the corpus respectively .
Eq. 3 is equivalent to a quadratic programming with | L | constraints .
Crammer et al. ( 2006 ) show that a single constraint with one hope E * and one fear E admits a closed - form update and performs well .
We denote one execution of the outer loop as an epoch .
The hope and fear are updated in each epoch .
Similar to ( Chiang et al. , 2008 ) , the hope and fear hypotheses are defined as following , E * = max E [ w ? H ( E ) + B ( E ) ] ( 5 ) E = max E [ w ? H ( E ) ? B ( E ) ] ( 6 ) Eq. 5 and 6 find the hypotheses with the best and worse BLEU that the decoder can easily achieve .
It is unnecessary to search the entire space of L for precise solution E * and E , because MIRA only at-tempts to separate the hope from the fear by a margin proportional to their BLEU differentials ( Cherry and Foster , 2012 ) .
We just construct E * and E respectively by , e E * , i = max e i , j [ w ? h( e i , j ) + b ( e i , j ) ]
e E , i = max e i , j [ w ? h( e i , j ) ? b ( e i , j ) ] where b is simply a BLEU with add one smoothing ( Lin and Och , 2004 ) .
A smoothed BLEU is good enough to pick up a " satisfying " pair of hope and fear .
However , the updating step ( Line 11 ) uses the corpus-level BLEU .
Justification c-MIRA treats a corpus as one sentence for decoding , while conventional decoders process sentences one by one .
We show the optimal solutions from the two methods are equivalent theoretically .
We follow the notations in ( Och and Ney , 2002 ) .
We search a hypothesis on corpus E = {e 1 , k 1 , e 2 , k 2 , ... , e M , k M } with the highest probability given the source corpus F = {f 1 , f 2 , ... , f M } , E = arg max E logP ( E| F ) = arg max E w ? M i=1 h( e i , ki ) ?
M i=1 log( Z i ) ( 7 ) = { arg max e i , k i w ? h( e i , ki ) } M i=1 ( 8 ) where Z i = N ( f i ) j=1 exp ( w ? h( e i , j ) ) , which is a constant with respective to E. Eq. 7 shows that the feature vector of E is determined by the sum of each candidate 's feature vectors .
Also , the model score can be decomposed into each sentence in Eq. 8 , which shows that decoding all sentences together equals to decoding one by one .
We also show that if the metric is decomposable , the loss in c-MIRA is actually the sum of the hinge loss l i ( w ) in structural SVM ( Tsochantaridis et al. , 2004 ; Cherry and Foster , 2012 ) .
We assume B( e ij ) to be the metric of a sentence hypothesis , then the loss of c-MIRA in Eq. 4 is , l corpus ( w ) ? max E M i= 1 [ B ( e i , k E * ) ? B( e i , k E ) ?w?h( e i , k E * ) + w ? h( e i , k E ) ] = M i=1 max eij [ B( e i , k E * ) ? B( e ij ) ?w?h( e i , k E * ) + w ? h( e ij ) ] = M i=1 l i ( w ) Instead of adopting a cutting - plane algorithm ( Tsochantaridis et al. , 2004 ) , we optimize the same loss with a MIRA pattern in a simpler way .
However , since BLEU is not decomposable , the structural SVM ( Cherry and Foster , 2012 ) uses an interpolated sentence BLEU ( Liang et al. , 2006 ) .
Although Algorithm 1 has an outlook similar to the batch - MIRA algorithm in ( Cherry and Foster , 2012 ) , their loss definitions differ fundamentally .
Batch MIRA basically uses a sentence - level loss , and they also follow the sentence - by-sentence tuning pattern .
In the future work , we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER ( Och and Ney , 2002 ) .
Experiments and Analysis
We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese- to - English machine translation system with 228 features .
Second , we show c-MIRA is also effective in the re-ranking task with more than 50,000 features .
In both experiments , we compare c-MIRA and three baselines : ( 1 ) MERT ( Och , 2003 ) , ( 2 ) Chiang et al . 's MIRA ( MIRA 1 ) in ( Chiang et al. , 2008 ) . ( 3 ) batch - MIRA ( MIRA 2 ) in ( Cherry and Foster , 2012 ) .
Here , we roughly choose C with the best BLEU on dev set , from { 0.1 , 0.01 , 0.001 , 0.0001 , 0.00001 } .
We convert Chiang et al . 's MIRA to the batch mode described in section 3.1 .
So the only difference between MIRA 1 and MIRA 2 is : MIRA 1 obtains multiple constraints before optimization , while MIRA 2 only uses one constraint .
We implement MERT and MIRA 1 , and directly use MIRA 2 from Moses ( Koehn et al. , 2007 ) .
We conduct experiments in a server of 8 - cores with 2.5 GHz Opteron .
We set the maximum number of epochs as we generally do not observe an obvious increase on the dev set BLEU .
The epoch size for MIRA 1 and MIRA 2 is 40 , while the one for c-MIRA is 400 .
c-MIRA runs more epochs , because we update the parameters by much fewer times .
However , we can implement Line 3?8 in Algorithm 1 in multi-thread ( we use eight threads in the following experiments ) , which makes our algorithm much faster .
Also , we increase the epoch sizes of MIRA 1 and MIRA 2 to 400 , and find there is no improvement on their performance .
Iterative Batch Training
In this experiment , we conduct the batch tuning procedure shown in section 3 .
We align the FBIS data including about 230 K sentence pairs with GIZA ++ for extracting grammar , and train a 4 - gram language model on the Xinhua portion of Gigaword corpus .
A hierarchical phrase - based model ( Chiang , 2007 ) length on each side of a hierarchical grammar is limited to 10 .
There are 4 ? 55 extra group features .
We also set the size of N-best list per sentence before merge as 200 .
All methods use 30 decoding iterations .
We select the iteration with the best BLEU of the dev set for testing .
We present the BLEU scores in Table 1 on two feature settings : ( 1 ) 8 basic features only , and ( 2 ) all 228 features .
In the first case , due to the small feature size , MERT can get a better BLEU of the dev set , and all MIRA algorithms fails to generally beat MERT on the test set .
However , as the feature size increase to 228 , MERT degrades on the dev-set BLEU , and also become worse on test sets , while MIRA algorithms improve on the dev set expectedly .
MIRA 1 performs better than MIRA 2 , probably because of more constraints .
c- MIRA can moderately improve BLEU by 0.2?0.4 from MIRA 1 and 0.2?0.6 from MIRA 2 .
This might indicate that a loss defined on corpus is more accurate than the one defined on sentence .
Table 2 lists the running time .
Only MIRA 2 is fairly faster than c-MIRA because of more epochs in c-MIRA .
Re-ranking Experiments
The baseline system is a state - of - the - art hierarchical phrase - based system , and trained on six million parallel sentences corpora available to the DARPA BOLT Chinese - English task .
This system includes 51 dense features ( including translation probabilities , provenance features , etc. ) and about 50 k sparse features ( mostly lexical and fertility - based ) .
The language model is a six-gram model trained on a 10 billion words monolingual corpus , including the English side of our parallel corpora plus other corpora such as Gigaword ( LDC2011T07 ) and Google News .
We use 1275 sentences for tuning and 1239 sentences for testing from the LDC2010E30 corpus respectively .
There are four reference translations for each input sentence in both tuning and testing datasets .
We use a N-best list which is an intermediate out -
3 . We observe that the effectiveness of c-MIRA is not harmed as the feature size is scaled up .
Analysis
To examine the simple search for hopes and fears ( Line 3?8 in Alg. 1 ) , we use two hope / fear building strategies to get E * and E : ( 1 ) simply connect each e * i and e i in Line 4?5 of Algorithm 1 , ( 2 ) conduct a slow beam search among the N-best lists of all foreign sentences from e 1 to e M and use Eq. 5 and 6 to prune the stack .
The stack size is 10 .
We observe that there is no significant difference between the two strategies on the BLEU of the dev set .
But the second strategy is about 10 times slower .
We also consider more constraints in Eq. 3 .
By beam search , we obtain one corpus-level oracle and 29 other hypotheses similar to ( Chiang et al. , 2008 ) , and optimize with SMO ( Platt , 1998 ) .
Unfortunately , experiments show that more constraints lead to an overfitting and no improved performance .
As shown in Table 4 , in one execution , our method updates the parameters by only 400 times ; MIRA 2 updates by 40 ? 878 = 35120 times ; and MIRA 1 updates much more ( about 1,966,720 times ) due to the SMO procedure .
We are surprised to find c-MIRA gets a higher training BLEU with such few parameter updates .
This probably suggests that there is a gap between sentence - level BLEU and corpuslevel BLEU , so standard MIRAs need to update the parameters more often .
Regarding simplicity , MIRA 1 uses a stronglyheuristic definition of a sentence BLEU , and MIRA 2 needs a pseudo-document with a decay rate of ? = 0.9 .
In comparison , c- MIRA avoids both the sentence level BLEU and the pseudo-document , thus needs fewer variables .
Conclusion
We present a simple and effective MIRA batch tuning algorithm without the heuristic -driven calculation of sentence - level BLEU , due to the indecomposability of a corpus-level BLEU .
Our optimization objective is directly defined on the corpus-level hypotheses .
This work simplifies the tuning process , and avoid the mismatch between the sentencelevel BLEU and the corpus-level BLEU .
This strategy can be potentially applied to other optimization paradigms , such as the structural SVM ( Cherry and Foster , 2012 ) , SGD and AROW ( Chiang , 2012 ) , and other forms of samples , such as forests ( Chiang , 2012 ) and lattice ( Cherry and Foster , 2012 ) .
Table 1 : 1 BLEUs ( % ) on the dev and test sets with 8 dense features only and all features .
The significant symbols ( + at 0.05 level ) are compared with MIRA 2 MERT MIRA 1 MIRA 2 c-MIRA C 0.0001 0.001 0.0001 8 dev 34.80 34.70 34.73 34.70 feat .
04 31.92 31.81 31.73 31.83 05 28.85 28.94 28.71 28.92 C 0.001 0.001 0.001 all dev 34.61 35.24 35.14 35.56 feat .
04 31.76 32.25 32.04 32.57 + 05 28.85 29.43 29.37 29.41 06news 30.91 31.43 31.24 31.82 + 06others 27.43 28.01 28.13 28.45 08news 25.62 26.11 26.03 26.40 08others 16.22 16.66 16.46 17.10 +
