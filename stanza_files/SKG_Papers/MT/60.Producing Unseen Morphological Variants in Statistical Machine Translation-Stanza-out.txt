title
Producing Unseen Morphological Variants in Statistical Machine Translation
abstract
Translating into morphologically rich languages is difficult .
Although the coverage of lemmas may be reasonable , many morphological variants cannot be learned from the training data .
We present a statistical translation system that is able to produce these inflected word forms .
Different from most previous work , we do not separate morphological prediction from lexical choice into two consecutive steps .
Our approach is novel in that it is integrated in decoding and takes advantage of context information from both the source language and the target language sides .
Introduction Morphologically rich languages exhibit a large amount of inflected word surface forms for most lemmas , which poses difficulties to current statistical machine translation ( SMT ) technology .
SMT systems , such as phrase - based translation ( PBT ) engines ( Koehn et al. , 2003 ) , are trained on parallel corpora and can learn the vocabulary that is observed in the data .
After training , the decoder can output words which have been seen on the target side of the corpus , but no unseen words .
Sparsity of morphological variants leads to many linguistically valid morphological word forms remaining unseen in practical scenarios .
This is a substantial issue under low-resource conditions , but the problem persists even with larger amounts of parallel training data .
When translating into the morphologically rich language , the system fails at producing the unseen morphological variants , leading to major translation errors .
Consider the Czech example in Table 1 .
A small parallel corpus of 50 K English - Czech sentences contains only a single variant of the morphological case surface form 50 K 500K 5 M 50M 1 ?ky ? ? ? ? 2 ?ek - ? ? ? 3 ?k? m - - ? ? 4 ?ky ? ? ? ? 5 ?ky ? ? ? ? 6 ?k?ch - ? ? ? 7 ?kami - - - ?
Table 1 : Morphological variants of the Czech lemma " ? ka " .
For differently sized corpora ( 50K/500K /5M/50M ) , " ? " indicates that the variant is present , and " ? " that the same surface form realization occurs , but in a different syntactic case .
forms of the Czech lemma " ? ka " ( plural of English : " kneecap " ) , out of seven syntactically valid cases .
The situation improves as we add in more training data ( 500K / 5 M/50M ) , but we can generally not expect the SMT system to learn all variants of each known lemma .
In Czech , the number of possible variants is even larger for other word categories such as verbs or adjectives .
Adjectives , for instance , have different suffixes depending on case , number , and gender of the governing noun .
In this paper , we propose an extension to phrase - based SMT that allows the decoder to produce any morphological variant of all known lemmas .
We design techniques for generating and scoring unseen morphological variants fully integrated into phrase - based search , with the decoder being able to choose freely amongst all possible morphological variants .
Empirically , we observe considerable gains in translation quality especially under medium -to low-resource conditions .
Related Work Translation into morphologically rich languages is often tackled through " two-step " , i.e. , separate modules for morphological prediction and generation ( Toutanova et al. , 2008 ; Bojar and Kos , 2010 ; Fraser et al. , 2012 ; Burlot et al. , 2016 ) .
An important problem is that lexical choice ( of the lemma ) is carried out in a separate step from morphological prediction .
Factored machine translation with separate translation and generation models represents a different approach , operating with a singlestep search .
However , too many options in decoding cause a blow - up of the search space ; and useful information is dropped when modeling source_word ?
target_lemma and tar-get_lemma? target_word separately .
Word forms not seen in parallel data are sometimes still available in monolingual data .
Backtranslation ( Bojar and Tamchyna , 2011 ) takes advantage of this .
The monolingual target language data is lemmatized , automatically translated to the source language , and the translations are aligned with the original , inflected target corpus to produce supplementary training data .
Disadvantages are both the computational expense and that the back - translated text may contain errors .
Previous work on synthetic phrases by is most similar to our work .
They commit to generation of a single candidate inflection of a lemma prior to decoding , chosen only based on a hierarchical rule and source-side information , a significant limitation .
We instead consider all morphological variants , and we are able to use dynamically - generated target - side context in choosing the correct variant , which is critical for capturing phenomena such as target - side verb-subject agreement , or the agreement between a preposition marking case and the case on the noun it marks .
Generating Unseen Morphological Variants
We investigate an approach based on synthesized morphological variants .
A morphological generation tool is utilized to synthesize all valid morphological forms from target - side lemmas .
The phrase table is then augmented with additional entries to provide complete coverage .
We process single target - word entries from the baseline phrase table and feed the lemmatized target word into the morphological generation tool .
If its output contains morphological forms that are not known as translations of the source side of the phrase , we add these morphological variants as new translation options .
We consider two settings : feature type configurations source indicator l , t source internal l , l+r , l+p , t , r+p source context l ( - 3,3 ) , t ( - 5,5 ) target indicator l , t target internal l , t target context l ( - 2 ) , t ( - 2 ) Table 2 : Feature templates for the discriminative classifier : l ( lemma ) , t ( morphosyntactic tag ) , r ( syntactic role ) , p ( lemma of dependency parent ) .
Numbers in parentheses indicate context size .
( 1. ) word , where morphological word forms are generated from phrase table entries of length 1 on both source and target side , and ( 2 . ) mtu ( for " minimal translation unit " ) , where the phrase source side can have arbitrary length .
Morphological generation for Czech , for instance , can be performed with the MorphoDiTa toolkit ( Strakov ?
et al. , 2014 ) , which we will use in our experiments .
MorphoDiTa knows a dictionary of most Czech lemmas and can generate all their morphological variants ( Haji ? , 2004 ) .
When not restricted , the morphological generator also produces forms which do not match in number , tense , degree of comparison , or even negation .
This may be undesirable and we therefore define a tag template .
The tag template prevents the generation of some forms of the given Czech lemma .
The template only allows freedom in the following morphological categories : gender , case , person , possessor 's number , and possessor 's gender .
All other attributes must match the original Czech word form .
The morphosyntax of the English source is not used to impose further constraints .
We will mark this configuration with an asterisk ( ) in our experiments .
Scoring Unseen Morphological Variants
Assigning dependable model scores to synthesized morphological forms is a primary challenge .
During decoding , the artificially added phrase table entries compete with baseline phrases that had been directly extracted from the parallel training data .
The correct choice has to be determined in search based on model scores .
A phrase - based model with linguistically motivated factors enables us to achieve better generalization capabilities when translating into a morphologically rich language .
In our baseline systems , we already draw on lemmas and morphosyntactic tags as factors on the target side , in addition to word surface forms .
1
The additional target-side factors allow us to integrate features that independently model word sense ( in terms of the lemma ) and morphological attributes ( in terms of the morphosyntactic tag ) .
All our translation engines ( cf. Section 5 ) incorporate ngram LMs over lemmas and over morphosyntactic tags , and an operation sequence model ( OSM ) ( Durrani et al. , 2013 ) with lemmas on the target side .
These models counteract sparsity , and where models over surface forms fail for unseen variants , they still assign scores which are based on reliable probability estimates .
When enhancing a system with synthesized phrase table entries , we add further features .
Since the usual phrase translation and lexical translation log-probabilities over surface forms cannot be estimated for unseen morphological variants , but all 1 But note that our factored systems operate without a division into separate translation and generation models .
new variants are generated from existing lemmas , we utilize the corresponding log-probabilities over target lemmas .
Those can be extracted from the parallel training data and added to the synthesized entries .
For baseline phrase table entries , we retain their four baseline phrase translation and lexical translation features , meaning that features over target lemmas score synthesized entries and features over surface forms score baseline entries .
The features have separate weights in the model combination .
Furthermore , a binary indicator distinguishes baseline phrases from synthesized phrases .
The final key to our approach is using a discriminative classifier ( morph-vw , Vowpal Wabbit 2 for Morphology ) which can take context from both the source side and the target side into account , as in ( Tamchyna et al. , 2016 ) .
We design feature templates for the classifier that generalize to unseen morphological variants , as listed in Table 2 . " Indicator " features are concatenations of words inside the phrase , " internal " features represent each word in the phrase separately .
Context features on the source side capture a fixed - sized window around the phrase .
Target - side context is only to the left of the current phrase .
The feature set is designed to force the classifier to learn two independent components : semantic ( choosing the right lemma ) and morphosyntactic ( choosing the right tag , i.e. , morphological variant of a word ) .
When scoring an unseen morphological variant of a known word , these two independent components should still be able to assign meaningful scores to the translation .
Note that the features require lemmatization and tagging on both sides and a dependency parse of the source side .
Empirical Evaluation
For an empirical evaluation of our technique , we build baseline phrase - based SMT engines using Moses .
We then enrich these baselines with linguistically motivated morphological variants that are unseen in the parallel training data , and we augment the model with the discriminative classifier to guide morphological selection during decoding .
Different flavors of synthetic morphological variants are compared , each either combined with the discriminative classifier or standalone .
We choose English ?
Czech as a task that is representative for machine translation from a morphologically underspecified language into a morphologically rich language .
Experimental Setup
We train a phrase - based translation system with three factors on the target side of the translation model ( but no separate generation model ) .
The target factors are the word surface form , lemma , and a morphosyntactic tag .
We use the Czech positional tagset ( Haji? and Hladk ? , 1998 ) which fully describes the word 's morphological attributes .
On the source side we use only surface forms , except for the discriminative classifier , which includes the features as shown in Table 2 .
We employ corpora that have been provided for the English ?
Czech News translation shared task at WMT16 ( Bojar et al. , 2016 b ) , including the CzEng parallel corpus ( Bojar et al. , 2016 a ) .
Word alignments are created using fast_align and symmetrized .
We extract phrases up to a maximum length of 7 .
The phrase table is pre-pruned by applying a minimum score threshold of 0.0001 on the source - to - target phrase translation probability , and the decoder loads a maximum of 100 best translation options per distinct source side .
We use cube pruning in decoding .
Pop limit and stack limit for cube pruning are set to 1000 for tuning and to 5000 for testing .
The distortion limit is 6 .
Weights are tuned on news - test2013 with k-best MIRA ( Cherry and Foster , 2012 ) over 200 - best lists for 25 iterations .
Translation quality is measured in BLEU ( Papineni et al. , 2002 ) on three different test sets , newstest 2014 , newstest2015 , and newstest 2016 .
3
Our training data amounts to around 50 million bilingual sentences overall , but we conduct sets of experiments with systems trained using different fractions of this data ( 50K , 500K , 5 M , 50M ) .
Whereas English ?
Czech has good coverage in terms of training corpora , we simulate lowand medium -resource conditions for the purpose of drawing more general conclusions .
Irrespective of this , we utilize the same large LMs in all setups , assuming that proper amounts of target language monolingual data can often be gathered , even when parallel data is scarce .
All other models ( including the morph - vw ) are trained using only the fraction of data as chosen for the respective set of experiments , and synthesized phrase table entries with generated morphological variants are produced individually for each baseline phrase table .
Experimental Results and Analysis Translation results are reported in Tables 3 to 6 .
Our method is effective at improving BLEU especially in the low- and medium - resource settings , but shows only slight gains in the 5 M and 50 M scenarios .
Overall , mtu leads to better results than word .
When we also add translations to phrases with multiple input words , we give the system more leeway in phrasal segmentation and our synthetic phrases can perhaps be applied more easily .
In the 50 K and 500K settings , we obtain considerable improvements even without using the discriminative model .
This suggests that our scoring scheme based on lemmas is indeed effective for the synthetic phrase pairs .
Additionally , model features such as the OSM with target - side lemmas as well as the LMs over lemmas and over morphosyntactic tags seem to cope with the synthetic word forms reasonably well .
However , when we do use the classifier , we obtain a small but consistent further improvement .
Figure 1 visualizes the BLEU scores achieved under the four training resource conditions with the baseline system and with the system extended via synthesized morphological word forms ( in the mtu variant ) plus the discriminative classifier , respectively .
In order to better understand why the improvements fall off as we increase training data size , we measure target - side out - of- vocabulary ( OOV ) rates of the various settings .
Our aim is to quantify the potential improvement that our method can bring .
Table 7 shows the statistics : at 50K , the baseline OOV rate is nearly 17 % and our technique successfully reduces it to less than 10 % .
The relative reduction of the OOV rate is quite steady as training data size increases .
Figure 2 illustrates the effect of our technique in a medium-size setting ( 500K ) .
The baseline system is forced to use the incorrect nominative case due to the lack of required surface forms .
Our method provides these inflections ( " republik ?n ? " , " Trumpa " ) and produces a mostly grammatical
We report sizes of the full phrase tables as well as after filtering towards the newstest2016 source .
Target - side OOV rates are calculated by comparing newstest2016 references against the filtered phrase tables .
translation ( but is still unable to correctly translate the preposition " in " ) .
Conclusion
We have studied the important problem of modeling all morphological variants of our SMT system 's vocabulary .
We showed that we can augment our system 's vocabulary with the missing variants and that we can effectively score these variants using a discriminative lexicon utilizing both source and target context .
We have shown that this leads to substantial BLEU score improvements , particularly on small to medium resource translation tasks .
Given the limited training data available for translation to many morphologically rich languages , our approach is widely applicable .
BLEUFigure 1 : 1 Figure 1 : Visualization of the English ?
Czech translation quality on newstest2016 , showing the benefit of our approach under different training resource conditions ( 50K/500K/5M/50M ) .
