title
Phrase Model Training for Statistical Machine Translation with Word Lattices of Preprocessing Alternatives
abstract
In statistical machine translation , word lattices are used to represent the ambiguities in the preprocessing of the source sentence , such as word segmentation for Chinese or morphological analysis for German .
Several approaches have been proposed to define the probability of different paths through the lattice with external tools like word segmenters , or by applying indicator features .
We introduce a novel lattice design , which explicitly distinguishes between different preprocessing alternatives for the source sentence .
It allows us to make use of specific features for each preprocessing type and to lexicalize the choice of lattice path directly in the phrase translation model .
We argue that forced alignment training can be used to learn lattice path and phrase translation model simultaneously .
On the newscommentary portion of the German ?
English WMT 2011 task we can show moderate improvements of up to 0.6 % BLEU over a stateof - the - art baseline system .
Introduction
The application of statistical machine translation ( SMT ) to word lattice input was first introduced for the translation of speech recognition output .
Rather than translating the single- best transcription , the speech recognition system encodes all possible transcriptions and their probabilities within a word lattice , which is then used as input for the machine translation system ( Ney , 1999 ; Bertoldi et al. , 2007 ) .
Since then , several groups have adapted this approach to model ambiguities in representing the source language with lattices and were able to report improvements over their respective baselines .
The probabilities for different paths through the lattice are usually modeled by assigning probabilities to arcs as a byproduct of the lattice generation or by defining binary indicator features .
Applying the first method only makes sense if the lattice construction is based on a single , comprehensive probabilistic method , like a Chinese word segmentation model as is used by Xu et al . ( 2005 ) .
In applications like the one described by Dyer et al . ( 2008 ) , where several different segmenters for Chinese are combined to create the lattice , this is not possible .
Also , our intuition suggests that simply defining indicator features for each of the segmenters may not be ideal , if we assume that there is not a single best segmenter , but rather that for different data instances a different one works best .
In this paper , we propose to model the lattice path implicitly within the phrase translation model .
We introduce a novel lattice design , which explicitly distinguishes between different ways of preprocessing the source sentence .
It enables us to define specific binary features for each preprocessing type and to learn lexicalized lattice path probabilities and the phrase translation model simultaneously with a forced alignment training procedure .
To train the phrase translation model , most stateof - the- art SMT systems rely on heuristic phrase extraction from a word-aligned training corpus .
Using a modified version of the translation decoder to force-align the training data provides a more consistent way of training .
Wuebker et al. ( 2010 ) introduce a leave- one - out method which can overcome the over-fitting effects inherent to this training procedure ( DeNero et al. , 2006 ) .
The authors report this to yield both a significantly smaller phrase table and higher translation quality than the heuristic phrase extraction .
We argue that applying forced alignment training helps to exploit the full potential of word lattice translation .
The effects of the training on lattice input are analyzed on the news -commentary portion of the German ?
English WMT 2011 task .
Our results show moderate improvements of up to 0.6 % BLEU over the baseline .
This paper is organized as follows :
We will review related work in Section 2 , describe the decoder in Section 3 and present our novel lattice design in Section 4 .
The phrase training algorithm is introduced in Section 5 , and Section 6 gives a detailed account of the experimental setup and discusses the results .
Finally , our findings are summarized in Section 7 .
Related work
Word lattices have been used for machine translation of text in a variety of ways .
Dyer et al. ( 2008 ) use it to encode different Chinese word segmentations or Arabic morphological analyses .
For the phrase - based model , they report improvements of up to 0.9 % BLEU for Chinese ?
English and 1.6 % BLEU for Arabic ?
English over the respective single best word segmented and morphologically analyzed source .
These results are achieved without an explicit way of modeling probabilities for different paths within the lattice .
The training of the phrase model is done by generating one version of the training data for each segmentation method or morphological analysis .
The word alignments are trained separately , and are then concatenated for phrase extraction .
Our work differs from ( Dyer et al. , 2008 ) in that we explicitly distinguish the various preprocessing types in the lattice so that we can define specific path features and lexicalize the lattice path probabilities within the phrase model .
In ( Xu et al. , 2005 ) the probability of a segmen-tation , as given by the Chinese word segmentation model , and the translation model are combined into a global decision rule .
This is done by weighting the lattice edges with a source language model .
The authors report an improvement of 1.5 % BLEU over translation of the single best segmentation with a phrase - based SMT system .
Dyer ( 2009 ) introduces a maximum entropy model for compound word splitting , which he uses to create word lattices for translation input .
He shows improvements in German-English , Hungarian - English and Turkish - English over stateof - the - art baselines .
For the German ?
English WMT 2010 task , Hardmeier et al. ( 2010 ) encode the morphological reduction and decompounding of the German surface form as alternative paths in a word lattice .
They show improvements of roughly 0.5 % BLEU over the baseline .
A binary indicator feature is added to the log-linear framework for the alternative edges .
Additionally , they integrate long- range reorderings of the source sentence into the lattice , in order to match the word order of the English language , which yields another improvement of up to 0.5 % BLEU .
Niehues and Kolss ( 2009 ) also use lattices to encode different alternative reorderings of the source sentence which results in an improvement of 2.0 % BLEU over the baseline on the WMT 2008 German ?
English task .
Onishi et al. ( 2010 ) propose a method of modeling paraphrases in a lattice .
They perform experiments on the English ?
Japanese and English ?
Chinese IWSLT 2007 tasks , and report improvements of 1.1 % and 0.9 % BLEU over a paraphrase - augmented baseline .
Schroeder et al. ( 2009 ) generalize usage of lattices to combine input from multiple source languages .
Factored translation models ( Koehn and Hoang , 2007 ) approach the idea of integrating annotation into translation from the opposite direction .
Where lattices allow the decoder to choose a single level of annotation as translation source , factored models are designed to jointly translate several annotation levels ( factors ) .
Thus , they are more suited to integrate low-level annotation that by itself does not provide sufficient information for accurate translation , like part- of-speech tags , gender , etc .
On the other hand , they require a one- to - one correspondence between the factors , which makes them unsuitable to model word segmentation or decompounding .
The problem of performing real training for the phrase translation model has been approached in a number of different ways in the past .
The first one , to the best of our knowledge , was the joint probability phrase model presented by Marcu and Wong ( 2002 ) .
It is shown to perform slightly inferior to the standard heuristic phrase extraction from word alignments by .
A detailed analysis of the inherent over-fitting problems when training a generative phrase model with the EM algorithm is given in ( DeNero et al. , 2006 ) .
These findings are in principle confirmed by Moore and Quirk ( 2007 ) who , however , can show that their model is less sensitive to reducing computational resources than the state - of - the - art heuristic .
Birch et al. ( 2006 ) and DeNero et al . ( 2008 ) present alternative training procedures for the joint model introduced by Marcu and Wong ( 2002 ) , which are shown to improve its performance .
In ( Mylonakis and Sima'an , 2008 ) a phrase model is described , whose training procedure is designed to counteract the inherent over-fitting problem by including prior probabilities based on Inversion Transduction Grammar and smoothing as learning objective .
It yields a small improvement over a standard phrase - based baseline .
Ferrer and Juan ( 2009 ) present an approach , where the phrase model is trained by a semi-hidden Markov model .
In this work we apply the phrase training method introduced by Wuebker et al . ( 2010 ) , where the phrase translation model of a fully competitive SMT system is trained in a generative way .
The key to avoiding the over-fitting effects described by DeNero et al . ( 2006 ) is their novel leave- one - out procedure .
Decoding
Phrase - based translation
We use a standard phrase - based decoder which searches for the best translation ?
1 for a given input sentence f J 1 by maximizing the posterior probability ?
1 = arg max I , e I 1 P r(e I 1 |f J 1 ) .
( 1 ) Generalizing the noisy channel approach ( Brown et al. , 1990 ) and making use of the maximum approximation ( Viterbi ) , the decoder directly models the posterior probability by a log-linear combination of several feature functions h m ( e I 1 , s K 1 , f J 1 ) weighted with scaling factors ?
m , which results in the decision rule ( Och and Ney , 2004 ) ?
1 = arg max I , e I 1 , K , s K 1 M m=1 ? m h m ( e I 1 , s K 1 , f J 1 ) . ( 2 ) Here , s K 1 denotes the segmentation of e I 1 and f J 1 into K phrase -pairs and their alignment .
The features used are the language model , phrase translation and lexical smoothing models in both directions , word and phrase penalty and a simple distancebased reordering penalty .
Lattice translation
For lattice input we generalize Equation 2 to also maximize over the set of sentences F ( L ) encoded by a given source word lattice L : ?
1 = arg max I , e I 1 , K , s K 1 , f J 1 ?F ( L ) M m=1 ? m h m ( e I 1 , s K 1 , f J 1 ) ( 3 )
Note that in this formulation there are no probabilities assigned to the arcs of L .
We define additional binary indicator features h m and lexicalize path probabilities by encoding the path into the word identities .
To translate lattice input , we adapt the standard phrase - based decoding algorithm as described in ( Matusov et al. , 2008 ) .
The decoder keeps track of the covered slots , which represent the topological order of the nodes , rather than the covered words .
When expanding a hypothesis , it has to be verified that there is no overlap between the covered nodes and that a path exists from start to goal node , which passes through all covered nodes .
In practice , when considering a possible expansion covering slots j , ... , j with start and end states n and n , we make sure that the following two conditions hold : ?
n is reachable from the lattice node that corresponds to the nearest already covered slot to the left of j . ?
The node that corresponds to the nearest already covered slot to the right of j is reachable from n .
It was noted by Dyer et al . ( 2008 ) that the standard distance - based reordering model needs to be redefined for lattice input .
We define the distortion penalty as the difference in slot number .
Using the shortest path within the lattice is reported to have better performance in ( Dyer et al. , 2008 ) , however we did not implement it due to time constraints .
Lattice design
We construct lattices from three different preprocessing variants of the German source side of the data .
The surface form is the standard tokenization of the source sentence .
The word compounds are produced by the frequency - based compound splitting method described in ( Koehn and Knight , 2003 ) , applied to the tokenized sentence .
From the compound split sentence we produce the lemma of the German words by applying the TreeTagger toolkit ( Schmid , 1995 ) .
Each of the different preprocessing variants is assigned a separate layer within the lattice .
For the phrase model , word identities are defined by both the word and its layer .
In this way , the phrase model can assign different scores to phrases in different layers , allowing it to guide the search towards a specific layer for each word .
In practice , this is done by annotating words with a unique identifier for each layer .
For example , the word sein from the lemmatized layer will be written as LEM .
sein within both the data and the phrase table .
If sein appears in the surface form layer , it will be written as SUR .
sein and is treated as a different word .
SUR is the identifier for the compound layer .
We experiment with two different lattice designs .
In the full lattice , all three layers are included for each source word in surface form .
The slim lattice only includes arcs for the lemma layer if it differs from the surface form , and only includes arcs for the compound layer if it differs from both surface form and lemma .
Figure 1 shows a slim and a full lattice for the same training data sentence .
For each layer , we add two indicator features to the phrase table :
One binary feature which is set to 1 if the phrase is taken from this layer , and one feature which is equal to the number of words from this layer .
This results in six additional feature functions , whose weights are optimized jointly with the standard features described in Section 3.1 .
We will denote them as layer features .
Phrase translation model training
To train the phrase model , we use a modified version of the translation decoder to force-align the training data .
We apply the method described in ( Wuebker et al. , 2010 ) , but with word lattices on the source side .
To avoid over-fitting , we use their cross-validation technique , which is described as a low-cost alternative to leave- one- out .
For cross-validation we segment the training data into batches containing 5000 sentences .
For each batch , the phrase table is updated by reducing the phrase counts by the local counts produced by the current batch in the previous training iteration .
For the first iteration , we perform the standard phrase extraction separately for each batch to produce the local counts .
Singleton phrases are assigned the probability ? ( | f |+|? | ) with the source and target phrase lengths | f | and |?| and fixed ? = e ?5 ( length - based leave-one- out ) .
Sentences for which the decoder is not able to find an alignment are discarded ( about 4 % for our experiments ) .
To estimate the probabilities of the phrase model , we count all phrase pairs used in training within an n-best list ( equally weighted ) .
The translation probability for a phrase pair ( f , ? ) is estimated as p F A ( ?| f ) = C F A ( f , ? ) C mon ( f ) , ( 4 ) where C F A ( f , ? ) is the count of the phrase pair ( f , ? ) in the force-aligned training data .
In order to learn the lattice path along with the phrase translation probabilities , we make the following modification to the original formulation in ( Wuebker et al. , 2010 ) .
The denominator C mon ( f ) is the count of f in the target side of the training data , rather than using the real marginal counts .
This means that it is independent of the training procedure , and can be computed by ignoring one side of the training data and performing a simple n-gram count on the other .
In way the model learns to prefer lattice paths which are taken more often in training .
For example , if the phrase ( LEM .
Streit LEM .
Kraft ) is used to align the sentence from Figure 1 , C mon ( f ) will be increased for f = ( SUR .
Streitkr?fte ) and f = ( SPL .
Streit SPL .Kr?fte ) without affecting their joint counts .
This leads to a lower probability for these phrases , which is not the case if marginal counts are used .
Note that on the source side we have one training corpus for each lattice layer , which are concatenated to compute C mon ( f ) .
The size of the nbest lists used in this work is fixed to 20000 .
Using smaller n-best lists was tested , but seems to have disadvantages for the application to lattices .
After reestimation of the phrase model , the feature weights are optimized again .
In order to achieve a good coverage of the training data , we allow the decoder to generate backoff phrases .
If a source phrase consisting of a single word does not have any translation candidates left after the bilingual phrase matching , one phrase pair is added to the translation candidates for each word in the target sentence .
The backoff phrases are assigned a fixed probability ? = e ?12 .
Note that this is smaller than the probability the phrase would be assigned according to the length - based leave - oneout heuristic , leading to a preference of singleton phrases over backoff phrases .
The lexical smoothing models are applied in the usual way to both singleton and backoff phrases .
After each sentence , the backoff phrases are discarded .
However , in the experiments for this work , introducing backoff phrases only increases the coverage from 95.8 % to 96.2 % of the sentences .
6 Experimental evaluation
Experimental setup
Our experiments are carried out on the newscommentary portion of the German ?
English data provided for the EMNLP 2011 Sixth Workshop on Statistical Machine Translation ( WMT 2011 ) .
*
We use newstest2008 as development set and newstest2009 and newstest2010 as unseen test sets .
The word alignments are produced with GIZA ++ ( Och and Ney , 2003 ) .
To optimize the loglinear parameters , the Downhill - Simplex algorithm ( Nelder and Mead , 1965 ) is applied with BLEU ( Papineni et al. , 2002 ) language model is a standard 4 - gram LM with modified Kneser - Ney smoothing ( Chen and Goodman , 1998 ) produced with the SRILM toolkit ( Stolcke , 2002 ) .
It is trained on the full bilingual data and parts of the monolingual News crawl corpus provided for WMT 2011 .
Numbers are replaced with a single category symbol in a separate preprocessing step and we apply the long- range part- of-speech based reordering rules proposed by ( Popovi ? and Ney , 2006 ) .
Table 1 shows statistics for the bilingual training data and the development and test corpora for the three different German preprocessing alternatives .
It can be seen that both compound splitting and lemmatization reduce the vocabulary size and number of out-of- vocabulary ( OOV ) words .
Results are measured in BLEU and TER ( Snover et al. , 2006 ) , which are computed case - insensitively with a single reference .
Baseline experiments
To get an overview over the effects of the different preprocessing alternatives for the German source , we built three baseline systems , one for each prepro-cessing type .
The phrase tables are extracted heuristically in the standard way from the word-aligned training data .
Additionally , we performed phrase training for the compound split version of the data .
The results are shown in Table 2 .
When moving from the Surface to the Compound layer , we observe improvements of up to 1.0 % in BLEU and 1.1 % in TER .
Reducing the morphological richness further ( Lemma ) leads to a clear performance drop .
Application of phrase training on the compound split data yields a small degradation in TER on all data sets and in BLEU on newstest2010 .
We assume that this is due to the small size of the training data and its heterogeneity , which makes it hard for the decoder to find good phrase alignments .
Lattice experiments : Heuristic extraction
We generated both slim and full lattices for all data sets .
Similar to ( Dyer et al. , 2008 ) , we concatenate the three training data sets and their word alignments to extract the phrases .
Note that this only produces single - layer phrases .
It can be seen in Table 2 that without the application of layer features the slim lattice slightly outperforms the full lattice .
In - troducing layer features boosts the performance for both lattice types .
However , the performance increase is considerably larger for the full lattice systems , which now outperform the slim lattice systems on newstest2009 and newstest2010 .
Compared to the Compounds baseline , the full lattice system with layer features shows a small improvement of up to 0.4 % BLEU on newstest2009 and newstest2010 , but a degradation in TER .
newstest2008 newstest2009 newstest2010 BLEU [ % ] TER [ % ] BLEU [ % ] TER [ % ] BLEU [ % ] TER [ % ]
Lattice experiments :
Phrase training
The experiments on phrase training are setup as follows .
The phrase table is initialized with the standard extraction and is identical to the one used for the experiments in Section 6.3 .
The log-linear scaling factors used in training are the optimized parameters on the corresponding lattice , also taken from the experiments described in Section 6.3 .
The forced alignment procedure was run for one iteration .
Further iterations were tested , but did not give any improvements .
The phrase training was performed on the full lattice design .
The reason for this is that we want the system to learn all possible phrases .
Even if there is no difference in wording between the layers in train-ing , the additional phrases could be useful for unseen test data .
The training was performed both with and without layer features .
The resulting systems were also optimized with and without layer features , resulting in four different setups .
From the results in Table 2 it is clear that phrase training without layer features does not have the desired effect .
Even if we apply layer features to the system trained without them , we do not reach the performance of the best standard lattice system .
We conclude that , without these indicator features , the standard lattice system does not produce good phrase alignments .
When the layer features are applied for both training and translation , we observe improvements of up to 0.2 % in BLEU and 0.5 % in TER over the corresponding standard lattice system .
The gap between the systems with and without layer features is much smaller than for the heuristically trained lattices .
This indicates that our goal of encoding the best lattice path directly in the phrase model was at least partially achieved .
However , in order to exceed the performance of our state - of - the - art baseline on both measures , the layer features are still needed within the phrase training procedure and for translation .
Al-source Das Warten hat gedauert mehr als NUM Minuten , was im Fall einer Stra?e , wo werden erwartet NUM Menschen , ist unverst ? ndlich .
reference
The wait lasted more than NUM minutes , something incomprehensible for a race where you expect more than NUM people .
lattice ( heuristic )
The wait has taken more than NUM minutes , which in the case of a street , where NUM people are expected to be , ca n't understand it .
lattice ( FA )
The wait has taken more than NUM minutes , which in the case of a street , where expected NUM people , is incomprehensible .
together , our phrase trained lattice approach outperforms the state - of - the - art baseline on all three data sets by up to 0.6 % BLEU .
On newstest2009 , this result is statistically significant with 95 % confidence according to the bootstrap resampling method described by Koehn ( 2004 ) .
For a direct comparison between the heuristic and phrase-trained full lattice systems , we manually inspected the optimized log-linear parameter values for the layer features .
We observe that for the standard lattices , paths through the lemmatized layer are heavily penalized .
In the phrase trained lattice setup , the penalty is much smaller .
As a result , the number of words from the Lemma layer used for translation of the newstest2009 data set is increased by 49 % from 1828 to 2715 words .
However , a manual inspection of the translations reveals that the main improvement seems to come from a better choice of phrases from the Compound layer .
More specifically , the used phrases tend to be shorter - the average phrase length of Compound layer phrases is 1.5 words for both the baseline and the heuristic lattice system .
In the phrase trained lattice system , it is 1.3 words .
An example is given in Figure 2 .
We focus on the end of the sentence , where the heuristic system uses the rather disfluent phrase ( ist unverst ? ndlich .
# ca n't understand it . ) , whereas the forced alignment trained system applies the three phrases ( ist # is ) , ( unverst? ndlich # incomprehensible ) and ( . # . ) .
This effect can be explained by the leave- one - out procedure .
As lemmatized phrases usually map to several phrases in the other layers , their count is generally higher .
Application of leave- one- out , which reduces the counts of all phrases extracted from the current sentence by a fixed value , therefore has a stronger penalizing effect on Surface and Compound layer phrases .
In the extreme case , phrases which are singletons in the Compound layer are unlikely to be used at all in training , if the corresponding phrase in the Lemma layer has a higher count .
While this rarely leads to the competing lemmatized phrases being used in free translation , it allows for shorter , more general phrases from the more expressive layers to be applied .
Indeed , the ' bad ' phrase ( ist unverst ? ndlich .
# ca n't understand it . ) from the example in Figure 2 is a singleton .
Conclusion and future work
In this work we apply a forced alignment phrase training technique to input word lattices in SMT for the first time .
The goal of encoding better lattice path probabilities directly into the phrase model was at least partially successful .
The proposed method outperforms our baseline by up to 0.6 % BLEU .
To achieve this , we presented a novel lattice design , which distinguishes between different layers , for which we can define separate indicator features .
Although these layer features are still necessary for the final system to improve over state - of - the - art performance , they are less important than in the heuristically trained setup .
One advantage of our approach is its adaptability to a variety of scenarios .
In future work , we plan to apply it to additional language pairs .
Arabic and Chinese on the source side , where the layers could represent different word segmentations , seem a natural choice .
We also hope to be able to leverage larger training data sets .
As a natural extension we plan to allow learning of cross-layer phrases .
Fur-ther , applying this framework to lattices modeling different reorderings could be an interesting direction .
Figure 2 : 2 Figure 2 : Example sentence from the newstest2009 data set .
The faulty phrase in the heuristic lattice translation is marked in boldface .
