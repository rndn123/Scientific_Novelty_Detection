title
Topic Models for Dynamic Translation Model Adaptation
abstract
We propose an approach that biases machine translation systems toward relevant translations based on topic-specific contexts , where topics are induced in an unsupervised way using topic models ; this can be thought of as inducing subcorpora for adaptation without any human annotation .
We use these topic distributions to compute topic-dependent lexical weighting probabilities and directly incorporate them into our translation model as features .
Conditioning lexical probabilities on the topic biases translations toward topicrelevant output , resulting in significant improvements of up to 1 BLEU and 3 TER on Chinese to English translation over a strong baseline .
Introduction
The performance of a statistical machine translation ( SMT ) system on a translation task depends largely on the suitability of the available parallel training data .
Domains ( e.g. , newswire vs. blogs ) may vary widely in their lexical choices and stylistic preferences , and what may be preferable in a general setting , or in one domain , is not necessarily preferable in another domain .
Indeed , sometimes the domain can change the meaning of a phrase entirely .
In a food related context , the Chinese sentence " ? " ( " f?ns ? h?ndu ? " ) would mean " They have a lot of vermicelli " ; however , in an informal Internet conversation , this sentence would mean " They have a lot of fans " .
Without the broader context , it is impossible to determine the correct translation in otherwise identical sentences .
This problem has led to a substantial amount of recent work in trying to bias , or adapt , the translation model ( TM ) toward particular domains of interest ( Axelrod et al. , 2011 ; Foster et al. , 2010 ; Snover et al. , 2008 ) . 1
The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation .
Matsoukas et al. ( 2009 ) introduced assigning a pair of binary features to each training sentence , indicating sentences ' genre and collection as a way to capture domains .
They then learn a mapping from these features to sentence weights , use the sentence weights to bias the model probability estimates and subsequently learn the model weights .
As sentence weights were found to be most beneficial for lexical weighting , Chiang et al . ( 2011 ) extends the same notion of conditioning on provenance ( i.e. , the origin of the text ) by removing the separate mapping step , directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature , estimated from only those sentences that comprise that genre or collection .
The common thread throughout prior work is the concept of a domain .
A domain is typically a hard constraint that is externally imposed and hand labeled , such as genre or corpus collection .
For example , a sentence either comes from newswire , or weblog , but not both .
However , this poses several problems .
First , since a sentence contributes its counts only to the translation table for the source it came from , many word pairs will be unobserved for a given table .
This sparsity requires smoothing .
Second , we may not know the ( sub ) corpora our training data come from ; and even if we do , " subcorpus " may not be the most useful notion of domain for better translations .
We take a finer- grained , flexible , unsupervised approach for lexical weighting by domain .
We induce unsupervised domains from large corpora , and we incorporate soft , probabilistic domain membership into a translation model .
Unsupervised modeling of the training data produces naturally occurring subcorpora , generalizing beyond corpus and genre .
Depending on the model used to select subcorpora , we can bias our translation toward any arbitrary distinction .
This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation .
In this work , we consider the underlying latent topics of the documents ( Blei et al. , 2003 ) .
Topic modeling has received some use in SMT , for instance Bilingual LSA adaptation ( Tam et al. , 2007 ) , and the BiTAM model ( Zhao and Xing , 2006 ) , which uses a bilingual topic model for learning alignment .
In our case , by building a topic distribution for the source side of the training data , we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership .
This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics .
We accomplish this by introducing topic dependent lexical probabilities directly as features in the translation model , and interpolating them log-linearly with our other features , thus allowing us to discriminatively optimize their weights on an arbitrary objective function .
Incorporating these features into our hierarchical phrase - based translation system significantly improved translation performance , by up to 1 BLEU and 3 TER over a strong Chinese to English baseline .
Model Description Lexical Weighting Lexical weighting features estimate the quality of a phrase pair by combining the lexical translation probabilities of the words in the phrase 2 ( Koehn et al. , 2003 ) . Lexical conditional probabilities p( e|f ) are obtained with maximum likelihood estimates from relative frequencies c( f , e ) / e c( f , e ) .
Phrase pair probabilities p( e|f ) are computed from these as described in Koehn et al . ( 2003 ) .
Chiang et al. ( 2011 ) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features , f s ( e|f ) , one for each domain s , which compute a new word translation table p s ( e|f ) estimated from only those sentences which belong to s : c s ( f , e ) / e c s ( f , e ) , where c s ( ? ) is the number of occurrences of the word pair in s. Topic Modeling for MT
We extend provenance to cover a set of automatically generated topics z n .
Given a parallel training corpus T composed of documents d i , we build a source side topic model over T , which provides a topic distribution p( z n |d i ) for z n = { 1 , . . . , K} over each document , using Latent Dirichlet Allocation ( LDA ) ( Blei et al. , 2003 ) .
Then , we assign p( z n |d i ) to be the topic distribution for every sentence x j ?
d i , thus enforcing topic sharing across sentence pairs in the same document instead of treating them as unrelated .
Computing the topic distribution over a document and assigning it to the sentences serves to tie the sentences together in the document context .
To obtain the lexical probability conditioned on topic distribution , we first compute the expected count e zn ( e , f ) of a word pair under topic z n : e zn ( e , f ) = d i ?T p( z n |d i ) x j ?d i c j ( e , f ) ( 1 ) where c j ( ? ) denotes the number of occurrences of the word pair in sentence x j , and then compute : p zn ( e|f ) = e zn ( e , f ) e e zn ( e , f ) ( 2 )
Thus , we will introduce 2 ?
K new word translation tables , one for each p zn ( e|f ) and p zn ( f |e ) , and as many new corresponding features f zn ( e| f ) , f zn ( f |e ) .
The actual feature values we compute will depend on the topic distribution of the document we are translating .
For a test document V , we infer topic assignments on V , p( z n | V ) , keeping the topics found from T fixed .
The feature value then becomes f zn ( e| f ) = ? log p zn ( e|f ) ? p( z n | V ) , a combination of the topic dependent lexical weight and the topic distribution of the sentence from which we are extracting the phrase .
To optimize the weights of these features we combine them in our linear model with the other features when computing the model score for each phrase pair 3 : p ?
p h p ( e , f ) unadapted features + zn ? zn f zn ( e|f ) adapted features ( 3 ) Combining the topic conditioned word translation table p zn ( e|f ) computed from the training corpus with the topic distribution p( z n | V ) of the test sentence being translated provides a probability on how relevant that translation table is to the sentence .
This allows us to bias the translation toward the topic of the sentence .
For example , if topic k is dominant in T , p k ( e|f ) may be quite large , but if p( k |V ) is very small , then we should steer away from this phrase pair and select a competing phrase pair which may have a lower probability in T , but which is more relevant to the test sentence at hand .
In many cases , document delineations may not be readily available for the training corpus .
Furthermore , a document may be too broad , covering too many disparate topics , to effectively bias the weights on a phrase level .
For this case , we also propose a local LDA model ( LTM ) , which treats each sentence as a separate document .
While Chiang et al. ( 2011 ) has to explicitly smooth the resulting p s ( e|f ) , since many word pairs will be unseen for a given domain s , we are already performing an implicit form of smoothing ( when computing the expected counts ) , since each document has a distribution over all topics , and therefore we have some probability of observing each word pair in every topic .
Feature Representation
After obtaining the topic conditional features , there are two ways to present them to the model .
They could answer the question F 1 : What is the probability under topic 1 , topic 2 , etc. , or F 2 : What is the probability under the most probable topic , second most , etc .
A model using F 1 learns whether a specific topic is useful for translation , i.e. , feature f 1 would be f 1 := p z=1 ( e|f ) ? p( z = 1 | V ) .
With F 2 , we are learning how useful knowledge of the topic distribution is , i.e. , f 1 := p ( arg maxz n ( p( zn |V ) ) ( e|f ) ? p( arg max zn ( p( z n | V ) ) |V ) .
Using F 1 , if we restrict our topics to have a oneto-one mapping with genre / collection 4 we see that our method fully recovers Chiang ( 2011 ) .
F 1 is appropriate for cross-domain adaptation when we have advance knowledge that the distribution of the tuning data will match the test data , as in Chiang ( 2011 ) , where they tune and test on web .
In general , we may not know what our data will be , so this will overfit the tuning set .
F 2 , however , is intuitively what we want , since we do not want to bias our system toward a specific distribution , but rather learn to utilize information from any topic distribution if it helps us create topic relevant translations .
F 2 is useful for dynamic adaptation , where the adapted feature weight changes based on the source sentence .
Thus , F 2 is the approach we use in our work , which allows us to tune our system weights toward having topic information be useful , not toward a specific distribution .
Experiments Setup
To evaluate our approach , we performed experiments on Chinese to English MT in two settings .
First , we use the FBIS corpus as our training bitext .
Since FBIS has document delineations , we compare local topic modeling ( LTM ) with modeling at the document level ( GTM ) .
The second setting uses the non -UN and non-HK Hansards portions of the NIST training corpora with LTM only .
Table 1 summarizes the data statistics .
For both settings , the data were lowercased , tokenized and aligned using GIZA ++ ( Och and Ney , 2003 ) to obtain bidirectional alignments , which were symmetrized using the grow-diag-final - and method ( Koehn et al. , 2003 ) .
The Chinese data were segmented using the Stanford segmenter .
We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the non -NYT and non -LAT portions of the Gigaword v4 corpus using modified Kneser - Ney smoothing ( Chen and Goodman , 1996 ) 2010 ) as our decoder , and tuned the parameters of the system to optimize BLEU ( Papineni et al. , 2002 ) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm ( MIRA ) ( Crammer et al. , 2006 ; Eidelman , 2012 ) .
Topic modeling was performed with Mallet ( Mccallum , 2002 ) , a standard implementation of LDA , using a Chinese stoplist and setting the per-document Dirichlet parameter ? = 0.01 .
This setting of was chosen to encourage sparse topic assignments , which make induced subdomains consistent within a document .
Results Results for both settings are shown in Table 2 .
GTM models the latent topics at the document level , while LTM models each sentence as a separate document .
To evaluate the effect topic granularity would have on translation , we varied the number of latent topics in each model to be 5 , 10 , and 20 .
On FBIS , we can see that both models achieve moderate but consistent gains over the baseline on both BLEU and TER .
The best model , LTM - 10 , achieves a gain of about 0.5 and 0.6 BLEU and 2 TER .
Although the performance on BLEU for both the 20 topic models LTM - 20 and GTM - 20 is suboptimal , the TER improvement is better .
Interestingly , the difference in translation quality between capturing document coherence in GTM and modeling purely on the sentence level is not substantial .
5
In fact , the opposite is true , with the LTM models achieving better performance .
6
On the NIST corpus , LTM - 10 again achieves the best gain of approximately 1 BLEU and up to 3 TER .
LTM performs on par with or better than GTM , and provides significant gains even in the NIST data setting , showing that this method can be effectively applied directly on the sentence level to large training 5
An avenue of future work would condition the sentence topic distribution on a document distribution over topics ( Teh et al. , 2006 ) . 6
As an empirical validation of our earlier intuition regarding feature representation , presenting the features in the form of F1 caused the performance to remain virtually unchanged from the baseline model .
corpora which have no document markings .
Depending on the diversity of training corpus , a varying number of underlying topics may be appropriate .
However , in both settings , 10 topics performed best .
Discussion and Conclusion Applying SMT to new domains requires techniques to inform our algorithms how best to adapt .
This paper extended the usual notion of domains to finergrained topic distributions induced in an unsupervised fashion .
We show that incorporating lexical weighting features conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations , as evidenced by significant performance gains .
This method presents several advantages over existing approaches .
We can construct a topic model once on the training data , and use it infer topics on any test set to adapt the translation model .
We can also incorporate large quantities of additional data ( whether parallel or not ) in the source language to infer better topics without relying on collection or genre annotations .
Multilingual topic models ( Boyd - Graber and Resnik , 2010 ) would provide a technique to use data from multiple languages to ensure consistent topics .
Table 1 : 1 . We used cdec ( Dyer et al. , Corpus statistics Corpus Sentences Tokens En Zh FBIS 269K 10.3 M 7.9M NIST 1.6M 44.4M 40.4M
Table 2 : 2 Performance using FBIS training corpus ( top ) and NIST corpus ( bottom ) .
Improvements are significant at the p < 0.05 level , except where indicated ( ns ) .
Model MT03 MT05 ?BLEU ?TER ?BLEU ?TER BL 28.72 65.96 27.71 67.58 GTM -5 28.95 ns 65.45 27.98 ns 67.38 ns GTM -10 29.22 64.47 28.19 66.15 GTM -20 29.19 63.41 28.00 ns 64.89 LTM -5 29.23 64.57 28.19 66.30 LTM -10 29.29 63.98 28.18 65.56 LTM - 20 29.09 ns 63.57 27.90 ns 65.17 Model MT03 MT05 ?BLEU ?TER ?BLEU ?TER BL 34.31 61.14 30.63 65.10 MERT 34.60 60.66 30.53 64.56 LTM -5 35.21 59.48 31.47 62.34 LTM -10 35.32 59.16 31.56 62.01 LTM - 20 33.90 ns 60.89 ns 30.12 ns 63.87
Language model adaptation is also prevalent but is not the focus of this work .
For hierarchical systems , these correspond to translation rules .
The unadapted lexical weight p( e |f ) is included in the model features .
By having as many topics as genres / collections and setting p( zn | di ) to 1 for every sentence in the collection and 0 to everything else .
