title
On the Exploration of English to Urdu Machine Translation
abstract
Machine Translation is the inevitable technology to reduce communication barriers in today 's world .
It has made substantial progress in recent years and is being widely used in commercial as well as non-profit sectors .
Such is only the case for European and other high resource languages .
For English - Urdu language pair , the technology is in its infancy stage due to scarcity of resources .
Present research is an important milestone in English - Urdu machine translation , as we present results for four major domains including Biomedical , Religious , Technological and General using Statistical and Neural Machine Translation .
We performed series of experiments in attempts to optimize the performance of each system and also to study the impact of data sources on the systems .
Finally , we established a comparison of the data sources and the effect of language model size on statistical machine translation performance .
Introduction Machine translation ( MT ) for low resource languages has been a challenging task ( Irvine , 2013 ; Zoph et al. , 2016 ) .
The dimensionality of difficulty increases when it comes to translating between a morphologically rich and morphologically poor language ( Habash and Sadat , 2006 ) .
In this study , we will be presenting one such pair , English to Urdu translation , with English being a morphologically simple language while Urdu is a language with rich inflectional and derivational morphology .
In case of Urdu-English translation topological distance between both languages is the biggest hurdle to get best results ( Jawaid et al. , 2016 ; . Findings of WMT 2011 evaluation ( Callison - Burch et al. , 2011 reported Urdu-English translation to be a relatively difficult problem .
With some works on rule based systems ( RBMT ) ( Tafseer and Alvi , 2002 ; Karamat , 2006 ; Naila Ata , 2007 ) and a small cascade of works on phrase based SMT systems ( Jawaid and Zeman , 2011 ; Ali et al. , 2013 ; Jawaid et al. , 2014a ) , hierarchical MT systems ( Khan et al. , 2013 ; Jawaid et al. , 2014a ) and NMT using transfer learning from a high resource language ( Zoph et al. , 2016 ) , it is still an arena requiring much work .
Present study is a consolidated study in this regard .
In this study we present results of some of the unexplored areas with reference to this language pair .
Previous works have built general domain translation systems , we present a domain analysis on Technological , Religious and General domain translations ( Section 5 ) .
This study is also an attempt to initiate the field of MT for Bio-medical domain despite zero resources available for the language pair .
Effect of smaller and larger language models on translations are also explored .
We have explored and used all the freely available English - Urdu corpora and also developed various small corpora by using human translations , synthetic corpora by machine translation and Hindi to Urdu transliteration .
Starting with a brief review of previous works we describe the resources used in Section 3 followed by detailed results in Section 4
The paper concludes with a brief discussion on results .
Related Works Perhaps , Tafseer and Alvi ( 2002 ) presents one of the earliest attempts on English to Urdu translation based on transforming the parse tree of the English sentence to Urdu using transformation rules .
Issues relating to translation for verbs in context of English to Urdu RBMT using lexical functional grammar are discussed by ( Karamat , 2006 ) .
A minimal English to Urdu RBMT system is presented in ( Naila Ata , 2007 ) ( Jawaid and Zeman , 2011 ) used phrase based models to solve the long distance word reordering problem between the two languages .
They used Emille ( Baker et al. , 2002 ) , Treebank ( Marcus et al. , 1993 ) , Quran and Bible corpora and report improvement in BLEU scores by the proposed reordering scheme .
Our general domain systems are built using these above mentioned corpora .
( Jawaid and Zeman , 2011 ) used phrase based models to solve the long distance word reordering problem between the two languages .
They used Emille ( Baker et al. , 2002 ) , Treebank ( Marcus et al. , 1993 ) , Quran and bible corpora and report improvement in BLEU scores by the proposed reordering scheme .
We also use these corpora in our general domain systems .
Building up on previous work ( Jawaid et al. , 2014a ) present a comparison of phrase based versus hierarchical systems .
They have added AFRL corpus ( not free ) to the earlier system and reported the hierarchical systems to outperform phrase based systems .
( Ali et al. , 2010 ; Ali et al. , 2013 ) built SMT using parallel ahadith corpus from Sahih bukhari and Sahih Muslim .
( Khan et al. , 2013 ) also presented a hierarchical SMT system .
Several other studies have also contributed , for instance ( Shahnawaz and Mishra , 2013 ) and ( Khan Jadoon et al. , 2017 ) present neural systems trained on small corpora .
Data Collection Data collection and its cleaning is an important but a challenging part for NLP , including machine translation .
Our Data collection scheme included 1 ) an extensive search of all the freely available parallel corpora .
2 ) Synthetic parallel corpus creation using a good translation system and 3 ) transliteration from a highly similar language , Hindi .
We have categorised the corpora in four categories , General , Biomedical , Religious and Technology , each explained in subsections 3.1 , 3.2 , 3.3 , and 3.4 respectively .
Corpus details are summarized in table 1 .
General
This section lists the corpora and their details for general category .
1 . The Emille 1 corpus ( Baker et al. , 2002 ) is a collection of annotated , parallel and monolingual data in written and spoken form .
It consists of multi domain corpora ( social , legal , educational , health , etc. ) ( Marcus et al. , 1993 ) .
The Urdu corpus was available online and we were able to get English sentences from LDC Treebank .
3 . Indic 3 is a freely available multi-domain parallel corpus created by using crowd- sourcing ( Post et al. , 2012 ) . 4 . TDIL 4 is an Indian Language Technology Proliferation and Deployment Center .
We were able to get a sample of this corpus in domains of tourism , art , culture and architecture etc .
5 . Opus 5 project ( Tiedemann , 2012 ) provides freely available annotated corpora to the research community .
We used their English - Urdu corpus comprising of Tanzil , Tatoeba , OpenSubtitles { 2016 , 2018 } , Ubuntu , GNOME and Global Voices .
Tanzil was a religious corpus , whereas Ubuntu and Gnome were technology related corpora .
We further sub categorized these according to the domains as shown in table 1 .
6 . Flickr corpora are the human and automatic translations of the flickr 8 6 Image to text Corpus .
The human translations are done from English captions to Urdu by human translators and Google translate was used for automatic translations .
7 . National Language Translations ( NLT ) are the translation documents obtained from a translation agency .
We collected translations of various articles , books , survey reports etc .
The data collected was in raw form , it was cleaned and sentence aligned .
8 . UMC002 Hindi-Urdu transliterations .
Hindi and Urdu are almost similar languages having different writing scripts .
To overcome data scarceness we experimented with transliterations from Hindi to Urdu .
A similar scheme has been used by ( Durrani et al. , 2014 ) but in the opposite direction , .i.e they transliterated from Urdu to Hindi .
Bio-Medical
Since no prior work exists in the Biomedical domain for English - Urdu , consequently there were no separate parallel corpora available .
However , Emille corpus had a small part comprising of 0.055 M English and 0.075 Urdu words respectively in health domain .
We used these as Biomedical corpus .
Furthermore , we developed Biomedical parallel corpora by using ideas from unsupervised learning techniques successfully used for other language pairs , where translations are used as additional bi-texts to cover up for data scarcity ( Lambert et al. , 2011 ) and domain adaptation ( Abdul Rauf et al. , 2016 ; Hira et al. , 2019 ) .
We collected Biomedical parallel corpora from various sources and translated them .
We are working on using domain adapted translation and language models for the biomedical domain , however , the translations used in this work are done using google translate .
We used the following corpora : 1 . Scielo 7 corpus contains documents retrieved from the scielo database comprising of titles and abstracts of published articles in bio-medical domain .
Our Scielo corpus comprises of 0.022 M sentences .
Overall it contains 0.60M English and 0.65 M Urdu words .
2 . Jang 8 group of news is a Pakistan based media corporation .
Their newspapers are published in both Urdu and English independently , but they are not the translations of each other .
We cleaned and extracted 6 k English sentences from the health news section and translated to Urdu to be used as parallel corpus .
We got a corpus of 0.11 M words in English and 0.14 M words in Urdu .
3 . EMEA 9 is a parallel corpus extracted out of documents published by European Medical Agency .
The corpus is freely available in a number of language pairs but is not available in Urdu .
We downloaded English part of corpus available in plain text and selected data related to medicines , disease , treatment and instructions .
We automatically translated the extracted dataset and produced Urdu parallel translations .
At the end of translation process we got a parallel dataset comprising of 1.03 M words in Urdu and 0.82 M words in English .
Religious
This section lists the corpora and their details for religious category .
1 . UMC005 ( Jawaid and Zeman , 2011 ) provides 6414 sentence pairs from Bible and 7957 sentence pairs form Quran corpus .
2 . QBJ corpus , which is another collection of Quran + Bible + Joshua was also available online with their own test and dev sets .
The data consists of 1.02 M English words and 1.13 M Urdu words .
3 . Tanzil is a collection of online Quranic Translations by different scholars and is a sub part of OPUS corpus .
The corpus contains 878 bi-texts with total of 0.75 M sentence fragments having 19.0M English tokens and 23.1 M Urdu tokens .
Technology
This consists of English - Urdu Parallel corpus from localization files of Ubuntu and Gnome .
Ubuntu contains 3.03 k sentences and 0.1M , 0.2M English and Urdu tokens respectively , Gnome has 0.05 M English and 0.06 M Urdu tokens .
Monolingual Urdu Corpus Monolingual corpus is an essential resource for building language models for SMT .
We used the corpus developed by ( Jawaid et al. , 2014 b ) .
This corpus consists of 95.4 million Urdu words , representing 5.4 million sentences of various domains including science , news , religion and education .
We also collected Urdu monolingual documents from Jang ( 0.03 M sentences ) and other sources comprising of ( 0.06 M sentences ) as shown at the end of table 1 .
Urdu side of all parallel corpora was also used to build the large language model used in the indicated experiments in results .
Data Preprocessing Data cleaning and preprocessing is highly important for the performance of MT systems .
The corpora provided by Emillie , NLT and Penn Tree-bank were partially parallel 9 http://opus.nlpl.eu/EMEA.php so we sentence aligned them using LF sentence aligner .
10 Due to the topological distance between the two languages we were not able to get fully aligned parallel corpus using LF aligner , thus manual alignment was done to ensure correctness .
Experimental Framework
To demonstrate the performance of MT systems on the corpora collected and generated in this work , we performed a number of experiments for SMT and a few experiments for NMT .
This section provides the description of the experimental frameworks and settings used for building SMT and NMT systems .
Statistical Machine Translation :
The goal of SMT is to produce a target sentence e from a source sentence f .
Among all possible target language sentences the one with the highest probability is chosen : e * = arg max e Pr( e |f ) ( 1 ) = arg max e Pr( f |e ) Pr(e ) ( 2 ) where Pr(f |e ) is the translation model and Pr(e ) is the target language model ( LM ) .
This approach is usually referred to as the noisy source- channel approach in SMT ( Brown et al. , 1993 ) .
Bilingual corpora are needed to train the translation model and monolingual texts to train the target language model .
Common practice is to use phrases as translation units ( Koehn et al. , 2003 ; Och and Ney , 2003a ) instead of the original word - based approach .
A phrase is defined as a group of source words f that should be translated together into a group of target words ?.
The translation model in phrase - based systems includes the phrase translation probabilities in both directions , i.e. P ( ?| f ) and P ( f | ? ) .
The use of a maximum entropy approach simplifies the introduction of several additional models explaining the translation process : e * = arg max P r( e|f ) = arg max e { exp ( i ? i h i ( e , f ) ) } ( 3 )
The feature functions h i are the system models and the ?
i weights are typically optimized to maximize a scoring function on a development set .
In our system fourteen features functions were used , namely phrase and lexical translation probabilities in both directions , seven features for the lexicalized distortion model , a word and a phrase penalty , and a target language model .
To built standard phrase - based SMT systems we used Moses toolkit ( Koehn et al. , 2007 ) , with the default settings for all the parameters .
A 5 - gram KenLM ( Heafield , 2011 ) language model was used .
For individual systems the language models were trained on the target side of the corpus .
For experiments on size of the language model , all the available monolingual and target side corpus was used ( 122.5 M Urdu words ) .
10 https://sourceforge.net/projects/aligner/
Word-alignment was done using Giza ++ ( Och and Ney , 2003 b ) with grow-diag-final - and symmetrization method .
Maximum sentence length was chosen to be 100 .
A distortion limit of 6 with 100 - best list was used .
Msdbidirectional - fe feature was used for lexical reordering with the phrase limit of 5 .
Systems were tuned on the development data using the MERT ( Och , 2003 ) . BLEU ( Papineni et al. , 2002 ) scores were computed on dev and test sets of the corpora , as well as on standard test sets .
BLEU scores were calculated using multi-bleu.perl .
Scoring is case sensitive and includes punctuation .
Neural Machine Translation :
We used OpenNMT 11 ( Klein et al. , 2017 ) for building Neural MT systems .
Two layered encoder-decoder architecture with global attention ( Luong et al. , 2015 ) was used .
We used RNN size of 500 and LSTM for cell structure for both encoder and decoder , applying dropout of 0.3 for each input cell .
Translations were evaluated on BLEU scores to enable comparison with the corresponding SMT systems .
Development and Test sets Most of the corpora available online had their own development ( dev ) and test sets , so we evaluated the systems according to these dev and test sets .
To be able to compare the systems in each domain , we created Standard test set ( STS ) for each domain comprising of 1 k sentences .
We randomly selected sentences from test sets of each data source of the particular domain .
This was done on the basis of data set size and combined these specific sized chunks so that each data-set is represented on the basis of its size in the standard test set .
We also used the test set of CLE 9 which was used to evaluate the general domain systems and the standard Scielo test set for Bio-Medical domain .
Results and Discussion
One of the endeavours of our study is to present domain specific translation results .
As is common in machine learning approaches , the domain of the system being built depends on the data used to train the system .
MT performance quickly degrades when the testing domain is different from the training domain .
Standalone SMT Systems
To build the best domain specific SMT system , we first explored the performance of each corpora for standalone SMT systems .
T anzil and Genome showed the best performance for Religious and technology domains respectively .
While over-fitting is observed in these two domains .
The performance of the systems , built for these two domains , have shown a uniform trend for both self and standard test sets .
Effect of size of Language Model Along with , the exploration of best SMT system for each category we also investigated the effect of the size of language model on each standalone SMT system .
To explore this dimension , a large language model was also build by concatenating the Urdu text of all the bi-texts and the monolingual corpus mentioned in section 3.5 .
The scores for large LM are shown in the third column in table 2 .
It is observed that the BLEU scores of all the standalone systems approximately doubled with large LM .
Figure 1 shows these results graphically for each domain .
These results highlight the effect of bigger language model on SMT quality , obviously a bigger language model helps improve translation quality by improving the grammar of the output sentences .
Concatenated SMT Systems
After building standalone systems for each corpus , we selected the corpora which resulted in best BLEU scores , for building systems by concatenating different combinations of corpora .
We selected systems on the basis of best score among the standalone systems from each domain ( baseline system ) and concatenated them with system having second highest BLEU score .
Table 3 reports these results .
Table 3 : Results of SMT on baselines and addition of bitexts .
Bio-Medical Domain Bio-medical domain is an interesting domain as the corpora are not of same type .
Emille are the health domain sentences taken from the Emille corpus , Jang sentences are taken from a semi-parallel comparable corpus and then sentence aligned and human corrected .
Whereas , EM EA and Scielo are synthetic forward translated corpora .
EM EA was chosen as baseline , for bio-medical domain , having the highest score 44.45 amongst other three standalone systems .
Then , we built a system on EM EA concatenated with the second best system Scielo , having score of 25.95 ( table 2 ) .
The BLEU score of the resultant system EM EA + Scielo is 50.34 ( table 3 ) .
We can see an improvement in the score after concatenation of these two data-sets .
Note that this system is built with only forward translated synthetic corpus , and we get an appreciable BLEU score .
This system EM EA + Scielo , is further concatenated with jang corpus ( standalone score 17.78 ) and the resultant score of the EM EA + scielo + jang system is 49.76 , which is a bit lower than the previous system 's score .
Contrary to the standard test set scores , addition of bitexts did not improve scores for dev and test , rather resulted in a de - Emille is again a standard biomedical corpus comprising of health documents from the EMILLE corpus ( section 3.6 ) , and its concatenation improved the overall BLEU score .
An increase of 6.26 points upon the addition of just 0.86 M words of Scielo + Jang + Emille corpora to 1.03 M words of EMEA ( baseline ) , has been observed which is a significant gain .
These are encouraging results for the development of standard corpora for the Bio-medical domain .
Religious , General and Technology Domain
For the religious domain we have two corpora namely T anzil and the other is concatenation of Quran , Bible and Joshua ( QBJ ) .
Firstly we built two standalone systems for both corpora as shown in
Impact of Various Corpora
We performed series of experiments using transliterations , human and machine translated data to compare the performance of such systems .
These results are reported in
NMT Systems
We are presenting NMT system performance only for Bio-Medical domain .
Table 5 shows the results of our experiments for NMT .
We maintained the same baseline and corpus concatenation combination as used in SMT experiments .
The results of Bio-Medical NMT are lower than the corresponding SMT systems ( Table 3 ) .
This is expected as NMT systems do n't perform well with small amounts of corpus .
A unanimous observation is that addition of bitexts improves the systems across all dev and test sets , a slight deviation to this trend is observed when Emille is added to EM EA + Scielo + jang ( last row in Table 5 ) .
Conclusion
We presented domain based results on SMT and NMT systems for translation from English to Urdu .
This is the first work being reported on several domains for the English - Urdu language pair .
We collected corpora for four main domains namely Bio-medical , Religious , Technology and General .
We experimented with various methods to reduce data scarcity which include , the use of automatic translations and transliterations .
We also collected and compiled human translations from translation agencies as well as produced human translations of Flickr 8 k dataset .
We performed series of experiments in attempts to optimize the performance of each system and also to study the impact of data sources on the systems .
Finally , we established a comparison of the data sources and the effect of Language Model size on statistical machine translation performance .
Figure 1 : 1 Figure 1 : Comparison of systems built on small and large language model ( x- axis represents words in millions )
