title
Hierarchical Phrase Table Combination for Machine Translation
abstract
Typical statistical machine translation systems are batch trained with a given training data and their performances are largely influenced by the amount of data .
With the growth of the available data across different domains , it is computationally demanding to perform batch training every time when new data comes .
In face of the problem , we propose an efficient phrase table combination method .
In particular , we train a Bayesian phrasal inversion transduction grammars for each domain separately .
The learned phrase tables are hierarchically combined as if they are drawn from a hierarchical Pitman - Yor process .
The performance measured by BLEU is at least as comparable to the traditional batch training method .
Furthermore , each phrase table is trained separately in each domain , and while computational overhead is significantly reduced by training them in parallel .
Introduction Statistical machine translation ( SMT ) systems usually achieve ' crowd- sourced ' improvements with batch training .
Phrase pair extraction , the key step to discover translation knowledge , heavily relies on the scale of training data .
Typically , the more parallel corpora used , the more phrase pairs and more accurate parameters will be learned , which can obviously be beneficial to improving translation performances .
Today , more parallel sentences are drawn from divergent domains , and the size keeps growing .
Consequently , how to effectively use those data and improve translation performance becomes a challenging issue .
Batch retraining is not acceptable for this case , since it demands serious computational overhead when training on a large data set , and it requires us to re-train every time new training data is available .
Even if we can handle the large computation cost , improvement is not guaranteed every time we perform batch tuning on the newly updated training data obtained from divergent domains .
Traditional domain adaption methods for SMT are also not adequate in this scenario .
Most of them have been proposed in order to make translation systems perform better for resource -scarce domains when most training data comes from resourcerich domains , and ignore performance on a more generic domain without domain bias .
As an alternative , incremental learning may resolve the gap by incrementally adding data sentence - by-sentence into the training data .
Since SMT systems trend to employ very large scale training data for translation knowledge extraction , updating several sentence pairs each time will be annihilated in the existing corpus .
This paper proposes a new phrase table combination method .
First , phrase pairs are extracted from each domain without interfering with other domains .
In particular , we employ the nonparametric Bayesian phrasal inversion transduction grammar ( ITG ) of Neubig et al . ( 2011 ) to perform phrase table extraction .
Second , extracted phrase tables are combined as if they are drawn from a hierarchical Pitman - Yor process , in which the phrase tables represented as tables in the Chinese restaurant process ( CRP ) are hierarchically chained by treating each of the previously learned phrase tables as prior to the current one .
Thus , we can easily update the chain of phrase tables by appending the newly extracted phrase table and by treating the chain of the previous ones as its prior .
Experiment results indicate that our method can achieve better translation performance when there exists a large divergence in domains , and can achieve at least comparable results to batch training methods , with a significantly less computational overhead .
The rest of the paper is organized as follows .
In Section 2 , we introduce related work .
In section 3 , we briefly describe the translation model with phrasal ITGs and Pitman - Yor process .
In section 4 , we explain our hierarchical combination approach and give experiment results in section 5 .
We conclude the paper in the last section .
Related Work Bilingual phrases are cornerstones for phrasebased SMT systems ( Och and Ney , 2004 ; Koehn et al. , 2003 ; Chiang , 2005 ) and existing translation systems often get ' crowd- sourced ' improvements ( Levenberg et al. , 2010 ) .
A number of approaches have been proposed to make use of the full potential of the available parallel sentences from various domains , such as domain adaptation and incremental learning for SMT .
The translation model and language model are primary components in SMT .
Previous work proved successful in the use of large-scale data for language models from diverse domains ( Brants et al. , 2007 ; Schwenk and Koehn , 2008 ) .
Alternatively , the language model is incrementally updated by using a succinct data structure with a interpolation technique ( Levenberg and Osborne , 2009 ; Levenberg et al. , 2011 ) .
In the case of the previous work on translation modeling , mixed methods have been investigated for domain adaptation in SMT by adding domain information as additional labels to the original phrase table ( Foster and Kuhn , 2007 ) .
Under this framework , the training data is first divided into several parts , and phase pairs are extracted with some sub-domain features .
Then all the phrase pairs and features are tuned together with different weights during decoding .
As a way to choose the right domain for the domain adaption , a classifier - based method and a feature - based method have been proposed .
Classification - based methods must at least add an explicit label to indicate which domain the current phrase pair comes from .
This is traditionally done with an automatic domain classifier , and each input sentence is classified into its corresponding domain .
As an alternative to the classification - based approach , employed a featurebased approach , in which phrase pairs are enriched by a feature set to potentially reflect the domain information .
The similarity calculated by a information retrieval system between the training subset and the test set is used as a feature for each parallel sentence ( Lu et al. , 2007 ) .
Monolingual topic information is taken as a new feature for a domain adaptive translation model and tuned on the development set ( Su et al. , 2012 ) .
Regardless of underlying methods , either classifier - based or featurebased method , the performance of current domain adaptive phrase extraction methods is more sensitive to the development set selection .
Usually the domain similar to a given development data is usually assigned higher weights .
Incremental learning in which new parallel sentences are incrementally updated to the training data is employed for SMT .
Compared to traditional frequent batch oriented methods , an online EM algorithm and active learning are applied to phrase pair extraction and achieves almost comparable translation performance with less computational overhead ( Levenberg et al. , 2010 ; Gonz?lez-Rubio et al. , 2011 ) .
However , their methods usually require numbers of hyperparameters , such as mini-batch size , step size , or human judgment to determine the quality of phrases , and still rely on a heuristic phrase extraction method in each phrase table update .
Phrase Pair Extraction with Unsupervised Phrasal ITGs Recently , phrase alignment with ITGs ( Cherry and Lin , 2007 ; Zhang et al. , 2008 ; Blunsom et al. , 2008 ) and parameter estimation with Gibbs sampling ( DeNero and Klein , 2008 ; Blunsom and Cohn , 2010 ) are popular .
Here , we employ a method proposed by Neubig et al . ( 2011 ) , which uses parametric Bayesian inference with the phrasal ITGs ( Wu , 1997 ) .
It can achieve comparable translation accuracy with a much smaller phrase table than the traditional GIZA ++ and heuristic phrase extraction methods .
It has also been proved successful in adjusting the phrase length granularity by applying character - based SMT with more sophisticated inference ( Neubig et al. , 2012 ) .
ITG is a synchronous grammar formalism which analyzes bilingual text by introducing inverted rules , and each ITG derivation corresponds to the alignment of a sentence pair ( Wu , 1997 ) .
Translation probabilities of ITG phrasal align-ments can be estimated in polynomial time by slightly limiting word reordering ( DeNero and Klein , 2008 ) .
More formally , P e , f ; ? x , ?
t are the probability of phrase pairs e , f , which is parameterized by a phrase pair distribution ?
t and a symbol distribution ? x . ?
x is a Dirichlet prior , and ?
t is estimated with the Pitman - Yor process ( Pitman and Yor , 1997 ; Teh , 2006 ) , which is expressed as ?
t ?
P Y d , s , P dac ( 1 ) where d is the discount parameter , s is the strength parameter , and , and P dac is a prior probability which acts as a fallback probability when a phrase pair is not in the model .
Under this model , the probability for a phrase pair found in a bilingual corpus E , F can be represented by the following equation using the Chinese restaurant process ( Teh , 2006 ) : P e i , f i ; E , F = 1 C + s ( c i ? d ? t i ) +
1 C + s ( s + d ? T ) ?
P dac ( e i , f i ) ( 2 ) where 1 .
c i and t i are the customer and table count of the ith phrase pair e i , f i found in a bilingual corpus E , F ; 2 .
C and T are the total customer and table count in corpus E , F ; 3 . d and s are the discount and strengthen hyperparameters .
The prior probability P dac is recursively defined by breaking a longer phrase pair into two through the recursive ITG 's generative story as follows ( Neubig et al. , 2011 ) c.
If x = IN V , follow a similar process as b , but concatenate f 1 and f 2 in reverse order e 1 e 2 , f 2 f 1 .
Note that the P dac is recursively defined through the binary branched P , which in turns employs P dac as a prior probability .
P base is a base measure defined as a combination of the IBM Models in two directions and the unigram language models in both sides .
Inference is carried out by a heuristic beam search based block sampling with an efficient look ahead for a faster convergence ( Neubig et al. , 2012 ) . Compared to GIZA ++ with heuristic phrase extraction , the Bayesian phrasal ITG can achieve competitive accuracy under a smaller phrase table size .
Further , the fallback model can incorporate phrases of all granularity by following the ITG 's recursive definition .
Figure 1 ( b ) illustrates an example of the phrasal ITG derivation for word alignment in Figure 1 ( a ) in which a bilingual sentence pair is recursively divided into two through the recursively defined generative story .
Hierarchical Phrase Table Combination
We propose a new phrase table combination method , in which individually learned phrase table are hierarchically chained through a hierarchical Pitman - Yor process .
Firstly , we assume that the whole training data E , F can be split into J domains , { E 1 , F 1 , . . . , E J , F J }.
Then phrase pairs are extracted from each domain j ( 1 ? j ? J ) separately with the method introduced in Section 3 .
In traditional domain adaptation approaches , phrase pairs are extracted together with their probabilities and / or frequencies so that the extracted phrase pairs are merged uniformly or after scaling .
In this work , we extract the table counts for each phrase pair under the Chinese restaurant process given in Section 3 .
In Figure 2 ( b ) , a CRP is illustrated which has K tables and N customers with each chair representing a customer .
Meanwhile there are two parameters , discount and strength for each domain similar to the ones in Equation ( 1 ) .
Our proposed hierarchical phrase table combination can be formally expressed as following : ? 1 ? P Y ( d 1 , s 1 , P 2 ) ? ? ? ? ? ? ? j ? P Y ( d j , s j , P j+1 ) ? ? ? ? ? ? ? J ? P Y d J , s J , P J base ( 3 ) Here the ( j + 1 ) th layer hierarchical Pitman - Yor process is employed as a base measure for the jth layer hierarchical Pitman - Yor process .
The hierarchical chain is terminated by the base measure from the Jth domain P J base .
The hierarchical structure is illustrated in Figure 2 ( a ) in which the solid lines implies a fall back using the table counts from the subsequent domains , and the dotted lines means the final fallback to the base measure P J base .
When we query a probability of a phrase pair e , f , we first query the probability of the first layer P 1 ( e , f ) .
If e , f is not in the model , we will fallback to the next level of P 2 ( e , f ) .
This process continues until we reach the J th base measure of P J ( e , f ) .
Each fallback can be viewed as a translation knowledge integration process between subsequent domains .
For example in Figure 2 ( a ) , the ith phrase pair e i , f i appears only in the domain 1 and domain 2 , so its translation probability can be calculated by substituting Equation ( 3 ) with Equation ( 2 ) : P e i , f i ; E , F = 1 C 1 + s 1 ( c 1 i ? d 1 ? t 1 i ) + s 1 + d 1 ? T 1 ( C 1 + s 1 ) ? ( C 2 + s 2 ) ( c 2 i ? d 2 ? t 2 i ) + J j=1 s j + d j ?
T j C j + s j ?
P J base ( e i , f i ) ( 4 ) where the superscript indicates the domain for the corresponding counts , i.e. c j i for the customer count in the jth domain .
The first term in Equation ( 4 ) is the phrase probability from the first domain , and the second one comes from the second domain , but weighted by the fallback weight of the 1st domain .
Since e i , f i does not appear in the rest of the layers , the last term is taken from all the fallback weight from the second layer to the Jth layer with the final P J base .
All the parameters ? j and hyperparameters d j and s j , are obtained by learning on the jth domain .
Returning the hyperparameters again when cascading another domain may improve the performance of the combination weight , but we will leave it for future work .
The hierarchical process can be viewed as an instance of adapted integration of translation knowledge from each sub-domain .
Algorithm 1 Translation Probabilities Estimation Input : c j i , t j i , P j base , C j , T j , d j and s j Output :
The translation probabilities for each pair 1 : for all phrase pair e i , f i do 2 : Initialize the P ( e i , f i ) = 0 and w i = 1 3 : for all domain E j , F j such that 1 j J ? 1 do 4 : if e i , f i ?
E j , F j then 5 : P ( e i , f i ) += w i ?
( C j i ? d j ? t j i ) /( C j + s j ) 6 : end if 7 : w i = w i ?
( s j + d j ? T j ) / ( C j + s j ) 8 : end for 9 : P ( e i , f i ) += w i ?
( C J i ? d J ? t J i + ( s J + d J ? T J ) ?
P J base ( e i , f i ) ) /( C J + s J ) 10 : end for Our approach has several advantages .
First , each phrase pair extraction can concentrate on a small portion of domain-specific data without interfering with other domains .
Since no tuning stage is involved in the hierarchical combination , we can easily include a new phrase table from a new domain by simply chaining them together .
Second , phrase pair phrase extraction in each domain is completely independent , so it is easy to parallelize in a situation where the training data is too large to fit into a small amount of memory .
Finally , new domains can be integrated incrementally .
When we encounter a new domain , and if a phrase pair is completely new in terms of the model , the phrase pair is simply appended to the current model , and computed without the fallback probabilities , since otherwise , the phrase pair would be boosted by the fallback probabilities .
Pitman - Yor process is also employed in n-gram language models which are hierarchically represented through the hierarchical Pitman - Yor process with switch priors to integrate different domains in all the levels ( Wood and Teh , 2009 ) .
Our work incrementally combines the models from different domains by directly employing the hierarchical process through the base measures .
Experiment
We evaluate the proposed approach on the Chinese-to - English translation task with three data sets with different scales .
Experiment Setup
The first data set comes from the IWSLT2012 OLYMPICS task consisting of two training sets : the HIT corpus , which is closely related to the Beijing 2008 Olympic Games , and the BTEC corpus , which is a multilingual speech corpus containing tourism-related sentences .
The second data set , the FBIS corpus , is a collection of news articles and does not have domain information itself , so a Latent Dirichlet Allocation ( LDA ) tool , PLDA 1 , is used to divide the whole corpus into 5 different sub-domains according to the concatenation of the source side and target side as a single sentence ( Liu et al. , 2011 ) .
The third data set is composed of 5 corpora 2 from LDC with various domains , including news , magazine , and finance .
The details are shown in Table 1 .
In order to evaluate our approach , four phrase pair extraction methods are performed : 1 . GIZA - linear : Phase pairs are extracted in each domain by GIZA ++ ( Och and Ney , 2003 ) and the " grow-diag-final - and " method with a maximum length 7 .
The phrase tables from various domains are linearly combined by averaging the feature values .
2 . Pialign-linear : Similar to GIZA - linear , but we employed the phrasal ITG method described in Section 3 using the pialign toolkit 3 ( al. , 2011 ) .
Extracted phrase pairs are linearly combined by averaging the feature values .
3 . GIZA - batch : Instead of splitting into each domain , the data set is merged as a single corpus and then a heuristic GZA - based phrase extraction is performed , similar as GIZA - linear .
4 . Pialign- batch : Similar to the GIZA - batch , a single model is estimated from a single , merged corpus .
Since pialign cannot handle large data , we did not experiment on the largest LDC data set .
5 . Pialign-adaptive : Alignment and phrase pairs extraction are same to Pialign - batch , while translation probabilities are estimated by the adaptive method with monolingual topic information ( Su et al. , 2012 ) .
The method established the relationship between the out-ofdomain bilingual corpus and in-domain monolingual corpora via topic distribution to estimate the translation probability .
?(?| f ) = t f ?( ? , t f | f ) = t f ?(?| t f , f ) ? P ( t f | f ) ( 5 ) where ?(?|t f , f ) is the probability of translating f into ?
given the source-side topic f , P ( t f | f ) is the phrase-topic distribution of f.
The method we proposed is named Hiercombin .
It extracts phrase pairs in the same way as the Pialign-linear .
In the phrase table combination process , the translation probability of each phrase pair is estimated by the Hier-combin and the other features are also linearly combined by averaging the feature values .
Pialign is used with default parameters .
The parameter ' samps ' is set to 5 , which indicates 5 samples are generated for a sentence pair .
The IWSLT data consists of roughly 2 , 000 sentences and 3 , 000 sentences each from the HIT and BTEC for development purposes , and the test data consists of 1 , 000 sentences .
For the FBIS and LDC task , we used NIST MT 2002 and 2004 for development and testing purposes , consisting of 878 and 1 , 788 sentences respectively .
We employ Moses , an open-source toolkit for our experiment ( Koehn et al. , 2007 ) . SRILM Toolkit ( Stolcke , 2002 ) is employed to train 4 - gram language models on the Xinhua portion of Gigaword corpus , while for the IWLST2012 data set , only its training set is used .
We use batch -MIRA ( Cherry and Foster , 2012 ) to tune the weight for each feature and translation quality is evaluated by the case-insensitive BLEU - 4 metric ( Papineni et al. , 2002 ) .
The BLEU scores reported in this paper are the average of 5 independent runs of independent batch - MIRA weight training , as suggested by ( Clark et al. , 2011 ) .
Result and Analysis
Performances of various extraction methods
We carry out a series of experiments to evaluate translation performance .
The results are listed in Table 2 .
Our method significantly outperforms the baseline Pialign-linear .
Except for the translation probabilities , the phrase pairs of two methods are exactly same , so the number of phrase pairs are equal in the two methods .
Further more , the performance of the baseline Pialign - adaptive is also higher than the baseline Pialign-linear 's and lower than ours .
This proves that the adaptive method with monolingual topic information is useful in the tasks , but our approach with the hierarchical Pitman - Yor process can estimate more accurate translation probabilities based on all the data from various domains .
Compared with the GIZA - batch , our approach achieves competitive performance with a much smaller phrase table .
The number of phase pairs generated by our method is only 73.9 % , 52.7 % , and 45.4 % of the GIZA - batch 's respectively .
In the IWLST2012 data set , there is a huge difference gap between the HIT corpus and the BTEC corpus , and our method gains 0.814 BLEU improvement .
While the FBIS data set is artificially divided and no clear human assigned differences among subdomains , our method loses 0.09 BLEU .
In the framework we proposed , phrase pairs are extracted from each domain completely independent of each other , so those tasks can be executed on different machines , at different times , and of course in parallel when we assume that the domains are not incrementally added in the training data .
The runtime of our approach and the batch - based ITGs sampling method in the FBIS data set is listed in Table 3 measured on a 2.7 GHz E5 - 2680 CPU and 128 Gigabyte memory .
When comparing the hier-combin with the pialign - batch , the BLEU scores are a little higher while the time spent for training is much lower , almost one quarter of the pialign - batch .
Even the performance of the pialign-linear is better than the Baseline GIZA - linear's , which means that phrase pair extraction with hierarchical phrasal ITGs and sampling is more suitable for domain adaptation tasks than the combination GIZA + + and a heuristic method .
Generally , the hierarchical combination method exploits the nature of a hierarchical Pitman - Yor process and gains the advantage of its smoothing effect , and our approach can incrementally generate a succinct phrase table based on all the data from various domains with more accurate prob-abilities .
Traditional SMT phrase pair extraction is batch - based , while our method has no obvious shortcomings in translation accuracy , not to mention efficiency .
Effect of Integration Order Here , we evaluate whether our hierarchical combination is sensitive to the order of the domains when forming a hierarchical structure .
Through Equation ( 3 ) , in our experiments , we chained the domains in the order listed in Table 1 , which is in almost chronological order .
Table 4 shows the BLEU scores for the three data sets , in which the order of combining phrase tables from each domain is alternated in the ascending and descending of the similarity to the test data .
The similarity between the data from each domain and the test data is calculated using the perplexity measure with 5 gram language model .
The model learned from the domain more similar to the test data is placed in the front so that it can largely influence the parameter computation with less backoff effects .
There is a big difference between the two opposite order in IWSLT 2012 data set , in which more than one point of decline in BLEU score when taking the BTEC corpus as the first layer .
Note that the perplexity of BTEC was 344.589 while that of HIT was 107.788 .
The result may indicate that our hierarchical phrase combination method is sensitive to the integration order when the training data is small and there exists large gap in the similarity .
However , if most domains are similar ( FBIS data set ) or if there are enough parallel sentence pairs ( NIST data set ) in each domain , then the translation performances are almost similar even with the opposite integrating orders .
Conclusion and Future Work
In this paper , we present a novel hierarchical phrase table combination method for SMT , which can exploit more of the potential from all of data coming from various fields and generate a suc-cinct phrase table with more accurate translation probabilities .
The method assumes that a combined model is derived from a hierarchical Pitman - Yor process with each prior learned separately in each domain , and achieves BLEU scores competitive with traditional batch - based ones .
Meanwhile , the framework has natural characteristics for parallel and incremental phrase pair extraction .
The experiment results on three different data sets indicate the effectiveness of our approach .
In future work , we will also introduce incremental learning for phase pair extraction inside a domain , which means using the current translation probabilities already obtained as the base measure of sampling parameters for the upcoming domain .
Furthermore , we will investigate any tradeoffs between the accuracy of the probability estimation and the coverage of phrase pairs .
Figure 1 : 1 Figure 1 : A word alignment ( a ) , and its hierarchical derivation ( b ) .
