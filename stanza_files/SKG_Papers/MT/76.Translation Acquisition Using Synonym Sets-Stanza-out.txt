title
Translation Acquisition Using Synonym Sets
abstract
We propose a new method for translation acquisition which uses a set of synonyms to acquire translations from comparable corpora .
The motivation is that , given a certain query term , it is often possible for a user to specify one or more synonyms .
Using the resulting set of query terms has the advantage that we can overcome the problem that a single query term 's context vector does not always reliably represent a terms meaning due to the context vector 's sparsity .
Our proposed method uses a weighted average of the synonyms ' context vectors , that is derived by inferring the mean vector of the von Mises - Fisher distribution .
We evaluate our method , using the synsets from the cross-lingually aligned Japanese and English WordNet .
The experiments show that our proposed method significantly improves translation accuracy when compared to a previous method for smoothing context vectors .
Introduction Automatic translation acquisition is an important task for various applications .
For example , finding term translations can be used to automatically update existing bilingual dictionaries , which are an indispensable resource for tasks such as cross-lingual information retrieval and text mining .
Various previous research like ( Rapp , 1999 ; Fung , 1998 ) has shown that it is possible to acquire word translations from comparable corpora .
We suggest here an extension of this approach which uses several query terms instead of a single query term .
A user who searches a translation for a query term that is not listed in an existing bilingual dictionary , might first try to find a synonym of that term .
For example , the user might look up a synonym in a thesaurus 1 or might use methods for automatic synonym acquisition like described in ( Grefenstette , 1994 ) .
If the synonym is listed in the bilingual dictionary , we can consider the synonym 's translations as the translations of the query term .
Otherwise , if the synonym is not listed in the dictionary either , we use the synonym together with the original query term to find a translation .
We claim that using a set of synonymous query terms to find a translation is better than using a single query term .
The reason is that a single query term 's context vector is , in general , unreliable due to sparsity .
For example , a low frequent query term tends to have many zero entries in its context vector .
To mitigate this problem it has been proposed to smooth a query 's context vector by its nearest neighbors ( Pekar et al. , 2006 ) .
However , nearest neighbors , which context vectors are close the query 's context vector , can have different meanings and therefore might introduce noise .
The contributions of this paper are two -fold .
First , we confirm experimentally that smoothing a query 's context vector with its synonyms leads in deed to higher translation accuracy , compared to smoothing with nearest neighbors .
Second , we propose a simple method to combine a set of context vectors that performs in this setting better than a method previously proposed by ( Pekar et al. , 2006 ) .
Our approach to combine a set of context vec-tors is derived by learning the mean vector of a von Mises - Fisher distribution .
The combined context vector is a weighted - average of the original contextvectors , where the weights are determined by the word occurrence frequencies .
In the following section we briefly show the relation to other previous work .
In Section 3 , we explain our method in detail , followed by an empirical evaluation in Section 4 .
We summarize our results in Section 6 .
Related Work
There are several previous works on extracting translations from comparable corpora ranging from ( Rapp , 1999 ; Fung , 1998 ) , and more recently ( Haghighi et al. , 2008 ; Laroche and Langlais , 2010 ) , among others .
Essentially , all these methods calculate the similarity of a query term 's context vector with each translation candidate 's context vector .
The context vectors are extracted from the comparable corpora , and mapped to a common vector space with the help of an existing bilingual dictionary .
The work in ( D? jean et al. , 2002 ) uses crosslingually aligned classes in a multilingual thesaurus to improve the translation accuracy .
Their method uses the probability that the query term and a translation candidate are assigned to the same class .
In contrast , our method does not need cross-lingually aligned classes .
Ismail and Manandhar ( 2010 ) proposes a method that tries to improve a query 's context vector by using in - domain terms .
In -domain terms are the terms that are highly associated to the query , as well as highly associated to one of the query 's highly associated terms .
Their method makes it necessary that the query term has enough highly associated context terms .
2
However , a low-frequent query term might not have enough highly associated terms .
In general if a query term has a low-frequency in the corpus , then its context vector is sparse .
In that case , the chance of finding a correct translation is reduced ( Pekar et al. , 2006 ) .
Therefore , Pekar et al. ( 2006 ) suggest to use distance - based averaging to smooth the context vector of a low-frequent query term .
Their smoothing strategy is dependent on the occurrence frequency of a query term and its close neighbors .
Let us denote q the context vector of the query word , and K be the set of its close neighbors .
The smoothed context vector q ? is then derived by using : q ? = ? ? q + ( 1 ? ? ) ? ? x?K w x ? x , ( 1 ) where w x is the weight of neighbor x , and all weights sum to one .
The context vectors q and x are interpreted as probability vectors and therefore L1 - normalized .
The weight w x is a function of the distance between neighbor x and query q.
The parameter ? determines the degree of smoothing , and is a function of the frequency of the query term and its neighbors : ? = log f ( q ) log max x?K?{q} f ( x ) ( 2 ) where f ( x ) is the frequency of term x .
Their method forms the baseline for our proposed method .
Proposed Method
Our goal is to combine the context vectors to one context vector which is less sparse and more reliable than the original context vector of query word q .
We assume that for each occurrence of a word , its corresponding context vector was generated by a probabilistic model .
Furthermore , we assume that synonyms are generated by the same probability distribution .
Finally we use the mean vector of that distribution to represent the combined context vector .
By using the assumption that each occurrence of a word corresponds to one sample of the probability distribution , our model places more weight on synonyms that are highly -frequent than synonyms that occur infrequently .
This is motivated by the assumption that context vectors of synonyms that occur with high frequency in the corpus , are more reliable than the ones of low-frequency synonyms .
When comparing context vectors , work like Laroche and Langlais ( 2010 ) observed that often the cosine similarity performs superior to other distance - measures , like , for example , the euclidean distance .
This suggests that context vectors tend to lie in the spherical vector space , and therefore the von Mises - Fisher distribution is a natural choice for our probabilistic model .
The von Mises - Fisher distribution was also successfully used in the work of ( Basu et al. , 2004 ) to cluster text data .
The von Mises - Fisher distribution with location parameter ? , and concentration parameter ? is defined as : p( x | ? , ? ) = c( ? ) ? e ?x?
T , where c( ? ) is a normalization constant , and || x || = ||?|| = 1 , and ? ? 0 . || denotes here the L2- norm .
The cosine-similarity measures the angle between two vectors , and the von Mises distribution defines a probability distribution over the possible angles .
The parameter ? of the von Mises distribution is estimated as follows ( Jammalamadaka and Sengupta , 2001 ) : Given the words x 1 , ... , x n , we denote the corresponding context vectors as x 1 , ... , x n , and assume that each context vector is L2 - normalized .
Then , the mean vector ? is calculated as : ? = 1 Z n ?
i=1 x i n where Z ensures that the resulting context vector is L2 - normalized , i.e. Z is || ? n i=1 x i n ||.
For our purpose , ? is irrelevant and is assumed to be any fixed positive constant .
Since we assume that each occurrence of a word x in the corpus corresponds to one observation of the corresponding word 's context vector x , we get the following formula : ? = 1 Z ? ? n ? i=1 f ( x i ) ? n j=1 f ( x j ) ?
x i where Z ? is now || ? n i=1 f ( x i ) ? n j=1 f ( x j ) ? x i ||.
We then use the vector ? as the combined vector of the words ' context vectors x i .
Our proposed procedure to combine the context vector of query word q and its synonyms can be summarized as follows :
1 . Denote the context vectors of q and its synonyms as x 1 , ... , x n , and L2 - normalize each context vector .
2 . Calculate the weighted average of the vectors x 1 , ... , x n , whereas the weights correspond to the frequencies of each word x i .
3 . L2 - normalize the weighted average .
Experiments
As source and target language corpora we use a corpus extracted from a collection of complaints concerning automobiles compiled by the Japanese Ministry of Land , Infrastructure , Transport and Tourism ( MLIT ) 3 and the USA National Highway Traffic Safety Administration ( NHTSA ) 4 , respectively .
The Japanese corpus contains 24090 sentences that were POS tagged using MeCab ( Kudo et al. , 2004 ) .
The English corpus contains 47613 sentences , that were POS tagged using Stepp Tagger ( Tsuruoka et al. , 2005 ) , and use the Lemmatizer ( Okazaki et al. , 2008 ) to extract and stem content words ( nouns , verbs , adjectives , adverbs ) .
For creating the context vectors , we calculate the association between two content words occurring in the same sentence , using the log-odds - ratio ( Evert , 2004 ) .
It was shown in ( Laroche and Langlais , 2010 ) that the log-odds - ratio in combination with the cosine-similarity performs superior to several other methods like PMI 5 and LLR 6 .
For comparing two context vectors we use the cosine similarity .
To transform the Japanese and English context vectors into the same vector space , we use a bilingual dictionary with around 1.6 million entries .
7
To express all context vectors in the same vector space , we map the context vectors in English to context vectors in Japanese .
8 First , for all the words which are listed in the bilingual dictionary we calculate word translation probabilities .
These translation probabilities are calculated using the EM - algorithm described in ( Koehn and Knight , 2000 ) .
We then create a translation matrix T which contains in each column the translation probabilities for a word in English into any word in Japanese .
Each context vector in English is then mapped into Japanese using the linear transformation described by the translation matrix T .
For word x with context vector x in English , let x ? be its context vector after transformation into Japanese , i.e. x ? = T ? x .
The gold-standard was created by considering all nouns in the Japanese and English WordNet where synsets are aligned cross-lingually .
This way we were able to create a gold-standard with 215 Japanese nouns , and their respective English translations that occur in our comparable corpora .
9 Note that the cross-lingual alignment is needed only for evaluation .
For evaluation , we consider only the translations that occur in the corresponding English synset as correct .
Because all methods return a ranked list of translation candidates , the accuracy is measured using the rank of the translation listed in the gold-standard .
The inverse rank is the sum of the inverse ranks of each translation in the gold-standard .
In Table 1 , the first row shows the results when using no smoothing .
Next , we smooth the query 's context vector by using Equation ( 1 ) and ( 2 ) .
The set of neighbors K is defined as the k-terms in the source language that are closest to the query word , with respect to the cosine similarity ( sim ) .
The weight w x for a neighbor x is set to w x = 10 0.13?sim ( x , q ) in accordance to ( Pekar et al. , 2006 ) .
For k we tried values between 1 and 100 , and got the best inverse rank when using k=19 .
The resulting method ( Topk Smoothing ) performs consistently better than the method using no smoothing , see Table 1 , second row .
Next , instead of smoothing the query word with its nearest neighbors , we use as the set K the set of synonyms of the query word ( Syn Smoothing ) .
Table 1 shows a clear improvement over the method that uses nearest neighbor-smoothing .
This confirms our claim that using synonyms for smoothing can lead to better translation accuracy than using nearest neighbors .
In the last row of Table 1 , we compare our proposed method to combine context vectors of synonyms ( Syn Mises - Combination ) , with the pre-vious method ( Syn Smoothing ) .
A pair-wise comparison of our proposed method with Syn Smoothing shows a statistically significant improvement ( p < 0.01 ) .
10 Finally , we also show the result when simply adding each synonym vector to the query 's context vector to form a new combined context vector ( Syn Sum ) .
11
Even though , this approach does not use the frequency information of a word , it performs better than Syn Smoothing .
We suppose that this is due to the fact that it actually indirectly uses frequency information , since the log-odds - ratio tends to be higher for words which occur with high frequency in the corpus .
Discussion
We first discuss an example where the query terms are ? ( cruise ) and ? ( cruise ) .
Both words can have the same meaning .
The resulting translation candidates suggested by the baseline methods and the proposed method is shown in Table 2 .
Using no smoothing , the baseline method outputs the correct translation for ? ( cruise ) and ? ? ( cruise ) at rank 10 and 15 , respectively .
When combining both queries to form one context vector our proposed method ( Syn Mises - Combination ) retrieves the correct translation at rank 2 .
Note that we considered all nouns that occur three or more times as possible translation candidates .
As can be seen in Table 2 , this also includes spelling mistakes like " sevice " and " infromation " .
1 ) with q corresponding to the context vector of the query word , and K contains only the context vector of the term that is used for smoothing .
Finally , we note that some terms in our test set are ambiguous , and the ambiguity is not resolved by using the synonyms of only one synset .
For example , the term ? ( steering , guidance ) belongs to the synset " steering , guidance " which includes the terms ? ( steering , guidance ) and ? ( guidance ) , ? ( guidance ) .
Despite this conflation of senses in one synset , our proposed method can improve the finding of ( one ) correct translation .
The baseline system using only ?
( steering , guidance ) outputs the correct translation " steering " at rank 4 , whereas our method using all four terms outputs it at rank 2 .
Conclusions
We proposed a new method for translation acquisition which uses a set of synonyms to acquire translations .
Our approach combines the query term 's context vector with all the context vectors of its synonyms .
In order to combine the vectors we use a weighted average of each context vector , where the weights are determined by a term 's occurrence frequency .
Our experiments , using the Japanese and English WordNet ( Bond et al. , 2009 ; Fellbaum , 1998 ) , show that our proposed method can increase the translation accuracy , when compared to using only a single query term , or smoothing with nearest neighbours .
Our results suggest that instead of directly searching for a translation , it is worth first looking for synonyms , for example by considering spelling variations or monolingual resources .
Table 2 : 2 Shows the results for ? and ? which both have the same meaning " cruise " .
The third column shows part of the ranked translation candidates separated by comma .
The last column shows the rank of the correct translation " cruise " .
Syn Smoothing uses Equation ( Method Query Output Rank No Smoothing ? ... , affinity , delco , cruise , sevice , sentrum , ... 10 No Smoothing ? ... , denali , attendant , cruise , abs , tactic , ... 15 Top-k Smoothing ? pillar , multi , cruise , star , affinity , ... 3 Top-k Smoothing ? ... , burnout , dipstick , cruise , infromation , speed , ... 8 Syn Smoothing ? smoothed with ? ... , affinity , delco , cruise , sevice , sentrum , ... 10 Syn Smoothing ? smoothed with ? ... , alldata , mode , cruise , expectancy , mph , ... 8 Syn Sum ? , ? assumption , level , cruise , reimbursment , infromation , ... 3 Syn Mises -Combination ? , ? pillar , cruise , assumption , level , speed , ... 2
Monolingual thesauri are , arguably , easier to construct than bilingual dictionaries .
In their experiments , they require that a query word has at least 100 associated terms .
http://www.mlit.go.jp/jidosha/carinf/rcl/defects.html
4 http://www-odi.nhtsa.dot.gov/downloads/index.cfm
5 point-wise mutual information 6 log-likelihood ratio7
The bilingual dictionary was developed in the course of our Japanese language processing efforts described in ( Sato et al. , 2003 ) .8
Alternatively , we could , for example , use canonical correlation analysis to match the vectors to a common latent vector space , like described in ( Haghighi et al. , 2008 ) .
The resulting synsets in Japanese and English , contain in average 2.2 and 2.8 words , respectively .
The ambiguity of a query term in our gold -standard is low , since , in average , a query term belongs to only 1.2 different synsets .
We use the sign-test ( Wilcox , 2009 ) to test the hypothesis that the proposed method ranks higher than the baseline .
11
No normalization is performed before adding the context vectors .
