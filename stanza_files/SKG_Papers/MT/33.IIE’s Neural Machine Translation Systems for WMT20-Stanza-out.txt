title
IIE 's Neural Machine Translation Systems for WMT20
abstract
In this paper we introduce the systems IIE submitted for the WMT20 shared task on German ?
French news translation .
Our systems are based on the Transformer architecture with some effective improvements .
Multiscale collaborative deep architecture , data selection , back translation , knowledge distillation , domain adaptation , model ensemble and re-ranking are employed and proven effective in our experiments .
Our German ?
French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French ?
German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .
Introduction
We participate in the WMT20 shared news translation task in one language pair and two language directions , German ?
French and French ?
German .
Our methods are based on techniques and approaches used in submissions from past years ( Deng et al. , 2018 ; Ng et al. , 2019 ; Sun et al. , 2019 ; Li et al. , 2019 ; Xia et al. , 2019 ) , including the use of subword models ( Sennrich et al. , 2016 ) , iterative back - translation , knowledge distillation , model ensembling and several techniques we proposed recently ( Wei et al. , 2020 b , a ) .
For our submissions of two language directions , we adopt the deep transformer architectures ( 48layer ) based on multiscale collaboration mechanism ( Wei et al. , 2020 b ) as our baseline , which outperformed the standard Transformer - Big as well as shallower models significantly in terms of translation quality .
We also use an iterative back -translation approach with the controllable sampling to extend the back translation method by jointly training source - to- target and target- to - source NMT models .
Moreover , the knowledge distillation ( Freitag et al. , 2017 ) is employed to leverage the source-side monolingual data .
For our final models , we apply a domainspecific fine-tuning process and model ensembling , and decode using noisy channel model re-ranking .
The paper is structured as follows :
Section 2 describes the techniques we used , then section 3 shows the experimental settings and results .
Finally , we conclude our work in Section 4 .
Our Techniques
Multiscale Collaborative Deep Models
The structure of NMT models has evolved quickly , such as RNN - based ( Wu et al. , 2016 ) , CNN - based ( Gehring et al. , 2017 ) and attentionbased ( Vaswani et al. , 2017 ) systems .
Deep neural networks have revolutionized the state - of - the - art in various communities , from computer vision to natural language processing .
We adopt the deep transformer model proposed by our work ( Wei et al. , 2020 b ) .
Instead of relying on the whole encoder stack to directly learn a desired representation , we let each encoder block learn a fine- grained representation and enhance it by encoding spatial dependencies using a bottom - up network .
For coordination , we attend each block of the decoder to both the corresponding representation of the encoder and the contextual representation with spatial dependencies .
This not only shortens the path of error propagation , but also helps to prevent the lower level information from being forgotten or diluted .
In this section we describe the details ( as illustrated in figure 1 ) of our deep architectures as below : Block - Scale Collaboration .
An intuitive extension of naive stacking of layers is to group few stacked layers into a block .
We suppose that the encoder and decoder of our model have the same number of blocks ( i.e. , N ) .
Each block of the encoder has M n ( n ? { 1 , 2 , ... , N } ) identical layers , while each decoder block contains one layer .
Thus , we can adjust the value of each M n flexibly to increase the depth of the encoder .
Formally , for the n-th block of the encoder : B n e = BLOCK e ( B n?1 e ) , ( 1 ) where BLOCK e ( ? ) is the block function , in which the layer function F ( ? ) is iterated M n times , i.e .
B n e = H n, Mn e , H n,l e = F( H n,l? 1 e ; ? n,l e ) + H n,l?1 e , H n,0 e = B n?1 e , ( 2 ) where l ? { 1 , 2 , ... , M n } , H n,l e and ?
n,l e are the representation and parameters of the l-th layer in the n-th block , respectively .
The decoder works in a similar way but the layer function G ( ? ) is iterated only once in each block , B n d = BLOCK d ( B n?1 d , B n e ) = G ( B n?1 d , B n e ; ? n d ) + B n?1 d . ( 3 ) Each block of the decoder attends to the corresponding encoder block .
Contextual Collaboration .
To model long-term spatial dependencies and reuse global representations , we define a GRU cell Q( c , x ) , which maps a hidden state c and an additional input x into a new hidden state : C n = Q( C n?1 , B n e ) , n ? [ 1 , N ] C 0 = E e , ( 4 ) where E e is the embedding matrix of the source input x .
The new state C n can be fused with each layer of the subsequent blocks in both the encoder and the decoder .
Formally , B n e in Eq. ( 1 ) can be re-calculated in the following way : ( 5 ) B n e = H n, Mn Similarly , for decoder , we have B n d = BLOCK d ( B n?1 d , B n e ) = G ( B n?1 d , B n e , C n ; ? n d ) + B n?1 d . ( 6 )
Back -Translation with Controllable Sampling Back-translation ( BT ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system .
Back - translation first trains an intermediate targetto-source system that is used to translate monolingual target data into additional synthetic parallel data .
This data is used in conjunction with human translated bitext data to train the desired source-totarget system .
In our work , we use an iterative back - translation approach to jointly train source - to- target and targetto-source NMT models .
The process can be summarized as below : ? step 1 : we train both a source- to- target model ( M 0 x?y ) and a target - to- source model ( M 0 y?x ) using the human translated data .
? step 2 : we use M t x?y to translate source-side monolingual data to target language , and use M t y?x to translate target - side monolingual data to source language , where t starts from 0 . ? step 3 : we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively .
?
Repeat steps 2 - 3 until the models converge .
In practice , we repeat 3 times for steps 2 - 3 .
We apply the controllable sampling strategy ( Wei et al. , 2020a ) to synthesize reasonable sentences which are at both high quality and diversity .
Knowledge Distillation and Ensemble
The early adoption of knowledge distillation ( KD ) ( Kim and Rush , 2016 ) is for model compression .
We use the same method as in Sun et al . ( 2019 ) that adopts hybrid heterogeneous teacher : base transformer , deep transformer , big transformer and RNMT + .
For each individual model , we use the other two models as the teacher model to further improve the performance .
In addition , model ensemble is also used to boost the performance by combining the predictions of above four models at each decoding step .
Domain-specific Fine-tuning Fine-tuning with domain-specific data is a common and effective method to improve translation quality for a downstream task .
After completing training on the bitext and back - translated data , we train for an additional epoch on a smaller in-domain corpus .
We first select 100K sentence - pairs from the bilingual as well as pseudo-generated data according to the filter method in Deng et al . ( 2018 ) and continue to train the model on the filtered data .
Reranking N - best reranking is a method of improving translation quality by scoring and selecting a candidate hypothesis from a list of n-best hypotheses generated by a source - to- target model .
For our submissions , we rerank the n-best hypotheses using two aspects as follows : log p( y|x ) + ? 1 log p( x|y ) + ? 2 log p ( y ) ( 7 )
The weights ?
1 and ?
2 are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance .
System Overview
We submit constrained systems to both German to French and French to German translations , with the same techniques .
Dataset
We use all available bilingual datasets and select 10 M bilingual data from WMT '20 corpora using the script filter interactive .py 1 . We share a vocabulary for the two languages and apply BPE for word segmentation with 32 K merge operations .
For monolingual data , we use 18 M German sentences and 18M French sentences from Newscrawl , and pre-process them in the same way as bilingual data .
We split 9 k sentences from the " dev08 - 14 " as the validation set and use newstest 2019 as the test set .
Model Configuration
We use the PyTorch implementation of Transformer 2 .
We choose the Transformer base setting , in which the encoder and decoder are of 48 and 6 layers , respectively .
The dropout rate is fixed as 0.1 .
We set the batch size as 4096 and the parameter -- update-freq as 16 .
Results Results and ablations for De?Fr Fr?
De are shown in Table 1 and 2 , respectively .
We report case-sensitive SacreBLEU scores using Sacre-BLEU ( Post , 2018 ) 3 , using international tokenization for German ?
French .
German ?
French For De?Fr , iterative BT improves our baseline performance on newstest 2019 2 https://github.com/pytorch/fairseq
3 SacreBLEU signatures : BLEU +case.mixed+lang .
by about 2.5 BLEU .
The addition of KD and model ensemble improves single model performance by 0.8 BLEU , but combining this with fine-tuning and reranking gives us a total of 2 BLEU .
Our final submission for WMT20 achieves 35.0 BLEU points for German ?
French translation ( ranked in the second place ) .
French ?
German For Fr? De , we see similar improvements with iterative BT by about 2.3 BLEU .
KD , ensembling , and fine-tuning add an additional 1.4 BLEU , with reranking contributing 0.9 BLEU .
Our final submission for WMT20 achieves 36.6 BLEU points for French ?
German translation ( ranked in the fourth among anonymous submissions ) .
Conclusion
This paper describes CAS IIE 's submission to the WMT20 German ?
French news translation task .
We investigate extremely deep models ( with 48 layers ) and exploit effective strategies to better utilize parallel data as well as monolingual data .
Finally , our German ?
French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French ?
German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .
Figure 1 : 1 Figure 1 : Illustration of Multiscale Collaborative Deep NMT Model .
N is the number of encoder and decoder blocks .
The n-th block of the encoder consists of M n layers , while each decoder block only contains one layer .
