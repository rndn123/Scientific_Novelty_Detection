title
Effective Domain Mixing for Neural Machine Translation
abstract
Neural Machine Translation ( NMT ) models are often trained on heterogeneous mixtures of domains , from news to parliamentary proceedings , each with unique distributions and language .
In this work we show that training NMT systems on naively mixed data can degrade performance versus models fit to each constituent domain .
We demonstrate that this problem can be circumvented , and propose three models that do so by jointly learning domain discrimination and translation .
We demonstrate the efficacy of these techniques by merging pairs of domains in three languages : Chinese , French , and Japanese .
After training on composite data , each approach outperforms its domain-specific counterparts , with a model based on a discriminator network doing so most reliably .
We obtain consistent performance improvements and an average increase of 1.1 BLEU .
Introduction Neural Machine Translation ( NMT ) ( Kalchbrenner and Blunsom , 2013 ; Sutskever et al. , 2014 ; is an end-to - end approach for automated translation .
NMT has shown impressive results ( Bahdanau et al. , 2015 ; Luong et al. , 2015a ; often surpassing those of phrase - based systems while addressing shortcomings such as the need for hand -engineered features .
In many translation settings ( e.g. web translation , assistant translators ) , input may come from more than one domain .
Each domain has unique properties that could confound models not explicitly fitted to it .
Thus , an important problem is to effectively mix a diversity of training data in a multi-domain setting .
Our problem space is as follows : how can we train a translation model on multi-domain data to improve test -time performance in each constituent domain ?
This setting differs from the majority of work in domain adaptation , which explores how models trained on some source domain can be effectively applied to outside target domains .
This setting is important , because previous research has shown that both standard NMT and adaptation methods degrade performance on the original source domain ( s ) ( Farajian et al. , 2017 ; Haddow and Koehn , 2012 ) .
We seek to prove that this problem can be overcome , and hypothesize that leveraging the heterogeneity of composite data rather than dampening it will allow us to do so .
To this extent , we propose three new models for multi-domain machine translation .
These models are based on discriminator networks , adversarial learning , and target -side domain tokens .
We evaluate on pairs of linguistically disparate corpora in three translation tasks ( EN - JA , EN - ZH , EN - FR ) , and observe that unlike naively training on mixed data ( as per current best practices ) , the proposed techniques consistently improve translation quality in each individual setting .
The most significant of these tasks is EN - JA , where we obtain state - of - the - art performance in the process of examining the ASPEC corpus ( Nakazawa et al. , 2016 ) of scientific papers and Sub-Crawl , a new corpus based on an anonymous manuscript ( Anonymous , 2017 ) .
In summary , our contributions are as follows : ?
We show that mixing data from heterogenous domains leads to suboptimal results compared to the single-domain setting , and that the more distant these domains are , the more their merger degrades downstream translation quality .
?
We demonstrate that this problem can be circumvented and propose novel , generalpurpose techniques that do so .
Neural Machine Translation Neural machine translation ( Sutskever et al. , 2014 ) directly models the conditional log probability log p( y|x ) of producing some translation y = y 1 , ... , y m of a source sentence x = x 1 , ... , x n .
It models this probability through the encoder-decoder framework .
In this approach , an encoder network encodes the source into a series of vector representations H = h 1 , ... , h n .
The decoder network uses this encoding to generate a translation one target token at a time .
At each step , the decoder casts an attentional distribution over source encodings ( Luong et al. , 2015 b ; Bahdanau et al. , 2014 ) .
This allows the model to focus on parts of the input before producing each translated token .
In this way the decoder is decomposing the conditional log probability into log p( y | x ) = m ?
t=1 log p(y t |y <t , H ) ( 1 )
In practice , stacked networks with recurrent Long Short - Term Memory ( LSTM ) units are used for both the encoder and decoder .
Such units can effectively distill structure from sequential data ( Elman , 1990 ) .
The cross-entropy training objective in NMT is formulated as , L gen = ? ( x, y ) ? D ? log p( y | x ) ( 2 ) Where D is a set of ( source , target ) sequence pairs ( x , y ) .
Models
We now describe three models we are proposing that leverage the diversity of information in heterogeneous corpora .
They are summarized in Figure 1 .
We assume dataset D consists of source sequences X , target sequences Y and domain class labels D that are only known at training time .
Discriminative Mixing
In the Discriminative Mixing approach , we add a discriminator network on top of the source encoder that takes a single vector encoding of the source c as input .
This network maximizes P ( d|H ) , the predicted probability of the correct domain class label d conditioned on the hidden states of the encoder H .
It does so by minimizing the negative cross-entropy loss L disc = ? log p( d |H ) .
In other words , the discriminator uses the encoded representation of the source sequence to predict the correct domain .
Intuitively , this forces the encoder to encode domain-related information into the features it generates .
We hypothesize that this information will be useful during the decoding process .
The encoder can employ an arbitrary mechanism to distill the source into a single-vector representation c .
In this work , we use an attention mechanism over the encoder states H , followed by a fully connected layer .
We set c to be the attention context , and calculate it according to Bahdanau et al . ( 2015 ) : c = ? j a j h j a = softmax ( ? ) ? i = v T a tanh ( W a h i )
The discriminator can be an arbitrary neural network .
For this work , we fed c into a fully connected layer with a tanh nonlinearity , then passed the result through a softmax to obtain probabilities for each domain class label .
The discriminator is optimized jointly with the rest of the Sequence- to - Sequence network .
If L gen is the standard sequence generator loss described in Section 2 , then the final loss we are optimizing is the sum of the generator and discriminator loss L = L gen + L disc .
Adversarial Discriminative Mixing
We also experiment with an adversarial approach to domain mixing .
This approach is similar to that of 3.1 , except that when backpropagating from the discriminator network to the encoder , we reverse the gradients by multiplying them by ?1 .
Though the discriminator is still using ?L disc to update its parameters , with the inclusion of the reversal layer , we are implicitly directing the encoder to optimize with ?L disc .
This has the opposite effect of what we described above .
The discriminator still learns to distinguish between domains , but the encoder is forced to compute domaininvariant representations that are not useful to the discriminator .
We hope that such representations lead to better generalization across domains .
Note the connections between this technique and that of the Generative Adversarial Network ( GAN ) paradigm ( Goodfellow et al. , 2014 ) .
GANs optimize two networks with two objective functions ( one being the negation of the other ) and periodically freeze the parameters of each network during training .
We are training a single network without freezing any of its components .
Furthermore , we reverse gradients in lieu of explicitly defining a second , negated loss function .
Last , the adversarial parts of this model are trained jointly with translation in a multitask setting .
Note also that the representations computed by this model are likely to be applicable to unseen , outside domains .
However , this setting is outside the scope of this paper and we leave its exploration to future work .
For our setting , we hypothesize that the domain-agnostic encodings encouraged by the discriminator may yield improvements in mixed - domain settings as well .
Target Token Mixing
A simpler alternative to adding a discriminator network is to prepend a domain token to the target sequence .
Such a technique can be readily incorporated into any existing NMT pipeline and does not require changes to the model .
In particular , we add a single special vocabulary word such as " domain=subtitles " , per domain and prepend this token to each target sequence therein .
The decoder must learn , similar to the more complex discriminator above , to predict the correct domain token based on the source representation at the first step of decoding .
We hypothesize that this technique has a similar regularizing effect as adding a discriminator network .
During inference , we remove the first predicted token corresponding to the domain .
The advantage of this approach verses the similar techniques discussed in related work ( Section 5 ) is that in our proposed method , the model must learn to predict the domain based on the source sequence alone .
It does not need to know the domain a-priori .
Experiments
Datasets
For the Japanese translation task we evaluate our domain mixing techniques on the standard ASPEC corpus ( Nakazawa et al. , 2016 ) consisting of 3 M scientific document sentence pairs , and the SubCrawl corpus , consisting of 3.2 M colloquial sentence pairs harvested from freely available subtitle repositories on the World Wide Web .
We use standard train / dev/ test splits ( 3 M , 1.8 k , and 1.8 k examples , respectively ) and preprocess the data using subword units 1 ( Sennrich et al. , 2015 ) to learn a shared English - Japanese vocabulary of size 32,000 .
To allow for fair comparisons , we use the same vocabulary and sentence segmentation for all experiments , including singledomain models .
To prove its generality , we also evaluate our techniques on a small set of about 200k / 1k/ 1 k training / dev/test examples of English Chinese ( EN - ZH ) and English - French ( EN - FR ) language pairs .
For EN - ZH , we use a news commentary corpus from WMT '17 2 and a 2012 database dump of TED talk subtitles ( Tiedemann , 2012 ) .
For EN - FR , we use professional translations of European Parliament Proceedings ( Koehn , 2005 ) and a 2016 dump of the OpenSubtitles database ( Lison and Tiedemann , 2016 ) .
The premise of evaluating on mixed - domain data is that the domains undergoing mixing are in fact disparate .
We need to quantifiably measure the disparity therein to obtain fair , valid , and explainable results .
Thus , we measured the distances between the domains of each language pair with A-distance , an important part of the upper generalization bounds for domain adaptation ( Ben - David et al. , 2007 ) .
Due to the intractability of computing A-distances , we instead compute a proxy for A-distance , dA , which is given theoretical justification in Ben-David et al . ( 2007 ) and used to measure domain distance in Gani et al .
( 2015 ) ; Glorot et al. ( 2011 ) .
The proxy A-distance is obtained by measuring the generalization error ? of a linear bag-of- words SVM classifier trained to discriminate between the two domains , and setting dA = 2 ( 1 ? 2 ? ) .
Note that by nature of its formulation , dA is only useful in comparative settings , and means little in isolation ( Ben - David et al. , 2007 ) .
However , it has a minimum value of 1 , implying exact domain match , and a maximum of 2 , implying that domains are polar opposites .
Experimental Protocol
All models are implemented using the Tensorflow framework and based on the Sequenceto- Sequence implementation of Britz et al . ( 2017 ) 3 . We use a 4 - layer bidirectional LSTM encoder with 512 units , and a 4 - layer LSTM decoder .
Recall from Section 3 that we use Bahdanau-style attention Bahdanau et al . ( 2015 ) . Dropout of 0.2 ( 0.8 keep probability ) is applied to the input of each cell .
We optimize using Adam and a learning rate of 0.0001 ( Kingma and Ba , 2014 ; Abadi et al. , 2016 ) .
Each model is trained on 8 Nvidia K40 m GPUs with a batch size of 128 .
The combined Japanese dataset took approximately a week to reach convergence .
During training , we save model checkpoints every hour and choose the best one using the BLEU score on the validation set .
To calculate BLEU scores for the EN - JA task , we follow the instruction from WAT 4 and use the KyTea tokenizer ( Neubig et al. , 2011 ) .
For the EN - FR and EN - ZH tasks , we follow the WMT ' 16 guidlines and tokenize with the Moses tokenizer .
perl script ( Koehn et al. , 2007 ) .
Results
The results of our proxy - A distance experiment are given in Table 1 . dA is a purely comparative metric that has little meaning in isolation ( Ben - David et al. , 2007 ) , so it is evident that the EN - JA and EN - ZH domains are more disparate , while the EN - FR domains are more similar .
To understand the interactions between these models and mixed - domain data , we train and test on ASPEC , SubCrawl , and their concatenation .
We do the same for the French and Chinese baselines .
In general , our results support the hypothesis that the naive concatenation of data from disparate domains can degrade in-domain translation quality ( Table 2 ) .
In both the EN - JA and EN - FR settings , the domains undergoing mixing are disparate enough to degrade performance when mixed , and the proposed techniques recover some of this performance drop .
In the EN - ZH setting , we observe that even when similar domains are mixed performance can drop .
Notably , in this setting , the proposed techniques successfully improve performance over single-domain training .
For a more detailed perspective on result , Figure 2a depicts the mixeddomain / individual - domain performance differential as a function of domain distance .
The two share a negative association , suggesting that the most distant two domains are , the more their merger degrades performance .
This degradation is particularly strong in Japanese due the vast structural differences between formal and casual language .
The vocabularies , conjugational patterns , and word attachments all follow different rules in this case ( Hori , 1986 ) .
We then trained and tested our proposed methods on the same mixed data ( Table 2 ) .
Our results generally agree with the hypothesis that the diversity of information in heterogeneous data can be leveraged to improve in-domain translation .
Overall , we find that all of the proposed methods outperform their respective baselines in most settings , but that the discriminator appears the most reliable .
It bested its counterparts in 4 of 6 trials , and was the only approach that outperformed both individually fit and naively mixed baselines in every trial .
Figure 2c depicts the dynamics of the discriminator approach .
More specifically , this figure shows the discriminator / naive-mixing performance differential as a function of domain distance .
The two share a positive association , suggesting that the more distant two domains are , the more the discriminator helps performance .
This may be because it is easier to classify distant domains , so the discriminator can fit the data better and its gradients encourage the upstream encoder to include more useful domain-related structure .
The adversarial discriminator architecture yielded improvements on the small datasets , but underperformed on EN - JA .
It is possible that the grammatical differences inherent to casual and polite domains are such that semantic information was lost in the process of forcing their encoded distributions to match .
Additionally , adversarial objective functions are notoriously difficult to optimize on , and this model was prone to falling into poor local optimum during training .
The simpler target token approach also yields improvement over the baselines , just barely surpassing that of the Discriminator for ASPEC .
This approach has the practical benefit of requiring no architectural changes to an off-the-shelf NMT system .
Our EN - FR results are particularly interesting .
Though the data seem like they should come from sufficiently distant domains ( parliament proceedings and subtitles ) , the domains are actually quite close according to dA ( Table 1 ) .
Since these domains are so close , their merger is able to improve baseline performance .
Thus , if the source and target domain are sufficiently close , then their merger does indeed help .
Next , we investigated the optimization dynamics of these models by examining their learning curves .
Curves for the baselines and discriminative models trained on EN - JA data are depicted in Figure 3a .
Single-domain training clearly outperforms mixed training , and it appears that adding a discriminative strategy provides additional gains .
From Figure 3 b we can see that the discriminator ap- proach ( not reversing gradients ) , learns to fit the domain distribution quickly , implying that the Japanese domains were in fact quite distant and easily classifiable .
Related Work
Our work builds on a recent literature on domain adaptation strategies in Neural Machine Translation .
Prior work in this space has proposed two general categories of methods .
The first proposed method is to take models trained on the source domain and finetune on target- domain data .
Luong and Manning ( 2015 ) ; Zoph et al. ( 2016 ) explores how to improve transfer learning for a low-resource language pair by finetuning only parts of the network .
Chu et al. ( 2017 ) empirically evaluate domain adaptation methods and propose mixing source and target domain data during finetuning .
Freitag and Al- Onaizan ( 2016 ) explored finetuning using only a small subset of target domain data .
Note that we did not compare directly against these techniques because they are intended to transfer knowledge to a new domain and perform well on only the target domain .
We are concerned with multi-domain settings , where performance on all constituent domains is important .
A second strain of " multi-domain " thought in NMT involves appending a domain indicator token to each source sequence ( Kobus et al. , 2016 ) .
Similarly , use a token for cross-lingual translation instead of domain identification .
This idea was further refined by Chu et al . ( 2017 ) , who integrated source-tokenization into the domain finetuning paradigm .
While it requires no changes to the NMT architecture , these approaches are inherently limited because they stipulate that domain information for unseen test examples be known .
For example , if using a trained model to translate user-generated sentences , we do not know the domain a-priori , and this approach cannot be used .
Apart from the recent progress in domain adaptation for NMT , we draw on work that transfers knowledge between domains in semisupervised settings .
Our strongest influence is adversarial domain adaptation ( Ganin et al. , 2015 ) , where feature distributions in the source and target domains are matched with a Domain-Adversarial Neural Network ( DANN ) .
Another approach to this problem is that of Long et al . ( 2015 ) , which measures and minimizes the distance between domain distribution means before training , thereby negating any unique properties .
There is some overlap between past research in multi-domain statistical machine translation ( SMT ) and the ideas of this paper .
( Farajian et al. , 2017 ) compared the efficacy of phrase - based SMT and NMT on multipledomain data , observing similar performance degradations as us in mixed - domain settings .
However , that study did not seek to understand the issue and offered no explanation , analysis , or solution to the problem .
Another line of work merged data by only selecting examples with a propensity for relevance in a multi-domain setting ( Mandal et al. , 2008 ; Axelrod et al. , 2011 ) .
In a strategy that echos NMT fine-tuning , Pecina et al . ( 2012 ) used a variety of in-domain development sets to tune hyperparameters to a generalized setting .
Similar to our domain discriminator network , Clark et al . ( 2012 ) crafted domain-specific features that are used by the decoder .
However , some of these systems ' features are downstream of binary indicators for domain identity .
This approach , then , faces the same inherent limitations as source-tokenization : domain knowledge is required for inference .
Furthermore , the domain features of this system are integral to the decoding process , while our discriminator network is an independent module that can be detached during inference .
Conclusion
We presented three novel models for applying Neural Machine Translation to multidomain settings , and demonstrated their efficacy across six domains in three language pairs , and in the process achieved a new stateof - the - art in EN - JA translation .
Unlike the naive combining of training data , these models improve their translational ability on each constituent domain .
Furthermore , these models are the first of their kind to not require knowledge of each example 's domain at inference time .
All the proposed approaches outperform the naive combining of training data , so we advise practitioners to implement whichever most easily fits into their preexisting pipelines , but an approach based on a discriminator network offered the most reliable results .
In future work we hope to explore the dynamics of adversarial discriminative training objectives , which force the model to learn domain-agnostic features , in the related problem of adaptation to unseen test-time domains .
Figure 1 : 1 Figure 1 : The novel mixing paradigms under consideration .
Discriminative mixing ( A ) , adversarial discriminative mixing ( B ) , and target - side token mixing ( C ) are depicted .
