title
Additive Neural Networks for Statistical Machine Translation
abstract
Most statistical machine translation ( SMT ) systems are modeled using a loglinear framework .
Although the log-linear model achieves success in SMT , it still suffers from some limitations : ( 1 ) the features are required to be linear with respect to the model itself ; ( 2 ) features cannot be further interpreted to reach their potential .
A neural network is a reasonable method to address these pitfalls .
However , modeling SMT with a neural network is not trivial , especially when taking the decoding efficiency into consideration .
In this paper , we propose a variant of a neural network , i.e. additive neural networks , for SMT to go beyond the log-linear translation model .
In addition , word embedding is employed as the input to the neural network , which encodes each word as a feature vector .
Our model outperforms the log-linear translation models with / without embedding features on Chinese-to- English and Japanese - to - English translation tasks .
Introduction Recently , great progress has been achieved in SMT , especially since Och and Ney ( 2002 ) proposed the log-linear model : almost all the stateof - the- art SMT systems are based on the log-linear model .
Its most important advantage is that arbitrary features can be added to the model .
Thus , it casts complex translation between a pair of languages as feature engineering , which facilitates research and development for SMT .
Regardless of how successful the log-linear model is in SMT , it still has some shortcomings .
On the one hand , features are required to be linear with respect to the objective of the translation model ( Nguyen et al. , 2007 ) , but it is not guaranteed that the potential features be linear with the model .
This induces modeling inadequacy ( Duh and Kirchhoff , 2008 ) , in which the translation performance may not improve , or may even decrease , after one integrates additional features into the model .
On the other hand , it cannot deeply interpret its surface features , and thus can not efficiently develop the potential of these features .
What may happen is that a feature p does initially not improve the translation performance , but after a nonlinear operation , e.g. log ( p ) , it does .
The reason is not because this feature is useless but the model does not efficiently interpret and represent it .
Situations such as this confuse explanations for feature designing , since it is unclear whether such a feature contributes to a translation or not .
A neural network ( Bishop , 1995 ) is a reasonable method to overcome the above shortcomings .
However , it should take constraints , e.g. the decoding efficiency , into account in SMT .
Decoding in SMT is considered as the expansion of translation states and it is handled by a heuristic search ( Koehn , 2004 a ) .
In the search procedure , frequent computation of the model score is needed for the search heuristic function , which will be challenged by the decoding efficiency for the neural network based translation model .
Further , decoding with non-local ( or state- dependent ) features , such as a language model , is also a problem .
Actually , even for the ( log - ) linear model , efficient decoding with the language model is not trivial ( Chiang , 2007 ) .
In this paper , we propose a variant of neural networks , i.e. additive neural networks ( see Section 3 for details ) , for SMT .
It consists of two components : a linear component which captures nonlocal ( or state dependent ) features and a non-linear component ( i.e. , neural nework ) which encodes lo - X te X ?} ?\
X X friendly cooperation over the last years Figure 1 : A bilingual tree with two synchronous rules , r 1 : X ? ?} ?\;friendly cooperation and r 2 : X ? te X ; X over the last years .
The inside rectangle denotes the partial derivation d 1 = {r 1 } with the partial translation e 1 =" friendly cooperation " , and the outside rectangle denotes the derivation d 2 = {r 1 , r 2 } with the translation e 2 =" friendly cooperation over the last years " .
cal ( or state independent ) features .
Compared with the log-linear model , it has more powerful expressive abilities and can deeply interpret and represent features with hidden units in neural networks .
Moreover , our method is simple to implement and its decoding efficiency is comparable to that of the log-linear model .
We also integrate word embedding into the model by representing each word as a feature vector ( Collobert and Weston , 2008 ) .
Because of the thousands of parameters and the non-convex objective in our model , efficient training is not simple .
We propose an efficient training methodology : we apply the mini-batch conjugate sub-gradient algorithm ( Le et al. , 2011 ) to accelerate the training ; we also propose pre-training and post-training methods to avoid poor local minima .
The biggest contribution of this paper is that it goes beyond the log-linear model and proposes a non-linear translation model instead of re-ranking model ( Duh and Kirchhoff , 2008 ; Sokolov et al. , 2012 ) .
On both Chinese-to- English and Japanese- to - English translation tasks , experiment results show that our model can leverage the shortcomings suffered by the log-linear model , and thus achieves significant improvements over the log-linear based translation .
2 Log-linear Model , Revisited 2.1 Log-linear Translation Model Och and Ney ( 2002 ) proposed the log-linear translation model , which can be formalized as follows : P( e , d|f ; W ) = exp W ? h( f , e , d ) e ,d exp W ? h( f , e , d ) , ( 1 ) where f denotes the source sentence , and e( e ) denotes its translation candidate ; d( d ) is a derivation over the pair f , e , i.e. , a collection of synchronous rules for Hiero grammar ( Chiang , 2005 ) , or phrase pairs in Moses ( Koehn et al. , 2007 ) ; h( f , e , d ) = ( h 1 ( f , e , d ) , h 2 ( f , e , d ) , ? ? ? , h K ( f , e , d ) ) is a K-dimensional feature vector defined on the tuple f , e , d ; W = ( w 1 , w 2 , ? ? ? , w K ) is a Kdimensional weight vector of h , i.e. , the parameters of the model , and it can be tuned by the toolkit MERT ( Och , 2003 ) .
Different from Brown 's generative model ( Brown et al. , 1993 ) , the loglinear model does not assume strong independency holds , and allows arbitrary features to be integrated into the model easily .
In other words , it can transform complex language translation into feature engineering : it can achieve high translation performance if reasonable features are chosen and appropriate parameters are assigned for the weight vector .
Decoding By Search Given a source sentence f and a weight W , decoding finds the best translation candidate ?
via the programming problem : ( 2 ) Since the range of e , d is exponential with respect to the size of f , the exact decoding is intractable and an inexact strategy such as beam search is used instead in practice .
The idea of search for decoding can be shown in Figure 1 : it encodes each search state as a partial translation together with its derivation , e.g. e 1 , d 1 ; it consequently expands the states from the initial ( empty ) state to the end state e 2 , d 2 according to the translation rules r 1 and r 2 .
During the state expansion process , the score w i ? h i ( f , e , d ) for a partial translation is calculated repeatedly .
In the log-linear model , if h i ( f , e , d ) is a local feature , the calculation of its score w i ?
h i ( f , e , d ) has a substructure , and thus it can be calculated with dynamic programming which accelerates its decoding .
For the non-local features such as the language model , Chiang ( 2007 ) proposed a cube-pruning method for efficient decoding .
The main reason why cube-pruning works is that the translation model is linear and the model score for the language model is approximately monotonic ( Chiang , 2007 ) .
Additive Neural Networks
Motivation
Although the log-linear model has achieved great progress for SMT , it still suffers from some pitfalls : it requires features be linear with the model and it can not interpret and represent features deeply .
The neural network model is a reasonable method to overcome these pitfalls .
However , the neural network based machine translation is far from easy .
As mentioned in Section 2 , the decoding procedure performs an expansion of translation states .
Firstly , let us consider a simple case in neural network based translation where all the features in the translation model are independent of the translation state , i.e. all the components of the vector h(f , e , d ) are local features .
In this way , we can easily define the following translation model with a single - layer neural network : S( f , e , d ; W , M , B ) = W ? ?( M ? h( f , e , d ) + B ) , ( 3 ) where M ? R u?K is a matrix , and B ?
R u is a vector , i.e. bias ; ? is a single- layer neural network with u hidden units , i.e. an element wise sigmoid function sigmoid ( x ) = 1 / 1 + exp ( ? x ) .
For consistent description in the rest , we also represent Eq. ( 3 ) as a function of a feature vector h , i.e .
S( h ; W , M , B ) = W ? ?( M ? h + B ) .
Now let us consider the search procedure with the model in Eq. ( 3 ) using Figure 1 as our example .
Suppose the current translation state is encoded as e 1 , d 1 , which is expanded into e 2 , d 2 using the rule r 2 ( d 2 = d 1 ? {r 2 } ) .
Since h is state- independent , h( f , e 2 , d 2 ) = h( f , e 1 , d 1 ) + h( r 2 ) .
However , since S(f , e , d ; W , M , B ) is nondecomposable as a linear model , there is no substructure for calculating S(f , e 2 , d 2 ; W , M , B ) , and one has to re-calculate it via Eq. ( 3 ) even if the score of S(f , e 1 , d 1 ; M , B ) for its previous state e 1 , d 1 is available .
When the size of the parameter ( W , M , B ) is relatively large , it will be a challenge for the decoding efficiency .
In order to keep the substructure property , S( f , e 2 , d 2 ; W , M , B ) should be represented as F S(f , e 1 , d 1 ; W , M , B ) ; S ( h( r 2 ) ; M , B ) by a function F . For simplicity , we suppose that the additive property holds in F , and then we can obtain a new translation model via the following recursive equation : S( f , e 2 , d 2 ; W , M , B ) = S(f , e 1 , d 1 ; W , M , B ) + S h( r 2 ) ; W , M , B . ( 4 ) Since the above model is defined only on local features , it ignores the contributions from nonlocal features .
Actually , existing works empirically show that some non-local features , especially language model , contribute greatly to machine translation .
Scoring for non-local features such as a ngram language model is not easily done .
In loglinear translation model , Chiang ( 2007 ) proposed a cube-pruning method for scoring the language model .
The premise of cube-pruning is that the language model score is approximately monotonic ( Chiang , 2007 ) .
However , if scoring the language model with a neural network , this premise is difficult to hold .
Therefore , one of the solutions is to preserve a linear model for scoring the language model directly .
Definition
According to the above analysis , we propose a variant of a neural network model for machine translation , and we call it Additive Neural Networks or AdNN for short .
The AdNN model is a combination of a linear model and a neural network : non-local features , e.g. LM , are linearly modeled for the cubepruning strategy , and local features are modeled by the neural network for deep interpretation and representation .
Formally , the AdNN based translation model is discriminative but non-probabilistic , and it can be defined as follows : S( f , e , d ; ? ) = W ? h( f , e , d ) + r?d W ? ? M ? h ( r ) + B , ( 5 ) where h and h are feature vectors with dimension K and K respectively , and each component of h is a local feature which can be defined on a rule r : X ? ? , ? ; ? = ( W , W , M , B ) is the model parameters with M ? R u?K .
In this paper , we focus on a single - layer neural network for its simplicity , and one can similarly define ? as a multilayer neural network .
Again for the example shown in Figure 1 , the model score defined in Eq. ( 5 ) for the pair e 2 , d 2 can be represented as follows : S( f , e 2 , d 2 ; ? ) = W ? h( f , e 2 , d 2 ) +
W ? M ?h ( r 1 ) +B +W ? M ?h ( r 2 ) +B . Eq. ( 5 ) is similar to both additive models ( Buja et al. , 1989 ) and generalized additive neural networks ( Potts , 1999 ) : it consists of many additive terms , and each term is either a linear or a nonlinear ( a neural network ) model .
That is the reason why our model is called " additive neural networks " .
Of course , our model still has some differences from both of them .
Firstly , our model is decomposable with respect to rules instead of the component variables .
Secondly , some of its additive terms share the same parameters ( M , B ) .
There are also strong relationships between AdNN and the log-linear model .
If we consider the parameters ( M , B ) as constant and ?
M ? h ( r ) + B as a new feature vector , then AdNN is reduced to a log-linear model .
Since both ( M , B ) and ( W , W ) are parameters in AdNN , our model can jointly learn the feature ?
M ? h ( r ) + B and tune the weight ( W , W ) of the log-linear model together .
That is different from most works under the log-linear translation framework , which firstly learn features or sub-models and then tune the log-linear model including the learned features in two separate steps .
By joint training , AdNN can learn the features towards the translation evaluation metric , which is the main advantage of our model over the log-linear model .
In this paper , we apply our AdNN model to hierarchical phrase based translation , and it can be similarly applied to phrase - based or syntax - based translation .
Similar to Hiero ( Chiang , 2005 ) , the feature vector h in Eq. ( 5 ) includes 8 default features , which consist of translation probabilities , lexical translation probabilities , word penalty , glue rule penalty , synchronous rule penalty and language model .
These default features are included because they empirically perform well in the loglinear model .
For the local feature vector h in Eq ( 5 ) , we employ word embedding features as described in the following subsection .
Word Embedding features for AdNN
Word embedding can relax the sparsity introduced by the lexicalization in NLP , and it improves the systems for many tasks such as language model , named entity recognition , and parsing ( Collobert and Weston , 2008 ; Turian et al. , 2010 ; Collobert , 2011 ) .
Here , we propose embedding features for rules in SMT by combining word embeddings .
Firstly , we will define the embedding for the source side ? of a rule r : X ? ? , ? . Let V S be the vocabulary in the source language with size | V S | ; R n?|V S | be the word embedding matrix , each column of which is the word embedding ( ndimensional vector ) for the corresponding word in V S ; and maxSource be the maximal length of ? for all rules .
We further assume that the ? for all rules share the same length as maxSource ; otherwise , we add maxSource ? |? | words " N U LL " to the end of ? to obtain a new ?.
We define the embedding of ? as the concatenation of the word embedding of each word in ?.
In particular , for the non-terminal in ? , we define its word embedding as the vector whose components are 0.1 ; and we define the word embedding of " N U LL " as 0 .
Then , we similarly define the embedding for the target side of a rule , given an embedding matrix for the target vocabulary .
Finally , we define the embedding of a rule as the concatenation of the embedding of its source and target sides .
In this paper , we apply the word embedding matrices from the RNNLM toolkit ( Mikolov et al. , 2010 ) with the default settings : we train two RNN language models on the source and target sides of training corpus , respectively , and then we obtain two matrices as their by -products 1 . It would be potentially better to train the word embedding matrix from a much larger corpus as ( Collobert and Weston , 2008 ) , and we will leave this as a future task .
Decoding Substituting the P(e , d|f ; W ) in Eq. ( 2 ) with S(f , e , d ; ? ) in Eq. ( 5 ) , we can obtain its corre-sponding decoding formula : ? , d = arg max e ,d
S( f , e , d ; ? ) .
Given the model parameter ? = ( W , W , M , B ) , if we consider ( M , B ) as constant and ? M ?h ( r ) +
B as an additional feature vector besides h , then Eq. ( 5 ) goes back to being a log-linear model with parameter ( W , W ) .
In this way , the decoding for AdNN can share the same search strategy and cube pruning method as the log-linear model .
Training Method
Training Objective
For the log-linear model , there are various tuning methods , e.g. MERT ( Och , 2003 ) , MIRA ( Watanabe et al. , 2007 ; Chiang et al. , 2008 ) , PRO ( Hopkins and May , 2011 ) and so on , which iteratively optimize a weight such that , after re-ranking a k-best list of a given development set with this weight , the loss of the resulting 1 - best list is minimal .
In the extreme , if the k-best list consists only of a pair of translations e * , d * , e , d , the desirable weight should satisfy the assertion : if the BLEU score of e * is greater than that of e , then the model score of e * , d * with this weight will be also greater than that of e , d .
In this paper , a e * , e for a source sentence f is called as a preference pair for f .
Following PRO , we define the following objective function under the maxmargin framework to optimize the AdNN model : where f is a source sentence in a given development set , and e * , d * , e , d is a preference pair for f ; N is the number of all preference pairs ; ? > 0 is a regularizer .
1 2 ? 2 + ? N f e * ,
Optimization Algorithm Since there are thousands of parameters in Eq. ( 6 ) and the tuning in SMT will minimize Eq. ( 6 ) repeatedly , efficient and scalable optimization methods are required .
Following Le et al. ( 2011 ) , we apply the mini-batch Conjugate Sub-Gradient ( mini- batch CSG ) method to minimize Eq. ( 6 ) .
Compared with the sub-gradient descent , minibatch CSG has some advantages : ( 1 ) it can accelerate the calculation of the sub-gradient since it calculates the sub-gradient on a subset of preference pairs ( i.e. mini- batch ) instead of all of the preference pairs ; ( 2 ) it reduces the number of iterations since it employs the conjugate information besides the sub-gradient .
Algorithm 1 shows the procedure to minimize Eq. ( 6 ) .
Algorithm 1 Mini-batch conjugate subgradient Input : ? 1 , T , CGIter , batch- size , k-best- list 1 : for all t such that 1 ? t ?
T do 2 : Sample mini- batch preference pairs with size batch- size from k-best- list 3 : Calculate some quantities for CG , e.g. training objective Obj , subgradient ? , according to Eq. ( 6 ) defined over the sampled preference pairs 4 : ? t+1 = CG (?
t , Obj , ? , CGIter ) 5 : end for Output : ? T +1
In detail , line 2 in Algorithm 1 firstly follows PRO to sample a set of preference pairs from k-best- list , and then uniformly samples batch - size pairs from the preference pair set .
Line 3 calculates some quantities for CG , and Line 4 calls a CG optimizer 2 and obtains ? t + 1 .
At the end of the algorithm , it returns the result ? T +1 .
In this work , we set the maximum number of CG iterations , CGIter , to a small number , which means ?
t+ 1 will be returned within CGIter iterations before the CG converges , for faster learning .
Pre-Training and Post-Training Since Eq. ( 6 ) is non-linear , there are many local minimal solutions .
Actually , this problem is inherent and is one many works based on the neural network for other NLP tasks such as language model and parsing , also suffer from .
And these works empirically show that some pre-training methods , which provide a reasonable initial solution , can improve the performance .
Observing the structure of Eq. ( 5 ) and the relationships between our model and a log-linear model , we propose the following simple pre-training method .
If we set W = 0 , the model defined in Eq. ( 5 ) can be regarded as a log-linear model with features h .
Therefore , we pre-train W using MERT or PRO by holding W = 0 , and use ( W , W = 0 , M , B ) as an initializer 3 for Algorithm 1 .
Although the above pre-training would provide a reasonable solution , Algorithm 1 may still fall into local minima .
We also propose a post-training method : after obtaining a solution with Algorithm 1 , we modify this solution slightly to get a new solution .
The idea of the post-training method is similar to that of the pre-training method .
Suppose ? = ( W , W , M , B ) be the solution obtained from Algorithm 1 .
If we consider both M and B to be constant , the Eq. ( 5 ) goes back to the log-linear model whose features are ( h , ? M ? h + B ) and parameters are ( W , W ) .
Again , we train the parameters ( W , W ) with MERT or PRO and get the new parameters ( W , W ) .
Therefore , we can set ? = ( W , W , M , B ) as the final solution for Eq. ( 6 ) .
The advantage of post-training is that it optimizes a convex programming derived from the original nonlinear ( non-convex ) programming in Eq. ( 6 ) , and thus it may decrease the risk of poor local optima .
Training Algorithm Algorithm 2 Training Algorithm Input : M axIter , a dev set , parameters ( e.g. ? ) for Algorithm 1 1 : Pre-train to obtain ?
1 = ( W , W = 0 , M , B ) as the initial parameter 2 : for all i such that 1 ? i ?
M axIter do 3 : Decode with ?
i on the dev set and merge all k-best-lists 4 : Run Algorithm 1 based on the merged kbest - list to obtain ?
i+1 5 : end for 6 : Post-train based on ?
M axIter +1 to obtain ?
Output : ?
The whole training for the AdNN model is summarized in Algorithm 2 .
Given a development set , we first run pre-training to obtain an initial parameter ?
1 for Algorithm 1 in line 1 .
Secondly , it iteratively performs decoding and optimization for M axIter times in the loop from line 2 to line 5 : it decodes with the parameter ?
i and merges all the k-best-lists in line 3 ; and it then runs Algorithm 1 to optimize ?
i + 1 .
Thirdly , it runs the post-training to get the result ? based on ?
M axIter +1 .
Of course , we can run post-training after running Algorithm 1 at each iteration i .
However , since each pass of post-training ( e.g. PRO ) takes several hours because of multiple decoding times , we run it only once , at the end of the iterations instead .
Experiments and Results
Experimental Setting
We conduct our experiments on the Chinese-to-English and Japanese - to - English translation tasks .
For the Chinese- to - English task , the training data is the FBIS corpus ( news domain ) with about 240k sentence pairs ; the development set is the NIST02 evaluation data ; the development test set is NIST05 ; and the test datasets are NIST06 , and NIST08 .
For the Japanese- to - English task , the training data with 300k sentence pairs is from the NTCIR - patent task ( Fujii et al. , 2010 ) ; the development set , development test set , and two test sets are averagely extracted from a given development set with 4000 sentences , and these four datasets are called test1 , test2 , test3 and test4 , respectively .
We run GIZA ++ ( Och and Ney , 2000 ) on the training corpus in both directions ( Koehn et al. , 2003 ) to obtain the word alignment for each sentence pair .
Using the SRILM Toolkits ( Stolcke , 2002 ) with modified Kneser - Ney smoothing , we train a 4 - gram language model for the Chinese- to - English task on the Xinhua portion of the English Gigaword corpus and a 4 - gram language model for the Japanese - to - English task on the target side of its training data .
In our experiments , the translation performances are measured by case-sensitive BLEU4 metric 4 ( Papineni et al. , 2002 ) .
The significance testing is performed by paired bootstrap re-sampling ( Koehn , 2004 b ) .
We use an in-house developed hierarchical phrase - based translation ( Chiang , 2005 ) for our baseline system , which shares the similar setting as Hiero ( Chiang , 2005 ) , e.g. beam-size= 100 , kbest -size=100 , and is denoted as L-Hiero to emphasize its log-linear model .
We tune L-Hiero with two methods MERT and PRO implemented in the Moses toolkit .
On the same experiment settings , the performance of L-Hiero is comparable Seconds / Sent L-Hiero 1.77 AdNN - Hiero-E 1.88 Table 1 : The decoding time comparison on NIST05 between L-Hiero and AdNN - Hiero -E. to that of Moses : on the NIST05 test set , L-Hiero achieves 25.1 BLEU scores and Moses achieves 24.8 .
Further , we integrate the embedding features ( See Section 3.3 ) into the log-linear model along with the default features as L-Hiero , which is called L-Hiero -E.
Since L-Hiero -E has hundreds of features , we use PRO as its tuning toolkit .
AdNN - Hiero-E is our implementation of the AddNN model with embedding features , as discussed in Section 3 , and it shares the same codebase and settings as L-Hiero .
We adopt the following setting for training AdNN - Hiero -E : u=10 ; batch-size=1000 and CGiter=3 , as referred in ( Le et al. , 2011 ) , and T =200 in Algorithm 1 ; the pre-training and post-training methods as PRO ; the regularizer ? in Eq. ( 6 ) as 10 and 30 , and M axIter as 16 and 20 in Algorithm 2 , for Chinese-to- English and Japanese - to - English tasks , respectively .
Although there are several parameters in AdNN which may limit its practicability , according to many of our internal studies , most parameters are insensitive to AdNN except ? and M axIter , which are common in other tuning toolkits such as MIRA and can be tuned 5 on a development test dataset .
Since both MERT and PRO tuning toolkits involve randomness in their implementations , all BLEU scores reported in the experiments are the average of five tuning runs , as suggested by Clark et al . ( 2011 ) for fairer comparisons .
For AdNN , we report the averaged scores of five post-training runs , but both pre-training and training are performed only once .
Results and Analysis
As discussed in Section 3 , our AdNN - Hiero -E shares the same decoding strategy and pruning method as L-Hiero .
When compared with L-Hiero , decoding for AdNN - Hiero -E only needs additional computational times for the features in the hidden units , i.e. ? M ? h ( r ) + B . Since Table 2 : The BLEU comparisons between AdNN - Hiero-E and Log-linear translation models on the Chinese-to-English and Japanese- to - English tasks .
+ means the comparison is significant over AdNN - Hiero -E with p < 0.05 .
these features are not dependent on the translation states , they are computed and saved to memory when loading the translation model .
During decoding , we just look up these scores instead of re-calculating them on the fly .
Therefore , the decoding efficiency of AdNN - Hiero -E is almost the same as that of L-Hiero .
As shown in Table 1 the average decoding time for L-Hiero is 1.77 seconds / sentence while that for AdNN - Hiero -E is 1.88 seconds / sentence on the NIST05 test set .
Word embedding features can improve the performance on other NLP tasks ( Turian et al. , 2010 ) , but its effect on log-linear based SMT is not as expected .
As shown in Table 2 , L-Hiero -E gains little over L-Hiero for the Japanese- to - English task , and even decreases the performance over L-Hiero for the Chinese- to - English task .
These results further prove our claim in Section 1 , i.e. the loglinear model requires the features to be linear with the model and thus limits its expressive abilities .
However , after the single - layer non-linear operator ( sigmoid functions ) on the embedding features for deep interpretation and representation , AdNN - Hiero -E gains improvements over both L-Hiero and L-Hiero -E , as depicted in Table 2 .
In detail , for the Chinese-to - English task , AdNN - Hiero -E improves more than 0.6 BLEU scores over L-Hiero on both test sets : the gains over L-Hiero tuned with PRO are 0.66 and 1.09 on NIST06 and NIST08 , respectively , and the gains over L-Hiero tuned with MERT are even more .
Similar results are achieved on the Japanese- to - English task .
In addition , to investigate the effect of different feature settings on AdNN , we alternatively design another setting for h in Eq. ( 5 ) : we use the default features for both h and h .
In particular , the language model of a rule for h is locally calculated without the contexts out of the rule as described in ( Chiang , 2007 ) .
We call the AdNN model with this setting AdNN - Hiero - D 6 .
Although there are serious overlaps between h and h for AdNN - Hiero - D which may limit its generalization abilities , as shown in Table 3 , it is still comparable to L-Hiero on the Japanese - to - English task , and significantly outperforms L-Hiero on the Chinese- to - English translation task .
To investigate the reason why the gains for AdNN - Hiero - D on the two different translation tasks differ , we calculate the perplexities between the target side of training data and test datasets on both translation tasks .
We find that the perplexity of the 4 - gram language model for the Chinese- to - English task is 321.73 , but that for the Japanese - to - English task is only 81.48 .
Based on these similarity statistics , we conjecture that the log-linear model does not fit well for difficult translation tasks ( e.g. translation task on the news domain ) .
The problem seems to be resolved by simply alternating feature representations through non-linear models , i.e. AddN -Hiero - D , even with single - layer networks .
Related Work Neural networks have achieved widespread attentions in many NLP tasks , e.g. the language model ( Bengio et al. , 2003 ) ; POS , Chunking , NER , and SRL ( Collobert and Weston , 2008 ) ; Parsing ( Collobert and Weston , 2008 ; Socher et al. , 2011 ) ; and Machine transliteration ( Deselaers et al. , 2009 ) .
Our work is , of course , highly motivated by these works .
Unlike these works , we propose a variant neural network , i.e. additive neural networks , starting from SMT itself and taking both of the model definition and its inference ( decoding ) together into account .
Our variant of neural network , AdNN , is highly related to both additive models ( Buja et al. , 1989 ) and generalized additive neural networks ( Potts , 1999 ; Waal and Toit , 2007 ) , in which an additive term is either a linear model or a neural network .
Unlike additive models and generalized additive neural networks , our model is decomposable with respect to translation rules rather than its component variables considering the decoding efficiency of machine translation ; and it allows its additive terms of neural networks to share the same parameters for a compact structure to avoid sparsity .
The idea of the neural network in machine translation has already been pioneered in previous works .
Casta?o et al. ( 1997 ) introduced a neural network for example - based machine translation .
In particular , Son et al . ( 2012 ) and Schwenk ( 2012 ) employed a neural network to model the phrase translation probability on the rule level ? , ? instead of the bilingual sentence level f , e as in Eq. ( 5 ) , and thus they did not go beyond the log-linear model for SMT .
There are also works which exploit non-linear models in SMT .
Duh and Kirchhoff ( 2008 ) proposed a boosting re-ranking algorithm using MERT as a week learner to improve the model 's expressive abilities ; Sokolov et al. ( 2012 ) similarly proposed a boosting re-ranking method from the ranking perspective rather than the classification perspective .
Instead of considering the reranking task in SMT , Xiao et al . ( 2010 ) employed a boosting method for the system combination in SMT .
Unlike their post-processing models ( either a re-ranking or a system combination model ) in SMT , we propose a non-linear translation model which can be easily incorporated into the existing SMT framework .
Conclusion and Future Work
In this paper , we go beyond the log-linear model for SMT and propose a novel AdNN based trans-lation model .
Our model overcomes some of the shortcomings suffered by the log-linear model : linearity and the lack of deep interpretation and representation in features .
One advantage of our model is that it jointly learns features and tunes the translation model and thus learns features towards the translation evaluation metric .
Additionally , the decoding of our model is as efficient as that of the log-linear model .
For Chinese-to-English and Japanese- to- English translation tasks , our model significantly outperforms the log-linear model , with the help of word embedding .
We plan to explore more work on the additive neural networks in the future .
For example , we will train word embedding matrices for source and target languages from a larger corpus , and take into consideration the bilingual information , for instance , word alignment ; the multi-layer neural network within the additive neural networks will be also investigated in addition to the single - layer neural network ; and we will test our method on other translation tasks with larger training data as well .
d * ,e ,d ?( f , e * , d * , e , d ; ? ) , ( 6 ) with ?(? ) = max S(f , e , d ; ? ) ?
S( f , e * , d * ; ? ) + 1 , 0
