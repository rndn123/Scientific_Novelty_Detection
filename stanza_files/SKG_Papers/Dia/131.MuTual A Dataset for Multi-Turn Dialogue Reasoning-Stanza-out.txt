title
MuTual : A Dataset for Multi-Turn Dialogue Reasoning
abstract
Non-task oriented dialogue systems have achieved great success in recent years due to largely accessible conversation data and the development of deep learning techniques .
Given a context , current systems are able to yield a relevant and fluent response , but sometimes make logical mistakes because of weak reasoning capabilities .
To facilitate the conversation reasoning research , we introduce Mu-Tual , a novel dataset for Multi-Turn dialogue Reasoning , consisting of 8,860 manually annotated dialogues based on Chinese student English listening comprehension exams .
Compared to previous benchmarks for non-task oriented dialogue systems , MuTual is much more challenging since it requires a model that can handle various reasoning problems .
Empirical results show that state - of - the - art methods only reach 71 % , which is far behind the human performance of 94 % , indicating that there is ample room for improving reasoning ability .
MuTual is available at https://github.
com / Nealcly / MuTual .
Introduction Building an intelligent conversational agent is one of the longest running goals in AI .
Existing conversational agents can be categorized into taskoriented dialogue systems ( Kannan et al. , 2016 ) and non-task - oriented chatbot systems ( Shum et al. , 2018 ; .
Owing to the rise of deep learning techniques and the large amount of conversation data for training ( Lowe et al. , 2015 ; Wu et al. , 2017 ; Zhang et al. , 2018 b ) , we are now witnessing promising results of chatbots both in academia and industry ( Pan et al. , 2019 ; Tao et al. , 2019 ) .
Neural dialogue systems are trained over a large dialogue corpus and used to predict responses given a context .
There are two lines of methods .
Retrievebased methods and generation based methods rely * Contribution during internship at MSRA .
M : Ma'am , you forgot your phone .
F : Oh , thanks , I could n't live without this little thing .
M : I know what you mean .
It is of great significance to you .
So did you enjoy your dinner ?
F : Oh yes , everything was just perfect .
It 's so hard to take the whole family out to eat , but your restaurant was perfect .
Johnny had his own place to play in and I had time to talk with my sisters and their husbands . ?
( A ) M : Thanks for your compliment for the restaurant . ?
( B ) M : I 'm sorry that you do n't have a good time .
?
( C ) M : Goodbye brother !
Love you . ?
( D ) M : Hurry up honey , or we will be late for the dinner .
on matching scores and perplexity scores , respectively .
Due to the development of text matching and pre-training models ( Devlin et al. , 2019 ; , a machine is able to achieve highly competitive results on these datasets , even close to human performance .
For instance , ESIM achieves 88 % on the Dialogue NLI ( Welleck et al. , 2019 ) , and BERT achieves 85.8 % , 93.1 % and 98.5 % in terms of R 10 @1 , R 10 @2 and R 10 @5 on the Ubuntu Corpus ( Whang et al. , 2019 ) .
However , there is still a huge gap between high performance on the leader - board and poor practical user experience .
Chatbot engines often generate responses that are logically incorrect or violate commonsense knowledge ( Shum et al. , 2018 ) .
A likely reason is that current dialogue systems do not have strong reasoning skills , and most of the cases in previous benchmarks can be tackled by linguistic information matching .
Previous work has demonstrated that neural encoders capture a rich hierarchy of syntactic and semantic information ( Jawahar et al. , 2019 ; Clark et al. , 2019 ) .
However , reasoning capability and commonsense knowledge are not captured sufficiently ( Young et al. , 2018 ) .
One important research question is how we can evaluate reasoning ability in chatbots , which can potentially allow us to bridge the gap between high performance on leader - board and unsatisfactory practical performance .
To this end , we develop dataset Task Reasoning Domain Manually Ubuntu ( Lowe et al. , 2015 ) Next Utterances Prediction an open domain Multi-Turn dialogue reasoning dataset ( MuTual ) to facilitate conversation model reasoning capabilities .
In particular , given a context , we prepare four response candidates , each of which is relevant to the context , but only one of them is logically correct .
As shown in Figure 1 , all responses follow the same topic , but only the first one is appropriated .
It requires reasoning ability on social etiquette and relationship to make the correct choice , which is not considered by existing dialogue benchmarks .
We build our dataset based on Chinese high school English listening comprehension test data , where students are excepted to select the best answer from three candidate options , given a multiturn dialogue and a question .
The original data is formatted as dialogue , question , answer , which is not directly suitable for our goal since chatbots only concern about how to respond contexts instead of answering an additional question .
Therefore , we ask human annotators to rewrite the question and answer candidates as response candidates .
Then our dataset follows the traditional response selection setting ( Lowe et al. , 2015 ) , where a model should recognize a correct response from others for a multi-turn dialogue .
The resulting dataset , MuTual , consists of 8,860 challenge questions , in terms of almost all questions involving reasoning , which are designed by linguist experts and high-quality annotators .
We evaluate state - of- the - art retrieval - based models and pre-training models on MuTual .
The best method gives a R@1 of 71 % , which significantly underperforms human performance ( 94 % ) .
To the best of our knowledge , MuTual is the first human-labeled reasoning - based dataset for multi-turn dialogue .
We provide detailed analysis to provide insights into developing potentially reasoning - based chitchat dialogue systems .
Dialogue : The Ubuntu Dialogue Corpus is a large retrieval - based dataset ( Lowe et al. , 2015 ) , extracted from Ubuntu chat logs .
PERSONA -CHAT ( Zhang et al. , 2018a ) considers consistent personality in dialogue .
Crowd workers are required to act the part of a given provided persona , and chat naturally .
Dialogue NLI ( Welleck et al. , 2019 ) is a natural language inference dataset modified from PERSONA - CHAT .
It demonstrates that NLI can be used to improve the consistency of dialogue models .
CoQA ( Reddy et al. , 2019 ) is collected by pairing two annotators to chat about a passage in the form of questions and answers .
Each question is dependent on the conversation history .
There are also several large-scale datasets in Chinese , such as Sina Weibo ( Shang et al. , 2015 ) , Douban Conversation Corpus ( Wu et al. , 2017 ) and E-commerce Dialogue Corpus ( Zhang et al. , 2018 b ) .
Related work
As shown in Table 1 , most of the existing conversation benchmarks do not focus on testing reasoning ability .
One exception is CoQA , which considers pragmatic reasoning .
The difference is that CoQA is a machine comprehension dataset , in which conversations are based on a given passage .
Another related reading comprehension dataset is DREAM ( Sun et al. , 2019 )
Oh yes , everything was just perfect .
It 's so hard to take the whole family out to eat , but your restaurant was perfect .
Johnny had his own place to play in and I had time to talk with my sisters and their husbands .
I 'm glad to hear it .
Our kids area is always popular .
Well , you can be sure we 'll be back .
Oh yes , everything was just perfect .
It 's so hard to take the whole family out to eat , but your restaurant was perfect .
Johnny had his own place to play in and I had time to talk with my sisters and their husbands .
comprehension .
It relies on an external question to test the model 's understanding capability .
In contrast to the above dataset , our dataset is a next utterance prediction task , which is the fundamental problem in retrieval - based chatbots .
In addition , our dataset requires various specific reasoning abilities , such as algebraic reasoning , intention prediction and so on , which is the main characteristic of our dataset .
Reasoning : Recently , efforts have been made to develop benchmarks and tasks to address reasoning for language understanding .
Winograd Schema Challenge ( Levesque et al. , 2012 ) is a reasoningbased coreference resolution task .
Each pair of sentences differs by only one phrase .
SWAG ( Zellers et al. , 2018 ) is derived from pairs of consecutive video captions , including 113 k short context each with four candidates endings .
CommonsenseQA ( Talmor et al. , 2019 ) is a question answering dataset extracted from CONCEPTNET ( Speer et al. , 2016 ) .
Utilizing CONCEPTNET to construct the dataset ensures that questions directly target commonsense reasoning .
RACE is a machine reading comprehension dataset collected from English exams for Chinese students .
AI2 Reasoning Challenge ( Clark et al. , 2018 ) contains 7,787 genuine grade-school level science questions with a corpus of 14 M science reference sentences .
DROP ( Dua et al. , 2019 ) and COSMOS ( Huang et al. , 2019 ) focus on factual understanding and commonsense comprehension , respectively .
Despite their success , these datasets can hardly help chatbots directly .
Following the traditional dia-logue response selection setting , we deeply modify English listening comprehension conversation to form an utterance prediction task .
Dataset
Collection
The original listening comprehension materials and question - answer pairs are designed by linguist experts .
Students are required to choose the best answer from three options for a question based on a piece of audio .
To ensure students fully understand the audio , most of the questions need to be answered with reasoning capability .
We crawled the listening exams from public websites 1 . Since the audio is either a conversation between two people or a simple passage , we only crawled data in the conversation format .
The raw data is formatted as triples Conversation ( audio ) , Question and Choices ( text ) , Answer ( image ) .
The following data pre-processing methods are applied to convert raw data to data in Figure 2 . Step 1 Pre-processing :
If question and candidate choices in two problems are the same , we consider them as duplicates and delete one of them .
If there are more than three candidate options in one problem , we randomly drop incorrect options until three candidates are left .
The answers are stored as images .
We apply a commercial OCR system to convert images to text .
It is easy to recognize the printed alphabet answer for the OCR system .
We manually correct all OCR outputs to ensure quality .
In the original listening comprehension test , the conversation is stored as audio .
We adopt a commercial ASR system to convert speech to text , and further recruit experienced annotators to correct the transcription errors .
To further ensure the quality of the transcripts , they are double -checked by annotators in the next step .
Step 2 Candidate Response Creation : Figure 2 illustrates the process of modifying the listening comprehension problem .
At first , an annotator is required to segment the original conversation , after clues to answer the question have appeared .
Then , they construct positive response ( Response A in Figure 2 ) and negative responses ( Response C and Response D ) by consulting correct choice ( Choice A ) and incorrect choices ( Choice B and Choice C ) , respectively .
To make MuTual more challenging , we further ask the annotator to construct one more negative response ( Response B ) based on the correct choice .
Through these steps , MuTual not only keeps the reasoning test designed by experts , but also introduces one more another type of reasoning for each instance .
As shown in Figure 2 , Response C and D can be excluded based on the relationship between two speakers .
But B is incorrect due to the attitude reasoning .
It is worth noting that all negative responses are logically correct if the context is not considered , but they are not appropriated responses if the context is taken into account .
Therefore , our dataset focuses on multi-turn conversation reasoning rather than the logic of a sentence .
When framing a negative response , we encourage annotators to copy some phrases in the context to discourage a model that can solve the problem by text matching .
We further calculate the lexical overlap between response and context .
There are 9.98 % ( 10.63 % ) words in the positive ( negative ) response that occur in the corresponding context , suggesting that MuTual is hard to solve by plain text matching .
Annotators in Step 2 are all English-major graduate students in Chinese , who are familiar with English language exams in China and fluent in English ( pass the TEM - 8 2 ) .
Annotators are required to draft annotate 170 instances repeatedly , until their labeling is sufficiently accurate to provide useful annotation .
Because not all conversations are adapted to construct a reasoning - based response problem , the annotator has the right to skip the con- versation .
We employ five annotators to construct the response , and two quality inspectors to check it .
We discard the instance when inspectors doubt the uniqueness or correctness of the answer .
Analysis
The detailed statistics of MuTual are summarized in Table 2 . MuTual has an average of 4.73 turns .
The vocabulary size is 11,343 , which is smaller than other dialogue datasets ( Lowe et al. , 2015 ; Wu et al. , 2017 ) .
Because MuTual is modified from listening tests of English as a foreign language , the complexity of morphology and grammar is much simpler than other datasets .
For human-annotated datasets , there is always a trade- off between the number of instances being annotated and the quality of annotations ( Kryciski et al. , 2019 ) .
Our dataset is smaller than the previous crawling - based dialogue dataset ( Lowe et al. , 2015 ; Wu et al. , 2017 ) due to the collection method .
But it is comparable with high-quality reasoning based dataset ( Clark et al. , 2018 ; Khashabi et al. , 2018 ; Talmor et al. , 2019 ) and human-designed dialogue dataset ( Zhang et al. , 2018a ) .
Moreover , around 10 k is sufficient to train a discriminative model ( Nivre et al. , 2019 ) or fine-tuning the pretraining model .
To assess the distribution of different reasoning types , we annotate the specific types of reasoning that are involved for instance , sampled from the test set and categorize them into six groups .
The definition and ratio of each group are shown as follows .
Attitude Reasoning :
This type of instance tests if a model knows the speaker 's attitude towards an object .
Algebraic Reasoning :
This type of instances tests whether a model is equipped with algebraic abilities when it chooses a response .
Intention Prediction :
This type tests whether a model can predict what the speaker is going to do next .
Situational Reasoning : Situation information ( e.g. , Location , Relationship between two speakers ) is considered in this type of instance .
A model should mine the implicit information from the previous context .
Context Candidates Responses Reasoning Type Multi-fact Reasoning :
In this type of instance , the correct response is related to multiple facts in context , which requires the model to deeply understand the context rather than simply text matching .
Others : .
There are 9 % of instances that require other commonsense knowledge .
For example , at the bottom of Figure 3 , the model should know that a fully reserved restaurant is usually very popular .
The six types of reasoning are considered the most relevant to real chatbots .
For example , it enables chatbots to make personal recommendations if a machine knows the user 's attitude .
The ability of intention prediction allows chatbots to respond more intelligently in a long conversation session .
MuTual plus
To further increase the difficulty , we use safe response to replace one of the candidate responses for each instance in MuTual .
To guarantee diversity , the safe response is sampled from a list including " I 'm afraid I did n't quite catch what you were saying . " , " Could you repeat that ? " , " I 'm really sorry , I did n't catch that . " , etc .
In particular , once the instance is chosen , we randomly select a response to replace .
If the positive response is replaced , the correct one is the safe response .
If the negative response is replaced , the original positive response is still the best one .
The motivation to build MuTual plus is to evaluate whether a model is able to select a safe response when the other candidates are inappropriate .
When we replace the positive response with a safe response , it simulates a scenario in which all the other candidates are incorrect .
The phenomenon is common in retrieval - based chatbots , because limited candidate responses cannot handle all cases in practice .
Similarly , we can evaluate if the model can choose the correct response instead of a safe response when a correct response exists .
Experiments
We split the data into training , development and test sets , with an 80 % , 10 % and 10 % ratio .
We pack instances constructed from the same conversation during splitting to avoid data leakage .
Following the standard dialogue setting ( Lowe et al. , 2015 ; Wu et al. , 2017 ) , we consider our task as a response selection task and employ traditional information retrieval evaluation methods , including recall at position 1 in 4 candidates ( R@1 ) , recall at position 2 in 4 candidates ( R@2 ) and Mean Reciprocal Rank ( MRR ) ( Voorhees , 2000 ) .
We compare the performance of several response selection models as well as pre-training models .
We simply introduce these works as follows :
Baselines
We evaluate individual scoring methods , multichoice methods and human performance in our experiment .
Given a context c and four candidates ( r 1 , r 2 , r 3 , r 4 ) , the individual scoring method computes a score for each choice independently with a score g( c , r i ) , and selects the individual with the highest score among four candidates .
On the contrary , the multi-choice method selects the best one by classification over all choices , formulated as h(c , r 1 , r 2 , r 3 , r 4 ) .
TF - IDF : The correct response tends to share more words with the context than the incorrect ones .
Following Lowe et al. ( 2015 ) , we calculate the TF - IDF vectors for the context and each of the candidate responses , respectively , and then select the highest cosine similarity between the context and the candidate response as the model output .
The " IDF " is calculated only on the training set .
Dual LSTM ( Lowe et al. , 2015 ) : Two LSTMs are used to encode context and response , respectively .
The relevance between context and response is calculated by the similarity of the final hidden state from both LSTMs .
Sequential Matching Network ( Wu et al. , 2017 ) :
To avoid losing information in the context , SMN constructs a word-word and a sequencesequence similarity matrix , instead of utilizing the last hidden state only , and then aggregates similarity matrix as a matching score .
Deep Attention Matching Network : Zhou et al . ( 2018 ) adopt self attention module ( Vaswani et al. , 2017 ) to encode response and each utterance , respectively .
To match utterance and response , DAM further applies cross-attention module and 3D matching to obtain final score .
BERT ( Devlin et al. , 2019 ) : Pre-training models have shown promising results on various multichoice and reasoning tasks ( Whang et al. , 2019 ; . Following Devlin et al. ( 2019 ) , we concatenate the context ( sentence A ) , and a candidate response ( sentence B ) as BERT input .
On the top of BERT , a fully - connected layer is used for transforming the [ CLS ] token representation to the matching score .
RoBERTa : Liu et al. ( 2019 ) re-establish BERT 's masked language model training objective by using more data and different hyper-parameters .
We fine- tune RoBERTa in the same way as BERT .
GPT -2 ( Radford et al. , 2019 ) : Given a context , the positive response has a higher probability compared with negative responses .
Motivated by this , we concatenate context and response as a sequence , and calculate the joint probability of an entire sequence .
The response in the lowest perplexity sequence is considered as the positive response .
Moreover , we fine- tune the GPT - 2 on [ Context , Positive Response ] pairs in MuTual training set , denoted as GPT -2- FT .
Multi-choice Method : Inspired by BERT for multiple choice ( Devlin et al. , 2019 ) , the task is considered as picking the most suitable response by comparing four candidates responses .
In particular , we concatenate each candidate response with the corresponding context .
Each input sequence is subsequently encoded to produce a [ CLS ] representation .
The positive response is predicted based on the concatenation of all [ CLS ] representations , on which a fully connected layer with softmax is used .
The method is denoted as BERT - MC .
Similarly , we implement RoBERTa - MC as another multi-choice method .
Human Performance :
To obtain the human performance , we employ 3 NLP experts to measure the ceiling performance on the test set .
Experiment Results
We report the performance of approaches introduced in 4.1 , and human performance .
Implementation details are shown in Appendix B .
Results on MuTual
All models perform significantly worse than on other popular conversation datasets , such as the Ubuntu Corpus ( Lowe et al. , 2015 ) and the Dialogue NLI dataset ( Welleck et al. , 2019 ) , while human can address the reasoning problems easily .
For example , BERT gives 85.8 % R 10 @1 on the Ubuntu Corpus , but RoBERTa only gives 71.3 % R 4 @1 on MuTual .
TF - IDF only slightly better than randomly guessing , which indicates that there is no obvious statistic clue between context and positive response .
In contrast , TF -IDF achieves 54.98 % R@1 score on the Ubuntu Corpus , showing our dataset is more difficult to get the correct answer by text overlap .
We evaluate typical retrieved - based dialogue models ' performance on MuTual .
From Table 3 ( Lowe et al. , 2015 ) 0.266 0.528 0.538 0.260 0.491 0.743 SMN
( Wu et al. , 2017 ) 0.274 0.524 0.575 0.299 0.585 0.595 DAM 0.239 0.463 0.575 0.241 0.465 0.518 BERT ( Devlin et al. , 2019 ) 0.657 0.867 0.803 0.648 0.847 0.795 RoBERTa 0.695 0.878 0.824 0.713 0.892 0.836 Individual scoring method ( generation ) GPT - 2 ( Radford et al. , 2019 ) 0.335 0.595 0.586 0.332 0.602 0.584 GPT -2 -FT ( Radford et al. , 2019 ) ( Wu et al. , 2017 ) 0.264 0.524 0.578 0.265 0.516 0.627 DAM 0.261 0.520 0.645 0.272 0.523 0.695 BERT ( Devlin et al. , 2019 ) 0.514 0.787 0.715 0.514 0.787 0.715 RoBERTa 0 can see that well - designed matching models do not give better performance compared with simple dual LSTM , moreover , they drop by more than 50 absolute R@1 points compared to their performance on the Ubuntu Corpus , indicating that text matching models cannot handle reasoning problem well .
Both BERT and RoBERTa outperform other models in MuTual , which is consistent with results in other literatures ( Talmor et al. , 2019 ) .
This is mainly because models learn reasoning capability during the pre-training on a large corpus .
Although RoBERTa only gets 71.3 % on R@1 , it achieves a surprising number , 89.2 % , on R@2 , indicating that the model is able to rank the correct response to the top - 2 position .
BERT -MC and RoBERTa - MC obtain similar results with BERT and RoBERTa , respectively .
However , even RoBERTa is far behind human performance 23 points on R@1 , indicating that MuTual is indeed a challenging dataset , which opens the door for tackling new and complex reasoning problems in multi-turn conversations .
GPT - 2 and GPT -2 - FT also perform undesirably on MuTual , even if the averaged perplexity on MuTual testset is 10.40 .
This phenomenon illustrates that 1 ) sentences in MuTual are fluent ; and 2 ) current generative models still have plenty of room to improve their reasoning ability .
Results on MuTual plus
As shown in Table 4 , all models perform worse on MuTual plus , indicating the dataset is more difficult than MuTual , which is consistent with our assumption .
We find that the performance of multichoice method is significantly better than individual scoring method .
One possible explanation is that multi-choice methods consider candidates together , so they can distinguish whether or not the safe response is the best one .
In contrast , individual scoring methods are not robust , and safe responses are easy to confuse methods in the training stage .
Moreover , RoBERTa - MC outperforms others by a large margin , showing its outstanding performance on reasoning problems .
Furthermore , we conduct a transfer experiment , in which models are trained on MuTual but tested on MuTual plus without fine-tuning .
The experiment investigates whether the model handles safe responses well if they have never seen them in training corpus .
As shown in Table 4 , RoBERTa -MC and RoBERTa drops 24.1 % and 6.8 % , respectively , in the transfer setting , demonstrating the benefits of seeing safe responses during the training process .
Moreover , the individual scoring RoBERTa outperforms RoBERTa - MC , showing that the individual scoring method is more robust , when the safe response is not fed during training .
Discussion
Performance across different reasoning types :
To analyze model performance across different reasoning types , we calculate BERT - MC and RoBERTa - MC performance on various question types as we introduce in Section 3.2 .
As shown in Figure 4 , we find that the trends of BERT - MC and RoBERTa - MC are similar across different categories .
RoBERTa - MC significantly outperforms BERT - MC in attitude reasoning and multi-fact reasoning .
One potential reason is that there are some normal patterns between action and attitude captured by RoBERTa - MC , such as " play football " and " excited " .
However , instances that involve algebraic and situation show poor performance .
These two reasoning types heavily depend on commonsense reasoning .
Taking Figure 5 as examples , it takes a simple subtraction step to derive the time difference ( 5:00 pm - 6h = 11:00 am ) , but this turns out a significant challenge for RoBERTa - MC .
In the second case , RoBERTa - MC fails to infer the dialogue situation , where the goal is to find a flat to rent .
Performance across different context lengths :
It is interesting that the performance of RoBERTa does not decrease significantly with the number of turns increasing , which is different from the phenomenon observed on other datasets .
As shown in Table 5 , the performance drops by only 1.9 points R@1 from 2 turns to long turns ( > 6 ) , and the performance of 5 turns is higher than those with 4 Table 5 : Performance comparison ( R@1 ) of different number of turns on the test set .
# T denotes number of turns .
# Instances is the number of instances turns , indicating the reasoning problems do not become much harder when the context becomes longer .
The results also show that the difficulty of MuTual is attributed to reasoning instead of complex conversation history .
Context ablation study :
We further verify whether our dataset requires multi-turn understanding rather than degenerating to a single turn reasoning problem .
We evaluate Roberta and Roberta - MC performance when some utterances are manually removed .
Figure 6 shows the performance when the earliest n utterances are removed in testing .
As the ablation utterance increases , the performance of RoBERTa and RoBERTa - MC significantly decreases , which conforms to intuition .
RoBERTa and RoBERTa - MC achieve only 43.7 % and 47.7 % after ablating all utterances in the context , respectively , indicating the importance of each utterance and the quality of the dataset .
Moreover , if we shuffle the sequence of utterance , the performance of RoBERTa - MC drops by 3.8 % only , showing that it is insensitive to the utterance sequence information .
Conclusion
We introduced MuTual , a high-quality manually annotated multi-turn dialogue reasoning dataset , which contains 8,860 dialogues and aims to test reasoning ability of dialogue models .
We describe the process for generating MuTual , and perform a detailed analysis .
We find that various state- ofthe - art models show poor performance in MuTual .
The best model RoBERTa only obtains 71.3 % R@1 .
There is a large gap between the model performance and human performance .
We hope that this dataset facilitates future research on multi-turn conversation reasoning problem .
Figure 1 : 1 Figure 1 : B is incorrect because there is no reason to apologize .
C and D can be excluded because the relationship between two speakers are waiter and customer based on the context .
