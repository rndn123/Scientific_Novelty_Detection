title
Towards Holistic and Automatic Evaluation of Open-Domain Dialogue Generation
abstract
Open-domain dialogue generation has gained increasing attention in Natural Language Processing .
Its evaluation requires a holistic means .
Human ratings are deemed as the gold standard .
As human evaluation is inefficient and costly , an automated substitute is highly desirable .
In this paper , we propose holistic evaluation metrics that capture different aspects of open-domain dialogues .
Our metrics consist of ( 1 ) GPT - 2 based context coherence between sentences in a dialogue , ( 2 ) GPT - 2 based fluency in phrasing , ( 3 ) n-gram based diversity in responses to augmented queries , and ( 4 ) textual - entailment - inference based logical self -consistency .
The empirical validity of our metrics is demonstrated by strong correlations with human judgments .
We open source the code and relevant materials .
1 * Equal contributions
Introduction Learning to communicate is a key capacity of intelligent agents .
Research on enabling a machine to have meaningful and natural conversations with humans plays a fundamental role in developing artificial general intelligence , as can be seen in the formulation of Turing test ( Turing , 1950 ) .
Recently open-domain or non-task - oriented dialogue systems have attracted a surge of research interest ( Bessho et al. , 2012 ; Sordoni et al. , 2015 ; Shang et al. , 2015 ; Vinyals and Le , 2015 ; Ghazvininejad et al. , 2018 ) .
Evaluating models of open-domain dialogue generation in an efficient manner poses a significant challenge in developing dialogue systems .
The prevalent method of open-domain dialogue evaluation is human- based rating with a given rubric .
Table 1 : Two responses from an dialogue system ( Wolf et al. , 2019 ) on Daily Dialogue Dataset .
The first generated response appears reasonable within the opendomain dialogue , while its BLEU score and semantic similarity between model response and reference response is low .
The second generated response conflicts with its prior utterances .
The italic text highlights the logical contradiction .
When various variations in the model and sets of hyper-parameters are needed , the labor-intensive human evaluation is deemed impracticable .
This key drawback may hinder the research progress and render the human evaluation approach not scalable .
Previous automatic evaluation metrics generally focus on the quality of the dialogue generation : context coherence and fluency .
Word-overlap metrics ( Papineni et al. , 2002 ; Banerjee and Lavie , 2005 ; Lin , 2004 ) or ad-hoc classifiers ( Tao et al. , 2018 ; Ghazarian et al. , 2019 ) are designed for measuring the quality .
In open-domain dialogue , the relation between two utterances is more critical as shown in the first example of Table 1 . Compared with the previous two approaches , a language model , trained on an enormous amount of text , can naturally capture coherence among both words and utterances .
On the other hand , a good evaluation metric should not only measure the quality of gen-eration , but also the diversity of generation , which is especially important for open-ended tasks like dialogue or story generation ( Hashimoto et al. , 2019 ) .
Some n-gram based metrics have been utilized to measure diversity ( Mou et al. , 2016 ; .
However , this metric might be improper for diversity evaluation since the generated utterances given various queries provided by the benchmark are generally diverse .
In our experiments , we observe constantly high diversity in terms of human ratings and n-gram based entropy when evaluating the generated responses directly .
In addition to the three aforementioned metrics , logical selfconsistency is also a key aspect of dialogue models ( Zhang et al. , 2018 ) .
An dialogue example with logical contradiction is displayed in the second example of Table 1 . Welleck et al. ( 2019 ) measured logical self -consistency by transferring each sentence into a rule- based triple , ( category , relation , category ) , with the help of human annotators .
We are nevertheless unaware of any reliable automatic measure of logical consistency in open-domain dialogue .
In this work , we propose holistic metrics that evaluate distinctive aspects of generated dialogues .
Specifically , we consider ( 1 ) context coherence of a dialogue : the meaningfulness of a response within the context of prior query , ( 2 ) language fluency of generated responses : the quality of phrasing relative to a human native speaker , ( 3 ) response diversity of a set of generated sentences : the variety in meaning and word choice of responses , and ( 4 ) logical self - consistency : the logical consistency of utterances from a dialogue agent .
Both context coherence and response fluency ( quality metrics ) can naturally be captured by metrics based on strong language models like GPT - 2 ( Radford et al. , 2019 ) .
Therefore , we propose to recruit and fine- tune GPT - 2 as a basis of our quality metrics .
With regard to response diversity and logical selfconsistency , we propose to measure them under augmented utterances with controlled paraphrasing .
We leverage two effective approaches to generate augmented utterances : word substitution and text generator with a k-best decoder .
Moreover , we utilize n-gram based entropy to capture response diversity and entailment based approach to capture logical self -consistency .
Our experiments show that the proposed metrics strongly correlate with human judgments .
Moreover , our augmented datasets allow for a more accurate and straightforward hu-man annotation , significantly improving the agreement between human evaluation .
We release the code and relevant materials as open-source contribution to pave the way towards further research .
Prior Art Heuristic - based metrics have been shown to align well with human judgments and widely applied in various language generation tasks .
For machine translation , BLEU ( Papineni et al. , 2002 ) computes n-gram precision , whereas METEOR ( Banerjee and Lavie , 2005 ) takes into account both precision and recall .
For summarization , ROUGE ( Lin , 2004 ) also considers both precision and recall by calculating F-measure .
These n-gram based metrics are well -suited for the generation tasks that are more source - determined or low conditional entropy such as translation , image captioning , and summarization .
Some dialogue studies adopted these metrics to evaluate the quality of generated conversation responses ( Ritter et al. , 2011 ; Su et al. , 2018 ; Sordoni et al. , 2015 ) .
They nevertheless are not suitable for open-ended generations or high conditional entropy tasks like dialogue generation where a diverse range of generations is acceptable conditional on a query .
Indeed , Liu et al. ( 2016 ) conducts extensive empirical studies on these metrics ( e.g. , BLEU , METEOR , and ROUGE ) to test their effectiveness on evaluating dialogue generation and find limited relation between these automatic metrics and human judgments .
The word-overlap metrics ( e.g. , BLEU ) fail to capture the semantic similarity between model and reference responses .
The following works leverage the distributed representation learned in neural network models to capture semantic similarity among context , model response , and reference response .
collect a dataset of human scores and train a hierarchical recurrent neural network ( RNN ) to predict human-like scores to input responses given the context , resulting in an automatic metric that has a medium level correlation with human judgments .
Obtaining this metric however requires a large dataset of human-annotated scores , thus rendering this approach less flexible and extensible .
Tao et al . ( 2018 ) proposes a referenced metric and unreferenced metric blended evaluation routine ( RUBER ) for open-domain dialogue systems .
This blended metric is a combination of two metrics .
A referenced metric measures the similarity between model- generated and reference responses on the basis of word-embeddings .
An unreferenced metric captures the relevance between the query and response .
It is obtained by training a neural network classifier to determine whether a response is appropriate .
The positive examples are the references , while the negative examples are reference responses randomly chosen from the dataset , hence avoiding the need of human-annotated data .
After training , the Softmax score is utilized to measure whether the generated response is coherent with the query .
Attempting to improve RUBER , Ghazarian et al. ( 2019 ) explores to use contextualized embeddings from BERT .
The BERT - based unreferenced metric improves over the word-embedding - based RUBER unreferenced metric .
Interestingly , they show that the combined metric has a reduced correlation with human judgments than the unreferenced metric alone .
Although this finding is counterintuitive , it is consistent with the characteristics of open-domain dialogue that a range of diverse responses is reasonable given a query .
Hence a response can be acceptable to human annotators even if it does not align well with the reference either in terms of word-overlap or semantic embedding .
Context Coherence .
One key component of dialogue response is its coherence to the query as explored in Tao et al . ( 2018 ) and Ghazvininejad et al . ( 2018 ) .
Prior work measures the coherence based on the Softmax score of a trained binary classifier .
Here we explore an alternative approach based on language modeling ( Bengio et al. , 2003 ) .
A language model can naturally capture the coherence of the response to the query without resorting to an ad-hoc classifier .
Language Fluency .
Besides coherence , a good response should be fluent .
Fluency is often measured by a language model ( Holtzman et al. , 2018 ; Xu et al. , 2018 ) .
We define the response fluency score as negative perplexity of generated responses .
Response Diversity .
In addition to quality metrics , response diversity is also critical , especially for high conditional entropy tasks like dialogue or story generation ( Hashimoto et al. , 2019 ) .
Some n-gram based metric has been utilized to measure diversity .
Mou et al . ( 2016 ) and compute unigram entropy across all generated utterances to measure the diversity .
This metric might be improper for diversity since the generated utterances given various queries are generally diverse .
In our experiments , we observe constantly high diversity in terms of human ratings and n-gram based entropy .
In another perspective , the entropy computed across all generated responses is essentially measuring the marginal entropy of the responses , while our actual interest is in the conditional entropy of the responses conditional on the queries .
Logical Self - Consistency .
Similar to diversity evaluation , current benchmarks are not suitable for evaluating logical self -consistency .
The current dataset is well - formed making the system to generate a simple and nonredundant response , but unfortunately , there still exist logical contradictions as shown in Table 1 .
The natural language inference ( NLI ) task ( Williams et al. , 2018 ) aiming to check whether the sentence is entailed or contradicted by a previous sentence is highly related to logic evaluation on open-domain dialogues .
Metrics
Context Coherence Language models , which predict the next token given previous tokens , naturally capture the coherence between sentences and particularly the dialogue query and response in our case .
GPT - 2 ( Radford et al. , 2019 ) is a large-scale pre-trained language model based on the transformer architecture ( Vaswani et al. , 2017 ) .
It is trained on a vast amount of diverse data and demonstrates impressive text generation capabilities .
In order to better capture the dependence between the queries and responses , GPT - 2 can be fine-tuned using the next sentence prediction task on the dialogue dataset of interest .
Suppose a query q contains tokens {q t : t = 1 , ... , T q } and a response r has tokens {r t : t = 1 , ... , T r }.
Let P denote the fine- tuned GPT - 2 , then the context coherence is defined as the loglikelihood of the response conditional on the the query normalized by the length of the response length : c raw ( r|q ) = 1 T r log P ( q , r ) P ( q ) = 1 T r Tr t log P ( r t |r <t , q ) .
( 1 ) Note that c raw ( r|q ) is some negative number and unbounded from below .
A single value is then hard to explain absolutely and can only be interpreted relative to other values .
Also , the unboundedness renders it prone to extreme values .
Hence , a normalized score is utilized instead .
Since the score distribution varies as a function of the dataset , the lower bound is defined as 5th percentile , denoted as c 5th , instead of some arbitrary value .
Then the normalized score , c ( r|q ) , is c( r|q ) = ? max ( c 5th , c raw ( r|q ) ) ?
c 5th c 5th ( 2 ) which ranges from 0 to 1 .
Response Fluency
To capture the fluency of responses , we also adopt the pretrained language model , GPT - 2 .
In particular , the raw response fluency score , f raw ( r ) , is defined as , f raw ( r ) = 1 T r Tr t log P ( r t |r < t ) .
( 3 ) Similar to context coherence , a normalized version , f ( r ) , of f raw ( r ) is employed .
Response Diversity Prior work ( Mou et al. , 2016 ; measured diversity by computing the n-gram entropy across all generated responses , which essentially reflects the marginal entropy of the responses .
Diversity of the responses conditional on the query ( e.g. , conditional entropy ) are however more of interest for dialogue models .
On the other hand , if we measure diversity based on responses randomly sampled from a model conditional on a single query , the response quality is generally low ( Caccia et al. , 2018 ) .
The current work instead proposes to measure response diversity utilizing augmented datasets with controlled paraphrasing , which allows for measuring diversity among top-ranked responses conditional on paraphrased queries and hence avoiding the tradeoff or dependency between diversity and quality .
In other words , for a given query , we slightly tilt the corresponding element in the query - response joint space along the query dimension ( achieved by paraphrasing - augmentation ) and then measure the entropy of high-quality responses in the neighbourhood of the targeted query .
While augmenting the queries to measure the conditional entropy of responses , we need to control the diversity of the augmented queries such that the augmented ones stay in the vicinity of the targeted query .
Hence the goal of controlled augmentation is to minimize diversity in both meaning and word use and avoid feeding the dialogue model identical inputs .
To achieve so , two augmentation approaches are considered : ( 1 ) WordNet ( Miller , 1998 ) Substitution ( WS ) and ( 2 ) Conditional Text Generator ( CTG ) .
WordNet Substitution ( WS ) is a word-level manipulation method that replaces some words with synonyms defined in WordNet .
Different from WS , Conditional Text Generator ( CTG ) is used to augment queries in multi-turn dialogue .
It requires a generator to produce augments conditioned on the context , which is defined as the prior utterance history to the selected query .
For instance , suppose [ u 1 ; ... ; u t?1 ] denotes the utterance history and u t indicates the query to be augmented , then the top - K beams , { u ( 1 ) t , ... , u ( K ) t } , from the CTG model conditional on the utterance history are produced .
Given the target query and a set of augmented queries for it with controlled paraphrasing , {u
Logical Self -Consistency Logical self - consistency measures if a generated response is logically contradictory to what the agent uttered in the multi-turn history .
The basic idea is to apply a pretrained Multi-Genre Natural Language Inference ( MNLI ; Williams et al. 2018 ) model to label if the relation of the response and the utterance history of the same agent is logically consistent .
More specifically , we train a ternary classifier that takes two utterances as input and predicts the relation as either contradiction , entailment or neutral on the MNLI dataset .
Then we average the contradiction class probabilities of the current utterance and each prior utterance from this agent as the contradiction score .
In order to match the human ratings , we use 1 minus the contradiction score as the final score of logical self -consistency evaluation .
Moreover , we measure logical self-consistency under augmented datasets with controlled paraphrasing , using WS and CTG introduced in Section 3.3 .
The main idea is to generate augmented multi-turn utterance history that more likely induces the dialogue system to produce contradictory responses .
We assume that it is more likely for the agent producing self -contradictory responses when responding to similar queries .
We use WS and CTG to paraphrase the query and then calcu- late the contradiction score of the current utterance and each prior utterance from this agent .
Experiments 4.1 Dataset
To facilitate comparison with prior work ( Ghazarian et al. , 2019 ) , the DailyDialog dataset ( Li et al. , 2017 ) is adopted for the empirical analysis of our proposed metrics .
This dataset contains 13,118 high-quality multi-turn dialogue dataset .
The dialogue is split into a 42,000 / 3,700 / 3,900 traintest - validation partitions .
Response Generation
A sequence-to-sequence ( seq2seq ) model with attention ( Bahdanau et al. , 2014 ) was trained with the train and validation partitions to generate dialogue responses .
The implementation in OpenNMT ( Klein et al. , 2017 ) was used to train the model .
The seq2seq consists of a 2 - layer LSTM with 500 hidden units on both the encoder and decoder .
The model was trained with SGD and learning rate of 1 .
To obtain responses on a wide spectrum of quality and diversity , we sample the data with top -k sampling where k = { 1 , 10 , 100} .
Language Model Fine-tuning
The base GPT - 2 model with 12 layers was used to compute our metrics 2 . The GPT - 2 model was fine-tuned on the training and validation data .
In fine-tuning , the queries and responses were concatenated together as a single sentence to feed into GPT - 2 .
The perplexity of the fine-tuned language model on the test dataset was 16.5 .
Controlled Query Generation WordNet substitution and conditional text generators were used to augment diversity -controlled queries .
The Stanford part-of-speech ( POS ) tagger ( Toutanova and Manning , 2000 ) and the WordNet by Miller ( 1998 ) on the training and validation splits for query augmentation , which was applied to the testing dataset to augment the query with the top -K beams .
For response diversity , five variants are obtained , the original query and four paraphrased ones ; for logical self -consistency , two variants are obtained , the original query and one paraphrase .
Metric Evaluation
To assess the validity of our proposed metrics , we utilize Amazon Turk to collect high quality human ratings from 10 subjects .
For each metric , we select a set of samples to be presented to humans and each datapoint is to be rated from 1 to 5 , with 1 being the worst and 5 being the best on each metric .
On both context coherence and response fluency , we select 200 datapoints with a diverse range of generation quality .
There are 200 query- response pairs to be rated for context coherence and 200 responses to be rated for response fluency .
For response diversity , we select 100 datapoints , totaling 500 responses , to be rated in groups of 5 , all of which are conditioned on the controlled inputs generated by CTG or WS given the same context .
For logical self-consistency , 100 datapoints are selected independent from response diversity .
After Amazon
Turk results are collected , we compute the Pearson and Spearman correlation between our automatic metrics and human ratings to assess the validity of our metrics .
We normalize the human rating scores to be in the range of 0 to 1 .
Results
Context Coherence Table 3 demonstrates the Pearson and Spearman correlations between the proposed context coherence metric and human judgments .
Also , the results were compared to the previous best-performing au- tomatic metric , RUBER with BERT embeddings ( Ghazvininejad et al. , 2018 ) .
Clearly both our language model based coherence metric shows higher correlation with human judgments than the classifier - based metric , RUBER .
In addition , we compared the proposed metric with a similar metric based on a GPT - 2 language model without fine-tuning on the target dataset .
The fine-tuned version improved the results , indicating that fine-tuning on the dialogue dataset enables the language model to better capture the dependency between the queries and replies .
Interestingly , even the metric based on the language model without fine-tuning correlated with human ratings stronger than RUBER .
We also examined the inter-rater reliability .
It is computed by holding out the ratings of one rater at a time , calculating its correlation with the average of other rater 's judgments , and finally averaging over or taking the maximum of all held - out correlation scores .
The inter-rater reliability results also support the strong performance of our proposed context coherence metric in that the correlation between the automatic metric and human evaluation was close to the inter-rater correlations .
In addition , Figure 1 tuning on GPT -2 .
It helps to improve the consistency between human rating and automatic metric .
Table 2 displays a case study .
Our coherence metric and the human evaluation agreed that the generated response is not coherent with the given query , while RUBER indicated that this reply is coherent .
This might be because RUBER simply compares the embeddings of the query and response and business travel related words in the query such as vacation , workweek and in the reply such as travel , company make RUBER judge that they are similar .
Response Fluency
Our findings show that the proposed fluency metric f ( r ) is highly correlated with human judgments .
Table 4 summarizes the relation between our proposed fluency metric and human ratings in terms of Pearson and Spearman correlation .
The importance of fine-tuning GPT - 2 ( as outlined in Section 4.3 ) is evident .
We observe an increase from 0.43 to 0.82 in Pearson correlation and an enhancement from 0.32 to 0.81 in Spearman correlation .
In addition , Figure 2 details the effect of fine-tuning .
Notably , a correction of outliers occurs .
Response Diversity
Table 5 shows the evaluation of the proposed diversity metric on the basis of the augmented datasets with WS and CTG .
We also include a baseline dataset which consists of responses from randomly chosen queries from the testing data .
Human ratings based on the paraphrasing augmented datasets show high inter-rater correlations and lower variance , indicating that raters generally agree with each other .
The poor baseline performance is likely due to the uncontrolled nature of input sentences such that outputs of evaluated models are generally diverse , making it difficult for humans to judge the diversity performance of the model .
Furthermore , our diversity metrics have correlations with human ratings close to the corresponding mean inter-rater correlations , suggesting that the diversity evaluation based on the paraphrasingaugmented data can reveal the diversity of a dialogue system consistent with humans .
Logical Self-Consistency
Table 8 displays the correlations between the proposed automatic ratings and human ratings on the the paraphrasing augmented data using WS and CTG and a baseline without augmentation .
The automatic metric based on augmented data has a stronger relation with that based on the baseline .
In particular , the metric based on CTG augmentation aligns with human judgments the closet .
Inter-rater Pearson and Spearman correlations are reported in Table 9 .
Human ratings on the augmented data are more consistent than those on the baseline , indicating the necessity and efficiency of using a refined dataset instead of the original one .
We show a case study in Table 7 .
Relation between the Four Metrics
Although the four proposed metrics are intuitively and theoretically important in evaluating a dialogue system , it is not entirely clear whether they are independent from each other such that it is necessary to measure all of them .
We empirically investigate their association .
We randomly choose 50 dialogues from the testing dataset and construct the evaluation data for the four metrics .
Five human evaluators rate on the four aspects of each dialogue .
We then examine the pairwise correlation of human ratings on the four metrics .
Response fluency correlates with context coherence ( r = 0.42 , p = 0.003 ) .
This is mainly due to the fact that inarticulate responses are often considered incoherent with the context .
All other pair-wise correlations are non-significant ( r s < 0.1 , p s > 0.25 ) 3 .
Thus , the four metrics are relatively independent from each other and it is critical to take into account all of them to obtain a holistic evaluation of a dialogue model .
Context of Conversation Speaker A : Are you more of a leader or a follower ?
Speaker B : I don 't try to lead people .
I ' d rather cooperate with everybody , and get the job done by working together .
Generated Utterance Speaker A : Are you more of a follower or a leader ?
Model Response Speaker B : I like to keep to myself .
I 'm a person who does not want to be a follower .
Our Score : 0.09 Human Score : 0.20
Conclusion
This paper provides a holistic and automatic evaluation method for open-domain dialogue models .
In contrast to prior art , our means of evaluation captures not only the quality of generation , but also the diversity and logical consistency of responses .
We recruit GPT - 2 as a strong language model to evaluate the context coherency and response fluency .
For response diversity and logical self -consistency , we propose to measure these two aspects under augmented utterances with controlled paraphrasing .
We leverage two effective approaches to generate augmented utterances : word substitution and text generator with k-best decoder .
Moreover , we utilize n-gram based entropy to capture response diversity and entailment based approach to measure logical self -consistency .
The proposed metrics show a strong correlation with human judgments .
It is our hope the proposed holistic metrics may pave the way towards the comparability of open-domain dialogue models .
k ? 0 , ... , K } where u ( 0 ) t := u t , the corresponding responses are generated by the model under test .
Then we can calculate the n-gram entropy for samples in the set { u ( k ) t+ 1 : k ? 0 , ... , K}.
