title
Dialogue Generation on Infrequent Sentence Functions via Structured Meta-Learning
abstract
Sentence function is an important linguistic feature indicating the communicative purpose in uttering a sentence .
Incorporating sentence functions into conversations has shown improvements in the quality of generated responses .
However , the number of utterances for different types of fine- grained sentence functions is extremely imbalanced .
Besides a small number of high- resource sentence functions , a large portion of sentence functions is infrequent .
Consequently , dialogue generation conditioned on these infrequent sentence functions suffers from data deficiency .
In this paper , we investigate a structured meta-learning ( SML ) approach for dialogue generation on infrequent sentence functions .
We treat dialogue generation conditioned on different sentence functions as separate tasks , and apply modelagnostic meta-learning to high- resource sentence functions data .
Furthermore , SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions .
Experimental results demonstrate that SML not only improves the informativeness and relevance of generated responses , but also can generate responses consistent with the target sentence functions .
Introduction Humans express intentions in conversations through sentence functions , such as interrogation for acquiring further information , declaration for making statements , and imperative for making requests and instructions .
For machines to interact with humans , it is therefore essential to enable them to make use of sentence functions for dialogue generation .
Sentence function is an important linguistic feature indicating the communicative purpose of a sentence in a conversation .
There are four major sentence functions : Declarative , Interrogative , Exclamatory and Imperative ( Rozakis , 2003 ) .
Each major sentence function can be further decomposed into fine- grained ones according to different purposes indicated in conversations .
For example , Interrogative is divided into Wh-style Interrogative , Yes - no Interrogative and other types .
These fine- grained sentence functions have great influences on the structures of utterances in conversations including word orders , syntactic patterns , and other aspects ( Akmajian , 1984 ; Yule , 2016 ) .
Figure 1 presents how sentence functions influence the responses .
Given the same query expressed in Positive Declarative , the responses expressed in Wh-style Interrogative and in Negative Declarative are completely different .
Although the use of sentence functions improves the overall quality of generated responses ( Ke et al. , 2018 ) , it suffers from the data imbalance issue .
For example , in the recently released response generation dataset with manually annotated sentence functions STC - SeFun ( Bi et al. , 2019 ) , more than 40 % of utterances are Positive Declarative while utterances annotated with Declarative with Interrogative words account for less than 1 % of the entire dataset .
Therefore , dialogue generation mod-els suffer from data deficiency for these infrequent sentence functions .
Recently , model- agnostic meta-learning ( MAML ) ( Finn et al. , 2017 ) has shown promising results on several low-resource natural language generation ( NLG ) tasks , including neural machine translation ( Gu et al. , 2018 ) , personalized response generation ( Madotto et al. , 2019 ) and domain-adaptive dialogue generation ( Qian and Yu , 2019 ) .
They treat languages of translation , personas of dialog and dialog domains as separate tasks in MAML respectively .
In the same spirit of previous works , we first treat dialogue generation conditioned on different sentence functions as separate tasks , and meta-train a dialogue generation model using high- resource sentence functions .
Moreover , we observe that sentence functions have hierarchical structures : four major sentence functions can be further divided into twenty fine - grained types .
Some fine- grained sentence functions may share some similarities while some others are disparate .
For example , utterances belong to Wh-style Interrogative and Yes -no Interrogative may share some transferable word patterns while utterances in Wh-style Interrogative and in Exclamatory with interjections totally differ from each other .
Motivated by this observation , we explore a structured meta-learning ( SML ) considering inherent structures among fine - grained sentence functions .
Inspired from recent advances on learning several initializations with a set of meta-learners ( Yao et al. , 2019 ; Vuorio et al. , 2019 ) , we develop our own approach to utilize the underlying structure of sentence functions .
More specifically , our proposed SML explicitly tailors transferable knowledge among different sentence functions .
It utilizes the learned representations of fine- grained sentence functions as parameter gates to influence the globally shared parameter initialization .
Therefore , conversation models for similar sentence functions can share similar parameter initializations and vice versa .
As a result , SML enhances meta-learning effectiveness by promoting knowledge customization among different sentence functions but simultaneously preserving knowledge generalization for similar sentence functions .
The experimental results on STC - SeFun dataset ( Bi et al. , 2019 ) show that responses generated from our proposed structured meta-learning algorithm are of better quality over several baselines in both human and automatic evaluations .
Moreover , our proposed model can generate responses consistent with the target sentence functions while baseline models may ignore the target sentence functions or generate some generic responses .
We further conduct a detailed analysis on our proposed model and show that it indeed can learn word orders and syntactic patterns for different fine - grained sentence functions .
Background Dialogue Generation with Sentence Function .
Open domain dialogue generation has been widely studied with sequence - to-sequence learning ( Seq2Seq ) .
To alleviate the generate generic and dull responses issue of Seq2Seq ( Li et al. , 2016 ) , some efforts provide additional controlling signals in dialogue generation , such as emotion , persona , and topic ( Xing et al. , 2017 ) .
Different from these local controlling factors , sentence function can influence the global structure of the entire response such as changing word orders and word patterns .
Zhao et al. ( 2017 ) utilize dialogue acts as prior linguistic knowledge and integrate it with conditional variational autoencoders to achieve the discourse- level diversity of generated responses .
Ke et al. ( 2018 ) adopt a conditional variational autoencoder to capture various word patterns and introduce a type controller to control the sentence function .
Xu et al . ( 2019 ) generalize the concept of dialogue act into meta words and use meta words for open domain dialogue generation .
Meta-Learning for Low-Resource NLG .
Humans can learn quickly with a few examples while data-driven models are mostly compute-intensive .
In meta-learning , the goal of the trained model is to quickly learn a new task from a small amount of data .
Therefore , the model should be able to learn transferable knowledge on a large number of different tasks .
Recently , model- agnostic metalearning ( MAML ) ( Finn et al. , 2017 ) has shown promising results on several few-shot classification tasks .
MAML directly optimizes the gradient towards a good parameter initialization for easy fine-tuning on low-resource scenarios .
Because of the model- agnostic nature of MAML , it can be directly applied to low-resource NLG tasks with modifications on corresponding training strategies .
Gu et al. ( 2018 ) frame machine translation between two language pairs as a single task in meta-learning , and learn to adapt to low-resource languages based on multilingual high- resource language tasks .
In a similar spirit , recent works apply MAML to personalized response generation ( Madotto et al. , 2019 ) and task - oriented dialogue agents ( Mi et al. , 2019 ; Qian and Yu , 2019 ) .
In this paper , we not only investigate how MAML helps for open domain dialogue generation on infrequent sentence functions , but also develop a structured approach to fit the hierarchical structure of sentence functions .
Problem Formulation
We define response generation conditioned on every query - response sentence function pair ( d X , d Y ) as a single task .
As the number of utterances for different sentence functions is extremely imbalanced , some tasks have abundant utterances while some others are low-resource .
We take K high- resource tasks as training data , denoted as : D k train = { ( X k n , Y k n , d k X , d k Y ) , n = 1 ... N } , k = 1 ...K ( 1 ) Then , we take T tasks with infrequent sentence functions as target tasks , denoted as : D t target = { ( X t n , Y t n , d t X , d t Y ) , n = 1 ... N } , t = 1 ... T ( 2 ) where N N .
During training , taking the query X , its sentence function d X and the target response sentence function d Y as inputs , a dialog model f parameterized by ?
learns the mapping between inputs and the corresponding response Y using training data D train of K tasks , f ? : X k ? ( d k X , d k Y ) ?
Y k , k = 1 ...K ( 3 )
The initialization parameters of model f learned from the training process , denoted by ?
0 , are used as the initialization parameters in the adaptation process .
The adaptation process on each target task D t target can be formulated as follows : f ? * = arg max ? log p( f ?
| D t target , f ? 0 ) ( 4 ) where f ? * is the fine-tuned model that could perform well on the target task D t target .
Proposed Approach
In this section , we first introduce the conditional sequence - to-sequence ( C- Seq2Seq ) model for open domain dialogue generation with fine- grained sentence function .
Then we describe how to meta-train C-Seq2Seq under the algorithm of model- agnostic meta-learning .
Finally , we explore the structure of fine- grained sentence functions and propose the structured meta-learning ( SML ) algorithm .
C-Seq2Seq Conditional sequence-to-sequence ( C- Seq2Seq ) ( Ficler and Goldberg , 2017 ) is the best generative model on STC - SeFun dataset ( Bi et al. , 2019 ) .
We use it to test the effectiveness of our proposed structured meta-training approach .
C-Seq2Seq follows the widely used encoder-decoder framework Vinyals et al. , 2015 ) .
The encoder transforms the query X into contextualized representation ( h 1 , h 2 , ... , h n ) through bidirectional LSTMs ( Hochreiter and Schmidhuber , 1997 ) .
For the decoder part , we learn an additional sentence function embedding [ s 1 , s 2 , ... , s K ] for each query - response sentence function pair , which plays a major role in our structured modeling ( Sec. 4.3 ) .
Then we takes the concatenation of word embedding w t and the sentence function embedding s k as input at each timestep , and updates its hidden state as follows , u t = LSTM ( u t?1 , [ w t ; s k ] ) .
( 5 ) The decoder utilizes soft attention mechanism ( Luong et al. , 2015 ) to derive the context vector c t , a t, i = exp ( u t W a h i ) j exp( u t W a h j ) , c t = i a t, i h i .
( 6 ) Finally , the predicted probability distribution over the vocabulary V is computed as : ht = tanh ( W h [ u t ; c t ] ) , ( 7 ) P V = softmax ( W V ht + b V ) , ( 8 ) where W a , W h , W V and b V are trainable parameters .
Meta-Learning for C-Seq2Seq
The fundamental idea behind meta-learning is based on a simple machine learning principle : test and train conditions must match .
In the context of meta-learning , it becomes that the conditions between task adaptation ( fine-tuning ) stage ( Eqn. 4 ) and meta-training stage ( Eqn. 3 ) must match .
To mimic the task adaptation stage , Model-Agnostic Meta-Learning ( MAML ) ( Finn et al. , 2017 ) explicitly train the parameters of the model such that a small number of gradient steps with a small amount c ) Structured Meta-Learning ( SML ) .
Tasks of training , validation and testing are colored in blue , orange and green respectively .
Solid lines represent the learning of initialization ?
0 while dashed lines show the path of finetuning .
Our structured modeling can learn the structure in different sentence functions so that similar tasks will be initialized from closer starting points than others ( T 1,2,3 and T 4,5,6 in ( c ) ) .
In the testing ( adaptation ) stage , a new sentence function such as Negative Declarative will benefit from this learned structure by initializing from a point that is close to other fine - grained sentence functions in the same category of Declarative ( T test in ( c ) ) .
! ! " # ! " ! " $ ! " % ! " & ! " ' ! " ( ) # !*+ , ! -./- ) $ ) % ) & ) ' ) ( ) *+ , ) -./- ! ! " ) # ) $ ) % ) & ) ' ) ( ) *+ , ) -./- ( b) MAML ( c ) SML ( a ) Transfer Learning ! ! " ) # ) $ ) % ) & ) ' ) ( ) *+ , ) -./- of training data will make rapid progress on new tasks .
The intuition behind MAML is that there exist some transferable internal representations across tasks .
MAML aims to find the most sensitive model parameters such that small changes in model parameters will produce large improvements on each task .
Here is how we apply MAML to response generation on infrequent sentence functions .
We uniformly sample one source task T k at random .
Then we independently sample two subsets of data ( D T k , D T k ) from task T k .
D T k is used to simulate the process when f ? adapts to the target lowresource tasks while D T k is used to evaluate the outcome of the adapted model .
In the simulation of adaptation stage , the model f parameterized by ?
adapts to this new task T k using one or more gradient descent updates , ? k = ? ? ? ? L D T k ( f ? ) , ( 9 ) where ? is a hyperparameter for task -specific learning rate .
Then the model evaluates the updated parameters ?
k towards D T k .
The loss can be formulated as , L D T k ( f ? k ) = L D T k ( f ? ? L D T k ( f ? ) ) ( 10 ) Afterward , the model is trained by optimizing the performance of L(f ? k ) with respect to ?
across randomly sampled tasks .
To learn the internal representation shared across tasks , it is possible to aggregate gradients ? ? L( f ? k ) sampled from sev-eral tasks in the meta-update , ? ? ? ? ? k ? ? L ( f ? k ) , ( 11 ) where ? is the meta learning rate across tasks and k is the sampled tasks for gradient aggregation 1 . Different from common gradient - based approaches , Eqn. 11 update the model not from ?
k but from ?
because MAML aims to learn the most sensitive parameters to facilitate fast adaptation .
As a result , the meta-learned model is not necessarily a good model on its own , but it adapts fast on any new task with a few gradient update steps .
Exploring Structure Modeling MAML learns some transferable knowledge in different training tasks ( a task in our paper is defined as response generation conditioned on a given sentence function ) .
In effect , the meta-learned model can adapt fast for the low-resource testing tasks ( sentence functions ) .
However , MAML assumes all tasks in training and adaptation stages distributed uniformly , which is not the case for our conditioned response generation - some tasks may share some similarities while some are exclusive to each other .
For example , utterances belong to Wh-style Interrogative and Yes -no Interrogative may share some transferable word patterns while word patterns in Wh-style Interrogative and Exclamatory with interjections are totally different .
Therefore , we propose to represent sentence functions explicitly through learned embeddings s 1 ; ... ; s K ( Sec. 4.1 ) .
Then sentence function embeddings are used to interact with each other via a gated self-attention mechanism , which can be viewed as a clustering process to make similar sentence function embeddings close to each other .
Finally , the self-attended representations of these sentence functions are used as parameter gates to tailor the transferable knowledge of the meta-learned prior parameters .
Recently , the task - aware modulation problem in metalearning is also investigated in the machine learning community ( Yao et al. , 2019 ; Vuorio et al. , 2019 ) .
In their approaches , they first learn the task mode from input data as a vectorized representation and use the identified mode to modulate the meta-learned prior parameters .
The key difference between our structured modeling and their approaches is our model does not learn the taskaware representation through input data because the sentence functions are predefined before generating the responses .
Moreover , we propose the gated self-attentive approach to learn the underlying structure which has never been used before .
Task Representation Learning .
To ensure similar tasks share similar representations , we design a gated self-attention module for sentence function embeddings .
For any fine- grained sentence function k , we first match the sentence function embedding matrix S = [ s 1 ; ... ; s K ] with itself s k to compute the self-matching representation m k , and then combine it with the original representation s k : a k = softmax ( S s k ) , m k = Sa k ( 12 ) f k = tanh ( W f [ s k ; m k ] ) , ( 13 )
The self-matching operation matches the sentence form embedding to other sentence form embeddings , which can be viewed as a clustering process so that embeddings from similar sentence forms will be close to each other .
The final representation sk is derived via a gated summation through a learnable gate vector g k , g k = sigmoid ( W g [ s k ; m k ] ) ( 14 ) sk = g k f k + ( 1 ? g k ) s k ( 15 ) where W f , W g are learnable weights , is the element-wise multiplication .
For sentence function embeddings s new in the adaptation stage , we use the already well - learned sentence function embeddings S = [ s 1 ; ... ; s K ] in the meta-training Update ? 0 k = ? 0 k ? ? ? 0 k L( f ? 0 k ) in Eqn. 9 10 : end for 11 : Update ? ? ? ? ? k ? ? 0 k L( f ? 0 k ) in Eqn. 11 with respect to all D T k 12 : end while stage , concatenate s new with learned embeddings as S = [ s 1 ; ... ; s K ; s new ] and apply Eqn. 12 ? 15 for task representation learning in the adaptation stage .
Because sentence form embeddings seen in training are learned to build the underlined structure : similar sentence functions are clustered close to each other .
In the adaptation phase , the unseen sentence function embedding can adapt fast by moving to the cluster it belongs to .
For example , a new sentence form Yes - no Interrogative can learn some transferable knowledge from trained sentence forms under the Interrogative category .
Task -Specific Knowledge Adaptation .
To adapt globally transferable knowledge ?
0 to each sentence function , we design a parameter gate o k for ?
0 , o k = FC ? Wp ( s k ) , ? 0 k = ? 0 o k ( 16 ) where FC ?
Wp is a fully connected layer parameterized by W p and activated by a sigmoid function ? , is the element-wise multiplication .
Intuitively , sentence functions with similar representations will activate similar initial parameters while dissimilar sentence functions trigger different ones .
One major problem for Eqn. 16 is that it introduces dozens of parameters compared to ?
0 to achieve the element- wise dot product with ?
0 . Here we only tailor parameters in the decoder to reduce the total amount of learnable parameters .
SML Algorithm and Visualization .
The whole algorithm of our proposed model is detailed in Algorithm 1 .
Figure 2 visually illustrates the difference between transfer learning , model - agnostic meta-learning ( MAML ) and our proposed structured meta-learning ( SML ) .
All methods in Figure 2
In effect , the learned model can adapt fast to any unseen task , such as T test .
SML in Figure 2 ( c ) additionally explores the structure across tasks so that similar tasks will be initialized from closer starting points than others .
In the testing ( adaptation ) stage , a new sentence function such as Negative Declarative will benefit from this learned structure by initializing from a point that is close to other fine - grained sentence functions in the same category of Declarative .
Experimental Settings Dataset .
We conduct experiments on STC - SeFun dataset ( Bi et al. , 2019 ) which is a large-scale Chinese short text conversation dataset with manually labeled sentence functions .
Utterances in STC - SeFun have two -level sentence function labels .
The four major sentence function types include : Declarative , Interrogative , Imperative and Exclamatory .
Each major sentence function is further divided into fine- grained sentence function labels like Wh-style Interrogative which in total is 20 categories .
Considering all query and response sentence functions , we could have 20 ? 20 = 400 meta tasks .
However , some tasks are extremely low-resource with less than 100 samples .
Incorporating these tasks as our adaptation tasks leads to a high variance of test performance .
To establish concrete evaluation , we only consider tasks with more than 700 samples , in which 100 samples are used for validation in the adaptation stage and 500 samples are used for the final testing .
Under this constraint , we receive 18 query - response fine - grained sentence function pairs as 18 tasks .
We select 9 high- resource tasks for meta-training , 4 tasks for meta-validation and 5 tasks for testing ( adaptation ) .
The dataset statistics is shown in ( Gu et al. , 2018 ) .
Moreover , the experimental results may generalize to more tasks if there is more data .
Baselines and Ablations .
We train the model C-Seq2Seq described in Section 4.1 under the following learning settings : ? Multi-Task Learning ( MTL ) :
We train C-Seq2Seq under the multi-task learning approach with training and validation data .
Then we directly apply the trained model on each target task without fine-tuning .
? Multi-Task Learning + Fine-tuning ( MTL + FT ) :
We train C-Seq2Seq under the same training paradigm as multi-task learning .
Then we finetune the model on each target task .
This setting corresponds to a transfer learning scenario .
? Model-Agnostic Meta-Learning ( MAML ) :
We meta-train the model under the methodology model- agnostic meta-learning in other lowresource NLG tasks ( Gu et al. , 2018 ; Qian and Yu , 2019 ) .
Then we fine- tune the meta-trained model on each target task .
This is the ablation without structure modeling ( Sec. 4.3 ) .
? Structured Meta-Learning ( SML ) :
We meta-train the model using structured meta-learning described in Algorithm 1 .
Then we fine- tune the meta-trained model on each target task .
Model Settings .
We take the most frequent 30 k words as our vocabulary and use the pretrained em- beddings ( Song et al. , 2018 ) for initialization .
The sentence function embedding with dimension 20 is randomly initialized and learned through training .
We use two - layer LSTMs in both encoder and decoder , and the LSTMs hidden unit size is set to 400 .
We use dropout ( Srivastava et al. , 2014 ) with the probability p = 0.3 .
All trainable parameters , except word embeddings , are randomly initialized with the uniform distribution in ( ?0.1 , 0.1 ) .
We adopt the teacher - forcing for the training .
In the testing , we select the model with the lowest perplexity and beam search with size 5 is employed for generation .
All hyper-parameters and models are selected on the validation dataset .
Learning Settings .
We use SGD as the optimizer with a minibatch size of 64 and an initial learning rate of 1.0 for both meta-learning ( line 9 and line 11 in Algorithm 1 ) and multi-task learning .
For meta-learning , we sample 3 tasks for line 3 in Algorithm 1 and take a single gradient step for line 9 and line 11 in Algorithm 1 .
We meta-train the model for 8 epochs and start having the learning rate after the 3 epoch .
All models are fine-tuned with a SGD optimizer with a minibatch size of 64 and learning rate of 0.1 .
We set the gradient norm upper bound to 3 and 1 during the training and finetuning respectively .
To avoid any random results , we report the average of five runs for all results .
Evaluation Metrics .
Since automatic metrics for open-domain conversations may not be consistent with human perceptions ( Liu et al. , 2016 ) , we hire 5 full-time human judges from a third - party data annotation company .
We provide them a detailed annotation guideline ( in Chinese ) with good and bad response samples for each metric .
They are first asked to annotate 100 responses for trial , and we select the top 3 judges according to the annotation quality .
Finally , the 3 selected judges independently evaluate 2,000 responses generated from our model and three baselines for all five adaptation tasks .
For each query , four responses generated from the proposed model , and three baselines are randomly shuffled to reduce the priming effect .
The annotators evaluate responses on four metrics : ( 1 ) " Fluency " ( Flue ) measures the grammatical correctness of responses ; ( 2 ) " Relevance " ( Rele ) measures whether the response is a relevant reply to the query ; ( 3 ) " Informativeness " ( Info ) evaluates whether the response provides any meaningful information with regard to the query ; ( 4 ) " Accuracy " ( Accu ) evaluates whether the response is coherent with the given response sentence function .
" Fluency " , " Relevance " and " Informativeness " are graded independently in a 1 - 5 scale where 5 is the best .
" Accuracy " takes a binary value ( 1 or 0 ) .
We further normalize the average scores over all rated samples into [ 0 , 1 ] .
Besides , we compare the adaption time of all models by calculating the " Fine-Tuning Step " ( FT Step ) till convergence on each test task .
For completeness , we also show the following automatic evaluation metrics : " Perplexity " ( PPL ) ; " BLEU - 1/2 " ( B1 / B2 ) ( Papineni et al. , 2002 ) ; " Distinct - 1/2 " ( Dist1 / Dist2 ) ( Li et al. , 2016 ) .
Results Performance of Human Evaluation .
Human evaluation on " Fluency " ( Flue ) , " Relevance " ( Rele ) , " Informativeness " ( Info ) and " Accuracy " ( Accu ) are shown in Table 2 .
The inter annotator agreement ? scores are shown in Table 3 .
We can make the following observations : ? MTL receives the worst performance on all human evaluation metrics .
Recall that MTL is trained on a mixture of all training tasks .
It can only learn some generic response patterns like " So do I " .
That is why MTL has the worst performance on " Informativeness " metric .
Moreover , since MTL never sees responses in target sentence functions in training , the generated responses are not coherent with the given sentence function at all .
? MTL +FT achieves better performance than MTL because it further fine-tunes on each target dataset .
However , the performance on the accuracy of target sentence function is still unsatisfactory .
This reveals that fine-tuning may not solve the adaptation problem on low-resource tasks .
?
SML and MAML achieve the best / second - best human evaluation results across most of the metrics .
This indicates that by simulating the lowresource testing scenarios in meta-training , the learned model adapts well on the low-resource testing tasks .
Moreover , there is a huge improvement on the accuracy of given sentence functions ( Accu ) , which reveals that MAML / SML can find model parameters that are sensitive to changes in the new task , such that small changes in the parameters produce large improvement on the accuracy of sentence functions .
?
SML outperforms MAML in most of the cases .
This tells us exploring the structure of sentence functions can balance knowledge generation and knowledge customization .
The task -specific initialized model can leverage the knowledge of similar tasks and thus adapts the target tasks better .
Performance of Automatic Evaluation .
We also show the results of automatic evaluations in Table 2 .
Compared to transfer learning based model MTL + FT , our meta-learning based models adapt faster ( lower fine - tune step ) and better ( lower perplexity ) .
Although BLEU is not reliable enough to evaluate response generation , MAML and SML still achieve slightly better results than baselines .
Presumably , they can capture frequent word pattern in low-resource tasks .
Finally , MTL +FT , MAML and SML achieve comparable performance with regard to the unigram / bigram diversity ( Dist1 / 2 ) of generated responses .
Effect of Structure Modeling .
To get more insight into how our proposed SML balances the knowledge generalization and customization , in Figure 3 , we visualize the heatmap of self-attention weight a k in Eqn. 12 for all 9 training sentence function representations .
Each row in Figure 3 demonstrates the similarity between sentence function s k and all sentence functions [ s 1 ; ... ; s K ] .
Take the first row in Figure 3 ger similar initializations and dissimilar sentence functions trigger different ones .
Case Study .
We present two examples in Figure 4 , each of which shows a test query with the target sentence function and responses generated by all models .
We see that responses generated by MTL are generic and can be used to reply to a large number of queries .
With fine-tuning on responses of the target sentence function , MTL + FT can capture the correct response pattern in some cases .
However , it is inferior to our proposed models MAML and SML , which can not only generate words related to the target sentence function but also keep the coherence and informativeness of responses .
Conclusion
In this paper , we propose a structured meta-learning algorithm for open domain dialogue generation on infrequent sentence functions .
To tackle the lowresource issue , our proposed model , based on the recently proposed model - agnostic meta-learning , can find both transferable internal representations and sensible parameters which can produce large improvement under a few adaptation steps .
Moreover , we further explore the structure across fine- grained sentence functions and such that the model can balance knowledge generalization and knowledge customization .
Extensive experiments show that our structured meta-learning ( SML ) algorithm outperforms existing approaches under the lowresource setting .
Figure 1 : 1 Figure 1 : Query-response pairs with fine- grained sentence functions .
Responses under different sentence functions are completely different in global structures .
