title
Dialogue State Tracking with Explicit Slot Connection Modeling
abstract
Recent proposed approaches have made promising progress in dialogue state tracking ( DST ) .
However , in multi-domain scenarios , ellipsis and reference are frequently adopted by users to express values that have been mentioned by slots from other domains .
To handle these phenomena , we propose a Dialogue State Tracking with Slot Connections ( DST - SC ) model to explicitly consider slot correlations across different domains .
Given a target slot , the slot connecting mechanism in DST - SC can infer its source slot and copy the source slot value directly , thus significantly reducing the difficulty of learning and reasoning .
Experimental results verify the benefits of explicit slot connection modeling , and our model achieves state - of- the - art performance on Mul-tiWOZ 2.0 and MultiWOZ 2.1 datasets .
Introduction
Task-oriented dialogue systems assist users to achieve their certain goals , such as making a restaurant reservation or booking a taxi .
To fulfill users ' goals , dialogue state tracking ( DST ) is employed to estimate dialogue states at each turn .
Dialogue states consist of constraints and requests conveyed by user utterances , typically are represented by a set of predefined slots and their corresponding values .
For instance , the user utterance " I am looking for a Korean restaurant in the centre " mentions two slots , food and area , whose values are Korean and centre respectively .
Numerous methods are proposed to tackle the challenge of DST recently , and these methods can be mainly categorized into two types : fixed vocabulary and open vocabulary ( Eric et al. , 2019 ) .
Fixed vocabulary models are designed in the paradigm of multi-class classification , relying on a predefined ( We omit some turns and slots for simplicity . ) ontology ( Henderson et al. , 2014a ; Mrk?i? et al. , 2017 ; Zhong et al. , 2018 ) .
Open vocabulary approaches ( Xu and Hu , 2018 ; Wu et al. , 2019 ; Ren et al. , 2019 ) break the assumption of predefined ontologies , turning to generate values only given target slots .
Wu et al . ( 2019 ) propose a copy-augmented encoder-decoder model to track dialogue states , which outperforms fixed vocabulary models and achieves the state - of - the - art result in multi-domain DST .
Despite significant improvements achieved by those open vocabulary models , they always suffer from understanding enormous ellipsis and reference expressions in multi-domain scenarios .
As shown in Table 1 , there are several slot connections across multiple domains and turns .
For example , at the second turn , the value of the target slot attraction - area is informed by a referring expression " in the same area as the restaurant " .
Thus , the system needs to retrieve the value of its source slot restaurant - area .
The last turn shows an obscurer utterance with multiple slot connections , in which target slots taxi- departure and taxi- destination are implicitly connected to their source slots attractionname and restaurant - name respectively .
For those slots that need connections , existing methods attempt to find their values out from the lengthy dialogue history , which usually fail because of high learning complexity .
In this paper , we formally consider the above challenge as related - slot problem and propose a novel model DST - SC ( Dialogue State Tracking with Slot Connections ) to address it .
We follow previous work to build a copy-augmented encoderdecoder model .
Specially , DST - SC is designed with a slot connecting mechanism to establish the connection between the target slot and its source slot explicitly .
Thus it can take advantage of the source slot value directly instead of reasoning from preceding turns .
The contributions of this work are two -fold : ?
To the best of our knowledge , this work is the first one to discuss the related - slot problem in multi-domain DST and address it by explicitly modeling slot connections across domains .
?
We demonstrate that DST - SC is more effective for handling the related - slot problem and outperforms state - of - the - art baselines .
Model
In this section , we will describe DST - SC model in detail .
DST - SC is an open vocabulary model based on the encoder-decoder architecture .
As shown in Figure 1 , there are three components that contribute to obtain the target slot value : ( 1 ) word generation from the vocabulary ; ( 2 ) word copying from the dialogue history ; ( 3 ) value copying from the source slot .
To reduce the burden on the decoder , DST - SC also equips with a slot gate ( Wu et al. , 2019 ) to predict for slot values of none and dontcare .
Encoder
Our model uses a bi-directional GRU to encode the dialogue history x = {w 1 , w 2 , ? ? ? , w m } , where m is the number of tokens in the dialogue history .
Each input token is first embedded using a word embedding function ?
emb and then encoded into a fix-length vector h i . h i = GRU (? emb ( w i ) ) . ( 1 )
Word Generation
We employ another GRU to decode slot values .
Each slot is comprised of a domain name and a slot name , e.g. , hotel - area .
While decoding the j-th slot s j , its summed embedding is fed as the first input .
The last hidden state of the encoder initializes the decoder hidden state .
At decoding step t , the hidden state is represented as h j t .
( The superscript j will be omitted for simplicity . )
Following the vanilla attention - based decoder architecture , h t is used to apply attention over encoder outputs and aggregate them to get the context vector c t . a t i = softmax ( f mlp ( [ h t , h i ] ) ) , ( 2 ) c t = m i=1 a t i h i . ( 3 )
The distribution of generating token y t is given by : P gen ( y t ) = softmax ( W gen [ h t , c t ] ) . ( 4 )
Word Copying
The copy mechanism is shown to be effective in DST ( Lei et al. , 2018 ; Xu and Hu , 2018 ; Wu et al. , 2019 ) .
Here , we follow Wu et al . ( 2019 ) to augment the vanilla attention - based decoder with pointergenerator copying , enabling it to capture slot values that explicitly occur in the dialogue history .
P wc ( y t = w ) = i:w i =w a t i . ( 5 ) A soft gate g 1 is used to combine word copying distribution and generative distribution .
g 1 = sigmoid ( W g 1 [ h t , c t , ? emb ( y t?1 ) ] ) , ( 6 ) P orig ( y t ) = g 1 P gen ( y t ) + ( 1 ? g 1 ) P wc ( y t ) . ( 7 )
Slot Connecting Mechanism
As claimed in Section 1 , connecting the target slot with its source slot helps to decrease the reasoning difficulty .
Therefore , we enhance the copyaugmented encoder-decoder model with a slot connecting mechanism to model slot correlations directly .
When decoding the target slot s j , DST - SC infers its source slot from last dialogue states , then copies its value for the final distribution .
Last dialogue states are represented by ( slot , value ) tuples : {( s 1 , v 1 ) , ( s 2 , v 2 ) , ? ? ? , ( s n , v n ) } .
We use h 0 as the query to attend the potential source slot .
a k = softmax ( f mlp ( [ h 0 , s k ] ) ) , ( 8 ) where s k is the summed slot embedding , k ? { 1 , 2 , how related s k is to the target slot s j .
It is computed only once at the first decoding step and maintained consistency to subsequent tokens in the value v k .
At the t-th decoding step , the t-th token v kt contributes to form value copying distribution P vc ( y t ) .
w 1 w m ? ? s 1 v 1 s 2 v 2 s n v n ?
P vc ( y t = w ) = k : v kt =w a k .
( 9 ) Similar to the copy - augmented decoder , we combine value copying distribution and original distributions using a soft gate g 2 to get final output distribution .
g 2 = sigmoid ( W g 2 c 0 ) , ( 10 ) P ( y t ) = g 2 P vc ( y t ) + ( 1 ? g 2 ) P orig ( y t ) . ( 11 ) 3 Experimental Setup
Datasets
To evaluate the effectiveness of DST - SC , we conducted experiments on MultiWOZ 2.0 ( Budzianowski et al. , 2018 ) and MultiWOZ 2.1 datasets ( Eric et al. , 2019 ) . MultiWOZ 2.0 is a multi-domain dialogues corpus , and some annotation errors are corrected in MultiWOZ 2.1 .
Baselines
We compare DST - SC with several baseline methods .
FJST and HJST ( Eric et al. , 2019 ) apply a separate feed -forward network to classify for every single state slot .
HyST ) is a hybrid approach , which combines the joint tracking fixed vocabulary approach and open vocabulary approach .
COMER ( Ren et al. , 2019 ) adopts three hierarchically stacked decoders to generate dialogue states .
TRADE
( Wu et al. , 2019 ) generates dialogue states from the dialogue history using a copy-augmented decoder .
Implementation Details
In our experiments , we used Glove ( Pennington et al. , 2014 ) and character embeddings ( Hashimoto et al. , 2017 ) to initialize word embeddings , each word is represented by a 400 - dimensional vector .
The hidden sizes of all GRU layers are set to 400 .
In the training phase , we used ground truth prior-turn dialogue states in the slot connecting mechanism .
Adam optimizer ( Kingma and Ba , 2015 ) is applied with 0.001 learning rate initially .
The learning rate then reduced by a factor of 0.2 , and the training stopped early when the performance in validation set was not improved for 6 consecutive epochs .
We used a batch size of 32 and dropout rate of 0.2 .
Greedy search strategy is used for decoding , with maximum 10 decoded tokens and 50 % probability of teacher forcing .
Also , we followed previous work to utilize our model with the word dropout ( Wu et al. , 2019 ) by masking input tokens with a 20 % probability .
All experiments are averaged across 3 seeds .
Results and Analysis
Experimental Results
We follow previous work to compare the performance of joint goal accuracy .
We get the joint goal correct if the predicted state exactly matches the ground truth state for every slot .
As shown in MultiWOZ 2.0 and MultiWOZ 2.1 , with the joint goal accuracy of 52.24 % and 49.58 % .
Related - slot Tests
We conducted further related - slot tests to verify the effectiveness of DST - SC in solving the relatedslot problem .
The dataset for related - slot tests is constructed by manually extracting dialogues with the related - slot problem from MultiWOZ 2.1 test set .
We made an observation that slot connections are common at target slots such as attraction - area , hotel - area , hotel - book day and so on .
We only need to focus on target slot accuracy of turns with slot connections .
However , some target slots occur infrequently in the extracted dataset .
Considering that target slots from different domains with the same slot type always correspond to similar slot connection expressions , we can neglect their domains and calculate the accuracy of each slot type instead .
For example , we can calculate the accuracy of slot type price instead of calculating the accuracy of hotel - price range and restaurant - price range separately .
Table 3 lists slot types and their corresponding target slots .
To make more convincing tests , we performed data augmentations to get more samples for each slot type .
We used two heuristic rules to augment the extracted data and obtained 100 dialogues for each slot type .
( 1 ) Paraphrasing : we rewrote some utterances to get multiple phrases with the same intent .
For example , the phrase " in the same area as the restaurant " can be rewritten as " close to the restaurant " .
( 2 ) Replacing values : we replaced some slot values to exclude the influence of overfitting .
For example , the phrase " stay in the east " can be replaced as " stay in the west " .
Slot Type Target Slots
As shown in Table 4 , DST - SC outperforms TRADE by a large margin at most slot types .
Case 1 in Table 5 illustrates the advantage of DST - SC explicitly .
We find that both generation and word copying miss the correct token .
However , the slot connecting mechanism in DST - SC helps to find out the correct source slot and merges its value into P under the control of gate g 2 .
Note that there are no obvious improvements on slot types departure and destination .
We suspect that this is caused by lots of missing annotations for attraction - name , hotel - name and restaurant - name , which usually act as source slots for departure and destination .
The absence of these critical information makes DST - SC pay less attention to values from source slots .
As shown in case 2 in Table 5 , even if the slot connection mechanism has inferred the correct source slot , the unconfidence of g 2 leads to the final incorrect output .
Related Work Traditional approaches for dialogue state tracking ( Henderson et al. , 2014 b ; Sun et al. , 2014 ; Zilka and Jurc?cek , 2015 ; Mrk?i? et al. , 2015 ) rely on manually constructed semantic dictionaries to extract features from input text , known as delexicalisation .
These methods are vulnerable to linguistic variations and difficult to scale .
To overcome these problems , Mrk?i? et al. ( 2017 ) tion learning ability .
By sharing parameters among slots Zhong et al. , 2018 ; Nouri and Hosseini-Asl , 2018 ) , the model is further improved to track rare slot values .
These approaches are all designed in the paradigm of multi-class classification over predefined slot value candidates and usually referred to as fixed vocabulary approaches .
Fixed vocabulary approaches always require a predefined ontology , which is usually impractical .
Their applications are usually limited in a single domain .
Therefore , several open vocabulary approaches in generative fashion ( Xu and Hu , 2018 ; Wu et al. , 2019 ; Ren et al. , 2019 ) are proposed to handle unlimited slot values in more complicated dialogues .
Open vocabulary models show the promising performance in multidomain DST .
However , ellipsis and reference phenomena among multi-domain slots are still less explored in existing literature .
Conclusion
In this paper , we highlight a regularly appeared yet rarely discussed problem in multi-domain DST , namely the related - slot problem .
We propose a novel dialogue state tracking model DST - SC , which equips with the slot connecting mechanism to build slot connections across domains .
Our model achieves significant improvements on two public datasets and shows effectiveness on relatedslot problem tests .
Annotations complement for MultiWOZ dataset in the future might enable DST - SC to handle the related - slot problem more effectively and further improve the joint accuracy .
Figure 1 : 1 Figure1 : DST -SC model architecture ( best viewed in color ) .
Three processing flows leading to P gen , P wc , P vc are respectively generation ( brown ) , copying from dialogue history ( green ) , copying from last dialogue states ( purple ) .
