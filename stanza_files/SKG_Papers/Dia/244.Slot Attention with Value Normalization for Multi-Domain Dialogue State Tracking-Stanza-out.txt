title
Slot Attention with Value Normalization for Multi-Domain Dialogue State Tracking
abstract
Incompleteness of domain ontology and unavailability of some values are two inevitable problems of dialogue state tracking ( DST ) .
Existing approaches generally fall into two extremes : choosing models without ontology or embedding ontology in models leading to over-dependence .
In this paper , we propose a new architecture to cleverly exploit ontology , which consists of Slot Attention ( SA ) and Value Normalization ( VN ) , referred to as SAVN .
Moreover , we supplement the annotation of supporting span for MultiWOZ 2.1 , which is the shortest span in utterances to support the labeled value .
SA shares knowledge between slots and utterances and only needs a simple structure to predict the supporting span .
VN is designed specifically for the use of ontology , which can convert supporting spans to the values .
Empirical results demonstrate that SAVN achieves the state - of - the - art joint accuracy of 54.52 % on MultiWOZ 2.0 and 54.86 % on MultiWOZ 2.1 .
Besides , we evaluate VN with incomplete ontology .
The results show that even if only 30 % ontology is used , VN can also contribute to our model .
Introduction Dialogue state tracking ( DST ) is a core component in the pipeline - based task - oriented dialog systems .
The goal of DST is to extract the dialogue states which are indicated by a set of ( domain , slot , value ) triples during conversation .
The ( domain , slot , value ) triple represents that previous conversation involves the slot of the domain and the specific content is the value .
For example , as shown in Figure 1 , ( restaurant , price , expensive ) triple means that user wants to reserve an expensive restaurant .
A highquality DST model plays a significant role in the * Corresponding Author .
dialogue system , because the dialogue states determine the next system action ( Chen et al. , 2017 ) .
Traditional DST approaches generally rely on ontology already defined , where all slots and their possible values are given .
With a predefined ontology , DST is simplified to a classification problem .
The goal is to choose the most appropriate value from ontology for the slot Zhong et al. , 2018 ) .
However , in practical applications , a complete ontology is almost impossible to be defined in advance .
To overcome the drawback , span-based ( Xu and Hu , 2018 ; and generation approaches spring up .
The second problem is that some values required by DST cannot be found in utterances due to the diverse descriptions during a conversation .
As shown in Figure 1 , value expensive was expressed as high end in the first turn .
The problem gives rise to the powerlessness of span-based approaches .
Recently , Zhang et al. ( 2019 ) show a dual strategy that combines the advantages of both the picklist- based and span-based methods .
They use ontology in spanbased approaches to deal with the problem and achieve the SOTA performance , which also shows that the ontology is powerful .
introduced a largescale multi-turn dialogue dataset ( MultiWOZ ) spanning over several domains and topics .
As shown in Figure 1 , the user initially wants to make a restaurant reservation , then requests information about attractions close to the restaurant , and finally books a taxi .
During the conversation , the models for DST should determine whether each ( domain , slot ) pair has a value in each turn to obtain the most relevant ( domain , slot , value ) triples .
However , unlike single domain DST problems , in which only a few slots need to be tracked , such as four slots in WOZ , there are a total of 30 ( domain , slot ) pairs of five domains in MultiWOZ , which
User System Dialogue state I would like to find a high end restaurant in the center of the city .
( restaurant , price , expensive ) ( restaurant , area , centre )
There is an expensive restaurant called the Ugly Duckling .
Can i get a reservation for 7 at 14:00 this coming Friday ?
( restaurant , price , expensive ) , ? ( restaurant , name , the Ugly Ducking ) , ( restaurant , people , 7 ) , ( restaurant , time , 14:00 ) , ( restaurant , day , Friday ) I have successfully booked your reservation .
Your reference number is ynceb914 .
Will this be all ?
I am also looking for some entertainment close to the restaurant .
( restaurant , price , expensive ) , ? ( attraction , area , centre )
Is there any type of attraction you would like me to search ?
Why do not you try an architectural attraction .
( restaurant , price , expensive ) , ? ( attraction , type , architectural ) All Saints Church looks good , would you like to head there ?
That sounds good .
( restaurant , price , expensive ) , ? ( attraction , name , All Saints Church )
Is there anything else I can help you ?
I also need to book a taxi between the restaurant and the church .
( restaurant , price , expensive ) , ? ( taxi , departure , the Ugly Ducking ) , ( taxi , destination , All Saints Church )
What time would you like the taxi from the Ugly Ducking ?
20:00 , please .
( restaurant , price , expensive ) , ( restaurant , area , centre ) , ( restaurant , name , the Ugly Ducking ) , ( restaurant , people , 7 ) , ( restaurant , time , 14:00 ) , ( restaurant , day , Friday ) , ( attraction , area , centre ) , ( attraction , name , All Saints Church ) , ( taxi , departure , the Ugly Ducking ) , ( taxi , destination , All Saints Church ) , ( taxi , leaveAt , 20:00 ) Figure 1 : An example of dialogue state tracking in a conversation .
Each turn contains a user utterance ( left ) and a system utterance ( right ) .
The blue words are supporting spans in the utterances and new ( domain , slot , value ) triples in the Dialogue state .
In each turn , the DST models need to track slot values mentioned by the user for all the ( domain , slot ) pairs .
can be more in practical applications .
Therefore , it requires DST models should determine the slots efficiently .
To tackle these challenges , we emphasize that DST models should optimize the structure of slots determination and utilize ontology more flexibly rather than abandon it .
In this paper , we propose to divide the model of DST into Slot Attention ( SA ) and Value Normalization ( VN ) .
Simple and efficient processing of slots and flexible use of ontology are the main advantages of SAVN .
Contributions in this work are summarized as ? : ?
SA shares knowledge between slots and utterances and is able to optimize the determination of all slots jointly .
Compared to the span-based approach in DS - DST ( Zhang et al. , 2019 ) , SA improves efficiency by nearly count( slots ) times in determining the slots .
?
Considering that the number of possible slot values in ontology could be large in the real scenario , VN is designed as a simple , flexible , and effective model to use an ontology , which only needs 8 minutes for training on a V100 GPU .
VN can choose to directly output the supporting span from SA or select a value in the ontology .
?
We supplement the annotation of supporting ?
The code will be released at https://github.com/ wyxlzsq / savn . span for labeled values unavailable in utterances on MultiWOZ 2.1 , which can help the span-based model learn semantics more fully and help ontology be better utilized .
?
We fully evaluate VN with incomplete ontology .
The results show that VN can gain positive performance for SAVN as long as the integrity of ontology is more than 30 % .
And as we expected , the more complete ontology is , the more VN can rely on it .
SAVN model
The overview of the model is shown in Figure 2 , which consists of Slot Attention and Value Normalization .
SA outputs the Supporting sPan ( SP ) from utterances for each ( domain , slot ) pair .
And VN chooses to output supporting span directly or convert supporting span to the value in ontology according to the gate .
Slot Attention
As shown in Figure 3 , Slot Attention ( SA ) accepts two inputs , one of which is the text of the previous conversations , and the other is a list of ( domain , slot ) pairs .
Similar to DS - DST model ( Zhang et al. , 2019 ) , we also employ BERT ( Devlin et al. , 2019 ) as the encoder for utterances .
The difference is that we separate slots and utterances and share knowledge between them .
Then we directly use the inner product to predict the span in utterances and use an attention module to interact with slots and utterances to classify .
Benefiting from this structure , our model can determine the slots in parallel , optimize the determination jointly , and only needs to encode the utterances once for each turn while DS - DST needs to encode count( slots ) times .
Additionally , for SA to have the ability to output some special words , we added some fixed candidate values in front of the utterances such as yes , no .
Let us define X = {( u 1 , r 1 ) , ... , ( u n , r n ) } as the set of user utterances and system responses in a conversation with n turns , C = [ a 1 , a 2 , ... , a k ] as the list of k fixed candidate values , and S = [ s 1 , s 2 , ... , s j ] as the list of j ( domain , slot ) pairs .
Due to the limitation of the maximum sequence length of BERT , sometimes it is not possible to encode all utterances .
Therefore , we set a parameter m to limit the number of turns entered .
The input utterances for turn t should be : X m t = [ U 1 , ... , U t?1 , u t ] if t ? m [ U t?m + 1 , ... , u t ] otherwise , ( 1 ) where U 1 represents u 1 ? r 1 , the ? means to con-catenate the utterances of u 1 and r 1 . r t is the system response of turn t , so r t / ?
X m t .
Then by encoding the utterances of turn t by BERT and embedding the slots by the Embedding module of BERT , we can get the hidden states of utterances and slots as follows : I = C ? X m t , H u t = BERT ( I ) , E s t = Embedding ( S ) , H s t = MeanPooling ( E s t ) , ( 2 ) where H u t ?
R p?h and H s t ?
R q?h . p is the sequence length of I , q is the number of ( domain , slot ) pairs and h is the dimension of the BERT hidden state .
Slot Gate Classification
As introduced in Section
1 , There are many ( domain , slot ) pairs in Multi-domain DST problem , which make it more challenging than singledomain DST problem .
Similar to TRADE model , we design a classification module with none , dontcare and span as a slot gate .
For each ( domain , slot ) pair , if the slot gate predicts none or dontcare , we ignore the span predicted from utterances and fill the pair with " none " or " do not care " .
The module to classify slots is similar to a Transformer ( Vaswani et al. , 2017 ) block .
We employ the " Scaled Dot-Product Attention " to get an utterances - aware slot representation A s u : Q t = H s t ?
W q + b q , K t = H u t ?
W k + b k , V t = H u t ?
W v + b v , A s u = Softmax ( Q t K t T ? d k ) V t , ( 3 ) where A s u ?
R q?h .
Then in order to better integrate the states of slots and utterances , we add A s u and H s t to get H c as the features to classify : H c = GELU ( H c ? W c + b c ) , G t = Softmax ( H c ? W l + b l ) ? R q?3 , ( 4 ) where G t is the slot gates of all ( domain , slot ) pairs at turn t.
Span-Based Value Prediction
For each ( domain , slot ) triple , span-based methods obtain the value by predicting a span with start and end position in utterances .
In order to make the slot determination more efficient , we simplify the structure of span-based predictions .
We can get the span predictions by : D s t = H s t ?
W s + b s , D e t = H s t ?
W e + b e , P s t = Softmax ( D s t ?
( H u t ) T ) ?
R q?p , P e t = Softmax ( D e t ?
( H u t ) T ) ?
R q?p , ( 5 ) where P s t and P e t are the start position distributions and end position distributions of all ( domain , slot ) pairs at turn t respectively .
Optimization
We can optimize all slots determination jointly .
The joint losses at turn t are as follows : L g = Q q=1 ?log ( G q ? ( y g q ) T ) , L s = Q q=1 ?log ( P s q ? ( y s q ) T ) , ( 6 ) where L g is the loss of the slot gate predictions , L s and L e are the loss of the start and end position predictions respectively .
And Q is the number of ( domain , slot ) pairs , y is the true one- hot label .
Similar to L s , we can get the end loss L e .
Then we optimize the weighted - sum of these three loss functions using hyper-parameters ? and ? , L = ?L g + ?( L s + L e ) . ( 7 )
Value Normalization Value Normalization is a flexible module for utilizing ontology , which can also be combined with other DST models .
Considering that there are numerous possible values in the ontology and few data for training , we design a simple and effective model for VN , which can also benefit from the pre-trained BERT model .
As shown in Figure 4 , VN is designed with one layer of the transformer block , which we call VN 1 .
By analogy , we can get VN 4 and VN 12 ( i.e. , use BERT - base model as encoder ) .
The model will load parameters from the corresponding layers of BERT .
In Section 4.2 , the experimental results show that VN 1 has done well enough for the Mul-tiWOZ dataset .
Let us define T 0 is the hidden state of the first token ( [ CLS ] ) after transformer .
Then we use T s 0 ?
R h as the hidden state of the supporting span after encoding and T o 0 ? R n?h as the hidden states of n possible values in ontology for the corresponding ( domain , slot ) pair .
We use the inner products of the supporting span and the possible values as the matching scores , which is defined as : M = Softmax ( T s 0 ? ( T o 0 ) T ) ? R n . ( 8 ) Then we can get the max matching value .
In addition , we employ the cosine similarity as the value gate since the ontology may be incomplete .
the final output is : cos( T V m 0 , T s 0 ) = T V m 0 ? ( T s 0 ) T T V m 0 ( T s 0 ) T , ( 9 ) output = V m cos( T V m 0 , T s 0 ) > ? SP cos( T V m 0 , T s 0 ) ? ? , ( 10 ) where V m is the max matching value , SP is the supporting span and ? is a hyper-parameter .
Our loss function for optimizing VN is defined as follows : L i = ?log ( M ? ( y v ) T ) r = 1 max( cos ( T o 0 , T s 0 ) ) + 1 r = 0 , ( 11 ) where y v is the true one- hot label and r = 1 means the value for the supporting span is in ontology .
However , r will always be equal to 1 without preprocessing in training because ontology is invariably complete for the training set .
In our experiments , we employ full training set to train VN with incomplete ontology in order to get dispersed vector representations for values .
Annotation for Supporting Span Our annotation work is based on the MultiWOZ 2.1 dataset ( Eric et al. , 2019 ) , which is a fixed version of the MultiWOZ 2.0 dataset .
MultiWOZ 2.1 dataset is a large-scale collection of human-human written conversations over multiple domains and topics , which has labeled 63,662 ( conversation , domain , slot , value ) quadruples ( except " none " value ) in the training set .
Annotation for supporting span is mainly to address the problem that some labeled value can not be found in the conversations .
The causes of this problem can be divided into three categories : varied expressions , spelling mistakes , and annotation errors .
The criterion of annotations is to find the shortest span in the conversations , which can help us get the labeled value .
Based on the criterion , we annotate 936 ( supporting span , value ) pairs on MultiWOZ 2.1 training set , in which varied expressions account for 637 ( 68 % ) , spelling mistakes account for 123 ( 13 % ) , and annotation errors account for 176 ( 19 % ) .
Table 1 shows some examples of supporting span annotation .
After annotation , we can change ( domain , slot , value ) triples in training set to ( domain , slot , supporting span , value ) quadruples , where the supporting span will be equal to the value if the value can be found in the conversations .
Then we employ ( domain , slot , supporting span ) triples to train SA and ( supporting span , value ) pairs to train VN .
Specifically , we do not use the annotation of annotation errors to train VN , for it should not convert Saturday to Sunday .
Experiments
We evaluate our model on two publicly available datasets : MultiWOZ 2.0 and MultiWOZ 2.1 , both of which are fully - labeled task - oriented corpora comprised of human-human written conversations and contain 8,438 multi-turn dialogues with each dialogue having 13.68 turns on average in training set .
The difference between MultiWOZ 2.0 and MultiWOZ 2.1 is that MultiWOZ 2.1 has changed more than 32 % of state annotations across 40 % of the dialogue turns to fix the noisy state annotations in MultiWOZ 2.0 ( Eric et al. , 2019 ) .
Following previous work , only five domains ( i.e. , restaurant , hotel , attraction , taxi , and train ) are employed in our experiments because the dialogues that belong to the other two domains ( i.e. , hospital and police ) are rare in the training set and do not appear in the test set .
As introduced in Section 3 , we get a new training set by using the supporting span annotations , which can be called the SP training set .
Additionally , there are no changes to the test set and the dev set .
Training Details
We use the pre-trained BERT - base-uncased model as the utterance encoder in SA , which has 12 hid - den layers with 768 units .
For the limitation of the maximum sequence length , We set m ( in equation 1 ) to 9 .
If the current conversation turn exceeds m , we will combine the predicted dialogue states with the previous dialogue states to complete dialogue states for the current turn .
In our experiments , SA and VN are both trained with Adam optimizer ( Kingma and Ba , 2014 ) in which the learning rate linearly decreases from 5e ?5 and 1e ?4 , respectively .
We have trained SA with 3 epochs and VN with 5 epochs both on Multi-WOZ 2.0 and MultiWOZ 2.1 .
Specifically for VN , we train VN 1 and VN 12 ( introduced in Section 2.2 ) to compare their performance .
Our results can be reproduced with a 16 GB V100 GPU in 2 hours ( 8 minutes for VN 1 ) .
Results
Two standard metrics , joint accuracy and slot accuracy , can be employed to evaluate the performance of our model .
Joint accuracy is the accuracy of dialogue states , which requires that all ( domain , slot , value ) triples in the dialogue states are predicted correctly .
And slot accuracy is the accuracy of ( domain , slot , value ) triples , which requires that the predicted value of ( domain , slot ) pair is predicted correctly .
The joint accuracy is a more challenging metric , for there is a considerable number of ( domain , slot ) pairs in dialogue states .
To better evaluate the role of supporting spans , we have trained two versions of SA , one of which utilizes the original training set called SA raw and the other employs the SP training set called SA sp .
And we make a comparison with the following existing models : ? GLAD ( Zhong et al. , 2018 ) among slots by virtue of global modules and applies the local modules to learn slot-specific features .
? Neural Reading formulates DST as a reading comprehension task .
The model encodes the word tokens by a pretrained BERT model , then obtains the contextual representation by LSTM .
?
SUMBT learns the slot and utterance representations by fine-tuning a pretrained BERT model .
Then they compute the similarity between possible values and utterances via a slot-utterance matching module .
?
TRADE employs an encoderdecoder architecture to generate the values for slots from the vocabulary and the dialogue history .
?
DSTQA ( Zhou and Small , 2019 ) models DST as a question answering problem , which generates a question to ask for the value of the slot at each turn .
? DS -DST ( Zhang et al. , 2019 ) proposes a Dual Strategy to combine the advantages of the picklist- based and span-based methods , which has been evaluated individually as DST - Picklist and DST - Span .
On MultiWOZ 2.0 , as shown in Table 2 , our model achieves the highest performance , 54.52 % of joint accuracy in which VN gains 6.08 % absolute improvement .
And on MultiWOZ 2.1 , as shown in Table 3 , our model also achieves the highest performance , 54.86 % of joint accuracy in which ? 10 % 20 % 30 % 40 % 50 % 60 % 70 % 80 % 90 % 100 % -1.0 8.41 12.05 13.19 14.67 19.46 22.41 31.49 38.50 46.76 54.86 -0.5 33.35 22.30 17.48 20.15 20.89 26.97 31.84 38.68 46.76 54.86 0 33.35 25.83 25.99 24.86 22.58 29.61 36.43 41.67 48.29 54.86 0.3 33.35 30.20 33.66 38.18 39.81 48.60 48.75 51.43 53.5 54.86 0.5 33.44 35.71 43.44 46.88 49.43 49.81 51.40 51.87 53.57 54.86 0.7 34.03 40.64 45.97 47.19 48.14 49.42 51.28 49.55 51.18 54.86 0.9 33.44 41.76 45.39 46.10 46.90 47.84 48.19 46.52 47.43 52.10 1.0 45.74 45.74 45.74 45.74 45.74 45.74 45.74 45.74 45.74 45.74 Table 4 : The performance of SAVN with incomplete ontology on MultiWOZ 2.1 .
The percentage in the header refers to the usage rate of ontology and the ? is introduced in Equation 10 . VN gains 9.12 % absolute improvement .
Combining results from two tables , we demonstrate that the performance of SA is similar to TRADE and SA has about 5 % higher absolute performance than DST - span , which is also a span-based method using BERT .
Comparing SA raw with SA sp , we find that their performance is similar without VN and the improvement of SA sp performance is obviously greater than that of SA raw by VN , which shows that SA sp has learned more about semantics so that it could output the supporting span and can be better combined with VN .
Furthermore , by comparing VN 1 with VN 12 , we prove that VN has enough performance for the MultiWOZ dataset with only one transformer layer .
Incomplete Ontology
The experimental results in Table 2 and Table 3 show that ontology is a powerful resource for DST .
However , it is impractical to get a full ontology in advance when the DST model is oriented to practical applications , which leads some models , such as TRADE , to abandon ontology .
In this section , We choose SA sp on MultiWOZ 2.1 as the base model to evaluate the performance of VN 1 with incomplete ontology .
There are many slots in the ontology .
We can divide them into two categories , common-value slots and special - value slots .
Common-value slots , such as hotel - price and hotel - type , are able to include all possible values as long as a few values are given .
And for special - value slots , such as restaurant - name and taxi- departure , it is difficult to cover all possible values by predefined values .
In our experiment , we only drop out values in specialvalue slots and always keep all values of commonvalue slots .
The results are shown in Table 4 . Even if there is only 30 % ontology , VN can bring positive performance as long as ? is appropriate .
Based on the results , we demonstrate that the performance of VN has steadily improved with the increased usage rate of ontology .
Furthermore , the more complete ontology is , the smaller ?
can be , which means VN can be more dependent on ontology .
Error Analysis
An error analysis of SA sp with VN 1 on MultiWOZ 2.1 is shown in Figure 5 .
The three slots with the highest error rates are restaurant - name with 6.37 % , hotel - type with 6.08 % and attraction - name with 5.93 % .
Through the detailed analysis of error samples , we observe many labeled states do not include the name that only appears in system response .
These states are similar to the example in Figure 1 with the restaurant -name and attractionname removed .
Once the difference occurs , it will lead to errors in the subsequent dialogue states , resulting in high error rates .
And the labels of hotel - type are found to be confusing .
For instance , for the sentence " I am looking for a hotel with ... " , sometimes the label of hotel - type is hotel and sometimes it is none .
Compared with SA , SAVN has significantly lower error rates on attraction -type and attractionname .
The improvement of attraction - name is mainly due to the repair of spelling mistakes , and the improvement of attraction - type mainly benefits from the normalization of varied expressions .
It is worth mentioning that VN can not improve the accuracy of some slots , which only need to be filled with yes or no , such as hotel - internet and hotel - parking .
Related Work Traditional dialogue state tracking models extract utterance semantics by hand -crafted features and complex domain-specific lexicons ( Wang and Lemon , 2013 ; Williams , 2014 ; Henderson et al. , 2014 ) to predict the dialogue states , which is hard to adapt to new domains .
Then , to overcome this drawback , propose a novel Neural Belief Tracking ( NBT ) framework with learning n-gram representation of utterance by using a convolutional neural network , and achieve better performance .
At the same time , Models for multi-domain DST have then been proposed .
Rastogi et al. ( 2017 ) build a multi-domain DST model by two -layer bi-GRU and Ramadan et al . ( 2018 ) track domain and the dialogue states jointly through multiple bi-LSTM .
They employ semantic similarity between utterances and the values in ontology and allow the knowledge to be shared across domains .
To transfer knowledge between slots , Zhong et al . ( 2018 ) propose a global-local architecture to share parameters among slots and Ren et al . ( 2018 ) propose StateNet that shares all parameters among slots and fix the word embeddings during training to handle new slots .
After the pre-trained BERT model showed superior performance , encoding by BERT has become the mainstream .
encode the slots and utterances with BERT , and then compute the similarity between possible values and utterances after a Multi-head attention layer .
And Zhang et al. ( 2019 ) also employ BERT to encode the utterances .
The difference is that they combine the picklistbased and span-based methods and get higher performance .
In order to eliminate the dependence on ontology , propose an encoderdecoder architecture with a pointer network to generate the value for each slot .
And Zhou and Small ( 2019 ) formulate multi-domain DST as a question answering problem and learn relationships between slots by a dynamically - evolving knowledge graph .
Most recently , Heck et al . ( 2020 ) propose to use copy mechanisms to fill slots with values , which combine span-based methods with memory methods to avoid the use of value picklists .
Conclusion
We introduce a new architecture that divides the prediction of slots and the use of ontology .
SA shares parameters not only among all slots but also between slots and utterances .
And VN can handle ontology flexibly with a simple and effective structure , which is able to work with incomplete ontology .
Combining SA with VN , SAVN has shown excellent performance on both MultiWOZ 2.0 and MultiWOZ 2.1 .
And we also introduce the annotation of supporting span .
In future work , the supporting span annotation can be added to the datasets of a task - oriented dialog system , for the reason that supporting span serves as a bridge between diverse descriptions of users and the normative values in the system .
Furthermore , DST models with supporting span allow for a fairer comparison regardless of whether the ontology is used .
Figure 2 : 2 Figure 2 : The overview of Slot Attention with Value Normalization for Dialogue State Tracking .
