title
Zero - Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking
abstract
Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition .
This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain .
We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT - based SUMBT model on the MultiWOZ 2.1 dataset .
We show training with only synthesized in- domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset .
We improve the zero-shot learning state of the art on average across domains by 21 % .
Introduction Automated conversational agents can reduce the costs of customer support , a necessary service in just about every business .
However , training a goal-directed dialogue agent for a domain often requires acquiring annotated dialogues to cover all possible conversation flows .
Commonly , this is done using the Wizard - of - Oz technique ( Kelley , 1984 ) , where two crowdsource workers converse with each other , while also annotating the state at each turn .
This technique has been employed to construct several datasets ( Hemphill et al. , 1990 ; . Recently , it has been used to build the MultiWOZ dataset ( Budzianowski et al. , 2018 ) , a large corpus of dialogues across 7 domains .
Unfortunately , not only is the initial acquisition expensive , annotating dialogues correctly has proven to be challenging due to human errors , delays in annotation , inconsistent conventions , and normalization issues S : START U : Can you help with information regarding a food place ?
I need to book at 15:45 .
S : SEARCHREQUEST restaurant ( book time = " 15:45 " )
A : How about the restaurant with name La Tasca and Italian food ?
U : Can you find something which serves seafood ?
S : SEARCHREQUEST restaurant ( book time = " 15:45 " , food = " seafood " )
A : What date are you looking for ?
U : Thursday please .
S : SEARCHREQUEST restaurant ( book time = " 15:45 " , food = " seafood " , book day = " thursday " )
A : How about the Copper Kettle ?
It is a food place with seafood food .
U : What is the price range and the area ?
S : SLOTQUESTION restaurant ( book time = " 15:45 " , food = " seafood " , book day = " thursday " , price range =? , area =? )
A : The Copper Kettle is a moderately priced restaurant in the north of the city .
Would you like a reservation ?
U : No , thanks .
S : CLOSE restaurant ( book time = " 15:45 " , food = " seafood " , book day = " thursday " )
A : Can I help with you anything else ?
U : Thank you , that will be it for now .
S : END restaurant ( book time = " 15:45 " , food = " seafood " , book day = " thursday " )
Figure 1 : An example of a dialogue that can be synthesized from our templates .
' U : ' indicates the user , ' A : ' the agent , and ' S : ' is the dialogue state at each turn .
Small , 2019 ) .
The MultiWOZ dataset still has significant inconsistencies ( Zhou and Small , 2019 ) despite having been constructed through multiple rounds of annotations ( Budzianowski et al. , 2018 ; .
We observe empirically from the MultiWOZ training data that conversations in all the domains follow the same pattern : the agent and user start by greeting each other , tehn they converse to find a proposal that satisfies the user , the user provides additional required information , and finally the agent completes the user 's transaction .
To facilitate transfer learning , we create an abstract model of dialogues that is independent of the domain of the conversation .
In this paper we will focus on dialogues for transactions ; other kinds of dialogues such as opinion sharing will have different models .
We have developed an algorithm that accepts an ontology of a domain and a few phrases commonly used in that domain .
The algorithm synthesizes dialogue training data based on an abstract dialogue model .
The dialogue synthesized consists of turns of conversation , each of which has a start state , an agent utterance , a user utterance , and an end state .
The start and end states summarize the semantics of the conversation at those points .
An example of a dialogue that can be synthesized by our model is shown in Fig.
1 .
To transfer knowledge to a new domain in a zero-shot setting , we train with the synthesized data for the new domain together with existing data for other domains .
In addition , we adapt training samples from related domains by substituting them with the vocabulary of the new domain .
We can improve the accuracy of the abstract dialogue model as well as the state- tracking neural network by iteratively refining the model based on the error analysis on the validation data , and by introducing additional annotations in the new domain .
Note that the abstract dialogue model can be also used directly to implement the agent itself .
The contributions of this paper are as follows : ?
A new zero-short transfer learning technique for dialogue state tracking where the indomain training data are all synthesized from an abstract dialogue model and the ontology of the domain .
?
Our approach improves over the previous state - of - the - art result on zero-shot transfer learning for MultiWOZ 2.1 tasks by 21 % on average across domains .
?
We show that our approach improves the accuracy for TRADE ( Wu et al. , 2019 ) , an RNN - based model , and SUMBT , a BERT - based model ( Devlin et al. , 2019 ) , suggesting that our technique is independent of the specific model used .
Related Work Dialogue Datasets and Synthesis .
Synthesized data ( in training and evaluation ) was proposed by Weston et al . ( 2015 ) to evaluate the ability of neural models to reason compositionally , and was also used in visual question answering ( Johnson et al. , 2017a ; Hudson and Manning , 2019 ) and semantic parsing ( Lake and Baroni , 2018 ) .
Wang et al. ( 2015 ) proposed synthesizing data , then crowdsourcing paraphrases to train semantic parsers .
Various semantic parsing datasets have been generated with this technique Zhong et al. , 2017 ) and the technique has also been adapted to the multiturn setting ( Cheng et al. , 2018 ; .
While it tends to be well - annotated , paraphrase data is expensive to acquire , and these datasets are very small .
More recently , we proposed training with both a large amount of synthesized data and a small amount of paraphrase data for semantic parsing of single sentences ( Campagna et al. , 2019 ; Xu et al. , 2020 ) .
We showed that training with such data can perform well on real-world evaluations .
This paper extends this work to the multi-turn setting .
Dialogues are more complex as they need to capture information , such as the abstract dialogue state , that is not present in the target annotation ( domain and slot values ) .
We extend the synthesis algorithm to operate based on a dialogue model , tracking enough information to continue the dialogue .
We also present a novel dialogue model that is suitable for synthesis .
Dialogue State Tracking .
Dialogue state tracking is a long-studied field , starting with the first Dialogue State Tracking Challenge ( Williams et al. , 2014 ) .
A review of prior work can be found by Williams et al . ( 2016 ) .
Previous works on DST use different approaches , ranging from using handcrafted features to elicit utterance information Wang and Lemon , 2013 ) . use Convolutional Neural Networks to learn utterance representations .
However , their models do not scale as they do not share parameters across different slots .
Zhong et al. ( 2018 ) and Nouri and Hosseini- Asl ( 2018 ) propose a new global module that shares information to facilitate knowledge transfer .
However , they rely on a predefined ontology .
Xu and Hu ( 2018 ) use a pointer network with a Seq2Seq architecture to handle unseen slot values .
use a pre-trained BERT model ( Devlin et al. , 2019 ) to encode slots and utterances and uses multi-head attention ( Vaswani et al. , 2017 ) to find relevant information in the dialogue context for predicting slot values .
Wu et al . ( 2019 ) introduce an encoder-decoder architecture with a copy mechanism , sharing all model parameters between all domains .
Zhou and Small ( 2019 ) formulate multidomain DST as a question answering task and use reading comprehension techniques to generate the answers by either span or value prediction .
Johnson et al. ( 2017 b ) propose single encoderdecoder models for zero-shot machine translation by encoding language and input sentence jointly , and Zhao and Eskenazi ( 2018 ) propose cross-domain zero-shot language generation using a cross-domain embedding space .
Modelling of Dialogues .
Previous work already proposed general models of dialogues as finite state machines ( Jurafsky et al. , 1997 ; Bunt et al. , 2017 ; . Existing models are optimized for analyzing existing human conversations .
Our dialogue model is the first suitable for synthesis , carrying enough information to continue the dialogue .
Gupta et al. ( 2018 ) previously proposed a different annotation scheme for dialogues , using a hierarchical representation scheme , instead of the more typical intent and slot .
Their work is complementary to ours : our method of dialogue synthesis is applicable to any annotation scheme .
In this paper , we focus on the existing annotation scheme used by the MultiWOZ dataset .
Dialogue -Model Based Synthesis
In this section , we first define abstract dialogue models , then describe how we can generate dialogues based on the model .
We also describe the techniques we use to adapt training dialogues from other domains to the new domain .
Abstract Dialogue Model
We define a dialogue model with finite sets of abstract states , agent dialogue acts , user dialogue acts , and transitions , defined below .
The abstract dialogue for transactions we use in this paper is shown in Table 1 .
The abstract states capture the typical flow of a conversation in that model , regardless of the domain .
For example , a transaction dialogue model has states GREET , SEARCHREQUEST , COMPLETEREQUEST , COMPLETETRANSACTION , and CLOSECONVERSATION , etc .
Each domain has a set of slots ; each slot can be assigned a value of the right type , a special DONTCARE marker indicating that the user has no preference , or a special " ? " marker indicating the user is requesting information about that slot .
Thus , we can summarize the content discussed up to any point of a conversation with a concrete state , consisting of an abstract state , and all the slot-value pairs mentioned up to that point .
Where it is not ambiguous , we refer to the concrete state as the state for simplicity .
All possible agent utterances in a dialogue model are classified into a finite set of agent dialogue acts , and similarly , all the possible user utterances into a finite set of user dialogue acts .
Examples of the former are GREETUSER , ASKQUES-TION , ANSWER , OFFERRESERVATION ; examples of the latter are ASKBYNAME , ADDCONSTRAINTS , ACCEPT , REJECT .
Each transition in the model describes an allowed turn in a dialogue .
A transition consists of an abstract start state , an agent dialogue act , a user dialogue act , and an abstract end state .
Dialogues from an Abstract Model
A dialogue is a sequence of turns , each of which consists of a start state , an agent utterance , a user utterance , and an end state .
We say that a dialogue belongs to a model , if and only if , 1 . for every turn , the start state 's abstract state , the dialogue act of the agent utterance , the dialogue act of the user utterance , and the end state 's abstract state constitute an allowed transition in the model .
2 . the slot-value pairs of each end state are derived by applying the semantics of the agent and user utterances to the start state .
3 . the first turn starts with the special START state , and every turn 's end state is the start state of the next turn , except for the last turn , where the end state is the special END state .
Synthesizing a Turn with Templates
We use templates to synthesize dialogues in a domain from an abstract dialogue model and a domain ontology .
In this paper , we introduce dialogue model templates which specify with grammar rules how to generate a turn of a dialogue from
In this case , the non-terminals NAME , NP , ADJ SLOT are expanded into domain-dependent phrases " Curry Garden " , " Indian restaurant in the south of town " , and " expensive " , respectively , and the results of their semantic functions , name , np , adj slot , are ( sets of ) slot-value pairs : name = " Curry Garden " ; { food = " Indian " , area= " south " } ; price = " expensive " .
The semantic function of SLOTQUESTION checks that the input state does not already include a value for the price slot , and the price is not mentioned by the agent at this turn .
It returns , as the new state , the old state with a " ? " on the price .
All the non-dialogue specific templates are introduced by Xu et al . ( 2020 ) .
We have extended this template library , originally intended for database queries , to return slot-value pairs as semantic function results .
Readers are referred to Xu et al . ( 2020 ) four kinds of domain templates .
Domain Subject Templates describe different noun phrases for identifying the domain .
Slot Name Templates describe ways to refer to a slot name without a value , such as " cuisine " , " number of people " or " arrival time " .
Slot Value Templates describe phrases that refer to a slot and its value ; they can be a noun phrase ( " restaurants with Italian food " ) , passive verb phrase ( " restaurants called Alimentum " ) , active verb phrase ( " restaurants that serve Italian food " ) , adjective - phrase ( " Italian restaurants " ) , preposition clauses ( " reservations for 3 people " ) .
Finally , Information Utterance Templates describe full sentences providing information , such as " I need free parking " , or " I want to arrive in London at 17:00 " .
These are domainspecific because they use a domain-specific construction ( " free parking " ) or verb ( " arrive " ) .
:= " restaurant " : ?() ? ? ADJ SLOT := FOOD | PRICE : ?( x ) ? x PREP SLOT := " in the " AREA " of town " : ?( x ) ? x NAME := " Curry Garden " | . . . : ?( x ) ? name = x FOOD := " Italian " | " Indian " | . . . : ?( x ) ? food = x AREA := " north " | " south " | . . . : ?( x ) ? area = x PRICE := " cheap " | " expensive " | . . . : ?( x ) ? price = x Developers using our methodology are expected to provide domain templates , by deriving them manually from observations of a small number of in-domain human conversations , such as those used for the validation set .
Synthesizing a Dialogue
As there is an exponential number of possible dialogues , we generate dialogues with a randomized search algorithm .
We sample all possible transitions uniformly to maximize variety and coverage .
Our iterative algorithm maintains a fixed - size working set of incomplete dialogues and their current states , starting with the empty dialogue in the START state .
At each turn , it computes a random sample of all possible transitions out of the abstract states in the working set .
A fixed number of transitions are then chosen , their templates expanded and semantic functions invoked to produce the new concrete states .
Extended dialogues become the working set for the next iteration ; unextended ones are added to the set of generated results .
The algorithm proceeds for a maximum number of turns or until the working set is empty .
The algorithm produces full well - formed dialogues , together with their annotations .
The annotated dialogues can be used to train any standard dialogue state tracker .
Training Data Adaptations
We also synthesize new training data by adapting dialogues from domains with similar slots .
For example , both restaurants and hotels have locations , so we can adapt a sentence like " find me a restaurant in the city center " to " find me a hotel in the city center " .
We substitute a matching domain noun phrase with the one for the new domain , and its slot values to those from the target ontology .
We also generate new multi-domain dialogues from existing ones .
We use heuristics to identify the point where the domain switches and we concatenate single-domain portions to form a multidomain dialogue .
Experimental Setting
The MultiWOZ Dataset The MultiWOZ dataset ( Budzianowski et al. , 2018 ; ) is a multi-domain fullylabeled corpus of human-human written conversations .
Its ontology has 35 slots in total from 7 domains .
Each dialogue consists of a goal , multiple user and agent utterances , and annotations in terms of slot values at every turn .
The dataset is created through crowdsourcing and has 3,406 singledomain and 7,032 multi-domain dialogues .
Of the 7 domains , only 5 have correct annotations and any data in the validation or test sets .
Following Wu et al. ( 2019 ) we only focus on these 5 domains in this paper .
The characteristics of the domains are shown in Table 2 .
Machine Learning Models
We evaluate our data synthesis technique on two state - of - the - art models for the MultiWOZ dialogue state tracking task , TRADE ( Wu et al. , 2019 ) and SUMBT Table 2 : Characteristics of the MultiWOZ ontology , the MultiWOZ dataset , the template library , and the synthesized datasets for the zero-shot experiment on the 5 MultiWOZ domains .
" user slots " refers to the slots the user can provide and the model must track , while " agent slots " refer to slots that the user requests from the agent ( such as the phone number or the address ) .
Note that total number of dialogues is smaller than the sum of dialogues in each domain due to multi-domain dialogues .
give a brief overview of each model ; further details are provided in the respective papers .
TRADE TRAnsferable Dialogue statE generator ( TRADE ) uses a soft copy mechanism to either copy slot-values from utterance pairs or generate them using an Recurrent Neural Network ( RNN ) ( Sutskever et al. , 2014 ) decoder .
This model can produce slot-values not encountered during training .
The model is comprised of three main parts : an RNN utterance encoder which generates a context vector based on the previous turns of the dialogue ; a slot-gate predictor indicating which ( domain , slot ) pairs need to be tracked , and a state generator that produces the final word distribution at each decoder time-step .
SUMBT Slot-Utterance Matching Belief Tracker ( SUMBT ) uses an attention mechanism over user-agent utterances at each turn to extract the slot-value information .
It deploys a distancebased non-parametric classifier to generate the probability distribution of a slot-value and minimizes the log-likelihood of these values for all slot-types and dialogue turns .
Specifically , their model includes four main parts : the BERT ( Devlin et al. , 2019 ) language model which encodes slot names , slot values , and utterance pairs , a multi-head attention module that computes an attention vector between slot and utterance representations , a RNN state tracking module , and a discriminative classifier which computes the probability of each slot value .
The use of similarity to find relevant slot values makes the model depend on the ontology .
Thus the model is unable to track unknown slot values .
Software and Hyperparameters
We used the Genie tool ( Campagna et al. , 2019 ) to synthesize our datasets .
We incorporated our dialogue model and template library into a new version of the tool .
The exact version of the tool used for the experiment , as well as the generated datasets , are available on GitHub 1 .
For each experiment , we tuned the Genie hyperparameters separately on the validation set .
For the models , we use the code that was released by the respective authors , with their recommend hyperparameters .
For consistency , we use the same data preprocessing to train both TRADE and SUMBT .
Experiments
Data synthesis
Our abstract transaction dialogue model has 13 abstract states , 15 agent dialogue acts , 17 user dialogue acts , and 34 transitions ( Table 1 ) .
We have created 91 dialogue templates for this model .
Dialogue templates were optimized using the validation data in the " Restaurant " domain .
We also created domain templates for each domain in MultiWOZ .
The number of templates and other characteristics of our synthesis are shown in Table 2 .
To simulate a zero-shot environment in which training data is not available , we derived the templates from only the validation data of that domain .
We did not look at in- domain training data to design the templates , nor did we look at any test data until the results reported here were obtained .
In the table , we also include the domain we chose to perform domain adaptation ( Section 3.5 ) and the number of slots from the adapted domain that are applicable to the new domain .
Note that the validation and test sets are the same datasets as the MultiWOZ 2.1 release .
Evaluation On All Domains
Our first experiment evaluates how our synthesized data affects the accuracy of TRADE and SUMBT on the full MultiWOZ 2.1 dataset .
As in previous work ( Wu et al. , 2019 ) , we evaluate the Joint Accuracy and the Slot Accuracy .
Joint Accuracy measures the number of turns in which all slots are predicted correctly at once , whereas Slot Accuracy measures the accuracy of predicting each slot individually , then averages across slots .
Slot Accuracy is significantly higher than Joint Accuracy because , at any turn , most slots do not appear , hence predicting an empty slot yields high accuracy for each slot .
Previous results were reported on the MultiWOZ 2.0 dataset , so we reran all models on MultiWOZ 2.1 .
Results are shown in Table 3 .
We observe that our synthesis technique , which is derived from the MultiWOZ dataset , adds no value to this set .
We obtain almost identical slot accuracy , and our joint accuracy is within the usual margin of error compared to training with the original dataset .
This is a sanity - check to make sure our augmentation method generates compatible data and training on it does not worsen the results .
Zero - Shot Transfer Learning
Before we evaluate zero-shot learning on new domains , we first measure the accuracy obtained for each domain when trained on the full dataset .
For each domain , we consider only the subset of dialogues that include that particular domain and only consider the slots for that domain when calculating the accuracy .
In other words , suppose we have a dialogue involving an attraction and a restaurant : a prediction that gets the attraction correct but not the restaurant will count as joint-accurate for the attraction domain .
This is why the joint accuracy of individual domains is uniformly higher than the joint accuracy of all the domains .
Table 4 shows that the joint accuracy for TRADE varies from domain to domain , from 50.5 % for " Hotel " to 74.0 % for " Train " .
The domain accuracy with the SUMBT model is better than that of TRADE by between 1 % and 4 % for all domains , except for " Taxi " where it drops by about 4.5 % .
In our zero-shot learning experiment , we withhold all dialogues that refer to the domain of interest from the training set , and then evaluate the joint and slot accuracies in the same way as before .
The joint accuracy with the TRADE model is poor throughout except for 59.2 % for " Taxi " .
The rest of the domains have a joint accuracy ranging from 16.4 % for " Restaurant " to 22.9 % for " Train " .
Upon closer examination , we found that simply predicting " empty " for all slots would yield the same joint accuracy .
The zero-shot results for SUMBT are almost identical to that of TRADE .
A different evaluation methodology is used by Wu et al . ( 2019 ) in their zero-shot experiment .
The model for each domain is trained with the full dataset , except that all the slots involving the domain of interest are removed from the dialogue state .
The slots for the new domain are present in the validation and test data , however .
The method they use , which we reproduce here 2 , has consistently higher slot accuracy , but slightly worse joint accuracy than our baseline , by 1.9 % to 5.8 % , except for " Taxi " which improves by 1 % to 60.2 % .
To evaluate our proposed technique , we add our synthesized data for the domain of interest to the training data in the zero-shot experiment .
Besides synthesizing from templates , we also apply domain adaptation .
The pairs of domain chosen for Table 4 : Accuracy on the zero-shot MultiWOZ experiment ( test set ) , with and without data augmentation .
TRADE refers to Wu et al . ( 2019 ) , SUMBT to .
" Zero-shot " results are trained by withholding in - domain data .
" Zero-shot ( Wu ) " results are obtained with the unmodified TRADE zero-shot methodology , trained on Multi-WOZ 2.1 .
" Zero-shot ( DM ) " refers to zero-shot learning using our Dialogue - Model based data synthesis .
The last line of each model compares DM with full training , by calculating the % of the accuracy of the former to the latter .
adaptation are shown in Table 2 , together with the number of slot names that are common to both domains .
" Taxi " uses a subset of the slot names as " Train " but with different values .
" Attraction " , " Restaurant " and " Hotel " share the " name " and " area " slot ; " Restaurant " and " Hotel " also share the " price range " , " book day " , " book time " and " book people " slots .
For slots that are not shared , the model must learn both the slot names and slot values exclusively from synthesized data .
Our dialogue -model based zero-shot result , reported as " Zero-shot ( DM ) " in Table 4 , shows that our synthesized data improves zero-shot accuracy on all domains .
For TRADE , the joint accuracy improves between 6 % on " Taxi " and 19 % on " Restaurant " , whereas for SUMBT , joint accuracy improves between 3 % on " Taxi " and 30 % on " Attraction " .
With synthesis , SUMBT outperforms TRADE by a large margin .
Except for " Taxi " which has uncharacteristically high joint accuracy of 65 % , SUMBT outperforms TRADE from 8 % to 18 % .
This suggests SUMBT can make better use of synthesized data .
To compare synthesized with real training data , we calculate how close the accuracy obtained with the synthetic data gets to full training .
We divide the accuracy of the former with that of the latter , as shown in the last row for each model in Table 4 . Overall , training with synthesized data is about half as good as full training for TRADE , but is 2/3 as good as for SUMBT ( the ratio is 61 % to 74 % , ignoring " Taxi " as an outlier ) .
This suggests that our synthesis algorithm is generating a reasonable variety in the dialogue flows ; the pretrained BERT model , which imbues the model with general knowledge of the English language , is better at compensating for the lack of language variety in synthesized data .
Thus , the model only needs to learn the ontology and domain vocabulary from the synthesized data .
Conversely , TRADE has no contextual pretraining and must learn the language from the limited dialogue data .
This suggests that the combination of unsupervised pretraining and training on synthesized data can be effective to bootstrap new domains .
Error Analysis
To analyze the errors , we break down the result according to the turn number and number of slots in the dialogues in the test set , as shown in Fig.
3 .
We perform this analysis using the TRADE model on the " Restaurant " domain , which is the largest domain in MultiWOZ .
We observe that the baseline model achieves 100 % accuracy for turns with no slots , and 0 % accuracy otherwise .
the percentage of dialogues with all empty slots at each turn .
It is possible for 5 - turn dialogues to have all empty slots because a multi-domain dialogue may not have filled any slot in one domain .
By and large , the accuracy degrades for both the " full dataset " model and the " zero-shot ( DM ) " model , with the latter losing more accuracy than the former when there are 3 or 4 slots .
The accuracy drops almost linearly with increasing turn numbers for the full model .
This is expected because a turn is considered correct only if the full dialogue state is correct , and the state accumulates all slots mentioned up to that point .
The results for the full and the zero-shot ( DM ) models look similar , but the zero-shot model has a larger drop in later turns .
Modeling the first few turns in the dialogue is easier , as the user is exclusively providing information , whereas in later turns more interactions are possible , some of which are not captured well by our dialogue model .
Few - Shot Transfer Learning Following
Wu et al. ( 2019 ) , we also evaluate the effect of mixing a small percentage of real training data in our augmented training sets .
We use a naive few-shot training strategy , where we directly add a portion of the original training data in the domain of interest to the training set .
Fig. 4 plots the joint accuracy achieved on the new domain with the addition of different percentages of real training data .
The results for 0 % are the same as the zero-shot experiment .
The advantage of the synthesized training data decreases as the percent of real data increases , because real data is more varied , informative , and more representative of the distribution in the test set .
The impact of synthesized data is more pronounced for SUMBT than TRADE for all domains even with 5 % real data , and it is significant for the " Attraction " domain with 10 % real data .
This suggests that SUMBT needs more data to train , due to having more parameters , but can utilize additional synthesized data better to improve its training .
Conclusion
We propose a method to synthesize dialogues for a new domain using an abstract dialogue model , combined with a small number of domain templates derived from observing a small dataset .
For transaction dialogues , our technique can bootstrap new domains with less than 100 templates per domain , which can be built in a few person-hours .
With this little effort , it is already possible to achieve about 2/3 of the accuracy obtained with a large-scale human annotated dataset .
Furthermore , this method is general and can be extended to dialogue state tracking beyond transactions , by building new dialogue models .
We show improvements in joint accuracy in zero-shot and few-shot transfer learning for both the TRADE and BERT - based SUMBT models .
Our technique using the SUMBT model improves the zero-shot state of the art by 21 % on average across the different domains .
This suggests that pretraining complements the use of synthesized data to learn the domain , and can be a general technique to bootstrap new dialogue systems .
We have released our algorithm and dialogue model as part of the open-source Genie toolkit , which is available on GitHub 3 . Figure 2 : 2 Figure 2 : The SLOTQUESTION template and other nondialogue specific templates used to generate the example interaction .
