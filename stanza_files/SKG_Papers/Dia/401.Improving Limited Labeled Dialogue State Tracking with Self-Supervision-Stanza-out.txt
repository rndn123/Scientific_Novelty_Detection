title
Improving Limited Labeled Dialogue State Tracking with Self- Supervision
abstract
Existing dialogue state tracking ( DST ) models require plenty of labeled data .
However , collecting high-quality labels is costly , especially when the number of domains increases .
In this paper , we address a practical DST problem that is rarely discussed , i.e. , learning efficiently with limited labeled data .
We present and investigate two self-supervised objectives : preserving latent consistency and modeling conversational behavior .
We encourage a DST model to have consistent latent distributions given a perturbed input , making it more robust to an unseen scenario .
We also add an auxiliary utterance generation task , modeling a potential correlation between conversational behavior and dialogue states .
The experimental results show that our proposed self-supervised signals can improve joint goal accuracy by 8.95 % when only 1 % labeled data is used on the MultiWOZ dataset .
We can achieve an additional 1.76 % improvement if some unlabeled data is jointly trained as semi-supervised learning .
We analyze and visualize how our proposed self-supervised signals help the DST task and hope to stimulate future data-efficient DST research .
Introduction
Dialogue state tracking is an essential component in task - oriented dialogue systems designed to extract user goals / intentions expressed during a conversation .
Accurate DST performance can facilitate downstream applications such as dialogue management .
However , collecting dialogue state labels is very expensive and timeconsuming , requiring dialogue experts or trained turkers to indicate all ( domain , slot , value ) information for each turn in dialogues .
This problem becomes important from single-domain to multi-domain scenarios .
It will be more severe for a massive-multi-domain setting , making DST models less scalable to a new domain .
Existing DST models require plenty of state labels , especially those ontology - based DST approaches ( Henderson et al. , 2014 ; Zhong et al. , 2018 ) .
They assume a predefined ontology that lists all possible values is available , but an ontology requires complete state annotation and is hard to get in real scenario ( Xu and Hu , 2018 ) .
They also cannot track unseen slot values that are not predefined .
Ontology - free approaches ( Xu and Hu , 2018 ; Chao and Lane , 2019 ) , on the other hand , are proposed to generate slot values from dialogue history directly .
They achieve good performance on multi-domain DST by copyattention mechanism but still observe a significant performance drop under limited labeled data scenario ( Wu et al. , 2019a ) .
In this paper , we approach the DST problem using copy - augmented ontology - free models from a rarely discussed perspective , assuming that only a few dialogues in a dataset have annotated state labels .
We present two self-supervised learning ( SSL ) solutions :
1 ) Preserving latent consistency :
We encourage a DST model to have similar latent distributions ( e.g. , attention weights and hidden states ) for a set of slightly perturbed inputs .
This assumption is known as consistency assumption ( Zhou et al. , 2004 ; Chapelle et al. , 2009 ; Berthelot et al. , 2019 ) in semi-supervised learning , making distributions sufficiently smooth for the intrinsic structure collectively .
2 ) Modeling conversational behavior :
We train a DST model to generate user utterances and system responses , hoping that this auxiliary generation task can capture intrinsic dialogue structure information and benefit the DST performance .
This training only needs dialogue transcripts and does not require any further annotation .
We hypothesize that modeling this potential correlation between utterances and states is helpful for generalization , making a DST model more Usr Can you help me find a nightclub in south Cambridge ?
Sys The Night is located at 22 Sidney St .
Their phone number is 01223324600 .
You will need to call for their entry fee .
Usr Can you schedule me a taxi to take me there ?
Sys Can book you a taxi .
Can you tell me the arrival or departure time ?
Usr Also , I need a hotel with parking and 2 stars .
Annotated State ( attraction , type , nightclub ) , ( attraction , area , south ) , ( attraction , name , The Night ) , ( hotel , parking , yes ) , ( hotel , stars , 2 ) robust to unseen scenarios .
We simulate limited labeled data using Multi-WOZ , one of the taskoriented dialogue benchmark datasets , with 1 % , 5 % , 10 % , and 25 % labeled data scenarios .
The experimental results of 1 % data setting show that we can improve joint goal accuracy by 4.5 % with the proposed consistency objective and with an additional 4.43 % improvement if we add the behavior modeling objective .
Furthermore , we found that a DST model can also benefit from those remaining unlabeled data if we joint train with their selfsupervised signals , suggesting a promising research direction of semi-supervised learning .
Lastly , we visualize the learned latent variables and conduct an ablation study to analyze our approaches .
Background Let us define X 1:T = {( U 1 , R 1 ) , . . . , ( U T , R T ) } as the set of user utterance and system response pairs in T turns of a dialogue , and B = { B 1 , . . . , B T } are the annotated dialogue states .
Each B t contains a set of ( domain , slot , value ) tuples accumulated from turn 1 to turn t , therefore , the number of tuples usually grows with turn t. Note that it is possible to have multiple domains triggered in the same state B t .
A dialogue example and its labeled states are shown in Table 1 .
We briefly introduce a common approach for ontology - free DST in the following .
As shown in Figure 1 , a context encoder encodes dialogue history X 1 :t , and a state generator decodes slot values V ij for each ( domain , slot ) pair { ( D i , S j ) } , where i denotes the domain index and j is the slot index .
The context encoder and the state generator can be either a pre-trained language model or a simple recurrent neural network .
During the decoding stage for each V ij , a copy-attention mechanism such as text span extraction ( Vinyals et al. , 2015 ) or pointer generator ( See et al. , 2017 ) approach is added to the state generator and strengthen its value generation process .
Moreover , many ontology - free DST models are also equipped with a slot gate mechanism ( Xu and Hu , 2018 ; Rastogi et al. , 2019 ; Zhang et al. , 2019 ) , which is a classifier that predicts whether a ( domain , slot ) pair is mentioned , not mentioned , or a user does not care about it .
In this pipeline setting , they can add additional supervision to their models and ignore the not mentioned pairs ' prediction .
More specifically , the ( domain , slot ) pair { ( D i , S j ) } obtains its context vector C ij to predict a slot gate distribution G ij .
The context vector C ij is the weighted - sum of encoder hidden states using the attention distribution A ij , and G ij is a threeway classification distribution mapping from the context vector : G ij = FFN ( C ij ) 2 R 3 , C ij = A ij h enc 2 R d emb , A ij = Softmax ( Dist ( h dec ij , h enc ) ) 2 R M , ( 1 ) where d emb is the hidden size , h enc 2 R M ?d emb is hidden states of the context encoder for M input words , and h dec ij 2 R d emb is the first hidden state of the state generator .
The Dist function can be any vector similarity metric , and FFN can be any kind of classifier .
Such model is usually trained end-to - end with two loss functions , one for slot values generation and the other for slot gate prediction .
The overall supervised learning objective from the annotated state labels is L sl = | ij | X H( V ij , Vij ) + H( G ij , ?ij ) , ( 2 ) where H is the cross-entropy function .
The total number of ( domain , slot ) pairs is | ij | , and there are 30 pairs in MultiWOZ .
3 Self-Supervised Approaches
This section introduces how to leverage dialogue history X , which is easy to collect , to boost DST performance without annotated dialogue state labels implicitly .
We first show how we preserve latent consistency using stochastic word dropout , and we discuss our design for utterance generation .
Latent Consistency
The goal of preserving latent consistency is that DST models should be robust to a small perturbation of input dialogue history .
As shown in Figure 2 , we first randomly mask out a small number of input words into unknown words for N drop times .
Then we use N drop dialogue history together with the one without dropping any word as input to the base model and obtain N drop + 1 model predictions .
Masking words into unknown words can also strengthen the representation learning because when important words are masked , a model needs to rely on its contextual information to obtain a meaningful representation for the masked word .
For example , " I want a cheap restaurant that does not spend much . " becomes " I want a [ UNK ] restaurant that [ UNK ] not spend much . "
This idea is motivated by the masked language model learning ( Devlin et al. , 2019 ) .
We randomly mask words instead of only hiding slot values because it is not easy to recognize the slot values without ontology .
Afterward , we produce a " gues " for its latent variables : the attention distribution and the slot gate distribution in our setting .
Using the N drop + 1 model 's predictions , we follow the label guessing process in MixMatch algorithm ( Berthelot et al. , 2019 ) to obtain a smooth latent distribution .
We compute the average of the model 's predicted distributions by ? ij , ? ij = N drop +1 P d=1 P ( A ij , G ij | X d 1 :t , ? )
N drop + 1 , ( 3 ) where ? is the model parameters .
A ij and G ij are the smooth latent distribution that we would like a DST model to follow .
We include the original input without word masking input the average .
During the early stage of training , we may not have a good latent distribution even if it has labeled supervision .
Furthermore , inspired by the common usage of entropy minimization ( Grandvalet and Bengio , 2005 ) , we perform one more step for the gate distribution .
We apply a sharpening function , adjusting the temperature T of the categorical distribution , to reduce the entropy of slot gate prediction .
? ij = Sharp ( ? ij , T ) , Sharp( p , T ) i = p 1 T i / P p 1 T i . ( 4 ) In this way , we encourage a DST model to be more confident to its gate prediction as T decreases , since the sharpen ? ij will approach a one-hot distribution when T = 0 .
The sharpening function is not applied to the predicted attention distribution because we do not expect and force attention distribution to be a sharp categorical distribution .
We use the two guessed distributions to train a DST model to be consistent for the attention and slot gate given noise inputs .
The following consistency loss is added : L cons = | ij | X N drop +1 X d ( MSE ( ? ij , ?d ij ) + MSE ( ? ij , ?d ij ) ) .
( 5 ) We follow Berthelot et al . ( 2019 ) to apply the meansquared error function as our loss function .
We train a model to be consistent in terms of latent distributions because it is hard to guarantee the quality of generated values in different perturbed input , especially when we do not have much labeled data .
Also , each perturbed sample may generate slot values that have different number of words , and maintaining consistency of sequential distributions could be challenging .
As a result , we use slot gate distribution and attention distribution as intermediate targets since the former is the first stage for the whole prediction process , and the latter directly influences the copy mechanism .
Conversational Behavior Modeling
We hypothesize that with similar dialogue states , a system will reply also similar responses .
For example , when a system asks " What is your taxi destination from Palo Alto ? " , then we can infer that system 's state may include ( taxi , departure , Palo Alto ) .
In this way , we can potentially model the correlation between dialogue states and dialogue behavior .
In practice , we use two decoders , one modeling user and one modeling system behavior , to generate utterances based on the learned representations from a DST model .
We use a gated recurrent unit ( GRU ) to generate the next system response based on the dialogue history X 1:t and current predicted dialogue states B t , and use another GRU to generate / recover user utterance based on last dialogue history X 1:t 1 and current predicted dialogue states B t .
Intuitively , we expect the system GRU to capture correlation between R t+1 and B t , and the user GRU to learn for U t and B t .
GRUs generate a sequence of words during training and compute cross-entropy losses between generated sentences and target sentences .
We do not use the attention mechanism intentionally because 1 ) our goal is not to have an outstanding performance on sentence generation , and 2 ) we expect the model can generate sentences by solely aligning its initial states from a DST model .
As shown in Figure 1 , we initial our system and user GRUs using latent variables from an ontologyfree DST model .
The initial state h init to be aligned is defined by h init = | ij | X [ h dec ij ; C ij ] , ( 6 ) where [ ; ] denotes vector concatenation and we sum representations from all ( domain , slot ) pairs .
We use the context vector C ij to represent dialogue history , and h dec ij to represent dialogue state .
The overall self-supervised loss function for modeling conversational behavior is L cb = H( R t+1 , Rt + 1 ) + H( U t , ?t ) , ( 7 ) where Rt + 1 and ?t are predicted response and user utterance initialized by the h init vector .
Overall Objectives
During training , we optimize both supervised signal and self-supervised signal using the labeled data .
The overall loss function is L label = L sl + ?L cb + L cons , ( 8 ) where ? and are hyper-parameters .
Other than labeled data , we can also sample unlabeled data to perform self-supervision as a regularization term .
This strategy can be considered as a semi-supervised approach , leveraging unlabeled data to learn a smooth prediction .
For unlabeled data , we use only the self-supervised signal to update the model , L unlabel = L cb + L cons . ( 9 ) In practice , we first draw a batch of samples from labeled data to update the model 's parameters and then draw another batch of samples from unlabeled data .
We find that taking turns to train unlabeled data with labeled data works better than pre-training with unlabeled data then fine-tuning on labeled data .
Experiments
Base Model
In this paper , we focus on applying self-supervision for ontology - free DST approaches .
We select TRADE ( Wu et al. , 2019a )
Dataset MultiWOZ is one of the largest existing human-human conversational corpus spanning over seven domains , containing around 8400 multi-turn dialogues , with each dialogue averaging 13.7 turns .
We follow Wu et al . ( 2019a ) to only use the five domains ( hotel , train , attraction , restaurant , taxi ) because the other two domains ( hospital , police ) have very few dialogues ( 10 % compared to others ) and only exist in the training set .
In total , there are 30 ( domain , slot ) pairs .
We also evaluate on its revised version 2.1 from Eric et al . ( 2019 ) in our experiments , due to the space limit , results on version 2.1 are reported in the Appendix .
We simulate a limited labeled data scenario by randomly selecting dialogues from the original corpus using a fixed random seed .
The dataset statistics of each labeled ratio is shown in Table 3 .
For example , in 1 % labeled data setting , there are 84 dialogues across five different domains .
Note that the summation of dialogues from each domain is more than the number of total dialogues because each dialogue could have more than one domain , e.g. , two domains are triggered in the Table 1 .
Training Details
The model is trained end-to - end using Adam optimizer ( Kingma and Ba , 2015 ) with a batch size of 8 or 32 .
A grid search is applied for ? and in the range of 0.1 to 1 , and we find that models are sensitive to different ? and .
The learning rate annealing is used with a 0.2 dropout ratio .
All the word embeddings have 400 dimensions by concatenating 300 Glove embeddings ( Pennington et al. , 2014 ) and 100 character embeddings ( Hashimoto et al. , 2016 ) .
A greedy decoding strategy is used for the state generator because the slot values are usually short in length .
We mask out 20 % - 50 % of input tokens to strengthen prediction consistency .
The temperature T for sharpening is set to 0.5 , and augmentation number N drop is 4 .
Results Joint goal accuracy and its fuzzy matching 2 version are used to evaluate the performance on multidomain DST .
The joint goal accuracy compares the predicted dialogue states to the ground truth B t at each dialogue turn t , and the output is considered correct if and only if all the ( domain , slot , value ) tuples exactly match the ground truth values in B t , which is a very strict metric .
The fuzzy joint goal accuracy is used to reward partial matches with the ground truth ( Rastogi et al. , 2019 ) .
For example , two similar values " Palo Alto " and " Palo Alto city " have a fuzzy score of 0.78 .
In Table 2 , we evaluate four different limited labeled data scenarios : 1 % , 5 % , 10 % , and 25 % .
We test our proposed self-supervised signals by only adding latent consistency objective ( row 2 ) , only adding conversational behavior objective ( row 3 ) , using both of them ( row 4 ) , and using both of them together with unlabeled data ( row 5 ) .
In general , we find that each self-supervision signal we presented is useful in its degree , especially for 1 % and 5 % labeled data scenarios .
Modeling conversational behavior seems to be more effective than preserving prediction consistency , which is not surprising because the latter is a point-wise selfsupervised objective function .
We also found that self-supervision becomes less dominant and less effective as the number of labeled data increases .
We try 100 % labeled data with self-supervision , and it only achieves slight improvement , 48.72 % joint goal accuracy compared to the original reported 48.62 % .
Taking a closer look to the results in Table 2 , preserving consistency has 4.52 % ( or 4.03 % fuzzy ) improvement for 1 % scenario .
Once the labeled data increases to 25 % ( 2105 dialogues ) , there is no difference with or without the consistency objective .
Meanwhile , modeling conversational behavior objective seems to be more effective than the consistency objective , as it has 8.61 % ( or 8.85 % fuzzy ) improvement .
A small improvement can be further observed if we combine both of them and jointly train end-to-end .
When we also leverage those remaining dialogue data and conduct semisupervised learning , we can achieve the highest joint goal accuracy , 20.41 % in 1 % setting , and 33.67 % in 5 % setting .
In these experiments , we simply use the remaining dialogues in the dataset as unlabeled data , e.g. , 1 % labeled with 99 % unlabeled , 5 % labeled with 95 % unlabeled , etc .
We also test some other DST trackers in the last few rows in Table 2 , which all of them are replied on the pre-trained language model BERT ( Devlin et al. , 2019 ) . SUMBT and TOD - BERT ( Wu et al. , 2020 ) are ontology - based approaches .
The former uses BERT to encode each utterance and builds an RNN tracker on top of BERT .
The latter uses its pre-trained task - oriented dialogue BERT to encode dialogue history and adds simple slot-dependent classifiers .
Note that we still assume they have a full ontology in this setting even though it is not a fair comparison under a limited labeled scenario .
DSDST - Span ( Zhang et al. , 2019 ) is an ontology - free DST tracker , it uses BERT to encode dialogue history together with each ( domain , slot ) pair separately and extract a corresponding text span as its slot values .
Analysis and Visualization
We would interpret how self-supervised signals help to learn better DST performance .
The first interesting observation is that the key improvement comes from the slot-dependent context vectors C ij .
If we remove the context vector C ij from Eq ( 6 ) , the performance of 1 % labeled data setting drops from 18.31 % to 11.07 % .
The next question is : what do these contextual vectors influence ?
First , context vectors are the weighted - sum of encoder hidden states , which means they correlate with the learned attention distribution .
Also , context vectors are used to predict slot gates , which is essential to be able to trigger the state generator .
Therefore , using self-supervision to align contextual slot vectors may help get better attention distributions and better slot gate prediction .
4 , gate accuracy of 1 % labeled data improves by around 3 % with self-supervision .
We also compare attention distributions among a model trained with 1 % labeled data , a model trained with 1 % labeled data and selfsupervision , and a model trained with 100 % labeled data .
We observe a smaller value of KL divergence with self-supervision ( the lower , the better ) , i.e. , the attention distribution becomes more similar to the one learned from 100 % labeled data , which we assume that it is supposed to be a better attention distribution .
Slot Gate
As shown in Table
We randomly pick up 2,000 dialogue turns on the test set to compute the correlation between latent Dialogue History 100 % Data ; hi ; hello , i am trying to find a train that goes from cambridge to london kings cross . can you help me book a ticket ?
; i can help with that . can you tell me what day you will be traveling ? ; i need to leave on saturday after 18:45 . ; the soonest departure time would be at 19:00 on saturday , is that okay ? ; yes , that s perfect . can you book that for 8 people ? ; you are all booked with reference number 144 vdbrm . the cost of 151.04 gbp will be payable at the station . can i be of further assistance today ? ; i am looking for an expensive place to eat in the centre , what is there that fits that criteria ? ; there 33 place -s that fit your criteria . do you have a particular cuisine type in mind so that i can narrow the results down ? ; it does not matter what kind of food . what would you recommend for a large group of 8 people ? ; how about don pasquale pizzeria ? ; that sounds great . please book it for 8 on saturday at 14:15 and get a reference number . ; unfortunately , the restaurant does not have a table for that time . can you do it earlier or later ? ; how about 13:15 ? ; great . that was successful . your reference number is q0ij8u6 u . ; thank you , you 've been a great help . ; is there anything else that i could help you with today ? ; no thank you , that s all for now ! ; 1 % Data w/o Self-supervision ; hi ; hello , i am trying to find a train that goes from cambridge to london kings cross . can you help me book a ticket ?
; i can help with that . can you tell me what day you will be traveling ? ; i need to leave on saturday after 18:45 . ; the soonest departure time would be at 19:00 on saturday , is that okay ? ; yes , that s perfect . can you book that for 8 people ? ; you are all booked with reference number 144 vdbrm . the cost of 151.04 gbp will be payable at the station . can i be of further assistance today ? ; i am looking for an expensive place to eat in the centre , what is there that fits that criteria ? ; there 33 place -s that fit your criteria . do you have a particular cuisine type in mind so that i can narrow the results down ? ; it does not matter what kind of food . what would you recommend for a large group of 8 people ? ; how about don pasquale pizzeria ? ; that sounds great . please book it for 8 on saturday at 14:15 and get a reference number . ; unfortunately , the restaurant does not have a table for that time . can you do it earlier or later ? ; how about 13:15 ? ; great . that was successful . your reference number is q0ij8u6 u . ; thank you , you 've been a great help . ; is there anything else that i could help you with today ? ; no thank you , that s all for now ! ; 1 % Data w/ Self-supervision ; hi ; hello , i am trying to find a train that goes from cambridge to london kings cross . can you help me book a ticket ? ; i can help with that .
can you tell me what day you will be traveling ? ; i need to leave on saturday after 18:45 . ; the soonest departure time would be at 19:00 on saturday , is that okay ? ; yes , that s perfect . can you book that for 8 people ? ; you are all booked with reference number 144 vdbrm . the cost of 151.04 gbp will be payable at the station . can i be of further assistance today ? ; i am looking for an expensive place to eat in the centre , what is there that fits that criteria ? ; there 33 place -s that fit your criteria . do you have a particular cuisine type in mind so that i can narrow the results down ? ; it does not matter what kind of food . what would you recommend for a large group of 8 people ? ; how about don pasquale pizzeria ? ; that sounds great . please book it for 8 on saturday at 14:15 and get a reference number . ; unfortunately , the restaurant does not have a table for that time . can you do it earlier or later ? ; how about 13:15 ? ; great . that was successful . your reference number is q0ij8u6 u . ; thank you , you 've been a great help . ; is there anything else that i could help you with today ? ; no thank you , that s all for now ! ; learned states ( h init ) of 1 % labeled data and the true gating status ( G ) of the ( domain , slot ) pairs .
As shown in Figure 3 , the x-axis is the cosine similarity score between two latent dialogue states the model learned , and the y-axis is the cosine similarity score of their true gating status .
Ideally , when the slot gate status is similar , then the learned representations should also have a high similarity score .
We find the model trained with self-supervision ( right ) has a higher Pearson correlation coefficient than the one without ( left ) , increasing from 0.4315 to 0.7035 , implying that with self-supervision , models can learn better state representations .
Copy Attention
We also visualize the attention distributions of a dialogue history in Figure 4 .
The darker red color means the higher attention weight and the higher copy probability .
We sum attention distributions of A ij for all ( domain , slot ) pairs and normalize it .
The 1 % labeled data model with selfsupervision has an attention distribution similar to the one using 100 % labeled data .
For example , both of them focus on some useful slot information such as " Cambridge " , " London " , " Saturday " , and " 18:45 " .
The results of attention distribution are crucial , especially in our limited labeled setting .
The higher the attention weight , the higher the probability that such word will be copied from the dialogue history to the output slot values .
More attention visualizations are shown in the Appendix .
Slot Accuracy Analysis
We are interested in which domains and which slots are easier to be self-supervised learned .
As shown in Figure 5 , the x-axis is each ( domain , slot ) pair , and the y-axis is its slot accuracy ( at each dialogue turn whether the pair is predicted correctly ) .
The blue bar is the performance of 1 % labeled data without selfsupervision .
The orange part is the improvement by using self-supervision .
The green part can be viewed as the upper-bound of the base model using 100 % labeled data .
The top three ( domain , slot ) pairs that is most effective with self-supervision are ( train , day ) , and ( train , departure ) , ( train , destination ) .
On the other hand , self- supervision are less helpful to pairs such as ( hotel , parking ) , ( hotel , internet ) , ( restaurant , name ) , and all the pairs in the taxi domain .
One possible reason is that self-supervision is sensitive to the unlabeled data size , i.e. , the major domain is dominant in the overall performance .
It is worth mentioning that in the taxi domain , all the slots perform relatively well with 1 % labeled data .
This could also explain why the zero-shot performance reported in Wu et al . ( 2019a ) is much better than the other four domains .
Related Work Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states ( Williams and Young , 2007 ; Thomson and Young , 2010 ; Wang and Lemon , 2013 ; Williams , 2014 ) , or to jointly learn speech understanding ( Henderson et al. , 2014 ; Zilka and Jurcicek , 2015 ) .
One drawback is that they rely on hand-crafted features and complex domain-specific lexicons besides the ontology , and are difficult to extend and scale to new domains .
As the need Figure 5 : Slot accuracy visualization for each ( domain , slot ) pairs .
Several slots such as ( train , day ) and ( hotel , book stay ) that using 1 % data with self-supervision almost perform the same as using 100 % data .
for domain expanding , research direction moves from single domain DST setting and datasets to multi-domain DST setting and datasets Eric et al. , 2019 ) .
There are three main categories to perform DST , ontology - based , partial - ontology - based , and ontology - free approaches .
Ontology - based methods Rastogi et al. , 2017 ; Zhong et al. , 2018 ; train metric learning functions for context encoder and ontology encoder , and score over a predefined slot value candidates .
Partial-ontologybased
Zhang et al. , 2019 ; Rastogi et al. , 2019 ) approaches only use part of an ontology to perform ranking and use generation techniques for the remaining slots .
Ontology - free methods ( Chao and Lane , 2019 ; Ren et al. , 2019 ; Kumar et al. , 2020 ; Wu et al. , 2019a ; Kumar et al. , 2020 ; rely on generation with copy mechanism without predefined ontology , which has better generalization ability to unseen slot values .
Our work is closer to ontology - free approaches because it is reasonable to assume that we cannot access an ontology under a limited labeled data scenario .
Self -Supervised Learning
There is a wide literature on self-supervision ( Barlow , 1989 ) and semi-supervised techniques ( Chapelle et al. , 2009 ) . Swayamdipta et al. ( 2018 ) introduce a syntactic scaffold , an approach to incorporate syntactic in-formation into semantic tasks .
Sankar et al. ( 2019 ) found that Seq2Seq models are rarely sensitive to most perturbations , such as missing or reordering utterances .
Shi et al. ( 2019 ) used variational RNN to extract latent dialogue structure and applied it to dialogue policy learning .
Wu et al . ( 2019 b ) introduced a self-supervised learning task , inconsistent order detection , to explicitly capture the flow of conversation in dialogues .
Jin et al. ( 2018 ) use unlabeled data to train probabilistic distributions over the vocabulary space as dialogue states for neural dialogue generation .
Su et al. ( 2020 ) provide both supervised and unsupervised learning algorithms to train language understanding and generation models in a dual learning setting .
Tseng et al. ( 2019 ) applied pseudo-labeling and Q - model ( Sajjadi et al. , 2016 ) as additional semi-supervision to bootstrap state trackers .
Our latent consistency comes from the consistency regularization ( Sajjadi et al. , 2016 ; Berthelot et al. , 2019 ) , leveraging the idea that a model should output the same class distribution for an unlabeled example even after it has been augmented .
Conclusion
We investigate the potential of using selfsupervised approaches for label - efficient DST in task - oriented dialogue systems .
We strengthen latent consistency by augmenting data with stochastic word dropout and label guessing .
We model conversational behavior by the next response generation and turn utterance generation tasks .
Ex -perimental results show that we can significantly boost the joint goal accuracy with limited labeled data by exploiting self-supervision .
We conduct comprehensive result analysis to cast light on and stimulate label - efficient DST .
Figure 1 : 1 Figure 1 : The block diagram of copy-attention ontology - free framework for dialogue state tracking .
The self-supervised modules ( dotted parts ) are discarded during inference time .
