title
Alignment over Heterogeneous Embeddings for Question Answering
abstract
We propose a simple , fast , and mostlyunsupervised approach for non-factoid question answering ( QA ) called Alignment over Heterogeneous Embeddings ( AHE ) .
AHE simply aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph , and weighs each alignment score with the inverse document frequency of the corresponding question / answer term .
AHE 's similarity function operates over embeddings that model the underlying text at different levels of abstraction : character ( FLAIR ) , word ( BERT and GloVe ) , and sentence ( InferSent ) , where the latter is the only supervised component .
Despite its simplicity and lack of supervision , AHE obtains a new state - of - the - art performance on the " Easy " partition of the AI2 Reasoning Challenge ( ARC ) dataset ( 64.6 % accuracy ) , toptwo performance on the " Challenge " partition of ARC ( 34.1 % ) , and top- three performance on the WikiQA dataset ( 74.08 % MRR ) , outperforming many other complex , supervised approaches .
Our error analysis indicates that alignments over character , word , and sentence embeddings capture substantially different semantic information .
We exploit this with a simple meta-classifier that learns how much to trust the predictions over each representation , which further improves the performance of unsupervised AHE 1 .
Introduction
The " deep learning tsunami " ( Manning , 2015 ) has had a major impact on important natural language processing ( NLP ) applications such as question answering ( QA ) .
Many neural approaches for QA have been proposed in the past few years , with impressive results on several QA tasks ( Seo et al. , 2016 ; Wang and Jiang , 2016 ; Wang et al. , 2017 b ; 1 Code : https://github.com/vikas95/AHE
Question
- Which sequence of energy transformations occurs after a battery - operated flashlight is turned on ?
1 . electrical ? light ? chemical 2 . electrical ?chemical ? light 3 . chemical ? light ? electrical 4 . chemical ? electrical ? light Supporting paragraph ( s ) : " a chemical cell converts chemical energy into electrical energy ; a flashlight chemical energy to light energy "
Figure 1 : A multiple -choice question from the ARC dataset with the correct answer in bold font .
This question is answered correctly by our alignment method that relies on contextualized word embeddings that capture the correct sequence , and cannot be answered correctly when relying on uncontextualized embeddings .
Tymoshenko et al. , 2017 ; Xiong et al. , 2016a ; Radford et al. , 2018 ; Li et al. , 2018 , inter alia ) .
However , an undesired effect of this focus on neural approaches was that other methods have fallen out of focus , including strong unsupervised benchmarks that are necessary to highlight the true gains of supervised approaches .
For instance , alignment approaches have received considerably less interest recently , despite their initial successes ( Echihabi and Marcu , 2003 ; Surdeanu et al. , 2011 , inter alia ) .
While a few recent efforts have adapted these alignment methods to operate over word representations ( Kenter and De Rijke , 2015 ; Kim et al. , 2017 ; Yadav et al. , 2018 ) , they generally underperfom supervised neural methods due to their underlying bag-of- word ( BoW ) assumptions and reliance on uncontextualized word representations such as GloVe ( Pennington et al. , 2014 ) .
In this work we argue that alignment approaches are more meaningful today after the advent of contextualized word representations , which mitigate the above BoW limitations .
For example , Figure 1 shows an example of a question from AI2's Reasoning Challenge ( ARC ) dataset , which is not answered correctly by a state - of- theart BoW alignment method ( Yadav et al. , 2018 ) , but is correctly answered by our alignment approach when operating over Bidirectional Encoder Representations from Transformers ( BERT ) embeddings ( Devlin et al. , 2018 ) .
We propose a simple , fast , and mostlyunsupervised approach for non-factoid QA called Alignment over Heterogeneous Embeddings ( AHE ) .
AHE uses an off-the-shelf information retrieval ( IR ) component to retrieve likely supporting paragraphs from a knowledge base ( KB ) given a question and candidate answer .
Then AHE aligns each word in the question and candidate answer with the most similar word in the retrieved supporting paragraph , and weighs each alignment score with the inverse document frequency ( IDF ) of the corresponding question / answer term .
AHE 's overall alignment score is the sum of the IDF weighted scores of each of the question / answer term .
Importantly , AHE 's alignment function operates over contextualized embeddings that model the underlying text at different levels of abstraction : character ( FLAIR ) ( Akbik et al. , 2018 ) , word ( BERT ) ( Devlin et al. , 2018 ) , and sentence ( InferSent ) ( Conneau et al. , 2017 ) , where the latter is the only supervised component in the proposed approach .
The different representations are combined through an ensemble approach that by default is unsupervised ( using a variant of the NoisyOr formula ) , but can be replaced with a supervised meta-classifier .
The contributions of our work are the following : 1 . To our knowledge , this is the first unsupervised alignment approach for QA that : ( a ) operates over contextualized embeddings , and ( b ) captures text at multiple levels of abstraction , including character , word , and sentence .
2 . We obtain ( near ) state - of - the - art results ( top three or higher ) on three QA datasets : Wik-iQA ( Yang et al. , 2015 ) ( 74.08 mean reciprocal rank ) , ARC the Challenge partition ( 34.1 % precision at 1 ( P@1 ) ) and ARC Easy ( 64.6 P@1 ) .
Our approach outperforms information retrieval methods , other unsupervised alignment approaches , and many supervised , neural approaches , despite the fact that it is mostly unsupervised and much simpler .
Importantly , unlike many neural approaches , our results are robust across several datasets .
Minimally , these results indicate that the work proposed here should be considered as a new , strong baseline for the task .
3 . Our analysis indicates that alignments over character , word , and sentence embeddings capture substantially different semantic information .
We highlight this complementarity with an oracle system that chooses the correct answer when it is proposed by any of the AHE 's representations , which achieves 68 % P@1 on ARC Challenge , 86 % on ARC Easy , and 93.7 % mean average precision ( MAP ) on WikiQA .
We exploit this complementarity with a simple meta-classifier that learns when and how much to trust the predictions over each representation , which further improves the performance of unsupervised AHE .
Related Work
We highlight major trends in the field , and how our work compares with them .
We focus mostly on non-factoid QA , which is usually implemented in two forms : multiple -choice QA such as AI2's Reasoning Challenge ( ARC ) , where the answer must be selected from multiple candidates and ( optionally ) supported by explanatory texts extracted from external knowledge bases ) ; or answer sentence selection , where candidate answer sentences are provided and the task is to select the sentences containing the correct answers ( Yang et al. , 2015 ) .
Alignment models have also been proposed for other types of QA , such as reading comprehension ( RC ) QA ( Chakravarti et al. , 2017 ) .
We believe AHE can be similarly extended to RC , but , in this work , we have limited our experiments to answer selection and multiple - choice QA tasks .
Most QA approaches today use neural , supervised methods .
Most use stacked architectures usually coupled with attention mechanisms ( He and Lin , 2016 ; Yin et al. , 2015 ; Seo et al. , 2016 ; Xiong et al. , 2016 b ; Tan et al. , 2015 ; Wang et al. , 2017a ; Chen et al. , 2016 ; Cheng et al. , 2016 ; Golub and He , 2016 ) .
Some of these works also rely on structured knowledge bases ( Zhong et al. , 2018a ; Ni et al. , 2018 ) such as ConceptNet ( Speer et al. , 2017 ) .
Some approaches use query expansion methods in addition to the above methods ( Musa et al. , 2018 ; Nogueira and Cho , 2017 ; Ni et al. , 2018 ) .
For example , Musa et al. ( 2018 ) used a sequence to sequence model ( Sutskever et al. , 2014 ) to generate an enhanced query for ARC which retrieves better supporting passages .
However , in general , all these approaches rely on annotated training data , and , some , on structured KBs , which are expensive to create ( Jauhar et al. , 2016 ) .
Further , as we demonstrate in Section 5 , these methods tend to be tailored to a specific dataset and do not port well to other domains or even within different splits of the same dataset .
In contrast , our method is mostly unsupervised and does not require training .
Even then , our approach performs well on three distinct QA datasets , with top three performance in all .
Our work is inspired by previous efforts on using alignment methods for NLP ( Echihabi and Marcu , 2003 ) .
Unsupervised alignment models have been proposed for several NLP tasks such as short text similarity ( Kenter and De Rijke , 2015 ) , answer phrase / sentence selection in reading comprehension ( RC ) ( Chakravarti et al. , 2017 ) , document retrieval ( Kim et al. , 2017 ) , etc .
Other works have utilized word alignments as features in supervised models ( Surdeanu et al. , 2011 ; Wang and Ittycheriah , 2015 ) .
For example , Wang and Ittycheriah ( 2015 ) utilized the alignment of words between two questions as a feature in a feedforward neural network that matches similar FAQ questions .
Recently , Yadav et al. ( 2018 ) showed that alignment methods remain competitive for non-factoid QA .
However , the majority of alignment models that rely on representation learning utilize uncontextualized word embeddings such as GloVe , coupled with other BoW models such as IBM Model 1 ( Brown et al. , 1993 ) for alignment ( Kenter and De Rijke , 2015 ; Kim et al. , 2017 ; Yadav et al. , 2018 ) .
To our knowledge , we are the first to adapt these ideas to contextualized embeddings , which mitigates the BoW limitations of previous efforts ( as shown in Figure 1 ) .
While contextualized representations have been shown to be extremely useful for multiple NLP tasks ( Devlin et al. , 2018 ; Peters et al. , 2018 ; Howard and Ruder , 2018 ) , our work is the first to apply them to an unsupervised alignment approach .
Further , we show that different contextualized representations of text ( character , word , sentence ) capture complementary information , and combining them improves performance further .
Approach
The core component of our approach computes the score of a candidate answer by aligning two texts .
For multiple -choice questions , the first text consists of the question concatenated with the candidate answer , and the second is a supporting paragraph such as the one shown in Figure 1 , which consists of one or more sentences retrieved from a larger textual KB using an off-the-shelf IR system ( Section 3.1 ) .
For answer selection tasks , the first text is the question and the second is the sentence that contains the candidate answer .
Answer candidates are then sorted in descending order of their alignment scores .
In both cases , the alignment approach operates over multiple contextualized embeddings that model the two texts at different levels of abstraction : character , word , and sentence .
The overall architecture is illustrated in Figure 2 .
We detail the alignment method in ?3.2 , the multiple representations of text considered in ?3.3 , and the ensemble strategies over these representations in ?3.4 .
Retrieving Supporting Paragraphs
For multiple -choice question datasets such as ARC , we retrieve supporting information from external KBs using Lucene , an off-the-shelf IR system 2 .
We use as query the question concatenated with the corresponding answer candidate , and BM25 ( Robertson et al. , 2009 ) as the ranking function 3 . For each query , we keep the top C Lucene documents , where each document consists of a sentence retrieved from the ARC corpus .
Similar to our previous work ( Yadav et al. , 2018 ) , we boost candidate answer terms by a factor of 3 while keeping question terms as it is in the BM25 ranking function .
All texts were preprocessed by discarding the case of the tokens , removing the stop words from Lucene 's list , and lemmatizing the remaining tokens using NLTK ( Bird , 2006 ) .
For all experiments reported on the ARC dataset we used C = 20 .
Here we also calculate the IDF of each query term q i ( required later during alignment ) : idf ( q i ) = log N ? docfreq ( q i ) + 0.5 docfreq ( q i ) + 0.5 ( 1 ) where N is the number of documents ( e.g. , 14.3 M for the ARC KB ) and docf req( q i ) is the number of documents that contain q i .
Alignment Algorithm
For representations that produce word embeddings ( e.g. , FLAIR , BERT , GloVe ) , we use the alignment algorithm in Figure 3 .
Our method computes the alignment score of each query token with every token in the given KB paragraph , using the cosine Embedding representation 4 ( InferSent ) Embedding representation 3 ( BERT ) Embedding representation 2 ( GloVe ) Embedding representation 1 ( FLAIR )
Ensemble Alignment Q F 1 KB F 1 Q F 2 KB F 2 Q F 3 KB F 3 ? ? ? ? ? ? Q F n KB F m Question + candidate answer text Supporting paragraph text similarity of the two embedding vectors .
Then , a max-pooling layer over this cosine similarity matrix is used to retrieve the most similar token in the supporting passage for each query token .
Lastly , this max-pooled vector of similarity scores is multiplied with the vector containing the IDF values of the query tokens and the resultant vector is summed to produce the overall alignment score s for the given query Q a ( formed from question Q and candidate answer a ) and the supporting paragraph P j : KB 1 KB 2 ? ? ? KB m Q n ? ? ? . . . . . . . . . . . . . . . . . . . . . Q 3 ? ? ? Q 2 ? ? ? Q 1 ? ? ? Cosine similarity matrix Max-pool Query IDF s( Q a , P j ) = | Qa| i=1 idf ( q i ) ? align( q i , P j ) ( 2 ) align( q i , P j ) = | P j | max k=1 cosSim(q i , p k ) ( 3 ) cosSim( q i , p k ) = q i ? p k || q i || ? || p k || ( 4 ) where q i and p k are the embedding vectors of the terms q i and p k .
In addition to alignments over word- level embeddings , we include InferSent ( Conneau et al. , 2017 ) , which generates sentence - level embeddings ( see ?3.3 for details ) .
For InferSent , the alignment score between a query Q a and a supporting paragraph P j is computed as the dot product of the two corresponding sentence vectors , Q a and P j , normalized using softmax over all candidate answers : s( Q a , P j ) = sof tmax ( Q a ? P j ) ( 5 ) For ARC , the above alignment scores are computed for each supporting paragraph in the set of C paragraphs retrieved in ?3.1 .
For WikiQA , this score is computed just for the sentence containing the candidate answer .
To aggregate the retrieved ARC paragraph scores ( for ARC ) into an overall score for the corresponding candidate answer , we consider : Max : selects the maximum alignment score between all available paragraphs as the final score for candidate answer a : S( cand a ) = C max j=1 ( s ( Q a , P j ) ) ( 6 ) Weighted average : averages all available paragraph scores , using as weights the inverse IR ranks of the corresponding paragraphs : S( cand a ) = C j=1 1 j ( s ( Q a , P j ) ) ( 7 ) During tuning , we observed that the max strategy is better for ARC Challenge , while the weighted average is better for ARC Easy .
We conjecture that this happens because Challenge questions require information that is sparser in the collection , and , thus , including more than the top paragraph tends to introduce noise .
Text Representations AHE computes alignments over four different embedding representations that model the text at different levels of abstraction : character , word , and sentence ( as detailed below ) .
Although all these embeddings can be tuned for specific domains to improve performance , here we highlight the potential of publicly - available , pre-trained embeddings .
Hence , we did not train embeddings on any domain specific corpus , and directly used off - the-shelf embeddings in all but one situation .
The details of all four component embeddings of AHE are discussed below .
Character - based embeddings :
We used the FLAIR contextual character language model of Akbik et al . ( 2018 ) .
They used long short-term memory ( LSTM ) networks that operate at character level over the entire text to generate character embeddings ( in both forward and backward directions ) .
Similar to them , to generate the embedding for token i , we concatenate the embedding from the forward LSTM for the character following the token , with the embedding from the backward LSTM for the character preceding the token : w F LAIR i := h f t i + 1?1 h b t i ?1 ( 8 ) where t i is the character offset of the i th token in the input text , and h is the corresponding LSTM 's hidden state .
We used the " mix -forward " and " mixbackward " pretrained models provided by the authors to produce two character embeddings , each of size 2048 , resulting in word embeddings of size 4096 .
Word - based embeddings :
We incorporated two different word - based embeddings : BERT - we used the Bidirectional Encoder Representations from Transformers ( BERT ) embedding model of Devlin et al . ( 2018 ) .
We concatenated the last four layers ( as suggested by the authors 4 ) of the BERT Large language model , where each layer has size 1024 , summing up to size 4096 embeddings for each token : w BERT i :=
[ Layer ?1 , .... , Layer ?4 ] ( 9 ) 4 https://github.com/google-research/ bert GloVe - we also include GloVe embeddings ( Pennington et al. , 2014 ) , under the hypothesis that these uncontextualized word embeddings will provide complementary information to the contextualized BERT embeddings .
We used GloVe embeddings of size 300 , trained over 840B tokens from Wikipedia , resulting in 2.2 M words vocabulary .
Sentence - based embeddings : Lastly , we used InferSent , the sentence - based embeddings of Conneau et al . ( 2017 ) .
InferSent was originally trained on several natural language inference ( NLI ) datasets to generate the sentence representations that maximize the probability of correct inference .
This model achieved poor performance on our QA tasks ( see rows 8a in Table 1 and row 7a in Table 2 ) .
Therefore , rather than using this NLI model , we trained InferSent on our data by maximizing the inference probability from the input query 5 to the supporting paragraph .
We used the same number of supporting passages ( C = 20 ) and the same scoring functions as explained in Section 3.2 .
We trained InferSent using batches of size 32 , the Adam optimizer , learning rate = 0.001 , and 50 epochs .
We used max pooling over the token 's LSTM hidden states to generate an overall sentence embedding .
We tuned the sentence representation size on the development sets , 6 which resulted in 128 for WikiQA and 384 for ARC .
Aggregating Multiple Representations
We aggregate the scores of candidate answers over the four different embedding representations using an unsupervised variant of the NoisyOr formula : N oisyOr M ( i ) = 1 ? ( M m=0 ( 1 ? ? m * S m i ) ) ( 10 ) which computes the overall score for answer candidate i . M is the total number of representations ( e.g. , 4 in our case ) , and S m i is the score of answer candidate i under representation m .
Lastly , ? m is a hyperparameter used to dampen peaky distributions of answer probabilities .
We included this hyperparameter because we observed that InferSent produces a probability distribution over candidate answers where one answer tends to take most of the probability mass , and these scores dominate in the NoisyOr .
Thus , the ? m weights are set to 1 for all representations with the exception of InferSent , for which we tuned its value to 0.2 .
Of course , other types of aggregation are possible .
To explore this space , we also implemented a supervised meta-classifier , which aims to learn the aggregation function directly from data .
We implemented this multi-classifier as a feed forward network with two fully connected dense layers of hidden size 16 and K respectively , where K is the maximum number of candidate answers for the given dataset .
The activation function of the first dense layer was tanh ; we used a softmax in the second output layer .
The input to this network was a vector of size M ? K .
For example , for ARC this vector has a size 4 ? 5 = 20 .
For WikiQA this vector has size 4 ? 22 = 88 .
Each element in the input vector is the score of one candidate answer under a given representation .
Additionally , for ARC we used an extra position in the input vector to indicate the grade of the corresponding exam question ( provided in the dataset ) with the intuition that the meta-classifier will learn to trust different representations for different grade levels .
Empirical Results
We evaluate AHE on two QA tasks : AI2 's Reasoning Challenge ( ARC ) : this is a multiple -choice question dataset , containing science exam questions .
The dataset is split in two partitions : Easy and Challenge , where the latter partition contains the more difficult questions that require reasoning Results and discussion :
Tables 1 and 2 summarize the performance of multiple AHE variants , compared against several baselines and previous works , on two datasets .
We draw several observations from these : ( 1 ) The mostly unsupervised AHE , i.e. , with the only supervised component being the InferSent embeddings , has solid and stable performance across the three datasets : best on ARC Easy , second best on ARC Challenge ( see lines 18 - 21 in Table 1 ) , and top three on WikiQA for MRR ( see lines 21 - 24 in Table 2 ) .
We find these results encouraging : AHE outperforms many complex supervised neural approaches , including methods having multiple RNNs and stacked attention layers ( Wang et al. , 2017 b ; He and Lin , 2016 ; Miller et al. , 2016 ; Yin et al. , 2015 ; Miao et al. , 2016 ; Musa et al. , 2018 ; Mihaylov et al. , 2018 ) , despite the fact that it relies mostly on simple , unsupervised components .
( 2 ) AHE ports well between different partitions ( Easy and Challenge ) of same dataset ( ARC ) , unlike many of the previous approaches .
For example , neural architectures that perform well on ARC Challenge perform worse than a simple IR baseline on ARC Easy ( see , e.g. , rows 14 and 15 in Table 1 ) or vice versa ( see lines 9 - 12 ) .
This lack of portability occurs despite these models being trained / tested within the same partition in Table 1 .
To emphasize this issue , we explore more aggressive domain transfer settings in Section 5.2 .
( 3 ) Ablation analysis - The alignment performance from individual components of AHE are shown in the baseline blocks of Tables 1 and 2 when selecting the correct answer when at least one of the representations proposes it , the oracle system achieves 85.1 P@1 on ARC Easy , 68.1 P@1 on ARC Challenge , and 93.71 MAP on WikiQA .
The supervised AHE , which uses a feed-forward neural network to learn when to trust each representation demonstrates that ( some of ) this complementarity can be learned : the supervised AHE consistently outperforms its unsupervised counterpart , albeit by small amounts .
Further , line 23 in Table 1 indicates that additional information about the questions ( i.e. , grade information ) is beneficial , as it provides the meta-classifier more grounding on when to trust which representation .
We analyze this complementarity further in Section 5.1 .
Analysis
To explore the potential of AHE and further understand its individual components , we conducted the following analyses : and [ 73 - 86 ] % in the Easy partition .
Our current meta-classifier only begins to mine this complementarity , but it is limited because it has no information about the question and candidate answers ( other than their scores ) .
We conjecture that considerable performance improvements are possible when such a meta-classifier includes additional information such as question type , question encoding , etc .
Our initial results that include grade information ( line 23 in Table 1 ) support this hypothesis .
We leave a further exploration of this direction as future work .
Domain Transfer
As shown in Table 1 and discussed in the previous section , many supervised neural methods do not perform robustly across different partitions ( Easy and Challenge ) of the same ARC dataset , even though they were trained within each partition .
This raises the question of how stable is their performance when trained / tested in different domains , which is closer to a real-world deployment scenario ?
To answer this question , we trained and tested two state - of- the - art neural models , BiL-STM Max-out ( Mihaylov et al. , 2018 ; Conneau et al. , 2017 ) and BiMPM ( Wang et al. , 2017 b ) , across three domains : ARC Easy , ARC Challenge , and WikiQA .
We selected these two approaches because of they are end-to - end neural methods , and they achieve good performance on all datasets .
Further , BiMPM is reminiscent of a supervised alignment method , since it computes the overall similarity of question and answers by aligning the tokens ' LSTM hidden states .
The results are summarized in Table 3 .
The table highlights that the performance of these systems varies considerably based on the training domain , even underperforming a random baseline in some configurations .
In contrast , the unsupervised AHE does not require training , and obtains state - of - theart , stable performance across the three datasets .
This analysis suggests that future QA evaluations should consider domain transfer as another evaluation measure , to quantify the performance of QA systems under realistic scenarios .
Brief Qualitative Analysis
We manually analyzed the questions answered incorrectly by AHE and observed that many of the candidate answers were partially answering the questions .
As shown in Figure 5 , candidate answers 2 and 5 are partially answering the question , while candidate answers 1 and 3 provide topically relevant information .
To select the correct answer in such complex questions , especially for short questions , a successful method would have to incorporate inference , e.g. , recognizing process questions such as the one in the figure and coupling with it with a dedicated problem solving method ( Clark et al. , 2013 ) .
We leave the integration of inference methods with AHE as future work .
Conclusion
We proposed a simple , mostly - unsupervised alignment model for non-factoid QA , which operates over multiple contextualized embedding representations that model the text at different levels of abstraction .
Despite its simplicity , our approach obtains good performance ( top three or higher ) that is stable across three QA datasets .
Our analysis indicates that the different levels of abstraction ( character , word , sentence ) capture distinct semantics .
We showed that this can be modeled with a metaclassifier that learns when and how much to trust Question - how a water pump works ?
1 . A large , electrically driven pump ( electropump ) for waterworks near the Hengsteysee , Germany .
2 . A pump is a device that moves fluids ( liquids or gases ) , or sometimes slurries , by mechanical action .
3 . Pumps can be classified into three major groups according to the method they use to move the fluid : direct lift , displacement , and gravity pumps .
4 . Pumps operate by some mechanism ( typically reciprocating or rotary ) , and consume energy to perform mechanical work by moving the fluid .
5 . Pumps operate via many energy sources , including manual operation , electricity , engines , or wind power . the predictions over each representation , and that this has a beneficial impact on performance .
All in all , our work indicates that the first , and possibly best , investment in the design of a QA system should be on contextualized embeddings rather than custom , complex neural architectures .
When such embeddings are available , state- ofthe - art performance that is competitive with modern neural approaches for QA can be obtained with simple alignment - based aggregation strategies .
Minimally , our work should be regarded as a new , strong baseline for non-factoid question answering or answer sentence selection .
Figure 2 : 2 Figure 2 : AHE architecture illustrated for the multiple -choice question setting .
The left text consists with of the question concatenated with the answer candidate ; the right text is a supporting paragraph retrieved from an external KB .
The same alignment score is computed over multiple representations of text , and then aggregated through an ensemble model .
