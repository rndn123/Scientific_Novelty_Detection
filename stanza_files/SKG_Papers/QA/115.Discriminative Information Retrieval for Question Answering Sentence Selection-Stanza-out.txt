title
Discriminative Information Retrieval for Question Answering Sentence Selection
abstract
We propose a framework for discriminative IR atop linguistic features , trained to improve the recall of answer candidate passage retrieval , the initial step in text - based question answering .
We formalize this as an instance of linear feature - based IR , demonstrating a 34 % - 43 % improvement in recall for candidate triage for QA .
Introduction Question answering ( QA ) with textual corpora is typically modeled as first finding a candidate set of passages ( sentences ) that may contain an answer to a question , followed by an optional candidate reranking stage , and then finally an information extraction ( IE ) step to select the answer string .
QA systems normally employ an information retrieval ( IR ) system to produce the initial set of candidates , usually treated as a black box , bag-of-words process that selects candidate passages best overlapping with the content in the question .
Recent efforts in corpus-based QA have been focused heavily on reranking , or answer sentence selection : filtering the candidate set as a supervised classification task to single out those that answer the given question .
Extensive research has explored employing syntactic / semantic features ( Yih et al. , 2013 ; Wang and Manning , 2010 ; Heilman and Smith , 2010 ; Yao et al. , 2013a ) and recently using neural networks ( Yu et al. , 2014 ; Severyn and Moschitti , 2015 ; Wang and Nyberg , 2015 ; Yin et al. , 2016 ) .
The shared aspect of all these approaches is that the quality of reranking a candidate set is upper-bounded by the initial set of candidates : unless one plans on reranking the entire corpus for each question as it arrives , one is still reliant on an initial IR stage in order to obtain a computationally feasible QA system .
Huang et al. ( 2013 ) used neural networks and cosine distance to rank the candidates for IR , but without providing a method to search for the relevant documents in sublinear time .
We propose a framework for performing this triage step for QA sentence selection and other related tasks in sublinear time .
Our method shows a log-linear model can be trained to optimize an objective function for downstream reranking , and the resulting trained weights can be reused to retrieve a candidate set .
The content that our method retrieves is what the downstream components are known to prefer : it is trainable using the same data as employed in training candidate reranking .
Our approach follows
Yao et al . ( 2013 b ) who proposed the automatic coupling of QA sentence selection and IR by augmenting a bag-of-words query with desired named entity ( NE ) types based on a given question .
While Yao et al. showed improved performance in IR as compared with an off-the-shelf IR system , the model was proof-of-concept , employing a simple linear interpolation between bagof-words and NE features with a single scalar value tuned on a development set , kept static across all types of questions at test time .
We generalize
Yao et al . 's intuition by casting the problem as an instance of classification - based retrieval ( Robertson and Sp?rck Jones , 1976 ) , formalized as a discriminative retrieval model ( Cooper et al. , 1992 ; Gey , 1994 ; Nallapati , 2004 ) allowing for the use of NLP features .
Our framework can then be viewed as an instance of linear feature - based IR , following Metzler and Croft ( 2007 ) .
To implement this approach , we propose a general feature - driven abstraction for coupling retrieval and answer sentence selection .
NETYPE = LOC 1 NETYPE = GPE 1 ? = ? 1 NE - GPE = Egypt 1 WORD = african 1 WORD = egypt 1 ? = ? 1 MIPS Retrieval t ? ( f NE ) t ? ( f TfIdf ) t ? ? ( f wh ? f lat ) ?
Figure 1 : Steps in mapping natural language questions into weighted features used in retrieval . ( 2006 ) , and we show significant improvements over a bag-of-words of baseline on a novel Wikipediaderived dataset we introduce here , based on WIK -IQA ( Yang et al. , 2015 ) .
Approach Formally , given a candidate set D = {p 1 , ? ? ? , p N } , a query q and a scoring function F ( q , p ) , an IR system retrieves the top-k items under the objective arg max p?D F ( q , p ) .
( 1 ) If the function F is simple enough ( e.g. tf-idf ) , it could be easily solved by traditional IR techniques .
However , tackling this problem with a complex F via straightforward application of supervised classification ( e.g. , recent neural network based models ) requires a traversal over all possible candidates , i.e. the corpus , which is computationally infeasible for any reasonable collection .
Let f Q ( q ) refer to feature extraction on the query q , with corresponding candidate - side feature extraction f P ( p ) on the candidate , and finally f QP ( q , p ) extracts features from a ( query , candidate ) pair is defined in terms of f Q and f P via composition ( defined later ) : f QP ( q , p ) = C(f Q ( q ) , f P ( p ) ) .
( 2 ) From a set of query / candidate pairs we can train a model M such that given the feature vector of a pair ( q , p ) , its returning value M ( f QP ( q , p ) ) represents the predicted probability of whether the passage p answers the question q.
This model is chosen to be a log-linear model with the feature weight vector ? , leading to the optimization problem arg max p?D ? ? f QP ( q , p ) .
( 3 )
This is in accordance with the pointwise reranker approach , and is an instance of the linear featurebased model of Metzler and Croft ( 2007 ) .
Under specific compositional operations in f QP the following transformation can be made : ? ? f QP ( q , p ) = t ? ( f Q ( q ) ) ? f P ( p ) . ( 4 ) This is elaborated in ?
4 . We project the original feature vector of the query f Q ( q ) to a transformed version t ? ( f Q ( q ) ) : this transformed vector is dependent on the model parameters ? , where the association learned between the query and the candidate is incorporated into the transformed vector .
This is a weighted , trainable generalization of query expansion in traditional IR systems .
Under this transformation we observe that the joint feature function f QP ( q , p ) is decomposed into two parts with no interdependency - the original problem in Eq. ( 4 ) is reduced to a standard maximum inner product search ( MIPS ) problem as seen on the RHS of Eq. ( 4 ) .
Under sparse assumptions ( where the query vector and the candidate feature vector are both sparse ) , this MIPS problem can be efficiently ( sublinearly ) solved using classical IR techniques ( multiway merging of postings lists ) .
Features
A feature vector can be seen as an associative array that maps features in the form " KEY=value " to realvalued weights .
One item in a feature vector f is denoted as " ( KEY = value , weight ) " , and a feature vector can be seen as a set of such tuples .
We write f ( KEY=value ) = weight to indicate that the features serve as keys to the associative array , and ?
X is the weight of the feature X in the trained model ?.
Question features f wh : Question word , typically the wh-word of a sentence .
If it is a question like " How many " , the word after the question word is also included in the feature , i.e. , feature " ( QWORD =how many , 1 ) " will be added to the feature vector .
f lat : Lexical answer type ( LAT ) , if the query has a question word : " what " or " which " , we identify the LAT of this question ( Ferrucci et al. , 2010 ) , which is defined as the head word of the first NP after the question word .
E.g. , " What is the city of brotherly love ? " would result in " ( LAT=city , 1 ) " .
2 f NE : All the named entities ( NE ) discovered in this question .
E.g. , " ( NE -PERSON = Margaret Thatcher , 1 ) " would be generated if Thatcher is mentioned .
f TfIdf :
The L 2 -normalized tf - idf weighted bag-ofwords feature of this question .
An example feature would be " ( WORD = author , 0.454 ) " .
Passage features
All passage features are constrained to be binary .
f BoW : Bag-of-words : any distinct word x in the passage will generate a feature " ( WORD=x , 1 ) " .
f NEType : Named entity type .
If the passage contains a name of a person , a feature " ( NE- TYPE=PERSON , 1 ) " will be generated .
f NE : Same as the NE feature for questions .
Feature vector operations Composition
Here we elaborate the composition C of the question feature vector and passage feature vector , defining two operators on feature vectors : Cartesian product ( ? ) and join ( ) .
For any feature vector of a question f Q ( q ) = {( k i = v i , w i ) } , ( w i ? 1 ) 3 and any feature vector of a passage f P ( p ) = {( k j = v j , 1 ) } , the Cartesian product and join of them is defined as f Q ( q ) ? f P ( p ) = {( ( k i , k j ) = ( v i , v j w i ) }
f Q ( q ) f P ( p ) = {( ( k i = k j ) = 1 , w i ) }.
Notation ( k i = k j ) = 1 denotes a feature for a question / passage pair , that when present , witnesses the fact that that the value for feature k i on the question side is the same as the feature k j on the passage side .
The composition that generates the feature vector for the question / passage pair is therefore defined as C ( f Q ( q ) , f P ( p ) ) = ( f wh ( q ) ? f lat ( q ) ) ? f NEType ( p ) + ( f wh ( q ) ? f lat ( q ) ) ? f BoW ( p ) + f NE ( q ) f NE ( p ) + f TfIdf ( q ) f BoW ( p ) . ( 5 ) ( f wh ( q ) ? f lat ( q ) ) ? f NEType ( p ) captures the association of question words and lexical answer types with the expected type of named entities .
( f wh ( q ) ? f lat ( q ) ) ? f BoW ( p ) captures the relation between some question types with certain words in the answer .
f NE ( q ) f NE ( p ) captures named entity overlap between questions and answering sentences .
f TfIdf ( q ) f BoW ( p ) measures general tf -idfweighted context word overlap .
Using only this feature without the others effectively reduces the system to a traditional tf - idf - based retrieval system .
Projection Given a question , it is desired to know what kind of features that its potential answer might have .
Once this is known , an index searcher will do the work to retrieve the desired passage .
For the Cartesian product of features , we define t ? ? ( f ) = {( k = v , w ? ( k , k ) = ( v , v ) ) |( k = v , w ) ? f } , for all k , v such that ? ( k , k ) =( v, v ) = 0 , i.e. feature ( k , k ) = ( v , v ) appears in the trained model .
For join , we have t ?
( f ) = {( k = v , w ? ( k=k ) =1 ) |( k = v , w ) ? f } , for all k such that ?
( k=k ) =1 = 0 , i.e. feature ( k = k ) = 1 appears in the trained model .
It can be shown from the definitions above that t ? ? ( f ) ? g = ? ? ( f ? g ) ; t ? ( f ) ? g = ? ? ( f g ) .
Then the transformed feature vector t( q ) of an expected answer passage given a feature vector of a question f Q ( q ) is : t( q ) = t ? ? ( f wh ( q ) ? f lat ( q ) ) + t ? ( f NE ( q ) + f TfIdf ( q ) ) .
Calculating the vector t( q ) is computationally efficient because it only involves sparse vectors .
We have formally proved Eq. ( 4 ) by the feature vectors we proposed , showing that given a question , we can reverse-engineer the features we expect to be present in a candidate using the transformation function t ? , which we will then use as a query vector for retrieval .
Retrieval
We use Apache LUCENE 4 to build the index of the corpus , which , in the scenario of this work , is the feature vectors of all candidates f P ( p ) , p ? D .
This is an instance of weighted bag-of-features instead of common bag-of-words .
For a given question q , we first compute its feature vector f ( q ) and then compute its transformed feature vector t ? ( q ) given model parameters ? , forming a weighted query .
We modified the similarity function of LUCENE when executing multiway postings list merging so that fast efficient maximum inner product search can be achieved .
This classical IR technique ensures sublinear performance because only vectors with at least one overlapping feature , instead of the whole corpus , is traversed .
5 5 Experiments TREC Data
We use the training and test data from Yao et al . ( 2013 b ) .
Passages are retrieved from the AQUAINT Corpus ( Graff , 2002 ) , which is NERtagged by the Illinois Named Entity Tagger ( Ratinov and Roth , 2009 ) with an 18 - label entity type set .
Questions are parsed using the Stanford CORENLP ( Manning et al. , 2014 ) package .
Each question is paired with 10 answer candidates from AQUAINT , annotated for whether it answers the question via crowdsourcing .
The test data derives from Lin and Katz ( 2006 ) , which contains 99 TREC questions that can be answered in AQUAINT .
We follow Nallapati ( 2004 ) and undersample the negative class , taking 50 sentences uniformly at random from the AQUAINT corpus , per query , filtered to ensure no such sentence matches a query 's answer pattern as negative samples to the training set .
Wikipedia Data
We introduce a novel evaluation for QA retrieval , based on WIKIQA ( Yang et al. , 2015 ) , which pairs questions asked to Bing with their most associated Wikipedia article , along with sentence - level annotations on the introductory section of those articles as to whether they answer the question .
6
We automatically aligned WIKIQA annotations , which was based on an unreported version of Wikipedia , with the Feb. 2016 snapshot , using for our corpus the introductory section of all Wikipedia articles , processed with Stanford CORENLP .
Alignment was performed via string edit distance , leading to a 55 % alignment to the original annotations .
We also plot the performance of these systems at different ks on a log-scale ( shown in Fig. 2 and Fig. 3 ) .
We use two metrics here : recall at k ( R@k ) and success at k ( S@k ) .
Success at k is the percentage of queries in which there was at least one relevant answer sentence among the first k retrieved result by a specific system , which is the true upper bound for downstream tasks .
Again , DiscIR demonstrated significantly higher 8 Results on dev data is not reported in Yao et al . ( 2013 b ) . DiscIR- R@k DiscIR - S@k Lucene-R@k Lucene-S@k Figure 3 : The R@k and S@k curve for different models in the WIKIQA / Wikipedia setting .
recalls than baselines at different ks and across different datasets .
Success rate at different ks are also uniformly higher than LUCENE , and at most ks higher than the model of Yao et al .'s.
6 Conclusion and Future Work Yao et al . ( 2013 b ) proposed coupling IR with features from downstream question answer sentence selection .
We generalized this intuition by recognizing it as an instance of discriminative retrieval , and proposed a new framework for generating weighted , feature - rich queries based on a query .
This approach allows for the straightforward use of a downstream feature - driven model in the candidate selection process , and we demonstrated how this leads to a significant gain in recall , b-pref and MAP , hence providing a larger number of correct candidates that can be provided to a downstream ( neural ) reranking model , a clear next step for future work .
NE - GPE = Egypt 0.923
What continent is Egypt in ?
QWORD , LAT = what , continent 1 NE - GPE = Egypt 1 WORD = continent 0.292 NETYPE = LOC NETYPE = GPE ? = ? 1.417 0.677 WORD = egypt 0.781 WORD = continent 3.581 q? f Q f Q ( q ) WORD = egypt 9.577 train f QP Training w t ? ( q ) f( p ) corpus f P Indexing f P ( p ) Retrieval 1 . 2 . 3 . He said that Egypt 's status among the African states has greatly been enhanced .
1 Our exper- iments demonstrate state - of- the - art results on QA sentence selection on the dataset of Lin and Katz
Table 1 1 dev/ test reflects the
Table 1 : 1 Summary of the datasets .
Setup
The model is trained using LIBLINEAR ( Fan et al. , 2008 ) , with heavy L 1 - regularization ( feature selection ) to the maximum likelihood objective .
The model is tuned on the dev set , with the objec- tive of maximizing recall .
Baseline systems
Recent work in neural network based reranking is not directly applicable here as those are linear with respect to the number of candi- date sentences , which is computationally infeasible given a large corpus .
Off - the-shelf LUCENE : Directly indexing the sen- tences in LUCENE and do sentence retrieval .
This is equivalent to maximum tf -idf retrieval .
Yao et al . ( 2013 b ) :
A retrieval system which aug - ments the bag-of-words query with desired named entity types based on a given question .
Evaluation metrics ( 1 ) R@1k :
The recall in top- 1000 retrieved list .
Contrary to normal IR systems which optimize precision ( as seen in metrics such as P@10 ) , our system is a triaging system whose goal is to retrieve good candidates for downstream reranking : high recall within a large set of initial candidates is our foremost aim . ( 2 ) b-pref ( Buck - ley and Voorhees , 2004 ) : is designed for situations where relevance judgments are known to be far from complete , 7 computing a preference relation of whether judged relevant documents are retrieved ahead of judged irrelevant document ; ( 3 ) MAP : here .
Table 2 : 2 Performance of the QA retrieval systems .
https://github.com/ctongfei/probe.
If the question word is not " what " or " which " , generate an empty feature ( LAT= ? , 1 ).3
If wi > 1 , the vector can always be normalized so that the weight of every feature is less than 1 .
http://lucene.apache.org.5
The closest work on indexing we are aware of is by Bilotti et al . ( 2007 ) , who transformed linguistic structures to structured constraints , which is different from our approach of directly indexing linguistic features .
6
Note that as compared to the TREC dataset , there are some questions in WIKIQA which are not answerable based on the provided context alone .
E.g. " who is the guy in the wheelchair who is smart " has the answer " Professor Stephen Hawking , known for being a theoretical physicist , has appeared in many works of popular culture . "
This sets the upper bound on performance with WIKIQA below 100 % when using contemporary question answering techniques , as assumed
This is usually the case in passage retrieval , where complete annotation of all sentences in a large corpus as to whether they answer each question is not feasible beyond a small set ( such as the work of Lin and Katz ( 2006 ) ) .
