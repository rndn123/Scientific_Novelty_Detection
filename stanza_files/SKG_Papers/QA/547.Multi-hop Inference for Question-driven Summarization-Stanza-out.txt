title
Multi-hop Inference for Question - driven Summarization *
abstract
Question - driven summarization has been recently studied as an effective approach to summarizing the source document to produce concise but informative answers for nonfactoid questions .
In this work , we propose a novel question - driven abstractive summarization method , Multi-hop Selective Generator ( MSG ) , to incorporate multi-hop reasoning into question - driven summarization and , meanwhile , provide justifications for the generated summaries .
Specifically , we jointly model the relevance to the question and the interrelation among different sentences via a human-like multi-hop inference module , which captures important sentences for justifying the summarized answer .
A gated selective pointer generator network with a multi-view coverage mechanism is designed to integrate diverse information from different perspectives .
Experimental results show that the proposed method consistently outperforms stateof - the - art methods on two non-factoid QA datasets , namely WikiHow and PubMedQA .
Introduction Recent years have witnessed several attempts on exploring question - driven summarization , which aims at summarizing the source document with respect to a specific question , to produce a concise but informative answer in non-factoid question answering ( QA ) ( Tomasoni and Huang , 2010 ; Chan et al. , 2012 ; Song et al. , 2017 ) .
Unlike factoid QA ( Rajpurkar et al. , 2016 ) , e.g. , " Who is the author of Harry Potter ? " , whose answer is generally a single phrase or a short sentence with limited information , the answers for non-factoid questions are supposed to be more informative , involving some detailed analysis to explain or justify the final answers , such as questions in community QA ( Ishida et al. , 2018 ; Deng et al. , 2020a ) or explainable QA ( Fan et al. , 2019 ; Nakatsuji and Okui , 2020 ) .
As the example from PubMedQA ( Jin et al. , 2019 ) presented in Figure 1 , the answer can be regarded as the summary over the document driven by the reasoning process of the given question .
Most of related studies focus on query - based summarization approaches for summarizing the query - related content from the source document ( Shen and Li , 2011 ; Wang et al. , 2013 ; Cao et al. , 2016 ; Nema et al. , 2017 ) .
However , these approaches fall short of tackling question - driven summarization problem in QA scenario , since the query - based summarization process is typically based on semantic relevance measurement without a careful reasoning or inference process , which is essential to question - driven summarization .
Currently , question - driven summarization is mainly explored by traditional information retrieval methods to select sentences from the source document to construct the final answer ( Wang et al. , 2014 ; Song et al. , 2017 ; Yulianti et al. , 2018 ) , which heavily rely on hand-crafted features or tedious multi-stage pipelines .
Besides , compared to extractive summarization ( Cao et al. , 2016 ) , abstractive methods ( Nema et al. , 2017 ) can produce more coherent and logical summaries to answer the given question .
To this end , we study question - driven abstractive summarization to generate natural form of answers by summarizing the source document with respect to a specific question .
To tackle question - driven abstractive summarization , the content selection process for summarization is not only determined by the semantic relevance to the given question , but it also requires a human-like reasoning and inference process to consider the content interrelationship comprehensively and carefully across the whole source text for gener - Question :
Are human coronaviruses uncommon in patients with gastrointestinal illness ?
Document : < S> Coronaviruses infect numerous animal species causing a variety of illnesses including respiratory , neurologic and enteric disease .
< S>Human coronaviruses ( HCoV ) are mainly associated with respiratory tract disease but have been implicated in enteric disease .
< S> To investigate the frequency of coronaviruses in stool samples from children and adults with gastrointestinal illness by RT - PCR .
< S> Clinical samples submitted for infectious diarrhea testing were collected from December 2007 through March 2008 .
< S>RNA extraction and RT - PCR was performed for stools negative for Clostridium difficile using primer sets against HCoV -229E , HCoV - OC43 , HCoV - NL63 , and HCoV - HKU1 .
< S> Clinical data from samples positive for coronaviruses were reviewed and recorded .
< S> Samples from 479 patients were collected including 151 pediatric ( < or = 18 years ) , and 328 adults ( > 18 years ) .
< S> Of these samples , 4 patients ( 1.3 % , 2 adult ; 2 pediatric ) screened positive for the presence of a coronavirus .
< S> All detected coronaviruses were identified as HCoV - HKU1 .
< S> No stools screened positive for either HCoV -229E , HCoV - NL63 or HCoV - OC43 .
< S> All HCoV -HKU1 positive samples occurred between mid-January to mid-February .
< S> Clinical manifestations from HCoV -HKU1 positive patients included diarrhea , emesis and respiratory complaints .
< S> Three ( 75 % ) patients were admitted to the hospital with a median length of stay of 6 days .
< S> Answer : Coronaviruses as a group are not commonly identified in stool samples of patients presenting with gastrointestinal illness .
HCoV -HKU1 can be identified in stool samples from children and adults with gastrointestinal disease , with most individuals having respiratory findings as well .
No stool samples screened positive for HCoV - NL63 , HCoV -229E , or HCoV - OC43 .
ating the summary .
For instance , in Figure 1 , given the specific question , there are several highlighted sentences required to be concentrated for conducting summarization so as to generate the answer .
It leads to the necessity of measuring the importance of each sentence , instead of regarding the source text as an undifferentiated whole .
Among these highlighted sentences , only the italic sentences are directly related to the given question , while other highlighted sentences need to be inferred from their interrelationships with other sentences .
In other words , the generated summary is likely to lose important information , if we only focus on the semantically relevant content to the given question .
Moreover , it can be observed that one - time inference sometimes is insufficient for collecting all the required information for producing a summary .
In this example , the answer is summarized from both the 1st-hop and ::::::: 3rd-hop inference sentences in the document , indicating the importance of multi-hop reasoning for content selection in question - driven summarization .
In this work , we propose a question - driven abstractive summarization model , namely Multi-hop Selective Generator ( MSG ) , which incorporates multi-hop inference to summarize abstractive answers over the source document for non-factoid questions .
Concretely , the document is regarded as a hierarchical text structure to be assessed with the importance degree in both word - and sentencelevel for content selection .
Then we develop a multi-hop inference module to enable human-like multi-hop reasoning in question - driven summarization , which considers the semantic relevance to the question as well as the information consistency among different sentences .
Finally , a gated selec-tive pointer generator network with multi-view coverage mechanism is proposed to generate a concise but informative summary as the answer to the given question .
The main contributions of this paper can be summarized as follows : ( 1 ) We propose a novel question - driven abstractive summarization model for generating answers in non-factoid QA , which incorporates multi-hop reasoning to infer the important content for facilitating answer generation ; ( 2 ) We propose a multi-view coverage mechanism to address the repetition issue along with the multiview pointer network and generate informative answers ; ( 3 ) Experimental results show that the proposed method achieves state - of - the - art performance on WikiHow and PubMedQA datasets , and it is able to provide justification sentences as the evidence for the answer .
Related Works Query - based Summarization .
Early works on query - based summarization focus on extracting query -related sentences to construct the summary ( Lin et al. , 2010 ; Shen and Li , 2011 ) , which are later improved by exploiting sentence compression on the extracted sentences ( Wang et al. , 2013 ; Li and Li , 2014 ) .
Recently , some data-driven neural abstractive models are proposed to generate natural form of summaries with respect to the given query ( Nema et al. , 2017 ; Hasselqvist et al. , 2017 ) .
However , current studies on query - based abstractive summarization are restricted by the lack of large-scale datasets ( Baumel et al. , 2016 ; Nema et al. , 2017 ) .
One the other hand , some researchers spark a new pave of question - driven summarization in non-factoid QA ( Song et al. , 2017 ; Yulianti et al. , 2018 ; Deng et al. , 2020 b ) , which requires the ability of reasoning or inference for supporting summarization , not merely relevance measurement , and also preserves remarkable testbeds of largescale datasets .
Non-factoid Question Answering .
Different from factoid QA that can be tackled by extracting answer spans ( Rajpurkar et al. , 2016 ) or generating short sentences ( Nguyen et al. , 2016 ; Kocisk ?
et al. , 2018 ) , non-factoid QA aims at producing relatively informative and complete answers .
In the past studies , non-factoid QA focused on retrievalbased methods , such as answer sentence selection ( Nakov et al. , 2015 ) or answer ranking .
Recently , several efforts have been made on tackling long -answer generative question answering over supporting documents , which targets on questions that require detailed explanations ( Fan et al. , 2019 ) .
This kind of QA problem contains a large proportion of non-factoid questions , such as " how " or " why " type questions ( Koupaee and Wang , 2018 ; Ishida et al. , 2018 ; Deng et al. , 2020a ) .
Besides , some studies aim at generating a conclusion for the concerned question ( Jin et al. , 2019 ; Nakatsuji and Okui , 2020 ) .
Fan et al. ( 2019 ) propose a multi-task Seq2Seq model with the concatenation of the question and support documents to generate long-form answers .
Iida et al. ( 2019 ) and Nakatsuji and Okui ( 2020 ) incorporate some background knowledge into Seq2Seq model for why questions and conclusion -centric questions .
Some latest works ( Feldman and El- Yaniv , 2019 ; Yadav et al. , 2019 ; Nishida et al. , 2019a ) attempt to provide evidence or justifications for humanunderstandable explanation of the multi-hop inference process in factoid QA , where the inferred evidences are only treated as the middle steps for finding the answer .
However , in non-factoid QA , the intermediate output is also important to form a complete answer , which requires a bridge between the multi-hop inference and summarization .
Proposed Framework
We propose a question - driven abstractive summarization model , namely Multi-hop Selective Generator ( MSG ) .
The overview of MSG is depicted in Figure 2 , which consists of three main components : ( 1 ) Co-attentive Encoder ( Section 3.1 ) , ( 2 ) Multi-hop Inference Module ( Section 3.2 ) , and ( 3 ) Gated Selective Generator ( Section 3.3 ) .
Moreover , Multi-view Coverage Loss is integrated to the overall training procedure ( Section 3.4 ) .
Co-attentive Encoder Pre-trianed word embeddings , E q and E s i , of the question q and each sentence s i in the document D = {s 1 , s 2 , ... , s n } are input into the model .
We first encode the question and each sentence in the document by a Bi-LSTM ( Bidirectional Long Short - Term Memory Networks ) shared encoder to learn the word-level contextual information , H q , H s i ?
R l?d h , where l and d h denotes the sentence length and the dimension of the encoder output respectively .
The overall word- level representations H d for the document is sequentially concatenated by [ H s 1 , H s 2 , ... , H sn ] .
We compute the attention weights to align the word-level information between the question and the document sentences , and obtain the attentionweighted vectors of each word for both the question and the document sentences .
For the question q and the i-th sentence s i in the document D , we have : O qs i = tanh H T q U H s i , ( 1 ) ?
q i = softmax ( Max ( O qs i ) ) , ( 2 ) ? s i = softmax ( Max ( O qs i T ) ) , ( 3 ) where U ?
R d h ?d h is the attention matrix to be learned ; ? q i and ?
s i are co-attention weights for the question and i-th sentence in the document .
We conduct dot product between the attention vectors and the word-level representations to generate the sentence representations for the question and the document : M q = 1 n n i=1 H T q ? q i ( 4 ) M s = [ H T s 1 ? s 1 : ... : H T sn ? sn ] , ( 5 ) where M q and M s denote the sentence - level representations for the question and the document .
Multi-hop Inference Module Multi-hop Inference
Module measures the degree of importance for each sentence in the document to generate the answer , through a multi-hop reasoning procedure , which contains two kinds of inference units : Attentive Unit and MAR Unit .
Attentive Unit Attentive
Unit basically measures the matching degree between each sentence in the document and the given question by the following vanilla attention mechanism : Shared Sentence Encoder s 1 s n q Shared Sentence Encoder Shared Sentence Encoder Co-attention Module Co-attention Module Multi-hop Inference Module ? ? ? ? ?
H q H d ?
Z q s 1 s n MAR Unit ? MAR Unit Attentive Unit Gated Selective Generator Answer 1st-hop 2nd-hop 3rd-hop Aggregation Z M s 1 M s n M q Figure 2 : The overview of Multi-hop Selective Generator ( MSG ) .
m dq = tanh ( M s W m M q ) , ( 6 ) ? s = softmax ( ?
T m m dq ) , ( 7 ) Attentive ( M s , M q ) = M s ? s , ( 8 ) where W m and ? m are the attention matrices to be learned .
? s is the sentence - level attention weight which measures the matching degree of each document sentence with the given question .
denotes the element- wise product for obtaining the attentive sentence - level representations for the document .
MAR Unit Maximal Marginal Relevance ( MMR ) is an IR model that can be adopted to measure the queryrelevancy and information - redundancy simultaneously for extractive summarization ( Carbonell and Goldstein , 1998 ) .
However , as for the content selection in abstractive summarization , the relevance to both the question and the other sentences in the document should be taken into consideration for a high recall of selecting necessary content .
Thus , we propose Maximal Absolute Relevance ( MAR ) to select highly salient sentences for generating the summary , which is formulated as : mar i =?Sim 1 ( M s i , M q ) + ( 1 ? ? ) max s j ?D , j =i Sim 2 ( M s i , M s j ) , ( 9 ) where ? is a hyper-parameter for balancing the question - relevancy and information - consistency measurement .
The relevance to the question is calculated by : Sim 1 ( M s i , M q ) = M s i U 1 M q , ( 10 ) where U 1 is a similarity matrix to be learned .
We apply an attention mechanism over other sentences in the document to choose the highest relevance score , which can be regarded as the reasoning procedure where the next -hop justification sentences are supposed to be highly related to the last - hop justification sentences .
e ij = tanh ( M s i U 2 M s j ) , ( 11 ) Sim 2 ( M s i , M s j ) = exp( e ij ) j exp( e ij ) , ( 12 ) where U 1 is a similarity matrix to be learned .
Then the weighted sentence representations are computed by the element-wise product of the original sentence representations and the MAR scores gated by a sigmoid function denoted as ? : MAR ( M s , M q ) = M s ?( mar ) .
( 13 ) Overall , MAR
Unit assigns higher weights to sentences in two situations : ( i ) Those sentences are correlated to the given question , due to the first term in Equation 9 , ( ii )
Those sentences are consistent with the highly weighted justification sentences from the last hop , due to the second term .
Reasoning Procedure
In accordance with human-like multi-hop inference procedure , the first hop is supposed to capture the semantic-relevant sentences to the given question .
Then the subsequent hops should consider not only the relevance to the question , but also the information - consistency with the previous attended sentences .
Hence , the Attentive Unit is adopted as the 1st-hop inference unit , while the MAR Unit is served as the kth-hop unit , where k >
1 . Before each hop , a Bi-LSTM layer is employed to refine the input sentence representation .
For instance , a 3 - hop inference procedure is as follows : M ( 1 ) s = Attentive ( Bi- LSTM ( M s ) , M q ) , ( 14 ) M ( 2 ) s = MAR ( Bi- LSTM ( M ( 1 ) s ) , M q ) , ( 15 ) M ( 3 ) s = MAR ( Bi- LSTM ( M ( 2 ) s ) , M q ) . ( 16 ) y Partial Answer Decoder ? ? ? ? s t ?
H q H d ? Z ? Question Attention Document Attention Sentence Attention
Then , we merge the 3 - hop sentence representations ,
Decoder State
Gated Attention
Final Distribution Ms = [ M ( 1 ) s , M ( 2 ) s , M ( 3 ) s ] , via the following attention mechanism : ? h = softmax ( ?
T h tanh ( W h Ms ) ) , ( 17 ) Z = M T s ? h , ( 18 ) where W h and ?
h are attention matrices to be learned .
Z is the final sentence - level document representation for justifying the importance degree of each sentence in the decoding phase .
Gated Selective Generator
We obtain the word- level representations H q and H d for the question and document , respectively , from the encoding phase , and the sentence - level document representation Z via the multi-hop inference module .
Figure 3 depicts the Gated Selective Pointer Generator Network in MSG .
A unidirectional LSTM is adopted as the decoder .
At each step t , the decoder produces hidden state s t with the input of the previous word w t?1 .
The attention for each word in the question and the document , ? q t and ?
d t , are generated by : e q j t = ?
q t T tanh ( W q H q j + W qs s t + b q ) , ( 19 ) ?
q t = softmax ( e q t ) , ( 20 ) e d i t = ?
d t T tanh ( W d H d i + W ds s t + b d ) , ( 21 ) ? d t = softmax ( e d t ) , ( 22 ) where W q , W qs , W d , W ds , ? q t , ?
d t , b q , b d are parameters to be learned .
Then , we incorporate the multi-hop inference results Z to compute the gated attention weights ?
t for each sentence in the document : ? t = ?(?
s t T tanh ( W s Z k + W ss s t + b s ) ) , ( 23 ) where W s , W ss , ?
s t , b s are parameters to be learned .
We re-weight the word-level document attention scores ?
d gated by the sentence - level document attention scores ? to attend important justification sentences along with the decoding process : ?d i t = ?
d i t ? t , d i ?s k i ?
d i t ? t , d i ?s k . ( 24 ) Thus , the re-weighted word- level document attention ?d naturally blends with the results from the multi-hop inference module to enhance the influence of those important justification sentences .
Finally , a multi-view pointer - generator architecture is designed to generate answers with multihop inference results as well as handle the multiperspective out - of- vocabulary ( OOV ) issue .
Such approach enables MSG to copy words from the question and be aware of the differential importance degree of different sentences in the document .
The attention weights ?
q t and ?d t are used to compute context vectors c q t and c d t as the probability distribution over the source words : c q t = H T q ?
q t , c d t = H T d ?d t . ( 25 )
The context vector aggregates the information from the source text for the current step .
We concatenate the context vector with the decoder state s t and pass through a linear layer to generate the answer representation h s t : h s t = W 1 [ s t : c q t : c d t ] + b 1 , ( 26 ) where W 1 and b 1 are parameters to be learned .
Then , the probability distribution P v over the fixed vocabulary is obtained by passing the answer representation h s t through a softmax layer : P v ( y t ) = softmax ( W 2 h s t + b 2 ) , ( 27 ) where W 2 and b 2 are parameters to be learned .
The final probability distribution of y t is obtained from three views of word distributions : P q ( y t ) = i:w i =w ? q i t , ( 28 ) P d ( y t ) = i:w i =w ?d i t , ( 29 ) P all ( y t ) = [ P v ( y t ) , P q ( y t ) , P d ( y t ) ] , ( 30 ) ? = softmax ( W ? [ s t : c q t : c d t ] + b ? ) , ( 31 ) P ( y t ) = ? ?
P all ( y t ) , ( 32 ) where W ? and b ? are parameters to be learned , ? is the multi-view pointer scalar to determine the weight of each view of the probability distribution .
End-to-end Training Multi-view Coverage Loss .
The original coverage mechanism ( See et al. , 2017 ) could only prevent repeated attention from one certain source text .
However , the repetition problem becomes more severe , as we leverage both the question and document as the source text .
Besides , similar to multi-view pointer network , coverage losses of different sources are supposed to be weighted by their contribution .
Therefore , we design a multi-view coverage mechanism to address this issue as well as balance the generating and copying processes .
In each decoder timestep t , the coverage vector c t = t?1 t =0 a t is used to represent the degree of coverage so far .
The coverage vector c t will be applied to compute the attention weight ? t in Equations 19 and 21 .
The coverage loss is trained to penalize the repetition in updated attention weight ?
t from all views .
The re-normalized pointer weights ? = ? c / c?{q , d} ? c are employed to balance the coverage loss of different views : L cov = ?
1 T T t=1 i min (?
i t , c i t ) .
( 33 ) Overall Loss Function .
The overall model is trained to minimize the negative log likelihood and the multi-view coverage loss : L = ? 1 T T t=0 logP ( w * t ) + ?L cov , ( 34 ) where ? is a hyper-parameter to balance losses .
Experiments
Datasets and Evaluation Metrics
We evaluate on a large-scale summarization dataset with non-factoid questions , WikiHow ( Koupaee and Wang , 2018 ) , and a non-factoid QA dataset with abstractive answers , PubMedQA ( Jin et al. , 2019 ) .
WikiHow is an abstractive summarization dataset collected from a community - based QA website , WikiHow 1 , in which each sample consists of a non-factoid question , a long article , and the corresponding summary as the answer to the given question .
PubMedQA is a conclusion - based biomedical QA dataset collected from PubMed 2 abstracts , in which each instance is composed of a question , a context , and an abstractive answer which is the summarized conclusion of the context corresponding to the question .
and PubMedQA datasets are shown in Table 1 3 .
We adopt ROUGE F1 ( R1 , R2 , RL ) for automatically evaluating the summarized answers .
Besides , human evaluation and Distinct scores are adopted for analysis .
Baseline Methods and Implementations
To evaluate the proposed method , we compare with several baselines and state - of - the - art methods on query - based abstractive summarization and generative QA .
We first employ four widely - adopted summarization baseline methods , including two unsupervised extractive methods , LEAD3 and MMR , and two abstractive methods , S2SA ( Bahdanau et al. , 2015 ) , and PGN ( See et al. , 2017 ) .
Then two popular query - based abstractive summarization methods are adopted for evaluation : ( 1 ) SD 2 ( Nema et al. , 2017 ) , which is a sequence - tosequence model with a query attention , and ( 2 ) QS ( Hasselqvist et al. , 2017 ) , which incorporates question information into the pointer - generator network with the vanilla attention mechanism .
Finally , we implement two latest generative QA models for comparisons : ( 1 ) S2S -MT ( Fan et al. , 2019 ) , which uses a multi-task Seq2Seq model with the concatenation of question and support document , and ( 2 ) QPGN ( Deng et al. , 2020a ) , which is a question - driven pointer - generator network with co-attention between the question and document .
We train all the models with pre-trained GloVE embeddings 4 of 300 dimensions and set the vocabulary size to 50k .
During training and testing procedure , we restrict the length of generated summaries within 50 words .
As for the proposed method , we train with a learning rate of 0.15 and an initial accumulator value of 0.1 .
The dropout rate is set to 0.5 .
The hidden unit sizes of the BiLSTM encoder and the LSTM decoder are all set to 256 .
We train our models with the batch size of 32 .
All other parameters are randomly initialized from [ - 0.05 , 0.05
Performance Comparison
Table 2 summarizes the experimental results on both datasets .
As for WikiHow , which is an abstractive summarization dataset with non-factoid questions , current query - based summarization ( SD 2 , QS ) and generative QA approaches ( S2S - MT , QPGN ) barely improve the performance from traditional summarization approaches .
It indicates that the question information is not fully exploited for summarization , while MSG outperforms all these methods with a noticeable margin , about 2 % .
Besides , since PubMedQA is a QA dataset with abstractive answers , we can observe that QPGN , which employs special design for modeling the interaction between the question and document , achieves relatively better performance than other summarization methods .
Favorably MSG raises the state- of- the - art result by about 3 % .
Furthermore , MSG achieves promising improvements via the multi-hop inference on these two datasets .
We conduct human evaluation to evaluate the generated answer from four aspects : ( 1 ) Informativity : how rich is the generated answer in information ?
( 2 ) Conciseness : how concise is the sum- mary ?
( 3 ) Readability : how fluent and coherent is the summary ?
( 4 ) Correctness : how well does the generated answer respond to the given question ?
We randomly sample 50 questions from two datasets and generate their answers with three query - based summarization methods , including SD 2 , QS , QPGN and the proposed MSG .
Three annotators are asked to score each generated answer with 1 to 5 ( higher the better ) .
Results are presented in Table 3 .
We observe that MSG consistently and substantially outperforms existing querybased summarization methods in all aspects , especially for the informativeness and correctness .
The results show that MSG effectively generates concise but also informative answers , since MSG not only considers question - related information , but also captures logically necessary content for answering the given question via multi-hop reasoning .
Consequently , it leads to a more precise answer .
Discussions
Ablation Study
We conduct ablation study to validate the effectiveness of different components in MSG as well as the detailed design for the multi-hop inference module .
The upper part in Table 4 presents the ablation study on multi-hop inference module .
First of all , the model performance suffers a great decrease from discarding the multi-hop inference module on two datasets , showing the necessity of incorporating the multi-hop reasoning into the questiondriven summarization .
In specific , the fusion of the selective sentence representations from all hops brings performance improvement , including aggre - gating all the hops as well as applying attention to weight the importance of each hop .
Besides , it also achieves better performance to apply the proposed MAR Unit as the multi-hop unit , instead of repeatedly using Attentive Unit , indicating that it is not enough to only consider the question - related information , while the interrelationship among different sentences also attaches great importance .
The second part in Table 4 presents the ablation study in terms of discarding other model components in MSG .
In general , all the components contribute to the final performance to a certain extent .
In detail , there are several notable observations : ( 1 ) Some existing works ( Hsu et al. , 2018 ; Nishida et al. , 2019 b ) apply softmax function to normalize the weights of different sentences in the decoding phase , which falls short of differentiating the importance degree of each sentence .
The result shows that MSG achieves better performance by employing gated attention to distinguish salient justification sentences for generating the summaries .
( 2 ) Discarding the question pointer casts a noticeably greater decrease on PubMedQA than WikiHow .
We conjecture that those questions from PubMedQA contain more words available to be copied for generating precise summaries , as the statistic of the question length shown in Table 1 .
These results also validate the importance of multi-view PGN on question - driven abstractive summarization , which is underutilized in current methods .
( 3 ) Multi-view coverage ( MVC ) loss makes a great contribution to the performance by alleviating the severe repetition problem along with the multi-view PGN .
Analysis of Multi-hop Reasoning
As the results presented in Section 4.3 , MSG ( 3 - Hop ) outperforms MSG ( 1- Hop ) by 0.5 % and 0.7 % on WikiHow and PubMedQA , respectively , indicating the effectiveness of incorporating multi-hop reasoning in question - driven summarization .
Figure 4 ( a ) presents the model performance in terms of using different hops of reasoning .
We can see that , as expected , the performance of the model begins with growth when increasing the number of hops for reasoning .
However , the performance becomes generally unchanged ( e.g. , WikiHow ) or even slightly decreases ( e.g. , PubMedQA ) when we further increase the number of hops .
In practice , it is actually unnecessary to reason for too many hops , which may cause over-fitting .
And adopting 3 - hops in the implementation can be regarded as a hyper-parameter that is tuned on the datasets .
In addition , we extract and normalize the sentence weights from Eq. 7&9 to analyze some characteristics of the justification sentences in multihop inference .
Figure 4 ( b ) summarizes the statistic result of the sentence importance degree in each hop .
We observe that the most important sentences in the 1st-hop of reasoning are likely to appear at the beginning of the document , while those in the 3rd - hop are concentrated in the latter part of the document .
Comparatively , the important sentences in the 2nd- hop appear equally in all positions of the document .
The results show that the proposed multi-hop inference procedure of justification sentences is generally in accordance with human- like reading habits .
Case Study
We present a case study in Figure 5 with generated answers from the proposed method and some baseline methods , QPGN , QS , and SD 2 , to intuitively compare these methods .
With the multi-hop reasoning process in MSG , we can obtain a clear clue of how to answer the given question .
As it can be observed that the reference answer is composed of the information from the 1st-hop and :::::::: 3rd-hop inference sentences , it is inadequate to simply summarize the question - related content for generating the answer .
For the generated summaries , there are several observations as follows : ( 1 ) MSG ( 3hop ) successfully summarizes the source document with all the necessary and correct information .
( 2 ) MSG ( 2- hop ) also effectively summarizes the 1sthop and 2nd - hop inference content in the document .
However , in this case , 3 - hop inference is required to answer the given question .
( 3 ) MSG ( 1- hop ) only measures the semantic relevance to the given question , leading to an incomplete summary that is lack of some necessary content , and even introduces some general sentences due to the data-driven learning .
( 4 ) QPGN only considers the semantic relevance to the given question , leading to an incomplete summary that is lack of some necessary content .
( 5 ) QS and SD 2 fail to capture the key information , resulting in generating irrelevant summaries to the given question , or producing some general sentences due to the data-driven learning .
It shows the capability of MSG to implement multihop reasoning and provide justification sentences .
Additionally , we observe that many cases probably require more than 3 - hop inference or only involve one or two hops .
However , we can still evaluate how MSG works in these cases .
Compared to the reference answer , MSG ( 3 - hop ) can still capture most of the useful information to generate a good summary for answering the question .
Besides , MSG ( 2- hop ) and MSG ( 1- hop ) also manage to attend some important content in the document .
In general , our model is able to only attend a single hop if one - hop is enough , while our model may regard several hops as an integral hop when more hops are required .
However , the baseline methods introduce much unnecessary or even incorrect information into the summarized answers .
Duplication Analysis in Answers
We adopt Distinct scores to analyze whether the multi-view coverage mechanism can alleviate the repetition issue in the generation procedure of multi-view PGN .
Figure 6 summarizes the percentage of n-grams duplication on the ground -truth answers and the generated answers with or without the original ( See et al. , 2017 ) and multi-view coverage mechanism .
We observe that the original
Conclusion
We propose a novel question - driven abstractive summarization method , Multi-hop Selective Generator ( MSG ) , to summarize concise but informative answers for non-factoid QA .
We incorporate multihop reasoning to infer justification sentences for abstractive summarization .
Experimental results show that the proposed method achieves state- ofthe - art performance on two benchmark non-factoid QA datasets , namely WikiHow and PubMedQA .
Figure 1 : 1 Figure 1 : An example from PubMedQA .
The highlighted sentences illustrate the inference process when humans answer the given question .
Italic represents direct matching sentences from the question .
Underlined and ::::::::::::::wavy-underlined represent sentences inferred by 2nd- hop and 3rd- hop reasoning , respectively , to justify the answer .
