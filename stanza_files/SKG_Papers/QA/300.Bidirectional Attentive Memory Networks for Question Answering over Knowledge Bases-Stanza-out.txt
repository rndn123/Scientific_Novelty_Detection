title
Bidirectional Attentive Memory Networks for Question Answering over Knowledge Bases
abstract
When answering natural language questions over knowledge bases ( KBs ) , different question components and KB aspects play different roles .
However , most existing embeddingbased methods for knowledge base question answering ( KBQA ) ignore the subtle interrelationships between the question and the KB ( e.g. , entity types , relation paths and context ) .
In this work , we propose to directly model the two -way flow of interactions between the questions and the KB via a novel Bidirectional Attentive Memory Network , called BAMnet .
Requiring no external resources and only very few hand -crafted features , on the WebQuestions benchmark , our method significantly outperforms existing informationretrieval based methods , and remains competitive with ( hand- crafted ) semantic parsing based methods .
Also , since we use attention mechanisms , our method offers better interpretability compared to other baselines .
Introduction
With the rapid growth in large-scale knowledge bases ( KBs ) such as DBPedia ( Auer et al. , 2007 ) and FreeBase ( Google , 2018 ) , knowledge base question answering ( KBQA ) has drawn increasing attention over the past few years .
Given questions in natural language ( NL ) , the goal of KBQA is to automatically find answers from the underlying KB , which provides a more natural and intuitive way to access the vast underlying knowledge resources .
One of the most prominent challenges of KBQA is the lexical gap .
For instance , the same question can be expressed in various ways in NL while a KB usually has a canonical lexicon .
It is therefore nontrivial to map an NL question to a structured KB .
The approaches proposed to tackle the KBQA task can be roughly categorized into two groups : semantic parsing ( SP ) and information retrieval ( IR ) approaches .
SP - based approaches address the problem by constructing a semantic parser that converts NL questions into intermediate logic forms , which can be executed against a KB .
Traditional semantic parsers ( Wong and Mooney , 2007 ) require annotated logical forms as supervision , and are limited to narrow domains with a small number of logical predicates .
Recent efforts overcome these limitations via the construction of hand -crafted rules or features ( Abujabal et al. , 2017 ; Hu et al. , 2018 ) schema matching ( Cai and Yates , 2013 ) , and using weak supervision from external resources ( Krishnamurthy and Mitchell , 2012 ) .
Unlike SP - based approaches that usually assume a pre-defined set of lexical triggers or rules , which limit their domains and scalability , IR - based approaches directly retrieve answers from the KB in light of the information conveyed in the questions .
These IR - based approaches usually do not require hand -made rules and can therefore scale better to large and complex
KBs .
Recently , deep neural networks have been shown to produce strong results on many NLP tasks .
In the field of KBQA , under the umbrella of IR - based approaches , many embedding - based methods ( Bordes et al. , 2014 b ; Hao et al. , 2017 ) have been proposed and have shown promising results .
These methods adopt various ways to encode questions and KB subgraphs into a common embedding space and directly match them in that space , and can be typically trained in an end-to - end manner .
Compared to existing embedding - based methods that encode questions and KB subgraphs independently , we introduce a novel Bidirectional Attentive Memory network , called BAMnet that captures the mutual interactions between questions and the underlying KB , which is stored in a contentaddressable memory .
We assume that the world knowledge ( i.e. , the KB ) is helpful for better understanding the questions .
Similarly , the questions themselves can help us focus on important KB aspects .
To this end , we design a two -layered bidirectional attention network .
The primary attention network is intended to focus on important parts of a question in light of the KB and important KB aspects in light of the question .
Built on top of that , the secondary attention network is intended to enhance the question and KB representations by further exploiting the two -way attention .
Through this idea of hierarchical two -way attention , we are able to distill the information that is the most relevant to answering the questions on both sides of the question and KB .
We highlight the contributions of this paper as follows : 1 ) we propose a novel bidirectional attentive memory network for the task of KBQA which is intended to directly model the two -way interactions between questions and the KB ; 2 ) by design , our method offers good interpretability thanks to the attention mechanisms ; 3 ) on the We- bQuestions benchmark , our method significantly outperforms previous information - retrieval based methods while remaining competitive with ( handcrafted ) semantic parsing based methods .
Related work
Two broad classes of SP - based and IR - based approaches have been proposed for KBQA .
The former attempts to convert NL questions to logic forms .
Recent work focused on approaches based on weak supervision from either external resources ( Krishnamurthy and Mitchell , 2012 ; Berant et al. , 2013 ; Yao and Van Durme , 2014 ; Hu et al. , 2018 ; Yih et al. , 2015 ; Yavuz et al. , 2016 ) , schema matching ( Cai and Yates , 2013 ) , or using hand -crafted rules and features ( Unger et al. , 2012 ; Berant et al. , 2013 ; Berant and Liang , 2015 ; Bao et al. , 2016 ; Abujabal et al. , 2017 ; Hu et al. , 2018 ; Bast and Haussmann , 2015 ; Yih et al. , 2015 ) .
A thread of research has been explored to generate semantic query graphs from NL questions such as using coarse alignment between phrases and predicates ( Berant et al. , 2013 ) , searching partial logical forms via an agenda- based strategy ( Berant and Liang , 2015 ) , pushing down the disambiguation step into the query evaluation stage ( Hu et al. , 2018 ) , or exploiting rich syntactic information in NL questions ( Xu et al. , 2018 a , b) . Notably , another thread of SP - based approaches try to exploit IR - based techniques ( Yao and Van Durme , 2014 ; Bast and Haussmann , 2015 ; Yang et al. , 2014 ; Yih et al. , 2015 ; Bao et al. , 2016 ; Yavuz et al. , 2016 ; Liang et al. , 2016 ) by computing the similarity of two sequences as features , leveraging a neural network - based answer type prediction model , or training end-to - end neural symbolic machine via REINFORCE ( Williams , 1992 ) .
However , most SP - based approaches more or less rely on handcrafted rules or features , which limits their scalability and transferability .
The other line of work ( the IR - based ) has focused on mapping answers and questions into the same embedding space , where one could query any KB independent of its schema without requiring any grammar or lexicon .
Bordes et al. ( 2014 b ) were the first to apply an embedding - based approach for KBQA .
Later , Bordes et al. ( 2014a ) proposed the idea of subgraph embedding , which encodes more information ( e.g. , answer path and context ) about the candidate answer .
In follow - up work ( Bordes et al. , 2015 ; Jain , 2016 ) , memory networks ( Weston et al. , 2014 ) were used to store candidates , and could be accessed iteratively to mimic multi-hop reasoning .
Unlike the above methods that mainly use a bag-of-words ( BOW ) representation to encode questions and KB resources , ( Dong et al. , 2015 ; Hao et al. , 2017 ) apply more advanced network modules ( e.g. , CNNs and LSTMs ) to encode questions .
Hybrid methods have also been proposed Xu et al. , 2016 ; Das et al. , 2017 ) , which achieve improved results by leveraging additional knowledge sources such as free text .
While most embedding - based approaches encode questions and answers independently , ( Hao et al. , 2017 ) proposed a cross-attention mechanism to encode questions according to various candidate answer aspects .
Differently , in this work , our method goes one step further by modeling the bidirectional interactions between questions and a KB .
The idea of bidirectional attention proposed in this work is similar to those applied in machine reading comprehension ( Wang and Jiang , 2016 ; Seo et al. , 2016 ; Xiong et al. , 2016 ) .
However , these previous works focus on capturing the interactions between two bodies of text , in this work , we focus on modeling the interactions between one body of text and a KB .
A modular bidirectional attentive memory network for KBQA Given an NL question , the goal is to fetch answers from the underlying KB .
Our proposed BAMnet
Input module
An input NL question Q = {q i } | Q| i=1 is represented as a sequence of word embeddings ( q i ) by applying a word embedding layer .
We then use a bidirectional LSTM ( Hochreiter and Schmidhuber , 1997 ) to encode the question as H Q ( in R d? |Q| ) which is the sequence of hidden states ( i.e. , the concatenation of forward and backward hidden states ) generated by the BiLSTM .
Memory module Candidate generation
Even though all the entities from the KB could in principle be candidate answers , this is computationally expensive and unnecessary in practice .
We only consider those entities which are " close " to the main topic entity of a question .
An answer is the text description ( e.g. , a name ) of an entity node .
For example , Ohio is the topic entity of the question " Who was the secretary of state of Ohio in 2011 ? " ( see Fig. 2 ) .
After getting the topic entity , we collect all the entities connected to it within h hops as candidate answers , which we denote as { A i } | A | i=1 .
KB representation
For each candidate answer from the KB , we encode three types of information : answer type , path and context .
Answer type Entity type information is an important clue in ranking answers .
For example , if a question uses the interrogative word where , then candidate answers with types relevant to the concept of location are more likely to be correct .
We use a BiLSTM to encode its text description to get i ( i.e. , the concatenation of last forward and backward hidden states ) .
Answer path
We define an answer path as a sequence of relations from a candidate answer to a topic entity .
For example , for the Ohio question ( see Fig. 2 ) , the answer path of Jon A .
Husted can be either represented as a sequence of relation ids [ office holder , governing officials ] or the text description [ office , holder , governing , officials ] .
We thus encode an answer path as H p 1 i via a BiLSTM , and as H p 2 i by computing the average of its relation embeddings via a relation embedding layer .
Answer context
The answer context is defined as the surrounding entities ( e.g. , sibling nodes ) of a candidate which can help answer questions with constraints .
For example , in Fig. 2 , the answer context of Jon A. Husted includes the government position title secretary of state and starting date 2011- 01-09 .
However , for simple questions without constraints , the answer context is unnec-essary and can potentially incorporate noise .
We tackle this issue with two strategies : 1 ) we use a novel importance module ( explained later ) to focus on important answer aspects , and 2 ) we only consider those context nodes that have overlap with the question .
Specifically , for each context node ( i.e. , a sequence of words ) of a candidate , we first compute the longest common subsequence between it and the question , we then encode it via a BiLSTM only if we get a non-stopwords substring .
Finally , the answer context of a candidate answer will be encoded as the average of all context node representations , which we denote as H c i .
Key-value memory module
In our model , we use a key-value memory network ( Miller et al. , 2016 ) to store candidate answers .
Unlike a basic memory network ( Weston et al. , 2014 ) , its addressing stage is based on the key memory while the reading stage uses the value memory , which gives greater flexibility to encode prior knowledge via functionality separation .
Thus , after encoding the answer type , path and context , we apply linear projections on them as follows : M k t i = f k t ( H t 1 i ) M v t i = f v t ( H t 1 i ) M kp i = f k p ( [ H p 1 i ; H p 2 i ] )
M vp i = f v p ( [ H p 1 i ; H p 2 i ] )
M kc i = f k c ( H c i ) M vc i = f v c ( H c i ) ( 1 ) where M kt i and M vt i are d-dimensional key and value representations of answer type A t i , respectively .
Similarly , we have key and value representations for answer path and answer context .
We denote M as a key-value memory whose row M i = { M k i , M v i } ( both in R d?3 ) , where M k i = [ M kt i ; M kp i ; M kc i ] comprises the keys , and M v i = [ M vt i ; M vp i ; M vc i ] comprises the values .
Here [ , ] and [ ; ] denote row-wise and column-wise concatenations , respectively .
Reasoning module
The reasoning module consists of a generalization module , and our novel two -layered bidirectional attention network which aims at capturing the twoway interactions between questions and the KB .
The primary attention network contains the KBaware attention module which focuses on the important parts of a question in light of the KB , and the importance module which focuses on the important KB aspects in light of the question .
The secondary attention network ( enhancing module in Fig. 1 ) is intended to enhance the question and KB vectors by further exploiting the two -way attention .
KB - aware attention module
Not all words in a question are created equal .
We use a KB - aware attention mechanism to focus on important components of a question , as shown in Fig.
3 . Specifically , we first apply self-attention ( Sel fAtt ) over all question word vectors H Q to get a d-dimensional question vector q as follows q = BiLSTM ( [ H Q A QQ T , H Q ] ) A QQ = softmax ( ( H Q ) T H Q ) ( 2 ) where softmax is applied over the last dimension of an input tensor by default .
Using question summary q , we apply another attention ( AddAtt ) over the memory to obtain answer type m t , path m p and context summary m c : m x = | A | i=1 a x i ?
M vx i a x = Att add ( q , M kx ) ( 3 ) where x ? {t , p , c} , and Att add ( x , y ) = softmax ( tanh ( [ x T , y]W 1 ) W 2 ) , with W 1 ? R 2d?d and W 2 ? R d?1 being trainable weights .
So far , we have obtained the KB summary m = [ m t ; m p ; m c ] in light of the question .
We proceed to compute the question - to - KB attention between question word q i and KB aspects as formulated by A Qm = H Q T m.
By applying max pooling over the last dimension ( i.e. , the KB aspect dimension ) of A Qm , that is , a Q i = max j A Qm ij , we select the strongest connection between q i and the KB .
The idea behind it is that each word in a question serves a specific purpose ( i.e. , indicating answer type , path or context ) , and max pooling can help find out that purpose .
We then apply a softmax over the resulting vector to obtain ?Q which is a KB - aware question attention vector since it indicates the importance of q i in light of the KB .
Importance module
The importance module focuses on important KB aspects as measured by their relevance to the questions .
We start by computing a | Q| ? | A| ?
3 attention tensor A QM which indicates the strength of connection between each pair of {q i , A x j } x={t , p , c} .
Then , we take the max of the question word dimension of A QM and normalize it to get an attention matrix ?M , which indicates the importance of each answer aspect for each candidate answer .
After that , we proceed to compute question - aware memory representations Mk .
Thus , we have : Mv = { Mv i } | A | i=1 ? R | A |?d
Mv i = 3 j=1 M v ij Mk = { Mk i } | A | i=1 ? R | A |?d
Mk i = ?M
i M k i ?M = softmax ( A M T ) T A M = max i { A QM i } | Q | i=1 A QM = M k H Q T ( 4 ) Enhancing module
We further enhance the question and KB representations by exploiting two -way attention .
We compute the KB- enhanced question representation q which incorporates the relevant KB information by applying max pooling over the last dimension ( i.e. , the answer aspect dimension ) of A QM , that is , A Q M = max k { A QM .,. , k } 3 k=1 , and then normalizing it to get a question - to - KB attention matrix ?Q
M from which we compute the question - aware KB summary and incorporate it into the question representation HQ = H Q + ?Q ( ?Q M Mv ) T . Finally , we obtain a ddimensional KB - enhanced question representation q = HQ ?Q .
Similarly , we compute a question - enhanced KB representation M k which incorporates the relevant question information : M k = Mk + ?M ( ?M Q ( HQ ) T ) ?M = ( ?Q M ) T ?Q ? R | A|?1 ?M
Q = softmax ( A Q M T ) ? R | A|?|Q| ( 5 ) Generalization module
We add a one-hop attention process before answering .
We use the question representation q to query over the key memory M k via an attention mechanism , and fetch the most relevant information from the value memory , which is then used to update the question vector using a GRU ( Cho et al. , 2014 ) .
Finally , we apply a residual layer ( He et al. , 2016 ) ( i.e. , y = f ( x ) + x ) and batch normalization ( BN ) ( Ioffe and Szegedy , 2015 ) , which help the model performance in practice .
Thus , we have q = BN ( q + q ) q = GRU ( q , m ) m = | A | i=1 a i ?
Mv i a = Att GRU add ( q , M k ) ( 6 )
Answer module Given the representation of question Q which is q and the representation of candidate answers { A i } | A | i=1 which is { M k i } | A | i=1 , we compute the matching score S(q , M k i ) between every pair ( Q , A i ) as S( q , a ) = q T ? a.
The candidate answers are then ranked by their scores .
Training and testing Training Intermediate modules such as the enhancing module generate " premature " representations of questions ( e.g. , q ) and candidate answers ( e.g. , M k ) .
Even though these intermediate representations are not optimal for answer prediction , we can still use them along with the final representations to jointly train the model , which we find helps the training probably by providing more supervision since we are directly forcing intermediate representations to be helpful for prediction .
Moreover , we directly match interrogative words to KB answer types .
A question Q is represented by a 16 - dimensional interrogative word ( we use " which " , " what " , " who " , " whose " , " whom " , " where " , " when " , " how " , " why " and " whether " ) embedding q w and a candidate answer A i is represented by entity type embedding H t 2 i with the same size .
We then compute the matching score S( q w , H t 2 i ) between them .
Although we only have weak labels ( e.g. , incorrect answers do not necessarily imply incorrect types ) for the type matching task , and there are no shared representations between two tasks , we find in practice this strategy helps the training process as shown in Section 4.4 .
Loss Function :
In the training phase , we force positive candidates to have higher scores than negative candidates by using a triplet - based loss function : o = g( H Q ?Q , 3 j=1 M k . , j ) + g( q , M k ) + g( q , M k ) + g( q w , H t 2 ) ( 7 ) where g(q , M ) = a + ?A + a ? ?A ? ( S( q , M a + ) , S(q , M a ? ) ) , and ( y , ? ) = max ( 0 , 1 + ? ? y ) is a hinge loss function , and A + and A ? denote the positive ( i.e. , correct ) and negative ( i.e. , incorrect ) answer sets , respectively .
Note that at training time , the candidate answers are extracted from the KB subgraph of the gold -standard topic entity , with the memory size set to N max .
We adopt the following sampling strategy which works well in practice : if N max is larger than the number of positive answers | A + | , we keep all the positive answers and randomly select negative answers to fill up the memory ; otherwise , we randomly select min( N max / 2 , |A ? | ) negative answers and fill up the remaining memory with random positive answers .
Testing
At testing time , we need to first find the topic entity .
We do this by using the top result returned by a separately trained topic entity predictor ( we also compare with the result returned by the Freebase Search API ) .
Then , the answer module returns the candidate answer with the highest scores as predicted answers .
Since there can be multiple answers to a given question , the candidates whose scores are close to the highest score within a certain margin , ? , are regarded as good answers as well .
Therefore , we formulate the inference process as follows : ? = {? | ? ?
A & max a ?A { S( q , M k a ) } ? S( q , M k ? ) < ?} ( 8 ) where max a ?A { S( q , M k a ) } is the score of the best matched answer and ? is the predicted answer set .
Note that ? is a hyper-parameter which controls the degree of tolerance .
Decreasing the value of ? makes the model become stricter when predicting answers .
Topic entity prediction Given a question Q , the goal of a topic entity predictor is to find the best topic entity ? from the candidate set { C i } | C | i=1 returned by external topic entity linking tools ( we use the Freebase Search API and S-MART ( Yang and Chang , 2016 ) in our experiments ) .
We use a convolutional network ( CNN ) to encode Q into a d-dimensional vector e.
For candidate topic entity C i , we encode three types of KB aspects , namely , the entity name , entity type and surrounding relations where both entity name and type are represented as a sequence of words while surrounding relations are represented as a bag of sequences of words .
Specifically , we use three CNNs to encode them into three d-dimensional vectors , namely , C n i , C t i and C r 1 i .
Note that for surrounding relations , we first encode each of the relations and then compute their average .
Additionally , we compute an average of the relation embeddings via a relation embedding layer which we denote as C r 2 i .
We then apply linear projections on the above vectors as follows : P k i = f k ( [ C n i ; C t i ; C r 1 i ; C r 2 i ] )
P v i = f v ( [ C n i ; C t i ; C r 1 i ; C r 2 i ] ) ( 9 ) where P k i and P v i are d-dimensional key and value representations of candidate C i , respectively .
Furthermore , we compute the updated question vector ? using the generalization module mentioned earlier .
Next , we use a dot product to compute the similarity score between Q and C i .
A tripletbased loss function is used as formulated by o = g(e , P k i ) + g( ? , P k i ) where g(. ) is the aforementioned hinge loss function .
When training the predictor , along with the candidates returned from external entity linking tools , we do negative sampling ( using string matching ) to get more supervision .
In the testing phase , the candidate with the highest score is returned as the best topic entity and no negative sampling is applied .
Experiments
This section provides an extensive evaluation of our proposed BAMnet model against state- ofthe - art KBQA methods .
The implementation of BAMnet is available at https://github.
com/hugochan / BAMnet .
Data and metrics
We use the Freebase KB and the WebQuestions dataset , described below : Freebase
This is a large-scale KB ( Google , 2018 ) that consists of general facts organized as subjectproperty - object triples .
It has 41 M non-numeric entities , 19 K properties , and 596M assertions .
WebQuestions
This dataset ( Berant et al. , 2013 ( Baudis and Pichl , 2016 ) , which additionally provides ( potentially noisy ) entity mentions for each question .
Following ( Berant et al. , 2013 ) , macro F1 scores ( i.e. , the average of F1 scores over all questions ) are reported on the WebQuestions test set .
Model settings
When constructing the vocabularies of words , entity types or relation types , we only consider those questions and their corresponding KB subgraphs appearing in the training and validation sets .
The vocabulary size of words is V = 100 , 797 .
There are 1,712 entity types and 4,996 relation types in the KB subgraphs .
Notably , in FreeBase , one entity might have multiple entity types .
We only use the first one available , which is typically the most concrete one .
For those non-entity nodes which are boolean values or numbers , we use " bool " or " num " as their types , respectively .
We also adopt a query delexicalization strategy where for each question , the topic entity mention as well as constraint entity mentions ( i.e. , those belonging to " date " , " ordinal " or " number " ) are replaced with their types .
When encoding KB context , if the overlap belongs to the above types , we also do this delexicalization , which will guarantee it matches up with the delexicalized question well in the embedding space .
Given a topic entity , we extract its 2 - hop subgraph ( i.e. , h = 2 ) to collect candidate answers , which is sufficient for WebQuestions .
At training time , the memory size is limited to N max = 96 candidate answers ( for the sake of efficiency ) .
If there are more potential candidates , we do random sampling as mentioned earlier .
We initialize word embeddings with pre-trained GloVe vectors ( Pennington et al. , 2014 ) with word embedding size d v = 300 .
The relation embedding size d p , entity type embedding size d t and hidden size d are set as 128 , 16 and 128 , respectively .
The dropout rates on the word embedding layer , question encoder side and the answer encoder side are 0.3 , 0.3 and 0.2 , respectively .
The batch size is set as 32 , and answer module threshold ? = 0.7 .
As for the topic entity prediction , we use the same hyperparameters .
For each question , there are 15 candidates after negative sampling in the training time .
When encoding a question , we use a CNN with filter sizes 2 and 3 .
A linear projection is applied to merge features extracted with different filters .
When encoding a candidate aspect , we use a CNN with filter size 3 . Linear activation and max-pooling are used together with CNNs .
In the training process , we use the Adam optimizer ( Kingma and Ba , 2014 ) to train the model .
The initial learning rate is set as 0.001 which is reduced by a factor of 10 if no improvement is observed on the validation set in 3 consecutive epochs .
The training procedure stops if no improvement is observed on the validation set in 10 consecutive epochs .
The hyper-parameters are tuned on the development set .
Performance comparison
As shown in Table 1 , our method can achieve an F1 score of 0.557 when the gold topic entity is known , which gives an upper bound of our model performance .
When the gold topic entity is unknown , we report the results using : 1 ) the Freebase Search API , which achieves a recall@1 score of 0.857 on the test set for topic entity linking , and 2 ) the topic entity predictor , which achieves a recall@1 score of 0.898 for entity retrieval .
As for the performance of BAMnet on WebQuestions , it achieves an F1 score of 0.518 using the topic entity predictor , which is significantly better than the F1 score of 0.497 using the Freebase Search API .
We can observe that BAMnet significantly outperforms previous state - of- the - art IRbased methods , which conclusively demonstrates the effectiveness of modeling bidirectional interactions between questions and the KB .
It is important to note that unlike the state- ofthe- art SP - based methods , BAMnet relies on no external resources and very few hand -crafted features , but still remains competitive with those approaches .
Based on careful hand - drafted rules , some SP - based methods ( Bao et al. , 2016 ; Yih et al. , 2015 ) can better model questions with constraints and aggregations .
For example , ( Yih et al. , 2015 ) applies many manually designed rules and features to improve performance on questions with constraints and aggregations , and ( Bao et al. , 2016 ) directly models temporal ( e.g. , " after 2000 " ) , ordinal ( e.g. , " first " ) and aggregation constraints ( e.g. , " how many " ) by adding detected constraint nodes to query graphs .
In contrast , our method is end-toend , with very few hand -crafted rules .
Additionally , ( Yavuz et al. , 2016 ; Bao et al. , 2016 ) train their models on external Q&A datasets to get extra supervision .
For a fairer comparison , we only show their results without training on external Q&A datasets .
Similarly , for hyhrid systems Xu et al. , 2016 ) , we only report results without using Wikipedia free text .
It is in - Methods ( ref ) Macro F 1 SP - based ( Berant et al. , 2013 ) 0.357 ( Yao and Van Durme , 2014 ) 0.443 ( Wang et al. , 2014 ) 0.453 ( Bast and Haussmann , 2015 ) 0.494 ( Berant and Liang , 2015 ) 0.497 ( Yih et al. , 2015 ) 0.525 0.503 ( Yavuz et al. , 2016 ) 0.516 ( Bao et al. , 2016 ) 0.524 0.471 0.495 ( Abujabal et al. , 2017 ) 0.510 ( Hu et al. , 2018 ) 0.496 IR - based ( Bordes et al. , 2014a ) 0.392 ( Yang et al. , 2014 ) 0.413 ( Dong et al. , 2015 ) 0.408 ( Bordes et al. , 2015 ) 0.422 ( Xu et al. , 2016 ) 0.471 ( Hao et al. , 2017 ) 0.429 Our Method : BAMnet w/ gold topic entity 0.557 w/ Freebase Search API 0.497 w/ topic entity predictor 0.518 teresting to note that both ( Yih et al. , 2015 ) and ( Bao et al. , 2016 )
Ablation study
We now discuss the performance impact of the different modules and strategies in BAMnet .
Note that gold topic entity is assumed to be known when we do this ablation study , because the error introduced by topic entity prediction might reduce the real performance impact of a module or strategy .
As shown in modules , which confirms that the real power of our method comes from the idea of hierarchical two -way attention .
As we can see , when turning off the two -layered bidirectional attention network , the model performance drops from 0.557 to 0.534 .
Among all submodules in the attention network , the importance module is the most significant since the F1 score drops to 0.540 without it , thereby confirming the effectiveness of modeling the queryto - KB attention flow .
On the flip side , the importance of modeling the KB - to - query attention flow is confirmed by the fact that replacing the KB - aware attention module with self-attention significantly degrades the performance .
Besides , the secondary attention layer , the enhancing module , also contributes to the overall model performance .
Finally , we find that the topic entity delexicalization strategy has a big influence on the model performance while the constraint delexicalization strategy only marginally boosts the performance .
Interpretability analysis
Here , we show that our method does capture the mutual interactions between question words and KB aspects , by visualizing the attention matrix A QM produced by the reasoning module .
Fig. 4 shows the attention heatmap generated for a test question " who did location surrender to in number " ( where " location " and " number " are entity types which replace the topic entity mention " France " and the constraint entity mention " ww2 " , respectively in the original question ) .
As we can see , the attention network successfully detects the interactions between " who " and answer type , " surrender to " and answer path , and focuses more on those words when encoding the question .
To further examine the importance of the twoway flow of interactions , in Table 3 , we show the predicted answers of BAMnet with and without the two -layered bidirectional attention network on samples questions from the WebQuestions test set .
We divide the questions into three categories based on which kind of KB aspect is the most crucial for answering them .
As we can see , compared to the simplified version which is not equipped with bidirectional attention , our model is more capable of answering all the three types of questions .
Error analysis
To better examine the limitations of our approach , we randomly sampled 100 questions on which our method performed poorly ( i.e. , with per-question F1 score less than 0.6 ) , and categorized the errors .
We found that around 33 % of errors are due to label issues of gold answers and are not real mistakes .
This includes incomplete and erroneous labels , and also alternative correct answers .
Constraints are another source of errors ( 11 % ) , with temporal constraints accounting for most .
Some questions have implicit temporal ( e.g. , tense ) constraints which our method does not model .
A third source of error is what we term type errors ( 13 % ) , for which our method generates more answers than needed because of poorly utilizing answer type information .
Lexical gap is another source of errors ( 5 % ) .
Finally , other sources of errors ( 38 % ) include topic entity prediction error , question ambiguity , incomplete answers and other miscellaneous errors .
Conclusions and future work
We introduced a novel and effective bidirectional attentive memory network for the purpose of KBQA .
To our best knowledge , we are the first to model the mutual interactions between questions and a KB , which allows us to distill the information that is the most relevant to answering the questions on both sides of the question and KB .
Experimental results show that our method significantly outperforms previous IR - based methods while remaining competitive with hand-crafted SP - based methods .
Both ablation study and interpretability analysis verify the effectiveness of the idea of modeling mutual interactions .
In addition , our error analysis shows that our method actually performs better than what the evaluation metrics indicate .
In the future , we would like to explore effective ways of modeling more complex types of constraints ( e.g. , ordinal , comparison and aggregation ) .
Figure 1 : 1 Figure 1 : Overall architecture of the BAMnet model .
Figure 2 : 2 Figure 2 : A working example from Freebase .
Relations in Freebase have hierarchies where high- level ones provide too broad or even noisy information about the relation .
Thus , we choose to use the lowest level one .
Figure 3 : 3 Figure 3 : KB - aware attention module .
CAT : concatenation , SelfAtt : self-attention , AddAtt : additive attention .
also use the ClueWeb dataset for learning more accurate semantics .
The F1 score of ( Yih et al. , 2015 ) drops from 0.525 to 0.509 if ClueWeb information is removed .
To summarize , BAMnet achieves state - of- the - art performance of 0.518 without recourse to any external resources and relies only on very few hand -crafted features .
If we assume gold-topic entities are given then BAMnet achieves an F1 of 0.557 .
Figure 4 : 4 Figure 4 : Attention heatmap generated by the reasoning module .
Best viewed in color .
Table 1 : 1 Results on the WebQuestions test set .
Bold : best in - category performance .
Table 2 2 Methods Macro F 1 all 0.557 w/o two - layered bidirectional attn 0.534 w/o kb-aware attn ( + self-attn ) 0.544 w/o importance module 0.540 w/o enhancing module 0.550 w/o generalization module 0.542 w/o joint type matching 0.545 w/o topic entity delexicalization 0.529 w/o constraint delexicalization 0.554 , significant performance drops were observed after turning off some key attention
Table 2 : 2 Ablation results on the WebQuestions test set .
Gold topic entity is assumed to be known .
Table 3 : 3 Predicted answers of BAMnet w/ and w/o bidirectional attention on the WebQuestions test set .
