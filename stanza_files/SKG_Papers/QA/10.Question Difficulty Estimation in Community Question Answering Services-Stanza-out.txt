title
Question Difficulty Estimation in Community Question Answering Services *
abstract
In this paper , we address the problem of estimating question difficulty in community question answering services .
We propose a competition - based model for estimating question difficulty by leveraging pairwise comparisons between questions and users .
Our experimental results show that our model significantly outperforms a PageRank - based approach .
Most importantly , our analysis shows that the text of question descriptions reflects the question difficulty .
This implies the possibility of predicting question difficulty from the text of question descriptions .
Introduction
In recent years , community question answering ( C- QA ) services such as Stackoverflow 1 and Yahoo !
Answers 2 have seen rapid growth .
A great deal of research effort has been conducted on CQA , including : ( 1 ) question search ( Xue et al. , 2008 ; Duan et al. , 2008 ; Suryanto et al. , 2009 ; Zhou et al. , 2011 ; Cao et al. , 2010 ; Zhang et al. , 2012 ; Ji et al. , 2012 ) ; ( 2 ) answer quality estimation ( Jeon et al. , 2006 ; Bian et al. , 2009 ; Liu et al. , 2008 ) ; ( 3 ) user expertise estimation ( Jurczyk and Agichtein , 2007 ; Zhang et al. , 2007 ; Bouguessa et al. , 2008 ; Pal and Konstan , 2010 ; ; and ( 4 ) question routing ( Zhou et al. , 2009 ; Li and King , 2010 ; .
However , less attention has been paid to question difficulty estimation in CQA .
Question difficulty estimation can benefit many applications : ( 1 ) Experts are usually under time constraints .
We do not want to bore experts by routing every question ( including both easy and hard ones ) to them .
Assigning questions to experts by matching question difficulty with expertise level , not just question topic , will make better use of the experts ' time and expertise ( Ackerman and McDonald , 1996 ) . ( 2 ) Nam et al. ( 2009 ) found that winning the point awards offered by the reputation system is a driving factor in user participation in CQA .
Question difficulty estimation would be helpful in designing a better incentive mechanism by assigning higher point awards to more difficult questions .
( 3 ) Question difficulty estimation can help analyze user behavior in CQA , since users may make strategic choices when encountering questions of different difficulty levels .
To the best of our knowledge , not much research has been conducted on the problem of estimating question difficulty in CQA .
The most relevant work is a PageRank - based approach proposed by Yang et al . ( 2008 ) to estimate task difficulty in crowdsourcing contest services .
Their key idea is to construct a graph of tasks : creating an edge from a task t 1 to a task t 2 when a user u wins task t 1 but loses task t 2 , implying that task t 2 is likely to be more difficult than task t 1 .
Then the standard PageRank algorithm is employed on the task graph to estimate PageRank score ( i.e. , difficulty score ) of each task .
This approach implicitly assumes that task difficulty is the only factor affecting the outcomes of competitions ( i.e. the best answer ) .
However , the outcomes of competitions depend on both the difficulty levels of tasks and the expertise levels of competitors ( i.e. other answerers ) .
Inspired by , we propose a competition - based approach which jointly models question difficulty and user expertise level .
Our approach is based on two intuitive assumptions : ( 1 ) given a question answering thread , the difficulty score of the question is higher than the expertise score of the asker , but lower than that of the best answerer ; ( 2 ) the expertise score of the best answerer is higher than that of the asker as well as all other answerers .
Given the two assumptions , we can determine the question difficulty score and user expertise score through pairwise comparisons between ( 1 ) a question and an asker , ( 2 ) a question and a best answerer , ( 3 ) a best answerer and an asker , and ( 4 ) a best answerer and all other non-best answerers .
The main contributions of this paper are : ?
We propose a competition - based approach to estimate question difficulty ( Sec. 2 ) .
Our model significantly outperforms the PageRank - based approach ( Yang et al. , 2008 ) for estimating question difficulty on the data of Stack Overflow ( Sec. 3.2 ) .
?
Additionally , we calibrate question difficulty scores across two CQA services to verify the effectiveness of our model ( Sec. 3.3 ) .
?
Most importantly , we demonstrate that different words or tags in the question descriptions indicate question difficulty levels .
This implies the possibility of predicting question difficulty purely from the text of question descriptions ( Sec. 3.4 ) .
Competition based Question Difficulty Estimation CQA is a virtual community where people can ask questions and seek opinions from others .
Formally , when an asker u a posts a question q , there will be several answerers to answer her question .
One answer among the received ones will be selected as the best answer by the asker u a or voted by the community .
The user who provides the best answer is called the best answerer u b , and we denote the set of all non-best answerers as S = {u o 1 , ? ? ? , u o M }. Assuming that question difficulty scores and user expertise scores are expressed on the same scale , we make the following two assumptions : ?
The difficulty score of question q is higher than the expertise score of asker u a , but lower than that of the best answerer u b .
This is intuitive since the best answer u b correctly responds to question q that asker u a does not know .
?
The expertise score of the best answerer u b is higher than that of asker u a and all answerers in S .
This is straightforward since the best answerer u b solves question q better than asker u a and all nonbest answerers in S. Let 's view question q as a pseudo user u q .
Taking a competitive viewpoint , each pairwise comparison can be viewed as a two -player competition with one winner and one loser , including ( 1 ) one competition between pseudo user u q and asker u a , ( 2 ) one competition between pseudo user u q and the best answerer u b , ( 3 ) one competition between the best answerer u b and asker u a , and ( 4 ) | S | competitions between the best answerer u b and all non-best answers in S. Additionally , pseudo user u q wins the first competition and the best answerer u b wins all remaining ( | S| + 2 ) competitions .
Hence , the problem of estimating the question difficulty score ( and the user expertise score ) is cast as a problem of learning the relative skills of players from the win-loss results of the generated twoplayer competitions .
Formally , let Q denote the set of all questions in one category ( or topic ) , and R q denote the set of all two -player competitions generated from question q ? Q , i.e. , R q = {( u a ? u q ) , ( u q ? u b ) , ( u a ? u b ) , ( u o 1 ? u b ) , ? ? ? , ( u o | S | ? u b ) } , where j ?
i means that user i beats user j in the competition .
Define R = ? q?Q R q ( 1 ) as the set of all two -player competitions .
Our problem is then to learn the relative skills of players from R .
The learned skills of the pseudo question users are question difficulty scores , and the learned skills of all other users are their expertise scores .
TrueSkill
In this paper , we follow and apply TrueSkill to learn the relative skills of players from the set of generated competitions R ( Equ. 1 ) .
TrueSkill ( Herbrich et al. , 2007 ) is a Bayesian skill rating model that is developed for estimating the relative skill levels of players in games .
In this paper , we present a two -player version of TrueSkill with no-draw .
TrueSkill assumes that the practical performance of each player in a game follows a normal distribu-tion N ( ? , ? 2 ) , where ? means the skill level of the player and ? means the uncertainty of the estimated skill level .
Basically , TrueSkill learns the skill levels of players by leveraging Bayes ' theorem .
Given the current estimated skill levels of two players ( priori probability ) and the outcome of a new game between them ( likelihood ) , TrueSkill model updates its estimation of player skill levels ( posterior probability ) .
TrueSkill updates the skill level ? and the uncertainty ?
intuitively : ( a ) if the outcome of a new competition is expected , i.e. the player with higher skill level wins the game , it will cause small updates in skill level ? and uncertainty ? ; ( b ) if the outcome of a new competition is unexpected , i.e. the player with lower skill level wins the game , it will cause large updates in skill level ? and uncertainty ?.
According to these intuitions , the equations to update the skill level ? and uncertainty ? are as follows : ? winner = ? winner + ? 2 winner c ? v ( t c , ? c ) , ( 2 ) ? loser = ? loser ? ? 2 loser c ? v ( t c , ? c ) , ( 3 ) ? 2 winner = ?
2 winner ? [ 1 ? ? 2 winner c 2 ? w ( t c , ? c ) ] , ( 4 ) ?
2 loser = ?
2 loser ? [ 1 ? ? 2 loser c 2 ? w ( t c , ? c ) ] , ( 5 ) where t = ? winner ? ? loser and c 2 = 2 ?
2 + ? 2 winner + ? 2 loser .
Here , ? is a parameter representing the probability of a draw in one game , and v(t , ? ) and w( t , ? ) are weighting factors for skill level ? and standard deviation ? respectively .
Please refer to ( Herbrich et al. , 2007 ) for more details .
In this paper , we set the initial values of the skill level ? and the standard deviation ? of each player the same as the default values used in ( Herbrich et al. , 2007 ) .
Experiments
Data Set
In this paper , we use Stack Overflow ( SO ) for our experiments .
We obtained a publicly available data set 3 of SO between July 31 , 2008 and August 1 , 2012 .
SO contains questions with various topics , such as programming , mathematics , and English .
In this paper , we use SO C ++ programming ( SO / CPP ) and mathematics 4 ( SO / Math ) questions for our main experiments .
Additionally , we use the data of Math Overflow 5 ( MO ) for calibrating question difficulty scores across communities ( Sec. 3.3 ) .
The statistics of these data sets are shown in Table 1 To evaluate the effectiveness of our proposed model for estimating question difficulty scores , we randomly sampled 300 question pairs from both SO / CPP and SO / Math , and we asked experts to compare the difficulty of every pair .
We had two graduate students majoring in computer science annotate the SO / CPP question pairs , and two graduate students majoring in mathematics annotate the SO / Math question pairs .
When annotating each question pair , only the titles , descriptions , and tags of the questions were shown , and other information ( e.g. users , answers , etc. ) was excluded .
Given each pair of questions ( q 1 and q 2 ) , the annotators were asked to give one of four labels : ( 1 ) q 1 ? q 2 , which means that the difficulty of q 1 was higher than q 2 ; ( 2 ) q 1 ? q 2 , which means that the difficulty of q 1 was lower than q 2 ; ( 3 ) q 1 = q 2 , which means that the difficulty of q 1 was equal to q 2 ; ( 4 ) Unknown , which means that the annotator could not make a decision .
The agreements between annotators on both SO / CPP ( kappa value = 0.741 ) and SO / Math ( kappa value = 0.873 ) were substantial .
When evaluating models , we only kept the pairs that annotators had given the same labels .
There were 260 SO / CPP question pairs and 280 SO / Math question pairs remaining .
Accuracy of Question Difficulty Estimation
We employ a standard evaluation metric for information retrieval : accuracy ( Acc ) , defined as follows :
Acc = the number of correct pairwise comparisons the total number of pairwise comparisons .
We use the PageRank - based approach proposed by Yang et al . ( 2008 ) as a baseline .
As described in Sec. 1 , this is the most relevant method for our problem .
Table 2 gives the accuracy of the baseline and our Competition - based approach on SO / CPP and SO / Math .
From the results , we can see that ( 1 ) the proposed Competition - based approach significantly outperformed the PageRank - based approach on both data sets ; ( 2 ) PageRank - based approach only achieved a similar performance as randomly guessing .
This is because the PageRank - based approach only models the outcomes of competitions affected by question difficulty .
However , the outcomes of competitions depend on both the question difficulty levels and the expertise levels of competitors .
Our Competition - based approach considers both these factors for modeling the competitions .
The experimental results demonstrate the advantage of our approach .
Acc@SO / CPP Acc@SO / Math PageRank 50.38 % 48.93 % Competition 66.54 % 71.79 % Table 2 : Accuracy on SO / CPP and SO / Math .
Calibrating Question Difficulty across CQA Services Both MO and SO / Math are CQA services for asking mathematics questions .
However , these two services are designed for different audiences , and they have different types of questions .
MO 's primary goal is asking and answering research level mathematics questions 6 .
In contrast , SO / Math is for people studying mathematics at any level in related fields 7 . Usually , the community members in MO are not interested in basic mathematics questions .
If 6 http://mathoverflow.net/faq 7 http://area51.stackexchange.com/ proposals / 3355 / mathematics a posted question is too elementary , someone will suggest moving it to SO / Math .
Similarly , if a posted question is advanced , the community members in SO / Math will recommend moving it to MO .
Hence , it is expected that the ratio of difficult questions in MO is higher than SO / Math .
In this section , we examine whether our competition - based model can identify such differences .
We first calibrate the estimated question difficulty scores across these two services on a same scale .
The key idea is to link the users who participate in both services .
In both MO and SO / Math , users can specify their home pages .
We assume that if a user u 1 on MO and a user u 2 on SO / Math have the same home page URL , they should be linked as one natural person in the real world .
We successfully linked 633 users .
They provided 18 , 196 answers in SO / Math among which 10 , 993 ( 60.41 % ) were selected as the best answers .
In contrast , they provided 8 , 044 answers in MO among which 3 , 215 ( 39.97 % ) were selected as the best answers .
This shows that these users reflect more competitive contests in MO .
After the common users are linked , we have a joint data set of MO and SO / Math .
Then , we can calibrate the estimated question difficulty scores across the two services by performing the competition - based model on the joint data set .
Figure 1 shows the distributions of the calibrated question difficulty scores of MO and SO / Math on the same scale .
As expected , we observed that the ratio of difficult questions in MO was higher than SO / Math .
Additionally , these two distributions were significantly different ( Kolmogorov -Smirnov Test , p-value < 0.05 ) .
This demonstrates that our competition - based model successfully identified the difference between questions on two CQA services .
Analysis on the Question Descriptions
In this section , we analyze the text of question descriptions on the scale of question difficulty scores estimated by the competition model .
Micro Level
We first examine the frequency distributions of individual words over the question difficulty scores .
Figure 3 shows the examples of four words in SO / CPP .
We observe that the words ' list ' and ' array ' have the lowest mean of difficulty scores , compared to the words ' virtual ' and ' gcc ' .
This is reasonable , since ' list ' and ' array ' are related to basic concepts in programming language , while ' virtual ' and ' gcc ' are related to more advanced topics .
It can be observed that the order of the means of the difficulty scores of these words are well aligned to our learning process .
Macro Level
We evenly split the range of question difficulty scores into n buckets , and we grouped the questions into the n buckets according to which bucket their difficulty scores were in .
Then , we had n question buckets and each bucket corresponded to a word distribution of questions .
Let variable X denote the distance between the difficulty scores in two question buckets ( which is the difference between the average difficulty scores of questions in the two buckets ) , and variable Y denote the Jensen - Shannon distance between word distributions in two question buckets .
We examined the correlation between vari-X and variable Y .
The experimental results showed that the correlation between these two variables were strongly positive .
Specifically , the correlation coefficient on SO / CPP was 0.8129 and on SO / Math was 0.7412 .
In other words , when the distance between the difficulty scores of two buckets become larger , the two word distributions in the two buckets become less similar , and vice versa .
We further visualized the word distribution in each question bucket .
We set n as 3 , and we had three question buckets : ( 1 ) easy questions ; ( 2 ) normal questions ; and ( 3 ) hard questions .
Figure 3 .4 plots the tag clouds of SO / Math questions in the three buckets .
The size of tags is proportional to the frequency of tags in each bucket .
We observed that ( 1 ) the tag ' homework ' and ' calculus ' become smaller from easy questions to hard questions ; ( 2 ) the tag ' set - theory ' becomes larger .
These observations also reflect our learning process .
The above experimental results show that different words or tags of question descriptions reflect the question difficulty levels .
This implies the possibility of predicting question difficulty purely from the text of question descriptions .
Conclusion and Future Work
In this paper , we address the problem of estimating question difficulty in CQA services .
Our proposed competition - based model for estimating question difficulty significantly outperforms the PageRankbased approach .
Most importantly , our analysis shows that the text of question descriptions reflects the question difficulty .
In the future , we would like to explore predicting question difficulty from the text of question descriptions .
We also will investigate non-technical areas , where there might be no strongly distinct notion of experts and non-experts .
Figure 1 : 1 Figure 1 : The distributions of calibrated question difficulty scores of MO and SO / Math .
