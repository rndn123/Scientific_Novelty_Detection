title
MMFT - BERT : Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering
abstract
We present MMFT -BERT ( MultiModal Fusion Transformer with BERT encodings ) , to solve Visual Question Answering ( VQA ) ensuring individual and combined processing of multiple input modalities .
Our approach benefits from processing multimodal data ( video and text ) adopting the BERT encodings individually and using a novel transformerbased fusion method to fuse them together .
Our method decomposes the different sources of modalities , into different BERT instances with similar architectures , but variable weights .
This achieves SOTA results on the TVQA dataset .
Additionally , we provide TVQA - Visual , an isolated diagnostic subset of TVQA , which strictly requires the knowledge of visual ( V ) modality based on a human annotator 's judgment .
This set of questions helps us to study the model 's behavior and the challenges TVQA poses to prevent the achievement of super human performance .
Extensive experiments show the effectiveness and superiority of our method 1 .
Introduction
In the real world , acquiring knowledge requires processing multiple information sources such as visual , sound , and natural language individually and collectively .
As humans , we can capture experience from each of these sources ( like an isolated sound ) ; however , we acquire the maximum knowledge when exposed to all sources concurrently .
Thus , it is crucial for an ideal Artificial Intelligence ( AI ) system to process modalities individually and jointly .
One of the ways to understand and communicate with the world around us is by observing the environment and using language ( dialogue ) to interact with it .
A smart
We treat input modalities as a sequence . [ FUSE ] is a trainable vector ; h q 0 j , h v 0 j , and h s 0 j are fixed - length features aggregated over question - answer ( QA ) pairs , visual concepts , and subtitles .
Using a transformer encoder block , [ FUSE ] attends all source vectors and assigns weights based on the importance of each input source .
Training end to end for VQA enables the MMFT module to learn to aggregate input sources w.r.t. the nature of the question .
For illustration purposes , we show that for a single head , MMFT collects more knowledge from the visual source h v 0 j ( green colored ) than from the QA and subtitles .
Best viewed in color .
system , therefore , should be able to process visual information to extract meaningful knowledge as well as be able to use that knowledge to tell us what is happening .
The story is incomplete if we isolate the visual domain from language .
Now that advancements in both computer vision and natural language processing are substantial , solving problems demanding multimodal understanding ( their fusion ) is the next step .
Answering questions about what can be seen and heard lies somewhere along this direction of investigation .
In research towards the pursuit of combining language and vision , visual features are extracted using pre-trained neural networks for visual perception ( He et al. , 2016 ; Ren et al. , 2015 ) , and word embeddings are obtained from pre-trained language models ( Mikolov et al. , 2013 b , a ; Pennington et al. , 2014 ; Devlin et al. , 2018 ) and these are merged to process multiple modalities for various tasks : visual question answering ( VQA ) , visual reasoning , visual grounding .
TVQA , a video-based ques-tion answering dataset , is challenging as it provides more realistic multimodal question answers ( QA ) compared to other existing datasets .
To answer TVQA questions , the system needs an understanding of both visual cues and language .
In contrast , some datasets are focused either visually : MovieFIB ( Maharaj et al. , 2017 ) , Video Context QA ( Zhu et al. , 2017 ) , TGIF - QA ( Jang et al. , 2017 ) ; or by language : MovieQA ( Tapaswi et al. , 2016 ) ; or based on synthetic environments : Mari-oQA ( Mun et al. , 2017 ) and PororoQA .
We choose TVQA because of its challenges .
The introduction of transformers ( Vaswani et al. , 2017 ) has advanced research in visual question answering and shows promise in the field of language and vision in general .
Here , we adopt the pretrained language - based transformer model , BERT ( Devlin et al. , 2018 ) to solve the VQA task .
The human brain has vast capabilities and probably conducts processing concurrently .
Like humans , an intelligent agent should also be able to process each input modality individually and collectively as needed .
Our method starts with independent processing of modalities and the joint understanding happens at a later stage .
Therefore , our method is one step forward toward better joint understanding of multiple modalities .
We use separate BERT encoders to process each of the input modalities namely Q-BERT , V-BERT and S-BERT to process question ( Q ) , video ( V ) , and subtitles ( S ) respectively .
Each BERT encoder takes an input source with question and candidate answer paired together .
This is important because we want each encoder to answer the questions targeted at its individual source input .
Thus , pairing up the question and candidate answers enables each stream to attend to the relevant knowledge pertinent to the question by using a multi-head attention mechanism between question words and a source modality .
We then use a novel transformer based fusion mechanism to jointly attend to aggregated knowledge from each input source , learning to obtain a joint encoding .
In a sense , our approach is using two levels of question - to - input attention : first , inside each BERT encoder to select only relevant input ; and second , at the fusion level , in order to fuse all sources to answer the common question .
We show in our experiments that using Q-BERT , a separate BERT encoder for question and answer is helpful .
Our contribution is three - fold : First , we propose a novel multi-stream end-to - end trainable architecture which processes each input source separately followed by feature fusion over aggregated source features .
Instead of combining input sources before input to BERT , we propose to process them individually and define an objective function to optimize multiple BERTs jointly .
Our approach achieves state - of - the - art results on the video- based question answering task .
Second , we propose a novel MultiModal Fusion Transformer ( MMFT ) module , repurposing transformers for fusion among multiple modalities .
To the best of our knowledge , we are the first to use transformers for fusion .
Third , we isolate a subset of visual questions , called TVQA - Visual ( questions which require only visual information to answer them ) .
Studying our method 's behavior on this small subset illustrates the role each input stream is playing in improving the overall performance .
We also present detailed analysis on this subset .
Related Work Image - based Question Answering .
Image - based VQA ( Yu et al. , 2015 ; Antol et al. , 2015 ; Zhu et al. , 2016 ; Jabri et al. , 2016 ; Chao et al. , 2018 ) has shown great progress recently .
A key ingredient is attention ( Ilievski et al. , 2016 ; Chen et al. , 2015 ; Yu et al. , 2017a , b ; Xu and Saenko , 2016 ; Anderson et al. , 2018 ) .
Image based VQA can be divided based on the objectives such as generic VQA on real world images ( Antol et al. , 2015 ; Goyal et al. , 2017 ) , asking binary visual questions and reasoning based VQA collecting visual information recurrently ( Kumar et al. , 2016 ; Xiong et al. , 2016 ; Weston et al. , 2014 ; Sukhbaatar et al. , 2015 ; Hudson and Manning , 2018 ) to answer the question both in synthetic ( Johnson et al. , 2016 ; Yang et al. , 2018 ; Suhr et al. , 2017 ) as well as real image datasets ( Hudson and Manning , 2019 ) . Video- based Question Answering .
Video- based QA is more challenging as it requires spatiotemporal reasoning to answer the question .
introduced a video- based QA dataset along with a two-stream model processing both video and subtitles to pick the correct answer among candidate answers .
Some studies are : grounding of spatiotemporal features to answer questions ( Lei et al. , 2019 ) ; a video fill in the blank version of VQA ( Mazaheri et al. , 2017 ) ; other examples include ( Kim et al. , 2019 b , a ; Zadeh et al. , 2019 ; Yi et al. , 2019 ; Mazaheri and Shah , 2018 ) . Figure 2 : Overview of the proposed approach .
Q- BERT , V-BERT and S-BERT represent text encoder , visual encoder and subtitles encoder respectively .
If h j =Q+A j is j th hypothesis , then Q-BERT takes h j , V-BERT takes visual concepts V+h j , and S-BERT takes subtitles S+h j as inputs respectively .
The aggregated features from each BERT are concatenated with [ FUSE ] , a special trainable vector , to form a sequence and input into the MMFT module ( see section 3.2.4 for details ) .
Outputs from the MMFT module for each answer choice are concatenated together and are input into a linear classifier to obtain answer probabilities .
We optimize individual BERTs along with optimizing the full model together .
Loss total denotes our objective function used to train the proposed architecture .
At inference time , we take features only from the MMFT module .
Representation Learning .
BERT has demonstrated effective representation learning using selfsupervised tasks such as masked language modeling and next sentence prediction tasks .
The pretrained model can then be finetuned for a variety of supervised tasks .
QA is one such task .
A singlestream approach takes visual input and text into a BERT - like transformer - based encoder ; examples are : VisualBERT ( Li et al. , 2019 b ) , VL-BERT
( Su et al. , 2019 ) , Unicoder-VL ( Li et al. , 2019a ) and B2T2 ( Alberti et al. , 2019 ) .
Two -stream approaches need an additional fusion step ; ViLBERT and LXMERT ( Tan and Bansal , 2019 ) employ two modality -specific streams for images .
We take this a step further by employing three streams .
We use a separate BERT encoder for the question - answer pair .
We are specifically targeting video QA and do not need any additional pre-training except using pre-trained BERT .
Approach
Our approach permits each stream to take care of the questions requiring only that input modality .
As an embodiment of this idea , we introduce the Mul-tiModal Fusion Transformer with BERT encodings ( MMFT - BERT ) to solve VQA in videos .
See fig .
1 for the proposed MMFT module and fig .
2 for illustration of our full architecture .
Problem Formulation
In this work , we assume that each data sample is a tuple ( V , T , S , Q , A , l ) comprised of the following : V : input video ; T : T = [ t start , t end ] , i.e. , start and end timestamps for answer localization in the video ; S : subtitles for the input video ; Q : question about the video and / or subtitles ; A : set of C answer choices ; l : label for the correct answer choice .
Given a question with both subtitles and video input , our goal is to pick the correct answer from C candidate answers .
TVQA has 5 candidate answers for each question .
Thus , it becomes a 5 - way classification problem .
MultiModal Fusion Transformer with BERT encodings ( MMFT - BERT )
Q- BERT : Our text encoder named Q-BERT takes only QA pairs .
The question is paired with each candidate answer A j , where , j = 0 , 1 , 2 , 3 , 4 ; | A | = C. BERT uses a special token [ CLS ] to obtain an aggregated feature for the input sequence , and uses [ SEP ] to deal with separate sentences .
We , therefore , use the output corresponding to the [ CLS ] token as the aggregated feature from Q-BERT and [ SEP ] is used to treat the question and the answer choice as sep- arate sentences .
The input I to the text encoder is formulated as : I q j = [ CLS ] + Q + [ SEP ] + A j , ( 1 ) where , + is the concatenation operator , [ CLS ] and [ SEP ] are special tokens , Q denotes the question , and A j denotes the answer choice j , I q j is the input sequence which goes into Q-BERT and represents the combination of question and the j th answer .
We initiate an instance of the pre-trained BERT to encode each of the I q j sequences : h q 0 j = Q-BERT ( I q j ) [ 0 ] , ( 2 ) where [ 0 ] denotes the index position of the aggregated sequence representation for only textual input .
Note that , the [ 0 ] position of the input sequence is [ CLS ] .
V- BERT : We concatenate each QA pair with the video to input to our visual encoder V-BERT .
V-BERT is responsible for taking care of the visual questions .
Pairing question and candidate answer with visual concepts allows V-BERT to extract visual knowledge relevant to the question and paired answer choice .
Input to our visual encoder is thus formulated as follows : I v j = [ CLS ] + V + " . " + Q + [ SEP ] + A j , ( 3 ) where , V is the sequence of visual concepts 2 , " . " is used as a special input character , I v j is the input sequence which goes into our visual encoder .
h v 0 j = V - BERT ( I v j ) [ 0 ] , ( 4 ) where , [ 0 ] denotes the index position of the aggregated sequence representation for visual input .
2 Visual concepts is a list of detected object labels using FasterRCNN ( Ren et al. , 2015 ) pre-trained on Visual Genome dataset .
We use visual concepts provided by .
S - BERT : The S-BERT encoder applies attention between each QA pair and subtitles and results in an aggregated representation of subtitles and question for each answer choice .
Similar to the visual encoder , we concatenate the QA pair with subtitles as well ; and the input is : I s j = [ CLS ] + S + " . " + Q + [ SEP ] + A j , ( 5 ) where , S is the subtitles input , I s j is the resulting input sequence which goes into the S-BERT encoder .
h s 0 j = S-BERT ( I s j ) [ 0 ] .
( 6 ) where , [ 0 ] denotes the index position of the aggregated sequence representation for subtitles input .
Fusion Methods Let I i ?
R d denote the feature vector for i th input modality with total n input modalities I 1 , I 2 , ... , I n , d represents the input dimensionality .
We discuss two possible fusion methods : Simple Fusion : A simple fusion method is a Hadamard product between all input modalities and given as follows : h FUSE = I 1 I 2 ... I n , ( 7 ) where , h FUSE is the resulting multimodal representation which goes into the classifier .
Despite being extremely simple , this method is very effective in fusing multiple input modalities .
MultiModal Fusion Tranformer ( MMFT ) :
The MMFT module is illustrated in fig .
1 . We treat I i as a fixed d-dimensional feature aggregated over input for modality i. Inspired by BERT ( Devlin et al. , 2018 ) , we treat aggregated input features from multiple modalities as a sequence of features by concatenating them together .
We concatenate a special trainable vector [ FUSE ] 3 as the first feature vector of this sequence .
The final hidden state output corresponding to this feature vector is used as the aggregated sequence representation over input from multiple modalities denoted as h FUSE . h FUSE = MMFT ( I 1 + I 2 + ... + I n ) [ 0 ] , ( 8 ) where , + is the concatenation operator , [ 0 ] indicates the index position of the aggregated sequence representation over all input modalities .
In our case , we have three input types : QA pair , visual concepts and subtitles .
For inputs i = { 1 , 2 , 3 } and answer index j = { 0 , 1 , 2 , 3 , 4 } , the input to our MMFT module is I 1 = h q 0 j , I 2 = h v 0 j , and I 3 = h s 0 j and the output is h FUSE denoting hidden output corresponding to the [ FUSE ] vector .
Here , h q 0 j , h v 0 j , and h s 0 j are the aggregated outputs we obtain from Q-BERT , V-BERT and S-BERT respectively .
Joint Classifier Assuming a hypothesis for each tuple ( V , T , S , Q , A j ) , where A j ?
A ; j = 0 , .. , 4 denotes five answer choices , our proposed Transformer Fusion module outputs h FUSE j ? R d .
We concatenate the aggregated feature representation for all the answers together and send this to a joint classifier to produce 5 answer scores , as follows : h f inal = h FUSE 0 + h FUSE 1 + ... + h FUSE 4 , ( 9 ) scores joint = classi f ier joint ( h f inal ) , ( 10 ) where , h f inal ?
R C?d and scores joint ?
R C , C denotes number of classes .
Objective Function
Along with joint optimization , each of the Q-BERT , V-BERT and S-BERT are optimized with a single layer classifier using a dedicated loss function for each of them .
Our objective function is thus composed of four loss terms : one each to optimize each of the input encoders Q-BERT , V-BERT and S-BERT , and a joint loss term over classification using the combined feature vector .
The formulation of the final objective function is as follows : L total = L q + L vid + L sub + L joint , ( 11 ) where , L q , L vid , L sub , and L joint denote loss functions for question-only , video , subtitles , and joint loss respectively ; all loss terms are computed using softmax cross-entropy loss function using label l .
The model is trained end-to - end using L total . 3 [ FUSE ] is initialized as a d-dimensional zero vector .
Input Model Acc ( % ) Q+V MTL ( Kim et al. , 2019a ) 44.42 Two-stream 68.48 STAGE ( Lei et al. , 2019 ) 70.23 WACV20 ( Yang et al. , 2020 )
All models are trained with localized input ( w / ts ) .
Dataset
In TVQA , each question ( Q ) has 5 answer choices .
It consists of 152 K QA pairs with 21.8 K video clips .
Each question - answer pair has been provided with the localized video V to answer the question Q , i.e. , start and end timestamps are annotated .
Subtitles
S have also been provided for each video clip .
See supplementary work for a few examples .
TVQA - Visual
To study the behavior of state - of - the - art models on questions where only visual information is required to answer the question correctly , we selected 236 such visual questions .
Due to imperfections in the object detection labels , only approximately 41 % of these questions have the adequate visual input available .
We , therefore , refer to TVQA - Visual in two settings : TVQA - Visual ( full ) : full set of 236 questions .
A human annotator looked into the video carefully to ensure that the raw video is sufficient to answer the question without using subtitles .
TVQA - Visual ( clean ) :
This is the subset of 96 questions where the relevant input was available , yet the models perform poorly .
For this subset , we rely on a human annotator 's judgement who verified that either the directly related visual concept or the concepts hinting toward the correct answer are present in the list of detected visual concepts .
For instance , if the correct answer is " kitchen " , ei- Solid lines : validation accuracy , dotted lines : visual set accuracy .
Although S-BERT is significantly above V- BERT for full validation set , for visual set , we can see that V-BERT is well above Q-BERT and S-BERT .
This shows that each BERT contributes to the questions it is responsible for .
Numbers are log-scaled .
ther " kitchen " or related concepts ( e.g. " stove " , " plate " , " glass " , etcetera ) should be present in the list of visual concepts .
Thus , this easier subset is termed as TVQA - Visual ( clean ) .
TVQA - visual , although small , is a diagnostic video dataset for systematic evaluation of computational models on spatio-temporal question answering tasks and will help in looking for ways to make the V-stream contribution more effective .
See supplementary material for the distribution of visual questions based on reasons for failure .
If a model is correctly answering TVQA - visual questions which are not " clean " ( the relevant concepts are missing from the visual input ) , that is because of statistical bias in the data .
LSTM for question and each answer choice is concatenated and is input to a 5 - way classifier to output 5 answer probability scores .
Experiments and Results
5
MMFT -BERT
For video representation , we use detected attribute object pairs as visual features provided by .
We follow and only unique attribute- object pairs are kept .
Q-BERT , V-BERT and S-BERT are initialized with BERT base pre-trained on lower - cased English text with masked language modeling task .
The MMFT module uses single transformer encoder layer ( L=1 ) with multi-head attention .
We use 12 heads ( H=12 ) for multi-head attention in the MMFT module for our best model .
We initialize the MMFT module with random weights .
A d-dimensional hidden feature output corresponding to [ CLS ] token is used as an aggregated source feature from each BERT .
We concatenate these aggregated features for each candidate answer together to acquire a feature of size 5 ? d. A 5 - way classifier is then used to optimize each of Q-BERT , V-BERT and S-BERT independently .
For joint optimization of the full model , we treat the encoders ' output as a sequence of features with the order [ [ FUSE ] , h q 0 j , h v 0 j , h s 0 j ] and input this into the MMFT module ( [ FUSE ] is a trainable d-dimensional vector parameter ) .
Output corresponding to [ FUSE ] token is treated as an accumulated representation h FUSE j over all input modalities for answer j .
We concatenate h FUSE j for each answer choice to obtain h f inal for the joint classification .
We learn four linear layers , one on top of each of the three input encoders and the MMFT encoder respectively .
Thus , each linear layer takes a ( 5 ? d ) - dimensional input and produces 5 prediction scores .
Training Details .
The entire architecture was implemented using Pytorch ( Paszke et al. , 2019 ) framework .
All the reported results were obtained using the Adam optimizer ( Kingma and Ba , 2014 ) with a minibatch size of 8 and a learning rate of 2e - 5 .
Weight decay is set to 1e - 5 .
All the experiments were performed under CUDA acceleration with two NVIDIA Turing ( 24 GB of memory ) GPUs .
In all experiments , the recommended train / validation / test split was strictly observed .
We use the 4th last layer from each BERT encoder for aggregated source feature extraction .
The training time varies based on the input configuration .
It takes ?4 hrs to train our model with Q+V and ?8 - 9 hrs to train on the full model for a single epoch .
All models were trained for 10 epochs .
Our method achieves its best accuracy often within 5 epochs .
Results
All results here use the following hyperparameters : input sequence length max seq len=256 , # heads H=12 , # encoder layers L=1 for the MMFT module , and pre-trained BERT base weights for Q-BERT , V-BERT and S-BERT unless specified explicitly .
With timestamp annotations ( w/ ts ) .
Columns with " w / ts " in table 1 show results for input with timestamp localization .
We get consistently better results when using localized visual concepts and subtitles .
We get 1.7 % and 0.65 % improvement over WACV20 ( Yang et al. , 2020 ) with simple fusion for Q+V and Q+V+S inputs respectively .
When using the MMFT for fusion , our method achieves SOTA performance with all three input settings : Q+V ( ? 2.41 % ) , Q+S ( ? 0.14 ) and Q+V+S ( ? 1.1 % ) ( see table 1 ) .
Our fusion approach contributes to improved performance and gives best results for localized input .
See also train our model on full length visual features and subtitles .
Our method with simple fusion and MMFT on Q+V input outperforms Two-stream ) by absolute 6.49 % and 5.59 % with simple fusion and MMFT respectively .
We truncate the input sequence if it exceeds max seq len .
Subtitles without timestamps are very long sequences ( 49 % of subtitles are longer than length 256 ) , hence QA pair might be truncated .
Thus , we rearrange our input without timestamps as follows : " Q [ SEP ] A j . V " and " Q [ SEP ] A j . S " for V-BERT and S-BERT respectively .
Models with Q+S input are trained with max seq len=512 and Q+V+S models are trained with max seq len= 256 due to GPU memory constraints .
For Q+S and Q +V+S , we observe 69.92 % and 65.55 % with simple fusion , using MMFT produces 69.98 % and 66.10 % val .
accuracy respectively .
Results on test set .
TVQA test-public set does not provide answer labels and requires submission of the model 's predictions to the evaluation server .
Only limited attempts are permitted .
The server 's evaluation results are shown in table 2 . MMFT improves results by ( ? 6.39 % ) on Q+V. For Q+V+S , WACV20 reported 73.57 % accuracy with a different input arrangement than MMFT .
When compared with the model with the same input , MMFT performs slightly better ( ? 0.17 % ) .
Due to limited chances for submission to the test server for evaluation , the reported accuracy for Q+V+S is from one of our earlier models , not from our best model .
Model Analysis Performance analysis on TVQA - Visual .
To study the models , we evaluate Two-stream , WACV20 ( Yang et al. , 2020 ) and our method on both TVQA - Visual ( full ) and TVQA - Visual ( clean ) .
See table 4 for full results .
TVQA - Visual ( full ) :
Our method outperforms Two-stream by 10.08 % but drops by 0.43 % compared to WACV20 ( Yang et al. , 2020 ) . TVQA - Visual ( full ) has approximately 59 % of the questions with missing visual concept or require extra visual knowledge .
All three models including ours were trained on visual concepts .
Inadequate input , therefore , makes it difficult for the models to attend the missing information .
TVQA - Visual ( clean ) :
We observe ( ? 11.46 % ) and ( ? 4.17 % ) improvement for clean set compared to Two-stream and WACV20 .
TVQA - Visual ( clean ) has relevant visual concepts or related concepts to the answer present in the input .
Yet , it is challenging for existing methods ( including ours ) to perform well .
Although our model observes significant improvement ( ?4- 11 % ) over baselines for this experiment , the take away message is that it is not enough .
This subset of TVQA , therefore , serves as a good diagnostic benchmark to study the progress of exploiting visual features for multimodal QA tasks .
Performance analysis w.r.t multimodal attention .
We study the behavior of the MMFT module for aggregating multimodal source inputs ( Q , V , and S ) , we take our best model trained on all three sources , and evaluate it on questions which need knowledge about either the visual world , dialogue or both .
We then visualize the average attention score map over all heads inside MMFT module ( H=12 ) for each candidate answer , see fig .
5 . Top 2 rows show attention scores computed among all 3 input sources and the [ FUSE ] vector for visual questions .
Since , [ FUSE ] is the aggregated output over all input modalities .
For instance , visual part should contribute more if the question is about the visual world .
We can see the attention map for the correct answer has high attention scores between V and [ FUSE ] vector .
The incorrect answers attend to the wrong sources ( either Q or S ) .
Similar is the behavior for rows 3 - 5 , where the question is about subtitles , and the correct answer gives most weight to the subtitles compared to the incorrect answers .
Heatmaps for incorrect answers are either focused more on a wrong single input source or the combination of them .
Positional Encodings for V-BERT .
Positional encoding is done internally in BERT .
When finetuned , for V - BERT , the positional encoding has no effect .
This has been verified by training our Q+V model with simple fusion ( Ours - SF ) , where the input to V-BERT is a shuffled sequence of objects ; no drastic difference was observed ( shuffled : 50.32 % vs. not shuffled : 50.65 % ) .
Ablations
All ablations were done with Q+V+S input .
See features pooled over time along with visual concepts .
We used question- words - to-region attention for aggregating visual features ; adding this aggregated visual feature to Ours - SF hurts the performance ( 71.82 % ) ; using object labels was consistently more useful than visual features in various other experimental settings .
Conclusion
Our method for VQA uses multiple BERT encodings to process each input type separately with a novel fusion mechanism to merge them together .
We repurpose transformers for using attention between different input sources and aggregating the information relevant to the question being asked .
A Supplementary Material
