title
Optimizing Question Answering Accuracy by Maximizing Log-Likelihood
abstract
In this paper we demonstrate that there is a strong correlation between the Question Answering ( QA ) accuracy and the log-likelihood of the answer typing component of our statistical QA model .
We exploit this observation in a clustering algorithm which optimizes QA accuracy by maximizing the log-likelihood of a set of question - and - answer pairs .
Experimental results show that we achieve better QA accuracy using the resulting clusters than by using manually derived clusters .
Introduction Question Answering ( QA ) distinguishes itself from other information retrieval tasks in that the system tries to return accurate answers to queries posed in natural language .
Factoid QA limits itself to questions that can usually be answered with a few words .
Typically factoid QA systems employ some form of question type analysis , so that a question such as What is the capital of Japan ?
will be answered with a geographical term .
While many QA systems use hand-crafted rules for this task , such an approach is time - consuming and does n't generalize well to other languages .
Machine learning methods have been proposed , such as question classification using support vector machines ( Zhang and Lee , 2003 ) and language modeling ( Merkel and Klakow , 2007 ) .
In these approaches , question categories are predefined and a classifier is trained on manually labeled data .
This is an example of supervised learning .
In this paper we present an unsupervised method , where we attempt to cluster question - and - answer ( q-a ) pairs without any predefined question categories , hence no manually class-labeled questions are used .
We use a statistical QA framework , described in Section 2 , where the system is trained with clusters of q-a pairs .
This framework was used in several TREC evaluations where it placed in the top 10 of participating systems ( Whittaker et al. , 2006 ) .
In Section 3 we show that answer accuracy is strongly correlated with the log-likelihood of the q-a pairs computed by this statistical model .
In Section 4 we propose an algorithm to cluster q-a pairs by maximizing the log-likelihood of a disjoint set of q-a pairs .
In Section 5 we evaluate the QA accuracy by training the QA system with the resulting clusters .
QA system
In our QA framework we choose to model only the probability of an answer A given a question Q , and assume that the answer A depends on two sets of features : W = W ( Q ) and X = X ( Q ) : P ( A|Q ) = P ( A|W , X ) , ( 1 ) where W represents a set of | W | features describing the question - type part of Q such as who , when , where , which , etc. , and X is a set of features which describes the " information - bearing " part of Q , i.e. what the question is actually about and what it refers to .
For example , in the questions
Where is Mount Fuji ? and How high is Mount Fuji ? , the question type features W differ , while the information - bearing features X are identical .
Finding the best answer ?
involves a search over all A for the one which maximizes the probability of the above model , i.e. :
? = arg max A P ( A|W , X ) .
( 2 ) Given the correct probability distribution , this will give us the optimal answer in a maximum likelihood sense .
Using Bayes ' rule , assuming uniform P ( A ) and that W and X are independent of each other given A , in addition to ignoring P ( W , X ) since it is independent of A , enables us to rewrite Eq. ( 2 ) as ?
= arg max A P ( A | X ) retrieval model ?
P ( W | A ) f ilter model . ( 3 )
Retrieval Model
The retrieval model P ( A|X ) is essentially a language model which models the probability of an answer sequence A given a set of informationbearing features X = {x 1 , . . . , x | X| }.
This set is constructed by extracting single - word features from Q that are not present in a stop-list of highfrequency words .
The implementation of the retrieval model used for the experiments described in this paper , models the proximity of A to features in X .
It is not examined further here ; see ( Whittaker et al. , 2005 ) for more details .
Filter Model
The question- type feature set W = {w 1 , . . . , w | W | } is constructed by extracting n-tuples ( n = 1 , 2 , . . . ) such as where , in what and when were from the input question Q . We limit ourselves to extracting single - word features .
The 2522 most frequent words in a collection of example questions are considered in- vocabulary words ; all other words are out - of- vocabulary words , and substituted with UNK .
Modeling the complex relationship between W and A directly is non-trivial .
We therefore introduce an intermediate variable C E = {c 1 , . . . , c | C E | } , representing a set of classes of example q-a pairs .
In order to construct these classes , given a set E = {t 1 , . . . , t | E| } of example q-a pairs , we define a mapping function f : E ? C E which maps each example q-a pair t j for j = 1 . . . | E| into a particular class f ( t j ) = c e .
Thus each class c e may be defined as the union of all component q-a features from each t j satisfying f ( t j ) = c e .
Hence each class c e constitutes a cluster of q-a pairs .
Finally , to facilitate modeling we say that W is conditionally independent of A given c e so that , P ( W | A ) = | C E | e=1 P ( W | c e W ) ? P ( c e A | A ) , ( 4 ) where c e W and c e A refer to the subsets of questiontype features and example answers for the class c e , respectively .
P ( W | c e W ) is implemented as trigram language models with backoff smoothing using absolute discounting ( Huang et al. , 2001 ) .
Due to data sparsity , our set of example q-a pairs cannot be expected to cover all the possible answers to questions that may ever be asked .
We therefore employ answer class modeling rather than answer word modeling by expanding Eq. ( 4 ) as follows : P ( W | A ) = | C E | e=1 P ( W | c e W ) ?
|K A | a=1 P ( c e A | k a ) P ( k a | A ) , ( 5 ) where k a is a concrete class in the set of | K A | answer classes K A .
These classes are generated using the Kneser - Ney clustering algorithm , commonly used for generating class definitions for class language models ( Kneser and Ney , 1993 ) .
In this paper we restrict ourselves to singleword answers ; see ( Whittaker et al. , 2005 ) for the modeling of multi-word answers .
We estimate P ( c e A | k A ) as P ( c e A | k A ) = f ( k A , c e A ) | C E | g=1 f ( k A , c g A ) , ( 6 ) where f ( k A , c e A ) = ?i:i?c e A ?( i ? k A ) |c e A | , ( 7 ) and ?(? ) is a discrete indicator function which equals 1 if its argument evaluates true and 0 if false .
P ( k a | A ) is estimated as P ( k a | A ) = 1 ?j:j? Ka ?( A ? j ) . ( 8 )
The Relationship between Mean Reciprocal Rank and Log-Likelihood We use Mean Reciprocal Rank ( M RR ) as our metric when evaluating the QA accuracy on a set of questions G = {g 1 ...g | G| } : where R i is the rank of the highest ranking correct candidate answer for g i .
Given a set D = ( d 1 ...d | D| ) of q-a pairs disjoint from the q-a pairs in C E , we can , using Eq. ( 5 ) , calculate the log-likelihood as M RR = | G | i=1 1 / R i |G | , ( 9 ) LL = | D | d=1 log P ( W d | A d ) = | D| d=1 log | C E | e=1 P ( W d | c e W ) ?
|K A | a=1 P ( c e A | k a ) P ( k a | A d ) . ( 10 )
To examine the relationship between M RR and LL , we randomly generate configurations C E , with a fixed cluster size of 4 , and plot the resulting M RR and LL , computed on the same data set D , as data points in a scatter plot , as seen in Figure 1 .
We find that LL and M RR are strongly correlated , with a correlation coefficient ? = 0.86 .
This observation indicates that we should be able to improve the answer accuracy of the QA system by optimizing the LL of the filter model in isolation , similar to how , in automatic speech recognition , the LL of the language model can be optimized in isolation to improve the speech recognition accuracy ( Huang et al. , 2001 ) .
Clustering algorithm
Using the observation that LL is correlated with M RR on the same data set , we expect that optimizing LL on a development set ( LL dev ) will also improve M RR on an evaluation set ( M RR eval ) .
Hence we propose the following greedy algorithm to maximize LL dev :
In this algorithm , c ?1 indicates the set of training pairs outside the cluster configuration , thus every training pair will not necessarily be included in the final configuration .
c | C|+1 refers to a new , empty cluster , hence this algorithm automatically finds the optimal number of clusters as well as the optimal configuration of them .
init : c 1 ? C E contains
Experiments
Experimental Setup
For our data sets , we restrict ourselves to questions that start with who , when or where .
Furthermore , we only use q-a pairs which can be answered with a single word .
As training data we use questions and answers from the Knowledge - Master collection 1 . Development / evaluation questions are the questions from TREC QA evaluations from TREC 2002 to TREC 2006 , the answers to which are to be retrieved from the AQUAINT corpus .
In total we have 2016 q-a pairs for training and 568 questions for development / evaluation .
We are able to retrieve the correct answer for 317 of the development / evaluation questions , thus the theoretical upper bound for our experiments is an answer accuracy of M RR = 0.558 .
Accuracy is evaluated using 5 - fold ( rotating ) cross-validation , where in each fold the TREC QA data is partitioned into a development set of Configuration LL eval M RR eval # clusters manual - 1.18 0.262 3 all-in-one - 1.32 0.183 1 one-in- each - 0.87 0.263 2016 automatic - 0.24 0.281 4 Table 1 : LL eval ( average per q-a pair ) and M RR eval ( over all held - out TREC years ) , and number of clusters ( median of the cross-evaluation folds ) for the various configurations .
4 years ' data and an evaluation set of one year 's data .
For each TREC question the top 50 documents from the AQUAINT corpus are retrieved using Lucene 2 .
We use the QA system described in Section 2 for QA evaluation .
Our evaluation metric is M RR eval , and LL dev is our optimization criterion , as motivated in Section 3 .
Our baseline system uses manual clusters .
These clusters are obtained by putting all who q-a pairs in one cluster , all when pairs in a second and all where pairs in a third .
We compare this baseline with using clusters resulting from the algorithm described in Section 4 .
We run this algorithm until there are no further improvements in LL dev .
Two other cluster configurations are also investigated : all q-a pairs in one cluster ( all - in-one ) , and each qa pair in its own cluster ( one -in- each ) .
The all- inone configuration is equivalent to not using the filter model , i.e. answer candidates are ranked solely by the retrieval model .
The one - in- each configuration was shown to perform well in the TREC 2006 QA evaluation ( Whittaker et al. , 2006 ) , where it ranked 9th among 27 participants on the factoid QA task .
Results In Table 1 , we see that the manual clusters ( baseline ) achieves an M RR eval of 0.262 , while the clusters resulting from the clustering algorithm give an M RR eval of 0.281 , which is a relative improvement of 7 % .
This improvement is statistically significant at the 0.01 level using the Wilcoxon signed - rank test .
The one - in- each cluster configuration achieves an M RR eval of 0.263 , which is not a statistically significant improvement over the baseline .
The all - in- one cluster configuration ( i.e. no filter model ) has the lowest accuracy , with an M RR eval of 0.183 .
Discussion Manual inspection of the automatically derived clusters showed that the algorithm had constructed configurations where typically who , when and where q-a pairs were put in separate clusters , as in the manual configuration .
However , in some cases both who and where q-a pairs occurred in the same cluster , so as to better answer questions like Who won the World Cup ? , where the answer could be a country name .
As can be seen from Table 1 , there are only 4 clusters in the automatic configuration , compared to 2016 in the one - in- each configuration .
Since the computational complexity of the filter model described in Section 2.2 is linear in the number of clusters , a beneficial side effect of our clustering procedure is a significant reduction in the computational requirement of the filter model .
In Figure 2 we plot LL and M RR for one of the cross-validation folds over multiple iterations ( the while loop ) of the clustering algorithm in Sec-tion 4 .
It can clearly be seen that the optimization of LL dev leads to improvement in M RR eval , and that LL eval is also well correlated with M RR eval .
Conclusions and Future Work
In this paper we have shown that the log-likelihood of our statistical model is strongly correlated with answer accuracy .
Using this information , we have clustered training q-a pairs by maximizing loglikelihood on a disjoint development set of q-a pairs .
The experiments show that with these clusters we achieve better QA accuracy than using manually clustered training q-a pairs .
In future work we will extend the types of questions that we consider , and also allow for multiword answers .
Figure 1 : 1 Figure1 : M RR vs. LL ( average per q-a pair ) for 100 random cluster configurations .
2 http://lucene.apache.org/
Evaluation set , 1 year 's TREC .
Figure 2 : 2 Figure2 : M RR and LL ( average per q-a pair ) vs. number of algorithm iterations for one crossvalidation fold .
all training pairs | E | while improvement > threshold do best LL dev ? ? for all j = 1 ...| E| do original cluster = f ( t j )
Take t j out of f ( t j ) for e = ?1 , 1 ...| C E | , |C E | + 1 do Put t j in c e Calculate LL dev if LL dev > best LL dev then best LL dev ?
LL dev best cluster ?
e best pair ?
j end if Take t j out of c e end for Put t j back in original cluster end for Take t best pair out of f ( t best pair )
Put t best pair into c best cluster end while
http://www.greatauk.com/
