title
SemEval - 2016 Task 3 : Community Question Answering
abstract
This paper describes the SemEval - 2016 Task 3 on Community Question Answering , which we offered in English and Arabic .
For English , we had three subtasks : Question - Comment Similarity ( subtask A ) , Question - Question Similarity ( B ) , and Question - External Comment Similarity ( C ) .
For Arabic , we had another subtask :
Rerank the correct answers for a new question ( D ) .
Eighteen teams participated in the task , submitting a total of 95 runs ( 38 primary and 57 contrastive ) for the four subtasks .
A variety of approaches and features were used by the participating systems to address the different subtasks , which are summarized in this paper .
The best systems achieved an official score ( MAP ) of 79.19 , 76.70 , 55.41 , and 45.83 in subtasks A , B , C , and D , respectively .
These scores are significantly better than those for the baselines that we provided .
For subtask A , the best system improved over the 2015 winner by 3 points absolute in terms of Accuracy .
7
This is the rank of the thread in the original list of Google results , before the thread filtering ; see above .
8
Here are some examples of Qatar Living categories :
Introduction Building on the success of SemEval - 2015 Task 3 " Answer Selection in Community Question Answering " 1 , we run an extension in 2016 , which covers a full task on Community Question Answering ( CQA ) and which is , therefore , closer to the real application needs .
All the information related to the task , data , participants , results and publications can be found on the SemEval - 2016 Task 3 website .
2 1 http://alt.qcri.org/semeval2015/task3 2 http://alt.qcri.org/semeval2016/task3 CQA forums such as Stack Overflow 3 and Qatar Living 4 , are gaining popularity online .
These forums are seldom moderated , quite open , and thus they typically have little restrictions , if any , on who can post and who can answer a question .
On the positive side , this means that one can freely ask any question and can then expect some good , honest answers .
On the negative side , it takes effort to go through all possible answers and to make sense of them .
For example , it is not unusual for a question to have hundreds of answers , which makes it very time - consuming for the user to inspect and to winnow through them all .
The present task could help to automate the process of finding good answers to new questions in a community - created discussion forum , e.g. , by retrieving similar questions in the forum and by identifying the posts in the comment threads of those similar questions that answer the original question well .
In essence , the main CQA task can be defined as follows : " given ( i ) a new question and ( ii ) a large collection of question - comment threads created by a user community , rank the comments that are most useful for answering the new question " .
The test question is new with respect to the collection , but it is expected to be related to one or several questions in the collection .
The best answers can come from different question -comment threads .
In the collection , the threads are independent of each other and the lists of comments are chronologically sorted and contain some meta information , e.g. , date , user , topic , etc .
The comments in a particular thread are intended to answer the question initiating that thread , but since this is a resource created by a community of casual users , there is a lot of noise and irrelevant material , apart from informal language usage and lots of typos and grammatical mistakes .
Interestingly , the questions in the collection can be semantically related to each other , although not explicitly .
Our intention was not to run just another regular Question Answering task .
Similarly to the 2015 edition , we had three objectives : ( i ) to focus on semantic - based solutions beyond simple " bag - ofwords " representations and " word matching " techniques ; ( ii ) to study the new natural language processing ( NLP ) phenomena arising in the community question answering scenario , e.g. , relations between the comments in a thread , relations between different threads and question - to-question similarity ; and ( iii ) to facilitate the participation of non IR / QA experts to our challenge .
The third point was achieved by explicitly providing the set of potential answers - the search engine step was carried out by us - to be ( re ) ranked and by defining two optional subtasks apart from the main CQA task .
Subtask A ( Question - Comment Similarity ) : given a question from a question - comment thread , rank the comments according to their relevance ( similarity ) with respect to the question ; Subtask B ( Question - Question Similarity ) : given the new question , rerank all similar questions retrieved by a search engine , assuming that the answers to the similar questions should be answering the new question too .
Subtasks
A and B should give participants enough tools to create a CQA system to solve the main task .
Nonetheless , one can approach CQA without necessarily solving the two tasks above .
Participants were free to use whatever approach they wanted , and the participation in the main task and / or the two subtasks was optional .
A more precise definition of all subtasks can be found in Section 3 .
Keeping the multilinguality from 2015 , we provided data for two languages : English and Arabic .
For English , we used real data from the communitycreated Qatar Living forum .
The Arabic data was collected from medical forums , with a slightly different procedure .
We only proposed the main ranking CQA task on this data , i.e. , finding good answers for a given new question .
Finally , we provided training data for all languages and subtasks with human supervision .
All examples were manually labeled by a community of annotators in a crowdsourcing platform .
The datasets and the annotation procedure are described in Section 4 , and some examples can be found in Figures 3 and 4 .
The rest of the paper is organized as follows : Section 2 introduces some related work .
Section 3 gives a more detailed definition of the task .
Section 4 describes the datasets and the process of their creation .
Section 5 explains the evaluation measures .
Section 6 presents the results for all subtasks and for all participating systems .
Section 7 summarizes the main approaches and features used by these systems .
Finally , Section 8 offers some further discussion and presents the main conclusions .
Related Work
Our task goes in the direction of passage reranking , where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages , e.g. , see ( Radlinski and Joachims , 2005 ; Jeon et al. , 2005 ; Shen and Lapata , 2007 ; Moschitti et al. , 2007 ; Severyn and Moschitti , 2015 ; Moschitti , 2008 ; Tymoshenko and Moschitti , 2015 ; Surdeanu et al. , 2008 ) .
In recent years , many advanced models have been developed for automating answer selection , producing a large body of work .
For instance , Wang et al . ( 2007 ) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers ; Heilman and Smith ( 2010 ) used an algorithm based on Tree Edit Distance ( TED ) to learn tree transformations in pairs ; Wang and Manning ( 2010 ) developed a probabilistic model to learn tree - edit operations on dependency parse trees ; and Yao et al . ( 2013 ) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers .
One interesting aspect of the above research is the need for syntactic structures ; this is also corroborated in ( Severyn and Moschitti , 2012 ; Severyn and Moschitti , 2013 ) .
Note that answer selection can use models for textual entailment , semantic similarity , and for natural language inference in general .
Using information about the thread is another important direction .
In the 2015 edition of the task , the top participating systems used thread - level features , in addition to the usual local features that only look at the question - answer pair .
For example , the second - best team , HITSZ - ICRC , used as a feature the position of the comment in the thread , whether the answer is first , whether the answer is last ( Hou et al. , 2015 ) .
Similarly , the third - best team , QCRI , used features that model a comment in the context of the entire comment thread , focusing on user interaction ( Nicosia et al. , 2015 ) .
Finally , the fifth- best team , ICRC - HIT , treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments ( Zhou et al. , 2015 b ) .
In a follow - up work , Zhou et al . ( 2015a ) included long- short term memory ( LSTM ) units in their convolutional neural network to learn the classification sequence for the thread .
In parallel , exploited the dependencies between the thread comments to tackle the same task .
This was done by designing features that look globally at the thread and by applying structured prediction models , such as Conditional Random Fields ( Lafferty et al. , 2001 ) .
This research direction was further extended by , who used the output structure at the thread level in order to make more consistent global decisions .
For this purpose , they modeled the relations between pairs of comments at any distance in the thread , and they combined the predictions of local classifiers in a graph - cut and in an ILP frameworks .
Finally , proposed two novel joint learning models that are on-line and integrate inference within the learning process .
The first one jointly learns two node- and edge-level MaxEnt classifiers with stochastic gradient descent and integrates the inference step with loopy belief propagation .
The second model is an instance of fully connected pairwise CRFs ( FCCRF ) .
The FCCRF model significantly outperforms all other approaches and yields the best results on the task ( SemEval - 2015 Task 3 ) to date .
Crucial elements for its success are the global normalization and an Ising - like edge potential .
Definition of the Subtasks
The challenge was structured as a set of four different and independent subtasks .
Three of them ( A , B and C ) were offered for English , while the fourth one ( D ) was offered for Arabic .
We describe them below in detail .
In order to make the subtask definitions more clear , we also provide some high - level information about the datasets we used ( they will be described in more detail later in Section 4 ) .
The English data comes from the Qatar Living forum , which is organized as a set of seemingly independent question -comment threads .
In short , for subtask A we annotated the comments in a questionthread as " Good " , " PotentiallyUseful " or " Bad " with respect to the question that started the thread .
Additionally , given original questions we retrieved related question -comment threads and we annotated the related questions as " PerfectMatch " , " Relevant " , or " Irrelevant " with respect to the original question ( subtask B ) .
We then annotated the comments in the threads of related questions as " Good " , " Potential - lyUseful " or " Bad " with respect to the original question ( subtask C ) .
For Arabic , the data was extracted from medical forums and has a different format .
Given an original question , we retrieved pairs of the form ( related question , answer to the related question ) .
These pairs were annotated as " Direct " answer , " Relevant " and " Irrelevant " with respect to the original question .
English subtask A Question - Comment Similarity .
Given a question Q and its first ten comments 5 in the question thread ( c 1 , . . . , c 10 ) , the goal is to rank these ten comments according to their relevance with respect to the question .
Note that this is a ranking task , not a classification task ; we use mean average precision ( MAP ) as an official evaluation measure .
This setting was adopted as it is closer to the application scenario than pure comment classification .
For a perfect ranking , a system has to place all " Good " comments above the " PotentiallyUseful " and " Bad " comments ; the latter two are not actually distinguished and are considered " Bad " in terms of evaluation .
Note also that subtask A this year is the same as subtask A at SemEval - 2015 Task 3 , but with slightly different annotation and evaluation measure .
English subtask B Question - Question Similarity .
Given a new question Q ( aka original question ) and the set of the first ten related questions from the forum ( Q 1 , . . . , Q 10 ) retrieved by a search engine , the goal is to rank the related questions according to their similarity with respect to the original question .
In this case , we consider the " PerfectMatch " and " Relevant " questions both as good ( i.e. , we do not distinguish between them and we will consider them both " Relevant " ) , and they should be ranked above the " Irrelevant " questions .
As in subtask A , we use MAP as the official evaluation measure .
To produce the ranking of related questions , participants have access to the corresponding related question - thread .
6
Thus , being more precise , this subtask could have been named Question - Ques-tion + Thread Similarity .
English subtask C Question - External Comment Similarity .
Given a new question Q ( aka the original question ) , and the set of the first ten related questions ( Q 1 , . . . , Q 10 ) from the forum retrieved by a search engine , each associated with its first ten comments appearing in its thread ( c 1 1 , . . . , c 10 1 , . . . , c 1 10 , . . . , c 10 10 ) , the goal is to rank the 100 comments {c j i } 10 i , j=1 according to their relevance with respect to the original question Q .
This is the main English subtask .
As in subtask A , we want the " Good " comments to be ranked above the " PotentiallyUseful " and " Bad " comments , which will be considered just bad in terms of evaluation .
Although , the systems are supposed to work on 100 comments , we take an application - oriented view in the evaluation , assuming that users would like to have good comments concentrated in the first ten positions .
We believe users care much less about what happens in lower positions ( e.g. , after the 10th ) in the rank , as they typically do not ask for the next page of results in a search engine such as Google or Bing .
This is reflected in our primary evaluation score , MAP , which we restrict to consider only the top ten results in subtask C. Arabic subtask D Rank the correct answers for a new question .
Given a new question Q ( aka the original question ) , the set of the first 30 related questions retrieved by a search engine , each associated with one correct answer ( ( Q 1 , c 1 ) . . . , ( Q 30 , c 30 ) ) , the goal is to rank the 30 question - answer pairs according to their relevance with respect to the original question .
We want the " Direct " and the " Relevant " answers to be ranked above the " Irrelevant " answers ; the former two are considered " Relevant " in terms of evaluation .
We evaluate the position of " Relevant " answers in the rank , therefore , this is again a ranking task .
Unlike the English subtasks , here we use 30 answers since the retrieval task is much more difficult , leading to low recall , and the number of correct answers is much lower .
Again , systems were evaluated using MAP , restricted to the top - 10 results .
Datasets
As we mentioned above , the task is offered for two languages , English and Arabic .
Below we describe the data for each language .
English Dataset
We refer to the English data as the CQA - QL corpus ; it is based on data from the Qatar Living forum .
The English data is organized with focus on the main task , which is subtask C , but it contains annotations for all three subtasks .
It consists of a list of original questions , where for each original question there are ten related questions from Qatar Living , together with the first ten comments from their threads .
The data is annotated with the relevance of each related question with respect to the original question ( subtask B ) , as well as with the relevance of each comment with respect to the related ( subtask A ) and also with respect to the original question ( subtask C ) .
To build the dataset , we first selected a set of questions to serve as original questions .
In a real-world scenario those would be questions that were never asked before ; however , here we used existing questions from Qatar Living .
For the training and for the development datasets , we used questions from SemEval - 2015 Task 3 , while we used new Qatar Living questions for testing .
From each original question , we generated a query , using the question 's subject ( after some word removal if the subject was too long ) .
Then , we executed the query in Google , limiting the search to the Qatar Living forum , and we collected up to 200 resulting question - comment threads as related questions .
Afterwards , we filtered out threads with less than ten comments as well as those for which the question was more than 2,000 characters long .
Finally , we kept the top - 10 surviving threads , keeping just the first 10 comments in each thread .
We formatted the results in XML with UTF - 8 encoding , adding metadata for the related questions and for their comments ; however , we did not provide any meta information about the original question , in order to emulate a scenario where it is a new question , never asked before in the forum .
In order to have a valid XML , we had to do some cleansing and normalization of the data .
We added an XML format definition at the beginning of the XML file and made sure it validates .
We provided a split of the data into three datasets : training , development , and testing .
A dataset file is a sequence of original questions ( OrgQuestion ) , where each question has a subject , a body ( text ) , and a unique question identifier ( ORGQ ID ) .
Each such original question is followed by ten threads , where each thread has a related question ( according to the search engine results ) and its first ten comments .
Each related question ( RelQuestion ) has a subject and a body ( text ) , as well as the following attributes : ? RELQ ID : question identifier ; ? RELQ RANKING ORDER : the rank of the related question in the list of results returned by the search engine for the original question ; 7 ? RELQ CATEGORY : the question category , according to the Qatar Living taxonomy ; 8 ? RELQ DATE : date of posting ; ? RELQ USERID : identifier of the user asking the question ; ? RELQ USERNAME : name of the user asking the question ; ? RELQ RELEVANCE2ORGQ : human assessement on the relevance this RelQuestion thread with respect to OrgQuestion .
This label can take one of the following values : - PerfectMatch : RelQuestion matches OrgQuestion ( almost ) perfectly ; at test time , this label is to be merged with Relevant ; - Relevant : RelQuestion covers some aspects of OrgQuestion ; - Irrelevant : RelQuestion covers no aspects of OrgQuestion .
Each comment has a body text , 9 as well as the following attributes : ? RELC ID : comment identifier ; ? RELC USERID : identifier of the user posting the comment ; ? RELC USERNAME : name of the user posting the comment ; ? RELC RELEVANCE2ORGQ : human assessment about whether the comment is Good , Bad , or Potentially Useful with respect to the original question , OrgQuestion .
This label can take one of the following values : - Good : at least one subquestion is directly answered by a portion of the comment ; - PotentiallyUseful : no subquestion is directly answered , but the comment gives potentially useful information about one or more subquestions ( at test time , this class will be merged with Bad ) ; - Bad : no subquestion is answered and no useful information is provided ( e.g. , the answer is another question , a thanks , dialog with another user , a joke , irony , attack of other users , or is not in English , etc. ) .
? RELC RELEVANCE2RELQ : human assessment about whether the comment is Good , Bad , or PotentiallyUseful ( again , the latter two are merged under Bad at test time ) with respect to the related question , RelQuestion .
We used the CrowdFlower 10 crowdsourcing platform to annotate the gold labels for the three subtasks , namely RELC RELEVANCE2RELQ for subtask A , RELQ RELEVANCE2ORGQ for subtask B , and RELC RELEVANCE2ORGQ for subtask C .
We collected several annotations for each decision ( there were at least three human annotators per example ) and we resolved the discrepancies using the default mechanisms of CrowdFlower , which take into account the general quality of annotation for each annotator ( based on the hidden tests ) .
Unlike SemEval - 2015
Task 3 , where we excluded comments for which there was a lot of disagreement about the labels between the human annotators , this time we did not eliminate any comments ( but we controlled the annotation quality with hidden tests ) , and thus we guarantee that for each question thread , we have the first ten comments without any comment being skipped .
To gather gold annotation labels , we created two annotation jobs on CrowdFlower , screenshots of which are shown in Figures 1 and 2 .
The first annotation job aims to collect labels for subtasks B and C .
We show a screenshot in Figure 1 .
An annotation example consists of an original question , a related question , and the first ten comments for that related question .
We asked the annotators to judge the relevance of the thread with respect to the original question ( RELQ RELEVANCE2ORGQ , for subtask B ) , as well as the relevance of each comment with respect to the original question ( RELC RELEVANCE2ORGQ , for subtask C ) .
Each example is judged by three annotators who must maintain 70 % accuracy throughout the job , measured on a hidden set of 121 examples .
11
The average inter-annotator agreement on the training , development , and testing datasets is 80 % , 74 % , and 87 % for RELQ RELEVANCE2ORGQ , and 83 % , 74 % , and 88 % for RELC RELEVANCE2ORGQ .
The second CrowdFlower job collects labels for subtask A ; a screenshot is shown in Figure 2 .
An annotation example consists of a question -comments thread , with ten comments , and we ask annotators to judge the relevance of each comment with respect to the thread question ( RELC RELEVANCE2 RELQ ) .
Again , each example is judged by three annotators who must maintain 70 % accuracy throughout the job , measured on a hidden set of 150 examples .
The average inter-annotator agreement on the training , development , and testing datasets is 82 % , 89 % , and 79 % for RELC RELEVANCE2RELQ .
A fully annotated example is shown in Figure 3 . Statistics about the datasets are shown in Table 1 .
Note that the training data is split into two parts , where part2 is noisier than part1 .
For part2 , a different annotation setup was used , 12 which confused the annotators , and they often provided annotation for RELC RELEVANCE2ORGQ while wrongly thinking that they were actually annotating RELC RELEVANCE2RELQ .
Note that the development data was annotated with the same setup as training part2 ; however , we manually doublechecked and corrected it .
Instead , the training part1 and testing datasets used the less confusing , and thus higher -quality annotation setup described above .
Note also that in addition to the above-described canonical XML format , we further released the data in an alternative uncleansed 13 multi-line format .
We further released a simplified file format containing only the relevant information for subtask A , where duplicated related questions are removed .
14 Finally , we reformatted the training , development , and test data from SemEval - 2015 Task 3 , to match the subtask A format for this year .
We released this reformatted SemEval - 2015 Task 3 , subtask A data as additional training data .
We further released a large unannotated dataset from Qatar Living with 189,941 questions and 1,894,456 comments , which is useful for unsupervised learning or for training domain-specific word embeddings .
Arabic Dataset While at SemEval - 2015 we used a dataset from the Fatwa website , this year we changed the domain to medical , which is largely ignored for Arabic .
We will refer to the Arabic corpus as CQA -MD .
We extracted data from three popular Arabic medical websites that allow visitors to post questions related to health and medical conditions , and to get answers by professional doctors .
We collected 1,531 question - answer ( QA ) pairs from WebTeb , 15 69,582 pairs from Al - Tibbi , 16 and 31,714 pairs from the medical corner of Islamweb .
17
We used the 1,531 questions from WebTeb as our original questions , and we looked to find related QA pairs from the other two websites .
We collected over 100,000 QA pairs in total from the other two websites , we indexed them in Solr , and we searched them trying to find answers to the WebTeb questions .
We used several different query / document formulations to perform 21 retrieval runs , and we merged the retrieved results , ranking them according to the reciprocal rank fusion algorithm ( Cormack et al. , 2009 ) .
Finally , we truncated the result list to the 30 top- ranked QA pairs , ending up with 45,164 QA pairs 18 for the 1,531 original questions .
Next , we used CrowdFlower to obtain judgments about the relevance of these QA pairs with respect to the original question using the following labels : ?
" D " ( Direct ) :
The QA pair contains a direct answer to the original question such that if the user is searching for an answer to the original question , the proposed QA pair would be satisfactory and there would be no need to search any further . ? " R " ( Related ) :
The QA pair contains an answer to the original question that covers some of the aspects raised in the original question , but this is not sufficient to answer it directly .
With this QA pair , it would be expected that the user will continue the search to find a direct answer or more information . ?
" I " ( Irrelevant ) :
The QA pair contains an answer that is irrelevant to the original question .
We controlled the quality of annotation using a hidden set of 50 test questions .
We had three judgments per example , which we combined using the CrowdFlower mechanism .
The average interannotator agreement was 81 % .
Finally , we divided the data into training , development and testing datasets , based on confidence , where the examples in the test dataset were those with the highest annotation confidence .
We further double-checked and manually corrected some of the annotations for the development and the testing datasets whenever necessary .
Figure 4 shows part of the XML file we generated .
We can see that , unlike the English data , there are no threads here , just a set of question - answer pairs ; moreover , we do not provide much meta data , but we give information about the confidence of annotation ( for the training and development datasets only , but not for the test dataset ) .
Table 2 shows some statistics about the dataset size and the distribution of the three classes in the CQA - MD corpus .
Scoring
The official evaluation measure we used to rank the participating systems is Mean Average Precision ( MAP ) calculated for the ten comments a participating system has ranked highest .
It is a wellestablished in Information Retrieval .
We further report the results for two unofficial ranking measures , which we also calculate for the top - 10 results only : Mean Reciprocal Rank ( MRR ) and Average Recall ( AvgRec ) .
Additionally , we report the results for four standard classification measures , which we calculate over the full list of results : Precision , Recall , F 1 ( with respect to the Good / Relevant class ) and Accuracy .
We released a specialized scorer that calculates and reports all above-mentioned seven scores .
Participants and Results
The list of all participating teams can be found in Table 7 .
The results for subtasks A , B , C , and D are shown in Tables 3 , 4 , 5 , and 6 , respectively .
In all tables , the systems are ranked by the official MAP scores for their primary runs 19 ( shown in the third column ) .
The following columns show the scores based on the other six unofficial measures ; the ranking with respect to these additional measures are marked with a subindex ( for the primary runs ) .
Eighteen teams participated in the challenge presenting a variety of approaches and features to address the different subtasks .
They submitted a total of 95 runs ( 38 primary and 57 contrastive ) , which are broken down by subtasks in the following way :
The English subtasks A , B and C attracted 12 , 11 , and 10 systems and 29 , 25 and 28 runs , respectively .
The Arabic subtask D got 5 systems and 13 runs .
The best MAP scores varied from 45.83 to 79.19 , depending on the subtask .
The best systems in each subtask were able to beat the baselines we provided by sizeable margins .
Subtask A , English ( Question - Comment Similarity )
Table 3 shows the results for subtask A , English , which attracted 12 teams , which submitted 29 runs : 12 primary and 17 contrastive .
The last four rows of the table show the performance of four baselines .
The first one is the chronological ranking , where the comments are ordered by their time of posting ; we can see that all submissions outperform this baseline on all three ranking measures .
The second baseline is a random baseline , which outperforms some systems in terms of F 1 , primarily because of having very high Recall .
Baseline 3 classifies all comments as Good , and it outperforms four of the primary systems in terms of F 1 . Finally , baseline 4 classifies all comments as Bad ; it outperforms one of the primary systems in terms of Accuracy .
The winning team is that of KeLP ( Filice et al. , 2016 ) , which achieved the highest MAP of 79.19 , outperforming the second best by a margin ; they are also first on AvgRec and MRR , and second on Accuracy .
They learn semantic relations between questions and answers using kernels and previously - proposed features from .
Their system is based on the KeLP machine learning platform , and thus the name of the team .
The second best system is that of ConvKN ( Barr ?n- Cede ?o et al. , 2016 ) with MAP of 77.66 ; it is also first on Accuracy , second on F 1 , and third on AvgRec .
The system combines convolutional tree kernels and convolutional neural networks , together with text similarity and thread-specific features .
Their contrastive1 run achieved even better results : MAP of 78.71 .
The third best system is SemanticZ ( Mihaylov and Nakov , 2016 b ) with MAP of 77.58 .
They use semantic similarity based on word embeddings and topics ; they are second on AvgRec and MRR .
Note also the cluster of systems of very close MAP : ConvKN ( Barr ?n- Cede?o et al. , 2016 ) with 77.66 , SemanticZ ( Mihaylov and Nakov , 2016 b ) with 77.58 , ECNU ( Wu and Lan , 2016 ) with 77.28 , and SUper team ( Mihaylova et al. , 2016 ) with 77.16 .
The latter also has a contrastive run with MAP of 77.68 , which would have ranked second .
6.2 Subtask B , English ( Question - Question Similarity )
Table 4 shows the results for subtask B , English , which attracted 11 teams and 25 runs : 11 primary and 14 contrastive .
This turns out to be a hard task .
For example , the IR baseline ( i.e. , ordering the related questions in the order provided by the search engine ) outperforms 5 of the 11 systems in terms of MAP ; it also outperforms several systems in terms of MRR and AvgRec .
The random baseline outperforms one system in terms of F 1 and Accuracy , again due to high recall .
The all - Good baseline outperforms two systems on F 1 , while the all - Bad baseline outperforms two systems on Accuracy .
The winning team is that of UH - PRHLT ( Franco - Salvador et al. , 2016 ) , which achieved MAP of 76.70 ( just 2 MAP points over the IR baseline ) .
They use distributed representations of words , knowledge graphs generated with BabelNet , and frames from FrameNet .
Their contrastive2 run is even better , with MAP of 77.33 .
The second best system is that of ConvKN ( Barr ?n- Cede ?o et al. , 2016 ) with MAP of 76.02 ; they are also first on MRR , second on AvgRec and F 1 , and third on Accuracy .
The third best system is KeLP ( Filice et al. , 2016 ) with MAP of 75.83 ; they are also first on AvgRec , F 1 , and Accuracy .
They have a contrastive run with MAP of 76.28 , which would have ranked second .
The fourth best , SLS ( Mohtarami et al. , 2016 ) is very close , with MAP of 75.55 ; it is also first on MRR and Accuracy , and third on AvgRec .
It uses a bag-of-vectors approach with various vector- and text - based features , and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers .
Subtask C , English ( Question - External Comment Similarity )
The results for subtask C , English are shown in Table 5 .
This subtask attracted 10 teams , and 28 runs : 10 primary and 18 contrastive .
Here the teams performed much better than they did for subtask B .
The first three baselines were all outperformed by all participating systems .
However , due to severe class imbalance , the all - Bad baseline outperformed 9 out of the 10 participating teams in terms of Accuracy .
The best system in this subtask is that of the SUper team ( Mihaylova et al. , 2016 ) , which achieved MAP of 55.41 ; the system is also first on AvgRec and MRR .
It used a rich set of features , grouped into three categories : question -specific features , answer-specific features , and question - answer similarity features .
This includes more or less standard metadata , lexical , semantic , and user-related features , as well as some exotic ones such as features related to readability , credibility , as well as goodness polarity lexicons .
20
It is important to note that this system did not try to solve subtask C directly , but rather just multipled their predicted score for subtask A by the reciprocal rank of the related question in the list of related questions ( as returned by the search engine , and as readily provided by the organizers as an attribute in the XML file ) for the original question .
In fact , this is not an isolated case , but an approach taken by several participants in subtask C .
The second best system is that of KeLP ( Filice et al. , 2016 ) , with MAP of 52.95 ; they are also first on F 1 , and second on AvgRec and MRR .
KeLP also has a contrastive run with a MAP of 55.58 , which would have made them first .
This team really tried to solve the actual subtask C by means of stacking classifiers : they used their subtask A classifier to judge how good the answer is with respect to the original and with respect to the related question .
Moreover , they used their subtask B classifier to judge the relatedness of the related question with respect to the original question .
Finally , they used these three scores , together with some features based on them , to train a classifier that solves subtask C. 20
These goodness polarity lexicons were at the core of another system , PMI - cool , which did not perform very well as it limited itself to lexicons and ignored other important features .
In fact , we anticipated solutions like this when we designed the task , i.e. , that participants would solve subtasks A and B , and use them as auxiliary tasks to attack the main task , namely subtask C.
Unfortunately , subtask B turned out to be too hard , and thus many participants decided to skip it and just to use the search engine 's reciprocal rank .
The third best system is SemanticZ ( Mihaylov and Nakov , 2016 b ) , with MAP of 51.68 .
Similarly to SUper team , they simply multiply their predicted score for subtask A by the reciprocal rank of the related question in the list of related questions for the original question .
Subtask D , Arabic ( Reranking the correct answers for a new question )
Finally , the results for subtask D , Arabic are shown in Table 6 .
It attracted 5 teams , which submitted 13 runs : 5 primary and 8 contrastive .
As the class imbalance here is even more severe than for subtask C , the all - Bad baseline outperforms all participating systems in terms of Accuracy .
In contrast , the all - Good baseline only outperforms one system in terms of F 1 .
Here the teams perform much better than for subtask B .
The random baseline outperforms one system in terms of both MAP and AvgRec .
The clear winner here is SLS ( Mohtarami et al. , 2016 ) , which is ranked first on all measures : MAP , AvgRec , MRR , F 1 , and Accuracy .
Yet , their MAP of 45.83 is only slightly better than that of ConvKN , 45.50 , which ranks second on MAP , AvgRec , MRR and F 1 , and third on Accuracy .
The third system is RDI ( Magooda et al. , 2016 ) with MAP of 43.80 , which is ranked third also on AvgRec and MRR .
The system combines a TF.IDF module with a recurrent language model and information from Wikipedia .
Features and Techniques
The systems that participated in several subtasks typically re-used some features for all subtasks , whenever possible and suitable .
Such features include the following : ( i ) similarity features between questions and comments from their threads or between original questions and related questions , e.g. , cosine similarity applied to lexical , syntactic and semantic representations or distributed representa-tions , often derived using neural networks , ( ii ) content features , which are special signals that can clearly indicate a bad answer , e.g. , when a comment contains " thanks " , ( iii ) thread level / meta features , e.g. , user ID , comment rank in the thread , and ( iv ) automatically generated features from syntactic structures using tree kernels .
Overall , most of the top positions are occupied by systems that used tree kernels , combined with similarity features .
Regarding the machine learning approaches used , most systems chose SVM classifiers ( often these were ranking versions such as SVM - Rank ) , or different kinds of neural networks .
Below we look in more detail in the features and the used learning methods .
Feature Types
Participants preferred different kinds of features for different subtasks : Subtask A. Similarities between question subject vs. comment , question body vs. comment , and question subject + body vs. comment .
Subtask B. Similarities between the original and the related question at different levels : subject vs. subject , body vs. body , and subject + body vs. sub-Subtask C .
The same from above , plus the similarities of the original question subject , body , and full levels with the comments from the thread of the related question .
The similarity scores to be used as features were computed in various ways , e.g. , the majority of teams used dot product calculated over word ngrams ( n=1,2,3 ) , character 3 - grams , or with TF - IDF weighting .
Or simply using word overlap , i.e. , the number of common words between two texts , often normalized , e.g. , by question / comment length .
Or overlap in terms of nouns or named entities .
Several systems , e.g. , UH - PRHLT , KeLP , SLS , SemanticZ , ECNU , used additional similarities based on distributed representations .
For example , using the continuous skip-gram model of word2vec or Glove , trained on Google News , on the English Wikipedia , or on the unannotated Qatar Living dataset .
In particular , UH - PRHLT used word alignments and distributed representations to align the words of the question with the words of the comment .
On the alignment topic , it is worth mentioned that MTE - NN applied a model originally defined for machine translation evaluation ( Guzm ? n et al. , 2015 ) , e.g. , based on features computed with BLEU , TER , NIST , and Meteor ( Guzm ? n et al. , 2016a ) .
Similarly , ECNU used Spearman , Pearson , and Kendall Ranking Coefficients as similarity scores for question similarity estimation , whereas ICL00 used word-toword translation probabilities , and UniMelb used convolutional neural networks ( CNNs ) fed with word embeddings and machine translation evaluation scores as input .
ConvKN used a CNN that also encodes relational links between the involved pieces of texts ( Severyn and Moschitti , 2015 ) . MTE -NN applied a simple neural network , ECNU and SLS used LSTM networks , and Overfitting applied Feedforward Neural Net Language Model ( FNNLM ) .
It should be noted that ConvKN and KeLP used tree kernels with relational links ( Tymoshenko and Moschitti , 2015 ; , i.e. , the questions are aligned with the comments ( or with the other questions ) by means of a special REL tag , directly annotated in the parse trees .
Regarding text structures , UH - PRHLT used Knowledge Graph Analysis , which consists in labeling , weighting , and expanding concepts in the text using a directed graph .
They also used frames from FrameNet to generate semantic features .
Several teams , e.g. , ConvKN , KeLP and SUper Team , used meta-features , such as the user ID .
In particular , the SUper Team collected statistics about the users , e.g. , the comments / questions they produced , time since their last activity , the number of good and bad comments in the training data , etc .
Other important features , which were used by most systems , are related to rank , e.g. , rank of the comment in the question thread , or rank of the related question in the list of questions retrieved by the search engine for the original question .
Some exotic features by the SUper Team modeled readability , credibility , sentiment analysis , trollness ( Mihaylov et al. , 2015a ; Mihaylov et al. , 2015 b ; Mihaylov and Nakov , 2016a ) , and goodness polarity , e.g. , based on PMI lexicons as for PMI - Cool .
Regarding Arabic , QU - IR and SLS used word2vec , whereas RDI Team relied on language models .
In particular , the winning SLS team used simple text - and vector-based features , where the text similarities are computed at the word - and the sentence - level .
Most importantly , they computed two sets of features : one between the original and the related questions , and one between the original question and the related answer , which are then concatenated in one feature vector .
The ConvKN team combined some basic SLS features with tree kernels applied to syntactic trees , obtaining a result that is very close to that of the winning SLS team .
QU - IR used a standard Average Word Embedding and also a new method , Covariance Word Embedding , which computes a covariance matrix between each pair of dimensions of the embedding , thus considering vector components as random variables .
Finally , RDI used Arabic-Wikipedia to boost the weights of medical terms , which improved their ranking function .
Learning Methods
The most popular machine learning approach was to use Support Vector Machines ( SVM ) on the features described in the previous section .
SVMs were used in three different learning tasks : classification , regression , and ranking .
Note that SVMs allow the use of complex convolutional kernels such as tree kernels , which were used by two systems ( which in fact combined kernels with other features ) .
Neural networks were also widely used , e.g. , in word2vec to train word embeddings .
As previously mentioned , there were also systems using CNNs , LSTMs and FNNLM .
Overfitting also used Random Forests .
Comparing tree kernels vs. neural networks : approaches based on the former were ranked first and second in Subtask A , second and third in Subtask B , and second in Subtasks C and D , while neural network - based systems did not win any subtask , but neural networks contributed to the best systems in all subtasks , e.g. , with word2vec .
Yet , post-competition improvements have shown that NN - based systems can perform on par with the best ( Guzm ? n et al. , 2016a ) .
Conclusion
We have described SemEval - 2016 Task 3 on Community Question Answering , which extended SemEval - 2015
Task 3 with new subtasks ( Question - Question similarity , Question - External Comment Similarity , and Reranking the correct answers for a new question ) , new evaluation metrics ( based on ranking ) , new datasets , and new domains ( biomedical for Arabic ) .
The overall focus was on answering new questions that were not already answered in the target community forum .
The task attracted 18 teams , which submitted 95 runs ; this is good growth compared to 2015 , when 13 teams submitted 61 runs .
The participants built on the lessons learned from the 2015 edition of the task , and further experimented with new features and learning frameworks .
It was interesting to see that the top systems used both word embeddings trained using neural networks and syntactic kernels , which shows the importance of both distributed representations and linguistic analysis .
It was also nice to see some new features being tried .
Apart from the new lessons learned from this year 's edition , we believe that the task has another important contribution : the datasets we have created as part of the task ( with over 7,000 questions and over 57,000 annotated comments ) , and which we have released for use to the research community , should be useful for follow up research beyond SemEval .
Finally , given the growth in the interest for the task , we plan a rerun at SemEval - 2017 with data from a new domain .
Table 3 : Subtask A , English ( Question - Comment Similarity ) : results for all submissions .
The first column shows the rank of the primary runs with respect to the official MAP score .
The second column contains the team 's name and its submission type ( primary vs. contrastive ) .
The following columns show the results for the primary , and then for other , unofficial evaluation measures .
The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column .
Table 4 : Subtask B , English ( Question - Question Similarity ) : results for all submissions .
The first column shows the rank of the primary runs with respect to the official MAP score .
The second column contains the team 's name and its submission type ( primary vs. contrastive ) .
The following columns show the results for the primary , and then for other , unofficial evaluation measures .
The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column .
Baseline 4 ( all ' false ' )
------90.66 Table 5 : Subtask C , English ( Question - External Comment Similarity ) : results for all submissions .
The first column shows the rank of the primary runs with respect to the official MAP score .
The second column contains the team 's name and its submission type ( primary vs. contrastive ) .
The following columns show the results for the primary , and then for other , unofficial evaluation measures .
The subindices show the rank of the primary runs with respect to the evaluation measure in the respective column .
Baseline 4 ( all ' false ' )
------80.76
