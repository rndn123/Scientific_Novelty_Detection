title
Repurposing Entailment for Multi-Hop Question Answering Tasks
abstract
Question Answering ( QA ) naturally reduces to an entailment problem , namely , verifying whether some text entails the answer to a question .
However , for multi-hop QA tasks , which require reasoning with multiple sentences , it remains unclear how best to utilize entailment models pre-trained on large scale datasets such as SNLI , which are based on sentence pairs .
We introduce Multee , a general architecture that can effectively use entailment models for multi-hop QA tasks .
Multee uses ( i ) a local module that helps locate important sentences , thereby avoiding distracting information , and ( ii ) a global module that aggregates information by effectively incorporating importance weights .
Importantly , we show that both modules can use entailment functions pre-trained on a large scale NLI datasets .
We evaluate performance on MultiRC and OpenBookQA , two multihop QA datasets .
When using an entailment function pre-trained on NLI datasets , Multee outperforms QA models trained only on the target QA datasets and the OpenAI transformer models .
Introduction
How can we effectively use textual entailment models for question answering ?
Previous attempts at this have resulted in limited success ( Harabagiu and Hickl , 2006 ; Sacaleanu et al. , 2008 ; Clark et al. , 2012 ) .
With recent large scale entailment datasets ( Bowman et al. , 2015 ; Williams et al. , 2018 ; pushing entailment models to high accuracies ( Chen et al. , 2017 ; Parikh et al. , 2016 ; Wang et al. , 2017 ) , we re-visit this challenge and propose a novel method for repurposing neural entailment models for QA .
A key difficulty in using entailment models for QA turns out to be the mismatch between the inputs to the two tasks : large-scale entailment datasets are typically framed at a sentence level , whereas question answering requires verifying whether multiple sentences , taken together as a premise , entail a hypothesis .
There are two straightforward ways to address this mismatch : ( 1 ) aggregate independent entailment decisions over each premise sentence , or ( 2 ) make a single entailment decision after concatenating all premise sentences .
Neither approach is fully satisfactory .
To understand why , consider the set of premises in Figure 1 , which entail the hypothesis H c .
Specifically , the combined information in P 1 and P 3 entails H c , which corresponds to the correct answer Cambridge .
On one hand , aggregating independent decisions will fail because no individual premise entails H C .
On the other hand , simply concatenating premises to form a single paragraph will fail because distracting information in P 2 and P 4 can muddle useful information in P 1 and P 3 .
An effective approach , therefore , must recognize relevant sentences ( i.e. , avoid distracting ones ) and compose their sentence - level information .
Our solution to this challenge is based on the observation that a sentence - level entailment function can be re-purposed for both recognizing relevant sentences , and for computing sentence - level representations .
Both tasks require comparing in-formation in a pair of texts , but the objectives of the comparison are different .
This means we can take an entailment function that is trained for basic entailment ( i.e. , comparing information in texts ) , and adapt it to work for both recognizing relevance and computing representations .
Thus , this architecture allows us to incorporate advances in entailment architectures and to leverage pretrained models obtained using large scale entailment datasets .
To this end , we propose a general architecture that uses a ( pre-trained ) entailment function f e for multi-sentence QA .
Given a hypothesis statement H qa representing a candidate answer , and the set of premise sentences { P i } , our proposed architecture uses the same function f e for two components : ( a ) a sentence relevance module that scores each P i based on its potential relevance to H qa , with the goal of weeding out distractors ; and ( b ) a relevance - weighted aggregator that combines entailment information from multiple P i .
Thus , we build effective entailment aware representations of larger contexts ( i.e. , multiple sentences ) from those of small contexts ( i.e. , individual sentences ) .
The main strength of our approach is that , unlike standard attention mechanisms , the aggregator module uses the attention scores from the relevance module at multiple levels of abstractions ( e.g. , multiple layers of a neural network ) within f e , using join operations that compose representations at each level .
We refer to this multilevel aggregation of textual entailment representations as Multee ( pronounced multi ) .
Our implementation of Multee uses ESIM ( Chen et al. , 2017 ) , a recent sentence - level entailment model , pre-trained on SNLI and MultiNLI datasets .
We demonstrate its effectiveness on two challenging multi-sentence reasoning datasets : MultiRC ( Khashabi et al. , 2018 ) and OpenBookQA ( Mihaylov et al. , 2018 ) .
Multee using ELMo contextual embeddings ( Peters et al. , 2018 ) matches state - of - the - art results achieved with large transfomer - based models ( Radford et al. , 2018 ) that were trained on a sequence of large scale tasks ( Sun et al. , 2019 ) .
Ablation studies demonstrate that both relevance scoring and multi-level aggregation are valuable , and that pre-training on large entailment corpora is particularly helpful for OpenBookQA .
This work makes three main contributions : ( i ) A novel approach to use pre-trained entailment models for question answering .
( ii ) A model that incorporates local ( sentence level ) entailment decisions with global ( document level ) entailment decisions to effectively aggregate information for multi-hop QA task .
( iii )
An empirical evaluation that shows entailment based QA can achieve stateof - the - art performance on two challenging multihop QA datasets , OpenBookQA and MultiRC .
Question Answering using Entailment Non-extractive question answering can be seen as a textual entailment problem , where we verify whether a hypothesis constructed out of a question and a candidate answer is entailed by the knowledge -a collection of sentences 1 in the source text .
The probability of an answer A , given a question Q , can be modeled as the probability of a set of premises { P i } entailing a hypothesis statement H qa constructed from Q and A : Pr[ A | Q , { P i } ] ? = Pr [ { P i } H qa ] ( 1 ) Here we use to denote textual entailment .
Given QA training data , we can then learn a model that approximates the entailment probability Pr [ { P i } H qa ] .
Can one build an effective QA model g e using an existing entailment model f e that has been pretrained on a large-scale entailment dataset ?
Figure 2 illustrates two straightforward ways of doing so , using f e as a black - box function :
Use f e to check how much each sentence P i entails H qa on its own , and aggregate these local entailment decisions , for instance , using a max operation .
g e ( { P i } , H qa ) = max i f e ( P i , H qa ) ( 2 ) ( ii ) Concatenate Premises ( Concat ) :
Combine the premise sentences in a sequence to form a single large passage P , and use f e to check whether this passage as a whole entails the hypothesis H qa , making a single entailment decision : g e ( { P i } , H qa ) = f e ( P , H qa ) ( 3 )
Our experiments reveal , however , that neither approach is an effective means of using pre-trained entailment models for QA ( see Table 1 ) .
For the example in Figure 1 , Max model would not be able to consider information from P1 and P3 together .
Instead , it will pickup Silicon Valley as the answer since P2 is close to H s , " Facebook was launched in Silicon Valley " .
Similarly , Concat would also be muddled by distracting information in P2 , which will weaken its confidence in answer Cambridge .
Therefore , without careful guidance , simple aggregation can easily add distracting information into the premise representation , causing entailment to fail .
This motivates the need for new , effective mechanisms for global reasoning over a collection of premises .
Our Approach : Multee
We propose a new entailment based QA model , Multee , with two components : ( i ) a sentence relevance model , which learns to focus on the relevant sentences , and ( ii ) a multi-layer aggregator , which uses an entailment model to obtain multiple layers of question - relevant representations for the premises and then composes them using the sentence - level scores from the relevance model .
Finding relevant sentences is a form of local entailment between each premise and the answer hypothesis , whereas aggregating questionrelevant representations is a form of global entailment between all premises and the answer hypothesis .
This means , we can effectively re-purpose the same pre-trained entailment function f e for both components .
Figure 3 shows an architecture that uses multiple copies of f e to achieve this .
Sentence Relevance Model
The goal of this module is to identify sentences in the paragraph that are important for the given hypothesis .
As shown in Figure 1 , this helps the global module aggregate relevant content while reducing the chances of picking up distracting information .
A sentence is considered important if it contains information that is relevant to answering the question .
In other words , the importance of a sentence can be modeled as its entailment probability , i.e. , how well the sentence by itself supports the answer hypothesis .
We can use a pre-trained entailment model to obtain this .
The importance ?
i of a sentence P i can be modeled as : ? i = f e ( P i , H qa ) ( 4 )
This can be further improved by modeling the sentence with its surrounding context .
This is especially useful for passage - level QA , where the neighboring sentences provide useful context .
Given a premise sentence P i , the entailment function f e computes a single hypothesis -aware representation x i containing information in the premise that is relevant to entailing the answer hypothesis H qa .
This is essentially the output of last layer of neural function f e before projecting it to logits .
We denote this part of f e that outputs last vector representation as f ev and full f e that gives entailment probability as f ep .
We use these hypothesis- aware x i vectors for each sentence as inputs to a BiLSTM producing a contextual representation c i for each premise sentence P i , which is then fed to a feedforward layer that predicts the sentence - level importance as : ?
i = softmax ( W T c i + b ) ( 5 )
The components for generating x i are part of the original entailment function f e and can be pretrained on the entailment dataset .
The BiLSTM to compute c i and the parameters W and b for computing ?
i are not part of the original entailment function and thus can only be trained on the target QA task .
We perform this additional contextualization only when sentences form contiguous text .
Additionally , for datasets such as MultiRC , where the relevant sentences have been marked , we introduce a loss term based on the true relevance label and predicted weights , ? i .
Multi-level Aggregation
The goal of this module is to aggregate representations from important sentences in order to make a global entailment decision .
There are two key questions to answer : ( 1 ) how to combine the sentence - level information into a paragraph - level representation and ( 2 ) how to use the sentence relevance weights {?
i }.
Most entailment models include many layers that transform the input premise and the hypothesis .
A typical neural entailment stack includes en- coding layers that independently generate contextual representations of the premise and the hypothesis , followed by some cross-attention layer that yields relationships between the premise and hypothesis words , and additional layers that use this cross-attention to generate premise attended representations of the hypothesis and vice versa .
The final layers are classification layers which determine entailment based on the representations from the previous layer .
Each layer thus generates intermediate representation that captures different type of entailment related information .
This presents us with a choice of multiple points for aggregation .
Figure 3 illustrates our approach for aggregating sentence - level representations into a single paragraph level representation .
For each premise P i in the passage , we first process the pair ( P i , H qa ) through the entailment stack ( f ev ) resulting in a set of intermediate representations { Xi } for each layer .
We can choose a particular layer to be the aggregation layer .
We then compute a weighted combination of the sentence - level outputs at this layer {
Xi } to produce a passage - level representation ? .
The weights for the sentences are obtained from the Sentence Relevance model .
We refer to this as a join operation as shown in the Figure 3 .
Layers of the entailment function f ev that are below the join operate at a sentencelevel , while layers above the join now operate over paragraph - wise representations .
The final layer ( i.e. the top most layer ) of f ev thus gives us a vector representation of the entire passage .
This type of join can be applied at multiple layers resulting in paragraph vectors that correspond to multiple levels of aggregation .
We concatenate these paragraph vectors and pass them through a feedforward network projecting them down to logits , that can be used to compute the final passage wide entailment probabilities .
Join Operations
Given a set of sentence - wise outputs from the lower layer { Xi } and the corresponding sentencerelevance weights {?
i } , the join operation combines them into a single passage - level representation ? , which can be directly consumed by the layer above it in the stack .
The specifics of the join operation depends on the shape of the outputs from the lower layer , and the shape of the inputs expected by the layer after the join .
Here we show four possible join operations , one for each layer .
The ones defined for Score Layer and Embedding Layer can be reduced to black - box baselines , while we use the other two in Multee .
Score Layer :
The score layer outputs the entailment probabilities {s i } for each premise to hypothesis independently , which need to be joined to one entailment score .
One way to do this is to simply take a weighted maximum of the individual entailment probabilities .
So we have Xi = s i ?i and ?
= max i ?
i s i .
This reduces to black - box Max model ( Equation 2 ) when using {?
i } = 1 . Embedding Layer :
The embedding layer outputs a sequence of embedded vectors of [ Pi ] 2 one sequence for each premise P i and another sequence of embedded vectors [ Hqa ] for the answer hypothesis H qa .
A join operation in this case scales each embedded vector in a premise by its relevance weight and concatenates them together to For non-contextual word embeddings , this reduces to Concat Premises ( Eq. 3 ) when {?
i } = 1 . Final Layer ( FL ) :
The final layer in the entailment stack usually outputs a single vector h which is then used in a linear layer and softmax to produce label probabilities .
The join operation here is a weighted sum of the premise-level vectors .
So we have Xi = hi ?i and ? = i ?
i hi .
This is similar to a standard attention mechanism , where attended representation is computed by summing the scaled representations .
However , such scaled addition is not possible when the outputs from lower layers are not of the same shapes , as in the following case .
Cross Attention Layer ( CA ) : Cross-attention is a standard component of many entailment and reading comprehension models .
This layer produces three outputs : ( i ) For each premise P i , we get a hypothesis to premise cross attention matrix M hp i with shape ( h ? p i ) , where h is the number of hypothesis tokens , and p i is the number of tokens in premise P i ; ( ii ) for each premise P i , we get a sequence of vectors [ Pi ] that corresponds to the token sequence of the premise P i ; and ( iii ) for the hypothesis , we get a single sequence of vectors [ Hqa ] that corresponds to its token sequence .
M hp i attention matrix was generated by cross attention from [ Hqa ] to [ Pi ] .
The join operation in this layer produces a cross attention matrix that spans the entire passage , i.e. , has shape ( h ? p ) , where p is the total number of tokens across all premises .
The operation first scales the cross-attention matrices by the sentence - relevance weights {?
i } in order to " tone down " the influence of distracting / irrelevant sentences , and then re-normalizes the final matrix : Xi = ( M hp i , [ Pi ] , [ Hqa ] ) ?i
M hp = ?
i M hp 1 ; . . . ; ?
i M hpn M hp ij = M hp ij k M hp ik [ P ] = [ P1 ] ; [ P2 ] ; ... ; [ Pn ] ? = ( M hp , P , Hqa ) where M hp ij is i th row and j th column of M hp .
Multee 's multi-layer aggregator module uses join operations at two levels : Cross Attention Layer ( CA ) and Final Layer ( FL ) .
The two corresponding aggregators share parameters up till the lower of the two join layers ( CA in this case ) , where they both operate at the sentence level .
Above this layer , one aggregator switches to operating at the paragraph level , where it has its own , unshared parameters .
In general , if Multee were to aggregate at layers i1 , i2 , . . . , ik , then the aggregators with joins at layers and respectively could share parameters at layers 1 , . . . , min { , }.
Implementation Details Multee uses the ESIM stack as the entailment function pre-trained on SNLI and MultiNLI for both the relevance module and for the multi-layer aggregator module .
It uses aggregation at twolevels , one at the cross-attention level ( CA ) and one at the final layer ( FL ) .
All uses of the entailment function in Multee are initialized with the same pre-trained entailment model weights .
The embedding layer and the BiLSTM layer process paragraph - level contexts but processing at higher layers are done either at premise level or paragraph - level depending on where the join operation is performed .
Experiments Datasets :
We evaluate Multee on two datasets , OpenBookQA ( Mihaylov et al. , 2018 ) and Mul-tiRC ( Khashabi et al. , 2018 ) , both of which are specifically designed to test reasoning over multiple sentences .
MultiRC is paragraph - based multiple -choice QA dataset derived from varying topics where the questions are answerable based on information from the paragraph .
In MultiRC , each question can have more than one correct answer choice , and so it can be viewed as a binary classification task ( one prediction per answer choice ) , with 4,848 / 4,583 examples in Dev / Test sets .
OpenBookQA , on the other hand , has multiple - choice science questions with exactly one correct answer choice and no associated paragraph .
As a result , this dataset requires the relevant facts to be retrieved from auxiliary resources including the open book of facts released with the paper and other sources such as WordNet ( Miller , 1995 ) and ConceptNet ( Speer and Havasi , 2012 ) .
It contains 500 questions in the Dev and Test sets .
( Mihaylov et al. , 2018 ) 53.9 48.9 --KER ( Mihaylov et al. , 2018 ) 55 Preprocessing :
For each question and answer choice , we create an answer hypothesis statement using a modified version of the script used in Sc-iTail construction .
We wrote a handful of rules to better convert the question and answer to a hypothesis .
We also mark the span of answer in the hypothesis with special begin and end tokens , @@@answer and answer@@@ respectively 3 . For MultiRC , we also apply an offthe-shelf coreference resolution model 4 and replace the mentions when they resolve to pronouns occurring in a different sentence 5 . For Open-BookQA , we use the exact same retrieval as released by the authors of OpenBookQA 6 and use the OpenBook and WordNet as the knowledge source with top 5 sentences retrieved per query .
Training Multee : For OpenBookQA we use cross entropy loss for labels corresponding to 4 answer choices .
For MultiRC , we use binary cross entropy loss for each answer-choice separately since in MultiRC each question can have more than one correct answer choice .
The entailment components are pre-trained on sentence - level entailment tasks and then fine-tuned as part of endto-end QA training .
The MultiRC dataset includes sentence - level relevance labels .
We supervise the Sentence Relevance module with a binary cross entropy loss for predicting these relevance labels when available .
We used PyTorch ( Paszke et al. , 2017 ) and AllenNLP to implement our models and ran them on Beaker 7 .
For pre-training we use the same hyper-parameters of ESIM ( Chen et al. , 2017 ) as available in implementation of AllenNLP ( Gardner et al. , 2017 ) and fine - tune the model parameters .
We do not perform any hyper-parameter tuning for any of our models .
We fine- tune all layers in ESIM except for the embedding layer .
Models Compared :
We experiment with Glove ( Pennington et al. , 2014 ) and ELMo ( Peters et al. , 2018 ) embeddings for Multee and compare with following three types of systems : ( A ) Baselines using entailment as a black - box
We use the pre-trained entailment model as a black - box in two ways : concatenate premises ( Concat ) and aggregate sentence level decisions with a max operation ( Max ) .
Both models were also pre-trained on SNLI and MultiNLI datasets and fine-tuned on the target QA datasets with same pre-processing .
( B ) Previously published results : For MultiRC , there are two published baselines : IR ( Information Retrieval ) and LR ( Logistic Regression ) .
These simple models turn out to be strong baselines on this relatively smaller sized dataset .
For Open-BookQA , we report published baselines from ( Mihaylov et al. , 2018 ) : Question Match with ELMo ( QM + ELMo ) , Question to Answer ESIM with ELMo ( ESIM + ELMo ) and their best result with the Knowledge Enhanced Reader ( KER ) .
( C ) Large Transformer based models :
We compare with OpenAI -Transformer ( OFT ) , pre-trained on large-scale language modeling task and finetuned on respective datasets .
A contemporaneous work , 8 which published these transformer results , also fine-tuned this transformer further on a large scale reading comprehension dataset , RACE ( Lai et al. , 2017 ) , before fine-tuning on the target QA datasets with their method , Reading Strategies .
Results
Table 1 summarizes the performance of all models .
Multee outperforms the black - box entailment baselines ( Concat and Max ) that were pretrained on the same data , previously published baselines , OpenAI transformer models .
We note that the 95 % confidence intervals around baseline accuracy for OpenBookQA and MultiRC are 4.3 % and 1.3 % , respectively .
On OpenBookQA test set , Multee with GloVe outperforms ensemble version of OpenAI transformer by 3.0 points in accuracy .
It also outperforms single model version of Reading Strategies system and is comparable to their ensemble version .
On MultiRC dev set , Multee with ELMo outperforms ensemble version of OpenAI transformer by 1.9 points in F1a , 2.7 in F1 m and 6.3 in EM .
It also outperforms single model version of Reading Strategies system and is comparable to their ensemble version .
Recall that the Reading Strategies results are reported with an additional fine-tuning on another larger QA dataset , RACE ( Lai et al. , 2017 ) aside from the target QA datasets we use here .
While ELMo contextual embeddings helped in MultiRC , it did not help OpenBookQA .
We believe this is in part due to the mismatch between our ELMo training setup where all sentences are treated as a single sequence , which , while true in MultiRC , is not the case in OpenBookQA .
In general , gains from Multee are more prominent in OpenBookQA than in MultiRC .
We hypothesize that a key contributor to this difference is distraction being a lesser challenge in Mul-tiRC , where premise sentences come from a single paragraph whose other sentences are often irrelevant and rarely distract towards incorrect answers .
OpenBookQA has a noisier set of sentences , since an equal number of sentences is retrieved for the correct and each incorrect answer choice .
Ablations Relevance Model Ablation .
Table 2 shows the utility of the relevance module .
We use the same setting as the full model ( aggregation at Cross Attention ( CA ) and the Final Layer ( FL ) ) .
As shown in the table , using the relevance module weights ( ?
i ) leads to improved accuracy on both datasets ( substantially so in OpenBook QA ) as compared to ignoring the module , i.e. , setting all weights to 1 ( ? i ) .
In MultiRC , we show that the additional supervision for the relevance module leads to even further improvements in score .
Multi-Level Aggregator Ablation .
Multee performs aggregation at two levels : Cross Attention Layer ( CA ) and Final Layer ( FL ) .
We denote this by CA +FL .
To show that multi-level aggregation is better than individual aggregations , we train models with aggregation at only FL and at only CA .
Table 3 shows that multi-layer aggregation is better than CA or FL alone on both the datasets .
Effect of Pre-training
One of the benefits of using entailment based components in a QA model is that we can pre-train them on large scale entailment datasets and finetune them as part of the QA model .
Table 4 shows that such pre-training is valuable .
The model trained from scratch is substantially worse in the case of OpenBookQA , highlighting the benefits of our entailment - based QA model .
Multee benefits come from two sources : ( i ) Re-purposing of entailment function for multisentence question answering , and ( ii ) transferring from a large-scale entailment task .
In the case of OpenBookQA , both are helpful .
For MultiRC , only the first is a significant contributor .
Table 5 shows that re-purposing was a bigger factor for MultiRC , since Max and Concat models do not work well when trained from scratch .
scores for a question in MultiRC .
Overall , we find that two types of behaviors emerge from different loss functions .
For instance , trying to minimize the sum of attention probability mass on irrelevant sentences i.e. i ?
i ( 1 ? y i ) , called IR Sum Loss , causes the attention scores to become " peaky " i.e , high for one or two sentences , and close to zero for others .
This leads to higher precision but at significantly lower recall for the QA system , as it now uses information from fewer but highly relevant sentences .
Binary cross entropy loss ( BCE ) allows the model to attend to more relevant sentences thereby increasing recall without too much drop in precision .
Failure Cases .
As Figure 5 shows , our model with BCE loss tends to distribute the attention , especially to sentences close to the relevant ones .
We hypothesize that the model is learning to use the contextualized BiLSTM representations to incorporate information from neighboring sentences , which is useful for this task and for passage understanding in general .
For example , more than 60 % of Dev questions in MultiRC have at least one adjacent relevant sentence pair .
Figure 4a illustrates this behavior .
On the other hand , if the relevant sentences are far apart , the model finds it difficult to handle such long- range cross sentence dependencies in its contextualized representations .
As a result , it ends up focusing attention on the most relevant sentence , missing out on other relevant sentences ( Figure 4 b ) .
When these unattended but relevant sentences contain the answer , the model fails .
Related Work Entailment systems have been applied to questionanswering before but have only had limited success ( Harabagiu and Hickl , 2006 ; Sacaleanu et al. , 2008 ; Clark et al. , 2012 ) in part because of the small size of the early entailment datasets ( Dagan et al. , 2006 ( Dagan et al. , , 2013 .
Recent large scale entailment datasets such as SNLI ( Bowman et al. , 2015 ) and MultiNLI ( Williams et al. , 2018 ) have led to many new powerful neural entailment models that are not only more effective , but also produce better representations of sentences ( Conneau et al. , 2017 ) .
Models such as Decomposable Attention ( Parikh et al. , 2016 ) and ESIM ( Chen et al. , 2017 ) , on the other hand , find alignments between the hypothesis and premise words through crossattention .
However , these improvements in entailment models have not yet translated to improvements in end tasks such as question answering .
SciTail was created from a science QA task to push for models with a direct impact on QA .
Entailment models trained on this dataset show minor improvements on the Aristo Reasoning Challenge Musa et al. , 2018 ) .
However , these QA systems make independent predictions and can not combine in-formation from multiple supporting sentences .
Combining information from multiple sentences is a key problem in language understanding .
Recent Reading comprehension datasets ( Welbl et al. , 2018 ; Khashabi et al. , 2018 ; Yang et al. , 2018 ; Mihaylov et al. , 2018 ) explicitly evaluate a system 's ability to perform such reasoning through questions that need information from multiple sentences in a passage .
Most approaches on these tasks perform simple attention - based aggregation ( Mihaylov et al. , 2018 ; Song et al. , 2018 ; Cao et al. , 2018 ) and do not exploit the entailment models trained on large scale datasets .
Conclusions
Using entailment for question answering has seen limited success .
Neural entailment models are designed and trained on tasks defined over sentence pairs , whereas QA often requires reasoning over longer texts spanning multiple sentences .
We propose Multee , a novel QA model that addresses this mismatch .
It uses an existing entailment model to both focus on relevant sentences and aggregate information from these sentences .
Results on two challenging QA datasets , as well as our ablation study , indicate that entailment based QA can achieve state - of - the - art performance and is a promising direction for further research .
Figure 1 : 1 Figure 1 : An example illustrating the challenges in using sentence - level entailment model for multi-sentence reasoning needed for QA , and the high- level approach used in Multee .
Figure 2 : 2 Figure 2 : Black Box Applications of Textual Entailment Model for QA : Max and Concat models ( i ) Aggregate Local Decisions ( Max ) : Use f e to check how much each sentence P i entails H qa on its own , and aggregate these local entailment decisions , for instance , using a max operation .
Figure 3 : 3 Figure 3 : Multee overview : Multee includes two main components , a relevance module , and a multi-layer aggregator module .
Both modules use pre-trained entailment functions ( f ep and f ev ) .
f ep is the full entailment model that gives entailment probability , and f ev is part of it excluding last projection to logits and softmax .
The multi-level aggregator uses multiple copies of entailment function f ev , one for each sub-aggregator performing a join at a different layer .
Right part of figure zooms in on one such sub-aggregator joining at layer .
form [ P ] .
Hqa is passed through unchanged .
Xi = ( [ Pi ] , [ Hqa ] ) ? i [ P ] = [?
1 [ P1 ] ; ? 2 [ P2 ] ; . . . ; ? n [ Pn ] ] ? = [ P ] , [ Hqa ]
Figure 4 : 4 Figure 4 : Success and failure examples of Multee from MultiRC .
R : annotated relevant sentences .
Green / yellow : high / low predicted relevance .
Figure 5 : 5 Figure 5 : Sentence level attentions for various sentence relevance losses .
R : annotated relevant sentences .
Table 2 : 2 Relevance Model Ablation of Multee .
? i : without relevance weights , ? i : with relevance weights respectively , ? i + supervise : with supervised relevance weights .
Test results on OpenBookQA and Dev results on MultiRC .
OpenBookQA MultiRC Accuracy F1a | F1 m ?i 50.6 67.3 | 70.3 ? i 55.8 67.4 | 71.0 ?i + supervise - 68.3 | 71.7 Aggregator OpenBookQA MultiRC Accuracy F1a | F1 m Cross Attention ( CA ) 45.8 67.2 | 71.1 Final Layer ( FL ) 51.0 68.3 | 71.5 CA + FL 55.8 68.3 | 71.7
Table 3 : 3 Aggregator Level Ablation of Multee .
On MultiRC , Multee uses relevance supervision but not on OpenBookQA because of unavailibility .
Test results on OpenBookQA and Dev results on MultiRC .
Table 4 : 4 Effect ( on test data ) of pre-training the entailment model used in Multee .
Table 5 : 5 Pre-training ablations of black - box entailment baselines for OpenBookQA ( test ) and MultiRC ( dev ) .
OpenBookQa MultiRc Accuracy F1a | F1 m Max Snli + MultiNli Scratch 47.6 32.4 66.8 | 70.3 42.8 | 44.0 Concat Snli + MultiNli Scratch 42.6 35.8 66.9 | 70.7 51.3 | 50.4 5 Analysis Relevance Loss .
The sentence - level relevance model provides a way to dig deeper into the over - all QA model 's behavior .
When sentence - level su- pervision is available , as in the case of MultiRC , we can analyze the impact of different auxiliary losses for the relevance module .
Table 6 shows the QA performance with different relevance losses , and Figure 5 shows a visualization of attention
Table 6 : 6 F1a precision and recall on MultiRC Dev with 2 kinds of relevance losses .
IR Sum is the sum of attention probability mass on irrelevant sentences .
BCE is Binary Cross Entropy loss .
This collection can be a sequence in the case of passage comprehension or a list of sentences , potentially from varied sources , in the case of QA over multiple documents .
We use [ . ] to denote a sequence and . to denote a vector
Answer span marking gave substantial gains for all entailment based models including the baselines .
4
https://github.com/huggingface/neuralcoref
5
It is hard to learn co-reference , as these target datasets are too small to learn this in an end-to- end fashion .
6
https://github.com/allenai/OpenBookQA
https://beaker.org/
Published on arXiv on Oct 31 , 2018 ( Sun et al. , 2019 ) .
