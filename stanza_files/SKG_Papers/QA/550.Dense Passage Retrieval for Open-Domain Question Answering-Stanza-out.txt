title
Dense Passage Retrieval for Open-Domain Question Answering
abstract
Open-domain question answering relies on efficient passage retrieval to select candidate contexts , where traditional sparse vector space models , such as TF - IDF or BM25 , are the de facto method .
In this work , we show that retrieval can be practically implemented using dense representations alone , where embeddings are learned from a small number of questions and passages by a simple dualencoder framework .
When evaluated on a wide range of open-domain QA datasets , our dense retriever outperforms a strong Lucene - BM25 system greatly by 9 % - 19 % absolute in terms of top - 20 passage retrieval accuracy , and helps our end-to - end QA system establish new state - of - the - art on multiple open-domain QA benchmarks .
1 * Equal contribution 1
The code and trained models have been released at https://github.com/facebookresearch/DPR.
Introduction Open-domain question answering ( QA ) ( Voorhees , 1999 ) is a task that answers factoid questions using a large collection of documents .
While early QA systems are often complicated and consist of multiple components ( Ferrucci ( 2012 ) ; Moldovan et al. ( 2003 ) , inter alia ) , the advances of reading comprehension models suggest a much simplified two -stage framework : ( 1 ) a context retriever first selects a small subset of passages where some of them contain the answer to the question , and then ( 2 ) a machine reader can thoroughly examine the retrieved contexts and identify the correct answer ( Chen et al. , 2017 ) .
Although reducing open-domain QA to machine reading is a very reasonable strategy , a huge performance degradation is often observed in practice 2 , indicating the needs of improving retrieval .
Retrieval in open-domain QA is usually implemented using TF - IDF or BM25 ( Robertson and Zaragoza , 2009 ) , which matches keywords efficiently with an inverted index and can be seen as representing the question and context in highdimensional , sparse vectors ( with weighting ) .
Conversely , the dense , latent semantic encoding is complementary to sparse representations by design .
For example , synonyms or paraphrases that consist of completely different tokens may still be mapped to vectors close to each other .
Consider the question " Who is the bad guy in lord of the rings ? " , which can be answered from the context " Sala Baker is best known for portraying the villain Sauron in the Lord of the Rings trilogy . "
A term- based system would have difficulty retrieving such a context , while a dense retrieval system would be able to better match " bad guy " with " villain " and fetch the correct context .
Dense encodings are also learnable by adjusting the embedding functions , which provides additional flexibility to have a task -specific representation .
With special in-memory data structures and indexing schemes , retrieval can be done efficiently using maximum inner product search ( MIPS ) algorithms ( e.g. , Shrivastava and Li ( 2014 ) ; Guo et al . ( 2016 ) ) .
However , it is generally believed that learning a good dense vector representation needs a large number of labeled pairs of question and contexts .
Dense retrieval methods have thus never be shown to outperform TF - IDF / BM25 for opendomain QA before ORQA , which proposes a sophisticated inverse cloze task ( ICT ) objective , predicting the blocks that contain the masked sentence , for additional pretraining .
The question encoder and the reader model are then finetuned using pairs of questions and answers jointly .
Although ORQA successfully demonstrates that dense retrieval can outperform BM25 , setting new state - of - the - art results on multiple open-domain QA datasets , it also suffers from two weaknesses .
First , ICT pretraining is computationally intensive and it is not completely clear that regular sentences are good surrogates of questions in the objective function .
Second , because the context encoder is not fine- tuned using pairs of questions and answers , the corresponding representations could be suboptimal .
In this paper , we address the question : can we train a better dense embedding model using only pairs of questions and passages ( or answers ) , without additional pretraining ?
By leveraging the now standard BERT pretrained model and a dual-encoder architecture ( Bromley et al. , 1994 ) , we focus on developing the right training scheme using a relatively small number of question and passage pairs .
Through a series of careful ablation studies , our final solution is surprisingly simple : the embedding is optimized for maximizing inner products of the question and relevant passage vectors , with an objective comparing all pairs of questions and passages in a batch .
Our Dense Passage Retriever ( DPR ) is exceptionally strong .
It not only outperforms BM25 by a large margin ( 65.2 % vs. 42.9 % in Top -5 accuracy ) , but also results in a substantial improvement on the end-to - end QA accuracy compared to ORQA ( 41.5 % vs. 33.3 % ) in the open Natural Questions setting .
Our contributions are twofold .
First , we demonstrate that with the proper training setup , simply fine-tuning the question and passage encoders on existing question - passage pairs is sufficient to greatly outperform BM25 .
Our empirical results also suggest that additional pretraining may not be needed .
Second , we verify that , in the context of open-domain question answering , a higher retrieval precision indeed translates to a higher end-to - end QA accuracy .
By applying a modern reader model to the top retrieved passages , we achieve comparable or better results on multiple QA datasets in the open-retrieval setting , compared to several , much complicated systems .
Background
The problem of open-domain QA studied in this paper can be described as follows .
Given a factoid question , such as " Who first voiced Meg on Family Guy ? " or " Where was the 8th Dalai Lama born ? " , a system is required to answer it using a large corpus of diversified topics .
More specifically , we assume the extractive QA setting , in which the answer is restricted to a span appearing in one or more passages in the corpus .
Assume that our collection contains D documents , d 1 , d 2 , ? ? ? , d D .
We first split each of the documents into text passages of equal lengths as the basic retrieval units 3 and get M total passages in our corpus C = {p 1 , p 2 , . . . , p M } , where each passage p i can be viewed as a sequence of tokens w ( i ) 1 , w ( i ) 2 , ? ? ? , w ( i ) | p i | .
Given a question q , the task is to find a span w ( i ) s , w ( i ) s+1 , ? ? ? , w ( i ) e from one of the passages p i that can answer the question .
Notice that to cover a wide variety of domains , the corpus size can easily range from millions of documents ( e.g. , Wikipedia ) to billions ( e.g. , the Web ) .
As a result , any open-domain QA system needs to include an efficient retriever component that can select a small set of relevant texts , before applying the reader to extract the answer ( Chen et al. , 2017 ) .
4 Formally speaking , a retriever R : ( q , C ) ?
C F is a function that takes as input a question q and a corpus C and returns a much smaller filter set of texts C F ? C , where | C F | = k | C | .
For a fixed k , a retriever can be evaluated in isolation on top-k retrieval accuracy , which is the fraction of questions for which C F contains a span that answers the question .
Dense Passage Retriever ( DPR )
We focus our research in this work on improving the retrieval component in open-domain QA .
Given a collection of M text passages , the goal of our dense passage retriever ( DPR ) is to index all the passages in a low-dimensional and continuous space , such that it can retrieve efficiently the top k passages relevant to the input question for the reader at run-time .
Note that M can be very large ( e.g. , 21 million passages in our experiments , described in Section 4.1 ) and k is usually small , such as 20- 100 .
Overview
Our dense passage retriever ( DPR ) uses a dense encoder E P ( ? ) which maps any text passage to a ddimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval .
At run-time , DPR applies a different encoder E Q ( ? ) that maps the input question to a d-dimensional vector , and retrieves k passages of which vectors are the closest to the question vector .
We define the similarity between the question and the passage using the dot product of their vectors : sim(q , p ) = E Q ( q ) E P ( p ) .
( 1 ) Although more expressive model forms for measuring the similarity between a question and a passage do exist , such as networks consisting of multiple layers of cross attentions , the similarity function needs to be decomposable so that the representations of the collection of passages can be precomputed .
Most decomposable similarity functions are some transformations of Euclidean distance ( L2 ) .
For instance , cosine is equivalent to inner product for unit vectors and the Mahalanobis distance is equivalent to L2 distance in a transformed space .
Inner product search has been widely used and studied , as well as its connection to cosine similarity and L2 distance ( Mussmann and Ermon , 2016 ; Ram and Gray , 2012 ) .
As our ablation study finds other similarity functions perform comparably ( Section 5.2 ; Appendix B ) , we thus choose the simpler inner product function and improve the dense passage retriever by learning better encoders .
Encoders
Although in principle the question and passage encoders can be implemented by any neural networks , in this work we use two independent BERT networks ( base , uncased ) and take the representation at the [ CLS ] token as the output , so d = 768 .
Inference During inference time , we apply the passage encoder E P to all the passages and index them using FAISS ( Johnson et al. , 2017 ) offline .
FAISS is an extremely efficient , open-source library for similarity search and clustering of dense vectors , which can easily be applied to billions of vectors .
Given a question q at run-time , we derive its embedding v q = E Q ( q ) and retrieve the top k passages with embeddings closest to v q .
Training Training the encoders so that the dot-product similarity ( Eq. ( 1 ) ) becomes a good ranking function for retrieval is essentially a metric learning problem ( Kulis , 2013 ) .
The goal is to create a vector space such that relevant pairs of questions and passages will have smaller distance ( i.e. , higher simi-larity ) than the irrelevant ones , by learning a better embedding function .
Let D = { q i , p + i , p ?
i,1 , ? ? ? , p ?
i , n } m i=1 be the training data that consists of m instances .
Each instance contains one question q i and one relevant ( positive ) passage p + i , along with n irrelevant ( negative ) passages p ? i , j . We optimize the loss function as the negative log likelihood of the positive passage : L( q i , p + i , p ?
i,1 , ? ? ? , p ? i , n ) ( 2 ) = ? log e sim( q i , p + i ) e sim( q i , p + i ) + n j=1 e sim( q i , p ? i , j ) . Positive and negative passages
For retrieval problems , it is often the case that positive examples are available explicitly , while negative examples need to be selected from an extremely large pool .
For instance , passages relevant to a question may be given in a QA dataset , or can be found using the answer .
All other passages in the collection , while not specified explicitly , can be viewed as irrelevant by default .
In practice , how to select negative examples is often overlooked but could be decisive for learning a high-quality encoder .
We consider three different types of negatives : ( 1 ) Random : any random passage from the corpus ; ( 2 ) BM25 : top passages returned by BM25 which do n't contain the answer but match most question tokens ; ( 3 ) Gold : positive passages paired with other questions which appear in the training set .
We will discuss the impact of different types of negative passages and training schemes in Section 5.2 .
Our best model uses gold passages from the same mini-batch and one BM25 negative passage .
In particular , re-using gold passages from the same batch as negatives can make the computation efficient while achieving great performance .
We discuss this approach below .
In - batch negatives
Assume that we have B questions in a mini-batch and each one is associated with a relevant passage .
Let Q and P be the ( B ?d ) matrix of question and passage embeddings in a batch of size B. S = QP T is a ( B ? B ) matrix of similarity scores , where each row of which corresponds to a question , paired with B passages .
In this way , we reuse computation and effectively train on B 2 ( q i , p j ) question / passage pairs in each batch .
Any ( q i , p j ) pair is a positive example when i = j , and negative otherwise .
This creates B training instances in each batch , where there are B ? 1 negative passages for each question .
The trick of in - batch negatives has been used in the full batch setting ( Yih et al. , 2011 ) and more recently for mini-batch ( Henderson et al. , 2017 ; Gillick et al. , 2019 ) .
It has been shown to be an effective strategy for learning a dual-encoder model that boosts the number of training examples .
Experimental Setup
In this section , we describe the data we used for experiments and the basic setup .
Wikipedia Data Pre-processing Following , we use the English Wikipedia dump from Dec. 20 , 2018 as the source documents for answering questions .
We first apply the pre-processing code released in DrQA ( Chen et al. , 2017 ) to extract the clean , text-portion of articles from the Wikipedia dump .
This step removes semi-structured data , such as tables , infoboxes , lists , as well as the disambiguation pages .
We then split each article into multiple , disjoint text blocks of 100 words as passages , serving as our basic retrieval units , following , which results in 21,015,324 passages in the end .
5 Each passage is also prepended with the title of the Wikipedia article where the passage is from , along with an [ SEP ] token .
Question Answering Datasets
We use the same five QA datasets and training / dev/testing splitting method as in previous work .
Below we briefly describe each dataset and refer readers to their paper for the details of data preparation .
Natural Questions ( NQ ) was designed for end-to - end question answering .
The questions were mined from real Google search queries and the answers were spans in Wikipedia articles identified by annotators .
TriviaQA ( Joshi et al. , 2017 ) contains a set of trivia questions with answers that were originally scraped from the Web .
WebQuestions ( WQ ) ( Berant et al. , 2013 ) consists of questions selected using Google Suggest API , where the answers are entities in Freebase .
CuratedTREC ( TREC ) ( Baudi ? and ?ediv? , 2015 )
Selection of positive passages
Because only pairs of questions and answers are provided in TREC , WebQuestions and TriviaQA 6 , we use the highest - ranked passage from BM25 that contains the answer as the positive passage .
If none of the top 100 retrieved passages has the answer , the question will be discarded .
For SQuAD and Natural Questions , since the original passages have been split and processed differently than our pool of candidate passages , we match and replace each gold passage with the corresponding passage in the candidate pool .
7
We discard the questions when the matching is failed due to different Wikipedia versions or pre-processing .
Table 1 shows the number of questions in training / dev/ test sets for all the datasets and the actual questions used for training the retriever .
Experiments : Passage Retrieval
In this section , we evaluate the retrieval performance of our Dense Passage Retriever ( DPR ) , along with analysis on how its output differs from traditional retrieval methods , the effects of different training schemes and the run-time efficiency .
The DPR model used in our main experiments is trained using the in- batch negative setting ( Section 3.2 ) with a batch size of 128 and one additional BM25 negative passage per question .
We trained the question and passage encoders for up to 40 epochs for large datasets ( NQ , TriviaQA , SQuAD ) and 100 epochs for small datasets ( TREC , WQ ) , with a learning rate of 10 ?5 using Adam , linear scheduling with warm - up and dropout rate 0.1 .
While it is good to have the flexibility to adapt the retriever to each dataset , it would also be desirable to obtain a single retriever that works well across the board .
To this end , we train a multidataset encoder by combining training data from all datasets excluding SQuAD .
8
In addition to DPR , we also present the results of BM25 , the traditional retrieval method 9 and BM25 + DPR , using a linear combination of their scores as the new ranking function .
Specifically , we obtain two initial sets of top - 2000 passages based on BM25 and DPR , respectively , and rerank the union of them using BM25 ( q , p ) + ? ? sim(q , p ) as the ranking function .
We used ? = 1.1 based on the retrieval accuracy in the development set .
Main Results
Table 2 compares different passage retrieval systems on five QA datasets , using the top-k accuracy ( k ? { 20 , 100 } ) .
With the exception of SQuAD , DPR performs consistently better than BM25 on all datasets .
The gap is especially large when k is small ( e.g. , 78.4 % vs. 59.1 % for top - 20 accuracy on Natural Questions ) .
When training with mul-8 SQuAD is limited to a small set of Wikipedia documents and thus introduces unwanted bias .
We will discuss this issue more in Section 5.1 .
9 Lucene implementation .
BM25 parameters b = 0.4 ( document length normalization ) and k1 = 0.9 ( term frequency scaling ) are tuned using development sets .
Results can be improved further in some cases by combining DPR with BM25 in both single - and multi-dataset settings .
We conjecture that the lower performance on SQuAD is due to two reasons .
First , the annotators wrote questions after seeing the passage .
As a result , there is a high lexical overlap between passages and questions , which gives BM25 a clear advantage .
Second , the data was collected from only 500 +
Wikipedia articles and thus the distribution of training examples is extremely biased , as argued previously by .
Ablation Study on Model Training
To understand further how different model training options affect the results , we conduct several additional experiments and discuss our findings below .
Sample efficiency
We explore how many training examples are needed to achieve good passage retrieval performance .
Figure 1 illustrates the top-k retrieval accuracy with respect to different numbers of training examples , measured on the development set of Natural Questions .
As is shown , a dense passage retriever trained using only 1,000 examples already outperforms BM25 .
This suggests that with a general pretrained language model , it is possible to train a high-quality dense retriever with a small number of question - passage pairs .
Adding more training examples ( from 1 k to 59 k ) further improves the retrieval accuracy consistently .
In - batch negative training
We test different training schemes on the development set of Natural Questions and summarize the results in Table 3 .
The top block is the standard 1 - of - N training setting , where each question in the batch is paired with a positive passage and its own set of n negative passages ( Eq. ( 2 ) ) .
We find that the choice of negatives - random , BM25 or gold passages ( positive passages from other questions ) - does not impact the top-k accuracy much in this setting when k ? 20 .
The middle bock is the in- batch negative training ( Section 3.2 ) setting .
We find that using a similar configuration ( 7 gold negative passages ) , in - batch negative training improves the results substantially .
The key difference between the two is whether the gold negative passages come from the same batch or from the whole training set .
Effectively , in - batch negative training is an easy and memory - efficient way to reuse the negative examples already in the batch rather than creating new ones .
It produces more pairs and thus increases the number of training examples , which might contribute to the good model performance .
As a result , accuracy consistently improves as the batch size grows .
Finally , we explore in - batch negative training with additional " hard " negative passages that have high BM25 scores given the question , but do not contain the answer string ( the bottom block ) .
These additional passages are used as negative passages for all questions in the same batch .
We find that adding a single BM25 negative passage improves the result substantially while adding two does not help further .
Our experiments on Natural Questions show that switching to distantly - supervised passages ( using the highest - ranked BM25 passage that contains the answer ) , has only a small impact : 1 point lower top-k accuracy for retrieval .
Appendix
A contains more details .
Impact of gold passages Similarity and loss Besides dot product , cosine and Euclidean L2 distance are also commonly used as decomposable similarity functions .
We test these alternatives and find that L2 performs comparable to dot product , and both of them are superior to cosine .
Similarly , in addition to negative loglikelihood , a popular option for ranking is triplet loss , which compares a positive passage and a negative one directly with respect to a question ( Burges et al. , 2005 ) .
Our experiments show that using triplet loss does not affect the results much .
More details can be found in Appendix B. Cross-dataset generalization
One interesting question regarding DPR 's discriminative training is how much performance degradation it may suffer from a non-iid setting .
In other words , can it still generalize well when directly applied to a different dataset without additional fine-tuning ?
To test the cross-dataset generalization , we train DPR on Natural Questions only and test it directly on the smaller WebQuestions and CuratedTREC datasets .
We find that DPR generalizes well , with 3 - 5 points loss from the best performing fine - tuned model in top - 20 retrieval accuracy ( 69.9/86.3 vs. 75.0/89.1 for WebQuestions and TREC , respectively ) , while still greatly outperforming the BM25 baseline ( 55.0/70.9 ) .
Qualitative Analysis
Although DPR performs better than BM25 in general , passages retrieved by these two methods differ qualitatively .
Term-matching methods like BM25 are sensitive to highly selective keywords and phrases , while DPR captures lexical variations or semantic relationships better .
See Appendix C for examples and more discussion .
Run-time Efficiency
The main reason that we require a retrieval component for open-domain QA is to reduce the number of candidate passages that the reader needs to consider , which is crucial for answering user 's questions in real-time .
We profiled the passage retrieval speed on a server with Intel Xeon CPU E5-2698 v4 @ 2.20 GHz and 512GB memory .
With the help of FAISS in-memory index for real- valued vectors 10 , DPR can be made incredibly efficient , processing 995.0 questions per second , returning top 100 passages per question .
In contrast , BM25 / Lucene ( implemented in Java , using file index ) processes 23.7 questions per second per CPU thread .
On the other hand , the time required for building an index for dense vectors is much longer .
Computing dense embeddings on 21 - million passages is resource intensive , but can be easily parallelized , taking roughly 8.8 hours on 8 GPUs .
However , building the FAISS index on 21 - million vectors on a single server takes 8.5 hours .
In comparison , building an inverted index using Lucene is much cheaper and takes only about 30 minutes in total .
Experiments : Question Answering
In this section , we experiment with how different passage retrievers affect the final QA accuracy .
End-to- end QA System
We implement an end-to- end question answering system in which we can plug different retriever systems directly .
Besides the retriever , our QA system consists of a neural reader that outputs the answer to the question .
Given the top k retrieved passages ( up to 100 in our experiments ) , the reader assigns a passage selection score to each passage .
In addition , it extracts an answer span from each passage and assigns a span score .
The best span from the passage with the highest passage selection score is chosen as the final answer .
The passage selection model serves as a reranker through crossattention between the question and the passage .
Although cross-attention is not feasible for retrieving relevant passages in a large corpus due to its nondecomposable nature , it has more capacity than the dual- encoder model sim(q , p ) as in Eq. ( 1 ) .
Applying it to selecting the passage from a small number of retrieved candidates has been shown to work well ( Wang et al. , , 2018 Lin et al. , 2018 ) .
Specifically , let P i ?
R L?h ( 1 ? i ? k ) be a BERT ( base , uncased in our experiments ) representation for the i-th passage , where L is the maximum length of the passage and h the hidden dimension .
The probabilities of a token being the starting / ending positions of an answer span and a passage being selected are defined as : P start , i ( s ) = softmax P i w start s , ( 3 ) P end , i ( t ) = softmax P i w end t , ( 4 ) P selected ( i ) = softmax P w selected i , ( 5 ) where P = [ P [ CLS ] 1 , . . . , P [ CLS ] k ] ?
R h?k and w start , w end , w selected ?
R h are learnable vectors .
We compute a span score of the s-th to t-th words from the i-th passage as P start , i ( s ) ? P end , i ( t ) , and a passage selection score of the i-th passage as P selected ( i ) .
During training , we sample one positive and m ?
1 negative passages from the top 100 passages returned by the retrieval system ( BM25 or DPR ) for each question .
m is a hyper-parameter and we use m = 24 in all the experiments .
The training objective is to maximize the marginal log-likelihood of all the correct answer spans in the positive passage ( the answer string may appear multiple times in one passage ) , combined with the log-likelihood of the positive passage being selected .
We use the batch size of 16 for large ( NQ , TriviaQA , SQuAD ) and 4 for small ( TREC , WQ ) datasets , and tune k on the development set .
For experiments on small datasets under the Multi setting , in which using other datasets is allowed , we fine - tune the reader trained on Natural Questions to the target dataset .
All experiments were done on eight 32GB GPUs .
Results
Table 4 summarizes our final end-to- end QA results , measured by exact match with the reference answer after minor normalization as in ( Chen et al. , 2017 ; ) .
For WQ and TREC in the Multi setting , we fine - tune the reader trained on NQ .
see that higher retriever accuracy typically leads to better final QA results : in all cases except SQuAD , answers extracted from the passages retrieved by DPR are more likely to be correct , compared to those from BM25 .
For large datasets like NQ and TriviaQA , models trained using multiple datasets ( Multi ) perform comparably to those trained using the individual training set ( Single ) .
Conversely , on smaller datasets like WQ and TREC , the multidataset setting has a clear advantage .
Overall , our DPR - based models outperform the previous stateof - the - art results on four out of the five datasets , with 1 % to 12 % absolute differences in exact match accuracy .
It is interesting to contrast our results to those of ORQA and also the concurrently developed approach , REALM ( Guu et al. , 2020 ) .
While both methods include additional pretraining tasks and employ an expensive end-to - end training regime , DPR manages to outperform them on both NQ and TriviaQA , simply by focusing on learning a strong passage retrieval model using pairs of questions and answers .
The additional pretraining tasks are likely more useful only when the target training sets are small .
Although the results of DPR on WQ and TREC in the single - dataset setting are less competitive , adding more question - answer pairs helps boost the performance , achieving the new state of the art .
To compare our pipeline training approach with joint learning , we run an ablation on Natural Questions where the retriever and reader are jointly trained , following .
This approach obtains a score of 39.8 EM , which suggests that our strategy of training a strong retriever and reader in isolation can leverage effectively available supervision , while outperforming a comparable joint training approach with a simpler design ( Appendix D ) .
One thing worth noticing is that our reader does consider more passages compared to ORQA , although it is not completely clear how much more time it takes for inference .
While DPR processes up to 100 passages for each question , the reader is able to fit all of them into one batch on a single 32GB GPU , thus the latency remains almost identical to the single passage case ( around 20 ms ) .
The exact impact on throughput is harder to measure : ORQA uses 2 - 3x longer passages compared to DPR ( 288 word pieces compared to our 100 tokens ) and the computational complexity is superlinear in passage length .
We also note that we found k = 50 to be optimal for NQ , and k = 10 leads to only marginal loss in exact match accuracy ( 40.8 vs. 41.5 EM on NQ ) , which should be roughly comparable to ORQA 's 5 - passage setup .
Related Work Passage retrieval has been an important component for open-domain QA ( Voorhees , 1999 ) .
It not only effectively reduces the search space for answer extraction , but also identifies the support context for users to verify the answer .
Strong sparse vector space models like TF - IDF or BM25 have been used as the standard method applied broadly to various QA tasks ( e.g. , Chen et al. , 2017 ; Yang et al. , 2019a , b ; Nie et al. , 2019 ; Min et al. , 2019a ; Wolfson et al. , 2020 ) .
Augmenting text - based retrieval with external structured information , such as knowledge graph and Wikipedia hyperlinks , has also been explored recently ( Min et al. , 2019 b ; Asai et al. , 2020 ) .
The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis ( Deerwester et al. , 1990 ) .
Using labeled pairs of queries and documents , discriminatively trained dense encoders have become popular recently ( Yih et al. , 2011 ; Huang et al. , 2013 ; Gillick et al. , 2019 ) , with applications to cross-lingual document retrieval , ad relevance prediction , Web search and entity retrieval .
Such approaches complement the sparse vector methods as they can potentially give high similarity scores to semantically relevant text pairs , even without exact token matching .
The dense representation alone , however , is typically inferior to the sparse one .
While not the focus of this work , dense representations from pretrained models , along with cross-attention mechanisms , have also been shown effective in passage or dialogue re-ranking tasks ( Nogueira and Cho , 2019 ; Humeau et al. , 2020 ) .
Finally , a concurrent work ( Khattab and Zaharia , 2020 ) demonstrates the feasibility of full dense retrieval in IR tasks .
Instead of employing the dual-encoder framework , they introduced a late-interaction operator on top of the BERT encoders .
Dense retrieval for open-domain QA has been explored by Das et al . ( 2019 ) , who propose to retrieve relevant passages iteratively using reformulated question vectors .
As an alternative approach that skips passage retrieval , Seo et al . ( 2019 ) propose to encode candidate answer phrases as vectors and directly retrieve the answers to the input questions efficiently .
Using additional pretraining with the objective that matches surrogates of questions and relevant passages , jointly train the question encoder and reader .
Their approach outperforms the BM25 plus reader paradigm on multiple open-domain QA datasets in QA accuracy , and is further extended by REALM ( Guu et al. , 2020 ) , which includes tuning the passage encoder asynchronously by re-indexing the passages during training .
The pretraining objective has also recently been improved by Xiong et al . ( 2020 b ) .
In contrast , our model provides a simple and yet effective solution that shows stronger empirical performance , without relying on additional pretraining or complex joint training schemes .
DPR has also been used as an important module in very recent work .
For instance , extending the idea of leveraging hard negatives , Xiong et al . ( 2020a ) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training iteration .
Starting from our trained DPR model , they show that the retrieval performance can be further improved .
Recent work ( Izacard and Grave , 2020 ; Lewis et al. , 2020 b ) have also shown that DPR can be combined with generation models such as BART ( Lewis et al. , 2020a )
Conclusion
In this work , we demonstrated that dense retrieval can outperform and potentially replace the traditional sparse retrieval component in open-domain question answering .
While a simple dual-encoder approach can be made to work surprisingly well , we showed that there are some critical ingredients to training a dense retriever successfully .
Moreover , our empirical analysis and ablation studies indicate that more complex model frameworks or similarity functions do not necessarily provide additional values .
As a result of improved retrieval performance , we obtained new state - of - the - art results on multiple open-domain question answering benchmarks .
