title
ProtoQA : A Question Answering Dataset for Prototypical Common-Sense Reasoning
abstract
Given questions regarding some prototypical situation - such as Name something that people usually do before they leave the house for work ?
- a human can easily answer them via acquired experiences .
There can be multiple right answers for such questions , with some more common for a situation than others .
This paper introduces a new question answering dataset for training and evaluating common sense reasoning capabilities of artificial intelligence systems in such prototypical situations .
The training set is gathered from an existing set of questions played in a longrunning international game show - FAMILY - FEUD .
The hidden evaluation set is created by gathering answers for each question from 100 crowd-workers .
We also propose a generative evaluation task where a model has to output a ranked list of answers , ideally covering all prototypical answers for a question .
After presenting multiple competitive baseline models , we find that human performance still exceeds model scores on all evaluation metrics with a meaningful gap , supporting the challenging nature of the task .
* Equal contribution .
Introduction Humans possess the ability to implicitly reason using a wealth of common background knowledge , much of which is acquired through shared experiences .
For example , consider the question in Figure 1 -" Name something that people usually do before they leave the house for work . " .
Humans can agree about the details and characteristics of a prototypical event or situation Abelson , 1975 , 1977 ) due to commonalities in their shared lived experiences , cultural norms and expectations .
This rough agreement extends beyond an agreement on a single top response , but can be viewed ( ii )
Name a piece of equipment that you are likely to find at your office and not at home ?
Categories : printer / copier ( 37 ) , office furniture ( 15 ) , computer equipment ( 17 ) , stapler ( 11 ) files ( 10 ) , office appliances ( 5 ) , security systems ( 1 ) ( iii )
Name something that an athlete would not keep in her refrigerator .
Categories : unhealthy / fast food ( 36 ) , unhealthy drinks ( 24 ) , clothing / shoes ( 24 ) accessories ( 7 ) ( iv ) Name something that you might forget in a hotel room ?
Categories : phone ( 24 ) , toothbrush / towels ( 17 ) , clothing / shoes ( 15 ) keys ( 14 ) , purse / wallet ( 14 ) , accessories ( 8 ) , charger ( 5 ) Figure 1 : We focus on common-sense reasoning over prototypical situations when there could be many different answers but some are more common than others .
Our task is in generative style ( not multiple-choice format ) .
Answers to a question are crowd-sourced from 100 workers and are then manually clustered into categories .
To perform well , a model has to output a ranked list of answers covering multiple categories .
as a ranked list of plausible answers , as demonstrated in Figure 1 . Such sets of diverse answers represent the nature of common sense knowledge and may be useful in applications such as dialogue systems , where multiple responses are appropriate for a given context ( Zhang et al. , 2019 b ) .
We present a new question / answer dataset capturing both the plausibility of the answers and the ranking preference of each answer about such prototypical situations inspired by the long-running American game show FAMILY - FEUD , which also provides the training data for the task .
1
The game show is played by prompting participants with queries such as Name something that people usually do before they leave the house for work ( as shown in Figure 1 ) .
The answers to such questions are provided by 100 randomly selected individuals and clustered into general categories by a professional polling company .
Contestants attempt to provide an answer which matches these categories and get points according to the proportion of surveyed responses within a matched category .
For example , when we polled 100 people with the same question ( Figure 1 ) , they provided 43 answers involving showering / cleaning , 30 answers mentioning breakfast , and the remainder fell into smaller groups such as locking a door / grabbing keys , saying goodbye , and praying .
In a FAMILY - FEUD game , if two participants on a team answered " grab a shower " and " eggs and coffee " , they would receive 73 points for providing answers which matched these two large categories .
We suggest that this is an appealing paradigm for such question answering tasks where a wide range of acceptable answers exist , as it encourages both highly popular answers as well as wide coverage over the range of good answers .
We frame this task as a generative evaluation task in which a model outputs a ranked list of answers to a given question .
Each answer string is then matched to one or more clusters of reference answers for that question .
Matching an answer cluster gives the model a score equal to the cluster size .
Our evaluation metrics ( ? 3 ) reward models which provide the most common answers , while also measuring the model 's ability to provide a diverse set of answers in order to match all the answer clusters .
While such an approach can penalize a correct model prediction when it does not match an existing reference answer , we counter this issue by ( a ) gathering and clustering a large number of reference answers , and ( b ) utilizing methods of matching non-exact matches , such as WordNet ( Miller , 1995 ) and contextual language models such as RoBERTa ( Liu et al. , 2019 ) .
Generative evaluation approaches are also used in other NLP tasks such as summarization ( Radev et al. , 2003 ) and translation ( Callison - Burch et al. , 2010 ) .
We evaluate on a set of competitive baseline models - from QA models powered by large masked LMs such as BERT , to the direct prediction of answers in a language -modeling paradigm using a large GPT - 2 LM ( Radford et al. , 2018 ) , as well as GPT - 2 fine - tuned upon the training data .
While most models perform quite poorly at this challenging task , when GPT - 2 was fine-tuned using the FAMILY - FEUD training set its performance did improved drastically , although remaining significantly below the score of human-level performance .
The contributions of this paper are as follows .
1 . We introduce a large-scale QA dataset of 9.7 k questions regarding common sense knowledge of prototypical situations with 7 - 8 labeled answer categories per question , and a corresponding evaluation set of 15,400 crowdsourced human judgments over 154 unseen questions .
2 . We present methods for robust evaluation of this task to encourage models to provide diverse answers covering all plausible answer categories .
3 . We evaluate against a range of plausible baselines , showing that while large contextualized language models fine-tuned on this data can perform well at the task , a meaningful gap still exists between model and human performance , suggesting room for improvement .
2 Dataset Creation and Analysis
Training Corpus Collection
A number of fan websites exist which have transcribed FAMILY - FEUD questions and answer clusters .
We use publicly available text from two such websites to provide a training dataset on this task .
2 Well over 10,000 questions ( with answer clusters ) were collected , and a set of 9,762 questions remained after filtering , quality control , and deduplication .
That filtering included the omission of questions that were taxonomic in character rather than probing common sense knowledge , such as name a vegetable , as well as the omission of questions encoding stereotypes .
A small set of training instances which ascribe specific stereotypes or expectations to a particular group or gender - such as " name something little boys love to build models of '' were separated from the main training data set to avoid encouraging trained models to learn such biases 3 .
We note , however , that common sense questions may carry a wide range of more nuanced culturally -specific information and biases .
Studying the bias in such datasets , and natural stereotypical biases which pre-trained language models have been shown to have ( Sheng et al. , 2019 ) , would be a valuable topic of future work .
Test Corpus Collection
In order to establish a rich , open-ended answer generation task , we created new questions similar to those seen in the training set , collected 100 answers for each question 4 from the crowd- sourcing platform FigureEight 5 and manually clustered them .
Because we gathered large sets of possible answers and clustered them , the evaluation set represents rough distributions over the expected raw string answers for each question , thereby increasing the ability to recognize any way of expressing one of those answers .
We attempted to make sure that this set of new questions maintained the same domain and the same common sense reasoning seen in the training data .
In order to maintain similarity to existing questions , these questions were created by removing a set of questions from the scraped data and perturbing important aspects , making sure that the perturbations were sufficient to meaningfully change the answer set ( thus being similar to the " counterfactually augmented " permutations of Kaushik et al . ( 2019 ) ) .
For example , given an existing question of " Name something a person might forget to put on if they leave the house in a hurry . " , changes of polarity and events would derive a related question " Name something that people usually do before they leave the house for work " .
Deriving such unseen test questions was especially important to avoid the risk of having a publicly - available question be included in the training data for contextual language models ; by making new data , we can be confident that any high - performing model has not yet seen the data .
In order to control the quality of perturbed questions , the quality of each each perturbed question was scored by four experts ( criteria listed in the appendix ) , and only the top-scoring questions were used to build the evaluation set .
We then created tasks on FigureEight for each selected question to be answered by 100 workers .
To match the training data ( which is inherently grounded in US culture ) , we limited workers to US locations .
Low-quality workers were automatically detected through test questions during annotation , and the clustering pass provided a second manual quality control check .
This left us with 154 questions which we split into a test set and development set of 102 and 52 respectively .
Answer Clustering Each list of 100 raw string answers was manually clustered by two different experts familiar with the task .
Clusters were assigned separately and then compared , and a final clustering was agreed on .
6 During this clustering phase answers could be marked as invalid as well - most commonly , either due to low-quality annotations or a clear misunderstanding of a question .
In order to keep these clusters roughly similar to the granularity of answers used in the training data and to avoid low-quality evaluation we eliminated questions for which the 8 most popular clusters did not contain at least 85 of the 100 responses .
Since each set of answers was clustered twice and adjudicated , we measure the agreement with a cluster agreement metric BLANC ( Recasens and Hovy , 2011 ; Luo et al. , 2014 ) , an extension of the Rand index used to score coreference clustering .
Using this , the similarity between the clusters produced by any two annotators averaged out to a BLANC score of 83.17 , suggesting a coherent amount of agreement regarding the clustering of answers .
Analysis of the Dataset
The data presented here involves a range of different types of common sense knowledge .
To explore the distribution of different kinds of reasoning , and to test whether that distribution of reasoning varied between the publicly available data and the crowdsourced development and test set , we propose a small inventory of six types of common sense reasoning .
We are not aware of an agreed -upon typology of all commonsense reasoning types .
Categorizations of different types of commonsense reasoning exist ( LoBue and Yates , 2011 ; Boratko et al. , 2018 ) , but since each provided categorizations needed for specific tasks ( RTE and the ARC dataset , respectively ) , neither fully covered the range of commonsense types seen in the current work .
After consulting both those prior works and a separate part of the training data , we characterize the data into the following six types .
These types consist of ( 1 Boratko et al. , 2018 ) , we annotated a random sample of questions ( 25 from dev and 25 from train ) using six basic common sense reasoning categories in order to provide a simple approximation of the distribution over reasoning types contained in the data .
Table 1 illustrates examples of questions with these types , and one can see the frequency of each type used in Table 2 .
The counts shown for each dataset illustrate that while the creation methodology varied between the two resources , the kind of common sense reasoning tasks evaluated by these models is quite similar between the two corpus types .
The greatest difference to note is that the crowd-sourced data makes less use of questions regarding specific entities , which were avoided as they tended to involve fact - based world - knowledge rather than common sense reasoning .
Evaluation
We present a number of methods for evaluating system- generated answers against these sets of clus-tered answers .
In each , models are evaluated by providing a ranked list of answers in response to a question .
These answers are then compared to the set of reference answers for that question and scored based upon how similar they are to the known answers .
While one might instead convert questionanswer pairs into a multiple -choice paradigm by generating negatives , it is difficult to generate good negative examples , and the quality of a dataset can be compromised if such examples are either too easy or easily identified using biases in the negative example generation process ( Mostafazadeh et al. , 2016 ; Zellers et al. , 2018 ; Talmor et al. , 2019 ; Schwartz et al. , 2017 ; Gururangan et al. , 2018 ; Poliak et al. , 2018 ) .
We outline here our proposed method for scoring these ranked lists of predicted answers .
The dataset ground truth is a ranked list of clusters of answers , including weights ( cluster sizes ) associated with each cluster .
A first component in such an evaluation is to match each answer to an existing cluster of answers , if any cluster is acceptable .
We try both simple methods such as exact match as well as more flexible ways of matching to clusters , such as using synonyms from WordNet ( Miller , 1995 ) or a vector-based similarity method using RoBERTa ( Liu et al. , 2019 ) .
The second component in this generative evaluation is to provide an overall score for the entire ranked list of answers by mapping individual answers to answer clusters or marking them wrong .
Scoring answers against clusters alone does not take into account the ranking .
To that end , we propose two different metrics , one similar to hits@k in traditional information retrieval task and one which limits the number of incorrect answers , which is closer to how humans are typically evaluated on this task .
In each case the score reported is calculated as a percentage of the oracle score .
Both proposed methods of scoring reward models which provide a
Reward Matrix Points Max Answers @ 1 : ( 43 ) / ( 43 ) = 1.0 Max Incorrect @ 1 : diverse set of guesses to a given query and penalize models which provide many variations of the same answer .
( See figure 2 for a general idea of the steps involved . )
3.1 Matching Answers to Clusters
Exact Match
In our simplest way of matching answers to clusters , we compare each answer with the answer strings from crowd- source workers for a given cluster , returning a score of 1 if it matched any string in the cluster and returning 0 if not .
By construction , therefore , a given answer string will match at most a single cluster with this method .
WordNet Similarity Reasonable answer strings may be incorrectly marked as wrong with an exact string match , even when they are clear synonyms of a reference answer .
METEOR ( Banerjee and Lavie , 2005 ; Lavie and Denkowski , 2009 ) addressed similar issues in machine translation via stemming and synonym matching .
We take a similar approach , tokenizing a proposed answer string and comparing it to the tokenization of the answers in each answer cluster .
Since some words in WordNet are multi-word phrases ( eg. " chewing gum " ) we furthermore perform this matching on all possible partitions of the tokenization .
For each answer in an answer cluster we return the maximum ( over all possible partitions ) of the average number of matched tokens .
The assignment of answers to clusters proceeds as in the exact match case .
Further details are included in the appendix .
RoBERTa Similarity Recent works in MT evaluation ( Zhang et al. , 2019a ; Sellam et al. , 2020 ) used pre-trained language models to compare predictions to reference answers .
We implement a simple version of such vector-based comparisons , but this current task differs in that we assign each predicted answer to a particular cluster of correct answers , or decide whether to reject the answer .
As clusters vary in size and specificity we cannot determine a universal threshold for how similar a mention must be to a cluster .
Instead , we train a small classifier in L2 distance space for each answer cluster in order to decide membership in that answer cluster .
We do this by obtaining a vector representation of each answer from RoBERTa ( Liu et al. , 2019 ) , concatenating each answer with the question , and taking the mean of answer token representations .
For each cluster we train a small one - vs- all classifier over the 100 answers to that question , predicting membership in that cluster ( using gaussian process regression ( Williams and Rasmussen , 1996 ) with an RBF kernel ) .
At test time , a given answer is assigned to the highest - scoring cluster , as long as its likelihood of membership exceeds a minimum probability threshold , set at 0.1 .
Such an approach allows us to match answers to clusters while omitting answers which do not match existing clusters .
Evaluating Diverse Lists of Answers
As mentioned previously , we want to design evaluation metrics that favor models which take into account the ranking while still covering all plausible answer categories .
We first compute an alignment score between each answer in the ranked list and each of our answer clusters .
After computing the alignment scores between all pairs of answers and clusters we create a reward matrix where , for each answer and cluster , we assign a reward equal to the cluster size if the alignment score was a 1 and 0 otherwise .
We employ the Hungarian matching algorithm ( Kuhn , 1955 ; Munkres , 1957 ) to compute the exact optimal matching of answers to clusters based on this reward matrix , so that an answer is assigned to only one cluster .
It is worth noting that a model which produces a ranked list of answers only in one cluster will do worse than a model which maximally covers all plausible clusters .
Lastly , to make the comparison between lists of different lengths uniform , we propose the following metrics .
1 . MAX ANSWERS@k limits the total number of answers allowed to up to k answers .
7 2 . MAX INCORRECT@k allows unlimited answers , but stops after k unmatched answers are provided .
In both conditions , we report the score as the percentage of the max score one could receive given that number of guesses , and only give credit for a given cluster once .
Baselines
We explore three baseline models for this task : a QA - based model which retrieves related posts in a discussion forum for each question , a languagemodeling baseline which examines how well modern pre-trained language models do at directly producing the answers , and a fine-tuned version of the language -model baseline .
Question -Answering Baseline
As this dataset is in the form of questions and answers it may be treated as a QA dataset , although the content is far from the fact - based data usually modeled in QA tasks .
As the training set only shows answers out of context , one must use distant supervision in order to train a QA model on the data , a well - explored situation in modern QA work ( Joshi et al. , 2017 ) .
Unlike factoid- based QA , one may expect a limit in the performance of such QA models for common sense reasoning , as common sense data is well -known to have a reporting bias ( Gordon and Van Durme , 2013 ) wherein many facts that are part of the common ground of known knowledge are less likely to be stated .
To train a model in this approach , we collected up to 20 documents for each of the 9.7 k questions in the FAMILY - FEUD training dataset by using a web search for each question constrained to Reddit .
This resulted in a set of 85,781 Reddit posts total .
Searches were constrained to Reddit in order to focus upon advice and personal narratives which might discuss common sense questions .
For any post matching that query , any strings matching an answer to that question in the training data would be treated as a positive example for the QA model .
The QA model used was the " Bert for QA " implementation within the Hugging Face Transformers package ( Wolf et al. , 2019 ) ; training details , and examples of the kind of noisy training data generated through this process , are provided in the appendix .
At test time documents were obtained by searching for the question in a google search restricted to Reddit , and the QA model was run on that set , taking the 20 best answers in context as possible answer strings .
Those best answer strings from each passage were combined together , summing scores for identical strings , to provide a ranked list .
Language Model Baseline
We also report a language model generation baseline , due to the improved representation power of modern language models and recent evidence of their power in modeling common sense reasoning tasks ( Weir et al. , 2020 ; Tamborrino et al. , 2020 ) .
The baseline is performed using the AI2 GPT - 2 large model ( Radford et al. , 2019 ) ( specifically , the Hugging Face PyTorch implementation ( Wolf et al. , 2019 ) ) .
We perform both a zero-shot evaluation and an evaluation after fine-tuning with using our training data .
Because the original FAMILY - FEUD prompts are not structured as completion tasks , we transform the original question by hand - designed transformation rules in order for it to be compatible with the GPT - 2 training data .
E.g " Name something people do when they wake up . " ?
" One thing people do when they wake up is ... " .
The hand - designed rules are including in the appendix .
The transformed questions are used as input to the language model , and GPT - 2 finishes the sentence .
The reported finetuning result is trained on the scraped training corpus and the best model selected based on performance on our annotated development set .
Training details and parameter setting for the model is provided in the appendix .
In order to generate diverse answers for a given sentence we use Nucleus Sampling as our decoding method .
We get 300 sampled answers for each question and group them by counts , returning a ranked list of 20 answers from most to least common .
Human Performance
To measure human performance against such models , we collected 30 additional human responses per question with the same setup in collecting test data and aggregated them by counts , just as the sampled answers from GPT - 2 models were ranked .
The last column in table 3 reports this human performance .
We can see that the best-performing automatic system is still meaningfully behind human performance in all metrics .
Discussion and Analysis Table 3 shows the results of the baseline models using different measures of similarity , and different measures for the MAX ANSWERS and MAX INCORRECT metrics .
One can see that GPT - 2 without fine-tuning outperforms the baseline QA implementation , and fine- tuned GPT - 2 outperforms both , but a large gap still remains between human performance and any of the baselines , even on the generous RoBERTa - based similarity metric .
The human baseline scores are relatively stable regardless of which similarity metric is used , whereas the model scores change drastically ( most significantly for the QA model ) as more generous similarity metrics are used .
We suggest that WordNet Similarity be used as the primary similarity metric as it strikes a reasonable balance between precision and recall , as discussed in ? 5.2 .
Knowledge Base Comparison
To show the dataset indeed containing meaningful commonsense knowledge , we did an additional analysis between our dataset and ConceptNet .
Con-ceptNet ( Speer et al. , 2017 ) is a knowledge base containing triples related to common sense which has been shown to be helpful for various downstream tasks ( Zhong et al. , 2019 ; and conversational text generation ( Wu et al. , 2020 ; .
We evaluate its potential relevance to this task by evaluating how often a ( question , answer cluster ) pair has a possible matching triple within ConceptNet .
We extract a list of keywords from the question and a ground - truth answer string ( by removing stop words ) and similarly extract keywords from the head and tail of each ConceptNet relation .
We then evaluate whether a given question - answer pair has potential " coverage " in ConceptNet by checking whether a keyword in the question is related to a keyword in the answer .
For example , given the question " Besides music , name something you might hear on a morning radio show " and the answer " weather report " , we would find the triples ( listen to radio , Cause , you hear local weather report ) and ( listen to radio , Has- Subevent , hear weather report ) .
By this measure , we find that 24.3 % of the answer clusters in our development set have some match within Concept - Net .
This suggests that a common sense KB might provide a useful resource for this task , however ConceptNet has a large number of relations with no direct ability to provide a ranking and thus we exclude such a model from our baseline comparisons .
A similar analysis shows that the human baseline match 46.5 % of the clusters , whereas a list of 20 top answers from the fine- tuned GPT - 2 model match 30.3 % .
Score Function Comparison
In order to compare the various similarity functions outlined in ?
3 , we manually annotated answersfrom both the human baseline and fine- tuned GPT - 2 outputs - to the correct answer clusters .
Four annotators separately mapped each answer string to an existing cluster .
Table 4 measures how well different similarity functions performed in comparison to the manual human cluster assignment .
Precision in this context measures how often an answer assigned by the automatic similarity measure is correctly assigned ; recall measures how often an answer which a should be assigned to a cluster is correctly assigned .
Unsurprisingly , exact match has perfect precision in this context , but has relatively low recall .
WordNet similarity increases recall while adding very little false positives .
As was hoped , RoBERTa similarity does dramatically increase how often an answer is mapped to the correct cluster , but does so at the expense of a large loss in precision ; we therefore suggest that the WordNet similarity is the safest evaluation option .
Error Analysis
To provide some notion for the tendencies of different models on this task we provide actual model outputs in Table 5 .
One can see that , before finetuning , GPT - 2 results are often acceptable and plausible situations ( e.g. refrigerators might be replaced ) , but can fail to answer the specific criteria requested by the prompt .
In contrast , the QA - based model is much noisier - occasionally providing very good answers , but often ( as in the examples provided ) failing to find answers that are even plausible .
Fine- tuned GPT - 2 , in contrast to both , clearly learns to actually focus upon the expected format and details of such prototypical activities , however it fails in situations where a high-scoring answer would be very rarely discussed , such as knowing that light bulbs are commonly changed around the house .
Related Work
A wide variety of common sense reasoning datasets address related topics .
Many datasets cover physical and spatial reasoning , social common sense ( Sap et al. , 2019 b ) , and common sense understanding of plausible sequences of events ( Zellers et al. , 2018
Huang et al. , 2019 ; Sap et al. , 2019a ) or understanding of the entailments of a sentence ( Zhang et al. , 2017 ; Bowman et al. , 2015 ; Roemmele et al. , 2011 ; Levesque et al. , 2012 ) .
There is also a long history of work in modeling scripts and frames ( Schank and Abelson , 1977 ; Chambers and Jurafsky , 2009 ; Fillmore et al. , 1976 ; Ferraro and Van Durme , 2016 ; Weber et al. , 2020 ) , which is related to the current focus on prototypical situations .
Recent works have also sought to characterize the ability of pre-trained language models to understand common sense reasoning , showing such models perform well at common sense reasoning tasks even without fine-tuning , allowing one to explore the common sense reasoning inherent in those models ( Tamborrino et al. , 2020 ; Weir et al. , 2020 ) .
Of particular relevance to the current work , Weir et al . ( 2020 ) explored the ability of pre-trained models to predict stereotypic tacit assumptions , generalizing about entire classes of entities with statements such as " everyone knows that a bear has " .
Interestingly , ProtoQA is not the first time FAMILY - FEUD has been referenced in the commonsense literature .
Common Consensus ( Lieberman and et al. , 2007 ) was a web-based game created with the intention of being a self-sustaining platform to collect and validate commonsense knowledge based on human goals .
Prior work had established the idea of using online games to simultaneously entertain and collect commonsense
Prompt
Name something around the house that 's often replaced .
et al. , 2006 ) , however the authors of Common Consensus found that the format of FAMILY - FEUD questions was more amenable to high-quality commonsense knowledge acquisition .
Human Common Consensus serves as an excellent proof of concept for future gamification of the style of data presented in this dataset .
ProtoQA differs from other datasets in three different ways : 1 . ProtoQA focuses on proto-typical situations .
Humans can agree about the details and characteristics of a prototypical event or situation due to commonalities in their shared lived experiences , cultural norms and expectations .
This rough agreement extends beyond an agreement on a single top response and that 's why our task and evaluation values diversity of answers .
2 . The evaluation ProtoQA is a generative evaluation task where a model has to output a ranked list of answers , ideally covering all prototypical answers for a question .
3 . ProtoQA has a large number of annotations for each example which makes the generation evaluation possible .
Conclusion
We have presented a new common sense dataset with many novel features .
The collection of a large set of raw answer strings and further clustering of these strings facilitates a generative evaluation method , enabling actual use of trained models to answer real common sense questions .
The inclusion of counts over clusters of answers provides a very rich dataset for training and evaluation .
As shown in table 3 , existing fine- tuned state - of - theart models have a way to go before modeling the distribution of this common sense data .
In addition to the elements of this task which are appealing from a common sense modeling perspective , the inherent appeal of this task to humans opens a number of possibilities for future data collection and evaluation .
Millions of people have played phone- based games based upon this same premise 8 , and prior works have obtained valuable annotations from trivia game participants ( Rodriguez et al. , 2019 ) .
This dataset lays the foundation for larger-scale data collection which leverages people 's natural interest to encourage high-quality answers to more common sense questions .
A WordNet Similarity Function 1 . Let S be the set of synsets in WordNet , and let S( x ) be the set of synsets associated with the string x.
2 . Let SynsetSim ( X , Y ) : S ? S ? [ 0 , 1 ] be a score for synset similarity , eg. SynsetSim ( X , Y ) := 1 if X = Y , 0 otherwise .
3 . A given string may corresponse to multiple synsets .
Given two strings x and y we define SynsetsScore ( x , y ) = max{SynsetSim ( S x , S y ) : S x ? S ( x ) , S y ? S ( y ) }.
