title
ECNU at SemEval - 2017 Task 3 : Using Traditional and Deep Learning Methods to Address Community Question Answering Task
abstract
This paper describes the systems we submitted to the task 3 ( Community Question Answering ) in SemEval 2017 which contains three subtasks on english corpora , i.e .
Introduction
The purpose of Community Question Answering task in SemEval 2017 ( Nakov et al. , 2017 ) is to provide a platform for finding good answers to new questions in a community -created discussion forum , where the main task ( subtask C ) is defined as follows : given a new question and a large collection of question - comment threads created by a user community , participants are required to rank the comments that are most useful for answering the new question .
Obviously , this main task consists of two optional subtasks , i.e. , Question - Comment Similarity ( subtask A , also known as answer ranking ) , which is to re-rank comments / answers according to their relevance with respect to the question , and Question - Question Similarity ( i.e. , subtask B , also known as question retrieval ) , which is to retrieve the simi-lar questions according to their semantic similarity with respect to the original question .
More , a new subtask : Multi-Domain Duplicate Detection Subtask ( i.e. , subtask E ) which is to identify duplicate questions in StackExchange has been added to Se-mEval 2017 task 3 .
To address subtask A , we explored a traditional machine learning method which uses multiple types of features , e.g. , Word Match Features , Topic Model - based Features , and Lexical Semantic Similarity Features .
Additionally , for subtask A , we also built a Convolutional Neural Network ( CNN ) model to learn joint representation for question -comment ( Q- C ) pair .
For subtask B , we utilized the information of snippets returned from Search Engine with question subject as query , e.g. , we counted the frequency of each word in each snippets list and added the words which appear in the subject of original question and the frequency is more than 1 to the subject of related question .
Since subtask C can be regarded as a joint work of the two above-mentioned subtasks , we ranked the comments by multiplying the probability of the pair / related question ? comment0 being Good by the reciprocal rank of the related question .
As for subtask E , we did not submit the results because of the large amount of dataset .
The rest of this paper is organized as follows .
Section 2 describes our system .
Section 3 describes experimental setting .
Section 4 and 5 report results on training and test sets .
Finally , Section 6 concludes this work .
Systems Description
For subtask A , we presented two different methods i.e. , using traditional linguistic features and learning a CNN model to represent question and comment sentences .
For subtask B , besides Word Match , Topic Model based , and Lexical Semantic Similarity features , we also extracted Search Engine Extensional feature .
For subtask C , we ranked the comments by multiplying the probability of the pair / relevant question ? comment0 being Good by the reciprocal rank of the related question .
Features Engineering
All three subtasks can be regarded as an estimation task of sentence semantic measures which can be modeled by various types of features .
Besides Word Match , Topic Model Based , Lexical Semantic Similarity , and Comment Information Features used in our previous work ( Wu and Lan , 2016 ) , we also extract three types of novel features , i.e. , Meta Data Features , Google Ranking Feature , and Search Engine Extensional Features .
The details of are described as follows .
Here we took the Q- Q pair for example .
Word Matching Feature ( WM ) : Inspired by the work of ( Zhao et al. , 2015 ) , we adopt word matching feature in our system .
This feature represents the the proportions of co-occurred words that between a given sentence pair .
Given a Q- Q pair , this feature is expressed in the following nine measures : | Q 0 ? Q 1 | , |Q 0 ? Q 1 |/|Q 0 | , |Q 0 ? Q 1 |/|Q 1 | , |Q 1 ? Q 0 |/|Q 1 | , |Q 0 ? Q 1 |/|Q 0 | , |Q 0 ? Q 1 |/|Q 0 ? Q 1 | , |Q 0 ? Q 1 |/|Q 1 ? Q 0 | , |Q 0 ? Q 1 |/|Q 0 ?Q 1 | , 2 * | Q 0 ?Q 1 |/(|Q 0 |+|Q 1 | ) , where | Q 0 | and | Q 1 | are the number of the words of Q 0 and Q 1 . Topic Model based Feature ( TMB ) :
Topic model based feature has been proved beneficial for question retrieval and answer ranking tasks by the work of ( Duan et al. , 2008 ; Qin et al. , 2009 ) .
We use the GibbsLDA ++ ( Phan and Nguyen , 2007 ) Toolkit with 100,000 random sampling question and answer pairs from Qatar Living data to train the topic model .
In training and test phase , Q 0 and Q 1 are transformed into an 100 - dimensional topicbased vectors using pre-trained topic model .
After that we calculate the cosine similarity , Manhattan distance and Euclidean distance between these two vectors and regard the scores as TMB feature .
Inspired by the work of ( Filice et al. , 2016 ) , we also adopt four kinds of nonlinear kernel functions to calculate the distance between two vectors , i.e. , " polynomial " , " rbf " , " laplacian " and " sigmoid " .
Lexical Semantic Similarity Feature ( LSS ) : Inspired by ( Yih et al. , 2013a ) , we included the lexical semantic similarity feature in our model .
Two types of 300 - dimensional vectors are pretrained on Qatar Living data with word2vec ( Yih et al. , 2013 b ) and Glove ( Pennington et al. , 2014 ) toolkits .
We select the maximum , minimum and average values for each dimension of words vectors to make up a vector to represent the sentence .
After obtained the vector representation of Q 0 and Q 1 , we also calculated the nine distance measures mentioned in TMB .
Note that all above three types of features are adopted in both answer ranking and question retrieval tasks .
Search Engine Extensional Feature ( SEE ) :
We first got two lists of 10 snippets returned by search engine ( i.e. , Google , Bing ) with the subjects of original question Q 0 and related question Q 1 as query .
Then we counted the frequency of each word in each snippets list and added the words which appear in the Q 1 /Q 0 and the frequency is more than 1 to the subject of Q 0 / Q 1 . Finally , the WM features are calculated based the changed subjects of Q 0 and Q 1 . Google Ranking Feature ( GR ) :
The reciprocal rank of the related question as given by Google is regarded as one dimensional feature .
Meta Data Feature ( MD ) : Meta data is often helpful for finding good answers and question category distribution of user posted answers is an important meta data information .
There are 28 question categories in the training data , we calculate the following values as features , i.e. , the numbers of answers answered by all users in a certain category and the numbers of answers answered by a single user in all categories are normalized using max -min scaling , forming two 28 - dimensional vectors .
We also take the quality ( i.e. , Good , Po-tentiallyUseful , and Bad ) of answers into consideration .
The numbers of different quality answers answered by all users under a category and the numbers of different quality answers answered by a users in all categories are normalized using maxmin scaling , forming two 3* 28 - dimensional vectors .
Comment Information Feature ( CI ) :
We also extracted following comment information features to measure the informativeness of a comment text : ( 1 ) comment unigram feature , we constructed a vocabulary with the words appeared more than twice in the training data , generating a 9000 - dimensional vector of one - hot for -m for each comment .
( 2 ) comment ner feature , we extracted nine types of name entity information in the comment , i.e. , " Duration " , " Location " , " Person " , " Organization " , " Percent " , " Ordinal " , " Time " , " Date " , and " Money " with the CoreNLP tool , generating a nine-dimensional one - hot forming vector .
( 3 ) comment special characters feature , We extracted the following five special characters features from the comment , i.e. , email , url , " @ " , " ... " , and " ? " , generating a 5 - dimensional vector of one - hot form for every comment .
Note that MD and CI features are used in answer ranking task only .
GR and SEE features are used in question retrieval task only .
CNN to address subtask A We proposed a convolutional neural network to model question -comment sentence .
As illustrated in Figure 1 , it first takes the embeddings ( here we used 300 - dimensional Glove vectors ) ( Pennington et al. , 2014 ) of question and comment words as inputs and then summarizes the meaning of question and comment through convolution and pooling .
Finally the softmax output of Good classes is regarded as ranking score between question and comment by a simple hidden layer building on the concatenation of two feature vectors and softmax operation .
For CNN model , we set the filter numbers as 1,2,3 and 4 with same feature map of 100 and the stochastic gradient descent algorithm is used to update the parameters with learning rate of 0.001 and cross entropy as loss function .
3 Experimental Setting
Datasets
Preprocessing Firstly , we removed stop words and punctuation , and changed words to their lowercase .
After that , we performed tokenization and stemming using NLTK 1 Toolkit .
Learning Algorithm
We compared various machine learning algorithms such as Logistic Regression , Random Forest and AdaBoost implemented by SKLearn 2 with default parameters setting for their good performance in preliminary experiments .
The probabilistic scores of P erf ectM atch and Good classes returned by classifiers are regarded as ranking scores of question -question pair and questioncomment pair .
According to their performances with diverse features in three subtasks , they are used in different subtasks in our final submitted results .
Table 2 : Results of subtask A with two different methods .
" All " means to all features and " - " means to exclude some feature groups .
Experiments on Training Data
Results on Subtask B
Conclusion on Experimental results Based on above experimental results , we find that ( 1 ) For subtask A , all the features ( e.g. , WM , TMB , MD , CI and LSS ) make contribution to the improvement of performance .
The CNN based model achieves comparable performance with traditional method and with the average value of scores returned by two methods as ranking score achieves the best performance .
( 2 ) For subtask B , three algorithms such as Logistic Regression , AdaBoost and Random Forest achieve comparable results with traditional NLP features .
Specially , LR with all features achieve the best performance .
( 3 ) For subtask C , AdaBoost with all features ( excluding MD feature ) makes the best result compared with Random Forest and Logistic Regression .
