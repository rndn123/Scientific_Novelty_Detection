title
Unsupervised Question Decomposition for Question Answering
abstract
We aim to improve question answering ( QA ) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering .
Since labeling questions with decompositions is cumbersome , we take an unsupervised approach to produce sub-questions , also enabling us to leverage millions of questions from the internet .
Specifically , we propose an algorithm for One-to-N Unsupervised Sequence transduction ( ONUS ) that learns to map one hard , multi-hop question to many simpler , singlehop sub-questions .
We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer .
We show large QA improvements on HOTPOTQA over a strong baseline on the original , out -ofdomain , and multi-hop dev sets .
ONUS automatically learns to decompose different kinds of questions , while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency .
Qualitatively , we find that using subquestions is promising for shedding light on why a QA system makes a prediction .
1 * KC was a part-time research scientist at Facebook AI Research while working on this paper .
1 Our code , data , and pretrained models are available at https://github.com/facebookresearch/
Unsupervised Decomposition .
Introduction
It has been a long-standing challenge in AI to answer questions of any level of difficulty ( Winograd , 1991 ) .
Question answering ( QA ) systems struggle to answer complex questions such as " What profession do H. L. Mencken and Albert Camus have in common ? " since the required information is scattered in different places ( Yang et al. , 2018 ) .
However , QA systems accurately answer simpler , related questions such as " What profession does H. L. Mencken have ? " and " Who was Albert Camus ? " ( Petrochuk and Zettlemoyer , 2018 ) .
Thus , a promising strategy to answer hard questions is divide- and - conquer : decompose a hard question into simpler sub-questions , answer the sub-questions with a QA system , and recompose the resulting answers into a final answer , as shown in Figure 1 .
This approach leverages strong performance on simple questions to help answer harder questions ( Christiano et al. , 2018 ) .
Existing work decomposes questions using a combination of hand-crafted heuristics , rule- based algorithms , and learning from supervised decompositions ( Talmor and Berant , 2018 ; Min et al. , 2019 b ) , which each require significant human effort .
For example , DECOMPRC ( Min et al. , 2019 b ) decomposes some questions using supervision and other questions using a heuristic algorithm with fine- grained , special case handling based on part - Hard Question Simple Question
? Step 1 Step 2
Seq2Seq or Step 2 Figure 2 : One-to-N Unsupervised Sequence transduction ( ONUS ) : Step 1 : We create a corpus of pseudodecompositions D by finding candidate sub-questions from a simple question corpus S which are similar to a multi-hop question in Q. Step 2 : We learn to map multi-hop questions to decompositions using Q and D as training data , via either standard sequence - to-sequence learning ( Seq2Seq ) or unsupervised sequence - to-sequence learning ( for ONUS ) .
of -speech tags and over 50 keywords .
Prior work also assumes that sub-questions only consist of words from the question , which is not always true .
Decomposing arbitrary questions requires sophisticated natural language generation , which often relies on many , high-quality supervised examples .
Instead of using supervision , we find it possible to decompose questions in a fully unsupervised way .
We propose an algorithm for One-to-N Unsupervised Sequence transduction ( ONUS ) that learns to map from the distribution of hard questions to that of many simple questions .
First , we automatically create a noisy " pseudo- decomposition " for each hard question by using embedding similarity to retrieve sub-question candidates .
We mine over 10 M possible sub-questions from Common Crawl with a classifier , showcasing the effectiveness of parallel corpus mining , a common approach in machine translation ( Xu and Koehn , 2017 ; Artetxe and Schwenk , 2019 ) , for QA .
Second , we train a decomposition model on the mined data with unsupervised sequence - to-sequence learning , allowing ONUS to improve over pseudo-decompositions .
As a result , we are able to train a large transformer model to generate decompositions , surpassing the fluency of heuristic / extractive decompositions .
Figure 2 overviews our approach to decomposition .
We validate ONUS on multi-hop QA , where questions require reasoning over multiple pieces of evidence .
We use an off-the-shelf single - hop QA model to answer decomposed sub-questions .
Then , we give sub-questions and their answers to a recomposition model to combine into a final answer .
We evaluate on three dev sets for HOTPOTQA , a standard benchmark for multi-hop QA ( Yang et al. , 2018 ) , including two challenge sets .
ONUS proves to be a powerful tool for QA in the following ways .
First , QA models that use decompositions outperform a strong RoBERTa baseline Min et al. , 2019a ) by 3.1 points in F1 on the original dev set , 10 points on the out-of- domain dev set from Min et al . ( 2019 b ) , and 11 points on the multi-hop dev set from Jiang and Bansal ( 2019a ) .
Our method is competitive with state - of- the - art methods SAE ( Tu et al. , 2020 ) and HGN ( Fang et al. , 2019 ) that use additional , strong supervision on which sentences are relevant to the question .
Second , our analysis shows that sub-questions improve multi-hop QA by using the single - hop QA model to retrieve question - relevant text .
Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black - box , neural QA models .
Third , ONUS automatically learns to generate useful decompositions for all four question types in HOTPOTQA , highlighting the general nature of ONUS over prior work , such as IBM Watson ( Ferrucci et al. , 2010 ) and DECOMPRC ( Min et al. , 2019 b ) , which decompose different question types separately .
Without finetuning , our trained ONUS model can even decompose some questions in visual QA ( Johnson et al. , 2017 b ) and knowledge - base QA ( Talmor and Berant , 2018 ) , as well as claims in fact verification ( Thorne et al. , 2018 ) , suggesting promising future avenues in other domains .
Method
We now formulate the problem and describe our high - level approach , with further details in ?3 .
The goal of this work is to leverage a QA model that is accurate on simple questions for answering hard questions , without using annotated question decompositions .
Here , we consider simple questions to be " single-hop " questions that require reasoning over one paragraph or piece of evidence , and we consider hard questions to be " multi-hop . "
Our aim is to train a multi-hop QA model M to provide the correct answer a to a multihop question q about a given context c ( e.g. , several paragraphs ) .
Normally , we would train M to maximize log p M ( a|c , q ) .
To facilitate learn-ing , we leverage a single - hop QA model that may be queried with sub-questions s 1 , . . . , s N , whose " sub-answers " a 1 , . . . , a N may be given to M . M may then maximize the potentially easier objective log p M ( a|c , q , [ s 1 , a 1 ] , . . . , [ a N , s N ] ) .
Supervised decomposition models learn to map each question q ?
Q to a decomposition d = [ s 1 ; . . . ; s N ] of N sub-questions s n ?
S using annotated ( q , d ) examples .
In this work , we do not assume access to strong ( q , d ) supervision .
To leverage the single- hop QA model without supervision , we follow a three - stage approach : 1 ) map a question q into sub-questions s 1 , . . . , s N via unsupervised techniques , 2 ) find sub-answers a 1 , . . . , a N with the single- hop QA model , and 3 ) use s 1 , . . . , s N and a 1 , . . . , a N to predict a .
Unsupervised Question Decomposition
To train an unsupervised decomposition model , we need suitable data .
We assume access to a hard question corpus Q and simple question corpus S. Instead of using supervised ( q , d ) examples , we design an algorithm that creates pseudodecompositions d to form ( q , d ) pairs from Q and S using an unsupervised method ( ?2.1.1 ) .
We then train a model to map q to a decomposition .
We explore learning to decompose with standard and unsupervised sequence - to-sequence learning ( ?2.1.2 ) .
Creating Pseudo-Decompositions Inspired by Zhou et al. ( 2015 ) in question retrieval , we create a pseudo-decomposition set d = {s 1 ; . . . ; s N } for each q ?
Q by retrieving simple question s i from S. We concatenate s 1 ; . . . ; s N to form d used downstream .
N may potentially vary based on q .
To retrieve useful simple questions for answering q , we face a joint optimization problem .
We want sub-questions that are both ( i ) similar to q according to a metric f ( first term ) and ( ii ) maximally diverse ( second term ) , so our objective is : argmax d ?S s i ?d f ( q , s i ) ?
s i , s j ?d , i =j f ( s i , s j ) ( 1 )
Learning to Decompose
With the above pseudo-decompositions , we explore various decomposition methods ( details in ?3.2.3 ) : PseudoD
We use sub-questions from pseudodecompositions directly in downstream QA .
Sequence-to-Sequence ( Seq2Seq )
We train a Seq2Seq model p ? to maximize log p ? ( d |q ) .
One-to-N Unsupervised Sequence transduction ( ONUS )
We use unsupervised learning to map one question to N sub-questions .
We start with paired ( q , d ) but do not learn from the pairing because it is noisy .
Instead , we use unsupervised Seq2Seq methods to learn a q ? d mapping .
Answering Sub-Questions
To answer the generated sub-questions , we use an off-the-shelf QA model .
The QA model may answer sub-questions using any free-form text ( i.e. , a word , phrase , sentence , etc. ) .
Any QA model is suitable , so long as it can accurately answer simple questions in S .
We thus leverage good accuracy on questions in S to help answer questions in Q .
Learning to Recompose Downstream QA systems may use sub-questions and sub-answers in various ways .
We train a recomposition model to combine the decomposed sub-questions / answers into a final answer , when also given the original input ( context+ question ) .
Experimental Setup
We now detail the implementation of our approach .
Question Answering Task
We test ONUS on HOTPOTQA , a standard multihop QA benchmark .
Questions require information from two distinct Wikipedia paragraphs to answer ( " Who is older , Annie Morton or Terry Richardson ? " ) .
For each question , HOTPOTQA provides 10 context paragraphs from Wikipedia .
Two paragraphs contain question - relevant sentences called " supporting facts , " and the remaining paragraphs are irrelevant , " distractor paragraphs . "
Answers in HOTPOTQA are either yes , no , or a text span in an input paragraph .
Accuracy is measured with F1 word overlap and Exact Match ( EM ) between predicted and gold spans .
Unsupervised Decomposition
Training Data and Question Mining Supervised decomposition methods are limited by the amount of available human annotation , but our unsupervised method faces no such limitation , similar to unsupervised QA .
Since we need to train data-hungry Seq2Seq models , we would benefit from large training corpora .
A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard question .
Thus , we take inspiration from parallel corpus mining in machine translation ( Xu and Koehn , 2017 ; Artetxe and Schwenk , 2019 ) .
We use questions from SQUAD 2 and HOTPOTQA to form our initial corpora S ( single - hop questions ) and Q ( multi-hop questions ) , respectively , and we augment Q and S by mining more questions from Common Crawl .
First , we select sentences that start with " wh " - words or end in " ? "
Next , we train an efficient , FastText classifier ( Joulin et al. , 2017 ) to classify between questions sampled from Common Crawl , SQUAD 2 , and HOTPOTQA ( 60 K in total ) .
Then , we classify our Common Crawl questions , adding those classified as SQUAD 2 questions to S and those classified as HOTPOTQA questions to Q. Mining greatly increases the number of single - hop questions ( 130 K ? 10.1M ) and multi-hop questions ( 90 K ? 2.4M ) , showing the power of parallel corpus mining in QA .
2
Creating Pseudo-Decompositions
To create pseudo-decompositions ( retrieval - based sub-questions for a given question ) , we experimented with using a variable number of subquestions N per question ( Appendix ?A.1 ) , but we found similar QA results with a fixed N = 2 , which we use in the remainder for simplicity .
Similarity - based Retrieval
To retrieve relevant sub-questions , we embed any text t into a vector v t by summing the FastText vectors ( Bojanowski et al. , 2017 ) 3 for words in t and use cosine as our similarity metric f .
4 Let q be a multi-hop question with a pseudo-decomposition ( s * 1 , s * 2 ) and v be the unit vector of v. Since N = 2 , Eq. 1 simplifies to : ( s * 1 , s * 2 ) = argmax {s 1 , s 2 }?S v q vs 1 + v q vs 2 ? v s 1 vs 2
The last term requires O ( |S| 2 ) comparisons , which is expensive as | S| > 10M .
Instead of solving the above equation exactly , we find an approximate pseudo-decomposition ( s 1 , s 2 ) by computing over S = topK { s?S} v q vs with K = 1000 .
We efficiently build S with FAISS ( Johnson et al. , 2017a ) . Random Retrieval
For comparison , we test a random pseudo-decomposition baseline , where we retrieve s 1 , . . . , s N by sampling uniformly from S. Editing Pseudo-Decompositions
Since subquestions are retrieval - based , they are often not about the same entities as q. Inspired by retrieve - and - edit methods ( e.g. , Guu et al. , 2018 ) , we replace each sub-question entity not in q with an entity from q of the same type ( e.g. , " Date " or " Location " ) if possible .
5
This step is important for PseudoD and Seq2Seq ( which would learn to hallucinate entities ) but not ONUS ( which must reconstruct entities in q from its own decomposition , as discussed next ) .
Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods ( Artetxe et al. , 2018 ; Lample et al. , 2018 ) , so we initialize all decomposition models ( Seq2Seq or ONUS ) with the same pretrained weights .
We warm-start our pretraining with the pretrained , English Masked Language Model ( MLM ) from Lample and Conneau ( 2019 ) , a 12 - block transformer ( Vaswani et al. , 2017 ) .
We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval , using the final weights to initialize a pretrained encoder-decoder .
See Appendix ?B.2 for details .
Seq2Seq
We finetune the pretrained encoderdecoder using maximum likelihood .
We stop training based on validation BLEU between generated decompositions and pseudo-decompositions .
ONUS
We finetune the pretrained encoderdecoder with back - translation ( Sennrich et al. , 2016 ) and denoising objectives simultaneously , similar to Lample and Conneau ( 2019 ) in unsupervised one - to - one translation .
6
For denoising , we produce a noisy input d by randomly masking , dropping , and locally shuffling tokens in d ?
D , and we train a model with parameters ? to maximize log p ? ( d|d ) .
We likewise maximize log p ? ( q|q ) for a noised version q of q ?
Q. For back - translation , we generate a multihop question q for a decomposition d ?
D , and we maximize log p ? ( d| q ) .
Similarly , we maximize log p ? ( q| d ) for a model- generated decomposition d of q ?
Q. We train on HOTPOTQA questions Q and their pseudo-decompositions D. 7
Single-hop Question Answering Model
We finetune a pretrained model for single - hop QA following prior work from Min et al . ( 2019 b ) on HOTPOTQA , as described below .
8 Model Architecture
Our model takes in a question and several paragraphs to predict the answer .
We compute a separate forward pass on each paragraph ( with the question ) .
For each paragraph , the model learns to predict the answer span if the paragraph contains the answer and to predict " no answer " otherwise .
We treat yes or no predictions as spans within the passage ( prepended to each paragraph ) , as in Nie et al . ( 2019 ) on HOT - POTQA .
During inference , for the final softmax , we consider all paragraphs as a single chunk .
Similar to Clark and Gardner ( 2018 ) , we subtract a paragraph 's " no answer " logit from the logits of all spans in that paragraph , to reduce or increase span probabilities accordingly .
In other words , we compute the probability p(s p ) of each span s p in a paragraph p ? { 1 , . . . , P } using the predicted span logit l(s p ) and " no answer " paragraph logit n ( p ) with p(s p ) ? e l( sp ) ?n( p ) . ROBERTA LARGE is used as our pretrained model .
Training Data and Ensembling Similar to Min et al . ( 2019 b ) , we train an ensemble of 2 single - hop QA models on SQUAD 2 and the " easy " ( singlehop ) subset of HOTPOTQA ( see Appendix ?C for training details ) .
We average model logits before predicting the answer .
We use the single- hop QA ensemble as a black - box model once trained , never training the model on multi-hop questions .
Returned Text Instead of returning only the predicted sub-answer span to the recomposition model , we return the sentence that contains the predicted sub-answer , which is more informative .
Recomposition Model
Our recomposition model architecture is identical to the single - hop QA model , but the recomposition model also uses sub-questions and sub-answers as input .
We append each ( sub-question , sub-answer ) pair to the question with separator tokens .
We train one recomposition model on all of HOTPOTQA , also including SQUAD 2 examples used to train the single - hop QA model .
All reported error margins show the mean and std .
dev. across 5 recomposition training runs using the same decompositions .
Results on Question Answering
We compare variants of our approach that use different learning methods and different pseudodecomposition training sets .
As a baseline , we compare ROBERTA with decompositions to ROBERTA without decompositions .
We use the best hyperparameters for the baseline to train our ROBERTA models with decompositions ( see Appendix ?D.3 for hyperparameters ) .
We report results on 3 dev set versions : ( 1 ) the original version , 9 ( 2 ) the multi-hop version from Jiang and Bansal ( 2019a ) who created some distractor paragraphs adversarially to test multi-hop reasoning , and ( 3 ) the out-of- domain ( OOD ) version from Min et al . ( 2019 b ) who retrieved distractor paragraphs with the same procedure as the original version but excluded the original paragraphs .
Main Results
Table 1 shows how unsupervised decompositions affect QA .
Our ROBERTA baseline does quite well on HOTPOTQA ( 77.0 F1 ) , in line with Min et al . ( 2019a ) who achieved strong results using a BERT - based version of the model ( Devlin et al. , 2019 ) .
We achieve large gains over the ROBERTA baseline by simply adding sub-questions and sub-answers to the input .
Using decompositions from ONUS trained on FastText pseudo- decompositions , we find a gain of 3.1 F1 on the original dev set , 11 F1 on multi-hop dev , and 10 F1 on OOD dev .
ONUS decompositions even match the performance of using supervised and heuristic decompositions from DECOMPRC ( i.e. , 80.1 vs. 79.8 F1 on the original dev set ) .
Pseudo-decomposition and ONUS training both contribute to decomposition quality .
FastText pseudo-decompositions themselves provide an improvement in QA over the baseline ( e.g. , 72.0 vs. 67.1 F1 on OOD dev ) and over random pseudo-decompositions ( 70.7 F1 ) , validating our retrieval - based algorithm for creating pseudodecompositions .
Seq2Seq trained on FastText pseudo-decompositions achieves comparable gains to FastText pseudo-decompositions ( 73.0 F1 on OOD dev ) , validating the quality of pseudodecompositions as training data .
As hypothesized , ONUS improves over PseudoD and Seq2Seq by learning to align hard questions and pseudodecompositions while ignoring the noisy pairing ( 77.1 F1 on OOD dev ) .
ONUS is relatively robust to the training data used but still improves further by using FastText vs .
Random pseudodecompositions ( 77.1 vs. 76.5 F1 on OOD dev ) .
We submitted the best QA approach based on dev evaluation ( using ONUS trained on FastText pseudo-decompositions ) for hidden test evaluation .
We achieved a test F1 of 79.34 and Exact Match ( EM ) of 66.33 .
Our approach is competitive with state - of- the - art systems SAE ( Tu et al. , 2020 ) and HGN ( Fang et al. , 2019 ) , which both ( unlike us ) learn from strong , supporting - fact supervision about which sentences are relevant to the question .
Question Type Breakdown
To understand where decompositions help , we break down QA accuracy across 4 question types Figure 3 : Multi-hop QA is better when the single - hop QA model answers with the ground truth " supporting fact " sentences .
We plot mean and std .
over 5 QA runs . from Min et al . ( 2019 b ) .
" Bridge " questions ask about an entity not explicitly mentioned ( " When was Erik Watts ' father born ? " ) .
" Intersection " questions ask to find an entity that satisfies multiple separate conditions ( " Who was on CNBC and Fox News ? " ) .
" Comparison " questions ask to compare a property of two entities ( " Which is taller , Momhil Sar or K2 ? " ) .
" Single-hop " questions are answerable using single - hop shortcuts or single - paragraph reasoning ( " Where is Electric Six from ? " ) .
We split the original dev set into the 4 types using the supervised type classifier from Min et al . ( 2019 b ) . Table 2 ( left ) shows F1 scores for ROBERTA with and without decompositions across the 4 types .
ONUS decompositions improve QA across all types .
Our single decomposition model does not need to be tailored to the question type , unlike Min et al . ( 2019 b ) who use a different model per question type .
For single - hop questions , our QA approach does not require falling back to a single - hop QA model and instead learns to leverage decompositions in that case also ( 76.9 vs. 73.9 F1 ) .
Answers to Sub-Questions are Crucial
To measure the usefulness of sub-questions and sub-answers , we train the recomposition model with various , ablated inputs , as shown in Table 2 ( right ) .
Sub-answers are crucial to improving QA , as sub-questions with no answers or random answers do not help ( 76.9 vs. 77.0 F1 for the baseline ) .
Only when sub-answers are provided do we see improved QA , with or without sub-questions ( 80.1 and 80.2 F1 , respectively ) .
It is important to provide the sentence containing the predicted answer span instead of the answer span alone ( 80.1 vs. 77.8 F1 , respectively ) , though the answer span alone still improves over the baseline ( 77.0 F1 ) .
How Do Decompositions
Help ?
Decompositions help by retrieving important supporting evidence to answer questions .
Fig. 3 shows that QA improves when the sub-answer sentences are gold " supporting facts . "
We retrieve these without relying on strong , supporting fact supervision , unlike many state - of - the - art models ( Tu et al. , 2020 ; Fang et al. , 2019 ; Nie et al. , 2019 ) . 10
Example Decompositions
To illustrate how decompositions help , Table 3 shows example sub-questions from ONUS with predicted sub-answers .
Sub-questions are singlehop questions relevant to the multi-hop question .
The single- hop QA model returns relevant subanswers , sometimes despite under-specified ( Q2 , SQ 1 ) or otherwise imperfect sub-questions ( Q3 , SQ 1 ) .
The recomposition model returns an answer consistent with the sub-answers .
Furthermore , the sub-answers used for QA are in natural language , adding a level of interpretability to otherwise black - box , neural QA models .
Decompositions are largely extractive , copying from the multi -
Analysis
To better understand our system , we now analyze our pipeline by examining the model for each stage : decomposition , single - hop QA , and recomposition .
Unsupervised Decomposition Model Intrinsic Evaluation of Decompositions
We evaluate the quality of decompositions on other metrics aside from downstream QA .
To measure the fluency of decompositions , we compute the likelihood of decompositions using the pretrained GPT - 2 language model ( Radford et al. , 2019 ) .
We train a BERT BASE classifier on the questionwellformedness dataset of Faruqui and Das ( 2018 ) , and we use the classifier to estimate the proportion of sub-questions that are well -formed .
We measure how abstractive decompositions are by computing ( i ) the token Levenstein distance between the multihop question and its generated decomposition and ( ii ) the ratio between the length of the decomposition and the length of the multi-hop question .
We compare ONUS to DECOMPRC ( Min et al. , 2019 b ) , a supervised + heuristic decomposition method .
As shown in Table 4 , ONUS decompositions are more natural and well -formed than DECOMPRC decompositions .
As an example , for Table 3 Q3 , DECOMPRC produces the sub-questions " Is Coldplay from which country ? " and " Is Pierre Bouvier from which country ? "
ONUS decompositions are also closer in edit distance and length to the multihop question , consistent with our observation that our decomposition model is largely extractive .
Quality of Decomposition Model A welltrained decomposition model should place higher probability on decompositions that are more helpful for QA .
We generate N = 5 hypotheses from our best decomposition model using beam search , and we train a recomposition model to use the n th - ranked hypothesis as a question decomposition ( Figure 4 , left ) .
QA accuracy decreases as we use lower probability decompositions , but accuracy remains relatively robust , at most decreasing from 80.1 to 79.3 F1 .
The limited drop suggests that decompositions are still useful if they are among the model 's top hypotheses , another indication that ONUS is trained well for decomposition .
Single-hop Question Answering Model Sub-Answer Confidence Figure 4 ( right ) shows that the single - hop model 's sub-answer confidence correlates with downstream multi-hop QA accuracy on all dev sets .
A low confidence sub-answer may be indicative of ( i ) an unanswerable or ill-formed sub-question or ( ii ) a sub-answer that is more likely to be incorrect .
In both cases , the single - hop QA model is less likely to retrieve useful supporting evidence for answering the multi-hop question .
Changing the Single-hop QA Model
We find that our approach is robust to the single - hop QA model used .
We test the BERT BASE ensemble from Min et al . ( 2019 b ) HOTPOTQA itself ( 56.3 vs. 66.7 F1 ) .
However , the model results in similar QA when used to answer single - hop sub-questions within our larger system ( 79.9 vs. 80.1 F1 for our ensemble ) .
Recomposition Model Varying the Base Model
To understand how decompositions impact performance as the recomposition model gets stronger , we vary the base pretrained model .
Related Work Answering complex questions has been a longstanding challenge in natural language processing .
Prior work explored decomposing questions with supervision and heuristic algorithms .
IBM Watson ( Ferrucci et al. , 2010 ) decomposes questions into sub-questions in multiple ways or not at all .
DECOMPRC
( Min et al. , 2019 b ) largely frames subquestions as extractive spans of a question , learning to predict span-based sub-questions via supervised learning on human annotations .
In other cases , DE -COMPRC decomposes a multi-hop question using a heuristic algorithm or not at all .
Watson and DE-COMPRC use special case handling to decompose different questions , while our algorithm is fully automated and requires little hand -engineering .
More traditional , semantic parsing methods map questions to compositional programs , whose subprograms can be viewed as question decompositions in a formal language ( Talmor and Berant , 2018 ; Wolfson et al. , 2020 ) .
Examples include classical QA systems like SHRDLU ( Winograd , 1972 ) and LUNAR ( Woods et al. , 1974 ) , as well as neural Seq2Seq semantic parsers ( Dong and Lapata , 2016 ) and neural module networks ( Andreas et al. , 2015 ( Andreas et al. , , 2016 .
Such methods usually require strong , program-level supervision to generate programs , as in visual QA ( Johnson et al. , 2017 c ) and on HOTPOTQA ( Jiang and Bansal , 2019 b ) .
Some models use other forms of strong supervision , e.g. , the sentences needed to answer a question , as annotated by HOTPOTQA .
Such an approach is taken by SAE ( Tu et al. , 2020 ) and HGN ( Fang et al. , 2019 ) , whose methods may be combined with ours .
Unsupervised decomposition complements strongly and weakly supervised decomposition approaches .
Our unsupervised approach enables methods to leverage millions of otherwise unusable questions , similar to work on unsupervised QA .
When decomposition examples exist , supervised and unsupervised learning can be used in tandem to learn from both labeled and unlabeled examples .
Such semi-supervised methods outperform supervised learning for tasks like machine translation ( Sennrich et al. , 2016 ) .
Other work on weakly supervised question generation uses a downstream QA model 's accuracy as a signal for learning to generate useful questions .
Weakly supervised question generation often uses reinforcement learning ( Nogueira and Cho , 2017 ; Wang and Lake , 2019 ; Strub et al. , 2017 ; Das et al. , 2017 ; , where an unsupervised initialization can greatly mitigate the issues of exploring from scratch ( Jaderberg et al. , 2017 ) .
Conclusion
We proposed a QA system that answers a question via decomposition , without supervised question decompositions , using three stages : ( 1 ) decompose a question into many sub-questions using One-to -N Unsupervised Sequence transduction ( ONUS ) , ( 2 ) answer sub-questions with an off-the-shelf QA system , and ( 3 ) recompose sub-answers into a final answer .
When evaluated on three HOTPOTQA dev sets , our approach significantly improved QA over an equivalent model that did not use decompositions .
Our approach relies only on the final answer as supervision but works as effectively as state- ofthe - art methods that rely on much stronger supervision , such as supporting fact labels or example decompositions .
We found that ONUS generates fluent sub-questions whose answers often match the gold - annotated , question - relevant text .
Overall , this work opens up exciting avenues for leveraging methods in unsupervised learning and natural language generation to improve the interpretability and generalization of machine learning systems .
