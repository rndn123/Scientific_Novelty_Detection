title
Universal Dependency Parsing with a General Transition - Based DAG Parser
abstract
This paper presents our experiments with applying TUPA to the CoNLL 2018 UD shared task .
TUPA is a general neural transition - based DAG parser , which we use to present the first experiments on recovering enhanced dependencies as part of the general parsing task .
TUPA was designed for parsing UCCA , a crosslinguistic semantic annotation scheme , exhibiting reentrancy , discontinuity and nonterminal nodes .
By converting UD trees and graphs to a UCCA - like DAG format , we train TUPA almost without modification on the UD parsing task .
The generic nature of our approach lends itself naturally to multitask learning .
Our code is available at https://github.
com/CoNLL -UD-2018/HUJI .
Introduction
In this paper , we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing ( Zeman et al. , 2018 ) .
We focus only on parsing , using the baseline system , UDPipe 1.2 ( Straka et al. , 2016 ; Straka and Strakov? , 2017 ) for tokenization , sentence splitting , part- of-speech tagging and morphological tagging .
Our system is based on TUPA ( Hershcovich et al. , 2017 ( Hershcovich et al. , , 2018 , see ?3 ) , a transition - based UCCA parser .
UCCA ( Universal Conceptual Cognitive Annotation ; Abend and Rappoport , 2013 ) is a cross-linguistic semantic annotation scheme , representing events , participants , attributes and relations in a directed acyclic graph ( DAG ) structure .
UCCA allows reentrancy to support argument sharing , discontinuity ( corresponding to non-projectivity in dependency formalisms ) and non-terminal nodes ( as opposed to dependencies , which are bi-lexical ) .
To parse Universal Dependencies ( Nivre et al. , 2016 ) using TUPA , we employ a bidirectional conversion protocol to represent UD trees and graphs in a UCCA - like unified DAG format ( ?2 ) .
Enhanced dependencies .
Our method treats enhanced dependencies 1 as part of the dependency graph , providing the first approach , to our knowledge , for supervised learning of enhanced UD parsing .
Due to the scarcity of enhanced dependencies in UD treebanks , previous approaches ( Schuster and Manning , 2016 ; Reddy et al. , 2017 ) have attempted to recover them using languagespecific rules .
Our approach attempts to learn them from data : while only a few UD treebanks contain any enhanced dependencies , similar structures are an integral part of UCCA and its annotated corpora ( realized as reentrancy by remote edges ; see ?2 ) , and TUPA supports them as a standard feature .
As their annotation in UD is not yet widespread and standardized , enhanced dependencies are not included in the evaluation metrics for UD parsing , and so TUPA 's ability to parse them is not reflected in the official shared task scores .
However , we believe these enhancements , representing case information , elided predicates , and shared arguments due to conjunction , control , raising and relative clauses , provide richer information to downstream semantic applications , making UD better suited for text understanding .
We propose an evaluation metric specific to enhanced dependencies , enhanced LAS ( ?5.1 ) , and use it to evaluate our method .
Unified DAG Format
To apply TUPA to UD parsing , we convert UD trees and graphs into a unified DAG format ( Hershcovich et al. , 2018 ) .
The format consists of a rooted DAG , where the tokens are the terminal nodes .
2 Edges are labeled ( but not nodes ) , and are divided into primary and remote edges , where the primary edges form a tree ( all nodes have at most one primary parent , and the root has none ) .
Remote edges ( denoted as dashed edges in Figure 1 ) enable reentrancy , and thus form a DAG together with primary edges .
Figure 1 shows an example UCCA graph , and a UD graph ( containing two enhanced dependencies ) before and after conversion .
Both annotate the same sentence from the English Web Treebank ( Silveira et al. , 2014 ) 3 . Conversion protocol .
To convert UD into the unified DAG format , we add a pre-terminal for each token , and attach the pre-terminals according to the original dependency edges : traversing the tree from the root down , for each head token we create a non-terminal parent with the edge label head , and add the node 's dependents as children of the created non-terminal node ( see Figure 1 c ) .
This creates a constituency - like structure , which is supported by TUPA 's transition set ( see ?3.1 ) .
Although the enhanced dependency graph is not necessarily a supergraph of the basic dependency tree , the graph we convert to the unified DAG format is their union : any enhanced dependnecies that are distinct from the basic dependency of a node ( by having a different head or universal dependency relation ) are converted to remote edges in the unified DAG format .
To convert graphs in the unified DAG format back into dependency graphs , we collapse all head edges , determining for each terminal what is the highest non-terminal headed by it , and then attaching the terminals to each other according to the edges among their headed non-terminals .
Input format .
Enhanced dependencies are encoded in the 9th column of the CoNLL -U format , by an additional head index , followed by a colon and dependency relation .
Multiple enhanced dependencies for the same node are separated by pipes .
Figure 2 demonstrates this format .
Note that if the basic dependency is repeated in the enhanced graph ( 3:nsubj : pass in the example ) , we do not treat it as an enhanced dependency , so that the converted graph will only contain each edge once .
In addition to the UD relations defined in the basic representations , enhanced dependencies may contain the relation ref , used for relative clauses .
In addition , they may contain more specific relation subtypes , and optionally also case information .
Language -specific extensions and case information .
Dependencies may contain language - 1 We we PRON PRP Case=Nom|Number=Plur|Person=1 | PronType=Prs 3 nsubj:pass 3:nsubj:pass|5:nsubj:xsubj|7:nsubj:xsubj
_ specific relation subtypes , encoded as a suffix separated from the universal relation by a colon .
These extensions are ignored by the parsing evaluation metrics , so for example , the subtyped relation nsubj : pass ( Figure 1 b ) is considered the same as the universal relation nsubj for evaluation purposes .
In the enhanced dependencies , these suffixes may also contain case information , which may be represented by the lemma of an adposition .
For example , the " peace " ? " earth " dependency in Figure 4 is augmented as nmod :on in the enhanced graph ( not shown in the figure because it shares the universal relation with the basic dependency ) .
In the conversion process , we strip any language -specific extensions from both basic and enhanced dependencies , leaving only the universal relations .
Consequently , case information that might be encoded in the enhanced dependencies is lost , and we do not handle it in our current system .
Ellipsis and null nodes .
In addition to enhanced dependencies , the enhanced UD representation adds null nodes to represented elided predicates .
These , too , are ignored in the standard evaluation .
An example is shown in Figure 4 , where an elided " wish " is represented by the node E9.1 .
The elided predicate 's dependents are attached to its argument " peace " in the basic representation , and the argument itself is attached as an orphan .
In the enhanced representation , all arguments are attached to the null node as if the elided predicate was present .
While UCCA supports empty nodes without surface realization in the form of implicit units , previous work on UCCA parsing has removed these from the graphs .
We do the same for UD parsing , dropping null nodes and their associated dependencies upon conversion to the unified DAG format .
We leave parsing elided predicates for future work .
Propagation of conjuncts .
Enhanced dependencies contain dependencies between conjoined predicates and their arguments , and between predicates and their conjoined arguments or modifiers .
While these relations can often be inferred from the basic dependencies , in many cases they require semantic knowledge to parse correctly .
For example , in Figure 3 , the enhanced dependencies represent the shared subject ( " he " ) among the conjoined predicates ( " went " and " finished " ) , and the conjoined modifiers ( " efficiently " and " promptly " ) for the second predicate ( " finished " ) .
However , there are no enhanced dependencies between the first predicate and the second predicate 's modifiers ( e.g. " went " ? " efficiently " ) , as semantically only the subject is shared and not the modifiers .
Relative clauses .
Finally , enhanced graphs attach predicates of relative clauses directly to the antecedent modified by the relative clause , adding a ref dependency between the antecedent and the relative pronoun .
An example is shown in Fig- ure 5a .
While these graphs may contain cycles ( " robe " ? " made " in the example ) , they are removed upon conversion to the unified DAG format by the introduction of non-terminal nodes ( see Figure 5 b ) .
General Transition - based DAG Parser
We now turn to describing TUPA ( Hershcovich et al. , 2017 ( Hershcovich et al. , , 2018 , a general transition - based parser ( Nivre , 2003 ) .
TUPA uses an extended set of transitions and features that supports reentrancies , discontinuities and non-terminal nodes .
The parser state is composed of a buffer B of tokens
He had a robe that was made back in the ' 60s . and nodes to be processed , a stack S of nodes currently being processed , and a graph G = ( V , E , ) of constructed nodes and edges , where V is the set of nodes , E is the set of edges , and : E ?
L is the label function , L being the set of possible labels .
Some states are marked as terminal , meaning that G is the final output .
A classifier is used at each step to select the next transition based on features encoding the parser 's current state .
During training , an oracle creates training instances for the classifier , based on gold -standard annotations .
Transition Set Given a sequence of tokens w 1 , . . . , w n , we predict a rooted graph G whose terminals are the tokens .
Parsing starts with the root node on the stack , and the input tokens in the buffer .
The TUPA transition set , shown in Figure 6 , includes the standard SHIFT and REDUCE operations , NODE X for creating a new non-terminal node and an X-labeled edge , LEFT - EDGE X and RIGHT -EDGE X to create a new primary X-labeled edge , LEFT -REMOTE X and RIGHT -REMOTE X to create a new remote X-labeled edge , SWAP to handle discontinuous nodes , and FINISH to mark the state as terminal .
The REMOTE X transitions are not required for parsing trees , but as we treat the problem as general DAG parsing due to the inclusion of enhanced dependencies , we include these transitions .
Transition Classifier
To predict the next transition at each step , TUPA uses a BiLSTM with feature embeddings as inputs , followed by an MLP and a softmax layer for classification .
The model is illustrated in Figure 7 .
Inference is performed greedily , and training is done with an oracle that yields the set of all optimal transitions at a given state ( those that lead to a state from which the gold graph is still reachable ) .
Out of this set , the actual transition performed in training is the one with the highest score given by the classifier , which is trained to maximize the sum of log-likelihoods of all optimal transitions at each step .
S x | B V E SHIFT S | x B V E ? S | x B V E REDUCE S B V E ? S | x B V E NODE X S | x y | B V ? {y } E ? {( y , x ) X } ?
x = root S | y , x B V E LEFT -EDGE X S | y , x B V E ? {( x , y ) X } ? ? ? ? x ? w 1:n , y = root , y ; G x S | x , y B V E RIGHT-EDGE X S | x , y B V E ? {( x , y ) X } ?
S | y , x B V E LEFT -REMOTE X S | y , x B V E ? {( x , y ) * X } ? S | x , y B V E RIGHT-REMOTE X S | x , y B V E ? {( x , y ) * X } ? S | x , y B V E SWAP S | y x | B V E ? i( x ) < i( y ) [ root ] ? V E FINISH ? ? V E + Figure 6 : The transition set of TUPA .
We write the stack with its top to the right and the buffer with its head to the left . ( ? , ?) X denotes a primary X-labeled edge , and ( ? , ? ) * X a remote X-labeled edge . i( x ) is the swap index ( see ?3.3 ) .
In addition to the specified conditions , the prospective child in an EDGE transition must not already have a primary parent .
To the feature embeddings , we concatenate numeric features representing the node height , number of ( remote ) parents and children , and the ratio between the number of terminals to total number of nodes in the graph G. Table 1 lists all feature used for the classifier .
Numeric features are taken as they are , whereas categorical features are mapped to real-valued embedding vectors .
For each non-terminal node , we select a head terminal for feature extraction , by traversing down the graph according to a priority order on edge labels ( otherwise selecting the leftmost child ) .
The priority order is : parataxis , conj , advcl , xcomp
Constraints
During training and parsing , we apply constraints on the parser state to limit the possible transitions to valid ones .
A generic constraint implemented in TUPA is that stack nodes that have been swapped should not be swapped again ( Hershcovich et al. , 2018 ) .
To implement this constraint , we define a swap index for each node , assigned when the node is created .
At initialization , only the root node and terminals exist .
We assign the root a swap index of 0 , and for each terminal , its position in the text ( starting at 1 ) .
Whenever a node is created as a result ? : one- character prefix .
$ : three - character suffix .
x ? y refers to the existing edge from x to y.
x is an indicator feature , taking the value of 1 if the edge exists or 0 otherwise , e refers to the edge label , and ai to the transition taken i + 1 steps ago .
A refers to the action type ( e.g. SHIFT / RIGHT -EDGE / NODE ) , and e to the edge label created by the action .
node ratio is the ratio between non-terminals and terminals ( Hershcovich et al. , 2017 ) . of a NODE transition , its swap index is the arithmetic mean of the swap indices of the stack top and buffer head .
In addition , we enforce UD - specific constraints , resulting from the nature of the converted DAG format : every non-terminal node must have a single outgoing head edge : once it has one , it may not get another , and until it does , the node may not be reduced .
Training details
The model is implemented using DyNet v2.0.3 ( Neubig et al. , 2017 ) .
6 Unless otherwise noted , we use the default values provided by the package .
We use the same hyperparameters as used in previous experiments on UCCA parsing ( Hershcovich et al. , 2018 )
Hyperparameters
We use dropout ( Srivastava et al. , 2014 ) between MLP layers , and recurrent dropout ( Gal and Ghahramani , 2016 ) between BiLSTM layers , both with p = 0.4 .
We also use word , lemma , coarseand fine - grained POS tag dropout with ? = 0.2 ( Kiperwasser and Goldberg , 2016 ) : in training , the embedding for a feature value w is replaced with a zero vector with a probability of ? # ( w ) + ? , where # ( w ) is the number of occurrences of w observed .
In addition , we use node dropout ( Hershcovich et al. , 2018 ) : with a probability of 0.1 at each step , all features associated with a single node in the parser state are replaced with zero vectors .
For optimization we use a minibatch size of 100 , decaying all weights by 10 ?5 at each update , and train with stochastic gradient descent for 50 epochs with a learning rate of 0.1 , followed by AMSGrad ( Sashank J. Reddi , 2018 ) for 250 epochs with ? = 0.001 , ? 1 = 0.9 and ? 2 = 0.999 .
We found this training strategy better than using only one of the optimization methods , similar to findings by Keskar and Socher ( 2017 ) .
We select the epoch with the best LAS - F1 on the development set .
Other hyperparameter settings are listed in Table 2 .
Small Treebanks
For corpora with less than 100 training sentences , we use 750 epochs of AMSGrad instead of 250 .
For corpora with no development set , we use 10 - fold cross-validation on the training set , each time splitting it to 80 % training , 10 % development and 10 % validation .
We perform the normal training procedure on the training and development subsets , and then select the model from the fold with the best LAS - F1 on the corresponding validation set .
Multilingual Model
For the purpose of parsing languages with no training data , we use a delexicalized multilingual model , trained on the shuffled training sets from all corpora , with no word , lemma , fine - grained tag , prefix and suffix features .
We train this model for two epochs using stochastic gradient descent with a learning rate of 0.1 ( we only trained this many epochs due to time constraints ) .
Out-of-domain Evaluation
For test treebanks without corresponding training data , but with training data in the same language , during testing we use the model trained on the largest training treebank in the same language .
Results Official evaluation was done on the TIRA online platform ( Potthast et al. , 2014 ) .
Our system ( named " HUJI " ) ranked 24th in the LAS - F1 ranking ( with an average of 53.69 over all test treebanks ) , 23rd by MLAS ( average of 44.6 ) and 21st by BLEX ( average of 48.05 ) .
Since our system only performs dependency parsing and not other pipeline tasks , we henceforth focus on LAS - F1 ( Nivre and Fang , 2017 ) for evaluation .
After the official evaluation period ended , we discovered several bugs in the conversion between the CoNLL -U format and the unified DAG format , which is used by TUPA for training and is output by it ( see ?2 ) .
We did not re-train TUPA on the training treebanks after fixing these bugs , but we did re-evaluate the already trained models on all test treebanks , and used the fixed code for converting their output to CoNLL -U .
This yielded an unofficial average test LAS - F1 of 58.48 , an improvement of 4.79 points over the official average score .
In particular , for two test sets , ar_padt and gl_ctg , TUPA got a zero score in the official evaluation due to a bug with the treatment of multi-token words .
These went up to 61.9 and 71.42 , respectively .
We also evaluated the trained TUPA models on all available development treebanks after fixing the bugs .
Table 3 presents the averaged scores on the shared task test sets , and Figure 8 the ( official and unofficial ) LAS - F1 scores obtained by TUPA on each of the test and development treebanks .
Evaluation on Enhanced Dependencies
Since the official evaluation ignores enhanced dependencies , we evaluate them separately using a modified version of the shared task evaluation script 7 .
We calculate the enhanced LAS , identical to the standard LAS except that the set of dependencies in both gold and predicted graphs are the enhanced dependencies instead of the basic dependencies : ignoring null nodes and any enhanced dependency sharing a head with a basic one , we align the words in the gold graph and the system 's graph as in the standard LAS , and define P = # correct # system , R = # correct # gold , F 1 = 2 ?
P ? R P + R .
Table 4 lists the enhanced LAS precision , recall and F1 score on the test treebanks with any enhanced dependencies , as well as the percentage of enhanced dependencies in each test treebank , calculated as 100 ? # enhanced # enhanced +# words .
Just as remote edges in UCCA parsing are more challenging than primary edges ( Hershcovich et al. , 2017 ) , parsing enhanced dependencies is a harder task than standard UD parsing , as the scores demonstrate .
However , TUPA learns them successfully , getting as much as 56.55 enhanced LAS - F1 ( on the Polish LFG test set ) .
Ablation Experiments
The TUPA transition classifier for some of the languages uses named entity features calculated by spaCy .
8 For German , Spanish , Portuguese , French , Italian , Dutch and Russian , the spaCy named entity recognizer was trained on Wikipedia ( Nothman et al. , 2013 ) .
However , the English model was trained on OntoNotes 9 , which is in fact not among the additional resources allowed by the shared task organizers .
To get a fair evaluation and to quantify the contribution of the NER features , we re-trained TUPA on the English EWT ( en_ewt ) training set with the same hyperparameters as in our submitted model , just without these features .
As Table 5 shows , removing the NER features ( ? NER ) only slightly hurts the performance , by 0.28 LAS - F1 points on the test treebank , and 0.63 on the development treebank .
As further ablation experiments , we tried removing POS features , pre-trained word embeddings , and remote edges ( the latter enabling TUPA to parse enhanced dependencies ) .
Removing the POS features does hurt performance to a larger degree , by 2.87 LAS - F1 points on the test set , while removing the pre-trained word embeddings even slightly improves the performance .
Removing remote edges and transitions from TUPA causes a very small decrease in LAS - F1 , and of course enhanced dependencies can then no longer be produced at all .
8 https://spacy.io/api/annotation 9 https://catalog.ldc.upenn.edu/ LDC2013T19
LAS - F1 Enhanced LAS -F1
Conclusion
We have presented the HUJI submission to the CoNLL 2018 shared task on parsing Universal Dependencies , based on TUPA , a general transitionbased DAG parser .
Using a simple conversion protocol to convert UD into a unified DAG format , training TUPA as - is on the UD treebanks yields results close to the UDPipe baseline for most treebanks in the standard evaluation .
While other systems ignore enhanced dependencies , TUPA learns to produce them too as part of the general dependency parsing process .
We believe that with hyperparameter tuning and more careful handling of cross-lingual and cross-domain parsing , TUPA can be competitive on the standard metrics too .
Furthermore , the generic nature of our parser , which supports many representation schemes , as well as domains and languages , will allow improving performance by multitask learning ( cf. Hershcovich et al. , 2018 ) , which we plan to explore in future work .
UD graph after conversion to unified DAG format .
