title
CRF Autoencoder for Unsupervised Dependency Parsing *
abstract
Unsupervised dependency parsing , which tries to discover linguistic dependency structures from unannotated data , is a very challenging task .
Almost all previous work on this task focuses on learning generative models .
In this paper , we develop an unsupervised dependency parsing model based on the CRF autoencoder .
The encoder part of our model is discriminative and globally normalized which allows us to use rich features as well as universal linguistic priors .
We propose an exact algorithm for parsing as well as a tractable learning algorithm .
We evaluated the performance of our model on eight multilingual treebanks and found that our model achieved comparable performance with state - of - the - art approaches .
Introduction Unsupervised dependency parsing , which aims to discover syntactic structures in sentences from unlabeled data , is a very challenging task in natural language processing .
Most of the previous work on unsupervised dependency parsing is based on generative models such as the dependency model with valence ( DMV ) introduced by Klein and Manning ( 2004 ) .
Many approaches have been proposed to enhance these generative models , for example , by designing advanced Bayesian priors ( Cohen et al. , 2008 ) , representing dependencies with features ( Berg- Kirkpatrick et al. , 2010 ) , and representing discrete tokens with continuous vectors ( Jiang et al. , 2016 ) .
Besides generative approaches , Grave and Elhadad ( 2015 ) proposed an unsupervised discrim-inative parser .
They designed a convex quadratic objective function under the discriminative clustering framework .
By utilizing global features and linguistic priors , their approach achieves stateof - the - art performance .
However , their approach uses an approximate parsing algorithm , which has no theoretical guarantee .
In addition , the performance of the approach depends on a set of manually specified linguistic priors .
Conditional random field autoencoder ( Ammar et al. , 2014 ) is a new framework for unsupervised structured prediction .
There are two components of this model : an encoder and a decoder .
The encoder is a globally normalized feature - rich CRF model predicting the conditional distribution of the latent structure given the observed structured input .
The decoder of the model is a generative model generating a transformation of the structured input from the latent structure .
Ammar et al. ( 2014 ) applied the model to two sequential structured prediction tasks , part- of-speech induction and word alignment and showed that by utilizing context information the model can achieve better performance than previous generative models and locally normalized models .
However , to the best of our knowledge , there is no previous work applying the CRF autoencoder to tasks with more complicated outputs such as tree structures .
In this paper , we propose an unsupervised discriminative dependency parser based on the CRF autoencoder framework and provide tractable algorithms for learning and parsing .
We performed experiments in eight languages and show that our approach achieves comparable results with previous state - of - the - art models .
Figure 1 : The CRF Autoencoder for the input sentence " These stocks eventually reopened " and its corresponding parse tree ( shown at the top ) .
x and x are the original and reconstructed sentence .
y is the dependency parse tree represented by a sequence where y i contains the token and index of the parent of token x i in the parse tree , e.g. , y 1 = stocks , 2 and y 2 = reopened , 4 .
The encoder is represented by a factor graph ( with a global factor specifying valid parse trees ) and the decoder is represented by a Bayesian net .
Method
Model Figure 1 shows our model with an example input sentence .
Given an input sentence x = ( x 1 , x 2 , . . . , x n ) , we regard its parse tree as the latent structure represented by a sequence y = ( y 1 , y 2 , . . . , y n ) where y i is a pair t i , h i , t i is the head token of the dependency connecting to token x i in the parse tree , and h i is the index of this head token in the sentence .
The model also contains a reconstruction output , which is a token sequence x = ( x 1 , x2 , . . . , xn ) .
Throughout this paper , we set x = x .
The encoder in our model is a log-linear model represented by a first-order dependency parser .
The score of a dependency tree can be factorized as the sum of scores of its dependencies .
For each dependency arc ( x , i , j ) , where i and j are the indices of the head and child of the dependency , a feature vector f ( x , i , j ) is specified .
The score of a dependency is defined as the inner product of the feature vector and a weight vector w , ?( x , i , j ) = w T f ( x , i , j)
The score of a dependency tree y of sentence x is ?( x , y ) = n i=1 ?( x , h i , i )
We define the probability of parse tree y given sentence x as P ( y|x ) = exp ( ?( x , y ) ) Z( x ) Z( x ) is the partition function , Z( x ) = y ?Y( x ) exp (?( x , y ) ) where Y( x ) is the set of all valid parse trees of x .
The partition function can be efficiently computed in O(n 3 ) time using a variant of the inside-outside algorithm ( Paskin , 2001 ) for projective tree structures , or using the Matrix - Tree Theorem for nonprojective tree structures ( Koo et al. , 2007 ) .
The decoder of our model consists of a set of categorical conditional distributions ?
x|t , which represents the probability of generating token x conditioned on token t.
So the probability of the reconstruction output x given the parse tree y is P ( x|y ) = n i=1 ? xi |t i
The conditional distribution of x , y given x is P ( y , x|x ) = P ( y|x ) P ( x|y )
Features Following McDonald et al. ( 2005 ) and Grave et al . ( 2015 ) , we define the feature vector of a dependency based on the part- of-speech tags ( POS ) of the head , child and context words , the direction , and the distance between the head and child of the dependency .
The feature template used in our parser is shown in Table 1 .
Parsing Given parameters w and ? , we can parse a sentence x by searching for a dependency tree y which has the highest probability P ( x , y|x ) .
y * = arg max y?Y ( x ) log P ( x , y|x ) = arg max y?Y ( x ) n i=1 ?( x , h i , i ) + log ?
xi |t i POS i ? dis ? dir POS j ? dis ? dir POS i ?
POS j ? dis ? dir POS i ?
POS i?1 ? POS j ? dis ? dir POS i ?
POS i+1 ? POS j ? dis ? dir POS i ?
POS j ? POS j?1 ? dis ? dir POS i ?
POS j ? POS j+1 ? dis ? dir Table 1 : Feature template of a dependency , where i is the index of the head , j is the index of the child , dis = | i ? j| , and dir is the direction of the dependency .
For projective dependency parsing , we can use Eisners algorithm ( 1996 ) to find the best parse in O(n 3 ) time .
For non-projective dependency parsing , we can use the Chu-Liu / Edmond algorithm ( Chu and Liu , 1965 ; Edmonds , 1967 ; Tarjan , 1977 ) to find the best parse in O( n 2 ) time .
et al. ( 2010 ) shows that Viterbi EM can improve the performance of unsupervised dependency parsing in comparison with EM .
Therefore , instead of using negative conditional log likelihood as our objective function , we choose to use negative conditional Viterbi log likelihood ,
Parameter Learning
Objective Function
Spitkovsky ?
N i=1 log max y?Y ( x i ) P ( x i , y|x i ) + ?( w ) ( 1 ) where ?( w ) is a L1 regularization term of the encoder parameter w and ? is a hyper-parameter controlling the strength of regularization .
To encourage learning of dependency relations that satisfy universal linguistic knowledge , we add a soft constraint on the parse tree based on the universal syntactic rules following Naseem et al . ( 2010 ) and Grave et al . ( 2015 ) .
Hence our objective function becomes ?
N i=1 log max y?Y ( x i ) P ( x i , y|x i ) Q ? ( x i , y ) +?( w ) where Q(x , y ) is a soft constraint factor over the parse tree , and ? is a hyper-parameter controlling the strength of the constraint factor .
The factor Q is also decomposable by edges in the same way as the encoder and the decoder , and therefore our parsing algorithm can still be used with this factor VERB ? VERB NOUN ? NOUN VERB ? NOUN NOUN ? ADJ VERB ? PRON NOUN ? DET VERB ? ADV NOUN ? NUM VERB ? ADP NOUN ? CONJ ADJ ? ADV ADP ? NOUN Table 2 : Universal linguistic rules taken into account .
Q( x , y ) = exp i 1 [ ( t i ? x i ) ? R ] where 1 [ ( t i ? x i ) ? R ] is an indicator function of whether dependency t i ?
x i satisfies one of the universal linguistic rules in R .
The universal linguistic rules that we use are shown in Table 2 ( Naseem et al. , 2010 ) .
Algorithm
We apply coordinate descent to minimize the objective function , which alternately updates w and ?.
In each optimization step of w , we run two epochs of stochastic gradient descent , and in each optimization step of ? , we run two iterations of the Viterbi EM algorithm .
To update w using stochastic gradient descent , for each sentence x , we first run the parsing algorithm to find the best parse tree y * = arg max y?Y ( x ) ( P ( x , y|x ) Q ? ( x , y ) ) ; then we can calculate the gradient of the objective function based on the following derivation , ? log P ( x , y * | x ) ?w = ?log P ( y * | x ) ?w + ?log P ( x|y * ) ?w = ?log P ( y * | x ) ?w = ?
n i=1 w T f ( x , h i , i ) ? log Z( x ) ?w = ( i , j ) ? D( x ) f ( x , i , j ) 1 [ y * j = i , x i ] ? ?( x , i , j ) where D( x ) is the set of all possible dependency arcs of sentence x , 1 [ ? ] is the indicator function , and ?( x , i , j ) is the expected count defined as follows , ?( x , i , j ) = y?Y ( x ) 1 [ y j = i , x i ] P ( y|x ) ( Klein and Manning , 2004 ) , Neural DMV ( Jiang et al. , 2016 ) , and Convex -MST ( Grave and Elhadad , 2015 ) Methods WSJ10 WSJ Basic Setup Feature DMV ( Berg- Kirkpatrick et al. , 2010 ) 63.0 - UR-A E-DMV ( Tu and Honavar , 2012 ) 71.4 57.0 Neural E-DMV ( Jiang et al. , 2016 ) 69.7 52.5 Neural E-DMV ( Good Init ) ( Jiang et al. , 2016 ) 72.5 57.6 Basic Setup + Universal Linguistic Prior Convex-MST ( Grave and Elhadad , 2015 ) 60.8 48.6 HDP-DEP ( Naseem et al. , 2010 ) 71.9 - CRFAE 71.7 55.7 Systems Using Extra Info LexTSG -DMV ( Blunsom and Cohn , 2010 ) 67.7 55.7 CS ( Spitkovsky et al. , 2013 ) 72.0 64.4 MaxEnc ( Le and Zuidema , 2015 ) 73.2 65.8 Table 3 : Comparison of recent unsupervised dependency parsing systems on English .
Basic setup is the same as our setup except that linguistic prior is not used .
Extra info includes lexicalization , longer training sentences , etc .
The expected count can be efficiently computed using the Matrix - Tree Theorem ( Koo et al. , 2007 ) for non-projective tree structures or using a variant of the inside-outside algorithm for projective tree structures ( Paskin , 2001 ) .
To update ? using Viterbi EM , in the E-step we again use the parsing algorithm to find the best parse tree y * for each sentence x ; then in the Mstep we update ? by maximum likelihood estimation .
? c|t = x i 1 [ x i = c , y * i = ? , t ] c x i 1 [ x i = c , y * i = ? , t ]
3 Experiments
Setup
We experimented with projective parsing and used the informed initialization method proposed by Klein and Manning ( 2004 ) to initialize our model before learning .
We tested our model both with and without using the universal linguistic rules .
We used AdaGrad to optimize w .
We used POS tags of the input sentence as the tokens in our model .
We learned our model on training sentences of length ? 10 and reported the directed dependency accuracy on test sentences of length ? 10 and on all test sentences .
Results on English
We evaluated our model on the Wall Street Journal corpus .
We trained our model on section 2 - 21 , tuned the hyperparameters on section 22 , and tested our model on section 23 .
Table 3 shows the directed dependency accuracy of our model ( CR - FAE ) compared with recently published results .
It can be seen that our method achieves a comparable performance with state - of - the - art systems .
We also compared the performances of CRF autoencoder using an objective function with negative log likelihood vs. using our Viterbi version of the objective function ( Eq.1 ) .
We find that the Viterbi version leads to much better performance ( 55.7 vs. 41.8 in parsing accuracy of WSJ ) , which echoes Spitkovsky et al. 's findings on Viterbi EM ( 2010 ) .
Multilingual Results
We evaluated our model on seven languages from the PASCAL Challenge on Grammar Induction ( Gelling et al. , 2012 ) .
We did not use the Arabic corpus because the number of training sentences with length ? 10 is less than 1000 .
The result is shown in Table 4 .
The accuracies of DMV and Neural DMV are from Jiang et.al ( 2016 ) .
Both our model ( CRFAE ) and Convex - MST were tuned on the validation set of each corpus .
It can be seen that our method achieves the best results on average .
Besides , our method outperforms Convex - MST both with and without linguistic prior .
From the results we can also see that utilizing universal linguistic prior greatly improves the performance of Convex - MST and our model .
Conclusion
In this paper , we propose a new discriminative model for unsupervised dependency parsing based on CRF autoencoder .
Both learning and inference of our model are tractable .
We tested our method on eight languages and show that our model is competitive to the state - of - the - art systems .
The code is available at https://github.
com/caijiong / CRFAE -Dep-Parser Table 4 : 4 Parsing accuracy on seven languages .
Our model is compared with DMV Basque Czech Danish Dutch Portuguese Slovene Swedish Avg Length : ? 10 DMV ( EM ) 41.1 31.3 50.8 47.1 36.7 36.7 43.5 41.0 DMV ( Viterbi ) 47.1 27.1 39.1 37.1 32.3 23.7 42.6 35.5 Neural DMV ( EM ) 46.5 33.1 55.6 49.0 30.4 42.2 44.3 43.0 Neural DMV ( Viterbi ) 48.1 28.6 39.8 37.2 36.5 39.9 47.9 39.7 Convex -MST ( No Prior ) 29.4 36.5 49.3 31.3 46.4 33.7 35.5 37.4 Convex-MST ( With Prior ) 30.0 46.1 51.6 35.3 55.4 63.7 50.9 47.5 CRFAE ( No Prior ) 49.0 33.9 28.8 39.3 47.6 34.7 51.3 40.6 CRFAE ( With Prior ) 49.9 48.1 53.4 43.9 68.0 52.5 64.7 54.3 Length : All DMV ( EM ) 31.2 28.1 40.3 44.2 23.5 25.2 32.0 32.0 DMV ( Viterbi ) 40.9 20.4 32.6 33.0 26.9 16.5 36.2 29.5 Neural DMV ( EM ) 38.5 29.3 46.1 46.2 16.2 36.6 32.8 35.1 Neural DMV ( Viterbi ) 41.8 23.8 34.2 33.6 29.4 30.8 40.2 33.4 Convex -MST ( No Prior ) 30.5 33.4 44.2 29.3 38.3 32.2 28.3 33.7 Convex -MST ( With Prior ) 30.6 40.0 45.8 35.6 46.3 51.8 40.5 41.5 CRFAE ( No Prior ) 39.8 25.4 24.2 35.2 52.2 26.4 40.0 34.7 CRFAE ( With Prior ) 41.4 36.8 40.5 38.6 58.9 43.3 48.5 44.0
