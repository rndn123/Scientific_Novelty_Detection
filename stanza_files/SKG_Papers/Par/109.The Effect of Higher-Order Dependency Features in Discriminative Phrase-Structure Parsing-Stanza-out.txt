title
The Effect of Higher -Order Dependency Features in Discriminative Phrase-Structure Parsing
abstract
Higher - order dependency features are known to improve dependency parser accuracy .
We investigate the incorporation of such features into a cube decoding phrase -structure parser .
We find considerable gains in accuracy on the range of standard metrics .
What is especially interesting is that we find strong , statistically significant gains on dependency recovery on out-of- domain tests ( Brown vs. WSJ ) .
This suggests that higher - order dependency features are not simply overfitting the training material .
Introduction
Higher- order dependency features encode more complex sub-parts of a dependency tree structure than first-order , bigram head -modifier relationships .
1
The clear trend in dependency parsing has been that the addition of such higher -order features improves parse accuracy ( McDonald & Pereira , 2006 ; Carreras , 2007 ; Koo & Collins , 2010 ; Zhang & Nivre , 2011 ; Zhang & McDonald , 2012 ) .
This finding suggests that the same benefits might be observed in phrase-structure parsing .
But , this is not necessarily implied .
Phrasestructure parsers are generally stronger than dependency parsers ( Petrov et al. , 2010 ; Petrov & McDonald , 2012 ) , and make use of more kinds of information .
So , it might be that the information modelled by higher - order dependency features adds less of a benefit in the phrase-structure case .
To investigate this issue , we experiment using Huang 's ( 2008 ) cube decoding algorithm .
This algorithm allows structured prediction with nonlocal features , as discussed in ?2 .
Collins 's ( 1997 ) strategy of expanding the phrase-structure parser 's dynamic program to incorporate head-modifier dependency information would not scale to the complex kinds of dependencies we will consider .
Using Huang 's algorithm , we can indeed incorporate arbitrary types of dependency feature , using a single , simple dynamic program .
Compared to the baseline , non-local feature set of Collins ( 2000 ) and Charniak & Johnson ( 2005 ) , we find that higher - order dependencies do in fact tend to improve performance significantly on both dependency and constituency accuracy metrics .
Our most interesting finding , though , is that higher - order dependency features show a consistent and unambiguous contribution to the dependency accuracy , both labelled and unlabelled , of our phrase -structure parsers on outof-domain tests ( which means , here , trained on WSJ , but tested on BROWN ) .
In fact , the gains are even stronger on out - of- domain tests than on indomain tests .
One might have thought that higherorder dependencies , being rather specific by nature , would tend to pick out only very rare events , and so only serve to over - fit the training material , but this is not what we find .
We speculate as to what this might mean in ?5.2 .
The cube decoding paradigm requires a firststage parser to prune the output space .
For this , we use the generative parser of Petrov et al . ( 2006 ) .
We can use this parser 's model score as a feature in our discriminative model at no additional cost .
However , doing so conflates the contribution to accuracy of the generative model , on the one hand , and the discriminatively trained , hand - written , features , on the other .
Future systems might use the same or a similar feature set to ours , but in an architecture that does not include any generative parser .
On the other hand , some systems might indeed incorporate this generative model 's score .
So , we need to know exactly what the generative model is contributing to the accuracy of a generative - discriminative model combination .
Thus , we conduct experiments in sets : in some cases the generative model score is used , and in others it is not used .
Compared to the faster and more psychologically plausible shift-reduce parsers ( Zhang & Nivre , 2011 ; Zhang & Clark , 2011 ) , cube decoding is a computationally expensive method .
But , cube decoding provides a relatively exact environment with which to compare different feature sets , has close connections with modern phrasebased machine translation methods ( Huang & Chiang , 2007 ) , and produces very accurate parsers .
In some cases , one might want to use a slower , but more accurate , parser during the training stage of a semi-supervised parser training strategy .
For example , Petrov et al . ( 2010 ) have shown that a fast parser ( Nivre et al. , 2007 ) can be profitably trained from the output of a slower but more accurate one ( Petrov et al. , 2006 ) , in a strategy they call uptraining .
We make the source code for these experiments available .
2 2 Phrase-Structure Parsing with Non-Local Features
Non-Local Features
To decode using exact dynamic programming ( i.e. , CKY ) , one must restrict oneself to the use of only local features .
Local features are those that factor according to the individual rule productions of the parse .
For example , a feature indicating the presence of the rule S ? NP VP is local .
3 But , a feature that indicates that the head word of this S is , e.g. , joined , is non-local , because the head word of a phrase cannot be determined by looking at a single rule production .
To find a phrase 's head word ( or tag ) , we must recursively find the 2 See http://gfcoppola.net/code.php.
This software is available for free for non-profit research uses .
3
A feature indicating that , e.g. , the first word dominated by S is Pierre is also local , since the words of the sentence are constant across hypothesized parses , and words can be referred to by their position with respect to a given rule production .
See Huang ( 2008 ) for more details .
head phrase of each local rule production , until we reach a terminal node ( or tag node ) .
This recursion would not be allowed in standard CKY .
Many discriminative parsers have used only local features ( Taskar et al. , 2004 ; Turian et al. , 2007 ; Finkel et al. , 2008 ) .
However , Huang ( 2008 ) shows that the use of non-local features does in fact contribute substantially to parser performance .
And , our desire to make heavy use of head - word dependency relations necessitates the use of non-local features .
Cube Decoding
While the use of non-local features destroys the ability to do exact search , we can still do inexact search using Huang 's ( 2008 ) cube decoding algorithm .
4
A tractable first-stage parser prunes the space of possible parses , and outputs a forest , which is a set of rule production instances that can be used to make a parse for the given sentence , and which is significantly pruned compared to the entire space allowed by the grammar .
The size of this forest is at most cubic in the length of the sentence ( Billot & Lang , 1989 ) , but implicitly represents exponentially many parses .
To decode , we fix an beam width of k ( an integer ) .
Then , when parsing , we visit each node n in the same bottomup order we would use for Viterbi decoding , and compute a list of the top k parses to n , according to a global linear model ( Collins , 2002 ) , using the trees that have survived the beam at earlier nodes .
The First-Stage Parser
As noted , we require a first-stage parser to prune the search space .
5
As a by - product of this pruning procedure , we are able to use the model score of the first-stage parser as a feature in our ultimate model at no additional cost .
As a first-stage parser , we use Huang et al . 's ( 2010 ) implementation of the LA - PCFG parser of Petrov et al . ( 2006 ) , which uses a generative , latent - variable model .
Features
Phrase-Structure Features
Our phrase-structure feature set is taken from Collins ( 2000 ) , Charniak & Johnson ( 2005 ) , and Huang ( 2008 ) .
Some features are omitted , with choices made based on the ablation studies of Johnson & Ural ( 2010 ) .
This feature set , which we call ?
phrase , contains the following , mostly nonlocal , features , which are described and depicted in Charniak & Johnson ( 2005 ) , Huang ( 2008 ) , and Johnson & Ural ( 2010 ) : ? CoPar
The depth ( number of levels ) of parallelism between adjacent conjuncts ?
CoParLen
The difference in length between adjacent conjuncts ?
Edges
The words or ( part- of-speech ) tags on the outside and inside edges of a given XP 6 ? NGrams Sub-parts of a given rule production ?
NGramTree
An n-gram of the input sentence , or the tags , along with the minimal tree containing that n-gram ?
HeadTree
A sub-tree containing the path from a word to its maximal projection , along with all siblings of all nodes in that path ?
Heads Head-modifier bigrams ?
Rule A single rule production ?
Tag
The tag of a given word ?
Word
The tag of and first XP above a word ?
WProj
The tag of and maximal projection of a word Heads is a first-order dependency feature .
Dependency Parsing Features McDonald et al. ( 2005 ) showed that chart- based dependency parsing , based on Eisner 's ( 1996 ) algorithm , could be successfully approached in a discriminative framework .
In this earliest work , each feature function could only refer to a single , bigram head- modifier relationship , e.g. , Modifier , below .
Subsequent work ( McDonald & Pereira , 2006 ; Carreras , 2007 ; Koo & Collins , 2010 ) looked at allowing features to access more complex , higher - order relationships , including trigram and 4 - gram relationships , e.g. , all features apart from Modifier , below .
With the ability to incorporate non-local phrase-structure parse features ( Huang , 2008 ) , we can recognize dependency features of arbitrary order ( cf. Zhang & McDonald ( 2012 ) ) .
Our dependency feature set , which we call ? deps , contains : ?
Modifier head and modifier 6
The tags outside of a given XP are approximated using the marginally most likely tags given the parse .
?
Sibling head , modifier m , and m's nearest inner sibling ?
Grandchild head , modifier m , and one of m's modifiers ?
Sibling + Grandchild head , modifier m , m's nearest inner sibling , and one of m's modifiers ?
Grandchild + Grandsibling head , modifier m , one of m's modifiers g , and g's inner sibling
These features are insensitive to arc labels in the present experiments , but future work will incorporate arc labels .
Each feature class contains more and less lexicalized versions .
Generative Model Score Feature Finally , we have a feature set , ? gen , containing only one feature function .
This feature maps a parse to the logarithm of the MAX - RULE - PRODUCT score of that parse according to the LA - PCFG parsing model , which is trained separately .
This score has the character of a conditional likelihood for the parse ( see Petrov & Klein ( 2007 b ) ) .
Training
We have two feature sets ?
phrase and ? deps , for which we fix weights using parallel stochastic optimization of a structured SVM objective ( Collins , 2002 ; Taskar et al. , 2004 ; Crammer et al. , 2006 ; Martins et al. , 2010 ; McDonald et al. , 2010 ) .
To the single feature in the set ? gen ( i.e. the generative model score ) , we give the weight 1 .
The combined models , ? phrase + deps , ? phrase + gen , and ?
phrase + deps +gen , are then model combinations of the first three .
The combination weights for these combinations are obtained using Och 's ( 2003 ) Minimum Error-Rate Training ( MERT ) .
The MERT stage helps to avoid feature undertraining ( Sutton et al. , 2005 ) , and avoids the problem of scaling involved in a model that contains mostly boolean features , but one , real-valued , logscale feature .
Training is conducted in three stages ( SVM , MERT , SVM ) , so that there is no influence of any data outside the given training set ( WSJ2 - 21 ) on the combination weights .
Experiments
Methods
All models are trained on WSJ2 - 21 , with WSJ22 used to pick the stopping iteration for online ( Marcus et al. , 1993 ) . 7
We evaluate using harmonic mean between labelled bracket recall and precision ( EVALB F 1 ) , unlabelled dependency accuracy ( UAS ) , and labelled dependency accuracy ( LAS ) .
Dependencies are extracted from full output trees using the algorithm of de Marneffe & Manning ( 2008 ) .
We chose this dependency extractor , firstly , because it is natively meant to be run on the output of phrase -structure parsers , rather than on gold trees with function tags and traces still present , as is , e.g. , the Penn-Converter of Johansson & Nugues ( 2007 ) .
Also , this is the extractor that was used in a recent shared task ( Petrov & McDonald , 2012 ) .
We use EVALB and eval.pl to calculate scores .
For hypothesis testing , we used the paired bootstrap test recently empirically evaluated in the context of NLP by Berg- Kirkpatrick et al . ( 2012 ) .
This involves drawing b subsamples of size n with replacement from the test set in question , and checking relative performance of the models on the subsample ( see the reference ) .
We use b = 10 6 and n = 500 in all tests .
Results
The performance of the models is shown in Table 1 , and Table 2 depicts the results of significance tests of differences between key model pairs .
We find that adding in the higher - order dependency feature set , ? deps , makes a statistically significant improvement in accuracy on most metrics , in most conditions .
On the in- domain WSJ test set , we find that ?
phrase + deps is significantly better than either of its component parts on all metrics .
But , ? phrase + deps +gen is significantly better than ?
phrase + gen only on F 1 , but not on UAS or LAS .
However , on the out-of- domain BROWN tests , we find that adding ?
deps always adds considerably , and in a statistically significant way , to both LAS and UAS .
That is , not only is ?
phrase + deps better at dependency recovery than its component parts , but ?
phrase + deps +gen is also considerably bet-ter on dependency recovery than ?
phrase + gen , which represents the previous state - of - the - art in this vein of research ( Huang , 2008 ) .
This result is perhaps counter-intuitive , in the sense that one might have supposed that higher - order dependency features , being highly specific by nature , might only have only served to over - fit the training material .
However , this result shows otherwise .
Note that the dependency features include various levels of lexicalization .
It might be that the more unlexicalized features capture something about the structure of correct parses , that transfers well out-ofdomain .
Future work should investigate this .
And , it of course remains to be seen how this result will transfer to other train-test domain pairs .
To our knowledge , this is the first work to specifically separate the role of the generative model feature from the other features of Collins ( 2000 ) and Charniak & Johnson ( 2005 ) .
We note that , even without the ?
gen feature , the discriminative parsing models are very strong , but adding ? gen nevertheless yields considerable gains .
Thus , while a fully discriminative model , perhaps implemented using a shift-reduce algorithm , can be expected to do very well , if the best accuracy is necessary ( e.g. , in a semi-supervised training strategy ) , it still seems to pay to use the generativediscriminative model combination .
Note that the LAS scores of our models without ? gen are relatively weak .
This is presumably largely because our dependency features are , at present , not sensitive to arc labels , so our results probably underestimate the capability of our general framework with respect to labelled dependency recovery .
Huang 's ( 2008 ) .
Note that our model ? phrase + gen uses essentially the same features as Huang ( 2008 ) , so the fact that our ?
phrase + gen is noticeably more accurate on F 1 is presumably due to the benefits in reduced feature under-training achieved by the MERT combination strategy .
Also , our ?
phrase + deps model is as accurate as Huang's , without even using the generative model score feature .
Conclusion
We have shown that the addition of higher -order dependency features into a cube decoding phasestructure parser leads to statistically significant gains in accuracy .
The most interesting finding is that these gains are clearly observed on out-ofdomain tests .
This seems to imply that higherorder dependency features do not merely over - fit the training material .
Future work should look at other train-test domain pairs , as well as look at exactly which higher - order dependency features are most important to out - of- domain accuracy .
Table 1 : 1 Performance of the various models in cube decoding experiments , on the test set ( indomain ) and the BROWN test set ( out - of- domain ) .
G abbreviates generative , D abbreviates discriminative , and G+D a combination .
Some cells are empty because ? deps features are only sensitive to unlabelled dependencies .
Best results in D and G+D conditions appear in bold face .
Test Set WSJ BROWN
