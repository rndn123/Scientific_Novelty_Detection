title
Concavity and Initialization for Unsupervised Dependency Parsing
abstract
We investigate models for unsupervised learning with concave log-likelihood functions .
We begin with the most well -known example , IBM Model 1 for word alignment ( Brown et al. , 1993 ) and analyze its properties , discussing why other models for unsupervised learning are so seldom concave .
We then present concave models for dependency grammar induction and validate them experimentally .
We find our concave models to be effective initializers for the dependency model of Klein and Manning ( 2004 ) and show that we can encode linguistic knowledge in them for improved performance .
Introduction
In NLP , unsupervised learning typically implies optimization of a " bumpy " objective function riddled with local maxima .
However , one exception is IBM Model 1 ( Brown et al. , 1993 ) for word alignment , which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function .
1 For other models , such as those used in unsupervised part- of-speech tagging and grammar induction , and indeed for more sophisticated word alignment models , the log-likelihood function maximized by EM is non-concave .
As a result , researchers are obligated to consider initialization in addition to model design ( Klein and Manning , 2004 ; Goldberg et al. , 2008 ) .
For example , consider the dependency grammar induction results shown in Table 1 when training the widely used dependency model with valence ( DMV ; Klein and Manning , 2004 ) .
Using uniform distributions for initialization ( UNIF ) results in an accuracy of 17.6 % on the test set , well below the baseline of attaching each word to its right neighbor ( ATTACHRIGHT , 31.7 % ) .
Furthermore , when using a set of 50 random initializers ( RAND ) , the standard deviation of the accuracy is an alarming 8.3 % .
In light of this sensitivity to initialization , it is compelling to consider unsupervised models with concave log-likelihood functions , which may provide stable , data-supported initializers for more complex models .
In this paper , we explore the issues involved with such an expedition and elucidate the limitations of such models for unsupervised NLP .
We then present simple concave models for dependency grammar induction that are easy to implement and offer efficient optimization .
We also show how linguistic knowledge can be encoded without sacrificing concavity .
Using our models to initialize the DMV , we find that they lead to an improvement in average accuracy across 18 languages .
IBM Model 1 and Concavity IBM Model 1 is a conditional model of a targetlanguage sentence e of length m and an alignment a given a source - language sentence f of length l .
The generation of m is assumed to occur with some ( inconsequential ) uniform probability .
The alignment vector a , a hidden variable , has an entry for each element of e that contains the index in f of the aligned word .
These entries are used to define which translation parameters t( e j | f a j ) are active .
Model 1 assumes that the probability of the ith ele-ment in a , denoted a( i | j , l , m ) , is simply a uniform distribution over all l source words plus the null word .
These assumptions result in the following log-likelihood for a sentence pair f , e under Model 1 ( marginalizing a ) : log p( e | f ) = log ( l + 1 ) m + m j=1 log l i=0 t( e j | f i ) ( 1 )
The only parameters to be learned in the model are t = { t ( e | f ) }
e , f .
Since a parameter is concave in itself , the sum of concave functions is concave , and the log of a concave function is concave , Eq. 1 is concave in t ( Brown et al. , 1993 ) . IBM
Model 2 involves a slight change to Model 1 in which the probability of a word link depends on the word positions .
However , this change renders it no longer concave .
Consider the log-likelihood function for Model 2 : log + m j=1 log l i=0 t( e j | f i ) ? a( i | j , l , m ) ( 2 ) Eq. 2 is not concave in the parameters t( e j | f i ) and a( i | j , l , m ) because a product is neither convex nor concave in its vector of operands .
This can be shown by computing the Hessian matrix of f ( x , y ) = xy and showing that it is indefinite .
In general , concavity is lost when the loglikelihood function contains a product of model parameters enclosed within a log .
If the sum is not present , the log can be used to separate the product of parameters , making the function concave .
It can also be shown that a " featurized " version ( Berg- Kirkpatrick et al. , 2010 ) of Model 1 is not concave .
More generally , any non-concave function enclosed within log will cause the log-likelihood function to be non-concave , though there are few other non-concave functions with a probabilistic semantics than those just discussed .
Concave , Unsupervised Models
Nearly every other model used for unsupervised learning in NLP has a non-concave log-likelihood function .
We now proceed to describe the conditions necessary to develop concave models for two tasks .
Part-of-Speech Tagging Consider a standard first-order hidden Markov model for POS tagging .
Letting y denote the tag sequence for a sentence e with m tokens , the singleexample log-likelihood is : log y p(stop | y m ) m j=1 p(y j | y j?1 ) ? p( e j | y j ) ( 3 ) where y 0 is a designated " start " symbol .
Unlike IBM
Models 1 and 2 , we cannot reverse the order of the summation and product here because the transition parameters p(y j | y j?1 ) cause each tag decision to affect its neighbors .
Therefore , Eq. 3 is non-concave due to the presence of a product within a log .
However , if the tag transition probabilities p(y j | y j?1 ) are all constants and also do not depend on the previous tag y j?1 , then we can rewrite Eq. 3 as the following concave log-likelihood function ( using C( y ) to denote a constant function of tag y , e.g. , a fixed tag prior distribution ) : log C(stop ) + log m j=1 y j C(y j ) ? p( e j | y j )
Lacking any transition modeling power , this model appears weak for POS tagging .
However , we note that we can add additional conditioning information to the p(e j | y j ) distributions and retain concavity , such as nearby words and tag dictionary information .
We speculate that such a model might learn useful patterns about local contexts and provide an initializer for unsupervised part- of-speech tagging .
Dependency Grammar Induction
To develop dependency grammar induction models , we begin with a version of Model 1 in which a sentence e is generated from a copy of itself ( denoted e ) : log p( e | e ) = log ( m + 1 ) m + m j=1 log m i=0 , i =j c( e j | e i ) ( 4 ) If a word e j is " aligned " to e 0 , e j is a root .
This is a simple child - generation model with no tree constraint .
In order to preserve concavity , we are forbidden from conditioning on other parent- child assignments or including any sort of larger constraints .
However , we can condition the child distributions on additional information about e since it is fully observed .
This conditioning information may include the direction of the edge , its distance , and any properties about the words in the sentence .
We found that conditioning on direction improved performance : we rewrite the c distributions as c( e j | e i , sign ( j ? i ) ) and denote this model by CCV1 .
We note that we can also include constraints in the sum over possible parents and still preserve concavity .
Naseem et al. ( 2010 ) found that adding parentchild constraints to a grammar induction system can improve performance dramatically .
We employ one simple rule : roots are likely to be verbs .
2
We modify CCV1 to restrict the summation over parents to exclude e 0 if the child word is not a verb .
3
We only employ this restriction during EM learning for sentences containing at least one verb .
For sentences without verbs , we allow all words to be the root .
We denote this model by CCV2 .
In related work , Brody ( 2010 ) also developed grammar induction models based on the IBM word alignment models .
However , while our goal is to develop concave models , Brody employed Bayesian nonparametrics in his version of Model 1 , which makes the model non-concave .
Experiments
We ran experiments to determine how well our concave grammar induction models CCV1 and CCV2 can perform on their own and when used as initializers for the DMV ( Klein and Manning , 2004 ) .
The DMV is a generative model of POS tag sequences and projective dependency trees over them .
It is the foundation of most state - of - the - art unsupervised grammar induction models ( several of which are listed in Tab . 1 ) .
The model includes multinomial distributions for generating each POS tag given its parent and the direction of generation : where e i is the parent POS tag and e j the child tag , these distributions take the form c( e j | e i , sign ( j ? i ) ) , analogous to the distributions used in our concave models .
The DMV also has multinomial distributions for deciding whether to stop or continue generating children in each direction considering whether any children have already been generated in that direction .
The majority of researchers use the original initializer from Klein and Manning ( 2004 ) , denoted here K&M. K&M is a deterministic harmonic initializer that sets parent-child token affinities inversely 2
This is similar to the rule used by Mare?ek and ? abokrtsk ? ( 2011 ) with empirical success .
3
As verbs , we take all tags that map to V in the universal tag mappings from Petrov et al . ( 2012 ) .
Thus , to apply this constraint to a new language , one would have to produce a similar tag mapping or identify verb tags through manual inspection .
proportional to their distances , then normalizes to obtain probability distributions .
K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments .
Procedure
We run EM for our concave models for 100 iterations .
We evaluate the learned models directly as parsers on the test data and also use them to initialize the DMV .
When using them directly as parsers , we use dynamic programming to ensure that a valid tree is recovered .
When using the concave models as initializers for the DMV , we copy the c parameters over directly since they appear in both models .
We do not have the stop / continue parameters in our concave models , so we simply initialize them uniformly for the DMV .
We train each DMV for 200 iterations and use minimum Bayes risk decoding with the final model on the test data .
We use several initializers for training the DMV , including the uniform initializer ( UNIF ) , K&M , and our trained concave models CCV1 and CCV2 .
Data
We use data prepared for the CoNLL 2006/07 shared tasks ( Buchholz and Marsi , 2006 ; Nivre et al. , 2007 ) . 4
We follow standard practice in removing punctuation and using short sentences ( ?
10 or ? 20 words ) for training .
For all experiments , we train on separate data from that used for testing and use gold POS tags for both training and testing .
We report accuracy on ( i ) test set sentences ?
10 words and ( ii ) all sentences from the test set .
Results Results for English are shown in Tab .
1 . We train on ?2- 21 and test on ?23 in the Penn Treebank .
The constraint on sentence roots helps a great deal , as CCV2 by itself is competitive with the DMV when testing on short sentences .
The true benefit of the concave models , however , appears when using them as initializers .
The DMV initialized with CCV2 achieves a substantial improvement over all others .
When training on sentences of length ? 20 words ( bold ) , the performance even rivals that of several more sophisticated models shown in the table , despite only using the DMV with a different initializer .
Tab .
2 shows results for 18 languages .
On average , CCV2 performs best and CCV1 does at least as well as K&M .
This shows that a simple , concave model can be as effective as a state - of - the - art handdesigned initializer ( K&M ) , and that concave models can encode linguistic knowledge to further improve performance .
4
In some cases , we did not use official CoNLL test sets but instead took the training data and reserved the first 80 % of the sentences for training , the next 10 % for development , and the final 10 % as our test set ; dataset details are omitted for space but are the same as those given by Cohen ( 2011 ) .
Average log-likelihoods ( micro- averaged across sentences ) achieved by EM training are shown in the final column of Tab .
2 . CCV2 leads to substantiallyhigher likelihoods than the other initializers , suggesting that the verb-root constraint is helping EM to find better local optima .
5
Discussion
Staged training has been shown to help unsupervised learning in the past , from early work in grammar induction ( Lari and Young , 1990 ) and word alignment ( Brown et al. , 1993 ) to more recent work in dependency grammar induction ( Spitkovsky et al. , 2010 ) .
While we do not yet offer a generic procedure for extracting a concave approximation from any model for unsupervised learning , our results contribute evidence in favor of the general methodology of staged training in unsupervised learning , and provide a simple and powerful initialization method for dependency grammar induction .
Table 2 : 2 Test set attachment accuracies for 18 languages ; first number in each cell is accuracy for sentences ?
10 words and second is for all sentences .
For training , sentences ?
10 words from each treebank were used .
In order , languages are Basque , Bulgarian , Catalan , Chinese , Czech , Danish , Dutch , English , German , Greek , Hungarian , Italian , Japanese , Portuguese , Slovenian , Spanish , Swedish , and Turkish .
Init .
eu bg ca zh cs da nl en de el hu UNIF 24/21 32/26 27/29 44/40 32/30 24/19 21/21 21/18 31/24 37/32 23/18 K&M 32/26 48/40 24/25 38/33 31/29 34/23 39/33 44/33 47/37 50/41 23/20 CCV1 22/21 34/27 44/51 46/45 33/31 19/14 24/24 45/31 46/31 51/45 32/28 CCV2 26/25 34/26 29/35 46/44 50/40 29/18 50/43 54/43 49/33 50/45 60/46 it ja pt sl es sv tr avg .
accuracy avg .
log-likelihood UNIF 31/24 35/30 49/36 20/20 29/24 26/22 33/30 29.8 / 25.7 - 15.05 K&M 32/24 39/31 44/28 33/27 19/11 46/33 39/36 36.7 / 29.4 - 14.84 CCV1 34/25 42/27 50/38 30/25 41/33 45/33 37/29 37.5 / 30.9 -14.93 CCV2 55/48 49/31 50/38 22/21 57/50 46/32 31/22 43.7 / 35.5 - 14.45
It is not strictly concave ( Toutanova and Galley , 2011 ) .
However , while CCV1 leads to a higher average accuracy than K&M , the latter reaches slightly higher likelihood , suggesting that the success of the concave initializers is only partially due to reaching high training likelihood .
