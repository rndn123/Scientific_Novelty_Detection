title
Stack - based Multi-layer Attention for Transition - based Dependency Parsing
abstract
Although sequence- to-sequence ( seq2seq ) network has achieved significant success in many NLP tasks such as machine translation and text summarization , simply applying this approach to transition - based dependency parsing cannot yield a comparable performance gain as in other stateof - the - art methods , such as stack - LSTM and head selection .
In this paper , we propose a stack - based multi-layer attention model for seq2seq learning to better leverage structural linguistics information .
In our method , two binary vectors are used to track the decoding stack in transition - based parsing , and multi-layer attention is introduced to capture multiple word dependencies in partial trees .
We conduct experiments on PTB and CTB datasets , and the results show that our proposed model achieves state - of - the - art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model .
Introduction
Deep learning models have been proven very effective in solving various NLP problems such as language modeling , machine translation and syntactic parsing .
For dependency parsing , one line of research aims to incrementally integrate distributed word representations into classic dependency parsing ( Chen and Manning , 2014 ; Weiss et al. , 2015 ; Andor et al. , 2016 ; Cross and Huang , 2016 ; Kiperwasser and Goldberg , 2016 ; Dozat and Manning , 2016 ) .
Another line of research attempts to leverage end-to - end neural network to perform dependency parsing , such as stack - LSTM and sequence- to-sequence ( seq2seq ) model ( Dyer et al. , 2015 ; Zhang et al. , 2017 ; Wiseman and Rush , 2016 ) .
Recently seq2seq model has made significant success in many NLP tasks , such as machine translation and text summarization ( Cho et al. , 2014 ; Rush et al. , 2015 ) .
Unfortunately , to our best knowledge , simply applying seq2seq model to transition - based dependency parsing cannot achieve comparable results as in other state - of - the - art methods like stack - LSTM and head selection ( Dyer et al. , 2015 ; Zhang et al. , 2017 ) .
One issue with the simple seq2seq neural network for dependency parsing is that structural linguistic information , which plays a key role in classic transition - based or graph - based dependency parsing models , cannot be explicitly employed .
For example , classic transition - based parsing algorithm utilizes a stack to manage the heads of partial sub-trees and leverages these evidents for action selection , while such information is missing from current seq2seq models .
Another problem is related to the limit of the conventional attention network being used in seq2seq network , which is unable to capture dependencies between words in the input .
As a matter of fact , various types of features ( word unigram , bigram , trigram , . . . ) traditionally adopted by transition - based parsing algorithm are usually ignored by the current attention mechanism , but they are very important to capture word dependencies in generated partial trees .
In this paper , we propose a stack - based multilayer attention mechanism to solve the above problems .
To simulate the stack used in the transition - based dependency parsing , we introduce two binary vectors , one indicates whether a word is pushed into the stack , and another indicates whether a word is popped out from it .
To model the complex structural information , we propose a multi-layer attention based on the stack information , previous action and input sentence .
The multi-layer attention aims to capture multiple word dependencies in partial trees for action prediction .
We evaluate our model on English and Chinese datasets .
Experimental results show that our proposed model can significantly outperform the basic seq2seq model with 1.87 UAS ( English ) and 1.61 UAS ( Chinese ) , matching the state - of - the - art parsing performance .
With 4 models ensembled , we obtain further improvements with accuracies of 94.16 UAS ( English ) and 87.97 UAS ( Chinese ) .
Neural Model for Sequence-to- Sequence Learning
In this work , we follow the encoder-decoder architecture proposed by Bahdanau et al . ( 2015 ) .
The whole architecture can be divided into three components : encoder , decoder and attention .
Encoder :
The encoder reads in the source sentence X = ( x 1 , x 2 , ... , x T ) and transforms it into a sequence of hidden states h = ( h 1 , h 2 , ... , h T ) , using a bi-directional recurrent neural network that is usually implemented as Gated Recurrent Unit ( GRU ) ( Cho et al. , 2014 ) or Long Short - Term Memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) . Attention Mechanism :
The context vector c i is a weighted sum of the hidden states ( h 1 , h 2 , ... , h T ) with the coefficients ?
i,1 , ? i,2 , ... , ?
i, T computed by ?
i , t = exp ( e i , t ) k exp ( e i , k ) ( 1 ) e i , t = v a tanh ( W a z i?1 + U a h t ) ( 2 ) where v a , W a , U a are the weight matrices .
Decoder :
The decoder uses another recurrent neural network to generate a corresponding target sequence Y = (y 1 , y 2 , ... , y T ) based on the encoded sequence of hidden state h .
At each time i , the conditional probability of target symbol y i is computed by z i = RNN ( [ y i?1 ; c i ] , z i?1 ) ( 3 ) p(y i |y < i , h ) = softmax ( g( y i?1 , z i , c i ) ) ( 4 ) where g is a non-linear function , z i is the i th hidden state of the decoder , and it is calculated conditional on the previous hidden state z i?1 , previous target symbol y i?1 and source context vector c i .
3 Sequence-to- Sequence Parsing Model ? 1 ? 2 ? ? ? 1 ? 2 ? ? ? ? [ w 1 , ? 1 ] encoder decoder attention start ? ? 1 ? ?1 ? ? ? 1 ? ?1 ? ? [ w 2 , ? 2 ] [ w ? , ? ? ] ? ? 1 ? 2 ? ? ? ? 1 ? 2 ? ? ? ? 1 1 ? 2 1 ? ? 1 ? ? 1 2 ? 2 2 ? ? 2 ? ? ? 1 ? ? 2 ? ? ? ? ? ? ? ? 1 ? ? 2 ? ? ? ? ? ? ( Transition - based dependency parsing conceptualizes the process of transforming a sentence into a dependency tree as a sequence of actions .
It can be formulated as a sequence - to-sequence problem , and seq2seq framework can be applied .
Compared with other tasks , such as machine translation , dependency parsing not only considers the previous action and input sentence , but also requires many structure information , such as the subtree structure during the parsing .
Such information plays an important role in transition - based dependency parsing , so traditional methods adopt a stack to save structure information and design different type of features ( word unigram , bigram , trigram , . . . ) to model them .
However , vanilla seq2seq models have no explicit structure to model these necessary structure information .
To better leverage the structure information , we extend the basic seq2seq architecture with a simulated stack and multi-layer attention , as illustrated in Figure 1 .
The main structure ( encoder , decoder and attention part in Figure 1 ) of our parsing model is detailed below .
Encoder :
As shown in the encoder part of Fig- ure 1 , to utilize POS tag information , each word w i is additionally represented by x i , the concatenation of two vectors corresponding to w i 's lexical and POS tag t i embedding : x i = [ W e * e( w i ) ; W t * e( t i ) ] , where e( w i ) and e( t i ) are one- hot vector representations of token w i and its POS tag t i , W e and W t are embedding matrices .
The rest part of the encoder is the same with the basic seq2seq model .
Attention Mechanism :
We improve the attention part in two aspects : introduction of stack information and multi-layer attention structure .
Stack information , which plays an essential role in the conventional algorithm , is simulated with two binary vectors s = ( s 1 , . . . , s T ) and r = ( r 1 , . . . , r T ) to record the state of each word w i and initialized to zero .
When parser pushes word w i into stack , s i is assigned to 1 , while r i is assigned to 1 only if word w i is removed from stack .
Intuitively , at each time step i in the decoding phase , stack information serves as an additional input to the attention model , which provides complementary information of that the source words is in the stack or not .
We expect the stack information would guide the attention model to focus more on words in the stack .
More formally , the coefficients ?
1 , ? 2 , ... , ?
T used in attention mechanism can be rewritten as ?
i , t = exp ( e i , t ) * ( 1 ? r t ) k exp ( e i , k ) * ( 1 ? r k ) ( 5 ) e i , t = v a tanh ( W a z i?1 + U a h t + S a s t ) ( 6 ) where S a is the weight matrix .
To extract complex structure information to help action prediction , we apply a l-layers network structure for attention mechanism as shown in the attention part of Figure 1 .
To further enhance connection between adjacent layers , we replace the state z i?1 in Equation 6 by the concatenation of z i?1 and context vector c m?1 i at each layer m( m > 1 ) .
The Equation 6 can be rewritten as : e m i , t = v a tanh ( W m a [ z i?1 ; c m?1 i ] + U a h t + S a s t ) ( 7 ) where W m is the weight matrix .
With this network structure , we obtain different context vectors ( c 1 i , c 2 i , . . . , c l i ) , and the final context vector c i , which is considered as complex context information , is replaced by the concatenation of those vectors : c i = [ c 1 i ; c 2 i ; . . . ; c l i ] .
Decoder : Unlike machine translation and text summarization in which seq2seq model is widely applied , a sequence of action in dependency parsing must satisfy some constraints so that they can generate a dependency tree .
Following the arcstandard algorithm ( Nivre , 2004 ) , the precondition can be categorized as 1 ) SHIFT ( SH ) :
There exists at least one word that is not pushed into the stack ; 2 ) LEFT - ARC ( LR ( d ) ) and RIGHT - ARC ( RR ( d ) ) :
There are at least two words in the stack .
These two constraints can be defined as indicator functions I( y i ) = ? ? ?
0 y i = SH , W c ? 0 0 y i = LR ( d ) or RR ( d ) , S c < 2 1 otherwise ( 8 ) where S c represents the number of words in the stack and W c is the number of source words that are not pushed into the stack .
To introduce these constraints , the conditional probability of each target symbol y i can be rewritten as p(y i |y < i , h ) = exp ( g i ) * I(y i ) k exp ( g k ) * I(y k ) ( 9 ) where g i is the ith element of g(y i?1 , z i , c i ) .
Experiments
In this section , we evaluate our parsing model on the English and Chinese datasets .
Following Dyer et al. ( 2015 ) , Stanford Dependencies ( de Marneffe and Manning , 2008 ) conversion of the Penn Treebank ( PTB ) ( Marcus et al. , 1993 ) and Chinese Treebank 5.1 ( CTB ) are adopted .
We leverage the arc-standard algorithm for our dependency parsing .
In addition , we limit the vocabulary to contain up to 20 k most frequent words and convert remaining words into the < unk > token .
Setup
For our model , 3 - layers GRU is used for encoder and decoder .
The dimension of word embedding is 300 , the dimension of POS - tag / action embedding is 32 , and the size of hidden units in GRU is 500 .
3 - layers attention structure is adopted in our model .
Following Chen and Manning ( 2014 ) ; Dyer et al. ( 2015 ) , we used 300 - dimensional pretrained GloVe vectors ( Pennington et al. , 2014 ) to initialize our word embedding matrix .
Other model parameters are initialized using a normal distribution with a mean of 0 and a variance of ( Zhang and Nivre , 2011 ) ; C&M14 ( Chen and Manning , 2014 ) ; ConBSO ( Wiseman and Rush , 2016 ) ; Dyer15 ( Dyer et al. , 2015 ) ; Weiss15 ( Weiss et al. , 2015 ) ; K&G16 ( Kiperwasser and Goldberg , 2016 ) ; DENSE ( Zhang et al. , 2017 ) . 6 / ( d row + d col ) , where d row and d col are the number of rows and columns ( Glorot and Bengio , 2010 ) .
Our models are trained on a Tesla K40 m GPU and optimized with vanilla SGD algorithm with mini-batch size 64 for English dataset and 32 for Chinese dataset .
The initial learning rate is set to 2 and will be halved when unlabeled attachment scores ( UAS ) on the development set do not increase for 900 batches .
To alleviate the gradient exploding problem , we rescale the gradient when its norm exceeds 1 . Dropout ( Srivastava et al. , 2014 ) is applied to our model with the strategy recommended in Zaremba et al . ( 2014 ) and the dropout rate is 0.2 .
For testing , beam search is employed to find the best action sequence with beam size 8 .
For evaluation , we report unlabeled ( UAS ) and labeled attachment scores ( LAS ) on the development and test sets .
Following Chen and Manning ( 2014 ) , the punctuation is excluded from the evaluation .
Main Results
Table 1 lists the accuracies of our parsing models , compared to other state - of - the - art parsers .
For the baseline , seq2seq model employs the same encoder and decoder network structure with our model .
We can see that our proposed model can significantly outperform the basic seq2seq model with 1.87 UAS ( English ) and 1.61 UAS ( Chinese ) improvements on the test set .
This demonstrates the effectiveness of our proposed multi-layer attention mechanism .
Besides , our model achieves better UAS accuracy than Z&N11 , C&M14 , ConBSO and Dyer15 on development and test set , while slightly lower than Weiss15 , K&G16 and DENSE .
Weiss15 adopts a structured training procedure which can be easiely applied to our model as well , and it will further improve the performance of our model .
K&G16 uses 11 bidirectional LSTM vectors as features , which will be fed to a transition - based parser .
It suggests a new direction that combines our model with feature engineering of the traditional transition - based parser to gain better performance .
DENSE formalizes dependency parsing as head selection and applies MST algorithms to correct non-tree outputs , while our model does n't require any post-processing at test time .
Dozat and Manning ( 2016 ) use deep biaffine attention instead of traditional attention in the graph - based architectures of K&G16 , achieving 95.74 UAS and 89.30 UAS on PTB -SD and CTB datasets respectively .
For ensemble , we train 4 models using the same network with different random initialization .
When we ensemble these 4 models , we simply average the output probabilities from different models and obtain the better result with accuracies of 94.16 UAS ( English ) and 87.97 UAS ( Chinese ) as shown in the Table 1 .
Impact of l
The hyper-parameter l represents the number of layers in our proposed multi-layer attention mech - anism .
Larger l would bring more capacity , but lead to more computational complexity and aggravate the risk of over-fitting .
We conduct a group of experiments to investigate the impact of l .
The results are shown in Table 2 .
Seq2seq model can be viewed as a special case of our model without any stack information .
With l = 1 , we can see that the introduction of stack information can strongly improve the parsing performance , especially for LAS .
When l is small ( l < 4 ) , the general trend is that larger l leads to better result .
However , further increasing l bring slightly damages to the parsing performance due to the over-fitting problem .
Although larger l would bring more capacity , multiple layers structure will double the training time compared with the vanilla seq2seq .
In our implementation , our model costs about 500 seconds for a round of training data on English PTB dataset , while the vanilla costs about 260 seconds .
Additional Results
We perform some ablation experiments in order to quantify the effect of the different components on our models .
As shown in Table 3 , the POS - tag information plays the most important role in our model .
We note that , different from Dyer et al . ( 2015 ) , we do n't utilize an external word embedding to tackle OOV problem , and it may cause our model to be more dependent on the POS - tag information .
For s and r vectors , same as discussion in last section , we find that the introduction of stack information can strongly improve the parsing performance .
Conclusion
In order to leverage structure information for seq2seq based dependency parsing , in this paper , we propose a stack based multi-layer attention method , in which , stack is simulated with two binary vectors , and multi-layer attention is in -
In the future , we plan to apply our approach in more languages and other transition - based system , such as arc-eager or arc-hybrid .
Another direction we are interested in is to train our model with complex training approaches proposed in Weiss et al . ( 2015 ) and Andor et al . ( 2016 ) . Figure 1 : 1 Figure 1 : The architecture of sequence- tosequence parsing model . SH , LR ( d ) , RR ( d ) denote the SHIFT , LEFT - ARC ( d ) , RIGHT - ARC ( d ) transitions in arc-standard algorithm and d is arclabel .
