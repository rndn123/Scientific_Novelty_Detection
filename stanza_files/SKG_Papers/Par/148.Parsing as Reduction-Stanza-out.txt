title
Parsing as Reduction
abstract
We reduce phrase - based parsing to dependency parsing .
Our reduction is grounded on a new intermediate representation , " head - ordered dependency trees , " shown to be isomorphic to constituent trees .
By encoding order information in the dependency labels , we show that any off-theshelf , trainable dependency parser can be used to produce constituents .
When this parser is non-projective , we can perform discontinuous parsing in a very natural manner .
Despite the simplicity of our approach , experiments show that the resulting parsers are on par with strong baselines , such as the Berkeley parser for English and the best non-reranking system in the SPMRL - 2014 shared task .
Results are particularly striking for discontinuous parsing of German , where we surpass the current state of the art by a wide margin . *
This research was carried out during an internship at Priberam Labs .
Introduction Constituent parsing is a central problem in NLP - one at which statistical models trained on treebanks have excelled ( Charniak , 1996 ; Klein and Manning , 2003 ; Petrov and Klein , 2007 ) .
However , most existing parsers are slow , since they need to deal with a heavy grammar constant .
Dependency parsers are generally faster , but less informative , since they do not produce constituents , which are often required by downstream applications ( Johansson and Nugues , 2008 ; Wu et al. , 2009 ; Berg-Kirkpatrick et al. , 2011 ; Elming et al. , 2013 ) .
How to get the best of both worlds ?
Coarse- to-fine decoding ( Charniak and Johnson , 2005 ) and shift-reduce parsing ( Sagae and Lavie , 2005 ; Zhu et al. , 2013 ) were a step forward to accelerate constituent parsing , but typical runtimes still lag those of dependency parsers .
This is only made worse if discontinuous constituents are allowed - such discontinuities are convenient to represent wh-movement , scrambling , extraposition , and other linguistic phenomena common in free word order languages .
While non-projective dependency parsers , which are able to model such phenomena , have been widely developed in the last decade ( Nivre et al. , 2007 ; McDonald et al. , 2006 ; Martins et al. , 2013 ) , discontinuous constituent parsing is still taking its first steps ( Maier and S?gaard , 2008 ; Kallmeyer and Maier , 2013 ) .
In this paper , we show that an off-the-shelf , trainable , dependency parser is enough to build a highly - competitive constituent parser .
This ( surprising ) result is based on a reduction 1 of constituent to dependency parsing , followed by a simple post-processing procedure to recover unaries .
Unlike other constituent parsers , ours does not require estimating a grammar , nor binarizing the treebank .
Moreover , when the dependency parser is non-projective , our method can perform discontinuous constituent parsing in a very natural way .
Key to our approach is the notion of headordered dependency trees ( shown in Figure 1 ) : by endowing dependency trees with this additional layer of structure , we show that they become isomorphic to constituent trees .
We encode this structure as part of the dependency labels , enabling a dependency - to- constituent conversion .
A related conversion was attempted by Hall and Nivre ( 2008 ) to parse German , but their complex encoding scheme blows up the number of arc labels , affecting the final parser 's quality .
By contrast , our light encoding achieves a 10 - fold decrease in the label alphabet , leading to more accurate parsing .
While simple , our reduction - based parsers are on par with the Berkeley parser for English ( Petrov and Klein , 2007 ) , and with the best single system in the recent SPMRL shared task ( Seddah et al. , 2014 ) , for eight morphologically rich languages .
For discontinuous parsing , we surpass the current state of the art by a wide margin on two German datasets ( TIGER and NEGRA ) , while achieving fast parsing speeds .
We provide a free distribution of our parsers along with this paper , as part of the TurboParser toolkit .
2
Background
We start by reviewing constituent and dependency representations , and setting up the notation .
Following Kong and Smith ( 2014 ) , we use c-/d-prefixes for convenience ( e.g. , we write c-parser for constituent parser and d-tree for dependency tree ) .
Constituent Trees Constituent - based representations are commonly seen as derivations according to a context-free grammar ( CFG ) .
Here , we focus on properties of the c-trees , rather than of the grammars used to generate them .
We consider a broad scenario that permits c-trees with discontinuities , such as the ones derived with linear context- free rewriting systems ( LCFRS ; Vijay - Shanker et al. ( 1987 ) ) .
We also assume that the c-trees are lexicalized .
Formally , let w 1 w 2 . . . w L be a sentence , where w i denotes the word in the ith position .
A ctree is a rooted tree whose leaves are the words { w i } L i=1 , and whose internal nodes ( constituents ) are represented as a tuple Z , h , I , where Z is a non-terminal symbol , h ? { 1 , . . . , L} indicates the lexical head , and I ? { 1 , . . . , L} is the node 's yield .
Each word 's parent is a pre-terminal unary node of the form p i , i , { i} , where p i denotes the word 's part- of-speech ( POS ) tag .
The yields and lexical heads are defined so that for every constituent Z , h , I with children and ( ii ) there is a unique k such that h = m k .
This kth node ( called the head- child node ) is commonly chosen applying a handwritten set of head rules ( Collins , 1999 ; Yamada and Matsumoto , 2003 ) . { X k , m k , J k } K k=1 , ( i ) we have I = K k=1 J k ; A c-tree is continuous if all nodes Z , h , I have a contiguous yield I , and discontinuous otherwise .
Trees derived by a CFG are always continuous ; those derived by a LCFRS may have discontinuities , the yield of a node being a union of spans , possibly with gaps in the middle .
Figure 1 shows an example of a continuous and a discontinuous c-tree .
Discontinuous c-trees have crossing branches , if the leaves are drawn in left-to - right surface order .
An internal node which is not a preterminal is called a proper node .
A node is called unary if it has exactly one child .
A c-tree without unary proper nodes is called unaryless .
If all proper nodes have exactly two children then it is called a binary c-tree .
Continuous binary trees may be regarded as having been generated by a CFG in Chomsky normal form .
Prior work .
There has been a long string of work in statistical c-parsing , shifting from simple models ( Charniak , 1996 ) to more sophisticated ones using structural annotation ( Johnson , 1998 ; Klein and Manning , 2003 ) , latent grammars ( Matsuzaki et al. , 2005 ; Petrov and Klein , 2007 ) , and lexicalization ( Eisner , 1996 ; Collins , 1999 ) .
An orthogonal line of work uses ensemble or reranking strategies to further improve accuracy ( Charniak and Johnson , 2005 ; Huang , 2008 ; Bj?rkelund et al. , 2014 ) .
Discontinuous c-parsing is considered a much harder problem , involving mildly context-sensitive formalisms such as LCFRS or range concatenation grammars , with treebankderived c-parsers exhibiting near-exponential runtime ( Kallmeyer and Maier , 2013 , Figure 27 ) .
To speed up decoding , prior work has considered restrictons , such as bounding the fan-out ( Maier et al. , 2012 ) and requiring well - nestedness ( Kuhlmann and Nivre , 2006 ; G?mez-Rodr ? guez et al. , 2010 ) .
Other approaches eliminate the discontinuities via tree transformations ( Boyd , 2007 ; K?bler et al. , 2008 ) , sometimes as a pruning step in a coarse- to -fine parsing approach ( van Cranenburgh and Bod , 2013 ) .
However , reported runtimes are still superior to 10 seconds per sentence , which is not practical .
Recently , Versley ( 2014a ) proposed an easy - first approach that leads to considerable speed-ups , but is less accurate .
In this paper , we design fast discontinuous c-parsers that outperform all the ones above by a wide margin , with similar runtimes as Versley ( 2014 a ) .
Dependency Trees
In this paper , we use d-parsers as a black box to parse constituents .
Given a sentence w 1 . . . w L , a d-tree is a directed tree spanning all the words in the sentence .
3 h , m , , expressing a typed dependency relation between the head word w h and the modifier w m .
A d-tree is projective if for every arc h , m , there is a directed path from h to all words that lie between h and m in the surface string ( Kahane et al. , 1998 ) .
Projective d-trees can be obtained from continuous c-trees by reading off the lexical heads and dropping the internal nodes ( Gaifman , 1965 ) .
However , this relation is many - to-one : as shown in Figure 2 , several c-trees may project onto the same d-tree , differing on their flatness and on left or right - branching decisions .
In the next section , we introduce the concept of head-ordered d-trees and express one - to - one mappings between these two representations .
Prior work .
There has been a considerable amount of work developing rich -feature d-parsers .
While projective d-parsers can use dynamic programming ( Eisner and Satta , 1999 ; Koo and consider an extra root symbol , as often done in the literature .
Collins , 2010 ) , non-projective d-parsers typically rely on approximate decoders , since the underlying problem is NP - hard beyond arc-factored models ( McDonald and Satta , 2007 ) .
An alternative are transition - based d-parsers ( Nivre et al. , 2006 ; Zhang and Nivre , 2011 ) , which achieve observed linear time .
Since d-parsing algorithms do not have a grammar constant , typical implementations are significantly faster than c-parsers ( Rush and Petrov , 2012 ; Martins et al. , 2013 ) .
The key contribution of this paper is to reduce c-parsing to dparsing , allowing to bring these runtimes closer .
Head-Ordered Dependency Trees
We next endow d-trees with another layer of structure , namely order information .
In this framework , not all modifiers of a head are " born equal . "
Instead , their attachment to the head occurs as a sequence of " events , " which reflect the head 's preference for attaching some modifiers before others .
As we will see , this additional structure will undo the ambiguity expressed in Figure 2 .
Strictly Ordered Dependency Trees
Let us start with the simpler case where the attachment order is strict .
For each head word h with modifiers M h = {m 1 , . . . , m K } , we endow M h with a strict order relation ?
h , so we can organize all the modifiers of h as a chain , m i 1 ? h m i 2 ? h . . . ? h m i K .
We regard this chain as reflecting the order by which words are attached ( i.e. , if m i ?
h m j this means that " m i is attached to h before m j " ) .
We represent this graphically by decorating d-arcs with indices ( # 1 , # 2 , . . . ) to denote the order of events , as we do in Figure 1 .
A d-tree endowed with a strict order for each head is called a strictly ordered d-tree .
We establish below a correspondence between strictly ordered d-trees and binary c-trees .
Before doing so , we need a few more definitions about c-trees .
For each word position h ? { 1 , . . . , L} , we define ?( h ) as the node higher in the c-tree whose lexical head is h .
We call the path from ?( h ) down to the pre-terminal p h the spine of h .
We may regard a c-tree as a set of L spines , one per word , which attach to each other to form a tree ( Carreras et al. , 2008 ) .
We then have the following Proposition 1 .
Binary c-trees and strictly - ordered d-trees are isomorphic , i.e. , there is a one- to - one correspondence between the two sets , where the number of symbols is preserved .
Proof .
We use the construction in Figure 3 .
A formal proof is given as supplementary material .
Weakly Ordered Dependency Trees Next , we relax the strict order assumption , restricting the modifier sets M h = {m 1 , . . . , m K } to be only weakly ordered .
This means that we can partition the K modifiers into J equivalence classes , M h = J j=1 M j h , and define a strict order ?
h on the quotient set : M 1 h ? h . . . ? h M J h .
Intuitively , there is still a sequence of events ( 1 to J ) , but now at each event j it may happen that multiple modifiers ( the ones in the equivalence set M j h ) are si- for every u :=
X , m , J which is a child of v do 5 : if m = h then 6 : Add to D an arc h , m , Z , and put it in M j( h ) h .
7 : end if 8 : end for 9 : Set j( h ) := j( h ) + 1. 10 : end for multaneously attached to h .
A weakly ordered d-tree is a d-tree endowed with a weak order for each head and such that any pair m , m in the same equivalence class ( written m ? h m ) receive the same dependency label .
We now show that Proposition 1 can be generalized to weakly ordered d-trees .
Proposition 2 . Unaryless c-trees and weaklyordered d-trees are isomorphic .
Proof .
This is a simple extension of Proposition 1 .
The construction is the same as in Figure 3 , but now we can collapse some of the nodes in the linked list , originating multiple modifiers attaching to the same position of the spine - this is only possible for sibling arcs with the same index and arc label .
Note , however , that if we start with a c-tree with unary nodes and apply the inverse procedure to obtain a d-tree , the unary nodes will be lost , since they do not involve attachment of modifiers .
In a chain of unary nodes , only the last node is recovered in the inverse transformation .
We emphasize that Propositions 1 - 2 hold without blowing up the number of symbols .
That is , the dependency label alphabet is exactly the same as the set of phrasal symbols in the constituent representations .
Algorithms 1 - 2 convert back and forth between the two formalisms , performing the construction of Figure 3 .
Both algorithms run in linear time with respect to the size of the sentence .
Continuous and Projective Trees
What about the more restricted class of projective d-trees ?
Can we find an equivalence relation with continuous c-trees ?
In this section , we give a precise answer to this question .
It turns out that we need an additional property , illustrated in Figure 4 .
We say that ?
h has the nesting property iff closer words in the same direction are always attached first , i.e. , iff h < m i < m j or h > m i >
Algorithm 2 Conversion from d-tree to c-tree Input : head-ordered d-tree D. Output : c-tree C. 1 : Nodes := GETPOSTORDERTRAVERSAL ( D ) .
2 : for h ?
Nodes do 3 : Create v := p h , h , { h} and set ?( h ) := v. 4 : Sort M h ( D ) , yielding M 1 h ? h M 2 h ? h . . . ? h M J h . 5 : for j = 1 , . . . , J do 6 : Let Z be the label in end for 12 : end for m j implies that either m i ?
h m j or m i ?
h m j .
A weakly - ordered d-tree which is projective and whose orders ?
h have the nesting property for every h is called a nested -weakly ordered projective d-tree .
We then have the following result .
Proposition
3 . Continuous unaryless c-trees and nested -weakly ordered projective d-trees are isomorphic .
{ h , m , Z | m ?
M j h }. 7 : Obtain c-nodes ?( h ) = X , Proof .
See the supplementary material .
Together , Propositions 1 - 3 have as corollary that nested -strictly ordered projective d-trees are in a one- to - one correspondence with binary continuous c-trees .
The intuition is simple : if ? h has the nesting property , then , at each point in time , all one needs to decide about the next event is whether to attach the closest available modifier on the left or on the right .
This corresponds to choosing between left- branching or right - branching in a ctree .
While this is potentially interesting for most continuous c-parsers , which work with binarized c-trees when running the CKY algorithm , our cparsers ( to be described in ?4 ) do not require any binarization since they work with weakly - ordered d-trees , using Proposition 2 .
Reduction - Based Constituent Parsers
We now invoke the equivalence results established in ?3 to build c-parsers when only a trainable dparser is available .
Given a c-treebank provided as input , our procedure is outlined as follows :
1 . Convert the c-treebank to dependencies ( Algorithm 1 ) .
2 . Train a labeled d-parser on this treebank .
3 . For each test sentence , run the labeled d-parser and convert the predicted d-tree into a c-tree without unary nodes ( Algorithm 2 ) .
4 . Do post-processing to recover unaries .
The next subsections describe each of these steps in detail .
Along the way , we illustrate with experiments using the English Penn Treebank ( Marcus et al. , 1993 ) , which we lexicalized by applying the head rules of Collins ( 1999 ) .
4
Dependency Encoding
The first step is to convert the c-treebank to headordered dependencies , which we do using Algorithm 1 .
If the original treebank has discontinuous c-trees , we end up with non-projective d-trees or with violations of the nested property , as established in Proposition 3 .
We handle this gracefully by training a non-projective d-parser in the subsequent stage ( see ?4.2 ) .
Note also that this conversion drops the unary nodes ( a consequence of Proposition 2 ) .
These nodes will be recovered in the last stage , as described in ?4.4 .
Since in this paper we are assuming that only an off-the-shelf d-parser is available , we need to convert head -ordered d-trees to plain d-trees .
We do so by encoding the order information in the dependency labels .
We tried two different strategies .
The first one , direct encoding , just appends suffixes # 1 , # 2 , etc. , as in Figure 1 .
A disadvantage is that the number of labels grows unbounded with the treebank size , as we may encounter complex substructures where the event sequences are long .
The second strategy is a delta-encoding scheme where , rather than writing the absolute indices in the dependency label , we write the differences between consecutive ones .
5
We used this strategy for the continuous treebanks only , whose d-trees are guaranteed to satisfy the nested property .
For comparison , we also implemented a replication of the encoding proposed by Hall and Nivre ( 2008 ) , which we call H&N-encoding .
This strategy concatenates all the c-nodes ' symbols in the modifier 's spine with the attachment position in the head 's spine ( e.g. , in Figure 3 , if the modifier m 2 has a spine with nodes X 1 , X 2 , X 3 , the generated d-label would be X 1 | X 2 | X 3 # 2 ; our direct encoding scheme generates Z 2 # 2 instead ) .
Since their strategy encodes the entire spines into com-plex arc labels , many such labels will be generated , leading to slower runtimes and poorer generalization , as we will see .
For the training portion of the English PTB , which has 27 non-terminal symbols , the direct encoding strategy yields 75 labels , while delta encoding yields 69 labels ( 2.6 indices per symbol ) .
By contrast , the H&N- encoding procedure yields 731 labels , more than 10 times as many .
We later show ( in Tables 1 - 2 ) that delta-encoding leads to a slightly higher c-parsing accuracy than direct encoding , and that both strategies are considerably more accurate than H&N -encoding .
Training the Labeled Dependency Parser
The next step is to train a labeled d-parser on the converted treebank .
If we are doing continuous cparsing , we train a projective d-parser ; otherwise we train a non-projective one .
In our experiments , we found it advantageous to perform labeled d-parsing in two stages , as done by McDonald et al . ( 2006 ) : first , train an unlabeled d-parser ; then , train a dependency labeler .
6 Table 1 compares this approach against a oneshot strategy , experimenting various off-theshelf d-parsers : MaltParser ( Nivre et al. , 2007 ) , MSTParser ( McDonald et al. , 2005 ) , ZPar Nivre , 2011 ) , and TurboParser ( Martins et al. , 2013 ) , all with the default settings .
For Tur-boParser , we used basic , standard and full models .
Our separate d-labeler receives as input a backbone d-structure and predicts a label for each arc .
For each head h , we predict the modifiers ' labels using a simple sequence model , with features of the form ?( h , m , ) and ?( h , m , m , , ) , where m and m are two consecutive modifiers ( possibly on opposite sides of the head ) and and are their labels .
We use the same arc label features ?( h , m , ) as TurboParser .
For ?( h , m , m , , ) , we use the POS triplet p h , p m , p m , plus unilexical features where each of the three POS is replaced by the word form .
Both features are conjoined with the label pair and .
Decoding under this model can be done by running the Viterbi algorithm independently for each head .
The runtime is almost negligible compared with the time to parse : it took 2.1 seconds to process PTB ? 22 , a fraction of about 5 % of the total runtime .
Decoding into Unaryless Constituents
After training the labeled d-parser , we can run it on the test data .
Then , we need to convert the predicted d-tree into a c-tree without unaries .
To accomplish this step , we first need to recover , for each head h , the weak order of its modifiers M h .
We do this by looking at the predicted dependency labels , extracting the event indices j , and using them to build and sort the equivalent classes { M j h } J j=1 .
If two modifiers have the same index j , we force them to have consistent labels ( by always choosing the label of the modifier which is the closest to the head ) .
For continuous c-parsing , we also decrease the index j of the modifier closer to the head as much as necessary to make sure that the nesting property holds .
In PTB ?22 , these corrections were necessary only for 0.6 % of the tokens .
Having done this , we use Algorithm 2 to obtain a predicted c-tree without unary nodes .
Recovery of Unary Nodes
The last stage is to recover the unary nodes .
Given a unaryless c-tree as input , we predict unaries by running independent classifiers at each node in the tree ( a simple unstructured task ) .
Each class is either NULL ( in which case no unary node is appended to the current node ) or a concatenation of unary node labels ( e.g. , S->ADJP for a node JJ ) .
We obtained 64 classes by processing the training sections of the PTB , the fraction of unary nodes being about 11 % of the total number of nodes .
To reduce complexity , for each node symbol we only consider classes that have been observed with that symbol in the training data .
In PTB ?22 , this yields an average of 9.9 candidates per node occurrence .
The classifiers are trained on the original ctreebank , stripping off unary nodes and trained to recover those nodes .
We used the following features ( conjoined with the class and with a flag indicating if the node is a pre-terminal ) : ?
The production rules above and beneath the node ( e.g. , S->NP VP and NP ->DT NN ) ; ?
The node 's label , alone and conjoined with the parent 's label or the left / right sibling 's label ; ?
The leftmost and rightmost word / lemma / POS tag / morpho-syntactic tags in the node 's yield ; ?
If the left / right node is a pre-terminal , the word / lemma / morpho-syntactic tags beneath .
This is a relatively easy task : when gold unaryless c-trees are provided as input , we obtain an EVALB F 1 - score of 99.43 % .
This large figure is due to the small amount of unary nodes , making this module have less impact on the final parser than the d-parser .
Being a lightweight unstructured task , this step took only 0.7 seconds to run on PTB ? 22 , a tiny fraction ( less than 2 % ) of the total runtime .
Table 1 shows the accuracies obtained with the d-parser followed by the unary predictor .
Since two -stage TP - Full with delta-encoding is the best strategy , we use this configuration in the sequel .
To further explore the impact of delta encoding , we report in Table 2 the scores obtained by direct and delta encodings on eight other treebanks ( see ?5.2 for details on these datasets ) .
With the exception of German , in all cases the delta encoding yielded better EVALB F 1 - scores with fewer labels .
Experiments
To evaluate the performance of our reductionbased parsers , we conduct experiments in a variety Parser LR LP F1 # Toks / s. Charniak ( 2000 ) 89.5 89.9 89.5 - Klein and Manning ( 2003 ) 85.3 86.5 85.9 143 Petrov and Klein ( 2007 ) 90 of treebanks , both continuous and discontinuous .
Results on the English PTB Table 3 shows the accuracies and speeds achieved by our system on the English PTB ?23 , in comparison to state - of - the - art c-parsers .
We can see that our simple reduction - based c-parser surpasses the three Stanford parsers ( Klein and Manning , 2003 ; Socher et al. , 2013 , and Stanford Shift-Reduce ) , and is on par with the Berkeley parser ( Petrov and Klein , 2007 ) , while being more than 5 times faster .
The best supervised competitor is the recent shift- reduce parser of Zhu et al . ( 2013 ) , which achieves similar , but slightly better , accuracy and speed .
Our technique has the advantage of being flexible : since the time for d-parsing is the dominating factor ( see ?4.4 ) , plugging a faster d-parser automatically yields a faster c-parser .
While reranking and semi-supervised systems achieve higher accuracies , this aspect is orthogonal , since the same techniques can be applied to our parser .
Results on the SPMRL Datasets
We experimented with datasets for eight languages , from the SPMRL14 shared task ( Seddah et al. , 2014 ) .
We used the official training , development and test sets with the provided predicted POS tags .
For French and German , we used the lexicalization rules detailed in Dybro-Johansen ( 2004 ) and Rehbein ( 2009 ) , respectively .
For Basque , Hungarian and Korean , we always took the rightmost modifier as head - child node .
For Hebrew and Polish we used the leftmost modifier instead .
For Swedish we induced head rules from the provided dependency treebank , as described in Versley ( 2014 b ) .
These choices were based on dev-set experiments .
Table 4 shows the results .
For all languages ex-cept French , our system outperforms the Berkeley parser ( Petrov and Klein , 2007 ) , with or without prescribed POS tags .
Our average F 1 - scores are superior to the best non-reranking system participating in the shared task ( Crabb? and Seddah , 2014 ) and to the c-parser of Hall et al . ( 2014 ) , achieving the best results for 4 out of 8 languages .
Results on the Discontinuous Treebanks Finally , we experimented on two widely - used discontinuous German treebanks : TIGER ( Brants et al. , 2002 ) and NEGRA ( Skut et al. , 1997 ) .
For the former , we used two different splits : TIGER - SPMRL , provided in the SPMRL14 shared task ; and TIGER - H&N , used by Hall and Nivre ( 2008 ) .
For NEGRA , we used the standard splits .
In these experiments , we skipped the unary recovery stage , since very few unary nodes exist in the data .
7
We ran TurboTagger to predict POS tags for TIGER - H&N and NEGRA , while in TIGER - SPMRL we used the predicted POS tags provided in the shared task .
All treebanks were lexicalized using the head-rule sets of Rehbein ( 2009 ) .
For comparison to related work , sentence length cut-offs of 30 , 40 and 70 were applied during the evaluation .
Table 5 shows the results .
We observe that our approach outperforms all the competitors considerably , achieving state - of - the - art accuracies for both datasets .
The best competitor , van Cranenburgh and Bod ( 2013 ) , is more than 3 points behind , both in TIGER - H&N and in NEGRA .
Our reduction - based parsers are also much faster : van Cranenburgh and Bod ( 2013 ) report 3 hours to parse NEGRA with L ? 40 .
Our system parses all NEGRA sentences ( regardless of length ) in 27.1 seconds in a single core , which corresponds to a rate of 618 tokens per second .
This approaches the speed of the easy - first system of Versley ( 2014 a ) , who reports runtimes in the range 670 - 920 tokens per second , but is much less accurate .
Related Work Conversions between constituents and dependencies have been considered by De Marneffe et al . ( 2006 ) in one direction , and by Collins et al . ( 1999 ) and Xia and Palmer ( 2001 ) in the other , toward multi-representational treebanks ( Xia et al. , 2008 ) .
This prior work aimed at linguistically sound conversions , involving grammar-specific transformation rules to handle the kind of ambiguities expressed in Figure 2 .
Our work differs in that we are not concerned about the linguistic plausibility of our conversions , but only with the formal aspects that underlie the two representations .
The work most related to ours is Hall and Nivre ( 2008 ) , who also convert dependencies to constituents to prototype a c-parser for German .
Their encoding strategy is compared to ours in ?4.1 : they encode the entire spines into the dependency labels , which become rather complex and numerous .
A similar strategy has been used by Versley ( 2014a ) for discontinuous c-parsing .
Both are largely outperformed by our system , as shown in ?5.3 .
The crucial difference is that we encode only the top node 's label and its position in the spinebesides being a much lighter representation , ours has an interpretation as a weak ordering , leading to the isomorphisms expressed in Propositions 1 - 3 .
Joint constituent and dependency parsing have been tackled by Carreras et al . ( 2008 ) and Rush et al . ( 2010 ) , but the resulting parsers , while accurate , are more expensive than a single c-parser .
Very recently , Kong et al. ( 2015 ) proposed a much cheaper pipeline in which d-parsing is performed first , followed by a c-parser constrained to be con- Petrov and Klein ( 2007 ) using the predicted POS tags provided by the organizers .
Crabb? and Seddah ( 2014 ) is the best non-reranking system in the shared task , and Bj?rkelund et al . ( 2014 ) the ensemble and reranking - based system which won the official task .
We report their published scores .
sistent with the predicted d-structure .
Our work differs in which we do not need to run a c-parser in the second stage - instead , the d-parser already stores constituent information in the arc labels , and the only necessary post-processing is to recover unary nodes .
Another advantage of our method is that it can be readily used for discontinuous parsing , while their constrained CKY algorithm can only produce continuous parses .
Conclusion
We proposed a reduction technique that allows to implement a c-parser when only a d-parser is given .
The technique is applicable to any d-parser , regardless of its nature or kind .
This reduction was accomplished by endowing d-trees with a weak order relation , and showing that the resulting class of head-ordered d-trees is isomorphic to constituent trees .
We showed empirically that the our reduction leads to highly - competitive c-parsers for English and for eight morphologically rich languages ; and that it outperforms the current state of the art in discontinuous parsing of German .
Figure 3 : 3 Figure 3 : Transformation of a strictly - ordered d-tree into a binary c-tree .
Each node is split into a linked list forming a spine , to which modifiers are attached in order .
