title
abstract
Introduction
This volume contains papers describing systems submitted to the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies , and two overview papers : one summarizing the main task , its features , evaluation methodology for the main and additional metrics , and some interesting observations about the submitted systems and the task as a whole ; the other overview paper discusses a complementary task focusing on Extrinsic Parser Evaluation ( EPE ) .
This Shared Task ( http://universaldependencies.org/conll18/) is an extension of the CoNLL 2017 Shared Task , with certain important differences .
Like in the previous year , the data come from the Universal Dependencies project ( http://universaldependencies.org), which provides annotated treebanks for a large number of languages using the same annotation scheme .
The number of treebanks in the task ( 82 ) is similar to the previous year ( 81 ) , but some treebanks from the 2017 task were not included in the present task , and some new treebanks were added instead .
The datasets are samples from 57 different languages ( all languages from the previous year , and eight new languages ) .
In comparison to 2017 , there were more low-resource languages with extremely little training data , calling for cross-lingual transfer techniques .
Unlike 2017 , none of the low-resource languages were " surprise languages " .
Participants had to process all the test sets .
The TIRA platform has been used for evaluation , as was the case already for the CoNLL 2015 , 2016 and 2017 Shared Tasks , meaning that participants had to provide their code on a designated virtual machine to be run by the organizers to produce official results .
However , test data have been published after the official evaluation period , and participants could run their systems at home to produce additional results they were allowed to include in the system description papers .
The systems were ranked according to three main evaluation metrics - LAS ( Labeled Attachment Score ) , MLAS ( Morphology - aware Labeled Attachment Score ) , and BLEX ( BiLEXical Dependency Score ) .
Like last year , participating systems minimally had to find labeled syntactic dependencies between words .
In addition , this year 's task featured new metrics that also scored a system 's capacity to predict a morphological analysis of each word , including a part- of-speech tag , morphological features , and a lemma .
Regardless of metric , the assumption was that the input should be raw text , with no goldstandard word or sentence segmentation , and no gold -standard morphological annotation .
However , for teams who wanted to concentrate on one or more subtasks , segmentation and morphology predicted by a baseline system was made available .
The complementary EPE task seeks to provide better estimates of the relative utility of different parsers for a variety of downstream applications that depend centrally on the analysis of grammatical structure , viz .
biomedical event extraction , negation resolution , and fine- grained opinion analysis .
EPE 2018 was organized as an optional add - on exercise to the core task : the submitted systems were applied to extra texts , about 1.1 million tokens of English .
It is interesting to see to what degree different intrinsic evaluation metrics from the core task correlate with end-to - end EPE results , and comparison to earlier EPE campaigns - with other types of dependency representations and additional sources of training data-further helps to put the core task into perspective .
A total of 25 systems ran successfully and have been ranked ( http://universaldependencies .
org/conll18/results.html ) ; 17 teams submitted parser outputs for the EPE test set .
While there are clear overall winners in each of the evaluation metrics , we would like to thank all participants for working hard on their submissions and adapting their systems not only to the datasets available , but also to the evaluation platform .
We would like to thank all of them for their effort , since it is the participants who are the core of any shared task 's success .
We would like to thank the CoNLL organizers for their support and the reviewers for helping to improve the submitted system papers .
Special thanks go to Martin Potthast of the TIRA platform for handling such a large number of systems , running often for several hours each , and for being very responsive and helpful to us and all system participants , round the clock during the evaluation phase and beyond .
We also thank to the 200 + people working on the Universal Dependencies project during the past four years , without whom there would be no data .
Daniel Zeman , Jan Haji ? , Martin Popel , Milan Straka , Joakim Nivre , Filip Ginter and Slav Petrov , the organizers of the CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies Stephan Oepen , main organizer of the EPE 2018 task Prague , September 2018
