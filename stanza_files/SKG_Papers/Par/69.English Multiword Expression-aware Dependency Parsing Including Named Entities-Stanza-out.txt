title
English Multiword Expression - aware Dependency Parsing Including Named Entities
abstract
Because syntactic structures and spans of multiword expressions ( MWEs ) are independently annotated in many English syntactic corpora , they are generally inconsistent with respect to one another , which is harmful to the implementation of an aggregate system .
In this work , we construct a corpus that ensures consistency between dependency structures and MWEs , including named entities .
Further , we explore models that predict both MWEspans and an MWE - aware dependency structure .
Experimental results show that our joint model using additional MWEspan features achieves an MWE recognition improvement of 1.35 points over a pipeline model .
Introduction
To solve complex Natural Language Processing ( NLP ) tasks that require deep syntactic analysis , various levels of annotation such as parse trees and named entities ( NEs ) must be consistent with one another ( Finkel and Manning , 2009 ) .
Otherwise , it is usually impossible to combine these pieces of information effectively .
However , the standard syntactic corpus of English , Penn Treebank , is not concerned with consistency between syntactic trees and spans of multiword expressions ( MWEs ) .
In Penn Treebank , that is , an MWE - span does not always correspond to a span dominated by a single non-terminal node .
Therefore , word - based dependency structures converted from Penn Treebank are generally inconsistent with MWE - spans ( Figure 1a ) .
To mitigate this inconsistency , Kato et al . ( 2016 ) estab - Figure 1 : A word- based and an MWE - aware dependency structure .
In the former , a span of an MWE ( " a number of " ) does not correspond to any subtree .
The MWE is represented as a single node in the latter structure .
lishes each span of functional MWEs 1 as a subtree of a phrase structure in the Wall Street Journal portion of Ontonotes ( Pradhan et al. , 2007 ) .
To pursue this direction further , we construct a corpus such that dependency structures are consistent with MWEs , by extending Kato et al . ( 2016 ) 's corpus 2 .
As is the case with their corpus , each MME is a syntactic unit in an MWE - aware dependency structure from our corpus ( Figure 1 b ) .
Moreover , our corpus includes not only functional MWEs but also NEs .
Because NEs are highly productive and occur more frequently than functional MWEs , they are difficult to cover in a dictionary .
Consistency between NE -spans and phrase structures is not guaranteed because they are independently annotated in most syntactic corpora .
1 By functional MWEs , we mean MWEs that function either as prepositions , conjunctions , determiners , pronouns , or adverbs .
2 We release our dependency corpus at https : //github.com/naist-cl-parsing / mwe-aware-dependency .
MWE - aware phrase structures will be distributed from LDC as a part of LDC2017T01 .
For instance , in Figure 2 , an NE - span is " Board of Investment , " which is inconsistent with the syntactic tree .
Therefore , we resolve this inconsistency by modifying phrase structures locally and establishing each NE as a subtree .
Furthermore , to evaluate the constructed corpus , we explore pipeline and joint models that predict both MWE - spans and an MWE - aware dependency tree 3 .
Our experimental results show that the proposed joint model with additional MWEspan features achieves an MWE recognition improvement of 1.35 points over the pipeline model .
Next , we describe the establishment of each NE as a subtree .
If an NE - span does not correspond to any non-terminal in a phrase structure , there are two possibilities : ( A ) the NE - span corresponds to multiple contiguous children of a subtree , or ( B ) the NE -span has crossing brackets with the spans in the parse tree ( Finkel and Manning , 2009 ; Kato et al. , 2016 ) . In Case ( A ) , we insert a new non-terminal ( " MWE NNP " ) that governs the NE-span 7 . In Case ( B ) , many instances correspond to a noun phrase ( NP ) comprised of a nested NP and a prepositional phrase ( Figure 2 ) .
In the main NP , a modifier , such as a determiner , an adjective , or a possessive NP , precedes an NE .
For these instances , according to Finkel and Manning ( 2009 ) , we reduce Case ( B ) to Case ( A ) by moving the modifier from the nested NP to the main NP .
Then , we establish each NE as a subtree by inserting an MWE - specific non-terminal .
Furthermore , in some instances it is more reasonable to enlarge NE - spans than to modify phrase structures .
As a typical example , there is an NE annotation that covers only part of a coordination structure , such as " Peter and Edward Bronfman , " where " Edward Bronfman " is annotated as an NE .
In this case , we extend an original NE - span to the whole coordination structure .
We show the statistics for the corpus in Table 1 8 .
This corpus has 27,949 MWE instances in 37,015 sentences .
A histogram Figure 3 : In the joint model , we directly infer an MWE - aware dependency tree in which an MWE ( " a number of " ) is represented as a head-initial structure by a dependency parser .
MWE - aware Dependency Corpus tabling the consistency between MWE - spans and phrase structures is shown in Table 2 .
For treeto-dependency conversion , we first replace a subtree corresponding to an MWE by a preterminal node and its child node .
The preterminal node has an MWE - level POS ( MWE POS ) tag .
The child node is generated by joining all components of the MWE with underscores .
We then convert a phrase structure into a Stanford -style dependency structure ( Marneffe and Manning , 2008 ) ( Figure 1 b ) .
Models for MWE identification and MWE - aware dependency parsing
In this section , we explore models that predict both MWE - spans and an MWE - aware dependency structure ( Figure 1 b ) .
Pipeline Model
The pipeline model involves the following three steps .
First , BIO tags encoding MWE - spans and MWE POS tags , such as " B NNP " and " I DT " are predicted by a sequential labeler based on Conditional Random Fields ( CRFs ) ( Lafferty et al. , 2001 ) .
Second , tokens belonging to each predicted MWE - span are concatenated into a single node .
Finally , an MWE - based dependency structure ( Figure 1 b ) is predicted by an arc-eager transition - based parser .
For the CRFs , in addition to word-form and character - based features , we use 1 - to 3 - gram features based on dictionaries of functional MWEs and NEs within 5 - word windows from a target token .
For a dictionary of functional MWEs , we use the dictionary by Shigeto et al . ( 2013 ) ( Section 2 ) .
Meanwhile , we create a dictionary of NEs from a title list of English Wikipedia articles , excepting stop words , provided by UniNE 9 .
Regarding parsing features , we use baseline features and rich non-local features proposed by Zhang and Nivre ( 2011 ) .
Joint Model
In the proposed joint model , MWE - spans and MWE POS tags are encoded as dependency labels , and conventional word - based dependency parsing is performed by an arc-eager transitionbased parser .
We use the same parsing features used in the pipeline model .
We convert MWEs in MWE - aware dependency structures ( Figure 1 b ) to head-initial structures ( Figure 3 ) that encode MWE - spans and MWE POS tags .
Note that this representation is similar to Universal Dependency ( McDonald et al. , 2013 ) .
When parsing , we use constraints based on a history of transitions and the dictionary of functional MWEs .
This is done to avoid invalid dependency trees .
Because NEs are highly productive , we do not use a constraint regarding NEs .
Joint ( + dict )
We designed additional features based on matches with dictionaries of NEs and functional MWEs .
Hereafter , we refer to the joint model coupled with these additional features as joint ( + dict ) .
For instance , given a sentence that starts with " a number of cities , " the additional features are as follows : a / B DT , number / I DT , of / I DT , cities / O. Based on these additional features , we extend the baseline features proposed by Zhang and Nivre ( 2011 ) to develop MWE - specific features whose atomic features include not only words and word- level POS tags , but also BIO tags encoding MWE - spans and MWE POS tags .
Joint ( + pred span )
Because dictionary matching is not concerned with context , in this setting , we use MWE - spans and MWE POS tags predicted by CRF , rather than dictionary matching .
Hereafter , we refer to this as joint ( + pred span ) .
By using features extracted from CRF predictions , we can mitigate error propagation from sequential labeling and consider information from a full sentence .
Moreover , we can alleviate difficulties in predicting MWE - spans and MWE POS tags encoded as head-initial structures ( Figure 3 ) by the parser .
Experimental Setting
We the POS tags predicted by the Stanford POS tagger ( Toutanova et al. , 2003 ) 10 . For the pipeline model and joint ( + pred span ) , we used MWEspans and MWE POS tags predicted by CRF 11 .
For dependency parsing , we used Redshift ( Honnibal et al. , 2013 ) for all models , with a beam size of 16 for decoding .
For training , we removed non-projective dependency trees .
For testing , we parsed all sentences .
To evaluate parsing , we used unlabeled and labeled attachment scores ( UAS / LAS ) 12 .
For the pipeline model , we converted each concatenated token corresponding to an MWE into a head-initial structure and compared this with the gold tree .
For the joint model , we directly compared a predicted tree with the gold tree .
To evaluate MWE recognition , we used the F-measure for untagged / tagged MWEs ( FUM / FTM ) 13 .
For the pipeline model , we compared the gold MWEs with predictions by CRF .
For the proposed joint model , we compared the gold MWEs with predicted MWE - spans and 10 We used 20 - way jackknifing for the training split .
The test split was automatically tagged by the POS tagger trained on the training split .
11 We used 20 - way jackknifing for the training split .
The test split was automatically tagged by the sequential labeler trained on the training split .
12 When calculating UAS / LAS , we removed punctuation .
13 FUM only focuses on MWE - spans , whereas FTM focuses on both MWE - spans and MWE POS tags .
MWE POS tags represented as dependency labels .
Experimental Results and Discussion
We present the experimental results in Table 3 .
Comparing the joint model with the pipeline model , there is not much difference between these models regarding UAS / LAS for all sentences .
However , the former is 2.13 / 0.48 points worse than the latter in terms of UAS / LAS regarding the first tokens of MWEs ( 1269 in 34,526 tokens ) , and 2.37 / 2.53 points worse than the latter regarding FUM / FTM .
These results suggest that the performance of the joint model with no additional features at predicting dependencies inside and around MWEs is worse than the pipeline model .
One of the reasons for this is that the exploitation of headinitial structures in the joint model ( Figure 3 ) involves the addition of MWE - specific labels .
This results in an increase in the total number of dependency labels from 41 to 50 .
Because of this broader output space , more search errors can occur in the joint model compared with the pipeline model .
Moreover , a breakdown by type of MWE ( Table 4 ) shows that most differences in performance between these two models are related to functional MWEs .
These results suggest that constraints regarding functional MWEs during parsing ( 3.2 ) are harmful to the joint model with no additional features in terms of its performance with respect to functional MWEs .
By adding MWE - specific features to the joint model , however , we observe at least a 2.52 / 3.00 point improvement in terms of UAS / LAS regarding the first tokens of MWEs , and a 2.90 / 2.99 point improvement regarding FUM / FTM .
As a result , we obtain a 1.35 / 1.28 point improvement with joint ( + pred span ) compared with the pipeline model in terms of FUM / FTM .
A breakdown by type of MWE shows that the addition of MWEspecific features leads to a performance improvement , especially for functional MWEs ( Table 4 ) .
These results suggest that MWE - specific features are effective at both MWE recognition through dependency parsing and the prediction of dependencies connecting inside and outside of MWEs .
Comparing the joint ( + pred span ) with the joint ( + dict ) , the former is 0.40 / 0.55 points better than the latter in terms of UAS / LAS regarding the first tokens of MWEs , and 0.82 / 0.82 points better than the latter regarding FUM / FTM .
We can attribute this gain in performance to the additional features extracted from more accurate predictions of MWE - spans and MWE POS tags by CRF than those by dictionary matching .
Related Work Whereas French Treebank is available for French MWEs ( Abeill ?
et al. , 2003 ) , there have been only limited corpora for English MWE - aware dependency parsing .
Schneider et al. ( 2014 ) constructs an MWE - annotated corpus on English Web Treebank ( Bies et al. , 2012 ) .
However , this corpus is relatively small as training data for a parser , and its MWE annotations are not consistent with syntactic trees .
By contrast , our corpus covers the whole of the WSJ portion of Ontonotes and ensures consistency between MWE annotations and parse trees .
Korkontzelos and Manandhar ( 2010 ) reports an improvement in base - phrase chunking by pregrouping MWEs as words - with-spaces .
They focus on compound nouns , adjective -noun constructions , and named entities .
However , they use gold MWE - spans , and this is not a realistic setting .
By contrast , we use predicted MWE - spans .
Three works concerned with a French MWEaware syntactic parsing are relevant .
First , Green et al. ( 2013 ) proposes a method for recognizing contiguous MWEs as a part of constituency parsing by using MWE - specific non-terminals .
They investigate a CFG - based model and a model based on tree-substitution grammars .
Second , Candito and Constant ( 2014 ) compares several architectures for graph - based dependency parsing and MWE recognition , in which MWE recognition is conducted before , during , and after parsing .
Finally , Nasr et al. ( 2015 ) explores a joint model of MWE recognition and dependency parsing .
They focus on complex function words .
In terms of data representation , they adopt one similar to ours , insofar as the components of an MWE are linked by dependency edges whose labels are MWEspecific .
Conclusion
We constructed a corpus that ensures consistency in Ontonotes between dependency structures and English MWEs , including named entities .
Furthermore , we explored models that can predict both MWE - spans and an MWE - aware dependency structure .
Our experiments show that by using additional MWE - span features , our joint model achieves an MWE recognition improvement of 1.35 points over the pipeline model . ( a ) a word - based dependency structure ( b ) an MWE - aware dependency structure
