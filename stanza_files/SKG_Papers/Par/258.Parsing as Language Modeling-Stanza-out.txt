title
Parsing as Language Modeling
abstract
We recast syntactic parsing as a language modeling problem and use recent advances in neural network language modeling to achieve a new state of the art for constituency Penn Treebank parsing - 93.8 F 1 on section 23 , using 2 - 21 as training , 24 as development , plus tri-training .
When trees are converted to Stanford dependencies , UAS and LAS are 95.9 % and 94.1 % .
Introduction
Recent work on deep learning syntactic parsing models has achieved notably good results , e.g. , Dyer et al . ( 2016 ) with 92.4 F 1 on Penn Treebank constituency parsing and Vinyals et al . ( 2015 ) with 92.8 F 1 .
In this paper we borrow from the approaches of both of these works and present a neural - net parse reranker that achieves very good results , 93.8 F 1 , with a comparatively simple architecture .
In the remainder of this section we outline the major difference between this and previous workviewing parsing as a language modeling problem .
Section 2 looks more closely at three of the most relevant previous papers .
We then describe our exact model ( Section 3 ) , followed by the experimental setup and results ( Sections 4 and 5 ) .
There is a one- to - one mapping between a tree and its sequential form .
( Part - of -speech tags are not used . )
Language Modeling Formally , a language model ( LM ) is a probability distribution over strings of a language : P ( x ) = P ( x 1 , ? ? ? , x n ) = n t=1 P ( x t |x 1 , ? ? ? , x t?1 ) , ( 1 ) where x is a sentence and t indicates a word position .
The efforts in language modeling go into computing P ( x t |x 1 , ? ? ? , x t?1 ) , which as described next is useful for parsing as well .
Parsing as Language Modeling A generative parsing model parses a sentence ( x ) into its phrasal structure ( y ) according to argmax y ?Y( x ) P ( x , y ) , where Y( x ) lists all possible structures of x .
If we think of a tree ( x , y ) as a sequence ( z ) ( Vinyals et al. , 2015 ) as illustrated in Figure 1 , we can define a probability distribution over ( x , y ) as follows : P ( x , y ) = P ( z ) = P ( z 1 , ? ? ? , z m ) = m t=1 P ( z t |z 1 , ? ? ? , z t?1 ) , ( 2 ) which is equivalent to Equation ( 1 ) .
We have reduced parsing to language modeling and can use language modeling techniques of estimating P ( z t |z 1 , ? ? ? , z t?1 ) for parsing .
Previous Work
We look here at three neural net ( NN ) models closest to our research along various dimensions .
The first ( Zaremba et al. , 2014 ) gives the basic language modeling architecture that we have adopted , while the other two ( Vinyals et al. , 2015 ; Dyer et al. , 2016 ) are parsing models that have the current best results in NN parsing .
LSTM -LM
The LSTM -LM of Zaremba et al. ( 2014 ) turns ( x 1 , ? ? ? , x t?1 ) into h t , a hidden state of an LSTM ( Hochreiter and Schmidhuber , 1997 ; Gers et al. , 2003 ; Graves , 2013 ) , and uses h t to guess x t : P ( x t |x 1 , ? ? ? , x t?1 ) = P ( x t | h t ) = softmax ( W h t ) [ x t ] , where W is a parameter matrix and [ i ] indexes ith element of a vector .
The simplicity of the model makes it easily extendable and scalable , which has inspired a character - based LSTM - LM that works well for many languages ( Kim et al. , 2016 ) and an ensemble of large LSTM - LMs for English with astonishing perplexity of 23.7 ( Jozefowicz et al. , 2016 ) .
In this paper , we build a parsing model based on the LSTM - LM of Zaremba et al . ( 2014 ) . et al. ( 2015 ) observe that a phrasal structure ( y ) can be expressed as a sequence and build a machine translation parser ( MTP ) , a sequence - tosequence model , which translates x into y using a conditional probability :
MTP
Vinyals P ( y|x ) = P (y 1 , ? ? ? , y l |x ) = l t=1 P ( y t |x , y 1 , ? ? ? , y t?1 ) , where the conditioning event ( x , y 1 , ? ? ? , y t?1 ) is modeled by an LSTM encoder and an LSTM decoder .
The encoder maps x into h e , a set of vectors that represents x , and the decoder obtains a summary vector ( h t ) which is concatenation of the decoder 's hidden state ( h d t ) and weighted sum of word representations ( n i=1 ? i h e i ) with an alignment vector ( ? ) .
Finally the decoder predicts y t given h t .
Inspired by MTP , our model processes sequential trees .
RNNG Recurrent Neural Network Grammars ( RNNG ) , a generative parsing model , defines a joint distribution over a tree in terms of actions the model takes to generate the tree ( Dyer et al. , 2016 ) : P ( x , y ) = P ( a ) = m t=1 P ( a t | a 1 , ? ? ? , a t?1 ) , ( 3 ) where a is a sequence of actions whose output precisely matches the sequence of symbols in z , which implies Equation ( 3 ) is the same as Equation ( 2 ) .
RNNG and our model differ in how they compute the conditioning event ( z 1 , ? ? ? , z t?1 ) : RNNG combines hidden states of three LSTMs that keep track of actions the model has taken , an incomplete tree the model has generated and words the model has generated whereas our model uses one LSTM 's hidden state as shown in the next section .
Model
Our model , the model of Zaremba et al . ( 2014 ) applied to sequential trees and we call LSTM - LM from now on , is a joint distribution over trees : P ( x , y ) = P ( z ) = m t=1 P ( z t |z 1 , ? ? ? , z t?1 ) = m t=1 P ( z t |h t ) = m t=1 softmax ( W h t ) [ z t ] , where h t is a hidden state of an LSTM .
Due to lack of an algorithm that searches through an exponentially large phrase -structure space , we use an n-best parser to reduce Y( x ) to Y ( x ) , whose size is polynomial , and use LSTM - LM to find y that satisfies argmax y ?Y ( x ) P ( x , y ) . ( 4 )
Hyper-parameters
The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50 .
We initialize starting states with previous minibatch 's last hidden states ( Sutskever , 2013 ) .
The forget gate bias is initialized to be one ( Jozefowicz et al. , 2015 ) and the rest of model parameters are sampled from U (?0.05 , 0.05 ) .
Dropout is applied to non-recurrent connections ( Pham et al. , 2014 ) and gradients are clipped when their norm is bigger than 20 ( Pascanu et al. , 2013 ) .
The learning rate is 0.25 ? 0.85 max ( ?15 , 0 ) where is an epoch number .
For simplicity , we use vanilla softmax over an entire vocabulary as opposed to hierarchical softmax ( Morin and Bengio , 2005 ) or noise contrastive estimation ( Gutmann and Hyv?rinen , 2012 ) .
Experiments
We describe datasets we use for evaluation , detail training and development processes .
1
Data
We use the Wall Street Journal ( WSJ ) of the Penn Treebank ( Marcus et al. , 1993 ) for training ( 2 - 21 ) , development ( 24 ) and testing ( 23 ) and millions of auto-parsed " silver " trees ( McClosky et al. , 2006 ; Huang et al. , 2010 ; Vinyals et al. , 2015 ) for tritraining .
To obtain silver trees , we parse the entire section of the New York Times ( NYT ) of the fifth Gigaword ( Parker et al. , 2011 ) with a product of eight Berkeley parsers ( Petrov , 2010 ) 2 and ZPar ( Zhu et al. , 2013 ) and select 24 million trees on which both parsers agree ( Li et al. , 2014 ) .
We do not resample trees to match the sentence length distribution of the NYT to that of the WSJ ( Vinyals et 1
The code and trained models used for experiments are available at github.com / cdg720/emnlp2016 .
2
We use the reimplementation by Huang et al . ( 2010 ) .
( Charniak , 2000 ) performed better when trained on all of 24 million trees than when trained on resampled two million trees .
Given x , we produce Y ( x ) , 50 - best trees , with Charniak parser and find y with LSTM -LM as Dyer et al . ( 2016 ) do with their discriminative and generative models .
3
Training and Development
Supervision
We unk words that appear fewer than 10 times in the WSJ training ( 6,922 types ) and drop activations with probability 0.7 .
At the beginning of each epoch , we shuffle the order of trees in the training data .
Both perplexity and F 1 of LSTM -LM ( G ) improve and then plateau ( Figure 2 ) .
Perplexity , the Base Final Vinyals et al . ( 2015 )
Semi-supervision
We unk words that appear at most once in the training ( 21,755 types ) .
We drop activations with probability 0.45 , smaller than 0.7 , thanks to many silver trees , which help regularization .
We train LSTM -LM ( GS ) on the WSJ and a different set of 400,000 NYT trees for each epoch except for the last one during which we use the WSJ only .
Training takes 26 epochs and 68 hours on a Titan X. LSTM -LM ( GS ) achieves 92.5 F 1 on the development .
Results
Supervision
As shown in Table 2 , with 92.6 F 1 LSTM -LM ( G ) outperforms an ensemble of five MTPs ( Vinyals et al. , 2015 ) and RNNG ( Dyer et al. , 2016 ) , both of which are trained on the WSJ only .
Semi-supervision
We compare LSTM -LM ( GS ) to two very strong semi-supervised NN parsers : an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus 4 ( HC ) ( Vinyals et al. , 2015 ) ; and an ensemble of six one - to - many sequence models trained on the HC and 4.5 millions of English - German translation sentence pairs ( Luong et al. , 2016 ) .
We also compare LSTM -LM ( GS ) to best performing non -NN parsers in the literature .
Parsers ' parsing performance along with their training data is reported in Table 3 . LSTM -LM ( GS ) outperforms all the other parsers with 93.1 F 1 .
Improved Semi-supervision
Due to search errors - good trees are missing in 50 - best trees - in Charniak ( G ) , our supervised and semi-supervised models do not exhibit their full potentials when Charniak ( G ) provides Y ( x ) .
To mitigate the search problem , we tri-train Charniak ( GS ) on all of 24 million NYT trees in addition to the WSJ , to yield Y ( x ) .
As shown in Table 3 , both LSTM -LM ( G ) and LSTM - LM ( GS ) are affected by the quality of Y ( x ) .
A single LSTM -LM ( GS ) together with Charniak ( GS ) reaches 93.6 and an ensemble of eight LSTM - LMs ( GS ) with Charniak ( GS ) achieves a new state of the art , 93.8 F 1 .
When trees are converted to Stanford dependencies , 5 UAS and LAS are 95.9 % and 94.1 % , 6 more than 1 % higher than those of the state of the art dependency parser ( Andor et al. , 2016 ) .
Why an indirect method ( converting trees to dependencies ) is more accurate than a direct one ( dependency parsing ) remains unanswered ( Kong and Smith , 2014 ) .
Conclusion
The generative parsing model we presented in this paper is very powerful .
In fact , we see that a generative parsing model , LSTM - LM , is more effective than discriminative parsing models ( Dyer et al. , 2016 ) .
We suspect building large models with character embeddings would lead to further improvement as in language modeling ( Kim et al. , 2016 ; Jozefowicz et al. , 2016 ) .
We also wish to develop a complete parsing model using the LSTM - LM framework .
Figure 1 : A tree ( a ) and its sequential form ( b ) .
There is a one- to- one mapping between a tree and its sequential form .
( Part - of -speech tags are not used . )
