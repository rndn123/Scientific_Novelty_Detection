title
Approximate PCFG Parsing Using Tensor Decomposition
abstract
We provide an approximation algorithm for PCFG parsing , which asymptotically improves time complexity with respect to the input grammar size , and prove upper bounds on the approximation quality .
We test our algorithm on two treebanks , and get significant improvements in parsing speed .
Introduction
The problem of speeding - up parsing algorithms based on probabilistic context-free grammars ( PCFGs ) has received considerable attention in recent years .
Several strategies have been proposed , including beam-search , best-first and A * .
In this paper we focus on the standard approach of approximating the source PCFG in such a way that parsing accuracy is traded for efficiency .
Nederhof ( 2000 ) gives a thorough presentation of old and novel ideas for approximating nonprobabilistic CFGs by means of finite automata , on the basis of specialized preprocessing of selfembedding structures .
In the probabilistic domain , approximation by means of regular grammars is also exploited by Eisner and Smith ( 2005 ) , who filter long-distance dependencies on - the-fly .
Beyond finite automata approximation , Charniak et al . ( 2006 ) propose a coarse- to -fine approach in which an approximated ( not necessarily regular ) PCFG is used to construct a parse forest for the input sentence .
Some statistical parameters are then computed on such a structure , and exploited to filter parsing with the non-approximated grammar .
The approach can also be iterated at several levels .
In the non-probabilistic setting , a similar filtering ap- proach was also proposed by Boullier ( 2003 ) , called " guided parsing . "
In this paper we rely on an algebraic formulation of the inside-outside algorithm for PCFGs , based on a tensor formulation developed for latent - variable PCFGs in .
We combine the method with known techniques for tensor decomposition to approximate the source PCFG , and develop a novel algorithm for approximate PCFG parsing .
We obtain improved time upper bounds with respect to the input grammar size for PCFG parsing , and provide error upper bounds on the PCFG approximation , in contrast with existing heuristic methods .
Preliminaries
This section introduces the special representation for probabilistic context - free grammars that we adopt in this paper , along with the decoding algorithm that we investigate .
For an integer i ?
1 , we let [ i ] = { 1 , 2 , . . . , i} .
Probabilistic Context - Free Grammars
We consider context-free grammars ( CFGs ) in Chomsky normal form , and denote them as ( N , L , R ) where : A probabilistic CFG ( PCFG ) is a CFG associated with a set of parameters defined as follows : ?
For each ( a ? b c ) ?
R , we have a parameter p( a ? b c | a ) . ?
For each ( a ? x ) ?
R , we have a parameter p( a ? x | a ) . ?
For each a ?
N , we have a parameter ?
a , which is the probability of a being the root symbol of a derivation .
The parameters above satisfy the following normalization conditions : ( a? b c ) ? R p( a ? b c | a ) + ( a? x ) ?
R p( a ? x | a ) = 1 , for each a ?
N , and a?N ? a = 1 .
The probability of a tree ? deriving a sentence in the language , written p( ? ) , is calculated as the product of the probabilities of all rule occurrences in ? , times the parameter ?
a where a is the symbol at the root of ? .
Tensor Form of PCFGs A three - dimensional tensor C ? R ( m?m?m ) is a set of m 3 parameters C i , j , k for i , j , k ? [ m ] .
In what follows , we associate with each tensor three functions , each mapping a pair of vectors in R m into a vector in R m .
Definition 1 Let C ? R ( m?m? m ) be a tensor .
Given two vectors y 1 , y 2 ?
R m , we let C(y 1 , y 2 ) be the m-dimensional row vector with components : [ C(y 1 , y 2 ) ] i = j? [ m ] , k? [ m ]
C i , j , k y 1 j y 2 k .
We also let C ( 1,2 ) (y 1 , y 2 ) be the m-dimensional column vector with components : [ C ( 1,2 ) ( y 1 , y 2 ) ] k = i ? [ m ] , j? [ m ]
C i , j , k y 1 i y 2 j .
Finally , we let C ( 1,3 ) (y 1 , y 2 ) be the m-dimensional column vector with components : [ C ( 1,3 ) ( y 1 , y 2 ) ] j = i ? [ m ] , k? [ m ]
C i , j , k y 1 i y 2 k .
For two vectors x , y ?
R m we denote by x y ?
R m the Hadamard product of x and y , i.e. , [ x y ]
i = x i y i .
Finally , for vectors x , y , z ?
R m , xy z is the tensor D ? R m?m? m where D i , j , k = x i y j z k ( this is analogous to the outer product : [ xy ] i , j = x i y j ) .
We extend the parameter set of our PCFG such that p( a ? b c | a ) = 0 for all a ?
b c not in R , and p( a ? x | a ) = 0 for all a ?
x not in R .
We also represent each a ?
N by a unique index in [ m ] , and we represent each x ?
L by a unique index in [ n ] : it will always be clear from the context whether these indices refer to a nonterminal in N or else to a word in L .
In this paper we assume a tensor representation for the parameters p( a ? b c | a ) , and we denote by T ? R m?m? m a tensor such that : T a, b, c p( a ? b c | a ) .
Similarly , we denote by Q ? R m?n a matrix such that : Q a , x p( a ? x | a ) .
The root probabilities are denoted using a vector ? ?
R m?1 such that ?
a is defined as before .
Minimum Bayes - Risk Decoding Let z = x 1 ? ? ? x N be some input sentence ; we write T ( z ) to denote the set of all possible trees for z .
It is often the case that parsing aims to find the highest scoring tree ?
* for z according to the underlying PCFG , also called the " Viterbi parse : " ? * = argmax ? ?T ( z ) p( ? ) Goodman ( 1996 ) noted that Viterbi parsers do not optimize the same metric that is usually used for parsing evaluation ( Black et al. , 1991 ) .
He suggested an alternative algorithm , which he called the " Labelled Recall Algorithm , " which aims to fix this issue .
Goodman 's algorithm has two phases .
In the first phase it computes , for each a ?
N and for each substring x i ? ? ?
x j of z , the marginal ?( a , i , j ) defined as : ?( a , i , j ) = ? ?T ( z ) : ( a , i , j ) ? p( ? ) .
Here we write ( a , i , j ) ? ? if nonterminal a spans words x i ? ? ?
x j in the parse tree ? . m?1 ) . Data structures : Inputs : Sentence x 1 ? ? ? x N , PCFG ( N , L , R ) , pa- rameters T ? R ( m?m?m ) , Q ? R ( m?n ) , ? ? R ( ? Each ?( a , i , j ) ?
R for a ?
N , i , j ?
[ N ] , i ? j , is a marginal probability .
?
Each ? i , j ? R for i , j ?
[ N ] , i ? j , is the highest score for a tree spanning substring x i ? ? ? x j . Algorithm : ( Marginals ) ?a ?
N , ?i , j ?
[ N ] , i ? j , compute the marginals ?( a , i , j ) using the inside-outside algorithm .
( Base case ) ?i ? [ N ] , ( 1996 ) .
The algorithm in this figure finds the highest score for a tree which maximizes labelled recall .
The actual parsing algorithm would use backtrack pointers in the score computation to return a tree .
These are omitted for simplicity . ?
i , i = max ( a?x i ) ? R ?( a , i , i ) ( Maximize Labelled Recall ) ?i , j ?
[ N ] , i < j , ?
i , j = max a?N ?( a , i , j ) + max i?k<j ? i , k + ? k + 1 , j
The second phase includes a dynamic programming algorithm which finds the tree ? * that maximizes the sum over marginals in that tree : ? * = argmax ? ?T ( z ) ( a , i , j ) ? ?( a , i , j ) .
Goodman 's algorithm is described in Figure 1 .
As Goodman notes , the complexity of the second phase ( " Maximize Labelled Recall , " which is also referred to as " minimum Bayes risk decoding " ) is O( N 3 + mN 2 ) .
There are two nested outer loops , each of order N , and inside these , there are two separate loops , one of order m and one of order N , yielding this computational complexity .
The reason for the linear dependence on the number of nonterminals is the lack of dependence on the actual grammar rules , once the marginals are computed .
In its original form , Goodman 's algorithm does not enforce that the output parse trees are included in the tree language of the PCFG , that is , certain combinations of children and parent nonterminals may violate the rules in the grammar .
In our experiments we departed from this , and changed Goodman 's algorithm by incorporating the grammar into the dynamic programming algorithm in Figure 1 .
The reason this is important for our experiments is that we binarize the grammar prior to parsing , and we need to enforce the links between the split nonterminals ( in the binarized grammar ) that refer to the same syntactic category .
See Matsuzaki et al. ( 2005 ) for more details about the binarization scheme we used .
This step changes the dynamic programming equation of Goodman to be linear in the size of the grammar ( figure 1 ) .
However , empirically , it is the insideoutside algorithm which takes most of the time to compute with Goodman 's algorithm .
In this paper we aim to asymptotically reduce the time complexity of the calculation of the inside-outside probabilities using an approximation algorithm .
Tensor Formulation of the Inside-Outside Algorithm
At the core of our approach lies the observation that there is a ( multi ) linear algebraic formulation of the inside-outside algorithm .
It can be represented as a series of tensor , matrix and vector products .
A similar observation has been made for latent- variable PCFGs and hidden Markov models , where only matrix multiplication is required ( Jaeger , 2000 ) . use this observation together with tensor decomposition to improve the speed of latent - variable PCFG parsing .
The representation of the inside-outside algorithm in tensor form is given in Figure 2 .
For example , if we consider the recursive equation for the inside probabilities ( where ? i , j is a vector varying over the nonterminals in the grammar , describing the inside probability for each nonterminal spanning words i to j ) : m?1 ) .
Data structures : ? i , j = j?1 k=i T ( ? i , k , ? k+1 , j ) Inputs : Sentence x 1 ? ? ? x N , PCFG ( N , L , R ) , pa- rameters T ? R ( m?m?m ) , Q ? R ( m?n ) , ? ? R ( ? Each ? i , j ? R 1 ? m , i , j ?
[ N ] , i ? j , is a row vector of inside terms ranging over a ?
N . ? Each ? i , j ? R m?1 , i , j ?
[ N ] , i ? j , is a column vector of outside terms ranging over a ?
N . ? Each ?( a , i , j ) ?
R for a ?
N , i , j ?
[ N ] , i ? j , is a marginal probability .
Algorithm : and then apply the tensor product from Definition 1 to this equation , we get that coordinate a in ?
i , j is defined recursively as follows : ( Inside base case ) ?i ? [ N ] , ?( a ? x i ) ? R , [?
i , i ] a = Q a , x ( Inside recursion ) ?i , j ?
[ N ] , i < j , ? i , j = j?1 k=i T ( ? i , k , ? k+1 , j ) ( Outside base case ) ?a ? N , [?
1 , N ] a = ? a ( Outside recursion ) ?i , j ? [ N ] , i ? j , ? i , j = i?1 k=1 T ( 1,2 ) ( ? k , j , ? k , i?1 ) + N k=j +1 T ( 1,3 ) ( ? i , k , ? j+1 , k ) ( Marginals ) ?a ?
N , ?i , j ? [ N ] , i ? j , ?( a , i , j ) = [ ?
i , j ] a ? [? i , j ] a [ ?
i , j ] a = j?1 k=i b , c T a, b , c ? ? i , k b ? ? k + 1 , j c = j?1 k=i b, c p( a ? b c | a ) ? ? i , k b ? ? k + 1 , j c , which is exactly the recursive definition of the inside algorithm .
The correctness of the outside recursive equations follows very similarly .
The time complexity of the algorithm in this case is O( m 3 N 3 ) .
To see this , observe that each tensor application takes time O ( m 3 ) .
Furthermore , the tensor T is applied O ( N ) times in the computation of each vector ? i , j and ?
i , j . Finally , we need to compute a total of O( N 2 ) inside and outside vectors , one for each substring of the input sentence .
Tensor Decomposition for the Inside-Outside Algorithm
In this section , we detail our approach to approximate parsing using tensor decomposition .
Tensor Decomposition
In the formulation of the inside-outside algorithm based on tensor T , each vector ? i , j and ?
i , j consists of m elements , where computation of each element requires time O ( m 2 ) .
Therefore , the algorithm has a O(m 3 ) multiplicative factor in its time complexity , which we aim to reduce by means of an approximate algorithm .
Our approximate method relies on a simple observation .
Given an integer r ? 1 , assume that the tensor T has the following special form , called " Kruskal form : " T = r i=1 ?
i u i v i w i . ( 1 ) In words , T is the sum of r tensors , where each tensor is obtained as the product of three vectors u i , v i and w i , together with a scalar ?
i . Exact Kruskal decomposition of a tensor is not necessarily unique .
See Kolda and Bader ( 2009 ) for discussion of uniqueness of tensor decomposition .
Consider now two vectors y 1 , y 2 ?
R m , associated with the inside probabilities for the left ( y 1 ) and right child ( y 2 ) of a given node in a parse tree .
Let us introduce auxiliary arrays U , V , W ? R r?m , with the i-th row being u i , v i and w i , respectively .
Let also ? = (?
1 , . . . , ? r ) .
Using the decomposition in Eq. ( 1 ) within Definition 1 we can express the array T (y 1 , y 2 ) as : T (y 1 , y 2 ) = r i=1 ?
i u i v i w i ( y 1 , y 2 ) = r i=1 ? i u i ( v i y 1 ) ( w i y 2 ) = U (?
V y 1 W y 2 ) . ( 2 ) The total complexity of the computation in Eq. ( 2 ) is now O ( rm ) .
It is well - known that an exact tensor decomposition for T can be achieved with r = m 2 ( Kruskal , 1989 ) .
In this case , there is no computational gain in using Eq. ( 2 ) for the inside calculation .
The minimal r required for an exact tensor decomposition can be smaller than m 2 .
However , identifying that minimal r is NP - hard ( H?astad , 1990 ) .
In this section we focused on the computation of the inside probabilities through vectors T ( ? i , k , ? k +1 , j ) .
Nonetheless , the steps above can be easily adapted for the computation of the outside probabilities through vectors T ( 1,2 ) ( ? k , j , ? k , i?1 ) and T ( 1,3 ) ( ? i , k , ? j+1 , k ) .
Approximate Tensor Decomposition
The PCFG tensor T will not necessarily have the exact decomposed form in Eq. ( 1 ) .
We suggest to approximate the tensor T by finding the closest tensor according to some norm over R m?m? m .
An example of such an approximate decomposition is the canonical polyadic decomposition ( CPD ) , also known as CANDECOMP / PARAFAC decomposition ( Carroll and Chang , 1970 ; Harshman , 1970 ; Kolda and Bader , 2009 ) .
Given an integer r , least squares CPD aims to find the nearest tensor in Kruskal form , minimizing squared error .
More formally , for a given tensor D ? R m?m? m , let | |D | | F = i , j , k D 2 i , j , k . Let the set of tensors in Kruskal form C r be : C r ={C ? R m?m? m | C = r i= 1 ?
i u i v i w i s.t. ? i ?
R , u i , v i , w i ?
R m , || u i || 2 = || v i || 2 = ||w i || 2 = 1 } .
The least squares CPD of C is a tensor ? such that ? ? argmin ? Cr ||C ? ?||
F . Here , we treat the argmin as a set because there could be multiple solutions which achieve the same accuracy .
There are various algorithms to perform CPD , such as alternating least squares , direct linear decomposition , alternating trilinear decomposition and pseudo alternating least squares ( Faber et al. , 2003 ) and even algorithms designed for sparse tensors ( Chi and Kolda , 2011 ) .
Most of these algorithms treat the problem of identifying the approximate tensor as an optimization problem .
Generally speaking , these optimization problems are hard to solve , but they work quite well in practice .
Parsing with Decomposed Tensors Equipped with the notion of tensor decomposition , we can now proceed with approximate tensor parsing in two steps .
The first is approximating the tensor using a CPD algorithm , and the second is applying the algorithms in Figure 1 and Figure 2 to do parsing , while substituting all tensor product computations with the approximate O( rm ) operation of tensor product .
This is not sufficient to get a significant speed - up in parsing time .
Re-visiting Eq. ( 2 ) shows that there are additional ways to speed - up the tensor application T in the context of the inside-outside algorithm .
The first thing to note is that the projections V y 1 and W y 2 in Eq. ( 2 ) can be cached , and do not have to be re-calculated every time the tensor is applied .
Here , y 1 and y 2 will always refer to an outside or an inside probability vector over the nonterminals in the grammar .
Caching these projections means that after each computation of an inside or outside probability , we can immediately project it to the necessary r-dimensional space , and then re-use this computation in subsequent application of the tensor .
The second thing to note is that the U projection in T can be delayed , because of rule of distributivity .
For example , the step in Figure 2 that computes the inside probability ?
i , j can be re-formulated as follows ( assuming an exact decomposition of T ) : ? i , j = j?1 k=i T ( ? i , k , ? k+1 , j ) = j?1 k=1 U (?
V ? i , k W ? k+1 , j ) = U j?1 k=1 ( ? V ? i , k W ? k +1 , j ) . ( 3 )
This means that projection through U can be done outside of the loop over splitting points in the sentence .
Similar reliance on distributivity can be used to speed - up the outside calculations as well .
The caching speed - up and the delayed projection speed - up make the approximate inside-outside computation asymptotically faster .
While na?ve application of the tensor yields an inside algorithm which runs in time O( rmN 3 ) , the improved algorithm runs in time O( rN 3 + rmN 2 ) .
Quality of Approximate Tensor Parsing
In this section , we give the main approximation result , that shows that the probability distribution induced by the approximate tensor is close to the original probability distribution , if the distance between the approximate tensor and the rule probabilities is not too large .
Denote by T ( N ) the set of trees in the tree language of the PCFG with N words ( any nonterminal can be the root of the tree ) .
Let T ( N ) be the set of pairs of trees ? = (?
1 , ? 2 ) such that the total number of binary rules combined in ?
1 and ?
2 is N ? 2 ( this means that the total number of words combined is N ) .
Let T be the approximate tensor for T .
Denote the probability distribution induced by T by p. 1 Define the vector ?(? ) such that [?( ? ) ]
a = T a, b , c ? p(? 1 | b ) ? p(? 2 | c ) where the root ?
1 is nonterminal b and the root of ?
2 is c. Similarly , define [ ?( ? ) ]
a = Ta , b , c ? p(? 1 | b ) ? p(? 2 | c ) . Define Z(a , N ) = ? ?T ( N ) [ ?( ? ) ]
a .
In addition , define D( a , N ) = ? ?T ( N ) | [ ?( ? ) ] a ? [?( ? ) ] a | and define F ( a , N ) = D ( a , N ) /Z( a , N ) .
De- fine ? = || T ? T || F . Last , define ? = min ( a? b c ) ?
R p( a ? b c | a ) .
Then , the following lemma holds : Lemma 1 For any a and any N , it holds : D ( a , N ) ? Z ( a , N ) ( 1 + ?/ ? ) N ? 1 Proof sketch :
The proof is by induction on N . Assuming that 1 + F ( b , k ) ? ( 1 + ?/ ? ) k and 1 + F ( c , N ? k ? 1 ) ? ( 1 + ?/ ? ) N ?k?1 for F defined as above ( this is the induction hypothesis ) , it can be shown that the lemma holds .
Lemma 2
The following holds for any N : ? ?T ( N ) | p ( ? ) ? p( ? ) | ? m ( 1 + ?/ ? ) N ? 1 Proof sketch : Using H?lder 's inequality and Lemma 1 and the fact that Z( a , N ) ? 1 , it follows that : ? ?T ( N ) | p ( ? ) ? p( ? ) | ? ? ?T ( N ) , a | [ ? ( ? ) ] a ? [ ?( ? ) ]
a | ? a Z( a , N ) ( 1 + ?/ ? ) N ? 1 ? m ( 1 + ?/ ? ) N ? 1 Then , the following is a result that explains how accuracy changes as a function of the quality of the tensor approximation : Theorem 1 For any N , and < 1/4 , it holds that if ? ? ?
2N m , then : ? ?T ( N ) | p ( ? ) ? p( ? ) | ?
Proof sketch :
This is the result of applying Lemma 2 together with the inequality ( 1 + y/t ) t ? 1 ? 2y for any t > 0 and y ? 1/2 .
We note that Theorem 1 also implicitly bounds the difference between a marginal ?( a , i , j ) and its approximate version .
A marginal corresponds to a sum over a subset of summands in Eq. ( 1 ) .
A question that remains at this point is to decide whether for a given grammar , the optimal ? that can be achieved is large or small .
We define : ? * r = min T ?Cr ||T ? T || F ( 4 )
The following theorem gives an upper bound on the value of ?
* r based on intrinsic property of the grammar , or more specifically T .
It relies on the fact that for three - dimensional tensors , where each dimension is of length m , there exists an exact decomposition of T using m 2 components .
Theorem 2 Let : T = m 2 i=1 ? * i u * i ( v * i ) ( w * i ) be an exact Kruskal decomposition of T such that || u * i || 2 = ||v * i || 2 = ||w * i || = 1 and ? * i ? ? * i+ 1 for i ? [ m 2 ? 1 ] .
Then , for a given r , it holds : ? * r ? m 2 i=r+1 |? * i | Proof : Let T be a tensor that achieves the minimum in Eq. ( 4 ) .
Define : T r = r i=1 ? * i u * i ( v * i ) ( w * i )
Then , noting that ?
* r is a minimizer of the norm difference between T and T and then applying the triangle inequality and then Cauchy - Schwartz inequality leads to the following chain of inequalities : ? * r = ||T ? T || F ? ||T ? T r || F = || m 2 i=r +1 ? * i u * i ( v * i ) ( w * i ) || F ? m 2 i=r +1 |? * i | ? || u * i ( v * i ) ( w * i ) || F = m 2 i=r +1 |? * i | as required .
Experiments
In this section , we describe experiments that demonstrate the trade- off between the accuracy of the tensor approximation ( and as a consequence , the accuracy of the approximate parsing algorithm ) and parsing time .
Experimental Setting
We compare the tensor approximation parsing algorithm versus the vanilla Goodman algorithm .
Both algorithms were implemented in Java , and the code for both is almost identical , except for the set of instructions which computes the dynamic programming equation for propagating the beliefs up in the tree .
This makes the clocktime comparison reliable for drawing conclusions about the speed of the algorithms .
Our implementation of the vanilla parsing algorithm is linear in the size of the grammar ( and not cubic in the number of nonterminals , which would give a worse running time ) .
In our experiments , we use the method described in Chi and Kolda ( 2011 ) for tensor decomposition .
2
This method is fast , even for large tensors , as long as they are sparse .
Such is the case with the tensors for our grammars .
We use two treebanks for our comparison : the Penn treebank ( Marcus et al. , 1993 ) and the Arabic treebank ( Maamouri et al. , 2004 ) .
With the Penn treebank , we use sections 2 - 21 for training a maximum likelihood model and section 22 for parsing , while for the Arabic treebank we divide the data into two sets , of size 80 % and 20 % , one is used for training a maximum likelihood model and the other is used for parsing .
The number of binary rules in the treebank grammar is 7,240 .
The number of nonterminals is 112 and the number of preterminals is 259 3 Unary rules are removed by collapsing non-terminal chains .
This increased the number of preterminals .
The number of binary rules in the Arabic treebank is significantly smaller and consists of 232 rules .
We run all parsing experiments on sentences of length ? 40. minals is 81 .
Results
Table 1 describes the results of comparing the tensor decomposition algorithm to the vanilla PCFG parsing algorithm .
The first thing to note is that the running time of the parsing algorithm is linear in r .
This indeed validates the asymptotic complexity of the insideoutside component in Goodman 's algorithm with the approximate tensors .
It also shows that most of the time during parsing is spent on the inside-outside algorithm , and not on the dynamic programming algorithm which follows it .
In addition , compared to the baseline which uses a vanilla CKY algorithm ( linear in the number of rules ) , we get a speed up of a factor of 4.75 for Arabic ( r = 140 ) and 6.5 for English ( r = 260 ) while retaining similar performance .
Perhaps more surprising is that using the tensor approximation actually improves performance in several cases .
We hypothesize that the cause of this is that the tensor decomposition requires less parameters to express the rule probabilities in the grammar , and therefore leads to better generalization than a vanilla maximum likelihood estimate .
We include results for a more complex model for Arabic , which uses horizontal Markovization of order 1 and vertical Markovization of order 2 ( Klein and Manning , 2003 ) .
This grammar includes 2,188 binary rules .
Parsing exhaustively using this grammar takes 1.30 seconds per sentence ( on average ) with an F 1 measure of 64.43 .
Parsing with tensor decomposition for r = 280 takes 0.62 seconds per sentence ( on average ) with an F 1 measure of 64.05 .
Discussion
In this section , we briefly touch on several other topics related to tensor approximation .
Approximating the Probability of a String
The probability of a sentence z under a PCFG is defined as p ( z ) = ? ?T ( z ) p( ? ) , and can be approximated using the algorithm in Section 4.3 , running in time O( rN 3 + rmN 2 ) .
Of theoretical interest , we discuss here a time O( rN 3 + r 2 N 2 ) algorithm , which is more convenient when r < m.
Observe that in Eq. ( 3 ) vector ?
i , j always appears within one of the two terms V ? i , j and W ? i , j in R r?1 , whose dimensions are independent of m .
We can therefore use Eq. ( 3 ) to compute V ? i , j as V ? i , j = V U j?1 k=1 ( ?
V ? i , k W ? k +1 , j ) , where V U is a R r?r matrix that can be computed off-line , i.e. , independently of z .
A symmetrical relation can be used to compute W ? i , j . Finally , we can write p ( z ) = ? U N ?1 k=1 ( ?
V ? 1 , k W ? k+1,N ) , where ?
U is a R 1 ? r vector that can again be computed off-line .
This algorithm then runs in time O( rN 3 + r 2 N 2 ) .
Applications to Dynamic Programming
The approximation method presented in this paper is not limited to PCFG parsing .
A similar approximation method has been used for latent- variable PCFGs , and in general , tensor approximation can be used to speed - up insideoutside algorithms for general dynamic programming algorithms or weighted logic programs ( Eisner et al. , 2004 ; Cohen et al. , 2011 ) .
In the general case , the dimension of the tensors will not be necessarily just three ( corresponding to binary rules ) , but can be of a higher dimension , and therefore the speed gain can be even greater .
In addition , tensor approximation can be used for computing marginals of latent variables in graphical models .
For example , the complexity of the forward - backward algorithm for HMMs can be reduced to be linear in the number of states ( as opposed to quadratic ) and linear in the rank used in an approximate singular- value decomposition ( instead of tensor decomposition ) of the transition and emission matrices .
Tighter ( but Slower ) Approximation Using Singular Value Decomposition
The accuracy of the algorithm depends on the ability of the tensor decomposition algorithm to decompose the tensor with a small reconstruction error .
The decomposition algorithm is performed on the tensor T which includes all rules in the grammar .
Instead , one can approach the approximation by doing a decomposition for each slice of T separately using singular value decomposition .
This will lead to a more accurate approximation , but will also lead to an extra factor of m during parsing .
This factor is added because now there is not a single U , V and W , but instead there are such matrices for each nonterminal in the grammar .
Conclusion
We described an approximation algorithm for probabilistic context-free parsing .
The approximation algorithm is based on tensor decomposition performed on the underlying rule table of the CFG grammar .
The approximation algorithm leads to significant speed - up in PCFG parsing , with minimal loss in performance .
T. Matsuzaki , Y. Miyao , and J. Tsujii . 2005 ?
N is the finite set of nonterminal symbols , with m = | N | , and L is the finite set of words ( lexical tokens ) , with L ? N = ? and with n = |L|. ?
R is a set of rules having the form a ?
b c , a , b , c ?
N , or the form a ? x , a ?
N and x ? L.
