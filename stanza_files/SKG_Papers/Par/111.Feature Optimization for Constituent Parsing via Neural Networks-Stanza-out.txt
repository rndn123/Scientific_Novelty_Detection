title
Feature Optimization for Constituent Parsing via Neural Networks
abstract
The performance of discriminative constituent parsing relies crucially on feature engineering , and effective features usually have to be carefully selected through a painful manual process .
In this paper , we propose to automatically learn a set of effective features via neural networks .
Specifically , we build a feedforward neural network model , which takes as input a few primitive units ( words , POS tags and certain contextual tokens ) from the local context , induces the feature representation in the hidden layer and makes parsing predictions in the output layer .
The network simultaneously learns the feature representation and the prediction model parameters using a back propagation algorithm .
By pre-training the model on a large amount of automatically parsed data , and then fine-tuning on the manually annotated Treebank data , our parser achieves the highest F 1 score at 86.6 % on Chinese Treebank 5.1 , and a competitive F 1 score at 90.7 % on English Treebank .
More importantly , our parser generalizes well on cross-domain test sets , where we significantly outperform Berkeley parser by 3.4 points on average for Chinese and 2.5 points for English .
Introduction Constituent parsing seeks to uncover the phrase structure representation of sentences that can be used in a variety of natural language applications such as machine translation , information extraction and question answering ( Jurafsky and Martin , 2008 ) .
One of the major challenges for this task is that constituent parsers require an inference algorithm of high computational complexity in order to search over their large structural space , which makes it very hard to efficiently train discriminative models .
So , for a long time , the task was mainly solved with generative models ( Collins , 1999 ; Charniak , 2000 ; Petrov et al. , 2006 ) .
In the last few years , however , with the use of effective parsing strategies , approximate inference algorithms , and more efficient training methods , discriminative models began to surpass the generative models ( Carreras et al. , 2008 ; Zhu et al. , 2013 ; Wang and Xue , 2014 ) .
Just like other NLP tasks , the performance of discriminative constituent parsing crucially relies on feature engineering .
If the feature set is too small , it might underfit the model and leads to low performance .
On the other hand , too many features may result in an overfitting problem .
Usually , an effective set of features have to be designed manually and selected through repeated experiments ( Sagae and Lavie , 2005 ; Wang et al. , 2006 ; Zhang and Clark , 2009 ) .
Not only does this procedure require a lot of expertise , but it is also tedious and time - consuming .
Even after this painstaking process , it is still hard to say whether the selected feature set is complete or optimal to obtain the best possible results .
A more desirable alternative is to learn features automatically with machine learning algorithms .
Lei et al. ( 2014 ) proposed to learn features by representing the cross-products of some primitive units with low-rank tensors for dependency parsing .
However , to achieve competitive performance , they had to combine the learned features with the traditional hand -crafted features .
For constituent parsing , Henderson ( 2003 ) employed a recurrent neural network to induce features from an unbounded parsing history .
However , the final performance was below the state of the art .
In this work , we design a much simpler neural network to automatically induce features from just the local context for constituent parsing .
Con-cretely , we choose the shift-reduce parsing strategy to build the constituent structure of a sentence , and train a feedforward neural network model to jointly learn feature representations and make parsing predictions .
The input layer of the network takes as input a few primitive units ( words , POS tags and certain contextual tokens ) from the local context , the hidden layer aims to induce a distributed feature representation by combining all the primitive units with different weights , and the output layer attempts to make parsing predictions based on the feature representation .
During the training process , the model simultaneously learns the feature representation and prediction model parameters using a backpropagation algorithm .
Theoretically , the learned feature representation is optimal ( or at least locally optimal ) for the parsing predictions .
In practice , however , our model does not work well if it is only trained on the manually annotated Treebank data sets .
However , when pre-trained on a large amount of automatically parsed data and then fine-tuned on the Treebank data sets , our model achieves a fairly large improvement in performance .
We evaluated our model on both Chinese and English .
On standard data sets , our model reaches F 1 = 86.6 % for Chinese and outperforms all the state - of - theart systems , and for English our final performance is F 1 = 90.7 % and this result surpasses that of all the previous neural network based models and is comparable to the state - of - the - art systems .
On cross-domain data sets , our model outperforms the Berkeley Parser 1 by 3.4 percentage points for Chinese and 2.5 percentage points for English .
The remainder of this paper is organized as follows : Section 2 introduces the shift-reduce constituent parsing approach .
Section 3 describes our feature optimization model and some parameter estimation techniques .
We discuss and analyze our experimental results in Section 4 .
Section 5 discusses related work .
Finally , we conclude this paper in Section 6 .
Shift -Reduce Constituent Parsing
Shift-reduce constituent parsing utilizes a series of shift-reduce decisions to construct syntactic trees .
Formally , the shift-reduce system is a quadruple C = ( S , T , s 0 , S t ) , where S is a set of parser states ( sometimes called configurations ) , T is a finite set of actions , s 0 is an initialization function to map each input sentence into a unique initial state , and S t ?
S is a set of terminal states .
Each action t ?
T is a transition function that maps a state into a new state .
A parser state s ?
S is defined as a tuple s = ( ? , ? ) , where ? is a stack which is maintained to hold partial subtrees that are already constructed , and ? is a queue which is used for storing remaining unprocessed words .
In particular , the initial state has an empty stack ? and a queue ?
containing the entire input sentence , and the terminal states have an empty queue ? and a stack ?
containing only one complete parse tree .
The task of parsing is to scan the input sentence from left to right and perform a sequence of shiftreduce actions to transform the initial state into a terminal state .
In order to jointly assign POS tags and construct a constituent structure for an input sentence , we define the following actions for the action set T , following Wang and Xue ( 2014 ) : ? SHIFT -X ( sh-x ) : remove the first word from ? , assign a POS tag X to the word and push it onto the top of ? ; ? REDUCE -UNARY -X ( ru-x ) : pop the top subtree from ? , construct a new unary node labeled with X for the subtree , then push the new subtree back onto ?.
The head of the new subtree is inherited from its child ; ? REDUCE -BINARY -{L/ R}-X ( rl/rr-x ) : pop the top two subtrees from ? , combine them into a new tree with a node labeled with X , then push the new subtree back onto ?.
The left ( L ) and right ( R ) versions of the action indicate whether the head of the new subtree is inherited from its left or right child .
With these actions , our parser can process trees with unary and binary branches easily .
For example , in Figure 1 , for the sentence " the assets are sold " , our parser can construct the parse tree by performing the action sequence { sh - DT , sh- NNS , rr-NP , sh- VBP , sh- VBN , ru-VP , rr-VP , rr-S} .
To process multi-branch trees , we employ binarization and debinarization processes described in Zhang and Clark ( 2009 ) to transform multi-branch trees into binary trees and restore the generated binary trees back to their original forms .
For inference , we employ the beam search decoding algorithm ( Zhang and Clark , 2009 ) to balance the tradeoff between accuracy and efficiency .
3 Feature Optimization Model
Model
To determine which action t ?
T should be performed at a given state s ?
S , we need a model to score each possible s , t combination .
In previous approaches ( Sagae and Lavie , 2005 ; Wang et al. , 2006 ; Zhang and Clark , 2009 ) , the model is usually defined as a linear model Score(s , t ) = ? ? w ? ?( s , t ) , where ?(s , t ) is a vector of handcrafted features for each state-action pair and ? ? w is the weight vector for these features .
The handcrafted features are usually constructed by compounding primitive units according to some feature templates .
For example , almost all the previous work employed the list of primitive units in Table 1 ( a ) , and constructed hand -crafted features by concatenating these primitive units according to the feature templates in Table 1 ( b ) .
Obviously , these feature templates are only a small subset of the cross products of all the primitive units .
This feature set is the result of a large number of experiments through trial and error from previous work .
Still we cannot say for sure that this is the optimal subset of features for the parsing task .
To cope with this problem , we propose to simultaneously optimize feature representation and parsing accuracy via a neural network model .
Figure 2 illustrates the architecture of our model .
Our model consists of input , projection , hidden and output layers .
First , in the input layer , all primitive units ( shown in Table 1 ( a ) ) are imported to the network .
We also import the suffixes and prefixes of the first word in the queue , because these units have been shown to be very effective for predicting POS tags ( Ratnaparkhi , 1996 ) .
Then , in the projection layer , each primitive unit is projected into a vector .
Specifically , word-type units are represented as word embeddings , and other units are transformed into one- hot representations .
The ( 1 ) p 0 w , p 0 t , p 0 c , p 1 w , p 1 t , p 1 c , p 2 w , p 2 t , p 2 c , p 3 w , p 3 t , p 3 c ( 2 ) p 0l w , p 0l c , p 0r w , p 0r c , p 0u w , p 0u c , p 1l w , p 1l c , p 1r w , p 1r c , p 1u w , p 1u c ( 3 ) q 0 w , q 1 w , q 2 w , q 3 w ( a ) Primitive Units unigrams p 0 tc , p 0 wc , p 1 tc , p 1 wc , p 2 tc p 2 wc , p 3 tc , p 3 wc , q 0 wt , q 1 wt q 2 wt , q 3 wt , p 0l wc , p 0r wc p 0u wc , p 1l wc , p 1r wc , p 1u wc bigrams p 0 wp 1 w , p 0 wp 1 c , p 0 cp 1 w , p 0 cp 1 c p 0 wq 0 w , p 0 wq 0 t , p 0 cq 0 w , p 0 cq 0 t q 0 wq 1 w , q 0 wq 1 t , q 0 tq 1 w , q 0 tq 1 t p 1 wq 0 w , p 1 wq 0 t , p 1 cq 0 w , p 1 cq 0 t trigrams p 0 cp 1 cp 2 c , p 0 wp 1 cp 2 c , p 0 cp 1 wq 0 t p 0 cp 1 cp 2 w , p 0 cp 1 cq 0 t , p 0 wp 1 cq 0 t p 0 cp 1 wq 0 t , p 0 cp 1 cq 0 w ( b ) Feature Templates Table 1 : Primitive units ( a ) and feature templates ( b ) for shift- reduce constituent parsing , where p i represents the i th subtree in the stack and q i denotes the i th word in the queue .
w refers to the head word , t refers to the head POS , and c refers to the constituent label .
p il and p ir refer to the left and right child for a binary subtree p i , and p iu refers to the child of a unary subtree p i . vectors of all primitive units are concatenated to form a holistic vector for the projection layer .
The hidden layer corresponds to the feature representation we want to learn .
Each dimension in the hidden layer can be seen as an abstract factor of all primitive units , and it calculates a weighted sum of all nodes from the projection layer and applies a non-linear activation function to yield its activation .
We choose the logistic sigmoid function for the hidden layer .
The output layer is used for making parsing predictions .
Each node in the output layer corresponds to a shift-reduce action .
We want to interpret the activation of the output layer as a probability distribution over all possible shiftreduce actions , therefore we normalize the output activations ( weighted summations of all nodes from the hidden layer ) with the softmax function .
Parameter Estimation
Our model consists of three groups of parameters : ( 1 ) the word embedding for each word type unit , ( 2 ) the connections between the projection layer and the hidden layer which are used for learning an optimal feature representation and ( 3 ) the connections between the hidden layer and the output layer which are used for making accurate parsing predictions .
We decided to learn word embeddings separately , so that we can take advantage of a large amount of unlabeled data .
The remaining two groups of parameters can be trained simultaneously by the back propagation algorithm ( Rumelhart et al. , 1988 ) to maximize the likelihood over the training data .
w 0 ? w m ? t 0 ? t n ? c 0 ? c n ? ? ? ? ? ? ? ... ...
We also employ three crucial techniques to seek more effective parameters .
First , we utilize minibatched AdaGrad ( Duchi et al. , 2011 ) , in which the learning rate is adapted differently for different parameters at different training steps .
With this technique , we can start with a very large learning rate which decreases during training , and can thus perform a far more thorough search within the parameter space .
In our experiments , we got a much faster convergence rate with slightly better accuracy by using the learning rate ? = 1 instead of the commonly - used ? = 0.01 .
Second , we initialize the model parameters by pre-training .
Unsupervised pre-training has demonstrated its effectiveness as a way of initializing neural network models ( Erhan et al. , 2010 ) .
Since our model requires many run-time primitive units ( POS tags and constituent labels ) , we employ an in- house shift- reduce parser to parse a large amount of unlabeled sentences , and pre-train the model with the automatically parsed data .
Third , we utilize the Dropout strategy to address the overfitting prob-lem .
However , different from Hinton et al . ( 2012 ) , we only use Dropout during testing , because we found that using Dropout during training did not improve the parsing performance ( on the dev set ) while greatly slowing down the training process .
Experiment
Experimental Setting
We conducted experiments on the Penn Chinese Treebank ( CTB ) version 5.1 ( Xue et al. , 2005 ) and the Wall Street Journal ( WSJ ) portion of Penn English Treebank ( Marcus et al. , 1993 ) .
To fairly compare with other work , we follow the standard data division .
For Chinese , we allocated Articles 001-270 and 400 - 1151 as the training set , Articles 301 - 325 as the development set , and Articles 271-300 as the testing set .
For English , we use sections 2 - 21 for training , section 22 for developing and section 23 for testing .
We also utilized some unlabeled corpora and used the word2vec 2 toolkit to train word embeddings .
For Chinese , we used the unlabeled Chinese Gigaword ( LDC2003T09 ) and performed Chinese word segmentation using our in-house segmenter .
For English , we randomly selected 9 million sentences from our in- house newswire corpus , which has no overlap with our training , testing and development sets .
We use Evalb 3 toolkit to evaluate parsing performance .
Characteristics of Our Model
There are several hyper-parameters in our model , e.g. , the word embedding dimension ( wordDim ) , the hidden layer node size ( hiddenSize ) , the Dropout ratio ( dropRatio ) and the beam size for inference ( beamSize ) .
The choice of these hyperparameters may affect the final performance .
In this subsection , we present some experiments to demonstrate the characteristics of our model , and select a group of proper hyper-parameters that we use to evaluate our final model .
All the experiments in this subsection were performed on Chinese data and the evaluation is performed on Chinese development set .
First , we evaluated the effectiveness of various primitive units .
We set wordDim = 300 , hiddenSize = 300 , beamSize = 8 , and did not apply Dropout ( dropRatio = 0 ) .
with row " All Units " , we found that ablating the Prefix and Suffix units ( " w/ o Prefix & Suffix " ) significantly hurts both POS tagging and parsing performance .
Ablating POS units ( " w / o POS " ) or constituent label units ( " w/ o NT " ) has little effect on POS tagging accuracy , but hurts parsing performance .
When only keeping the word type units ( " Only Word " ) , both the POS tagging and parsing accuracy drops drastically .
So the Prefix and Suffix units are crucial for POS tagging , and POS units and constituent label units are helpful for parsing performance .
All these primitive units are indispensable to better performance .
Second , we uncovered the effect of the dimension of word embedding .
We set hiddenSize = 300 , beamSize = 8 , dropRatio = 0 and varied wordDim among { 50 , 100 , 300 , 500 , 1000 }.
Figure 3 ( a ) draws the parsing performance curve .
When increasing wordDim from 50 to 300 , parsing performance improves more than 1.5 percentage points .
After that , the curve flattens out , and parsing performance only gets marginal improvement .
Therefore , in the following experiments , we fixed wordDim = 300 .
Third , we tested the effect of hidden layer node size .
We varied hiddenSize among { 50 , 100 , 300 , 500 , 1000 } .
Figure 3 ( b ) draws the parsing performance curve .
We found increasing hiddenSize is helpful for parsing performance .
However , higher hiddenSize would greatly increase the amount of computation .
To keep the efficiency of our model , we fixed hiddenSize = 300 in the following experiments .
Fourth , we applied Dropout and tuned the Dropout ratio through experiments .
Figure 3 ( c ) shows the results .
We found that the peak performance occurred at dropRatio = 0.5 , which brought about an improvement of more than 1 percentage point over the model without Dropout ( dropRatio = 0 ) .
Therefore , we fixed dropRatio = 0.5 .
Finally , we investigated the effect of beam size .
Figure 3 ( d ) shows the curve .
We found increasing beamSize greatly improves the performance initially , but no further improvement is observed after beamSize is greater than 8 .
Therefore , we fixed beamSize = 8 in the following experiments .
Semi-supervised Training
In this subsection , we investigated whether we can train more effective models using automatically parsed data .
We randomly selected 200K sentences from our unlabeled data sets for both Chinese and English .
Then , we used an in-house shift- reduce parser 4 to parse these selected sentences .
The size of the automatically parsed data set may have an impact on the final model .
So we trained many models with varying amounts of automatically parsed data .
We also designed two strategies to exploit the automatically parsed data .
The first strategy ( Mix -Train ) is to directly add the automatically parsed data to the hand-annotated training set and train models with the mixed data set .
The second strategy ( Pre-Train ) is to first pretrain models with the automatically parsed data , and then fine -tune models with the hand -annotated training set .
Table 3 shows results of different experimental configurations for Chinese .
strategy , when we only use 50 K automatically parsed sentences , the performance drops in comparison with the model trained without using any automatically parsed data .
When we increase the automatically parsed data to 100K sentences , the parsing performance improves about 1 percent but the POS tagging accuracy drops slightly .
When we further increase the automatically parsed data to 200K sentences , both the parsing performance and POS tagging accuracy improve .
For the Pre-Train strategy , the performance of all three configurations improves performance against the model that does not use any automatically parsed data .
The Pre-Train strategy consistently outperforms the Mix-Train strategy when the same amount of automatically parsed data is used .
Therefore , for Chinese , the Pre-Train strategy is much more helpful , and the more automatically parsed data we use the better performance we get .
Table 4 presents results of different experimental configurations for English .
The performance trend for the Mix-Train strategy is different from that of Chinese .
Here , no matter how much automatically parsed data we use , there is a consistent degradation in performance against the model that does not use any automatically parsed data at all .
And the more automatically parsed data we use , the larger the drop in accuracy .
For the Pre-Train strategy , the trend is similar to Chinese .
The parsing performance of the Pre-Train setting consistently improves as the size of automatically parsed data increases .
Comparing With State-of- the- art Systems
In this subsection , we present the performance of our models on the testing sets .
We trained two systems .
The first system ( " Supervised " ) is trained only with the hand - annotated training set , and the second system ( " Pretrain - Finetune " ) is trained with the Pre-Train strategy described in subsection 4.3 using additional automatically parsed data .
The best parameters for the two systems are set based on their performance on the development set .
To further illustrate the effectiveness of our systems , we also compare them with some state - of - the - art systems .
We group parsing systems into three categories : supervised single systems ( SI ) , semi-supervised single systems ( SE ) and reranking systems ( RE ) .
Both of our two models belong to semi-supervised single systems , because our " Supervised " system utilized word embeddings in its input layer .
Table 5 lists the performance of our systems as well as the state - of - the - art systems on Chinese test set .
Comparing the performance of our two systems , we see that our " Pretrain - Finetune " system shows a fairly large gain over the " Supervised " system .
One explanation is that our neural network model is a non-linear model , so the back propagation algorithm can only reach a local optimum .
In our " Supervised " system the starting points are randomly initialized in the parameter space , so it only reaches local optimum .
In comparison , our " Pretrain - Finetune " system gets to see large amount of automatically parsed data , and initializes the starting points with the pre-trained parameters .
So it finds a much better local optimum than the " Supervised " system .
Comparing our " Pretrain - Finetune " system with all the stateof - the - art systems , we see our system surpass all the other systems .
Although our system only utilizes some basic primitive units ( in Table 1 ( a ) ) , it still outperforms Wang and Xue ( 2014 ) 's shiftreduce parsing system which uses more complex structural features and semi-supervised word cluster features .
Therefore , our model can simultaneously learn an effective feature representation and make accurate parsing predictions for Chinese .
Table 6 presents the performance of our systems as well as the state - of - the - art systems on the English test set .
Our " Pretrain - Finetune " system still achieves much better performance than the " Supervised " system , although the gap is smaller than that of Chinese .
Our " Pretrain - Finetune " system also outperforms all other neural network based systems ( systems marked with * ) .
Although our system does not outperform all the state - of - the - art systems , the performance is comparable to most of them .
So our model is also effective for English parsing .
Cross Domain Evaluation
In this subsection , we examined the robustness of our model by evaluating it on data sets from various domains .
We use the Berkeley Parser as our baseline parser , and trained it on our training set .
For Chinese , we performed our experiments on the cross domain data sets from Chinese Treebank 8.0 .
It consists of six domains : newswire ( nw ) , magazine articles ( mz ) , broadcast news ( bn ) , broadcast conversation ( bc ) , weblogs ( wb ) and discussion forums ( df ) .
Since all of the mz domain data is already included in our training set , we only selected sample sentences from the other five domains as the test sets 5 , and made sure these test sets had no overlap with our treebank training , development and test sets .
Note that we did not use any data from these five domains for training or development .
The models are still the ones described in the previous subsection .
The results are presented in Table 7 . Although our " Supervised " model got slightly worse performance than the Berkeley Parser ( Petrov and Klein , 2007 ) , as shown in Table 5 , it outperformed the Berkeley Parser on the cross-domain data sets .
This suggests that the learned features can better adapt to cross-domain situations .
Compared with the Berkeley Parser , on average our " Pretrain - Finetune " model is 3.4 percentage points better in terms of parsing accuracy , and 3.2 percentage points better in terms of POS tagging accuracy .
We also presented the performance of our pre-trained model ( " Only - Pretrain " ) .
We found the " Only - Pretrain " model performs poorly on this cross-domain data sets .
But even pretraining based on this less than competitive model , our " Pretrain - Finetune " model achieves significant improvement over the " Supervised " model .
So the Pre-Train strategy is crucial to our model .
For English , we performed our experiments on the cross-domain data sets from OntoNote 5.0 ( Weischedel et al. , 2013 ) , which consists of nw , mz , bn , bc , wb , df and telephone conversations ( tc ) .
We also performed experiments on the SMS domain , using data annotated by the LDC for the DARPA BOLT Program .
We randomly selected 300 sentences for each domain as the test sets 5 . Table 8 ( Petrov and Klein , 2007 ) when evaluated on the standard Penn TreeBank test set ( Table 6 ) , our parser is 2.5 percentage points better on average on the cross domain data sets .
So our parser is also very robust for English on crossdomain data sets .
Related Work
There has been some work on feature optimization in dependency parsing , but most prior work in this area is limited to selecting an optimal subset of features from a set of candidate features ( Nilsson and Nugues , 2010 ; Ballesteros and Bohnet , 2014 ) .
Lei et al. ( 2014 ) proposed to learn features for dependency parsing automatically .
They first represented all possible features with a multi-way tensor , and then transformed it into a low-rank tensor as the final features that are actually used by their system .
However , to obtain competitive performance , they had to combine the learned features with traditional hand -crafted features .
Chen and Manning ( 2014 ) proposed to learn a dense feature vector for transition - based dependency parsing via neural networks .
Their model had to learn POS tag embeddings and dependency label embeddings first , and then induced the dense feature vector based on these embeddings .
Comparing with their method , our model is much simpler .
Our model learned features directly based on the original form of primitive units .
There have also been some attempts to use neural networks for constituent parsing .
Henderson ( 2003 ) presented the first neural network for broad coverage parsing .
Later , he also proposed to rerank k-best parse trees with a neural network model which achieved state - of- the - art performance ( Henderson , 2004 ) . Collobert ( 2011 ) designed a recurrent neural network model to construct parse tree by stacks of sequences labeling , but its final performance is significantly lower than the state - of - the - art performance .
Socher et al. ( 2013 ) built a recursive neural network for constituent parsing .
However , rather than performing full inference , their model can only score parse candidates generated from another parser .
Our model also requires a parser to generate training samples for pre-training .
However , our system is different in that , during testing , our model performs full inference with no need of other parsers .
Vinyals et al. ( 2014 ) employed a Long Short - Term Memory ( LSTM ) neural network for parsing .
By training on a much larger hand - annotated data set , their performance reached 91.6 % for English .
Conclusion
In this paper , we proposed to learn features via a neural network model .
By taking as input the primitive units , our neural network model learns feature representations in the hidden layer and made parsing predictions based on the learned features in the output layer .
By employing the backpropagation algorithm , our model simultaneously induced features and learned prediction model parameters .
We show that our model achieved significant improvement from pretraining on a substantial amount of pre-parsed data .
Evaluated on standard data sets , our model outperformed all stateof - the - art parsers on Chinese and all neural network based models on English .
We also show that our model is particularly effective on crossdomain tasks for both Chinese and English .
Figure 1 : 1 Figure 1 : An example of constituent tree .
