title
Fast ( er ) Exact Decoding and Global Training for Transition - Based Dependency Parsing via a Minimal Feature Set
abstract
We first present a minimal feature set for transition - based dependency parsing , continuing a recent trend started by Kiperwasser and Goldberg ( 2016 a ) and Cross and Huang ( 2016a ) of using bi-directional LSTM features .
We plug our minimal feature set into the dynamic-programming framework of Huang and Sagae ( 2010 ) and Kuhlmann et al . ( 2011 ) to produce the first implementation of worst- case Opn 3 q exact decoders for arc-hybrid and arceager transition systems .
With our minimal features , we also present Opn 3 q global training methods .
Finally , using ensembles including our new parsers , we achieve the best unlabeled attachment score reported ( to our knowledge ) on the Chinese Treebank and the " second- best-in- class " result on the English Penn Treebank .
Introduction
It used to be the case that the most accurate dependency parsers made global decisions and employed exact decoding .
But transition - based dependency parsers ( TBDPs ) have recently achieved state - of - the - art performance , despite the fact that for efficiency reasons , they are usually trained to make local , rather than global , decisions and the decoding process is done approximately , rather than exactly ( Weiss et al. , 2015 ; Dyer et al. , 2015 ; Andor et al. , 2016 ) .
The key efficiency issue for decoding is as follows .
In order to make accurate ( local ) attachment decisions , historically , TBDPs have required a large set of features in order to access rich information about particular positions in the stack and buffer of the current parser configuration .
But consulting many positions means that although polynomial - time exact- decoding algo-rithms do exist , having been introduced by Huang and Sagae ( 2010 ) and Kuhlmann et al . ( 2011 ) , unfortunately , they are prohibitively costly in practice , since the number of positions considered can factor into the exponent of the running time .
For instance , Huang and Sagae employ a fairly reduced set of nine positions , but the worst - case running time for the exact- decoding version of their algorithm is Opn 6 q ( originally reported as Opn 7 q ) for a length -n sentence .
As an extreme case , Dyer et al . ( 2015 ) use an LSTM to summarize arbitrary information on the stack , which completely rules out dynamic programming .
Recently , Kiperwasser and Goldberg ( 2016 a ) and Cross and Huang ( 2016a ) applied bidirectional long short - term memory networks ( Graves and Schmidhuber , 2005 , bi-LSTMs ) to derive feature representations for parsing , because these networks capture wide - window contextual information well .
Collectively , these two sets of authors demonstrated that with bi-LSTMs , four positional features suffice for the arc-hybrid parsing system ( K&G ) , and three suffice for arcstandard ( C&H ) .
1 Inspired by their work , we arrive at a minimal feature set for arc-hybrid and arc-eager : it contains only two positional bi-LSTM vectors , suffers almost no loss in performance in comparison to larger sets , and out-performs a single position .
( Details regarding the situation with arc-standard can be found in ?2 . )
Our minimal feature set plugs into Huang and Sagae 's and Kuhlmann et al . 's dynamic program-ming framework to produce the first implementation of Opn 3 q exact decoders for arc-hybrid and arc-eager parsers .
We also enable and implement Opn 3 q global training methods .
Empirically , ensembles containing our minimal -feature , globallytrained and exactly - decoded models produce the best unlabeled attachment score ( UAS ) reported ( to our knowledge ) on the Chinese Treebank and the " second- best-in- class " result on the English Penn Treebank .
2 Additionally , we provide a slight update to the theoretical connections previously drawn by Weir ( 2008 , 2011 ) between TBDPs and the graph- based dependency parsing algorithms of Eisner ( 1996 ) and Eisner and Satta ( 1999 ) , including results regarding the arc-eager parsing system .
A Minimal Feature Set TBDPs incrementally process a sentence by making transitions through search states representing parser configurations .
Three of the main transition systems in use today ( formal introduction in ?3.1 ) all maintain the following two data structures in their configurations : ( 1 ) a stack of partially parsed subtrees and ( 2 ) a buffer ( mostly ) of unprocessed sentence tokens .
To featurize configurations for use in a scoring function , it is common to have features that extract information about the first several elements on the stack and the buffer , such as their word forms and part-of-speech ( POS ) tags .
We refer to these as positional features , as each feature relates to a particular position in the stack or buffer .
Typically , millions of sparse indicator features ( often developed via manual engineering ) are used .
In contrast , Chen and Manning ( 2014 ) introduce a feature set consisting of dense word - , POS - , and dependency - label embeddings .
While dense , these features are for the same 18 positions that have been typically used in prior work .
Recently , Kiperwasser and Goldberg ( 2016 a ) and Cross and Huang ( 2016a ) adopt bi-directional LSTMs , which have nice expressiveness and context-sensitivity properties , to reduce the number of positions considered down to four and three , 2 Our ideas were subsequently adapted to the labeled setting by Shi , Wu , Chen , and Cheng ( 2017 ) in their submission to the CoNLL 2017 shared task on Universal Dependencies parsing .
Their team achieved the second-highest labeled attachment score in general and had the top average performance on the surprise languages .
for different transition systems , respectively .
This naturally begs the question , what is the lower limit on the number of positional features necessary for a parser to perform well ?
Kiperwasser and Goldberg ( 2016a ) reason that for the arc-hybrid system , the first and second items on the stack and the first buffer item - denoted by s 0 , s 1 , and b 0 , respectively - are required ; they additionally include the third stack item , s 2 , because it may not be adjacent to the others in the original sentence .
For arc-standard , Cross and Huang ( 2016a ) argue for the necessity of s 0 , s 1 , and b 0 .
We address the lower-limit question empirically , and find that , surprisingly , two positions suffice for the greedy arc-eager and arc-hybrid parsers .
We also provide empirical support for Cross and Huang 's argument for the necessity of three features for arc-standard .
In the rest of this section , we explain our experiments , run only on an English development set , that support this conclusion ; the results are depicted in Table 1 .
We later explore the implementation implications in ?3 - 4 and then test-set parsing - accuracy in ?6 .
We employ the same model architecture as Kiperwasser and Goldberg ( 2016 a ) .
Specifically , we first use a bi-LSTM to encode an n-token sentence , treated as a sequence of per-token concatenations of word - and POS - tag embeddings , into a sequence of vectors r ? w 1 , . . . , ?
w n s , where each ?
w i is the output of the bi-LSTM at time step i .
( The double - arrow notation for these vectors emphasizes the bi-directionality of their origin ) .
Then , for a given parser configuration , stack positions are represented by ?
s j , defined as ?
w ips j q where ips j q gives the position in the sentence of the token that is the head of the tree in s j .
Similarly , buffer positions are represented by ?
b j , defined as ?
w ipb j q for the token at buffer position j.
Finally , as in Chen and Manning ( 2014 ) , we use a multilayer perceptron to score possible transitions from the given configuration , where the input is the concatenation of some selection of the ?
s j s and ?
b k s.
We use greedy decoders , and train the models with dynamic oracles ( Goldberg and Nivre , 2013 ) .
Table 1 reports the parsing accuracy that results for feature sets of size four , three , two , and one for three commonly - used transition systems .
The data is the development section of the English Penn Treebank ( PTB ) , and experimental settings are as described in our other experimental section , ?6 .
We see that we can go down to three or , in the arc-hybrid and arc-eager transition systems , even two positions with very little loss in performance , but not further .
We therefore call t ? s 0 , ? b 0 u our minimal feature set with respect to arc-hybrid and arc-eager , and empirically confirm that Cross and Huang 's t ? s 0 , ? s 1 , ? b 0 u is minimal for arc-standard ; see Table 1 for a summary .
3
Dynamic Programming for TBDPs
As stated in the introduction , our minimal feature set from ?2 plugs into Huang and Sagae and Kuhlmann et al . 's dynamic programming ( DP ) framework .
To help explain the connection , this section provides an overview of the DP framework .
We draw heavily from the presentation of Kuhlmann et al . ( 2011 ) .
Three Transition Systems Transition - based parsing ( Nivre , 2008 ; K?bler et al. , 2009 ) is an incremental parsing framework based on transitions between parser configura - 3
We tentatively conjecture that the following might explain the observed phenomena , but stress that we do n't currently see a concrete way to test the following hypothesis .
With t ? s 0 , ? b 0u , in the arc-standard case , situations can arise where there are multiple possible transitions with missing information .
In contrast , in the arc-hybrid case , there is only one possible transition with missing information ( namely , re? , introduced in ?3.1 ) ; perhaps ?
s 1 is therefore not so crucial for arc-hybrid in practice ?
tions .
For a sentence to be parsed , the system starts from a corresponding initial configuration , and attempts to sequentially apply transitions until a configuration corresponding to a full parse is produced .
Formally , a transition system is defined as S " pC , T , c s , C ? q , where C is a nonempty set of configurations , each t P T : C ?
C is a transition function between configurations , c s is an initialization function that maps an input sentence to an initial configuration , and C ? ?
C is a set of terminal configurations .
All systems we consider share a common tripartite representation for configurations : when we write c " p? , ? , Aq for some c P C , we are referring to a stack ?
of partially parsed subtrees ; a buffer ?
of unprocessed tokens and , optionally , at its beginning , a subtree with only left descendants ; and a set A of elements ph , mq , each of which is an attachment ( dependency arc ) with head h and modifier m. 4
We write m ?
h to indicate that m left-modifies h , and h ? m to indicate that m rightmodifies h.
For a sentence w " w 1 , ... , w n , the initial configuration is p?
0 , ? 0 , A 0 q , where ?
0 and A 0 are empty and ?
0 " rROOT|w 1 , ... , w n s ; ROOT is a special node denoting the root of the parse tree 5 ( vertical bars are a notational convenience for indicating different parts of the buffer or stack ; our convention is to depict the buffer first element leftmost , and to depict the stack first element rightmost ) .
All terminal configurations have an empty buffer and a stack containing only ROOT .
Arc-Standard
The arc-standard system ( Nivre , 2004 ) is motivated by bottom - up parsing : each dependent has to be complete before being attached .
The three transitions , shift ( sh , move a token from the buffer to the stack ) , right- reduce ( re ? , reduce and attach a right modifier ) , and left-reduce ( re ? , reduce and attach a left modifier ) , are defined as : shrp ? , b 0 |? , Aqs " p?|b 0 , ? , Aq re ? rp?|s 1 |s 0 , ? , Aqs " p?|s 1 , ? , A Y tps 1 , s 0 quq re ? rp ?|s 1 |s 0 , ? , Aqs " p?|s 0 , ? , A Y tps 0 , s 1 quq Arc-Hybrid
The arc-hybrid system ( Yamada and Matsumoto , 2003 ; G?mez-Rodr ? guez et al. , 2008 ; Kuhlmann et al. , 2011 ) has the same definitions of sh and re ? as arc-standard , but forces the collection of left modifiers before right modifiers via its b 0 - modifier re ? transition .
This contrasts with arc-standard , where the attachment of left and right modifiers can be interleaved on the stack .
shrp ? , b 0 |? , Aqs " p?|b 0 , ? , Aq re ? rp?|s 1 |s 0 , ? , Aqs " p?|s 1 , ? , A Y tps 1 , s 0 quq re ? rp?|s 0 , b 0 |? , Aqs " p? , b 0 |? , A Y tpb 0 , s 0 quq Arc-Eager
In contrast to the former two systems , the arc-eager system ( Nivre , 2003 ) makes attachments as early as possible - even if a modifier has not yet received all of its own modifiers .
This behavior is accomplished by decomposing the right-reduce transition into two independent transitions , one making the attachment ( ra ) and one reducing the right- attached child ( re ) .
( Pereira and Warren , 1983 ; Shieber et al. , 1995 ) , wherein transitions serve as inference rules ; these are given as the lefthand sides of the first three subfigures in Figure 1 . For a given w " w 1 , ... , w n , assertions take the form ri , j , ks ( or , when applicable , a two -index shorthand to be discussed soon ) , meaning that there exists a sequence of transitions that , starting from a configuration wherein head ps 0 q " w i , results in an ending configuration wherein head ps 0 q " w j and head pb 0 q " w k .
If we define w 0 as ROOT and w n`1 as an endof-sentence marker , then the goal theorem can be stated as r0 , 0 , n `1s .
For arc-standard , we depict an assertion ri , h , ks as a subtree whose root ( head ) is the token at h .
Assertions of the form ri , i , ks play an important role for arc-hybrid and arc-eager , and we employ the special shorthand ri , ks for them in Figure 1 .
In that figure , we also graphically depict such situations as two consecutive half - trees with roots w i and w k , where all tokens between i and k are already attached .
The superscript b in an arc-eager assertion ri b , js is an indicator variable for whether w i has been attached to its head ( b " 1 ) or not ( b " 0 ) after the transition sequence is applied .
Kuhlmann et al. ( 2011 ) show that all three deduction systems can be directly " tabularized " and dynamic programming ( DP ) can be applied , such that , ignoring for the moment the issue of incorporating complex features ( we return to this later ) , time and space needs are low-order polynomial .
Specifically , as the two -index shorthand ri , js suggests , arc-eager and arc-hybrid systems can be implemented to take Opn 2 q space and Opn 3 q time ; the arc-standard system requires Opn 3 q space and Opn 4 q time ( if one applies the so-called hook trick ( Eisner and Satta , 1999 ) ) .
Since an Opn 4 q running time is not sufficiently practical even in the simple - feature case , in the remainder of this paper we consider only the archybrid and arc-eager systems , not arc-standard .
Practical Optimal Algorithms Enabled By Our Minimal Feature Set Until now , no one had suggested a set of positional features that was both information - rich enough for accurate parsing and small enough to obtain the Opn 3 q running - time promised above .
Fortunately , our bi-LSTM - based t ? s 0 , ? b 0 u feature set qualifies , and enables the fast optimal procedures described in this section .
Exact Decoding Given an input sentence , a TBDP must choose among a potentially exponential number of corresponding transition sequences .
We assume access to functions f t that score individual configurations , where these functions are indexed by the transition functions t P T .
For a fixed transition sequence t " t 1 , t 2 , . . . , we use c i to denote the configuration that results after applying t i .
Typically , for efficiency reasons , greedy left-toright decoding is employed : the next transition t i out of c i?1 is arg max t f t pc i?1 q , so that past and future decisions are not taken into account .
The score F ptq for the transition sequence is induced by summing the relevant f t i pc i?1 q values .
However , our use of minimal feature sets enables direct computation of an argmax over the entire space of transition sequences , arg max t F ptq , via dynamic programming , because our positions do n't rely on any information " outside " the deduction rule indices , thus eliminating the need for ad- ditional state-keeping .
We show how to integrate the scoring functions for the arc-eager system ; the arc-hybrid system is handled similarly .
The score-annotated rules are as follows : ri b , js : v rj 0 , j ` 1s : 0 pshq rk b , is : v 1 ri 0 , js : v 2 rk b , js : v 1 `v2 `? pre ? q where ? " f sh p ? w k , ?
w i q `fre ?
p ? w i , ?
w j q -abusing notation by referring to configurations by their features .
The left-reduce rule says that we can first take the sequence of transitions asserted by rk b , is , which has a score of v 1 , and then a shift transition moving w i from b 0 to s 0 .
This means that the initial condition for ri 0 , js is met , so we can take the sequence of transitions asserted by ri 0 , js - say it has score v 2 - and finally a left-reduce transition to finish composing the larger transition sequence .
Notice that the scores for sh and ra are 0 , as the scoring of these transitions is accounted for by reduce rules elsewhere in the sequence .
i i j k j k ?
i re? rk , is ri , js rk , js k i i j k j i ? j Goal r0 , n `1s 0 n ` 1 ( b) Arc-hybrid Axiom r0 0 , 1s 0 0 1 Inference Rules sh ri b , js rj 0 , j ` 1s i b j j 0 j ` 1 j ?
n ra ri b , js rj 1 , j ` 1s i b j j 1 j `1 i ? j j ?
n re? rk b , is ri 0 , js rk b , js k b i i 0 j k b j i ?
j re rk b , is ri 1 , js rk b , js k b i i 1 j k b j Goal r0 0 , n ` 1s 0 0 n ` 1 ( c ) Arc-eager Axioms i i ` 1 j j 0 ? i , j ?
n Inference Rules right - attach k i i j k j right- reduce i k k j i j i ?
j left-attach k i i j k j left-reduce i k k j i j i ? j Goal 0 n `1 (
Global Training
We employ large-margin training that considers each transition sequence globally .
Formally , for a training sentence w " w 1 , . . . , w n with gold transition sequence t gold , our loss function is max t ?
F ptq `costpt gold , tq ?F pt gold q where costpt gold , tq is a custom margin for taking t instead of t gold - specifically , the number of mis-attached nodes .
Computing this max can again be done efficiently with a slight modification to the scoring of reduce transitions : rk b , is : v 1 ri 0 , js : v 2 rk b , js : v 1 `v2 `?1 pre ? q where ?
1 " ? ` 1 phead pw i q ? w j q.
This lossaugmented inference or cost-augmented decoding ( Taskar et al. , 2005 ; Smith , 2011 ) technique has previously been applied to graph - based parsing by Kiperwasser and Goldberg ( 2016 a ) .
Efficiency Note
The computation decomposes into two parts : scoring all feature combinations , and using DP to find a proof for the goal theorem in the deduction system .
Time-complexity analysis is usually given in terms of the latter , but the former might have a large constant factor , such as 10 4 or worse for neural - network - based scoring functions .
As a result , in practice , with a small n , scoring with the feature set t ? s 0 , ? b 0 u ( Opn 2 q ) can be as time - consuming as the decoding steps ( Opn 3 q ) for the arc-hybrid and arc-eager systems .
Theoretical Connections
Our minimal feature set brings implementation of practical optimal algorithms to TBDPs , whereas previously only graph - based dependency parsers ( GBDPs ) - a radically different , non-incremental paradigm - enjoyed the ability to deploy them .
Interestingly , for both the transition - and graphbased paradigms , the optimal algorithms build dependency trees bottom - up from local structures .
It is thus natural to wonder if there are deeper , more formal connections between the two .
In previous work , Kuhlmann et al . ( 2011 ) related the arc-standard system to the classic CKY algorithm ( Cocke , 1969 ; Kasami , 1965 ; Younger , 1967 ) in a manner clearly suggested by Figure 1a ; CKY can be viewed as a very simple graph - based approach .
G?mez-Rodr ?guez et al. ( 2008 , 2011 formally prove that sequences of steps in the edgefactored GBDP ( Eisner , 1996 ) can be used to emulate any individual step in the arc-hybrid system ( Yamada and Matsumoto , 2003 ) and the Eisner and Satta ( 1999 , Figure 1d ) version .
However , they did not draw an explicitly direct connection between Eisner and Satta ( 1999 ) and TBDPs .
Here , we provide an update to these previous findings , stated in terms of the expressiveness of scoring functions , considered as parameterization .
For the edge-factored GBDP , we write the score for an edge as f G p ? h , ? mq , where h is the head and m the modifier .
A tree 's score is the sum of its edge scores .
We say that a parameterized dependency parsing model A contains model B if for every instance of parameterization in model B , there exists an instance of model A such that the two models assign the same score to every parse tree .
We claim : Lemma 1 .
The arc-eager model presented in ?4.1 contains the edge-factored model .
Proof Sketch .
Consider a given edge-factored GBDP parameterized by f G .
For any parse tree , every edge i ?
j involves two deduction rules , and their contribution to the score of the final proof is f sh ( ? w k , ? w i ) ` fre ?
p ? w i , ? w j q.
We set f sh ( ? w k , ? w i ) " 0 and f re ?
p ? w i , ? w j q " f G p ? w j , ? w i q. Similarly , f ra ( ? w k , ? w i ) " f G p ? w k , ?
w i q and f re p ? w i , ? w j q " 0 .
The parameterization we arrive at emulates exactly the scoring model of f G .
We further claim that the arc-eager model is more expressive than not only the edge-factored GBDP , but also the arc-hybrid model in our paper .
Lemma 2 .
The arc-eager model contains the archybrid model .
Proof Sketch .
We leverage the fact that the arceager model divides the sh transition in the archybrid model into two separate transitions , sh and ra .
When we constrain the parameters f sh " f ra in the arc-eager model , the model hypothesis space becomes exactly the same as arc-hybrid's .
The extra expressiveness of the arc-eager model comes from the scoring functions f sh and f re that capture structural contexts other than headmodifier relations .
Unlike traditional higher - order graph - based parsing that directly models relations such as siblinghood ( McDonald and Pereira , 2006 ) or grandparenthood ( Carreras , 2007 ) , however , the arguments in those two functions do not have any fixed type of structural interactions .
Experiments Data and Evaluation
We experimented with English and Chinese .
For English , we used the Stanford Dependencies ( de Marneffe and Manning , 2008 ) conversion ( via the Stanford parser 3.3.0 ) of the Penn Treebank ( Marcus et al. , 1993 , PTB ) .
As is standard , we used ?2 - 21 of the Wall Street Journal for training , ?22 for development , and ?23 for testing ; POS tags were predicted using 10 - way jackknifing with the Stanford max entropy tagger ( Toutanova et al. , 2003 ) . For Chinese , we used the Penn Chinese Treebank 5.1 ( Xue et al. , 2002 , CTB ) , with the same splits and head -finding rules for conversion to dependencies as Zhang and Clark ( 2008 ) .
We adopted the CTB 's goldstandard tokenization and POS tags .
We report unlabeled attachment score ( UAS ) and sentencelevel unlabeled exact match ( UEM ) .
Following prior work , all punctuation is excluded from evaluation .
For each model , we initialized the network parameters with 5 different random seeds and report performance average and standard deviation .
Implementation Details
Our model structures reproduce those of Kiperwasser and Goldberg ( 2016 a ) .
We use 2 - layer bi-directional LSTMs with 256 hidden cell units .
Inputs are concatenations of 28 - dimensional randomly - initialized partof-speech embeddings and 100 - dimensional word vectors initialized from GloVe vectors ( Pennington et al. , 2014 ) ( English ) and pre-trained skipgram-model vectors ( Mikolov et al. , 2013 ) ( Chinese ) .
The concatenation of the bi-LSTM feature vectors is passed through a multi-layer perceptron ( MLP ) with 1 hidden layer which has 256 hidden units and activation function tanh .
We set the dropout rate for the bi-LSTM ( Gal and Ghahramani , 2016 ) and MLP ( Srivastava et al. , 2014 ) for each model according to development -set performance .
6
All parameters except the word embed - dings are initialized uniformly ( Glorot and Bengio , 2010 ) .
Approximately 1,000 tokens form a mini-batch for sub-gradient computation .
We train each model for 20 epochs and perform model selection based on development UAS .
The proposed structured loss function is optimized via Adam ( Kingma and Ba , 2015 ) .
The neural network computation is based on the python interface to DyNet ( Neubig et al. , 2017 )
Analogously to the dev-set results given in ?2 , on the test data , the minimal feature sets perform as well as larger ones in locally - trained models .
And there exists a clear trend of global models outperforming local models for the two different transition systems on both datasets .
This illustrates the effectiveness of exact decoding and global training .
Of the three types of global models , the arceager arguably has the edge , an empirical finding resonating with our theoretical comparison of their model expressiveness .
Comparison with State- of- the- Art Models Figure 2 compares our algorithms ' results with those of the state - of- the- art .
9
Our models are competitive and an ensemble of 15 globallytrained models ( 5 models each for arc-eager DP , arc- hybrid DP and edge-factored ) achieves 95.33 and 90.22 on PTB and CTB , respectively , reach-ing the highest reported UAS on the CTB dataset , and the second highest reported on the PTB dataset among dependency - based approaches .
Related Work Not Yet Mentioned Approximate Optimal Decoding / Training Besides dynamic programming ( Huang and Sagae , 2010 ; Kuhlmann et al. , 2011 ) , various other approaches have been proposed for approaching global training and exact decoding .
Best-first and A* search Sagae and Lavie , 2006 ; Sagae and Tsujii , 2007 ; Zhao et al. , 2013 ; Thang et al. , 2015 ; Lee et al. , 2016 ) give optimality certificates when solutions are found , but have the same worst - case time complexity as the original search framework .
Other common approaches to search a larger space at training or test time include beam search ( Zhang and Clark , 2011 ) , dynamic oracles Nivre , 2012 , 2013 ; Cross and Huang , 2016 b ) and error states ( Vaswani and Sagae , 2016 ) .
Beam search records the k best-scoring transition prefixes to delay local hard decisions , while the latter two leverage configurations deviating from the gold transition path during training to better simulate the test-time environment .
Neural Parsing Neural- network - based models are widely used in state - of- the - art dependency parsers ( Henderson , 2003 ( Henderson , , 2004 Chen and Manning , 2014 ; Weiss et al. , 2015 ; Andor et al. , 2016 ; Dozat and Manning , 2017 ) because of their expressive representation power .
Recently , Stern et al. ( 2017 ) have proposed minimal span-based features for constituency parsing .
Recurrent and recursive neural networks can be used to build representations that encode complete configuration information or the entire parse tree ( Le and Zuidema , 2014 ; Dyer et al. , 2015 ; Kiperwasser and Goldberg , 2016 b ) , but these models cannot be readily combined with DP approaches , because their state spaces cannot be merged into smaller sets and thus remain exponentially large .
Concluding Remarks
In this paper , we have shown the following .
?
The bi-LSTM - powered feature set t ? s 0 , ? b 0 u is minimal yet highly effective for arc-hybrid and arc-eager transition - based parsing .
?
Since DP algorithms for exact decoding ( Huang and Sagae , 2010 ; Kuhlmann et al. , 2011 ) have a run-time dependence on the number of positional features , using our mere two effective positional features results in a running time of Opn 3 q , feasible for practice .
?
Combining exact decoding with global training - which is also enabled by our minimal feature set - with an ensemble of parsers achieves 90.22 UAS on the Chinese Treebank and 95.33 UAS on the Penn Treebank : these are , to our knowledge , the best and secondbest results to date on these data sets among " purely " dependency - based approaches .
There are many directions for further exploration .
Two possibilities are to create even better training methods , and to find some way to extend our run-time improvements to other transition systems .
It would also be interesting to further investigate relationships between graph - based and dependency - based parsing .
In ?5 we have mentioned important earlier work in this regard , and provided an update to those formal findings .
In our work , we have brought exact decoding , which was formerly the province solely of graphbased parsing , to the transition - based paradigm .
We hope that the future will bring more inspiration from an integration of the two perspectives .
shrp ? , b 0 |? , Aqs " p?|b 0 , ? , Aq re ? rp?|s 0 , b 0 |? , Aqs " p? , b 0 |? , A Y tpb 0 , s 0 quq ( precondition : s 0 not attached to any word ) rarp ?|s 0 , b 0 |? , Aqs " p?|s 0 |b 0 , ? , A Y tps 0 , b 0 quq rerp ?|s 0 , ? , Aqs " p? , ? , Aq ( precondition : s 0 has been attached to its head ) 3.2 Deduction and Dynamic Programming Kuhlmann et al. ( 2011 ) reformulate the three transition systems just discussed as deduction systems
d) Edge-factored graph - based parsing .
Figure 1 : 1 Figure 1 : 1a- 1c : Kuhlmann et al . 's inference rules for three transition systems , together with CKY - style visualizations of the local structures involved and , to their right , conditions for the rule to apply .
1d : the edge-factored graph- based parsing algorithm ( Eisner and Satta , 1999 ) discussed in ?5 .
Table 2 : 2 for edges k ?
i in the other direction , we set Test set performance for different training regimes and feature sets .
The models use the same decoders for testing and training .
For each setting , the average and standard deviation across 5 runs with different random initializations are reported .
Boldface : best ( averaged ) result per dataset / measure .
Model Training Features PTB UAS ( % ) UEM ( % ) CTB UAS ( % ) UEM ( % ) Arc-standard Local t ? s 2 , ? s 1 , ? s 0 , ? b 0u 93.95 ?0.12 52.29 ?0.66 88.01 ?0.26 36.87 ?0.53 Arc-hybrid Local Local Global t ? s 2 , t ? s 1 , ? s 0 , t ? s 0 , ? s 0 , ? b 0u ? b 0u ? b 0u 93.89 ?0.10 50.82 ?0.75 93.80 ?0.12 49.66 ?0.43 94.43 ?0.08 53.03 ?0.71 87.87 ?0.17 35.47 ?0.48 87.78 ?0.09 35.09 ?0.40 88.38 ?0.11 36.59 ?0.27 Arc-eager Local Local Global t ? s 2 , t ? s 1 , ? s 0 , t ? s 0 , ? s 0 , ? b 0u ? b 0u ? b 0u 93.80 ?0.12 49.66 ?0.43 93.77 ?0.08 49.71 ?0.24 94.53 ?0.05 53.77 ?0.46 87.49 ?0.20 33.15 ?0.72 87.33 ?0.11 34.17 ?0.41 88.62 ?0.09 37.75 ?0.87 Edge-factored Global t ? h , ? mu 94.50 ?0.13 53.86 ?0.78 88.25 ?0.12 36.42 ?0.52
Comparing our UAS results with results from the literature .
x-axis : PTB ; y-axis : CTB .
Most datapoint labels give author initials and publication year ; citations are in the bibliography .
Ensemble datapoints are annotated with ensemble size .
Weiss et al. ( 2015 ) and Andor et al . ( 2016 ) achieve UAS of 94.26 and 94.61 on PTB with beam search , but did not report CTB results , and are therefore omitted .
90.5 ?15
Our all global 90.0 ?20 KBKDS16 ?5 Our arc-eager DP 89.5 ?5 Our arc-hybrid DP ( % ) 89.0 ?KBKDS16 ?DM17 on CTB 88.0 88.5 ?
Our best local ?CFHGD16 ? Our arc-eager DP ? Our arc-hybrid DP UAS 87.5 ? BGDS16 ? DBLMS15 ? KG16a ?WC16 87.0 ? KG16 b 86.5 ? CH16 ?KG16a 86.0 93.0 93.5 94.0 94.5 95.0 95.5 96.0 UAS on PTB ( % ) ? Local ? Global ?
Our Local ?
Our Global ? Ensemble Figure 2 :
, and the exact decoding algorithms are implemented in Cython.7 Main Results
We implement exact decoders for the arc-hybrid and arc-eager systems , and present the test performance of different model configu - rations in Table 2 , comparing global models with local models .
All models use the same decoder for testing as during the training process .
Though no global decoder for the arc-standard system has been explored in this paper , its local models are listed for comparison .
We also include an edge- factored graph - based model , which is convention - ally trained globally .
The edge-factored model scores bi-LSTM features for each head-modifier pair ; a maximum spanning tree algorithm is used to find the tree with the highest sum of edge scores .
For this model , we use Dozat and Man - 7 See https://github.com/tzshi/dp-parser-emnlp17 . ning 's ( 2017 ) biaffine scoring model , although in our case the model size is smaller .8
We note that K&G were not focused on minimizing positions , although they explicitly noted the implications of doing so : " While not explored in this work , [ fewer positions ] results in very compact state signatures , [ which is ] very appealing for use in transition - based parsers that employ dynamicprogramming search " ( pg. 319 ) .
C&H also noted in their follow - up ( Cross and Huang , 2016 b ) the possibility of future work using dynamic programming thanks to simple features .
For simplicity , we only present unlabeled parsing here .
See Shi et al. ( 2017 ) for labeled - parsing results .5
Other presentations place ROOT at the end of the buffer or omit it entirely ( Ballesteros and Nivre , 2013 ) .
For bi-LSTM input and recurrent connections , we consider dropout rates in t0. , 0.2u , and for MLP , t0. , 0.4u .
The same architecture and model size as other transitionbased global models is used for fair comparison .9
We exclude Choe and Charniak ( 2016 ) , Kuncoro et al . ( 2017 ) and Liu and Zhang ( 2017 ) , which convert constituentbased parses to dependency parses .
They produce higher PTB UAS , but access more training information and do not directly apply to datasets without constituency annotation .
