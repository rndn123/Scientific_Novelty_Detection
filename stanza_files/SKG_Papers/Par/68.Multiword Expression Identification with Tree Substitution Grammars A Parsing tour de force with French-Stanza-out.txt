title
Multiword Expression Identification with Tree Substitution Grammars : A Parsing tour de force with French
abstract
Multiword expressions ( MWE ) , a known nuisance for both linguistics and NLP , blur the lines between syntax and semantics .
Previous work on MWE identification has relied primarily on surface statistics , which perform poorly for longer MWEs and cannot model discontinuous expressions .
To address these problems , we show that even the simplest parsing models can effectively identify MWEs of arbitrary length , and that Tree Substitution Grammars achieve the best results .
Our experiments show a 36.4 % F1 absolute improvement for French over an n-gram surface statistics baseline , currently the predominant method for MWE identification .
Our models are useful for several NLP tasks in which MWE pre-grouping has improved accuracy .
Introduction Multiword expressions ( MWE ) have long been a challenge for linguistic theory and NLP .
There is no universally accepted definition of the term , but MWEs can be characterized as " idiosyncratic interpretations that cross word boundaries ( or spaces ) " ( Sag et al. , 2002 ) such as traffic light , or as " frequently occurring phrasal units which are subject to a certain level of semantic opaqueness , or noncompositionality " ( Rayson et al. , 2010 ) .
MWEs are often opaque fixed expressions , although the degree to which they are fixed can vary .
Some MWEs do not allow morphosyntactic variation or internal modification ( e.g. , in short , but * in shorter or * in very short ) .
Other MWEs are " semifixed , " meaning that they can be inflected or undergo internal modification .
The type of modification is often limited , but not predictable , so it is not possible to enumerate all variants ( Table 1 ) .
1 : Semi-fixed MWEs in French and English .
The French adverb ? terme ' in the end ' can be modified by a small set of adjectives , and in turn some of these adjectives can be modified by an adverb such as tr ?s ' very ' .
Similar restrictions appear in English .
Merging known MWEs into single tokens has been shown to improve accuracy for a variety of NLP tasks : dependency parsing ( Nivre and Nilsson , 2004 ) , constituency parsing ( Arun and Keller , 2005 ) , sentence generation ( Hogan et al. , 2007 ) , and machine translation ( Carpuat and Diab , 2010 ) .
Most experiments use gold MWE pre-grouping or languagespecific resources like WordNet .
For unlabeled text , the best MWE identification methods , which are based on surface statistics ( Pecina , 2010 ) , suffer from sparsity induced by longer n-grams ( Ramisch et al. , 2010 ) .
A dilemma thus exists : MWE knowledge is useful , but MWEs are hard to identify .
In this paper , we show the effectiveness of statistical parsers for MWE identification .
Specifically , Tree Substitution Grammars ( TSG ) can achieve a 36.4 % F1 absolute improvement over a state - of - theart surface statistics method .
We choose French , which has pervasive MWEs , for our experiments .
Parsing models naturally accommodate discontinuous MWEs like phrasal verbs , and provide syntactic subcategorization .
By contrast , surface statistics methods are usually limited to binary judgements for contiguous n-grams or dependency bigrams .
We first introduce a new instantiation of the French Treebank that , unlike previous work , does not use gold MWE pre-grouping .
Consequently , our experimental results also provide a better baseline for parsing raw French text .
French Treebank Setup
The corpus used in our experiments is the French Treebank ( Abeill ?
et al. ( 2003 ) , version from June 2010 , hereafter FTB ) .
In French , there is a linguistic tradition of lexicography which compiles lists of MWEs occurring in the language .
For example , Gross ( 1986 ) shows that dictionaries contain about 1,500 single - word adverbs but that French con-tains over 5,000 multiword adverbs .
MWEs occur in every part-of-speech ( POS ) category ( e.g. , noun trousse de secours ' first - aid kit ' ; verb faire mainbasse [ do hand - low ] ' seize ' ; adverb comme dans du beurre [ as in butter ] ' easily ' ; adjective '? part enti?re ' ' wholly ' ) .
The FTB explicitly annotates MWEs ( also called compounds in prior work ) .
We used the subset of the corpus with functional annotations , not for those annotations but because this subset is known to be more consistently annotated .
POS tags for MWEs are given not only at the MWE level , but also internally : most tokens that constitute an MWE also have a POS tag .
Table 2 compares this part of the FTB to the WSJ portion of the Penn Treebank .
Preprocessing
The FTB requires significant pre-processing prior to parsing .
Tokenization
We changed the default tokenization for numbers by fusing adjacent digit tokens .
For example , 500 000 is tagged as an MWE composed of two words 500 and 000 .
We made this 500000 and retained the MWE POS , although we did not mark the new token as an MWE .
For consistency , we used one token for punctuated numbers like " 17,9 " .
MWE Tagging
We marked MWEs with a flat bracketing in which the phrasal label is the MWElevel POS tag with an " MW " prefix , and the preterminals are the internal POS tags for each terminal .
The resulting POS sequences are not always unique to MWEs : they appear in abundance elsewhere in the corpus .
However , some MWEs contain normally ungrammatical POS sequences ( e.g. , adverb ? la va vite ' in a hurry ' :
P D V ADV [ at the goes quick ] ) , and some words appear only as part of an MWE , such as insu in ? l'insu de ' to the ignorance of ' .
Labels
We augmented the basic FTB label setwhich contains 14 POS tags and 19 phrasal tags - in two ways .
First , we added 16 finer- grained POS tags for punctuation .
1 Second , we added the 11 MWE labels shown in Table 3 , resulting in 24 total phrasal categories .
Corrections
Historically , the FTB suffered from annotation errors such as missing POS and phrasal tags ( Arun and Keller , 2005 ) .
We found that this problem has been largely resolved in the current release .
However , 1,949 tokens and 36 MWE spans still lacked tags .
We restored the labels by first assigning each token its most frequent POS tag elsewhere in the treebank , and then assigning the most frequent MWE phrasal category for the resulting POS sequence .
2 Split We used the 80/10/10 split described by Crabb? and Candito ( 2008 ) .
However , they used a previous release of the treebank with 12,531 trees .
3,391 trees have been added to the present version .
We appended these extra trees to the training set , thus retaining the same development and test sets .
Comparison to Prior FTB Representations
Our pre-processing approach is simple and automatic 3 unlike the three major instantiations of the FTB that have been used in previous work :
A -C and A - E ( Arun and Keller , 2005 ) :
Two instantiations of the full 20,000 sentence treebank that differed principally in their treatment of MWEs : ( 1 ) C , in which the tokens of each MWE were concatenated into a single token ( en moyenne ? en_moyenne ) ; ( 2 ) E , in which they were marked with a flat structure .
For both representations , they also gave results in which coordinated phrase structures were flattened .
In the published experiments , they mistakenly removed half of the corpus , believing that the multi-terminal ( per POS tag ) annotations of MWEs were XML errors ( Schluter and Genabith , 2007 ) . MFT ( Schluter and Genabith , 2007 ) : Manual revision to 3,800 sentences .
Major changes included coordination raising , an expanded POS tag set , and the 2 73 of the unlabeled word types did not appear elsewhere in the treebank .
All but 11 of these were nouns .
We manually assigned the correct tags , but we would not expect a negative effect by deterministically labeling all of them as nouns .
3
We automate tree manipulation with Tregex / Tsurgeon ( Levy and Andrew , 2006 ) .
Our pre-processing package is available at http://nlp.stanford.edu/software/lex-parser.shtml.
correction of annotation errors .
Like A - C , MFT contains concatenated MWEs . :
An instantiation of the functionally annotated section that makes a distinction between MWEs that are " syntactically regular " and those that are not .
Syntactically regular MWEs were given internal structure , while all other MWEs were concatenated into single tokens .
For example , nouns followed by adjectives , such as loi agraire ' land law ' or Union mon? taire et ?conomique ' monetary and economic Union ' were considered syntactically regular .
They are MWEs because the choice of adjective is arbitrary ( loi agraire and not * loi agricole , similarly to ' coal black ' but not * ' crow black ' for example ) , but their syntactic structure is not intrinsic to MWEs .
In such cases , FTB - UC gives the MWE a conventional analysis of an NP with internal structure .
Such analysis is indeed sufficient to recover the meaning of these semantically compositional MWEs that are extremely productive .
On the other hand , the FTB - UC loses information about MWEs with noncompositional semantics .
FTB -UC
Almost all work on the FTB has followed A - C and used gold MWE pre-grouping .
As a result , most results for French parsing are analogous to early results for Chinese , which used gold word segmentation , and Arabic , which used gold clitic segmentation .
Candito et al. ( 2010 ) were the first to acknowledge and address this issue , but they still used FTB - UC ( with some pre-grouped MWEs ) .
Since the syntax and definition of MWEs is a contentious issue , we take a more agnostic view - which is consistent with that of the FTB annotators - and leave them tokenized .
This permits a data-oriented approach to MWE identification that is more robust to changes to the status of specific MWE instances .
our work is on models and data representations that enable MWE identification .
MWEs in Lexicon- Grammar
The MWE representation in the FTB is close to the one proposed in the Lexicon - Grammar ( Gross , 1986 ) .
In the Lexicon- Grammar , MWEs are classified according to their global POS tags ( noun , verb , adverb , adjective ) , and described in terms of the sequence of the POS tags of the words that constitute the MWE ( e.g. , " N de N " garde d'enfant [ guard of child ] ' daycare ' , pied de guerre [ foot of war ] ' at the ready ' ) .
In other words , MWEs are represented by a flat structure .
The Lexicon - Grammar distinguishes between units that are fixed and have to appear as is ( en tout et pour tout [ in all and for all ] ' in total ' ) and units that accept some syntactic variation such as admitting the insertion of an adverb or adjective , or the variation of one of the words in the expression ( e.g. , a possessive as in ' from the top of one 's hat ' ) .
It also notes whether the MWE displays some selectional preferences ( e.g. , it has to be preceded by a verb or by an adjective ) .
Our FTB instantiation is largely consistent with the Lexicon- Grammar .
Recall that we defined different MWE categories based on the global POS .
We now detail three of the categories .
MWN
The MWN category consists of proper nouns ( 1a ) , foreign common nouns ( 1 b ) , as well as common nouns .
The common nouns appear in several syntactically regular sequences of POS tags ( 2 ) .
Multiword nouns allow inflection ( singular vs. plural ) but no insertion . ( 1 ) a. fragments larger than basic CFG rules .
PTSG rules may also be lexicalized .
This means that commonly observed collocations -some of which are MWEscan be stored in the grammar .
Stanford Parser
We configure the Stanford parser with settings that are effective for other languages : selective parent annotation , lexicon smoothing , and factored parsing .
We use the head-finding rules of Dybro-Johansen ( 2004 ) , which we find to yield an approximately 1.0 % F1 development set improvement over those of Arun ( 2004 ) .
Finally , we include a simple unknown word model consisting entirely of surface features : - Nominal , adjectival , verbal , adverbial , and plural suffixes - Contains a digit or punctuation - Is capitalized ( except the first word in a sentence ) - Consists entirely of capital letters - If none of the above , add a one - or two -character suffix Combined with the grammar features , this unknown word model yields 97.3 % tagging accuracy on the development set .
Grammar Development
Table 4 lists the symbol refinements used in our grammar .
Most of the features are POS splits as many phrasal tag splits did not lead to any improvement .
Parent annotation of POS tags ( tagPA ) captures information about the external context .
mark - Inf and markPart accomplish a finite / nonfinite distinction : they respectively specify whether the verb is an infinitive or a participle based on the type of the grandparent node .
markVN captures the notion of verbal distance as in Klein and Manning ( 2003 ) .
We opted to keep the COORD phrasal tag , and to capture parallelism in coordination , we mark CO - ORD with the type of its child ( NP , AP , VPinf , etc. ) .
markDe identifies the preposition de and its variants ( du , des , d ' ) which is very frequent and appears in several different contexts .
markP identifies prepositions which introduce PPs modifying a noun .
Marking other kinds of prepositional modifiers ( e.g. , verb ) did not help .
markMWE adds an annotation to several MWE categories for frequently occuring POS sequences .
For example , we mark MWNs that occur more than 600 times ( e.g. , " N P N " and " N N " ) .
DP -TSG Parser
A shortcoming of CFG - based grammars is that they do not explicitly capture idiomatic usage .
For example , consider the two utterances :
The examples in ( 5 ) may be equally probable and receive the same analysis under a PCFG ; words are generated independently .
However , recall that in our representation , ( 5a ) should receive a flat analysis as MWV , whereas ( 5 b ) should have a conventional analysis of the verb kicked and its two arguments .
An alternate view of parsing is one in which new utterances are built from previously observed fragments .
This is the original motivation for data oriented parsing ( DOP ) ( Bod , 1992 ) , in which " idiomaticity is the rule rather than the exception " ( Scha , 1990 ) .
If we have seen the collocation kicked the bucket several times before , we should store that whole fragment for later use .
We consider a variant of the non-parametric PTSG model of Cohn et al . ( 2009 ) in which tree fragments are drawn from a Dirichlet process ( DP ) prior .
4 The DP -TSG can be viewed as a DOP model with Bayesian parameter estimation .
A PTSG is a 5 - tuple V , ? , R , ? , ? where c ?
V are non-terminals ; t ? ? are terminals ; e ?
R are elementary trees ; 5 ? ?
V is a unique start symbol ; and ?
c,e ? ? are parameters for each tree fragment .
A PTSG derivation is created by successively applying the substitution operator to the leftmost frontier node ( denoted by c + ) .
All other nodes are internal ( denoted by c ? ) .
In the supervised setting , DP - TSG grammar extraction reduces to a segmentation problem .
We have a treebank T that we segment into the set R , a process that we model with Bayes ' rule : p( R | T ) ? p( T | R ) p ( R ) ( 1 ) Since the tree fragments completely specify each tree , p( T | R ) is either 0 or 1 , so all work is performed by the prior over the set of elementary trees .
The DP -TSG contains a DP prior for each c ?
V ( Table 5 defines further notation ) .
We generate c , e tuples as follows : ? c |c , ? c , P 0 ( ?|c ) ? DP (? c , P 0 ) e|? c ? ? c
The data likelihood is given by the latent state z and the parameters ? : p( z| ? ) = z?z ? nc , e( z ) c, e .
Integrating out the parameters , we have : p( z ) = c?V e ( ? c P 0 ( e| c ) ) nc , e( z ) ? nc , ?( z ) c ( 2 ) where x n = x( x + 1 ) . . . ( x + n ? 1 ) is the rising factorial .
( ?A.1 contains ancillary details . )
Base Distribution
The base distribution P 0 is the same maximum likelihood PCFG used in the Stan - 5 We use the terms tree fragment and elementary tree interchangeably .
Sites ( 1 ) and ( 2 ) above have the same type since t( z , s 1 ) = t( z , s 2 ) .
However , the two sites conflict since the probabilities of setting b s1 and b s2 both depend on counts for the tree fragment rooted at NP .
Consequently , sites ( 1 ) and ( 2 ) are not exchangeable : the probabilities of their assignments depend on the order in which they are sampled .
ford parser .
6 , 7 After applying the manual state splits , we perform simple right binarization , collapse unary rules , and replace rare words with their signatures ( Petrov et al. , 2006 ) .
For each non-terminal type c , we learn a stop probability s c ? Beta ( 1 , 1 ) .
Under P 0 , the probability of generating a rule A + ? B ? C + composed of nonterminals is P 0 ( A + ? B ? C + ) = p MLE ( A ? B C )s B ( 1?s C ) ( 3 )
For lexical insertion rules , we add a penalty proportional to the frequency of the lexical item : P 0 ( c ? t ) = p MLE ( c ? t ) p( t ) ( 4 ) where p( t ) is equal to the MLE unigram probability of t in the treebank .
Lexicalizing a rule makes it very specific , so we generally want to avoid lexicalization with rare words .
Empirically , we found that this penalty reduces overfitting .
Type-based Inference Algorithm
To learn the parameters ? we use the collapsed , block Gibbs sampler of Liang et al . ( 2010 ) .
We sample binary variables b s associated with each non-terminal node / site in the treebank .
The key idea is to select a block of exchangeable sites S of the same type that do not conflict ( Figure 1 ) .
Since the sites in S are exchangeable , we can set b S randomly so long as we know m , the number of sites with b s = 1 .
Because this algorithm is a not a contribution of this paper , we refer the reader to Liang et al . ( 2010 ) .
After each Gibbs iteration , we sample each s c directly using binomial - Beta conjugacy .
We re-sample the DP concentration parameters ? c with the auxiliary variable procedure of West ( 1995 ) .
Decoding
We compute the rule score of each tree fragment from a single grammar sample as follows : ? c , e = n c,e ( z ) + ? c P 0 ( e|c ) n c , ? ( z ) + ? c ( 5 ) To make the grammar more robust , we also include all CFG rules in P 0 with zero counts in n.
Scores for these rules follow from ( 5 ) with n c,e ( z ) = 0 .
For decoding , we note that the derivations of a TSG are a CFG parse forest ( Vijay - Shanker and Weir , 1993 ) .
As such , we can use a Synchronous Context Free Grammar ( SCFG ) to translate the 1 - best parse to its derivation .
Consider a unique tree fragment e i rooted at X with frontier ? , which is a sequence of terminals and non-terminals .
We encode this fragment as an SCFG rule of the form [ X ? ? , X ? i , Y 1 , . . . , Y n ] ( 6 ) where Y 1 , . . . , Y n is the sequence of non-terminal nodes in ?.
8 During decoding , the input is rewritten as a sequence of tree fragment ( rule ) indices { i , j , k , . . . }.
Because the TSG substitution operator always applies to the leftmost frontier node , we can deterministically recover the monolingual parse with top-down re-writes of ?.
The SCFG formulation has a practical benefit : we can take advantage of the heavily - optimized SCFG decoders for machine translation .
We use cdec ( Dyer et al. , 2010 ) to recover the Viterbi derivation under a DP - TSG grammar sample .
Experiments
Standard Parsing Experiments
We evaluate parsing accuracy of the Stanford and DP - TSG models ( Table 6 ) .
For comparison , we also include the Berkeley parser ( Petrov et al. , 2006 ) . 9
For the DP - TSG , we initialized all b s with fair coin tosses and ran for 400 iterations , after which likelihood stopped improving .
We report two different parsing metrics .
Evalb is the standard labeled precision / recall metric .
10 Leaf Ancestor measures the cost of transforming guess trees to the reference ( Sampson and Babarczy , 2003 ) .
It was developed in response to the nonterminal / terminal ratio bias of Evalb , which penalizes flat treebanks like the FTB .
The range of the score is between 0 and 1 ( higher is better ) .
We report micro-averaged ( whole corpus ) and macro-averaged ( per sentence ) scores .
In terms of parsing accuracy , the Berkeley parser exceeds both Stanford and DP - TSG .
This is consistent with previous experiments for French by Seddah et al . ( 2009 ) , who show that the Berkeley parser outperforms other models .
It also matches the ordering for English ( Cohn et al. , 2010 ; Liang et al. , 2010 ) .
However , the standard baseline for TSG models is a simple parent- annotated PCFG ( PA - PCFG ) .
For English , Liang et al. ( 2010 ) showed that a similar DP - TSG improved over PA - PCFG by 4.2 % F1 .
For French , our gain is a more substantial 8.2 % F1 .
MWE Identification Experiments
Table 7 lists overall and per-category MWE identification results for the parsing models .
Although DP - TSG is less accurate as a general parsing model , it is more effective at identifying MWEs .
The predominant approach to MWE identification is the combination of lexical association measures ( surface statistics ) with a binary classifier ( Pecina , 2010 ) .
A state- of- the-art , language independent package that implements this approach for higher order n-grams is mwetoolkit ( Ramisch et al. , 2010 ) .
11
In mwetoolkit and the CFG from the which the TSG is extracted .
The TSG - based parsing model outperforms mwetoolkit by 36.4 % F1 while providing syntactic subcategory information .
Discussion Automatic learning methods run the risk of producing uninterpretable models .
However , the DP - TSG model learns useful generalizations over MWEs .
A sample of the rules is given in data ) .
For MWV , " V de N " as in avoir de cesse ' give no peace ' , perdre de vue [ lose from sight ] ' forget ' , prendre de vitesse [ take from speed ] ' outpace ' ) , is learned .
For prepositions , the grammar stores full subtrees of MWPs , but can also generalize the structure of very frequent sequences : " en N de " occurs in many multiword prepositions ( e.g. , en compagnie de , en face de , en mati ?
re de , en terme de , en cours de , en faveur de , en raison de , en fonction de ) .
A significant fraction of errors for MWNs occur with adjectives that are not recognized as part of the MWE .
For example , since ? tablissements priv?s ' private corporation ' is unseen in the training data , it is not found .
Sometimes the parser did not recognize the whole structure of an MWE .
Figure 2 shows an example where the parser only found a subpart of the MWN tour de passe- passe ' magic trick ' .
Other DP - TSG errors are due to inconsistencies in the FTB annotation .
For example , sous pr?texte que 732 ' on the pretext of ' is tagged as both MWC and as a regular PP structure ( Figure 3 ) .
However , the parser always assigns a MWC structure , which is a better analysis than the gold annotation .
We expect that more consistent annotation would help the DP - TSG more than the CFG - based parsers .
The DP -TSG is not immune to false positives : in Le march ?
national , fait - on remarquer , est enfin en r?gression . . .
' The national economy , people at last note , is going down ' the parser tags march ?
national as MWN .
As noted , the boundary of what should and should not count as an MWE can be fuzzy , and it is therefore hard to assess whether or not this should be an MWE .
The FTB does not mark it one .
There are multiple examples were the DP - TSG found the MWE whereas Stanford ( its base distribution ) did not , such as in Figure 4 . Note that the " N P N " structure is quite frequent for MWNs , but the TSG correctly identifies the MWADV in emplois ? domicile [ jobs at home ] ' homeworking ' .
Related Work
There is a voluminous literature on MWE identification .
Here we review closely related syntaxbased methods .
12
The linguistic and computational attractiveness of lexicalized grammars for modeling idiosyncratic constructions in French was identified by Abeill ? ( 1988 ) and Abeill ? and Schabes ( 1989 ) .
They manually developed a small Tree Adjoining Grammar ( TAG ) of 1,200 elementary trees and 4,000 lexical items that included MWEs .
The classic statistical approach to MWE identification , Xtract ( Smadja , 1993 ) , used an in - 12 See Seretan ( 2011 ) for a comprehensive survey of syntaxbased methods for MWE identification .
For an overview of ngram methods like mwetoolkit , see Pecina ( 2010 ) . cremental parser in the third stage of its pipeline to identify predicate - argument relationships .
Lin ( 1999 ) applied information -theoretic measures to automatically - extracted dependency relations to find MWEs .
To our knowledge , Wehrli ( 2000 ) was the first to use syntactically annotated corpora to improve a parser for MWE identification .
He proposed to rank analyses of a symbolic parser based on the presence of collocations , although details of the ranking function were not provided .
The most similar work to ours is that of Nivre and Nilsson ( 2004 ) , who converted a Swedish corpus into two versions : one in which MWEs were left as tokens , and one in which they were merged .
On the first version , they showed that a deterministic dependency parser could identify MWEs at 71.1 % F1 , albeit without subcategory information .
On the second version - which simulated perfect MWE identification - they showed that labeled attachment improved by about 1 % .
Recent statistical parsing work on French has included Stochastic Tree Insertion Grammars ( STIGs ) , which are related to TAGs , but with a restricted adjunction operation .
13 Seddah et al. ( 2009 ) and Seddah ( 2010 ) showed that STIGs underperform CFGbased parsers on the FTB .
In their experiments , MWEs were concatenated .
Conclusion
The main result of this paper is that an existing statistical parser can achieve a 36.4 % F1 absolute improvement for MWE identification over a state- ofthe - art n-gram surface statistics package .
Parsers also provide syntactic subcategorization , and do not require pre-filtering of the training data .
We have also demonstrated that TSGs can capture idiomatic usage better than a PCFG .
While the DP - TSG , which is a relatively new parsing model , still lags state - ofthe - art parsers in terms of overall labeling accuracy , we have shown that it is already very effective for other tasks like MWE identification .
We plan to improve the DP - TSG by experimenting with alternate parsing objectives ( Cohn et al. , 2010 ) , lexical representations , and parameterizations of the base distribution .
A particularly promising base distribution is the latent variable PCFG learned by the Berkeley parser .
However , initial experiments with this distribution were negative , so we leave further development to future work .
We chose French for these experiments due to the pervasiveness of MWEs and the availability of an annotated corpus .
However , MWE lists and syntactic treebanks exist for many of the world 's major languages .
We will investigate automatic conversion of these treebanks ( by flattening MWE bracketings ) for MWE identification .
