title
CoNLL 2018 Shared Task : Multilingual Parsing from Raw Text to Universal Dependencies
abstract
Every year , the Conference on Computational Natural Language Learning ( CoNLL ) features a shared task , in which participants train and test their learning systems on the same data sets .
In 2018 , one of two tasks was devoted to learning dependency parsers for a large number of languages , in a real-world setting without any gold -standard annotation on the input .
All test sets followed the unified annotation scheme of Universal Dependencies ( Nivre et al. , 2016 ) .
This shared task constitutes a 2 nd edition - the first one took place in 2017 ( Zeman et al. , 2017 ) ; the main metric from 2017 was kept , allowing for easy comparison , and two new main metrics were introduced .
New datasets added to the Universal Dependencies collection between mid-2017 and the spring of 2018 contributed to the increased difficulty of the task this year .
In this overview paper , we define the task and the updated evaluation methodology , describe data preparation , report and analyze the main results , and provide a brief categorization of the different approaches of the participating systems .
There are eight new languages this year : Afrikaans , Armenian , Breton , Faroese , Naija , Old French , Serbian , and Thai ; see Section 2 for more details .
The two new evaluation metrics are described in Section 3 .
Introduction
The 2017 CoNLL shared task on universal dependency parsing ( Zeman et al. , 2017 ) picked up the thread from the influential shared tasks in 2006 and 2007 ( Buchholz and Marsi , 2006 ; Nivre et al. , 2007 ) and evolved it in two ways : ( 1 ) the parsing process started from raw text rather than gold standard tokenization and part- of-speech tagging , and ( 2 ) the syntactic representations were consistent across languages thanks to the Universal Dependencies framework ( Nivre et al. , 2016 ) .
The 2018 CoNLL shared task on universal dependency parsing starts from the same premises but adds a focus on morphological analysis as well as data from new languages .
Like last year , participating systems minimally had to find labeled syntactic dependencies between words , i.e. , a syntactic head for each word , and a label classifying the type of the dependency relation .
In addition , this year 's task featured new metrics that also scored a system 's capacity to predict a morphological analysis of each word , including a part- of-speech tag , morphological features , and a lemma .
Regardless of metric , the assumption was that the input should be raw text , with no gold -standard word or sentence segmentation , and no gold -standard morphological annotation .
However , for teams who wanted to concentrate on one or more subtasks , segmentation and morphology predicted by the baseline UDPipe system ( Straka et al. , 2016 ) was made available just like last year .
Data
In general , we wanted the participating systems to be able to use any data that is available free of charge for research and educational purposes ( so that follow - up research is not obstructed ) .
We deliberately did not place upper bounds on data sizes ( in contrast to e.g. Nivre et al. ( 2007 ) ) , despite the fact that processing large amounts of data may be difficult for some teams .
Our primary objective was to determine the capability of current parsers provided with large amounts of freely available data .
In practice , the task was formally closed , i.e. , we listed the approved data resources so that all participants were aware of their options .
However , the selection was rather broad , ranging from Wikipedia dumps over the OPUS parallel corpora ( Tiedemann , 2012 ) to morphological transducers .
Some of the resources were proposed by the participating teams .
We provided dependency - annotated training and test data , and also large quantities of crawled raw texts .
Other language resources are available from third - party servers and we only referred to the respective download sites .
Training Data : UD 2.2
Training and development data came from the Universal Dependencies ( UD ) 2.2 collection ( Nivre et al. , 2018 ) .
This year , the official UD release immediately followed the test phase of the shared task .
The training and development data were available to the participating teams as a prerelease ; these treebanks were then released exactly in the state in which they appeared in the task .
1
The participants were instructed to only use the UD data from the package released for the shared task .
In theory , they could locate the ( yet unreleased ) test data in the development repositories on GitHub , but they were trusted that they would not attempt to do so .
82 UD treebanks in 57 languages were included in the shared task ; 2 however , nine of the smaller treebanks consisted solely of test data , with no data at all or just a few sentences available for training .
16 languages had two or more treebanks from different sources , often also from different domains .
3 See Table 1 for an overview .
61 treebanks contain designated development data .
Participants were asked not to use it for training proper but only for evaluation , development , tuning hyperparameters , doing error analysis etc .
Seven treebanks have reasonablysized training data but no development data ; only two of them , Irish and North S?mi , are the sole treebanks of their respective languages .
For those treebanks cross-validation had to be used during development , but the entire dataset could be used for training once hyperparameters were determined .
Five treebanks consist of extra test sets : they have no training or development data of their own , but large training data exist in other treebanks of the same languages ( Czech - PUD , English - PUD , Finnish - PUD , Japanese -Modern and Swedish - PUD , respectively ) .
The remaining nine treebanks are low-resource languages .
Their " training data " was either a tiny sample of a few dozen sentences ( Armenian , Buryat , Kazakh , Kurmanji , Upper Sorbian ) , or there was no training data at all ( Breton , Faroese , Naija , Thai ) .
Unlike in the 2017 task , these languages were not " surprise languages " , that is , the participants knew well in advance what languages to expect .
The last two languages are particularly difficult : Naija is a pidgin spoken in Nigeria ; while it can be expected to bear some similarity to English , its spelling is significantly different from standard English , and no resources were available to learn it .
Even harder was Thai with a writing system that does not separate words by spaces ; the Facebook word vectors were probably the only resource among the approved additional data where participants could learn something about words in Thai ( Rosa and Mare?ek , 2018 ; Smith et al. , 2018 ) .
It was also possible to exploit the fact that there is a 1 - 1 sentence mapping between the Thai test set and the other four PUD test sets .
4 Participants received the training and development data with gold-standard tokenization , sentence segmentation , POS tags and dependency re- lations ; and for most languages also lemmas and morphological features .
Cross-domain and cross-language training was allowed and encouraged .
Participants were free to train models on any combination of the training treebanks and apply it to any test set .
Supporting Data
To enable the induction of custom embeddings and the use of semi-supervised methods in general , the participants were provided with supporting resources primarily consisting of large text corpora for many languages in the task , as well as embeddings pre-trained on these corpora .
In total , 5.9 M sentences and 90 G words in 45 languages are available in CoNLL - U format ( Ginter et al. , 2017 ) ; the per-language sizes of the corpus are listed in Table 2 . See Zeman et al. ( 2017 ) for more details on how the raw texts and embeddings were processed .
Note that the resource was originally prepared for the 2017 task and it was not extended to include the eight new languages ; however , some of the new languages are covered by the word vectors provided by Facebook ( Bojanowski et al. , 2016 ) and approved for the shared task .
Evaluation Metrics
There are three main evaluation scores , dubbed LAS , MLAS and BLEX .
All three metrics reflect word segmentation and relations between content words .
LAS is identical to the main metric of the 2017 task , allowing for easy comparison ; the other two metrics include part- of-speech tags , morphological features and lemmas .
Participants who wanted to decrease task complexity could concentrate on improvements in just one metric ; however , all systems were evaluated with all three metrics , and participants were strongly encouraged to output all relevant annotation , even if they just copy values predicted by the baseline model .
When parsers are applied to raw text , the metric must be adjusted to the possibility that the number of nodes in gold -standard annotation and in the system output vary .
Therefore , the evaluation starts with aligning system nodes and gold nodes .
A dependency relation cannot be counted as correct if one of the nodes could not be aligned to a gold node .
See Section 3.4 and onward for more details on alignment .
The evaluation software is a Python script that computes the three main metrics and a number of additional statistics .
It is freely available for download from the shared task website .
6
LAS : Labeled Attachment Score
The standard evaluation metric of dependency parsing is the labeled attachment score ( LAS ) , i.e. , the percentage of nodes with correctly assigned reference to the parent node , including the label ( type ) of the relation .
For scoring purposes , only nsubj , obj , iobj , csubj , ccomp , xcomp , obl , vocative , expl , dislocated , advcl , advmod , discourse , nmod , appos , nummod , acl , amod , conj , fixed , flat , compound , list , parataxis , orphan , goeswith , reparandum , root , dep Function aux , cop , mark , det , clf , universal dependency labels were taken into account , which means that language -specific subtypes such as expl:pv ( pronoun of a pronominal verb ) , a subtype of the universal relation expl ( expletive ) , were truncated to expl both in the gold standard and in the system output before comparing them .
Content
In the end-to - end evaluation of our task , LAS is re-defined as the harmonic mean ( F 1 ) of precision P and recall R , where P = # correctRelations # systemNodes ( 1 ) R = # correctRelations # goldNodes ( 2 ) LAS = 2P R P + R ( 3 ) Note that attachment of all nodes including punctuation is evaluated .
LAS is computed separately for each of the 82 test files and a macro-average of all these scores is used to rank the systems .
3.2 MLAS : Morphology - Aware Labeled Attachment Score MLAS aims at cross-linguistic comparability of the scores .
It is an extension of CLAS ( Nivre and Fang , 2017 ) , which was tested experimentally in the 2017 task .
CLAS focuses on dependencies between content words and disregards attachment of function words ; in MLAS , function words are not ignored , but they are treated as features of content words .
In addition , part- of-speech tags and morphological features are evaluated , too .
The idea behind MLAS is that function words often correspond to morphological features in other languages .
Furthermore , languages with many function words ( e.g. , English ) have longer sentences than morphologically rich languages ( e.g. , Finnish ) , hence a single error in Finnish costs the parser significantly more than an error in English according to LAS .
The core part is identical to LAS ( Section 3.1 ) : for aligned system and gold nodes , their respective parent nodes are considered ; if the system parent is not aligned with the gold parent , or if the universal relation label differs , the word is not counted as correctly attached .
Unlike LAS , certain types of relations ( Table 3 ) are not evaluated directly .
Words attached via such relations ( in either system or gold data ) are not counted as independent words .
Instead , they are treated as features of the content words they belong to .
Therefore , a system-produced word counts as correct if it is aligned and attached correctly , its universal POS tag and selected morphological features ( Table 4 ) are correct , all its function words are attached correctly , and their POS tags and features are also correct .
Punctuation nodes are neither content nor function words ; their attachment is ignored in MLAS .
BLEX : Bilexical Dependency Score BLEX is similar to MLAS in that it focuses on relations between content words .
Instead of morphological features , it incorporates lemmatization in the evaluation .
It is thus closer to semantic content and evaluates two aspects of UD annota-tion that are important for language understanding : dependencies and lexemes .
The inclusion of this metric should motivate the competing teams to model lemmas , the last important piece of annotation that is not captured by the other metrics .
A system that scores high in all three metrics will thus be a general - purpose language - analysis tool that tackles segmentation , morphology and surface syntax .
Computation of BLEX is analogous to LAS and MLAS .
Precision and recall of correct attachments is calculated , attachment of function words and punctuation is ignored ( Table 3 ) .
An attachment is correct if the parent and child nodes are aligned to the corresponding nodes in gold standard , if the universal dependency label is correct , and if the lemma of the child node is correct .
A few UD treebanks lack lemmatization ( or , as in Uyghur , have lemmas only for some words and not for others ) .
A system may still be able to predict the lemmas if it learns them in other treebanks .
Such system should not be penalized just because no gold standard is available ; therefore , if the gold lemma is a single underscore character ( " " ) , any system - produced lemma is considered correct .
Token Alignment UD defines two levels of token / word segmentation .
The lower level corresponds to what is usually understood as tokenization .
However , unlike some popular tokenization schemes , it does not include any normalization of the non-whitespace characters .
We can safely assume that any two tokenizations of a text differ only in whitespace while the remaining characters are identical .
There is thus a 1 - 1 mapping between gold and system nonwhitespace characters , and two tokens are aligned if all their characters match .
Syntactic Word Alignment
The higher segmentation level is based on the notion of syntactic word .
Some languages contain multi-word tokens ( MWT ) that are regarded as contractions of multiple syntactic words .
For example , the German token zum is a contraction of the preposition zu " to " and the article dem " the " .
Syntactic words constitute independent nodes in dependency trees .
As shown by the example , it is not required that the MWT is a pure concatenation of the participating words ; the simple token alignment thus does not work when MWTs are involved .
Fortunately , the CoNLL -U file format used in UD clearly marks all MWTs so we can detect them both in system output and in gold data .
Whenever one or more MWTs have overlapping spans of surface character offsets , the longest common subsequence algorithm is used to align syntactic words within these spans .
Sentence Segmentation
Words are aligned and dependencies are evaluated in the entire file without considering sentence segmentation .
Still , the accuracy of sentence boundaries has an indirect impact on attachment scores : any missing or extra sentence boundary necessarily makes one or more dependency relations incorrect .
Invalid Output
If a system fails to produce one of the 82 files or if the file is not valid CoNLL - U format , the score of that file ( counting towards the system 's macroaverage ) is zero .
Formal validity is defined more leniently than for UD - released treebanks .
For example , a nonexistent dependency type does not render the whole file invalid , it only costs the system one incorrect relation .
However , cycles and multi-root sentences are disallowed .
A file is also invalid if there are character mismatches that could make the token - alignment algorithm fail .
Extrinsic Parser Evaluation
The metrics described above are all intrinsic measures : they evaluate the grammatical analysis task per se , with the hope that better scores correspond to output that is more useful for downstream NLP applications .
Nevertheless , such correlations are not automatically granted .
We thus seek to complement our task with an extrinsic evaluation , where the output of parsing systems is exploited by applications like biological event extraction , opinion analysis and negation scope resolution .
This optional track involves English only .
It is organized in collaboration with the EPE initiative ; 7 for details see Fares et al . ( 2018 ) .
TIRA : The System Submission Platform Similarly to our 2017 task and to some other recent CoNLL shared tasks , we employed the cloud- based evaluation platform TIRA ( Potthast et al. , 2014 ) , 8 which implements the evaluation as a service paradigm ( Hanbury et al. , 2015 ) .
Instead of processing test data on their own hardware and submitting the outputs , participants submit working software .
Naturally , software submissions bring about additional overhead for both organizers and participants , whereas the goal of an evaluation platform like TIRA is to reduce this overhead to a bearable level .
Blind Evaluation Traditionally , evaluations in shared tasks are halfblind ( the test data are shared with participants while the ground truth is withheld ) .
TIRA enables fully blind evaluation , where the software is locked in a datalock together with the test data , its output is recorded but all communication channels to the outside are closed or tightly moderated .
The participants do not even see the input to their software .
This feature of TIRA was not too important in the present task , as UD data is not secret , and the participants were simply trusted that they would not exploit any knowledge of the test data they might have access to .
However , closing down all communication channels also has its downsides , since participants cannot check their running software ; before the system run completes , even the task moderator does not see whether the system is really producing output and not just sitting in an endless loop .
In order to alleviate this extra burden , we made two modifications compared to the previous year :
1 . Participants were explicitly advised to invoke shorter runs that process only a subset of the test files .
The organizers would then stitch the partial runs into one set of results .
2 . Participants were able to see their scores on the test set rounded to the nearest multiple of 5 % .
This way they could spot anomalies possibly caused by illselected models .
The exact scores remained hidden because we did not want the participants to fine - tune their systems against the test data .
Replicability
It is desirable that published experiments can be re-run yielding the same results , and that the algorithms can be tested on alternative test data in the future .
Ensuring both requires that a to -beevaluated software is preserved in working con-dition for as long as possible .
TIRA supplies participants with a virtual machine , offering a range of commonly used operating systems .
Once deployed and tested , the virtual machines are archived to preserve the software within .
In addition , some participants agreed to share their code so that we decided to collect the respective projects in an open source repository hosted on GitHub .
9
Baseline System
We prepared a set of baseline models using UD - Pipe 1.2 ( Straka and Strakov ? , 2017 ) .
The baseline models were released together with the UD 2 .
In addition to the treebank -specific models , we also trained a " mixed model " on samples from all treebanks .
Specifically , we utilized the first 200 training sentences of each treebank ( or less in case of small treebanks ) as training data , and at most 20 sentences from each treebank 's development set as development data .
The baseline models , together with all information needed to replicate them ( hyperparameters , the modified train - dev split where applicable , and pre-computed word embeddings for the parser ) are available from http://hdl.handle.net/11234/ 1-2859 .
Additionally , the released archive also contains the training and development data with predicted morphology .
Morphology in development data was predicted using the baseline models , morphology in training data via " jack- knifing " ( split the training set into 10 parts , train a model on 9 parts , use it to predict morphology in the tenth part , repeat for all 10 target parts ) .
The same hyperparameters were used as those used to train the baseline model on the entire training set .
The UDPipe baseline models are able to reconstruct nearly all annotation from CoNLL - U files - they can generate segmentation , tokenization , Baseline UDPipe Shared Task System
The shared task baseline system employs the UDPipe 1.2 baseline models .
For the nine treebanks without their own training data , a substitution model according to Table 5 was used .
6 Results
Official Parsing Results
Table 6 gives the main ranking of participating systems by the LAS F 1 score macro- averaged over all 82 test files .
The table also shows the performance of the baseline UDPipe system ; 17 of the 25 systems managed to outperform it .
The baseline is comparatively weaker than in the 2017 task ( only 12 out of 32 systems beat the baseline there ) .
The ranking of the baseline system by MLAS is similar ( Table 7 ) but in BLEX , the baseline jumps to rank 13 ( Table 8 ) .
Besides the simple explanation that UDPipe 1.2 is good at lemmatization , we could also hypothesize that some teams put less effort in building lemmatization models ( see also the last column in Table 10 ) .
Each ranking has a different winning system , although the other two winners are typically closely following .
The same 8- 10 systems occupy best positions in all three tables , though with variable mutual ranking .
Some teams seem to have deliberately neglected some of the evaluated attributes : Uppsala is rank 7 in LAS and MLAS , but 24 in Team LAS 1 . HIT -SCIR ( Che et al . ) 75.84 2 . TurkuNLP ( Kanerva et al. ) 73.28 3 . UDPipe Future ( Straka ) 73.11 LATTICE ( Lim et al . ) 73.02 ICS PAS ( Rybak and Wr?blewska ) 73.02 6 . CEA LIST ( Duthoo and Mesnard ) 72.56 7 . Uppsala ( Smith et al . ) 72.37 Stanford ( Qi et al. ) 72.29 9 . AntNLP ( Ji et al . ) 70.90 NLP - Cube ( Boros , et al. ) 70.82 11 .
ParisNLP
( Jawahar et While the LAS scores on individual treebanks are comparable to the 2017 task , the macro average is not , because the set of treebanks is different , and the impact of low-resource languages seems to be higher in the present task .
We used bootstrap resampling to compute 95 % confidence intervals : they are in the range ?0.11 to ?0.16 ( % LAS / MLAS / BLEX ) for all systems except SParse ( where it is ?0.00 ) .
We used paired bootstrap resampling to compute whether the difference between two neighboring systems is significant ( p < 0.05 ) .
10
Team
Secondary Metrics
In addition to the main LAS ranking , we evaluated the systems along multiple other axes , which may shed more light on their strengths and weaknesses .
This section provides an overview of selected secondary metrics for systems matching or surpassing the baseline ; a large number of additional results are available at the shared task website .
11
The website also features a LAS ranking of unofficial system runs , i.e. those that were not marked by their teams as primary runs , or were even run after the official evaluation phase closed and test data were unblinded .
The difference from the official results is much less dramatic than in 2017 , with the exception of the team SParse , who managed to fix their software and produce more valid output files .
As an experiment , we also applied the 2017 system submissions to the 2018 test data .
This allows us to test how many systems can actually be used to produce new data without a glitch , as well as to see to what extent the results change over one year and two releases of UD .
Here it should be noted that not all of the 2018 task languages and treebanks were present in the 2017 task , therefore causing many systems fail due to an unknown language or treebank code .
The full results of this experiment are available on the shared task website .
12 Table 9 evaluates detection of tokens , syntactic words and sentences .
About a third of the systems trusted the baseline segmentation ; this is less than in 2017 .
For most languages and in aggregate , the segmentation scores are very high and their impact on parsing scores is not easy to prove ; but it likely played a role in languages where segmentation is hard .
For example , HIT - SCIR 's word segmentation in Vietnamese surpasses the second system by a margin of 6 percent points ; likewise , the system 's advantage in LAS and MLAS ( but not in BLEX ! ) amounts to 7 - 8 points .
Similarly , Uppsala and ParisNLP achieved good segmenta - 12 http://universaldependencies.org/ conll18/results-2017-systems.html tion scores ( better than their respective macroaverages ) on Arabic .
They were able to translate it into better LAS , but not MLAS and BLEX , where there were too many other chances to make an error .
The complexity of the new metrics , especially MLAS , is further underlined by Table 10 : Uppsala is the clear winner in both UPOS tags and morphological features , but 6 other teams had better dependency relations and better MLAS .
Note that as with segmentation , morphology predicted by the baseline system was available , though only a few systems seem to have used it without attempting to improve it .
Partial Results
Table 11 gives the three main scores averaged over the 61 " big " treebanks ( training data larger than Regarding ranking , the Stanford system makes a remarkable jump when it does not have to carry the load of underresourced languages : from rank 8 to 2 in LAS , from 3 to 1 in MLAS and from 5 to 2 in BLEX .
Table 12 gives the LAS F 1 score on the nine low-resource languages only .
Here we have a true specialist :
The team CUNI x-ling lives up to its name and wins in all three scores , although in the overall ranking they fall even slightly behind the baseline .
On the other hand , the scores are extremely low and the outputs are hardly useful for any downstream application .
Especially morphol - ogy is almost impossible to learn from foreign languages , hence the much lower values of MLAS and BLEX .
BLEX is a bit better than MLAS , which could be explained by cases where a word form is identical to its lemma .
However , there are significant language - by- language differences ; the best LAS on Faroese and Upper Sorbian surpassing 45 % .
This probably owes to the presence of many Germanic and Slavic treebanks in training data , including some of the largest datasets in UD .
Three languages , Buryat , Kurmanji and Upper Sorbian , were introduced in the 2017 task as surprise languages and had higher scores there .
13
This is because in 2017 , the segmentation , POS tags and morphology UDPipe models were trained on the test data , applied to it via cross-validation , and made available to the systems .
Such an approach makes the conditions unrealistic , therefore it was not repeated this year .
Consequently , parsing these languages is now much harder .
In contrast , the results on the 7 treebanks with " small " training data and no development data ( Table 13 ) are higher on average , but again the variance is significant .
The smallest treebank BLEX ) .
There are also two treebanks that are the sole representatives of their languages : Irish and North S?mi .
Their best LAS is around 70 % : com-parable to Nynorsk LIA but much better than SST .
ICS PAS is the most successful system in the domain of small treebanks , especially when judged by MLAS and BLEX .
Table 14 gives the average LAS on the 5 extra test sets ( no own training data , but other treebanks of the same language exist ) .
Four of them come from the Parallel UD ( PUD ) collection introduced in the 2017 task ( Zeman et al. , 2017 ) .
The fifth , Japanese Modern , turned out to be one of the toughest test sets in this shared task .
There is another Japanese treebank , GSD , with over 160K training tokens , but the Modern dataset seems almost inapproachable with models trained on GSD .
A closer inspection reveals why : despite its name , it is actually a corpus of historical Japanese , although from the relatively recent Meiji and Taish ?
periods .
An average sentence in GSD is about 1.3 ?
longer than in Modern .
GSD has significantly more tokens tagged as auxiliaries , but more importantly , the top ten AUX lemmas in the two treebanks are completely disjoint sets .
Some other words are out - of- vocabulary because their preferred spelling changed .
For instance , the demonstrative pronoun sore is written using hiragana in GSD , but a kanji character is used in Modern .
Striking differences can be observed also in dependency relations : in GSD , 3.7 % relations are nsubj ( subject ) , and 1.2 % are cop ( copula ) .
In Modern , there is just 0.13 % of subjects , and not a single occurrence of a copula .
See Tables 15 , 16 and 17 for a ranking of all test sets by the best scores achieved on them by any parser .
Note that this cannot be directly interpreted as a ranking of languages by their parsing difficulty : many treebanks have high ranks simply because the corresponding training data is large .
Table 18 compares average LAS and MLAS for each treebank .
Finally , Tables 19 and 20 show the treebanks where word and sentence segmentation was extremely difficult ( judged by the average parser score ) .
Not surprisingly , word segmentation is difficult for the low-resource languages and for languages like Chinese , Vietnamese , Japanese and Thai , where spaces do not separate words .
Notably the Japanese GSD set is not as difficult , but whoever trusted it , crashed on the " Modern " set .
Sentence segmentation was particularly hard for treebanks without punctuation , i.e. , most of the classical languages and spoken data .
Analysis of Submitted Systems
Table 21 gives an overview of 24 of the systems evaluated in the shared task .
The overview is based on a post-evaluation questionnaire to which 24 of 25 teams responded .
Systems are ordered alphabetically by name and their LAS rank is indicated in the second column .
Looking first at word and sentence segmentation , we see that , while a clear majority of systems ( 19/24 ) rely on the baseline system for segmentation , slightly more than half ( 13 / 24 ) have developed their own segmenter , or tuned the baseline segmenter , for at least a subset of languages .
This is a development from 2017 , where only 7 out of 29 systems used anything other than the baseline segmenter .
When it comes to morphological analysis , including universal POS tags , features and lemmas , all systems this year include some such component , and only 6 systems rely entirely on the base -
In all columns , Base ( or B ) refers to the Baseline UDPipe system or the baseline word embeddings provided by the organizers , while None means that there is no corresponding component in the system .
line UDPipe system .
This is again quite different from 2017 , where more than half the systems either just relied on the baseline tagger ( 13 systems ) or did not predict any morphology at all ( 3 systems ) .
We take this to be primarily a reflection of the fact that two out of three official metrics included ( some ) morphological analysis this year , although 3 systems did not predict the lemmas required for the BLEX metric ( and 2 systems only predicted universal POS tags , no features ) .
As far as we can tell from the questionnaire responses , only 3 systems used a model where morphology and syntax were predicted jointly .
14
For syntactic parsing , most teams ( 19 ) use a single parsing model , while 5 teams , including the winning HIT - SCIR system , build ensemble models , either for all languages or a subset of them .
When it comes to the type of parsing model , we observe that graph - based models are more popular than transition - based models this year , while the opposite was true in 2017 .
We hypothesize that this is due to the superior performance of the Stanford graph - based parser in last year 's shared task , and many of the high- performing systems this year either incorporate that parser or a reimplementation of it .
15
The majority of parsers make use of pre-trained word embeddings .
Most popular are the Facebook embeddings , which are used by 17 systems , followed by the baseline embeddings provided by the organizers ( 11 ) , and embeddings trained on web crawl data ( 4 ) .
16
When it comes to additional data , over and above the treebank training sets and pretrained word embeddings , the most striking observation is that a majority of systems ( 16 ) did not use any at all .
Those that did primarily used OPUS ( 5 ) , Wikipedia dumps ( 3 ) , Apertium morphological analyzers ( 2 ) , and Universal Morphological Lattices ( 2 ) .
The CUNI x-ling system , which focused on low-resource languages , also exploited UniMorph and WALS ( in addition to OPUS and Wikipedia ) .
Finally , we note that a majority of systems make use of models trained on multiple languages to improve parsing for languages with little or no training data .
According to the questionnaire responses , 15 systems use multilingual models for the languages classified as " low- resource " , while 7 systems use them for the languages classified as " small " .
17
Only one system relied on the baseline delexicalized parser trained on data from all languages .
Conclusion
The CoNLL 2018 Shared
Task on UD parsing , the second in the series , was novel in several respects .
Besides using cross-linguistically consistent linguistic representations , emphasizing end-to - end processing of text , and in using a multiply parallel test set , as in 2017 , it was unusual also in featuring an unprecedented number of languages and treebanks and in integrating cross-lingual learning for resource - poor languages .
Compared to the first edition of the task in 2017 , this year several languages were provided with little - to - no resources , whereas in 2017 , predicted morphology trained on the language in question was available for all of the languages .
The most extreme example of these is Thai , where the only accessible resource was the Facebook Research Thai embeddings model and the OPUS parallel corpora .
This year 's task also introduced two additional metrics that take into account morphology and lemmatization .
This encouraged the development of truly end-to - end full parsers , producing complete parses including morphological features and lemmas in addition to the syntactic tree .
This also aimed to improve the utility of the systems developed in the shared task for later downstream applications .
For most UD languages , these parsers represent a new state of the art for end-to - end dependency parsing .
The analysis of the shared task results has so far only scratched the surface , and we refer to the system description papers for more in - depth analysis of individual systems and their performance .
For many previous CoNLL shared tasks , the task itself has only been the starting point of a long and fruitful research strand , enabled by the resources created for the task .
We hope and believe that the 2017 and 2018 UD parsing tasks will join this tradition .
2 training data .
For each of the 73 treebanks with non-empty training data we trained one UDPipe model , utilizing training data for training and development data for hyperparameter tuning .
If a treebank had no development data , we cut 10 % of the training sentences and considered it as development data for the purpose of tuning hyperparameters of the baseline model ( employing only the remainder of the original training data for the actual training in that case ) .
