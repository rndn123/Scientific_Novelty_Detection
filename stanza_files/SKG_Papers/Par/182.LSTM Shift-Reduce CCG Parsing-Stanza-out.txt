title
LSTM Shift-Reduce CCG
Parsing
abstract
We describe a neural shift-reduce parsing model for CCG , factored into four unidirectional LSTMs and one bidirectional LSTM .
This factorization allows the linearization of the complete parsing history , and results in a highly accurate greedy parser that outperforms all previous beam-search shift-reduce parsers for CCG .
By further deriving a globally optimized model using a task - based loss , we improve over the state of the art by up to 2.67 % labeled F1 .
Introduction Combinatory Categorial Grammar ( CCG ; Steedman , 2000 ) parsing is challenging due to its so-called " spurious " ambiguity that permits a large number of non-standard derivations ( Vijay - Shanker and Weir , 1993 ; Kuhlmann and Satta , 2014 ) .
To address this , the de facto models resort to chart- based CKY ( Hockenmaier , 2003 ; Clark and Curran , 2007 ) , despite CCG being naturally compatible with shiftreduce parsing ( Ades and Steedman , 1982 ) .
More recently , Zhang and Clark ( 2011 ) introduced the first shift- reduce model for CCG , which also showed substantial improvements over the long-established state of the art ( Clark and Curran , 2007 ) .
The success of the shift-reduce model ( Zhang and Clark , 2011 ) can be tied to two main contributing factors .
First , without any feature locality restrictions , it is able to use a much richer feature set ; while intensive feature engineering is inevitable , it has nevertheless delivered an effective and conceptually simpler alternative for both parameter estimation and inference .
Second , it couples beam search with global optimization ( Collins , 2002 ; Collins and Roark , 2004 ; Zhang and Clark , 2008 ) , which makes it less prone to search errors than fully greedy models ( Huang et al. , 2012 ) .
In this paper , we present a neural architecture for shift-reduce CCG parsing based on long short-term memories ( LSTMs ; Hochreiter and Schmidhuber , 1997 ) .
Our model is inspired by Dyer et al . ( 2015 ) , in which we explicitly linearize the complete history of parser states in an incremental fashion by requiring no feature engineering ( Zhang and Clark , 2011 ; Xu et al. , 2014 ) , and no atomic feature sets ( Chen and Manning , 2014 ) .
However , a key difference is that we achieve this linearization without relying on any additional control operations or compositional tree structures ( Socher et al. , 2010 ; Socher et al. , 2011 ; Socher et al. , 2013 ) , both of which are vital in the architecture of Dyer et al . ( 2015 ) .
Crucially , unlike the sequence- to-sequence transduction model of Vinyals et al . ( 2015 ) , which primarily conditions on the input words , our model is sensitive to all aspects of the parsing history , including arbitrary positions in the input .
As another contribution , we present a global LSTM parsing model by adapting an expected Fmeasure loss ( Xu et al. , 2016 ) .
As well as naturally incorporating beam search during training , this loss optimizes the model towards the final evaluation metric ( Goodman , 1996 ; Smith and Eisner , 2006 ; Auli and Lopez , 2011 b ) , allowing it to learn shiftreduce action sequences that lead to parses with high expected F-scores .
We further show the globally optimized model can be leveraged with greedy inference , resulting in a deterministic parser as accurate as its beam-search counterpart .
On standard CCGBank tests , we clearly outperform all previous shift-reduce CCG parsers ; and by combining the parser with an attention - based LSTM supertagger ( ?4 ) , we obtain further significant improvements ( ?5 ) .
Shift-Reduce CCG
Parsing CCG is strongly lexicalized by definition .
A CCG grammar extracted from CCGBank ( Hockenmaier and Steedman , 2007 ) contains over 400 lexical types and over 1,500 non-terminals ( Clark and Curran , 2007 ) , which is an order of magnitude more than those of a typical CFG parser .
This lexicalized nature raises another unique challenge for parsingany parsing model for CCG needs to perform lexical disambiguation .
This is true even in the approach of Fowler and Penn ( 2010 ) , in which a contextfree cover grammar extracted from CCGBank is used to parse CCG .
Indeed , as noted by Auli and Lopez ( 2011a ) , the search problem for CCG parsing is equivalent to finding an optimal derivation in the weighted intersection of a regular language ( generated by the supertagger ) and a mildly contextsensitive language ( generated by the parser ) , which can quickly become expensive .
The shift-reduce paradigm ( Aho and Ullman , 1972 ; Yamada and Matsumoto , 2003 ; Nivre and Scholz , 2004 ) applied to CCG ( Zhang and Clark , 2011 ) presents a more elegant solution to this problem by allowing the parser to conduct lexical assignment " incrementally " as a complete parse is being built by the decoder .
This is not possible with a chart- based parser , in which complete derivations must be built first .
Therefore , a shift-reduce parser is able to consider a much larger set of categories per word for a given input , achieving higher lexi-cal assignment accuracy than the C&C parser ( Clark and Curran , 2007 ) , even with the same supertagging model ( Zhang and Clark , 2011 ; Xu et al. , 2014 ) .
In our parser , we follow this strategy and adopt the Zhang and Clark ( 2011 ) style shift-reduce transition system , which assumes a set of lexical categories has been assigned to each word using a supertagger ( Bangalore and Joshi , 1999 ; Clark and Curran , 2004 ) .
Parsing then proceeds by applying a sequence of actions to transform the input maintained on a queue , into partially constructed derivations , kept on a stack , until the queue and available actions on the stack are both exhausted .
At each time step , the parser can choose to shift ( sh ) one of the lexical categories of the front word onto the stack , and remove that word from the queue ; reduce ( re ) the top two subtrees on the stack using a CCG rule , replacing them with the resulting category ; or take a unary ( un ) action to apply a CCG type-raising or type-changing rule to the stack - top element .
For example , the deterministic sequence of shift-reduce actions that builds the derivation in Fig. 1 is : sh ? NP , un ?
S /( S \NP ) , sh ?
( S \NP ) / NP , re ?
S / NP , sh ? NP and re ?
S , where we use ? to indicate the CCG category produced by an action .
1 3 LSTM Shift-Reduce Parsing 3.1 LSTM
Recurrent neural networks ( RNNs ; e.g. , see Elman , 1990 ) are factored into an input layer x t and a hidden state ( layer ) h t with recurrent connections , and they can be represented by the following recurrence : h t = ? ? ( x t , h t?1 ) , ( 1 ) where x t is the current input , h t?1 is the previous hidden state and ? is a set of affine transformations parametrized by ?.
Here , we use a variant of RNN referred to as LSTMs , which augment Eq. 1 with a cell state , c t , s.t. h t , c t = ? ? ( x t , h t?1 , c t?1 ) .
( 2 ) Compared with conventional RNNs , this extra facility gives LSTMs more persistent memories over longer time delays and makes them less susceptible to the vanishing gradient problem ( Bengio et al. , 1994 ) .
Hence , they are better at modeling temporal events that are arbitrarily far in a sequence .
Several extensions to the vanilla LSTM have been proposed over time , each with a modified instantiation of ? ? that exerts refined control over e.g. , whether the cell state could be reset ( Gers et al. , 2000 ) or whether extra connections are added to the cell state ( Gers and Schmidhuber , 2000 ) .
Our instantiation is as follows for all LSTMs : i t = ?( W ix x t + W ih h t?1 + W ic c t?1 + b i ) f t = ?( W f x x t + W f h h t?1 + W f c c t?1 + b f ) c t = f t c t?1 + i t tanh ( W cx x t + W ch h t?1 + b c ) o t = ?( W ox x t + W oh h t?1 + W oc c t + b o ) h t = o t tanh ( c t ) , where ? is the sigmoid activation and is the element-wise product .
In addition to unidirectional LSTMs that model an input sequence x 0 , x 1 , . . . , x n?1 in a strict leftto-right order , we also use bidirectional LSTMs ( BLSTMs ; Graves and Schmidhuber , 2005 ) , which read the input from both directions with two independent LSTMs .
At each step , the forward hidden state h t is computed using Eq. 2 for t = ( 0 , 1 , . . . , n ? 1 ) ; and the backward hidden state ?t is computed similarly but from the reverse direction for t = ( n ? 1 , n ? 2 , . . . , 0 ) .
Together , the two hidden states at each step t capture both past and future contexts , and the representation for each x t is obtained as the concatenation [ h t ; ?t ] .
Embeddings
The neural network model employed by Chen and Manning ( 2014 ) , and followed by a number of other parsers ( Weiss et al. , 2015 ; Zhou et al. , 2015 ; Ambati et al. , 2016 ; Andor et al. , 2016 ; Xu et al. , 2016 ) allows higher - order feature conjunctions to be automatically discovered from a set of dense feature embeddings .
However , a set of atomic feature templates , which are only sensitive to contexts from the top few elements on the stack and queue are still needed to dictate the choice of these embeddings .
Instead , we dispense with such templates and seek input : w 0 . . . w n?1 axiom : 0 : ( 0 , , ? , ? ) goal : 2n ? 1 + ? : ( n , ? , , ? ) t : ( j , ? , x w j |? , ?) t + 1 : ( j + 1 , ?|x w j , ? , ? ) ( sh ; 0 ? j < n) t : ( j , ?|s 1 |s 0 , ? , ? ) t + 1 : ( j , ?|x , ? , ? ? x ) ) ( re ; s 1 s 0 ? x ) t : ( j , ?|s 0 , ? , ? ) t + 1 : ( j , ?|x , ? , ? ) ( un ; s 0 ? x ) Figure 2 : The shift-reduce deduction system .
For the sh deduction , xw j denotes an available lexical category for wj ; for re , x denotes the set of dependencies on x. to design a model that is sensitive to both local and non-local contexts , on both the stack and queue .
Consequently , embeddings represent atomic input units that are added to our parser and are preserved throughout parsing .
In total we use four types of embeddings , namely , word , CCG category , POS and action , where each has an associated look - up table that maps a string of that type to its embedding .
The look - up table for words is L w ?
R k?|w | , where k is the embedding dimension and | w | is the size of the vocabulary .
Similarly , we have look - up tables for CCG categories , L c ?
R l?|c| , for the three types of actions , L a ? R m?3 , and for POS tags , L p ? R n?|p| .
Model Parser .
Fig. 2 shows the deduction system of our parser .
2
We denote each parse item as ( j , ? , ? , ? ) , where j is the positional index of the word at the front of the queue , ? is the stack ( with its top element s 0 to the right ) , and ? is the queue ( with its top element w j to the left ) and ? is the set of CCG dependencies realized for the input consumed so far .
Each item is also associated with a step indicator t , signifying the number of actions applied to it and the goal is reached in 2n ? 1 + ? steps , where ? is the total number of un actions .
We also define each action in our parser as a 4 - tuple ( ?
t , c t , w ct , p wc t ) , where ?
t ?
{ sh , re , un} for t ?
1 , c t is the resulting category of ?
t , and w ct is the head word attached to ?t = [ h U t ; h V t ; h X t ; h Y t ] ( Eq. 3 ) , and the shaded cells on the right represent wj = [ h W j ; ?W j ] .
c t with p wc t being its POS tag .
3 LSTM model .
LSTMs are designed to handle time -series data , in a purely sequential fashion , and we try to exploit this fact by completely linearizing all aspects of the parsing history .
Concretely , we factor the model as five LSTMs , comprising four unidirectional ones , denoted as U , V , X and Y , and an additional BLSTM , denoted as W ( Fig. 3 ) .
Before parsing each sentence , we feed W with the complete input ( padded with a special embedding ? as the end of sentence token ) ; and we use w j = [ h W j ; ?W j ] to represent w j in subsequent steps .
4
We also add ? to the other 4 unidirectional LSTMs as initialization .
Given this factorization , the stack representation for a parse item ( j , ? , ? , ? ) at step t , for t ?
1 , is obtained as ? t = [ h U t ; h V t ; h X t ; h Y t ] , ( 3 ) and together with w j , [?
t ; w j ] gives a representation for the parse item .
For the axiom item , we represent it as [ ?
0 ; w 0 ] , where ?
0 = [ h U ? ; h V ? ; h X ? ; h Y ? ] .
Each time the parser applies an action ( ?
t , c t , w ct , p wc t ) , we update the model by adding the embedding of ?
t , denoted as L a ( ? t ) , onto U , and adding the other three embeddings of the action 4 - tuple , that is , L c ( c t ) , L w ( w ct ) and L p ( p wc t ) , onto V , X and Y respectively .
To predict the next action , we first derive an action hidden layer b t , by passing the parse item representation [ ?
t ; w j ] through an affine transformation , s.t. b t = f ( B [ ?
t ; w j ] + r ) , ( 4 ) where B is a parameter matrix of the model , r is a bias vector and f is a ReLU non-linearity ( Nair and Hinton , 2010 ) .
Then we apply another affine transformation ( with A as the weights and s as the bias ) to b t : a t = f ( Ab t + s ) , and obtain the probability of the i th action in a t as p (?
i t | b t ) = exp{a i t } ? k t ?T ( ? t , ? t ) exp{a k t } , where T ( ? t , ? t ) is the set of feasible actions for the current parse item , and ?
i t ? T (? t , ? t ) .
Derivations and Dependency Structures
Our model naturally linearizes CCG derivations " incrementally " following their post-order traversals .
As such , the four unidirectional LSTMs always have the same number of steps ; and at each step , the concatenation of their hidden states ( Eq. 3 ) represents a point in a CCG derivation ( i.e. , an action 4 - tuple ) .
Due to the large amount of flexibility in how dependencies are realized in CCG ( Hockenmaier , 2003 ; Clark and Curran , 2007 ) , and in line with most existing CCG parsing models , including dependency models , we have chosen to model CCG derivations , rather than dependency structures .
5
We also hypothesize that tree structures are not necessary for the current model , since they are already implicit in the linearized derivations ; similarly , we have found the action embeddings to be nonessential ( ?5.2 ) .
Training
As a baseline , we first train a greedy model , in which we maximize the log-likelihood of each target action in the training data .
More specifically , let ( ?
g 1 , . . . , ? g
Tn ) be the gold-standard action sequence for a training sentence n , a cross-entropy criterion is used to obtain the error gradients , and for each sentence , training involves minimizing L ( ? ) = ? log Tn t=1 p( ? g t | b t ) = ?
Tn t=1 log p (?
g t | b t ) , where ? is the set of all parameters in the model .
As other greedy models ( e.g. , see Chen and Manning ( 2014 ) and Dyer et al . ( 2015 ) ) , our greedy model is locally optimized , and suffers from the label bias problem ( Andor et al. , 2016 ) .
A partial solution to this is to use beam search at test time , thereby recovering higher scoring action sequences that would otherwise be unreachable with fully greedy inference .
In practice , this has limited effect ( Table 2 ) , and a number of more principled solutions have been recently proposed to derive globally optimized models during training ( Watanabe and Sumita , 2015 ; Weiss et al. , 2015 ; Zhou et al. , 2015 ; Andor et al. , 2016 ) .
Here , we extend our greedy model into a global one by adapting the expected F-measure loss of Xu et al . ( 2016 ) .
To our best knowledge , this is the first attempt to train a globally optimized LSTM shift-reduce parser .
Let ? = { U , V , X , Y , W , B , A} be the weights of the baseline greedy model , 6 we initialize the weights of the global model , which has the same architecture as the baseline , to ? , and we reoptimize ? in multiple training epochs as follows : 1 . Pick a sentence x n from the training set , decode it with beam search , and generate a k-best list of output parses with the current ? , denoted as ?( x n ) .
7 2 . For each parse y i in ?( x n ) , compute its sentence - level F1 using the set of dependencies in the ? field of its parse item .
In addition , let |y i | be the total number of actions that derived y i and s ? ( y j i ) be the softmax action score of the j th action , given by the LSTM model .
Compute the log-linear score of its action sequence as ?( y i ) = |y i | j=1 log s ? ( y j i ) .
3 . Compute the negative expected F1 objective ( defined below ) for x n and minimize it using stochastic gradient descent ( maximizing expected F1 ) .
Repeat these three steps for the remaining sentences .
More formally , the loss J ( ? ) , is defined as J ( ? ) = ?XF1 ( ? ) = ? y i ?( xn ) p(y i |? ) F1 ( ?
y i , ? G xn ) , where F1 ( ?
y i , ?
G xn ) is the sentence level F1 of the parse derived by y i , with respect to the gold -standard dependency structure ?
G xn of x n ; p(y i |? ) is the normalized probability score of the action sequence y i , computed as p(y i | ? ) = exp {?( y i ) } y?( xn ) exp {?( y ) } , where ? is a parameter that sharpens or flattens the distribution ( Tromble et al. , 2008 ) .
8 Different from the maximum-likelihood objective , XF1 optimizes the model on a sequence level and towards the final evaluation metric , by taking into account all action sequences in ?( x n ) .
Attention - Based LSTM Supertagging
In addition to the size of the label space , supertagging is difficult because CCG categories can encode long-range dependencies and tagging decisions frequently depend on non-local contexts .
For example , in He went to the zoo with a cat , a possible category for with , ( S \NP ) \( S \ NP ) / NP , depends on the word went further back in the sentence .
Recently a number of RNN models have been proposed for CCG supertagging ( Xu et al. , 2015 ; Lewis et al. , 2016 ; Vaswani et al. , 2016 ; Xu et al. , 2016 ) , and such models show dramatic improvements over non-recurrent models ( Lewis and Steedman , 2014 b ) .
Although the underlying models differ in their exact architectures , all of them make each tagging decision using only the hidden states at the current input position , and this imposes a potential bottleneck in the model .
To mitigate this , we generalize the attention mechanisms of Bahdanau et al . ( 2015 ) and Luong et al . ( 2015 ) , and adapt them to supertagging , by allowing the model to explicitly use hidden states from more than one input positions for tagging each word .
Similar to Bahdanau et al. ( 2015 ) and Luong et al . ( 2015 ) , a key feature in our model is a soft alignment vector that weights the relative importance of the considered hidden states .
For an input sentence w 0 , w 1 , . . . , w n?1 , we consider w t = [ h t ; ?t ] ( ?3.1 ) to be the representation of the t th word ( 0 ? t < n , w t ? R 2d?1 ) , given by a BLSTM with a hidden state size d for both its forward and backward layers .
9 Let k be a context window size hyperparameter , we define H t ? R 2d ?( k?1 ) as H t = [ w t? k/2 , . . . , w t?1 , w t+1 , . . . , w t+ k/2 ] , which contains representations for all words in the size k window except w t .
At each position t , the attention model derives a context vector c t ?
R 2d?1 ( defined below ) from H t , which is used in conjunction with w t to produce an attentional hidden layer : x t = f ( M [ c t ; w t ] + m ) , ( 5 ) where f is a ReLU non-linearity , M ? R g?4d is a learned weight matrix , m is a bias term , and g is the size of x t .
Then x t is used to produce another hidden layer ( with N as the weights and n as the bias ) : z t = Nx t + n , and the predictive distribution over categories is obtained by feeding z t through a softmax activation .
In order to derive the context vector c t , we first compute b t ? R ( k?1 ) ?1 from H t and w t using ? ? R 1?4d , s.t. the i th entry in b t is b i t = ?[ w T [ i ] ; w t ] , for i ? [ 0 , k?1 ) , T = [ t ?
k/2 , . . . , t?1 , t +1 , . . . , t+ k/2 ] ; and c t is derived as follows : a t = softmax ( b t ) , c t = H t a t , where a t is the alignment vector .
We also experiment with two types of attention reminiscent of the global and local models in Luong et al . ( 2015 ) , where the first attends over all input words ( k = n ) and the second over a local window .
It is worth noting that two other works have concurrently tackled supertagging with BLSTM models .
In Vaswani et al. ( 2016 ) , a language model layer is added on top of a BLSTM , which allows embeddings of previously predicted tags to propagate through and influence the pending tagging decision .
However , the language model layer is only effective when both scheduled sampling for training ( Bengio et al. , 2015 ) and beam search for inference are used .
We show our attention - based models can match their performance , with only standard training and greedy decoding .
Additionally , Lewis et al . ( 2016 ) presented a BLSTM model with two layers of stacking in each direction ; and as an internal baseline , we show a non-stacking BLSTM without attention can achieve the same accuracy .
Experiments Dataset and baselines .
We conducted all experiments on CCGBank ( Hockenmaier and Steedman , 2007 ) with the standard splits .
10
We assigned POS tags with the C&C POS tagger , and used 10 - fold jackknifing for both POS tagging and supertagging .
All parsers were evaluated using F1 over labeled CCG dependencies .
For supertagging , the baseline models are the RNN model of Xu et al . ( 2015 ) , the bidirectional RNN ( BRNN ) model of Xu et al . ( 2016 ) , and the BLSTM supertagging models in Vaswani et al . ( 2016 ) and Lewis et al . ( 2016 ) .
For parsing experiments , we compared with the global beam-search shift-reduce parsers of Zhang and Clark ( 2011 ) and Xu et al . ( 2014 ) .
One neural shift- reduce CCG parser baseline is Ambati et al . ( 2016 ) , which is a beam-search shift-reduce parser based on Chen and Manning ( 2014 ) and Weiss et al . ( 2015 ) ; and the others are the RNN shift- reduce models in Xu et al . ( 2016 ) .
Additionally , the chart- based C&C parser was included by default .
Model and training parameters .
11 All our LSTM models are non-stacking with a single layer .
12 For the supertagging models , the LSTM 10 Training : Sections 02-21 ( 39,604 sentences ) .
Development : Section 00 ( 1,913 sentences ) .
Test : Section 23 ( 2,407 sentences ) .
11 We implemented all models using the CNN toolkit : https://github.com/clab/cnn.
12
The BLSTMs have a single layer in each direction .
We experimented with 2 layers in all models during development and found negligible improvements .
hidden state size is 256 , and the size of the attentional hidden layer ( x t , Eq. 5 ) is 200 .
All parsing model LSTMs have a hidden state size of 128 , and the size of the action hidden layer ( b t , Eq. 4 ) is 80 .
Pretrained word embeddings for all models are 100 - dimensional ( Turian et al. , 2010 ) , and all other embeddings are 50 - dimensional .
We also pretrained CCG lexical category and POS embeddings on the concatenation of the training data and a Wikipedia dump parsed with C&C. 13
All other parameters were uniformly initialized in ? 6 / ( r + c ) , where r and c are the number of rows and columns of a matrix ( Glorot and Bengio , 2010 ) .
For training , we used plain non-minibatched stochastic gradient descent with an initial learning rate ?
0 = 0.1 and we kept iterating in epochs until accuracy no longer increases on the dev set .
For all models , a learning rate schedule ? e = ? 0 /( 1 + ?e ) with ? = 0.08 was used for e ? 11 .
Gradients were clipped whenever their norm exceeds 5 .
Dropout training as suggested by Zaremba et al . ( 2014 ) , with a dropout rate of 0.3 , and an 2 penalty of 1 ? 10 ?5 , were applied to all models .
Supertagging Results
Table 1 summarizes 1 - best supertagging results .
Our baseline BLSTM model without attention achieves the same level of accuracy as Lewis et al . ( 2016 ) and the baseline BLSTM model of Vaswani et al . ( 2016 ) .
Compared with the latter , our hidden state size is 50 % smaller ( 256 vs. 512 ) .
For training and testing the local attention model ( BLSTM - local ) , we used an attention window size 13 We used the gensim word2vec toolkit : https:// radimrehurek.com/gensim /. of 5 ( tuned on the dev set ) , and it gives an improvement of 0.94 % over the BRNN supertagger ( Xu et al. , 2016 ) , achieving an accuracy on par with the beam-search ( size 12 ) model of Vaswani et al . ( 2016 ) that is enhanced with a language model .
Despite being able to consider wider contexts than the local model , the global attention model ( BLSTMglobal ) did not show further gains , hence we used BLSTM - local for all parsing experiments below .
Parsing Results
All parsers we consider use a supertagger probability cutoff ? to prune categories less likely than ? times the probability of the best category in a distribution : for the C&C parser , it uses an adaptive strategy to backoff to smaller ?
values if no spanning analysis is found given an initial ? setting ; for all the shift-reduce parsers , fixed ? values are used without backing off .
Since ? determines the derivation space of a parser , it has a large impact on the final parsing accuracy .
For the maximum-likelihood greedy model , we found using a small ? value ( bigger ambiguity ) for training significantly improved accuracy , and we chose ? = 1 ? 10 ?5 ( 5.22 categories per word with jackknifing ) via development experiments .
This reinforces the findings in a number of other CCG parsers ( Clark and Curran , 2007 ; Auli and Lopez , 2011a ; Lewis and Steedman , 2014a ) : even though a smaller ?
increases ambiguity , it leads to more accurate models at test time .
On the other hand , we found using larger ?
values at test time led to significantly better results ( Table 2 ) .
And this differs from the beam-search models that use the same ?
value for both training and testing ( Zhang and Clark , 2011 ; Xu et al. , 2014 ) .
The greedy model .
Table 3 shows the dev set results for all greedy models , where the four types of embeddings , that is , word ( w ) , CCG category ( c ) , action ( a ) and POS ( p ) , are gradually introduced .
The full model LSTM -w+c+a+ p surpasses all previous shift-reduce models ( Table 4 ) , achieving a dev set accuracy of 86.56 % .
Category embeddings ( LSTM -w+c ) yielded a large gain over using word embeddings alone ( LSTM - w ) ; action embeddings ( LSTM -w+c+ a ) provided little improvement , but further adding POS embeddings ( LSTM -w+c+ a+ p ) gave noticeable recall ( + 0.61 % ) and F1 improvements ( + 0.36 % ) over LSTM -w+c.
Fig. 4a shows the learning curves , where all models converged in under 30 epochs .
The XF1 model .
Effect of the supertagger .
To isolate the parsing model from the supertagging model , we first experimented with the BRNN supertagging model as in Xu et al . ( 2016 ) for both training and testing the full greedy LSTM parser .
Using this supertagger , we still achieved the highest F1 ( 85.86 % ) on the dev set ( LSTM - BRNN , Table 5 ) in comparison with all previous shift- reduce models ; and an improvement of 1.42 % F1 over the greedy model of Xu et al . ( 2016 ) was obtained on the test set ( Table 4 ) .
We then experimented with using the baseline BLSTM supertagging model for parsing ( LSTM - BLSTM ) , and observed the attention - based setup ( LSTM - greedy ) outperformed it , despite the attention - based supertagger ( BLSTM - local ) did not give better multi-tagging accuracy .
We owe this to the fact that large ?
cutoff values - resulting in almost deterministic supertagging decisions on average - are required by the parser during inference ; for instance , BLSTM - local has an average ambiguity of 1.09 on the dev set with ? = 0.06 .
14 Comparison with chart- based models .
For completeness and to put our results in perspective , we compare our XF1 models with other CCG parsers in the literature ( Table 6 ) : Xu et al. ( 2015 ) is the log-linear C&C dependency hybrid model with an RNN supertagger front-end ; Lewis et al . ( 2016 ) is an LSTM supertagger - factored parser using the A * CCG parsing algorithm of Lewis and Steedman ( 2014 a ) ; Vaswani et al. ( 2016 ) combine a BLSTM supertagger with a new version of the C&C parser ( Clark et al. , 2015 ) that a max-violation perceptron , which significantly improves over the 14 All ? cutoffs were tuned on the dev set ; for BRNN , we found the same ? settings as in Xu et al . ( 2016 ) to be optimal ; for BLSTM , ? = 4 ? 10 ?5 for training ( with an ambiguity of 5.27 ) and ? = 0.02 for testing ( with an ambiguity of 1.17 ) .
original C&C models ; and finally , a global recursive neural network model with A * decoding ( Lee et al. , 2016 ) .
We note that all these alternative modelswith the exception of Xu et al . ( 2015 ) and Lewis et al . ( 2016 ) - use structured training that accounts for violations of the gold -standard , and we conjecture further improvements for our model are possible by incorporating such mechanisms .
15
Conclusion
We have presented an LSTM parsing model for CCG , with a factorization allowing the linearization of the complete parsing history .
We have shown that this simple model is highly effective , with results outperforming all previous shift-reduce CCG parsers .
We have also shown global optimization benefits an LSTM shift- reduce model ; and contrary to previous findings with the averaged perceptron ( Zhang and Clark , 2008 ) , we empirically demonstrated beamsearch inference is not necessary for our globally optimized model .
For future work , a natural direction is to explore integrated supertagging and parsing in a single neural model ( Zhang and Weiss , 2016 ) .
Figure 1 : 1 Figure1 : A CCG derivation , in which each point corresponds to the result of a shift-reduce action .
In this example , composition ( B ) and application ( > ) are re actions , and type-raising ( T ) is a un action .
