title
Studying Summarization Evaluation Metrics in the Appropriate Scoring Range
abstract
In summarization , automatic evaluation metrics are usually compared based on their ability to correlate with human judgments .
Unfortunately , the few existing human judgment datasets have been created as by -products of the manual evaluations performed during the DUC / TAC shared tasks .
However , modern systems are typically better than the best systems submitted at the time of these shared tasks .
We show that , surprisingly , evaluation metrics which behave similarly on these datasets ( average - scoring range ) strongly disagree in the higher - scoring range in which current systems now operate .
It is problematic because metrics disagree yet we ca n't decide which one to trust .
This is a call for collecting human judgments for high-scoring summaries as this would resolve the debate over which metrics to trust .
This would also be greatly beneficial to further improve summarization systems and metrics alike .
Introduction
The progress in summarization is tightly intertwined with the capability to quickly measure improvements .
Thus , a significant body of research was dedicated to the development of automatic metrics ( Lloret et al. , 2018 ) .
Yet , this remains an open problem ( Rankel et al. , 2013 ) .
Typically , evaluation metrics are compared based on their ability to correlate with humans ( Lin and Hovy , 2003 ) .
Then , the selected metrics heavily influence summarization research by guiding progress ( Lloret et al. , 2018 ) and by providing supervision for training summarization systems ( Yogan et al. , 2016 ) .
Despite their central role , few human judgment datasets have been created .
The existing ones are the result of the manual evaluations performed during shared tasks ( Dang and Owczarzak , 2008 Owczarzak , , 2009 .
Thus , the annotated summaries are mostly average compared to nowadays standards .
Indeed , the best systems submitted at the time of these sharedtasks have typically served as baselines for subsequent works .
This is illustrated by figure 1 , which compares the score distribution of summaries in human judgment datasets with the score distribution of modern summarization systems .
1
The score distribution on which evaluation metrics are tested ( blue zone ) differs from the one in which they now operate ( red zone ) .
Thus , there is no guarantee that evaluation metrics behave according to human judgments in the high-scoring range .
Yet , summarization systems explicitly target highscoring summaries ( Radev et al. , 2003 ) .
In this work , we study several evaluation metrics in this high-scoring range based on an automatically generated dataset .
We show that , even though current evaluation metrics correlate well with each other in the average range , they strongly disagree for high-scoring summaries .
This is related to the Simpson paradox , where different conclusions are drawn depending on which slice of the population is considered ( Wagner , 1982 ) .
This is problematic because current metrics cannot be distinguished based solely on an analysis of available human judgments .
Nevertheless , they will promote very different summaries and systems .
These results call for the gathering of human judgments in the high-scoring range .
We provide data and code to reproduce our experiments .
2
Contributions : ( i) We present a simple methodology to study the behavior of metrics in the high-scoring range .
( ii ) We observe low and even some negative correlations in this range .
( iii )
This work serves as a motivation to gather human annotations in the relevant scoring range .
Background Usually , evaluation metrics are compared based on their ability to correlate with human judgments ( Lin and Hovy , 2003 ) .
Several works followed this principle and provided different recommendations about which metric to use .
For instance , Owczarzak et al . ( 2012 ) used a signed Wilcoxon test to find significant differences between metrics and recommended to use ROUGE - 2 recall with stemming and stopwords not removed .
In a wider study , Graham ( 2015 ) found ROUGE - 2 precision with stemming and stopwords removed to be the best .
Rankel et al. ( 2013 ) used accuracy and found ROUGE - 4 to perform well .
They also observe that the correlation between ROUGE and human judgments decreases when looking at the best systems only .
This is in agreement with our work , except that we look at summaries better than the current state - of- the - art .
Radev et al. ( 2003 ) also observed that the high-scoring range is the most relevant for comparing evaluation metrics because summarizers aim to extract high -scoring summaries .
However , they performed analysis on the best scoring summaries from 6 systems which remain average compared to nowadays standard .
Our analysis differs from such meta-evaluation ( evaluation of evaluation metrics ) because we do not provide another metric recommendation .
In - 2 https://github.com/PeyrardM/ acl-2019-Compare_Evaluation_Metrics stead , we start from the observation that human judgments are limited in their coverage and analyze the behavior of existing candidate metrics in the high-scoring range not available in these datasets .
These previous works computed correlations between metrics and humans , we compute correlations between pairs of metrics in scoring ranges for which there are no human judgments available .
Data Generation
In this work , we study the following metrics : ROUGE - 2 ( R - 2 ) : measures the bigram overlap between the candidate summary and the pool of reference summaries ( Lin , 2004 ) . ROUGE -L ( R-L ) : a variant of ROUGE which measures the size of the longest common subsequence between candidate and reference summaries .
ROUGE -WE ( R- WE ) : instead of hard lexical matching of bigrams , R - WE uses soft matching based on the cosine similarity of word embeddings ( Ng and Abrecht , 2015 ) . JS divergence ( JS - 2 ) : uses Jensen - Shannon divergence between bigram distributions of references and candidate summaries ( Lin et al. , 2006 ) . S3 : a metric trained explicitly to maximize its correlation with manual Pyramid annotations .
We chose these metrics because they correlate well with available human judgments ( about .4 Kendall 's ? ; the exact numbers are provided in appendix A ) and are easily available .
For a recent overview of evaluation metrics , we recommend Lloret et al . ( 2018 ) .
Once an evaluation metric becomes standard , it is optimized , either directly by supervised methods or indirectly via repeated comparisons of unsupervised systems .
To mimic this procedure , we optimized each metric using a recently introduced genetic algorithm for summarization ( Peyrard and Eckle-Kohler , 2016 ) .
3
The metric m is used as the fitness function .
The resulting population is a set of summaries ranging from random to upperbound according to m .
For both TAC - 2008 and TAC - 2009 , we used a population of 400 summaries per topic ( per metric ) .
The final dataset contains 160 , 523 summaries for an average of R-WE R-L JS -2 S3 R - 2 ( W ) ( A ) ( T ) . 1 , 763 summaries per topic ( less than 5 * 400 due to removed duplicates ) .
We refer to this dataset as ( W ) as it covers the whole scoring range .
In order to focus on the top-scoring summaries , we preserve the summaries scoring higher than the LexRank baseline ( Erkan and Radev , 2004 ) for at least one metric .
LexRank is a graph- based extractive summarizer often used as a baseline .
Thus , most current and future summarization systems should perform better and should be covered by the selected scoring range .
Besides , LexRank is strong enough to discard a large number of average scoring summaries .
The resulting dataset contains an average of 102 summaries kept per topic .
This dataset of top-scoring summaries is noted ( T ) .
The ROUGE - 2 score distribution of ( T ) is depicted by the green area in figure 1 .
We provide the pseudo-code and other details concerning the data generation procedure in appendix B. Additionally , we refer to the summaries available as part of the human judgment datasets as ( A ) because they cover the average -scoring range .
Correlation Analysis
We compute the pairwise correlations between evaluation metrics averaged over all topics for different scoring ranges and report the results in table 1 . For ( A ) and ( W ) , we observe high correlations between pairs of metrics ( > .6 Kendall's ? ) . JS - 2 and R - 2 have the strongest correlation , while R-L is less correlated with the others .
It is worth remembering that JS - 2 and R - 2 both operate on bigrams which also explain their stronger connection .
However , in the high-scoring range ( T ) , correlations are low and often negative .
Even , R - 2 and JS - 2 only retain little correlation ( < 0.3 ? ) .
For most pairs , the correlations are close to what would be expected from random behavior .
Additionally , R-L has negative correlations with other metrics .
It indicates that there is no global agreement on what constitutes improvements when the summaries are already better than the baseline .
This is akin to the Simpson paradox because considering different sub-populations yields different conclusions ( Wagner , 1982 ) .
In fact , it is simple to distinguish obviously bad from obviously good summaries , which results in superficially high correlations when the whole scoring range is considered ( Radev et al. , 2003 ) .
However , summarization systems target the high-scoring range and evaluation metrics should accurately distinguish between high-scoring summaries .
Unfortunately , existing metrics disagree wildly in this range .
Disagreement increases with higher -scoring summaries :
We also visualize the gradual change in metrics agreement when moving from the average to the high-scoring range in figure 2 .
For each pair of metrics , the disagreement clearly increases for higher scoring summary pairs .
This confirms that metrics disagree for high-scoring summaries .
It is more pronounced for some pairs like the ones involving R-L as already observed in table 1 .
The problem with reporting several disagreeing metrics :
It is a common good practice to report the results of several evaluation metrics .
In the average scoring range , where metrics generally agree , this creates a robust measure of progress .
The specificities of each metric are averaged - out putting the focus on the general trend .
However , when the metrics do not correlate , improvements according to one metric are rarely improvements for the other ones .
Let M = {m 1 , . . . , m n } be the set of evaluation metrics .
Then , for a topic T from the dataset ( W ) , we select a summary s and ask : among the summaries which are better than s for one metric ( N ) , how many are better for all metrics ( F ) ?
This is given by : F N = |{x ? T | ?m ? M , m ( x ) > m ( s ) }| |{ x ? T | ?m ? M , m ( x ) > m ( s ) }| ( 1 ) Here , m ( x ) is the score of the summary x according to the metric m .
Thus , F N measures the difficulty of finding consistent improvements across metrics .
The process is repeated for 5 , 000 randomly sampled summaries in the sources .
In figure 3 , the resulting F N ratios are reported against the normalized average score of the selected summaries s.
We observe a quick decrease in the ratio F N .
The proportion of consistent improvements ( agreed by all metrics ) is dropping when the average score of summaries increases .
When the baseline scores go up , the disagreement between metrics is strong enough that we cannot identify summaries which are considered better than the baseline for each metric .
Thus , there is no common trend between metrics that can be exploited by reporting them together .
Discussion : Intuitively , smaller populations and narrow scoring ranges can also lead to lower correlations .
However , ( T ) displays low correlations with 102 summaries per topic whereas ( A ) has strong correlations with 50 summaries per topic .
Also , the high-scoring range covers 38 % of the full scoring range ( from LexRank to upper-bound ) , while human judgments cover 35 % of the full scoring range .
Thus , the width of the scoring range and the population size do not explain the As a limitation of this study , we can note that the data generation procedure simulates further progress in summarization by stochastically optimizing each evaluation metric .
While this constitutes a good approximation , there is no guarantee that high-scoring summaries are sampled with the same distribution as future summarization systems .
However , the sampling still covers a large diversity of high-scoring summary and reveal general properties of evaluation metrics .
Other tasks :
Our analysis is performed on TAC - 2008 and TAC - 2009 because they are benchmark datasets typically used for comparing evaluation metrics .
However , our approach can be applied to any dataset .
In particular , for future work , this study could be replicated for related fields like Machine Translation or Natural Language Generation .
Conclusion Evaluation metrics behave similarly on the average scoring range covered by existing human judgment datasets .
Thus , we cannot clearly decide which one is the best .
Yet , we showed that they will promote very different summaries in the highscoring range .
This disagreement is strong enough that there is no common trend which could be captured by reporting improvements across several metrics .
This casts some doubts on the evaluation methodologies in summarization and calls for the collection of human annotations for high-scoring summaries .
Indeed , since metrics strongly disagree in the high-scoring regime , at least some of them are deviating largely from humans .
By collecting human judgments in this specific range , we could identify the best ones using standard meta-evaluation techniques .
Such annotations would also be greatly beneficial to improve summarization systems and evaluation metrics alike .
