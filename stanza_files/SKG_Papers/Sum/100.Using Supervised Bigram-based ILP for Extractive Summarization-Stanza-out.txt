title
Using Supervised Bigram - based ILP for Extractive Summarization
abstract
In this paper , we propose a bigram based supervised method for extractive document summarization in the integer linear programming ( ILP ) framework .
For each bigram , a regression model is used to estimate its frequency in the reference summary .
The regression model uses a variety of indicative features and is trained discriminatively to minimize the distance between the estimated and the ground truth bigram frequency in the reference summary .
During testing , the sentence selection problem is formulated as an ILP problem to maximize the bigram gains .
We demonstrate that our system consistently outperforms the previous ILP method on different TAC data sets , and performs competitively compared to the best results in the TAC evaluations .
We also conducted various analysis to show the impact of bigram selection , weight estimation , and ILP setup .
Introduction Extractive summarization is a sentence selection problem : identifying important summary sentences from one or multiple documents .
Many methods have been developed for this problem , including supervised approaches that use classifiers to predict summary sentences , graph based approaches to rank the sentences , and recent global optimization methods such as integer linear programming ( ILP ) and submodular methods .
These global optimization methods have been shown to be quite powerful for extractive summarization , because they try to select important sentences and remove redundancy at the same time under the length constraint .
Gillick and Favre ( Gillick and Favre , 2009 ) introduced the concept- based ILP for summariza-tion .
Their system achieved the best result in the TAC 09 summarization task based on the ROUGE evaluation metric .
In this approach the goal is to maximize the sum of the weights of the language concepts that appear in the summary .
They used bigrams as such language concepts .
The association between the language concepts and sentences serves as the constraints .
This ILP method is formally represented as below ( see ( Gillick and Favre , 2009 ) for more details ) : max i w i c i ( 1 ) s.t. s j Occ ij ? c i ( 2 ) j s j Occ ij ? c i ( 3 ) j l j s j ? L ( 4 ) c i ? { 0 , 1 } ?i ( 5 ) s j ? { 0 , 1 } ?j ( 6 ) c i and s j are binary variables ( shown in ( 5 ) and ( 6 ) ) that indicate the presence of a concept and a sentence respectively .
w i is a concept 's weight and Occ ij means the occurrence of concept i in sentence j. Inequalities ( 2 ) ( 3 ) associate the sentences and concepts .
They ensure that selecting a sentence leads to the selection of all the concepts it contains , and selecting a concept only happens when it is present in at least one of the selected sentences .
There are two important components in this concept- based ILP : one is how to select the concepts ( c i ) ; the second is how to set up their weights ( w i ) .
Gillick and Favre ( Gillick and Favre , 2009 ) used bigrams as concepts , which are selected from a subset of the sentences , and their document frequency as the weight in the objective function .
In this paper , we propose to find a candidate summary such that the language concepts ( e.g. , bigrams ) in this candidate summary and the reference summary can have the same frequency .
We expect this restriction is more consistent with the ROUGE evaluation metric used for summarization ( Lin , 2004 ) .
In addition , in the previous conceptbased ILP method , the constraints are with respect to the appearance of language concepts , hence it cannot distinguish the importance of different language concepts in the reference summary .
Our method can decide not only which language concepts to use in ILP , but also the frequency of these language concepts in the candidate summary .
To estimate the bigram frequency in the summary , we propose to use a supervised regression model that is discriminatively trained using a variety of features .
Our experiments on several TAC summarization data sets demonstrate this proposed method outperforms the previous ILP system and often the best performing TAC system .
Proposed Method
Bigram Gain Maximization by ILP
We choose bigrams as the language concepts in our proposed method since they have been successfully used in previous work .
In addition , we expect that the bigram oriented ILP is consistent with the ROUGE - 2 measure widely used for summarization evaluation .
We start the description of our approach for the scenario where a human abstractive summary is provided , and the task is to select sentences to form an extractive summary .
Then Our goal is to make the bigram frequency in this system summary as close as possible to that in the reference .
For each bigram b , we define its gain : Gain ( b , sum ) = min{n b, ref , n b , sum } ( 7 ) where n b, ref is the frequency of b in the reference summary , and n b , sum is the frequency of b in the automatic summary .
The gain of a bigram is no more than its frequency in the reference summary , hence adding redundant bigrams will not increase the gain .
The total gain of an extractive summary is defined as the sum of every bigram gain in the summary : Gain( sum ) = b Gain ( b , sum ) = b min{n b, ref , s z( s ) * n b , s } ( 8 ) where s is a sentence in the document , n b , s is the frequency of b in sentence s , z ( s ) is a binary variable , indicating whether s is selected in the summary .
The goal is to find z that maximizes Gain( sum ) ( formula ( 8 ) ) under the length constraint L .
This problem can be casted as an ILP problem .
First , using the fact that min{a , x} = 0.5 ( ?|x ? a| + x + a ) , x , a ?
0 we have b min{n b, ref , s z( s ) * n b , s } = b 0.5 * ( ? | n b, ref ? s z( s ) * n b, s | + n b, ref + s z( s ) * n b , s )
Now the problem is equivalent to : max z b ( ? | n b, ref ? s z( s ) * n b , s | + n b, ref + s z( s ) * n b , s ) s.t. s z( s ) * | S| ? L ; z( s ) ? { 0 , 1 } This is equivalent to the ILP : max b ( s z( s ) * n b , s ? C b ) ( 9 ) s.t. s z ( s ) * | S| ? L ( 10 ) z( s ) ? { 0 , 1 } ( 11 ) ?C b ? n b, ref ? s z( s ) * n b , s ? C b ( 12 ) where C b is an auxiliary variable we introduce that is equal to |n b, ref ? s z( s ) * n b , s | , and n b , ref is a constant that can be dropped from the objective function .
Regression Model for Bigram Frequency Estimation
In the previous section , we assume that n n( b , ref ) b n( b , ref ) is the normalized frequency in the summary .
Now the problem is to automatically estimate N b, ref .
Since the normalized frequency N b, ref is a real number , we choose to use a logistic regression model to predict it : N b, ref = exp{w ? f ( b ) } j exp{w ? f ( b j ) } ( 13 ) where f ( b j ) is the feature vector of bigram b j and w ? is the corresponding feature weight .
Since even for identical bigrams b i = b j , their feature vectors may be different ( f ( b i ) = f ( b j ) ) due to their different contexts , we sum up frequencies for identical bigrams { b i | b i = b} : N b, ref = i , b i =b N b i , ref = i , b i =b exp{w ? f ( b i ) } j exp{w ? f ( b j ) } ( 14 )
To train this regression model using the given reference abstractive summaries , rather than trying to minimize the squared error as typically done , we propose a new objective function .
Since the normalized frequency satisfies the probability constraint b N b, ref = 1 , we propose to use KL divergence to measure the distance between the estimated frequencies and the ground truth values .
The objective function for training is thus to minimize the KL distance : min b N b, ref log N b, ref N b, ref ( 15 ) where N b, ref is the true normalized frequency of bigram b in reference summaries .
Finally , we replace N b, ref in Formula ( 15 ) with Eq ( 14 ) and get the objective function below : max b N b, ref log i , b i =b exp{w ? f ( b i ) } j exp{w ? f ( b j ) } ( 16 )
This shares the same form as the contrastive estimation proposed by ( Smith and Eisner , 2005 ) .
We use gradient decent method for parameter estimation , initial w is set with zero .
Features
Each bigram is represented using a set of features in the above regression model .
We use two types of features : word level and sentence level features .
Some of these features have been used in previous work ( Aker and Gaizauskas , 2009 ; Brandow et al. , 1995 ; Edmundson , 1969 ; Radev , 2001 ) : ? Word Level : - 1 . Term frequency1 : The frequency of this bigram in the given topic .
- 2 . Term frequency2 : The frequency of this bigram in the selected sentences 1 . - 3 .
Stop word ratio : Ratio of stop words in this bigram .
The value can be { 0 , 0.5 , 1 } . - 4 . Similarity with topic title :
The number of common tokens in these two strings , divided by the length of the longer string .
- 5 . Similarity with description of the topic : Similarity of the bigram with topic description ( see next data section about the given topics in the summarization task ) .
? Sentence Level : ( information of sentence containing the bigram ) - 6 . Sentence ratio :
Number of sentences that include this bigram , divided by the total number of the selected sentences .
- 7 . Sentence similarity : Sentence similarity with topic 's query , which is the concatenation of topic title and description .
- 8 . Sentence position : Sentence position in the document .
- 9 . Sentence length :
The number of words in the sentence .
- 10 .
Paragraph starter : Binary feature indicating whether this sentence is the beginning of a paragraph .
Experiments
Data
We evaluate our method using several recent TAC data sets , from 2008 to 2011 .
The TAC summarization task is to generate at most 100 words summaries from 10 documents for a given topic query ( with a title and more detailed description ) .
For model training , we also included two years ' DUC data ( 2006 and 2007 ) .
When evaluating on one TAC data set , we use the other years of the TAC data plus the two DUC data sets as the training data .
Summarization System
We use the same system pipeline described in ( Gillick et al. , 2008 ; McDonald , 2007 ) .
The key modules in the ICSI ILP system ( Gillick et al. , 2008 ) are briefly described below .
?
Step 1 : Clean documents , split text into sentences .
? Step 2 : Extract bigrams from all the sentences , then select those bigrams with document frequency equal to more than 3 .
We call this subset as initial bigram set in the following .
The difference between the ICSI and our system is in the 4th step .
In our method , we first extract all the bigrams from the selected sentences and then estimate each bigram 's N b , ref using the regression model .
Then we use the top -n bigrams with their N b, ref and all the selected sentences in our proposed ILP module for summary sentence selection .
When training our bigram regression model , we use each of the 4 reference summaries separately , i.e. , the bigram frequency is obtained from one reference summary .
The same pre-selection of sentences described above is also applied in training , that is , the bigram instances used in training are from these selected sentences and the reference summary .
Experiment and Analysis
Experimental Results
Table 1 shows the ROUGE - 2 results of our proposed system , the ICSI system , and also the best performing system in the NIST TAC evaluation .
We can see that our proposed system consistently outperforms ICSI ILP system ( the gain is statistically significant based on ROUGE 's 95 % confidence internal results ) .
Compared to the best reported TAC result , our method has better performance on three data sets , except 2011 data .
Note that the best performing system for the 2009 data is the ICSI ILP system , with an additional compression step .
Our ILP method is purely extrac-tive .
Even without using compression , our approach performs better than the full ICSI system .
The best performing system for the 2011 data also has some compression module .
We expect that after applying sentence compression and merging , we will have even better performance , however , our focus in this paper is on the bigram- based extractive summarization .
There are several differences between the ICSI system and our proposed method .
First is the bigrams ( concepts ) used .
We use the top 100 bigrams from our bigram estimation module ; whereas the ICSI system just used the initial bigram set described in Section 3.2 .
Second , the weights for those bigrams differ .
We used the estimated value from the regression model ; the ICSI system just uses the bigram 's document frequency in the original text as weight .
Finally , two systems use different ILP setups .
To analyze which factors ( or all of them ) explain the performance difference , we conducted various controlled experiments for these three factors ( bigrams , weights , ILP ) .
All of the following experiments use the TAC 2009 data as the test set .
ICSI
Effect of Bigram Weights
In this experiment , we vary the weighting methods for the two systems : our proposed method and the ICSI system .
We use three weighting setups : the estimated bigram frequency value in our method , document frequency , or term frequency from the original text .
Table 2 and 3 show the results using the top 100 bigrams from our system and the initial bigram set from the ICSI system respectively .
We also evaluate using the two different ILP configurations in these experiments .
First of all , we can see that for both ILP systems , our estimated bigram weights outperform the other frequency - based weights .
For the ICSI ILP system , using bigram document frequency achieves better performance than term frequency ( which verified why document frequency is used in their system ) .
In contrast , for our ILP method , the bigram 's term frequency is slightly more useful than its document frequency .
This indicates that our estimated value is more related to bigram 's term frequency in the original text .
When the weight is document frequency , the ICSI 's result is better than our proposed ILP ; whereas when using term frequency as the weights , our ILP has better results , again suggesting term frequency fits our ILP system better .
When the weight is estimated value , the results depend on the bigram set used .
The ICSI 's ILP performs slightly better than ours when it is equipped with the initial bigram , but our proposed ILP has much better results using our selected top100 bigrams .
This shows that the size and quality of the bigrams has an impact on the ILP modules .
The Effect of Bigram Set 's size
In our proposed system , we use 100 top bigrams .
There are about 80 bigrams used in the ICSI ILP system .
A natural question to ask is the impact of the number of bigrams and their quality on the summarization system .
Increasing the number of bigrams used in the system will lead to better coverage , however , the incorrect bigrams also increase and have a negative impact on the system performance .
To examine the best tradeoff , we conduct the experiments by choosing the different top - N bigram set for the two ILP systems , as shown in Fig 2 .
For both the ILP systems , we used the estimated weight value for the bigrams .
We can see that the ICSI ILP system performs better when the input bigrams have less noise ( those bigrams that are not in summary ) .
However , our proposed method is slightly more robust to this kind of noise , possibly because of the weights we use in our system - the noisy bigrams have lower weights and thus less impact on the final system performance .
Overall the two systems have similar trends : performance increases at the beginning when using more bigrams , and after certain points starts degrading with too many bigrams .
The optimal number of bigrams differs for the two systems , with a larger number of bigrams in our method .
We also notice that the ICSI ILP system achieved a ROUGE - 2 of 0.1218 when using top 60 bigrams , which is better than using the initial bigram set in their method ( 0.1160 ) .
Oracle Experiments
Based on the above analysis , we can see the impact of the bigram set and their weights .
The following experiments are designed to demonstrate the best system performance we can achieve if we have access to good quality bigrams and weights .
Here we use the information from the reference summary .
The first is an oracle experiment , where we use all the bigrams from the reference summaries that are also in the original text .
In the ICSI ILP system , the weights are the document frequency from the multiple reference summaries .
In our ILP module , we use the term frequency of the bigram .
The oracle results are shown in Table 5 .
We can see these are significantly better than the automatic systems .
From Table 5 , we notice that ICSI 's ILP performs marginally better than our proposed ILP .
We hypothesize that one reason may be that many bigrams in the summary reference only appear once .
Table 6 shows the frequency of the bigrams in the summary .
Indeed 85 % of bigram only appear once
ILP System ROUGE-2 Our ILP 0.2124 ICSI ILP 0.2128 Table 5 : Oracle experiment : using bigrams and their frequencies in the reference summary as weights .
and no bigrams appear more than 9 times .
For the majority of the bigrams , our method and the ICSI ILP are the same .
For the others , our system has slight disadvantage when using the reference term frequency .
We expect the high term frequency may need to be properly smoothed / normalized .
We also treat the oracle results as the gold standard for extractive summarization and compared how the two automatic summarization systems differ at the sentence level .
This is different from the results in Table 1 , which are the ROUGE results comparing to human written abstractive summaries at the n-gram level .
We found that among the 188 sentences in this gold standard , our system hits 31 and ICSI only has 23 .
This again shows that our system has better performance , not just at the word level based on ROUGE measures , but also at the sentence level .
There are on average 3 different sentences per topic between these two results .
In the second experiment , after we obtain the estimated N b, ref for every bigram in the selected sentences from our regression model , we only keep those bigrams that are in the reference summary , and use the estimated weights for both ILP modules .
Table 7 shows the results .
We can consider these as the upper bound the system can achieve if we use the automatically estimated weights for the correct bigrams .
In this experiment ICSI ILP 's performance still performs better than ours .
This might be attributed to the fact there is less noise ( all the bigrams are the correct ones ) and thus the ICSI ILP system performs well .
We can see that these results are worse than the previous oracle experiments , but are better than using the automatically generated bigrams , again showing the bigram and weight estimation is critical for summarization .
7 : Summarization results when using the estimated weights and only keeping the bigrams that are in the reference summary .
#
Effect of Training Set Since our method uses supervised learning , we conduct the experiment to show the impact of training size .
In TAC 's data , each topic has two sets of documents .
For set A , the task is a standard summarization , and there are 4 reference summaries , each 100 words long ; for set B , it is an update summarization task - the summary includes information not mentioned in the summary from set A .
There are also 4 reference summaries , with 400 words in total .
Table 8 shows the results on 2009 data when using the data from different years and different sets for training .
We notice that when the training data only contains set A , the performance is always better than using set B or the combined set A and B .
This is not surprising because of the different task definition .
Therefore , for the rest of the study on data size impact , we only use data set A from the TAC data and the DUC data as the training set .
In total there are about 233 topics from the two years ' DUC data ( 06 , 07 ) and three years ' TAC data ( 08 , 10 , 11 ) .
We incrementally add 20 topics every time ( from DUC06 to TAC11 ) and plot the learning curve , as shown in Fig
Summary of Analysis
The previous experiments have shown the impact of the three factors : the quality of the bigrams themselves , the weights used for these bigrams , and the ILP module .
We found that the bigrams and their weights are critical for both the ILP setups .
However , there is negligible difference between the two ILP methods .
An important part of our system is the supervised method for bigram and weight estimation .
We have already seen for the previous ILP method , when using our bigrams together with the weights , better performance can be achieved .
Therefore we ask the question whether this is simply because we use supervised learning , or whether our proposed regression model is the key .
To answer this , we trained a simple supervised binary classifier for bigram prediction ( positive means that a bigram appears in the summary ) using the same set of features as used in our bigram weight estimation module , and then used their document frequency in the ICSI ILP system .
The result for this method is 0.1128 on the TAC 2009 data .
This is much lower than our result .
We originally expected that using the supervised method may outperform the unsupervised bigram selection which only uses term frequency information .
Further experiments are needed to investigate this .
From this we can see that it is not just the supervised methods or using annotated data that yields the overall improved system performance , but rather our proposed regression setup for bigrams is the main reason .
Related Work
We briefly describe some prior work on summarization in this section .
Unsupervised methods have been widely used .
In particular , recently several optimization approaches have demonstrated competitive performance for extractive summarization task .
Maximum marginal relevance ( MMR ) ( Carbonell and Goldstein , 1998 ) uses a greedy algorithm to find summary sentences .
( Mc- Donald , 2007 ) improved the MMR algorithm to dynamic programming .
They used a modified objective function in order to consider whether the selected sentence is globally optimal .
Sentencelevel ILP was also first introduced in ( McDonald , 2007 ) , but ( Gillick and Favre , 2009 ) revised it to concept - based ILP .
( Woodsend and Lapata , 2012 ) utilized ILP to jointly optimize different aspects including content selection , surface realization , and rewrite rules in summarization .
( Galanis et al. , 2012 ) uses ILP to jointly maximize the importance of the sentences and their diversity in the summary .
( Berg - Kirkpatrick et al. , 2011 ) applied a similar idea to conduct the sentence compression and extraction for multiple document summarization .
( Jin et al. , 2010 ) made a comparative study on sentence / concept selection and pairwise and list ranking algorithms , and concluded ILP performed better than MMR and the diversity penalty strategy in sentence / concept selection .
Other global optimization methods include submodularity ( Lin and Bilmes , 2010 ) and graph- based approaches ( Erkan and Radev , 2004 ; Leskovec et al. , 2005 ; Mihalcea and Tarau , 2004 ) .
Various unsupervised probabilistic topic models have also been investigated for summarization and shown promising .
For example , ( Celikyilmaz and Hakkani - T?r , 2011 ) used it to model the hidden abstract concepts across documents as well as the correlation between these concepts to generate topically coherent and non-redundant summaries .
( Darling and Song , 2011 ) applied it to separate the semantically important words from the lowcontent function words .
In contrast to these unsupervised approaches , there are also various efforts on supervised learning for summarization where a model is trained to predict whether a sentence is in the summary or not .
Different features and classifiers have been explored for this task , such as Bayesian method ( Kupiec et al. , 1995 ) , maximum entropy ( Osborne , 2002 ) , CRF ( Galley , 2006 ) , and recently reinforcement learning ( Ryang and Abekawa , 2012 ) .
( Aker et al. , 2010 ) used discriminative reranking on multiple candidates generated by A* search .
Recently , research has also been performed to address some issues in the supervised setup , such as the class data imbalance problem ( Xie and Liu , 2010 ) .
In this paper , we propose to incorporate the supervised method into the concept - based ILP framework .
Unlike previous work using sentencebased supervised learning , we use a regression model to estimate the bigrams and their weights , and use these to guide sentence selection .
Compared to the direct sentence - based classification or regression methods mentioned above , our method has an advantage .
When abstractive summaries are given , one needs to use that information to automatically generate reference labels ( a sentence is in the summary or not ) for extractive summarization .
Most researchers have used the similarity between a sentence in the document and the abstractive summary for labeling .
This is not a perfect process .
In our method , we do not need to generate this extra label for model training since ours is based on bigrams - it is straightforward to obtain the reference frequency for bigrams by simply looking at the reference summary .
We expect our approach also paves an easy way for future automatic abstractive summarization .
One previous study that is most related to ours is ( Conroy et al. , 2011 ) , which utilized a Naive Bayes classifier to predict the probability of a bigram , and applied ILP for the final sentence selection .
They used more features than ours , whereas we use a discriminatively trained regression model and a modified ILP framework .
Our proposed method performs better than their reported results in TAC 2011 data .
Another study closely related to ours is ( Davis et al. , 2012 ) , which leveraged Latent Semantic Analysis ( LSA ) to produce term weights and selected summary sentences by computing an approximate solution to the Budgeted Maximal Coverage problem .
Conclusion and Future Work
In this paper , we leverage the ILP method as a core component in our summarization system .
Different from the previous ILP summarization approach , we propose a supervised learning method ( a discriminatively trained regression model ) to determine the importance of the bigrams fed to the ILP module .
In addition , we revise the ILP to maximize the bigram gain ( which is expected to be highly correlated with ROUGE - 2 scores ) rather than the concept / bigram coverage .
Our proposed method yielded better results than the previous state - of - the - art ILP system on different TAC data sets .
From a series of experiments , we found that there is little difference between the two ILP modules , and that the improved system performance is attributed to the fact that our proposed supervised bigram estimation module can successfully gather the important bigram and assign them appropriate weights .
There are several directions that warrant further research .
We plan to consider the context of bigrams to better predict whether a bigram is in the reference summary .
We will also investigate the relationship between concepts and sentences , which may help move towards abstractive summarization .
Fig 1 Figure 1 : 11
Fig 1 shows the bigram coverage ( number of bigrams used in the system that are also in reference summaries ) when we vary N selected bigrams .
As expected , we can see that as n increases , there are more reference summary bigrams included in the system .
There are 25 summary bigrams in the top -50 bigrams and about 38 in top - 100 bigrams .
Compared with the ICSI system that has around 80 bigrams in the initial bigram set and 29 in the reference summary , our estimation module has better coverage .
