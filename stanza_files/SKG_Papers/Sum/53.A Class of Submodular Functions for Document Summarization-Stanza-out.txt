title
A Class of Submodular Functions for Document Summarization
abstract
We design a class of submodular functions meant for document summarization tasks .
These functions each combine two terms , one which encourages the summary to be representative of the corpus , and the other which positively rewards diversity .
Critically , our functions are monotone nondecreasing and submodular , which means that an efficient scalable greedy optimization scheme has a constant factor guarantee of optimality .
When evaluated on DUC 2004 - 2007 corpora , we obtain better than existing state - of - art results in both generic and query -focused document summarization .
Lastly , we show that several well - established methods for document summarization correspond , in fact , to submodular function optimization , adding further evidence that submodular functions are a natural fit for document summarization .
Introduction
In this paper , we address the problem of generic and query - based extractive summarization from collections of related documents , a task commonly known as multi-document summarization .
We treat this task as monotone submodular function maximization ( to be defined in Section 2 ) .
This has a number of critical benefits .
On the one hand , there exists a simple greedy algorithm for monotone submodular function maximization where the summary solution obtained ( say ? ) is guaranteed to be almost as good as the best possible solution ( say S opt ) according to an objective F. More precisely , the greedy algorithm is a constant factor approximation to the cardinality constrained version of the problem , so that F ( ? ) ? ( 1 ? 1/e) F ( S opt ) ? 0.632F ( S opt ) .
This is particularly attractive since the quality of the solution does not depend on the size of the problem , so even very large size problems do well .
It is also important to note that this is a worst case bound , and in most cases the quality of the solution obtained will be much better than this bound suggests .
Of course , none of this is useful if the objective function F is inappropriate for the summarization task .
In this paper , we argue that monotone nondecreasing submodular functions F are an ideal class of functions to investigate for document summarization .
We show , in fact , that many well - established methods for summarization ( Carbonell and Goldstein , 1998 ; Filatova and Hatzivassiloglou , 2004 ; Takamura and Okumura , 2009 ; Riedhammer et al. , 2010 ; Shen and Li , 2010 ) correspond to submodular function optimization , a property not explicitly mentioned in these publications .
We take this fact , however , as testament to the value of submodular functions for summarization : if summarization algorithms are repeatedly developed that , by chance , happen to be an instance of a submodular function optimization , this suggests that submodular functions are a natural fit .
On the other hand , other authors have started realizing explicitly the value of submodular functions for summarization ( Lin and Bilmes , 2010 ; Qazvinian et al. , 2010 ) .
Submodular functions share many properties in common with convex functions , one of which is that they are closed under a number of common combination operations ( summation , certain compositions , restrictions , and so on ) .
These operations give us the tools necessary to design a powerful submodular objective for submodular document summarization that extends beyond any previous work .
We demonstrate this by carefully crafting a class of submodular func-tions we feel are ideal for extractive summarization tasks , both generic and query-focused .
In doing so , we demonstrate better than existing state - of - the - art performance on a number of standard summarization evaluation tasks , namely DUC - 04 through to DUC - 07 .
We believe our work , moreover , might act as a springboard for researchers in summarization to consider the problem of " how to design a submodular function " for the summarization task .
In Section 2 , we provide a brief background on submodular functions and their optimization .
Section 3 describes how the task of extractive summarization can be viewed as a problem of submodular function maximization .
We also in this section show that many standard methods for summarization are , in fact , already performing submodular function optimization .
In Section 4 , we present our own submodular functions .
Section 5 presents results on both generic and query -focused summarization tasks , showing as far as we know the best known ROUGE results for DUC - 04 through DUC - 06 , and the best known precision results for DUC - 07 , and the best recall DUC - 07 results among those that do not use a web search engine .
Section 6 discusses implications for future work .
Background on Submodularity
We are given a set of objects V = {v 1 , . . . , v n } and a function F : 2 V ?
R that returns a real value for any subset S ? V .
We are interested in finding the subset of bounded size | S| ? k that maximizes the function , e.g. , argmax S?V F ( S ) .
In general , this operation is hopelessly intractable , an unfortunate fact since the optimization coincides with many important applications .
For example , F might correspond to the value or coverage of a set of sensor locations in an environment , and the goal is to find the best locations for a fixed number of sensors ( Krause et al. , 2008 ) .
If the function F is monotone submodular then the maximization is still NP complete , but it was shown in ( Nemhauser et al. , 1978 ) that a greedy algorithm finds an approximate solution guaranteed to be within e?1 e ? 0.63 of the optimal solution , as mentioned in Section 1 .
A version of this algorithm ( Minoux , 1978 ) , moreover , scales to very large data sets .
Submodular functions are those that satisfy the property of diminishing returns : for any A ? B ? V \v , a sub- modular function F must satisfy F ( A +v ) ? F( A ) ? F ( B + v ) ? F ( B ) .
That is , the incremental " value " of v decreases as the context in which v is considered grows from A to B .
An equivalent definition , useful mathematically , is that for any A , B ? V , we must have that F ( A ) + F ( B ) ? F( A ? B ) + F ( A ? B ) .
If this is satisfied everywhere with equality , then the function F is called modular , and in such case F ( A ) = c + a?A f a for a sized | V | vector f of real values and constant c.
A set function F is mono- tone nondecreasing if ?A ? B , F ( A ) ? F ( B ) .
As shorthand , in this paper , monotone nondecreasing submodular functions will simply be referred to as monotone submodular .
Historically , submodular functions have their roots in economics , game theory , combinatorial optimization , and operations research .
More recently , submodular functions have started receiving attention in the machine learning and computer vision community ( Kempe et al. , 2003 ; Narasimhan and Bilmes , 2005 ; Krause and Guestrin , 2005 ; Narasimhan and Bilmes , 2007 ; Krause et al. , 2008 ; Kolmogorov and Zabin , 2004 ) and have recently been introduced to natural language processing for the tasks of document summarization ( Lin and Bilmes , 2010 ) and word alignment ( Lin and Bilmes , 2011 ) .
Submodular functions share a number of properties in common with convex and concave functions ( Lov?sz , 1983 ) , including their wide applicability , their generality , their multiple options for their representation , and their closure under a number of common operators ( including mixtures , truncation , complementation , and certain convolutions ) .
For example , if a collection of functions { F i } i is submodular , then so is their weighted sum F = i ?
i F i where ?
i are nonnegative weights .
It is not hard to show that submodular functions also have the following composition property with concave functions :
Theorem 1 . Given functions F : 2 V ? R and f : R ? R , the composition F = f ? F : 2 V ? R ( i.e. , F ( S ) = f ( F ( S ) ) ) is nondecreasing submodular , if f is non-decreasing concave and F is nondecreasing submodular .
Submodularity in Summarization
Summarization with knapsack constraint
Let the ground set V represents all the sentences ( or other linguistic units ) in a document ( or document collection , in the multi-document summarization case ) .
The task of extractive document summarization is to select a subset S ? V to represent the entirety ( ground set V ) .
There are typically constraints on S , however .
Obviously , we should have | S| < | V | = N as it is a summary and should be small .
In standard summarization tasks ( e.g. , DUC evaluations ) , the summary is usually required to be length-limited .
Therefore , constraints on S can naturally be modeled as knapsack constraints : i?S c i ?
b , where c i is the non-negative cost of selecting unit i ( e.g. , the number of words in the sentence ) and b is our budget .
If we use a set function F : 2 V ?
R to measure the quality of the summary set S , the summarization problem can then be formalized as the following combinatorial optimization problem : Problem 1 . Find S * ? argmax S?V F ( S ) subject to : i?S c i ? b. Since this is a generalization of the cardinality constraint ( where c i = 1 , ?i ) , this also constitutes a ( well-known ) NP - hard problem .
In this case as well , however , a modified greedy algorithm with partial enumeration can solve Problem 1 near-optimally with ( 1?1/e ) - approximation factor if F is monotone submodular ( Sviridenko , 2004 ) .
The partial enumeration , however , is too computationally expensive for real world applications .
In ( Lin and Bilmes , 2010 ) , we generalize the work by Khuller et al . ( 1999 ) on the budgeted maximum cover problem to the general submodular framework , and show a practical greedy algorithm with a ( 1 ? 1 / ? e ) - approximation factor , where each greedy step adds the unit with the largest ratio of objective function gain to scaled cost , while not violating the budget constraint ( see ( Lin and Bilmes , 2010 ) for details ) .
Note that in all cases , submodularity and monotonicity are two necessary ingredients to guarantee that the greedy algorithm gives near-optimal solutions .
In fact , greedy - like algorithms have been widely used in summarization .
One of the more popular approaches is maximum marginal relevance ( MMR ) ( Carbonell and Goldstein , 1998 ) , where a greedy algorithm selects the most relevant sentences , and at the same time avoids redundancy by removing sentences that are too similar to ones already selected .
Interestingly , the gain function defined in the original MMR paper ( Carbonell and Goldstein , 1998 ) satisfies diminishing returns , a fact apparently unnoticed until now .
In particular , Carbonell and Goldstein ( 1998 ) define an objective function gain of adding element k to set S ( k / ? S ) as : ? Sim 1 ( s k , q ) ? ( 1 ? ? ) max i?S Sim 2 ( s i , s k ) , ( 1 ) where Sim 1 ( s k , q ) measures the similarity between unit s k to a query q , Sim 2 ( s i , s k ) measures the similarity between unit s i and unit s k , and 0 ? ? ?
1 is a trade- off coefficient .
We have : Theorem 2 . Given an expression for F MMR such that F MMR ( S ? { k } ) ?
F MMR ( S ) is equal to Eq. 1 , F MMR is non-monotone submodular .
Obviously , diminishing -returns hold since max i?S Sim 2 ( s i , s k ) ? max i?R Sim 2 ( s i , s k ) for all S ? R , and therefore F MMR is submodular .
On the other hand , F MMR , would not be monotone , so the greedy algorithm 's constant -factor approximation guarantee does not apply in this case .
When scoring a summary at the sub-sentence level , submodularity naturally arises .
Concept- based summarization ( Filatova and Hatzivassiloglou , 2004 ; Takamura and Okumura , 2009 ; Riedhammer et al. , 2010 ; Qazvinian et al. , 2010 ) usually maximizes the weighted credit of concepts covered by the summary .
Although the authors may not have noticed , their objective functions are also submodular , adding more evidence suggesting that submodularity is natural for summarization tasks .
Indeed , let S be a subset of sentences in the document and denote ?( S ) as the set of concepts contained in S .
The total credit of the concepts covered by S is then F concept ( S ) i ? ( S ) c i , where c i is the credit of concept i .
This function is known to be submodular ( Narayanan , 1997 ) .
Similar to the MMR approach , in ( Lin and Bilmes , 2010 ) , a submodular graph based objective function is proposed where a graph cut function , measuring the similarity of the summary to the rest of document , is combined with a subtracted redundancy penalty function .
The objective function is submodular but again , non-monotone .
We theoretically justify that the performance guarantee of the greedy algorithm holds for this objective function with high probability ( Lin and Bilmes , 2010 ) .
Our justification , however , is shown to be applicable only to certain particular non-monotone submodular functions , under certain reasonable assumptions about the probability distribution over weights of the graph .
Summarization with covering constraint Another perspective is to treat the summarization problem as finding a low-cost subset of the document under the constraint that a summary should cover all ( or a sufficient amount of ) the information in the document .
Formally , this can be expressed as Problem 2 .
Find S * ? argmin S?V i?S c i subject to : F ( S ) ? ? , where c i are the element costs , and set function F ( S ) measure the information covered by S .
When F is submodular , the constraint F ( S ) ? ? is called a submodular cover constraint .
When F is monotone submodular , a greedy algorithm that iteratively selects k with minimum c k /( F ( S ? { k } ) ? F ( S ) ) has approximation guarantees ( Wolsey , 1982 ) .
Recent work ( Shen and Li , 2010 ) proposes to model document summarization as finding a minimum dominating set and a greedy algorithm is used to solve the problem .
The dominating set constraint is also a submodular cover constraint .
Define ?( S ) be the set of elements that is either in S or is adjacent to some element in S .
Then S is a dominating set if |?( S ) | = | V |. Note that F dom ( S ) |?( S ) | is monotone submodular .
The dominating set constraint is then also a submodular cover constraint , and therefore the approaches in ( Shen and Li , 2010 ) are special cases of Problem 2 .
The solutions found in this framework , however , do not necessarily satisfy a summary 's budget constraint .
Consequently , a subset of the solution found by solving Problem 2 has to be constructed as the final summary , and the near-optimality is no longer guaranteed .
Therefore , solving Problem 1 for document summarization appears to be a better framework regarding global optimality .
In the present paper , our framework is that of Problem 1 .
Automatic summarization evaluation Automatic evaluation of summary quality is important for the research of document summarization as it avoids the labor-intensive and potentially inconsistent human evaluation .
ROUGE ( Lin , 2004 ) is widely used for summarization evaluation and it has been shown that ROUGE -N scores are highly correlated with human evaluation ( Lin , 2004 ) .
Interestingly , ROUGE -N is monotone submodular , adding further evidence that monotone submodular functions are natural for document summarization .
Theorem 3 . ROUGE -N is monotone submodular .
Proof .
By definition ( Lin , 2004 ) , ROUGE -N is the n-gram recall between a candidate summary and a set of reference summaries .
Precisely , let S be the candidate summary ( a set of sentences extracted from the ground set V ) , c e : 2 V ?
Z + be the number of times n-gram e occurs in summary S , and R i be the set of n-grams contained in the reference summary i ( suppose we have K reference summaries , i.e. , i = 1 , ? ? ? , K ) .
Then ROUGE -N can be written as the following set function : F ROUGE -N ( S ) K i=1 e?R i min( c e ( S ) , r e , i ) K i=1 e?R
i r e , i , where r e , i is the number of times n-gram e occurs in reference summary i. Since c e ( S ) is monotone modular and min(x , a ) is a concave non-decreasing function of x , min( c e ( S ) , r e , i ) is monotone submodular by Theorem 1 .
Since summation preserves submodularity , and the denominator is constant , we see that F ROUGE -N is monotone submodular .
Since the reference summaries are unknown , it is of course impossible to optimize F ROUGE -N directly .
Therefore , some approaches ( Filatova and Hatzivassiloglou , 2004 ; Takamura and Okumura , 2009 ; Riedhammer et al. , 2010 ) instead define " concepts " .
Alter-natively , we herein propose a class of monotone submodular functions that naturally models the quality of a summary while not depending on an explicit notion of concepts , as we will see in the following section .
Monotone Submodular Objectives
Two properties of a good summary are relevance and non-redundancy .
Objective functions for extractive summarization usually measure these two separately and then mix them together trading off encouraging relevance and penalizing redundancy .
The redundancy penalty usually violates the monotonicity of the objective functions ( Carbonell and Goldstein , 1998 ; Lin and Bilmes , 2010 ) .
We therefore propose to positively reward diversity instead of negatively penalizing redundancy .
In particular , we model the summary quality as F ( S ) = L ( S ) + ?R ( S ) , ( 2 ) where L( S ) measures the coverage , or " fidelity " , of summary set S to the document , R ( S ) rewards diversity in S , and ? ?
0 is a trade- off coefficient .
Note that the above is analogous to the objectives widely used in machine learning , where a loss function that measures the training set error ( we measure the coverage of summary to a document ) , is combined with a regularization term encouraging certain desirable ( e.g. , sparsity ) properties ( in our case , we " regularize " the solution to be more diverse ) .
In the following , we discuss how both L( S ) and R ( S ) are naturally monotone submodular .
Coverage function L( S ) can be interpreted either as a set function that measures the similarity of summary set S to the document to be summarized , or as a function representing some form of " coverage " of V by S.
Most naturally , L ( S ) should be monotone , as coverage improves with a larger summary .
L( S ) should also be submodular : consider adding a new sentence into two summary sets , one a subset of the other .
Intuitively , the increment when adding a new sentence to the small summary set should be larger than the increment when adding it to the larger set , as the information carried by the new sentence might have already been covered by those sentences that are in the larger summary but not in the smaller summary .
This is exactly the property of diminishing returns .
Indeed , Shannon entropy , as the measurement of information , is another well - known monotone submodular function .
There are several ways to define L( S ) in our context .
For instance , we could use L( S ) = i? V , j?S w i , j where w i , j represents the similarity between i and j. L ( S ) could also be facility location objective , i.e. , L( S ) = i?V max j?S w i , j , as used in ( Lin et al. , 2009 ) .
We could also use L ( S ) = i ? ( S ) c i as used in concept- based summarization , where the definition of " concept " and the mechanism to extract these concepts become important .
All of these are monotone submodular .
Alternatively , in this paper we propose the following objective that does not reply on concepts .
Let L( S ) = i?V min { C i ( S ) , ? C i ( V ) } , ( 3 ) where C i : 2 V ?
R is a monotone submodular function and 0 ? ? ? 1 is a threshold co-efficient .
Firstly , L ( S ) as defined in Eqn. 3 is a monotone submodular function .
The monotonicity is immediate .
To see that L( S ) is submodular , consider the fact that f ( x ) = min( x , a ) where a ?
0 is a concave non-decreasing function , and by Theorem 1 , each summand in Eqn.
3 is a submodular function , and as summation preserves submodularity , L ( S ) is submodular .
Next , we explain the intuition behind Eqn .
3 . Basically , C i ( S ) measures how similar S is to element i , or how much of i is " covered " by S .
Then C i ( V ) is just the largest value that C i ( S ) can achieve .
We call i " saturated " by S when min { C i ( S ) , ?C i ( V ) } = ?C i ( V ) .
When i is already saturated in this way , any new sentence j can not further improve the coverage of i even if it is very similar to i ( i.e. , C i ( S ? { j } ) ? C i ( S ) is large ) .
This will give other sentences that are not yet saturated a higher chance of being better covered , and therefore the resulting summary tends to better cover the entire document .
One simple way to define C i ( S ) is just to use C i ( S ) = j?S w i , j ( 4 ) where w i , j ?
0 measures the similarity between i and j .
In this case , when ? = 1 , Eqn. 3 reduces to the case where L( S ) = i? V , j?S w i , j . As we will see in Section 5 , having an ? that is less than 1 significantly improves the performance compared to the case when ? = 1 , which coincides with our intuition that using a truncation threshold improves the final summary 's coverage .
Diversity reward function Instead of penalizing redundancy by subtracting from the objective , we propose to reward diversity by adding the following to the objective : R ( S ) = K i=1 j?P i ?S r j . ( 5 ) where P i , i = 1 , ? ? ? K is a partition of the ground set V ( i.e. , i P i = V and the P i s are disjoint ) into separate clusters , and r i ?
0 indicates the singleton reward of i ( i.e. , the reward of adding i into the empty set ) .
The value r i estimates the importance of i to the summary .
The function R ( S ) rewards diversity in that there is usually more benefit to selecting a sentence from a cluster not yet having one of its elements already chosen .
As soon as an element is selected from a cluster , other elements from the same cluster start having diminishing gain , thanks to the square root function .
For instance , consider the case where k 1 , k 2 ? P 1 , k 3 ? P 2 , and r k 1 = 4 , r k 2 = 9 , and r k 3 = 4 .
Assume k 1 is already in the summary set S. Greedily selecting the next element will choose k 3 rather than k 2 since ?
13 < 2 + 2 .
In other words , adding k 3 achieves a greater reward as it increases the diversity of the summary ( by choosing from a different cluster ) .
Note , R ( S ) is distinct from L ( S ) in that R ( S ) might wish to include certain outlier material that L ( S ) could ignore .
It is easy to show that R ( S ) is submodular by using the composition rule from Theorem 1 .
The square root is non-decreasing concave function .
Inside each square root lies a modular function with non-negative weights ( and thus is monotone ) .
Applying the square root to such a monotone submodular function yields a submodular function , and summing them all together retains submodularity , as mentioned in Section 2 .
The monotonicity of R ( S ) is straightforward .
Note , the form of Eqn. 5 is similar to structured group norms ( e.g. , ( Zhao et al. , 2009 ) ) , recently shown to be related to submodularity ( Bach , 2010 ; Jegelka and Bilmes , 2011 ) .
Several extensions to Eqn. 5 are discussed next : First , instead of using a ground set partition , intersecting clusters can be used .
Second , the square root function in Eqn. 5 can be replaced with any other non-decreasing concave functions ( e.g. , f ( x ) = log ( 1 + x ) ) while preserving the desired property of R ( S ) , and the curvature of the concave function then determines the rate that the reward diminishes .
Last , multi-resolution clustering ( or partitions ) with different sizes ( K ) can be used , i.e. , we can use a mixture of components , each of which has the structure of Eqn .
5 .
A mixture can better represent the core structure of the ground set ( e.g. , the hierarchical structure in the documents ( Celikyilmaz and Hakkani -t ?r , 2010 ) ) .
All such extensions preserve both monotonicity and submodularity .
Experiments
The document understanding conference ( DUC ) ( http://duc.nist.org) was the main forum providing benchmarks for researchers working on document summarization .
The tasks in DUC evolved from single -document summarization to multi-document summarization , and from generic summarization ( 2001 ) ( 2002 ) ( 2003 ) ( 2004 ) to query -focused summarization ( 2005 ) ( 2006 ) ( 2007 ) .
As ROUGE ( Lin , 2004 ) has been officially adopted for DUC evaluations since 2004 , we also take it as our main evaluation criterion .
We evaluated our approaches on DUC data 2003 - 2007 , and demonstrate results on both generic and query -focused summarization .
In all experiments , the modified greedy algorithm ( Lin and Bilmes , 2010 ) was used for summary generation .
Generic summarization Summarization tasks in DUC -03 and DUC - 04 are multi-document summarization on English news articles .
In each task , 50 document clusters are given , each of which consists of 10 documents .
For each document cluster , the system generated summary may not be longer than 665 bytes including spaces and punctuation .
We used DUC -03 as our development set , and tested on DUC - 04 data .
We show ROUGE - 1 scores 1 as it was the main evaluation criterion for DUC - 03 , 04 evaluations .
Documents were pre-processed by segmenting sentences and stemming words using the Porter Stemmer .
Each sentence was represented using a bag-of- terms vector , where we used context terms up to bi-grams .
Similarity between sentence i and sentence j , i.e. , w i , j , was computed using cosine similarity : w i , j = w?s i tf w , i ? tf w , j ? idf 2 w w?s i tf 2 w , s i idf 2 w w?s j tf 2 w , j idf 2 w , where tf w , i and tf w , j are the numbers of times that w appears in s i and sentence s j respectively , and idf w is the inverse document frequency ( IDF ) of term w ( up to bigram ) , which was calculated as the logarithm of the ratio of the number of articles that w appears over the total number of all articles in the document cluster .
We first tested our coverage and diversity reward objectives separately .
For coverage , we use a modular C i ( S ) = j?S w i , j for each sentence i , i.e. , L 1 ( S ) = i?V min ? ? ? j?S w i , j , ? k?V w i , k ? ? ? . ( 6 ) When ? = 1 , L 1 ( S ) reduces to i? V , j?S w i , j , which measures the overall similarity of summary set S to ground set V .
As mentioned in Section 4.1 , using such similarity measurement could possibly over-concentrate on a small portion of the document and result in a poor coverage of the whole document .
As shown in Table 1 , optimizing this objective function gives a ROUGE -1 F-measure score 32.44 % .
On the other hand , when using L 1 ( S ) with an ? < 1 ( the value of ? was determined on DUC -03 using a grid search ) , a ROUGE - 1 F-measure score 38.65 % is achieved , which is already better than the best performing system in DUC - 04 .
As for the diversity reward objective , we define the singleton reward as r i = 1 N j w i , j , which is the average similarity of sentence i to the rest of the document .
It basically states that the more similar to the whole document a sentence is , the more reward there will be by adding this sentence to an empty summary set .
By using this singleton reward , we have the following diversity reward function : R 1 ( S ) = K k=1 j?S?P k 1 N i?V w i , j . ( 7 ) In order to generate P k , k = 1 , ? ? ? K , we used CLUTO 2 to cluster the sentences , where the IDFweighted term vector was used as feature vector , and a direct K-mean clustering algorithm was used .
In this experiment , we set K = 0.2N .
In other words , there are 5 sentences in each cluster on average .
And as we can see in Table 1 , optimizing the diversity reward function alone achieves comparable performance to the DUC - 04 best system .
Combining L 1 ( S ) and R 1 ( S ) , our system outperforms the best system in DUC - 04 significantly , and it also outperforms several recent systems , including a concept- based summarization approach ( Takamura and Okumura , 2009 ) , a sentence topic model based system ( Wang et al. , 2009 ) , and our MMR - styled submodular system ( Lin and Bilmes , 2010 ) .
Figure 1 illustrates how ROUGE - 1 scores change when ? and K vary on the development set ( DUC - 03 ) .
Daum ? et al. ( 2009 ) 7.67 - Vine , Daum ?
et al. ( 2009 ) 8.24 -
Query - focused summarization
We evaluated our approach on the task of queryfocused summarization using DUC 05 - 07 data .
In DUC -05 and DUC - 06 , participants were given 50 document clusters , where each cluster contains 25 news articles related to the same topic .
Participants were asked to generate summaries of at most 250 words for each cluster .
For each cluster , a title and a narrative describing a user 's information need are provided .
The narrative is usually composed of a set of questions or a multi-sentence task description .
The main task in DUC -07 is the same as in DUC - 06 .
In DUC 05 - 07 , ROUGE - 2 was the primary criterion for evaluation , and thus we also report ROUGE - 2 3 ( both recall R , and precision F ) .
Documents were processed as in Section 5.1 .
We used both the title and the narrative as query , where stop words , including some function words ( e.g. , " describe " ) that appear frequently in the query , were removed .
All queries were then stemmed using the Porter Stemmer .
Note that there are several ways to incorporate query -focused information into both the coverage and diversity reward objectives .
For instance , C i ( S ) could be query - dependent in how it measures how much query - dependent information in i is covered by S. Also , the coefficient ?
could be query and sentence dependent , where it takes larger value when a sentence is more relevant to query ( i.e. , a larger value of ? means later truncation , and therefore more possible coverage ) .
Similarly , sentence clustering and singleton rewards in the diversity function can also be query-dependent .
In this experiment , we explore an objective with a query - independent coverage function ( R 1 ( S ) ) , indicating prior importance , combined with a query -dependent diversity reward function , where the latter is defined as : R Q ( S ) = K k=1 j?S?P k ?
N i?V w i , j + ( 1 ? ? ) r j, Q , where 0 ? ? ? 1 , and r j , Q represents the relevance between sentence j to query Q .
This query - dependent reward function is derived by using a singleton reward that is expressed as a convex combination of the query - independent score ( 1 N i?V w i , j ) and the query - dependent score ( r j , Q ) of a sentence .
We simply used the number of terms ( up to a bi-gram ) that sentence j overlaps the query Q as r j, Q , where the IDF weighting is not used ( i.e. , every term in the query , after stop word removal , was treated as equally important ) .
Both query - independent and query - dependent scores were then normalized by their largest value respectively such that they had roughly the same dynamic range .
To better estimate of the relevance between query and sentences , we further expanded sentences with synonyms and hypernyms of its constituent words .
In particular , part- of-speech tags were obtained for each sentence using the maximum entropy part- of-speech tagger ( Ratnaparkhi , 1996 ) , and all nouns were then expanded with their synonyms and hypernyms using WordNet ( Fellbaum , 1998 ) .
Note that these expanded documents were only used in the estimation r j, Q , and we plan to further explore whether there is benefit to use the expanded documents either in sentence similarity estimation or in sentence clustering in our future work .
We also tried to expand the query with synonyms and observed a performance decrease , presumably due to noisy information in a query expression .
While it is possible to use an approach that is similar to ( Toutanova et al. , 2007 ) to learn the coefficients in our objective function , we trained all coefficients to maximize ROUGE - 2 F-measure score using the Nelder - Mead ( derivative -free ) method .
Using L 1 ( S ) +?R Q ( S ) as the objective and with the same sentence clustering algorithm as in the generic summarization experiment ( K = 0.2N ) , our system , when both trained and tested on DUC -05 ( results in Table 2 ) , outperforms the Bayesian query - focused summarization approach and the search - based structured prediction approach , which were also trained and tested on DUC -05 ( Daum ?
et al. , 2009 ) .
Note that the system in ( Daum ?
et al. , 2009 ) that achieves its best performance ( 8.24 % in ROUGE - 2 recall ) is a so called " vine - growth " system , which can be seen as an abstractive approach , whereas our system is purely an extractive system .
Comparing to the extractive system in ( Daum ?
et al. , 2009 ) , our system performs much better ( 8.38 % v.s. 7.67 % ) .
More importantly , when trained only on DUC -06 and tested on DUC -05 ( results in Table 3 ) , our approach outperforms the best system in DUC - 05 significantly .
We further tested the system trained on DUC -05 on both DUC -06 and DUC - 07 .
The results on DUC - 06 are shown in Table .
4 . Our system outperforms the best system in DUC - 06 , as well as two recent approaches ( Shen and Li , 2010 ; Celikyilmaz and Hakkani-t ?r , 2010 ) .
On DUC - 07 , in terms of ROUGE - 2 score , our system outperforms PYTHY ( Toutanova et al. , 2007 ) , a state - of - the - art supervised summarization system , as well as two recent systems including a generative summarization system based on topic models ( Haghighi and Vanderwende , 2009 ) , and a hybrid hierarchical summarization system ( Celikyilmaz and Hakkani -t ?r , 2010 ) .
It also achieves comparable performance to the best DUC -07 system .
Note that in the best DUC -07 system ( Pingali et al. , 2007 ; Jagarlamudi et al. , 2006 ) , an external web search engine ( Yahoo ! ) was used to estimate a language model for query relevance .
In our system , no such web search expansion was used .
To further improve the performance of our system , we used both DUC -05 and DUC - 06 as a training set , and introduced three diversity reward terms into the objective where three different sentence clusterings with different resolutions were produced ( with sizes 0.3N , 0.15N and 0.05N ) .
Denoting a diversity reward corresponding to clustering ? as R Q , ? ( S ) , we model the summary quality as L 1 ( S ) + 3 ?=1 ? ? R Q , ? ( S ) .
As shown in Table 5 , using this objective function with multi-resolution diversity rewards improves our results further , and outperforms the best system in DUC - 07 in terms of ROUGE - 2 F-measure score .
Conclusion and discussion
In this paper , we show that submodularity naturally arises in document summarization .
Not only do many existing automatic summarization methods correspond to submodular function optimization , but also the widely used ROUGE evaluation is closely related to submodular functions .
As the corresponding submodular optimization problem can be solved efficiently and effectively , the remaining question is then how to design a submodular objective that best models the task .
To address this problem , we introduce a powerful class of monotone submodular functions that are well suited to document summarization by modeling two important properties of a summary , fidelity and diversity .
While more advanced NLP techniques could be easily incorporated into our functions ( e.g. , language models could define a better C i ( S ) , more advanced relevance estimations for the singleton rewards r i , and better and / or overlapping clustering algorithms for our diversity reward ) , we already show top results on standard benchmark evaluations using fairly basic NLP methods ( e.g. , term weighting and WordNet expansion ) , all , we believe , thanks to the power and generality of submodular functions .
As information retrieval and web search are closely related to query -focused summarization , our approach might be beneficial in those areas as well .
Figure 1 : 1 Figure 1 : ROUGE -1 F-measure scores on DUC -03 when ? and K vary in objective function L 1 ( S ) + ?R 1 ( S ) , where ? = 6 and ? = a N .
