title
Efficient Online Summarization of Microblogging Streams
abstract
The large amounts of data generated on microblogging services are making summarization challenging .
Previous research has mostly focused on working in batches or with filtered streams .
Input data has to be saved and analyzed several times , in order to detect underlying events and then summarize them .
We improve the efficiency of this process by designing an online abstractive algorithm .
Processing is done in a single pass , removing the need to save any input data and improving the running time .
An online approach is also able to generate the summaries in real time , using the latest information .
The algorithm we propose uses a word graph , along with optimization techniques such as decaying windows and pruning .
It outperforms the baseline in terms of summary quality , as well as time and memory efficiency .
Introduction Coined in 2006 - 2007 , the term microblogging is used to describe social networks that allow users to exchange small elements of content .
The widespread use of services like Facebook or Twitter means users have access to information that is otherwise unavailable .
Yet , as popular events commonly generate hundreds of thousands of tweets , following them can be difficult .
Stream summarization - generating a short text based on a sequence of posts - has been seen as the best approach in solving this problem .
This paper introduces Twitter Online Word Graph Summarizer .
TOWGS is the first online abstractive summarization algorithm and is capable of state - of - the - art processing speeds .
Most previous algorithms process a stream in batches .
They require several passes through the data or a feed specifically filtered for an event .
Batch summarization is suitable for small experiments , but it is not capable of efficiently handling thousands of tweets per second .
We collect a 3.4 million tweets dataset for evaluation purposes .
We choose as baseline an algorithm designed to summarize related tweets .
We determine a set of important events relative to the input data .
A group of judges rate the summaries generated by both algorithms for the given events .
Our solution is not only capable of online summarization , but it also outperforms the batch - based event-filtered baseline in terms of result quality .
The code for our algorithm is available online , along with the summaries , event keywords , ratings and tweet IDs : https://github.com/ andreiolariu/online-summarizer .
Related Work
Summarization
We distinguish two approaches in performing multi-document summarization : extractive and abstractive .
With the risk of oversimplification , we view extractive summarization as a process of selecting sentences from the documents , while abstractive summarization generates phrases that may not appear in the input data .
The extractive approach is usually modeled as an optimization problem ( Erkan and Radev , 2004 ) .
It can be combined with other techniques , such as clustering ( Silveira and Branco , 2012 ) or topic modeling .
Although actually performing word-level extraction , we consider word graph summarization algorithms abstractive because they are able to generate summaries not found among the input sentences .
Word graphs are used in compressing similar sentences ( Filippova , 2010 ) or summarizing product reviews ( Ganesan et al. , 2010 ) .
A relevant reference for the problem of up-date summarization is TAC update summarization track ( Dang and Owczarzak , 2008 ) .
Summarization on Twitter Regarding summarizing Twitter streams , we notice that all approaches are either restricted to specific filtered streams , or combined with event detection .
Extractive summarization is predominant when working with Twitter data .
It was first used for streams following simple and structured events , such as sports matches ( Takamura et al. , 2011 ; Chakrabarti and Punera , 2011 ; Nichols et al. , 2012 ; Zubiaga et al. , 2012 ) .
The Phrase Reinforcement algorithm , introduced by Sharifi et al . ( 2010a ; 2010 b ) , extracts frequently used sequences of words .
It was first applied in summarizing topic streams .
Subsequent research emphasized evolving topics ( Gao et al. , 2013 ) or event decomposition ( Olariu , 2013 ) .
Other approaches are based on integer linear programming ( Liu et al. , 2011 ) or LDA ( Khan et al. , 2013 ) .
Yang et al. ( 2012 ) develop a framework for summarization , highlighting its scalability .
Shou et al . ( 2013 ) introduce Sumblr , capable of cluster - based online extractive summarization .
Abstractive summarization is difficult on Twitter streams .
It is easily affected by noise or by the large variety of tweets .
Olariu ( 2013 ) showed that abstractive summarization is feasible if posts are clustered based on similarity or underlying events .
3 Twitter Online Word Graph Summarizer
Building the Word Graph
By employing a word graph , TOWGS does n't have to save any of the tweets , like extractive approaches do .
It can also skip the clustering step applied by the other online algorithm ( Shou et al. , 2013 ) , leading to faster summarization .
Previous word graph algorithms are based on bigrams .
Words are mapped to nodes in the graph , while an edge is added for each bigram .
When applied to Twitter messages , the results depend on the similarity of the summarized tweets ( Olariu , 2013 ) .
A set of related tweets generates a quality summary .
When applied to unrelated tweets , the generated summary lacks any meaning .
This happens because event-related signals ( in our case bigrams ) stand out when analyzing similar tweets , but get dominated by noise ( bigrams of common words ) when analyzing unrelated tweets .
We solve this issue by building the word graph from trigrams .
In our version , each node in the graph is a bigram .
Having a sentence ( w 1 , w 2 , w 3 , w 4 ) , we will first add two special words ( to mark the beginning and end of the sentence ) and generate the following edges : ( S , w 1 ) ? ( w 1 , w 2 ) , ( w 1 , w 2 ) ? ( w 2 , w 3 ) , ( w 2 , w 3 ) ? ( w 3 , w 4 ) and ( w 3 , w 4 ) ? ( w 4 , E ) .
Weights are added to nodes and edges in order to store the count for each bigram or trigram .
A negative effect of building the word graph from trigrams is that it significantly increases the number of nodes , leading to an increase in both memory and time .
We approach this issue by pruning the graph .
We implement pruning by periodically going through the whole graph and removing edges that were not encountered in the previous time window .
The length of this hard window can be set based on how much memory we would like to allocate , as well as on the size of the soft window introduced in the next subsection .
Word Graph Online Updating
In previous work , word graphs are discarded after generating the summary .
For our online summarization task , the graph is being constantly updated with tweets .
It can also respond , at any time , to queries for generating summaries starting from given keywords .
In order to keep the results relevant to what is popular at query time , we would like the graph to forget old data .
We implement this behavior by using decaying windows ( Rajaraman and Ullman , 2011 ) .
They are applied not only to graph weights ( counts of bigrams and trigrams ) , but also to counts of words and word pair cooccurrences .
At each time step ( in our case , each second ) , all counts are multiplied by 1 ? c , where c is a small constant .
For example , after one hour ( 3600 seconds ) , a value of 1 would become 0.48 with c = 0.0002 ( given by ( 1 ? c ) 3600 ) and 0.05 with c = 0.0008 .
In order to optimize the implementation , we explicitly multiply the counts only when they are read or incremented .
For each record , we keep the timestamp for its latest update t k .
Knowing the current timestamp t n , we update the count by multiplying with ( 1 ? c ) tn ? t k .
The size of the decaying window influences the results and the memory requirements for TOWGS .
A larger window requires less pruning and more memory , while also leading to more general summaries .
For example , given a stream of tweets related to a sporting event , summaries generated over very narrow windows would probably highlight individual goals , touchdowns or penalties .
The summary for a two hour window would instead capture just the final score .
Generating Summaries Given a word graph , generating a summary involves finding the highest scoring path in the graph .
That path connects the special words which mark the beginning and end of each sentence .
Since finding the exact solution is unfeasible given our real time querying scenario , we will employ a greedy search strategy .
The search starts by selecting the node ( bigram ) with the highest weight .
If we are interested in summarizing an event , we select the top ranking bigram containing one of the event 's keywords .
At this point , we have a path with one node .
We expand it by examining forward and backward edges and selecting the one that maximizes the scoring function : score ( n , e , m , p , k ) = c 1 f requency ( n ) ( 1a ) + c 2 edge score ( e , m ) ( 1 b ) + c 3 word score ( n , p ) ( 1 c ) + c 4 word score ( n , k ) ( 1d ) ? c 5 f requent word pen(n ) ( 1e ) ? c 6 repeated word pen(n ) ( 1f ) where p is a path representing a partial summary , n is a node adjacent to one of the path 's endpoints m by edge e and k is a list of keywords related to an event .
The constants c 1 through c 6 determine the influence each helper function has on the overall score .
The node n represents a bigram composed of the words w i ( already in the path as part of m ) and w o ( currently being considered for extending p ) .
The helper functions are defined as :
In all these cases , weights are counts implemented using decaying windows ( subsection 3.2 ) .
The scoring function gives a higher score to frequent bigrams ( equations 1a and 2a ) .
In the same time , individual words are penalized on their frequency ( equations 1e and 2d ) .
Such scores favor words used in specific contexts as opposed to general ones .
Trigrams are scored relative to bigrams ( equations 1 b and 2 b ) .
Again , this favors context specific bigrams .
The word score function ( equation 2 c ) computes the average correlation between a word ( w o from the bigram represented by node n ) and a set of words .
The set of words is either the current partial summary ( equation 1c ) or the event-related keywords ( equation 1d ) .
f requency ( n ) = log ( W b [ n ] ) ( 2a ) edge score ( e , m ) = log W t [ e ] W b [ m ] ( 2 b ) word score ( n , p ) = w?p 1 | p| log W d [ w , w o ] W w [ w ] W w [ w o ] ( 2 c ) f requent word pen( n ) = log( W w [ w o ] ) ( 2d We use logarithms in order to avoid floating point precision errors .
Evaluation
Corpus and Baseline Our corpus is built using the Twitter Search API .
We gathered an average of 485000 tweets per day for a total of seven days , between the 4 th and the 10 th of November 2013 .
This volume of tweets represents around 0.1 % of the entire Twitter stream .
Because of Twitter 's terms of service , sharing tweets directly is not allowed .
Instead , the source code we 've released comes with the tweet IDs needed for rebuilding the corpus .
The algorithm chosen as baseline is Multi-Sentence Compression ( or MSC ) , as presented in ( Olariu , 2013 ) .
MSC is a batch algorithm for abstractive summarization .
It performs best on groups of similar tweets , such as the ones related to an event .
After receiving a summarization query for a set of keywords , the tweets are filtered based on those keywords .
MSC processes the remaining tweets and generates a word graph .
After building the summary , the graph is discarded .
Because it has to store all tweets , MSC is not as memory -efficient as TOWGS .
It is also not timeefficient .
Each summarization query requires fil-tering the whole stream and building a new word graph .
The advantage MSC has is that it is working with filtered data .
Olariu ( 2013 ) has shown how susceptible word graphs are to noise .
Evaluation Procedure
The list of 64 events to be summarized was determined using a frequency based approach .
A simple procedure identified words that were used significantly more in a given day compared to a baseline .
The baselines were computed on a set of tweets posted between the 1 st and the 3 rd of November 2013 .
Words that often appeared together were grouped , with each group representing a different event .
The MSC algorithm received a cluster of posts for each event and generated summaries of one sentence each .
TOWGS processed the posts as a stream and answered to summarization requests .
The requests were sent after the peak of each event ( at the end of the hour during which that event registered the largest volume of posts ) .
The metrics used for assessing summary quality were completeness ( how much information is expressed in the summary , relative to the event tweets ) and grammaticality .
They were rated on a scale of 1 ( lowest ) to 5 ( highest ) .
We asked five judges to rate the summaries using a custom built web interface .
The judges were not native English speakers , but they were all proficient .
Three of them were Twitter users .
While the judges were subjective in assessing summary quality , each one did rate all of the summaries and the differences between the two algorithms ' ratings were consistent across all judges .
The constants c 1 through c 6 ( introduced in subsection 3.3 ) were set to 2 , 3 , 3 , 10 , 1 and 100 , respectively .
These values were manually determined after experimenting with a one day sample not included in the evaluation corpus .
Results
The average ratings for completeness are very similar , with a small advantage for TOWGS ( 4.29 versus MSC 's 4.16 ) .
We believe this is a good result , considering TOWGS does n't perform clustering and summarizes events that account for less than 1 % of the total volume .
Meanwhile , MSC processes only the event-related tweets .
The average rating for grammaticality is significantly higher for TOWGS ( 4.30 ) , as compared to MSC ( 3.78 ) .
While not engineered for speed , our implementation can process a day of data from our corpus ( around 485000 tweets ) in just under three minutes ( using one 3.2 GHz core ) .
In comparison , Sumblr ( Shou et al. , 2013 ) can process around 30000 tweets during the same interval .
TOWGS requires an average of 0.5 seconds for answering each summarization query .
Regarding memory use , pruning kept its value constant .
In our experiments , the amount of RAM used by the algorithm was between 1.5 - 2 GB .
The code for TOWGS is available online , along with the summaries , keywords , ratings and tweet IDs : https://github.com/ andreiolariu/online-summarizer .
Conclusion Summarizing tweets has been a popular research topic in the past three years .
Yet developing efficient algorithms has proven a challenge , with most work focused on small filtered streams .
This paper introduces TOWGS , a highly efficient algorithm capable of online abstractive microblog summarization .
TOWGS was tested on a seven day 0.1 % sample of the entire Twitter stream .
We asked five judges to rate the summaries it generated , along with those from a baseline algorithm ( MSC ) .
After aggregating the results , the summaries generated by TOWGS proved to have a higher quality , despite the fact that MSC processed just the batches of event-filtered tweets .
We also highlighted the state - of - the - art time efficiency of our approach . ) repeated word pen(n ) = 1 p ( w o ) ( 2e ) where W w [ w ] is the weight for word w , W b [ m ] is the weight for the bigram represented by node m , W t [ e ] is the weight for the trigram represented by edge e and W d [ w , w o ] is the weight for the cooccurrences of words w and w o in the same tweets .
1 p ( w o ) is the indicator function .
