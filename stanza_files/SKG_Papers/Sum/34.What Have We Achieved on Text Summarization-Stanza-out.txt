title
What Have We Achieved on Text Summarization ?
abstract
Deep learning has led to significant improvement in text summarization with various methods investigated and improved ROUGE scores reported over the years .
However , gaps still exist between summaries produced by automatic summarizers and human professionals .
Aiming to gain more understanding of summarization systems with respect to their strengths and limits on a fine-grained syntactic and semantic level , we consult the Multidimensional Quality Metric 1 ( MQM ) and quantify 8 major sources of errors on 10 representative summarization models manually .
Primarily , we find that 1 ) under similar settings , extractive summarizers are in general better than their abstractive counterparts thanks to strength in faithfulness and factual - consistency ; 2 ) milestone techniques such as copy , coverage and hybrid extractive / abstractive methods do bring specific improvements but also demonstrate limitations ; 3 ) pre-training techniques , and in particular sequence - to-sequence pre-training , are highly effective for improving text summarization , with BART giving the best results .
* Equal contribution .
?
Corresponding author .
1 MQM is a framework for declaring and describing human writing quality which stipulates a hierarchical listing of error types restricted to human writing and translation .
Introduction Automatic text summarization has received constant research attention due to its practical importance .
Existing methods can be categorized into extractive ( Dorr et al. , 2003 ; Mihalcea and Tarau , 2004 ; Nallapati et al. , 2017 ) and abstractive ( Jing and McKeown , 2000 ; Rush et al. , 2015 ; See et al. , 2017 ) methods , with the former directly selecting phrases and sentences from the original text as summaries , and the latter synthesizing an abridgment by using vocabulary words .
Thanks to the resurgence of deep learning , neural architectures have been investigated for both extractive ( Cheng and Lapata , 2016 ; Xu and Durrett , 2019 ) and abstractive ( Nallapati et al. , 2016 ; Lewis et al. , 2019 ; Balachandran et al. , 2020 ) summarization systems .
Although improved ROUGE scores have been reported on standard benchmarks such as Gigaword ( Graff et al. , 2003 ) , NYT ( Grusky et al. , 2018 ) and CNN / DM ( Hermann et al. , 2015 ) over the years , it is commonly accepted that the quality of machine - generated summaries still falls far behind human written ones .
As a part of the reason , ROUGE has been shown insufficient as a precise indicator on summarization quality evaluation ( Liu and Liu , 2008 ; B?hm et al. , 2019 ) .
In the research literature , human evaluation has been conducted as a complement ( Narayan et al. , 2018 ) .
However , human evaluation reports that accompany ROUGE scores are limited in scope and coverage .
On a fine- grained level , it still remains uncertain what we have achieved overall and what fundamental changes each milestone technique has brought .
We aim to address the above issues by quantifying the primary sources of errors over representative models .
In particular , following MQM ( Mariana , 2014 ) , we design 8 metrics on the Accuracy and Fluency aspects .
Models are analyzed by the overall error counts on a test set according to each metric , and therefore our evaluation can be more informative and objective compared with existing manual evaluation reports .
We call this set of metrics PolyTope .
Using PolyTope , we manually evaluate 10 text summarizers including Lead-3 , TextRank ( Mihalcea and Tarau , 2004 ) , Sequenceto-sequence with Attention ( Rush et al. , 2015 ) , SummaRuNNer ( Nallapati et al. , 2017 ) , Point-Generator ( See et al. , 2017 ) , Point-Generator-with -Coverage ( Tu et al. , 2016 ; See et al. , 2017 ) , Bottom - Up ( Gehrmann et al. , 2018 ) , BertSumExt ( Liu and Lapata , 2019 ) , BertSumExtAbs ( Liu and Lapata , 2019 ) and BART ( Lewis et al. , 2019 ) , through which we compare neural structures with traditional preneural ones , and abstractive models with their extractive counterparts , discussing the effectiveness of frequently - used techniques in summarization systems .
Empirically , we find that :
1 . Preneural vs Neural : Traditional rule- based methods are still strong baselines given powerful neural architectures .
2 . Extractive vs Abstractive : Under similar settings , extractive approaches outperform abstractive models in general .
The main shortcoming is unnecessity for extractive models , and omission and intrinsic hallucination for abstractive models .
3 . Milestone Techniques :
Copy works effectively in reproducing details .
It also reduces duplication on the word level but tends to cause redundancy to a certain degree .
Coverage solves repetition errors by a large margin , but shows limits in faithful content generation .
Hybrid extractive / abstractive models reflect the relative strengths and weaknesses of the two methods .
4 . Pre-training :
Pre-training is highly effective for summarization , and even achieves a better content selection capability without copy and coverage mechanisms .
Particularly , joint pre-training combining text understanding and generation gives the most salient advantage , with the BART model achieving by far the state - of - the - art results on both automatic and our human evaluations .
We release the test set , which includes 10 system outputs and their manually - labeled errors based on PolyTope , and a user-friendly evaluation toolkit to help future research both on evaluation methods and automatic summarization systems 2 .
Related Work Extractive Summarization
Early efforts based on statistical methods ( Neto et al. , 2002 ; Mihalcea and Tarau , 2004 ) make use of expertise knowledge to manually design features or rules .
Recent work based on neural architectures considers summarization as a word or sentence level classification problem and addresses it by calculating sentence representations ( Cheng and Lapata , 2016 ; Nallapati 2 https://github.com/hddbang/PolyTope
Xu and Durrett , 2019 ) .
Most recently , Zhong et al. ( 2020 ) adopts document - level features to rerank extractive summaries .
Abstractive Summarization Jing and McKeown ( 2000 ) presented a cut-paste based abstractive summarizer , which edited and merged extracted snippets into coherent sentences .
Rush et al. ( 2015 ) proposed a sequence - to-sequence architecture for abstractive summarization .
Subsequently , Transformer was used and outperformed traditional abstractive summarizer by ROUGE scores ( Duan et al. , 2019 ) .
Techniques such as AMR parsing ( Liu et al. , 2015 ) , copy , coverage ( Tu et al. , 2016 ; See et al. , 2017 ) , smoothing ( M?ller et al. , 2019 ) and pre-training ( Lewis et al. , 2019 ; Liu and Lapata , 2019 ) were also examined to enhance summarization .
Hybrid abstractive and extractive methods adopt a two-step approach including content selection and text generation ( Gehrmann et al. , 2018 ; Hsu et al. , 2018 ; Celikyilmaz et al. , 2018 ) , achieving higher performance than end-to - end models in ROUGE .
Analysis of Summarization
There has been much work analyzing summarization systems based on ROUGE .
Lapata and Barzilay ( 2005 ) explored the fundamental aspect of " coherence " in machine generated summaries .
Zhang et al. ( 2018 ) analyzed abstractive systems , while Kedzie et al . ( 2018 ) and Zhong et al . ( 2019 ) searched for effective architectures in extractive summarization .
Kryscinski et al. ( 2019 ) evaluated the overall quality of summarization in terms of redundancy , relevance and informativeness .
All the above rely on automatic evaluation metrics .
Our work is in line with these efforts in that we conduct a fine-grained evaluation on various aspects .
Different from the above work , we use human evaluation instead of automatic evaluation .
In fact , while yielding rich conclusions , the above analytical work has also exposed deficiencies of automatic toolkits .
The quality of automatic evaluation is often criticized by the research community ( Novikova et al. , 2017 ; Zopf , 2018 ) for its insufficiency in neither permeating into the overall quality of generation - based texts ( Liu and Liu , 2008 ) nor correlating with human judgements ( Kryscinski et al. , 2019 ) .
There has also been analysis work augmenting ROUGE with human evaluation ( Narayan et al. , 2018 ; Liu and Lapata , 2019 ) ically consist of 2 to 3 aspects such as informativeness , fluency and succinctness .
Recently , Maynez et al. ( 2020 ) conducted a human evaluation of 5 neural abstractive models on 500 articles .
Their main goal is to verify the faithfulness and factuality in abstractive models .
In contrast , we evaluate both rule- based baselines and extractive / abstractive summarizers on 8 error metrics , among which faithfulness and factuality are included .
Our work is also related to research on human evaluation for summarization .
To this end , Pyramid ( Nenkova and Passonneau , 2004 ) scores a summarizer based on its system output and multiple references .
Annotators are requested to identify the smallest content units of semantic meaning , and then associate each unit with a weight by counting how many reference summaries contain this unit .
The score of a summary is computed according to the number and weight of units .
In addition to Pyramid , there are human evaluation metrics based on ranking ( Narayan et al. , 2018 ) , best-worst scaling ( Kiritchenko and Mohammad , 2017 ) and question answering ( Clarke and Lapata , 2010 ) .
The above methods assign one score to each summarization output .
In contrast to these methods , our errorcount based metrics are motivated by MQM for human writing , and are more fine-grained and informative .
We show more empirical contrast between evaluation metrics in Figure 3 in Section 6 .
Most recently , Stiennon et al . ( 2020 ) uses human evaluation as a reward for training automatic summarizers , reporting significant improvement compared with models trained using reference summaries .
Their work also demonstrates the usefulness of human evaluation in text summarization .
Models
We re-implement and evaluate 10 representative and influential methods .
Their publicly reported ROUGE F1 scores are illustrated in Table 1 .
Extractive Methods Lead-3 Lead - 3 is a commonly - used baseline , which simply selects the first three sentences as the summary .
It is used as a standard baseline by most recent work ( Cheng and Lapata , 2016 ; Gehrmann et al. , 2018 ) .
Intuitively , the first three sentences of an article in news domain can likely be its abstract , so the results of Lead - 3 can be a highly faithful approximation of human-written summary .
TextRank TextRank ( Mihalcea and Tarau , 2004 ) is an unsupervised key text units selection method based on graph - based ranking models ( Page et al. , 1998 ) .
It defines " recommendation " by calculating co-similarity between sentences and yielding a weighted graph accordingly .
Sentences with high weights are extracted as summaries .
It is selected as a representative of statistical models .
SummaRuNNer SummaRuNNer ( Nallapati et al. , 2017 ) is a representative neural extractive model which selects full sentences from the input as a summary .
It first encodes the input with a hierarchical BiGRU , then scans input sentences from left to right .
An accumulated summary representation is generated by a weighted sum of all previous selections , which is fed into a logistic classifier to make the final prediction on summary .
BertSumExt BertSumExt ( Liu and Lapata , 2019 ) takes pre-trained BERT ( Devlin et al. , 2019 ) as the sentence encoder and an additional Transformer as the document encoder .
A classifier on sentence representation is used for sentence selection .
It takes advantages of knowledge from finetuned BERT for generating better summaries .
Abstractive Methods Seq2Seq with Attention
The sequence - tosequence model structure is first used for abstractive summarization by Rush et al . ( 2015 ) .
To allow effective and free text generation rather than simple selection and rearrangement , a target - to -source attention module is adopted to capture the information from every encoder hidden state .
We follow the implementation of See et al . ( 2017 ) . Pointer-Generator See et al. ( 2017 ) introduces the pointer network ( Vinyals et al. , 2015 ) to address the problem that seq2seq models tend to reproduce factual details inaccurately .
The method can both generate words from the vocabulary via a generator , and copy content from the source via a pointer .
Pointer-Generator-with -Coverage See et al. ( 2017 ) use the coverage mechanism ( Tu et al. , 2016 ) to avoid repetition problems .
This mechanism calculates a coverage vector as an extra input for the attention mechanism to strengthen attention to different locations .
Gehrmann et al. ( 2018 ) propose a two-step approach , first selecting potential output words and then generating a summary based on pointer - generator network .
Bottom - Up is selected as a representative of hybrid models which integrate extractive and abstractive methods .
Bottom - Up BertSumExtAbs BertSumExtAbs ( Liu and Lapata , 2019 ) adopts the same encoder as BertSumExt , and a 6 - layer Transformer decoder with randomly initialized parameters .
It is selected as a representative of neural abstractive models with pretrained contextualized sentence representation .
BART Instead of pre-training the encoder only , BART ( Lewis et al. , 2019 ) jointly pre-trains a seq2seq model combining a bidirectional encoder and an auto-regressive decoder .
Further fine-tuned on summarization datasets , it achieves the current state - of - the - art result in terms of ROUGE scores .
Evaluation Method
We analyze system performance by using ROUGE ( Lin , 2004 ) for automatic scoring and PolyTope for human scoring .
ROUGE has been adopted by most work on summarization .
It is a recall - based metric calculating lexical overlap between system output and human summaries .
Particularly , ROUGE - 1 is based on unigram overlaps , ROUGE - 2 on bigrams and ROUGE -L on longest common subsequences .
PolyTope is an error-oriented fine - grained human evaluation method based on Multidimensional Quality Metric ( MQM ) ( Mariana , 2014 ) .
In particular , it consists of 8 issue types ( Section 4.1 ) , 8 syntactic labels ( Section 4.2 ) and a set of severity rules ( Section 4.3 ) to locate errors and to automatically calculate an overall score for the tested document .
As illustrated in Figure 3 , compared with ROUGE , PolyTope is more fine- grained in offering detailed and diagnostic aspects of overall quality .
We develop an operating interface for annotation , which is shown in Appendix A.1 .
Particularly , a human annotator is presented the original text and an output summary in juxtaposition , and is asked to select segments that are deemed incorrect after reading .
Upon a preliminary selection , he is asked to make a further selection among 8 issue types and 8 syntactic labels , respectively .
An embedded severity score is then generated automatically for every incorrect segment , and the quality score is calculated for the annotated summary as : Score = ( 1 ? ?I ? * Severity ? word count ) * 100 , where I ? { MINOR , MAJOR , CRITICAL } , indicating the error count for each severity .
Severity scores are deducted for errors of different severity , with the deduction ratio being set as 1:5:10 for MINOR , MAJOR and CRITICAL , respectively .
word count is the total number of words in samples .
For a skilled annotator , it takes 2.5 - 4 minutes averagely to complete annotation of one sample , of which 2 - 3 minutes are used for extensive reading and 0.5 - 1 minutes for annotation .
After PolyTope evaluation , 3 - dimensional error points show the overall quality of the tested model ( Figure 1 ) .
be traced to the MQM principle .
Accuracy - related issues refer to the extent to which the content conveyed by the target summarization does not match or accurately reflect the source text .
It comprises five sub-types : Addition Unnecessary and irrelevant snippets from the source are included in the summary .
Omission
Key point is missing from the output .
Inaccuracy Intrinsic
Terms or concepts from the source are misrepresented and thus unfaithful .
Inaccuracy Extrinsic
The summary has content not presented in the source and factually incorrect .
Positive -Negative Aspect
The output summary represents positive statements whereas the source segment is negative , and vice versa .
Fluency issues refer to linguistic qualities of the text .
Unlike Accuracy , Fluency is independent of the relationship between the source and the target .
It comprises three sub-types : Duplication
A word or longer portion of the text is repeated unnecessarily .
Word Form Problems in the form of a word , including agreement , POS , tense-mood- aspect , etc .
Word Order Problems in the order of syntactic constituents of a sentence .
Their examples are elaborated in Appendix A.2 .
Syntactic Label Syntactic labels aim to locate errors , allowing tighter relevance between error issues and sentence constituents .
According to ACE2005 ( Automatic Content Extraction ) , we define 8 syntactic labels to distinguish sentence components , namely Subject , Predicate , Object , Number&Time , Place &Name , Attribute , Function Word and Whole Sentence .
Their definitions are elaborated in Appendix A.3 .
Severity Severity is an indication of how severe a particular error is .
It has three levels : MINOR , MAJOR and CRITICAL , calculated by the evaluation tool automatically given the human decision on the error type and syntactic label .
In practice , each cell in Table 2 corresponds to a specific severity level .
Issues with higher severity have more impact on perceived quality of the summary .
Minor Issues that do not impact usability or understandability of the content .
For example , if grammar function word repeats itself , the redundant preposition is considered an error but does not render the text difficult to use or problematic .
Major Issues that impact usability or understandability of the content but do not render it unusable .
For example , an additional attribute may result in extra effort for the reader to understand the intended meaning , but does not make the content unfit for purpose .
Critical Issues that render the content unfit for use .
For example , an omitted subject that changes the meaning of the text would be considered critical .
If the error prevents the reader from using the content as intended or if it presents incorrect information that could result in harm to the user , it must be categorized as critical .
In general , even a single critical error is likely to cause serious problems .
Evaluating Model Performance
We evaluate the aforementioned 10 models using the above two metrics , focusing on comparisons between pre-neural and neural methods , extractive and abstractive methods , and better understanding the effects of milestone techniques such as copy , coverage , pre-training and hybrid abstractive / extractive models .
We randomly sample 150 trials from the non-anonymized CNN / DM dataset ( Hermann et al. , 2015 ) .
When predicting summaries , we select three sentences as the summary for extractive models following the original papers , and let the algorithms self-stop for abstractive models , which also give three sentences as the decoding result in most cases .
Table 3 presents the performances based on PolyTope and ROUGE .
Cases supporting observations below are illustrated in Appendix C.
Preneural vs Neural Models
On ROUGE -1 , Lead - 3 ranks the 2nd among extractive models , and the 4th among all models .
On PolyTope , it ranks the 3rd among extractive models and the 4th among all models .
This shows that the simple method stands as a strong baseline even among neural methods .
TextRank ranks the 9th and 7th among all methods on ROUGE and PolyTope , respectively , still competitive to some abstractive neural models .
On the negative side , these two methods show the largest numbers of Addition errors , which demonstrates that unsupervised methods are relatively weaker in filtering out useless information compared to the supervised methods .
Extractive vs Abstractive Summarization
On ROUGE , there is no strong gap between extractive and abstractive methods , with BART and Bert - SumExt being the top abstractive and extractive models , respectively .
On PolyTope , as a representative of abstractive models , BART overwhelmingly outperforms the others ( p < 0.01 using t-test ) .
However , excluding BART , extractive models take the following top three places .
Under similar settings , extractive methods are better ( p < 0.01 using t-test ) compared with abstractive counterparts ( e.g. Bert-SumExt vs BertSumExtAbs , SummaRuNNer vs Point-Generator , Point-Generator -with -Coverage ) .
Extractive models tend to make only 3 types of errors , namely Addition , Omission , Duplication , while abstractive models make 4 to 7 types of errors .
With respect to Accuracy , extractive methods are notably stronger in terms of Inacc Intrinsic and Extrinsic , which reflects that through directly copying snippets from the source , extractive methods are guaranteed to produce a summary with fair grammaticality , rationality and loyalty .
However , extractive methods do not show stronger performances in Addition and Omission , which is because extracted sentences contain information not directly relevant to the main points .
With regard to Fluency , two approaches are generally competitive with each other , showing that nowadays neural models are relatively effective in synthesizing coherent summaries .
Extractive Methods
We first compare neural methods BertSumExt and SummaRuNNer .
BertSumExt gives better ROUGE - 1/2 compared to SummaRuNNer , but the difference is not significant under ROUGE -L or PolyTope .
Among detailed errors , BertSumExt demonstrates advantages only in Duplication , for the likely reason that the contextualized representations of the same phrases can be different by BERT encoding .
It co-insides with previous findings ( Kedzie et al. , 2018 ) which demonstrate that more complicated architectures for producing sentence representations do not lead to better performance under the setting of extractive summarization .
Given the fact that gold-standard extractive summaries are constructed according to ROUGE , the better ROUGE score of BertSumExt reflects the effectiveness of stronger representation on fitting training data . ( 0,5 ] ( 5,10 ] ( 10,15 ] ( 15,20 ] ( 20,25 ] ( 25,30 ] ( 30,35 ] ( 35,40 ] ( 40,45
We then take statistical models into account .
Figure 2a shows the distribution of source sentences used for content generation by each method .
There is a high proportion in the first five sentences and a smooth tail over all positions for reference summaries .
In contrast , BertSumExt and SummaRuN - Ner extract sentences mostly from the beginning , thereby missing useful information towards the end .
TextRank improves the coverage slightly as it is graph - based and does not depend on sequence information .
But as lack of supervision , the model has a large number of Addition and Omission .
Abstractive Methods Copy
The na?ve seq2seq model suffers an Inacc-Intrinsic count of 304 , the worst among all models compared .
In contrast , the Point- Generator model reduces the error count to 14 , demonstrating the effectiveness of the copy mechanism in faithfully reproducing details .
Another interesting finding is that Duplication errors are also sharply reduced from 139 to 68 , although the copy mechanism is not explicitly designed to address this problem .
Further investigation shows that the reduced duplication patterns are mostly on the word level , while the effect on sentence - level duplication reduction is nearly zero .
One likely reason is that the seq2seq decoder relies heavily on short- term history when deciding the next output word , without effective use of long-term dependencies .
The Point- Generator model solves this problem by interpolating vocabulary level probability with copy probability , reducing reliance on previous outputs .
On the negative side , the copy mechanism introduces Addition errors , because the auto-regressive point generator network tends to copy long sequences in entirety from the source , failing to interrupt copying at desirable length .
This is also observed by Gehrmann et al . ( 2018 ) and Balachandran et al . ( 2020 ) . Coverage Coverage ( Tu et al. , 2016 ) is introduced to neural summarization systems to solve repetition issues .
Compared with Point-Generator , Point-Generator- with -Coverage reduces Duplication errors from 68 to 11 and Omission errors from 286 to 256 , proving that coverage is useful for better content selection .
However , Point-Generator - with -Coverage yields more Addition and Inacc Intrinsic errors than Point-Generator .
We further extract outputs of Point- Generator that do not have Duplication errors , finding that introducing the coverage mechanism reduces the average Poly - Tope scores from 77.54 to 74.07 .
It indicates that the coverage mechanism lacks inference capability and tends to generate summaries that incorrectly combine contents from the source into irrelevant information ( see Figure10 and Figure11 in Appendix C as examples ) .
This is likely because the coverage mechanism forces attention values from the decoder to the encoder to move monotonically to the right , and therefore can interfere with the original content selection process .
Hybrid Abstractive / Extractive Model Bottom - Up gives high ROUGE scores , but ranks the second worst on PolyTope .
Compared with others , it suffers more from Inaccuracy errors .
The inconsistency between ROUGE and PolyTope reflects the relative strengths and weaknesses of this method .
On the positive side , it combines the advantages of extractive and abstractive models in selecting segments from the source and generating new contents in the summary , leading to a better recall .
On the negative side , the abstractive generation model constrains copy attention only on the extracted snippets , thereby suffering from incomplete information sources for making inference and consequently lack of faithfulness and factual consistency .
Pre-training Both BertSumExtAbs and BART outperform the non-pretraining abstractive models by a large margin .
They differ from the other methods in two aspects , namely the Transformer ar-Source Document : A quokka was the innocent victim of a cruel act by two French tourists who tried to set the Australian animal alight .
The two men allegedly ignited an aerosol spray with a lighter causing a large flame to make contact with a quokka on Rottnest island off Perth in western Australia on April 3 .
The lucky little critter survived the reckless incident but was singed by the flame .
Two French male tourists allegedly ignited an aerosol spray with a lighter causing a large flame to make contact with a quokka on Rottnest island off Perth in western Australia on April 3 .
Detectives went to Rottnest island on Saturday and questioned the two men and also seized video evidence of the careless act .
Both men aged 18 and 24 , and both currently living in Cockburn central , were evicted from the island .
They have each been charged with animal cruelty and will appear in Fremantle magistrates court on April 17 .
Quokkas can be found on some small islands off the west Australian coast , in particular off Rottnest island off Perth and Bald island near Albany .
The lucky little critter survived the reckless incident but was singed by the flame and the men were charged .
Reference : Two French tourists allegedly ignited aerosol spray with a lighter and singed the animal .
The lucky little critter survived the reckless incident but was singed by the flame .
Both have been charged for animal cruelty and will appear in court on April 17 .
Detectives allegedly ignited an aerosol spray with a lighter causing a large flame to make contact with a quokka on Rottnest island off Perth in western Australia on April 3 .
Survived the reckless incident but was singed by the flame .
Detectives male tourists allegedly ignited an aerosol spray with a lighter causing a large flame to make contact with a quokka on Rottnest island off Perth in western Australia on April 3 . Two French tourists allegedly ignited an aerosol spray with a lighter causing a large flame to make contact with a quokka on Rottnest island off Perth in western Australia on April 3 .
They have each been charged with animal cruelty and will appear in Fremantle magistrates court on April 3 .
Detectives went to Rottnest island on Saturday and questioned the two men and seized video evidence of the careless act .
It is worth noting that BART ranks the 1st on both ROUGE and PolyTope among the 10 models .
Different from BertSumExtAbs which pretrains the encoder only , BART pre-trains the encoder and decoder jointly with seq2seq denoising auto-encoder tasks .
It gives large improvements on Addition , Omission and Inacc errors , proving that unified pre-training for both understanding and generation is highly useful for content selection and combination .
In particular , BART shows superior performance in handling the leading bias of CNN / DM dataset .
Figure 2 b shows the distribution of source sentences used for content generation by the abstractive methods .
As can be seen , abstractive models tend to neglect sentences in the middle and at the end of source documents ( e.g. , Bottom - Up , BertSumExtAbs ) , indicating that performance of abstractive summarizers is strongly affected by the leading bias of dataset .
In contrast , BART can attend to sentences all around the whole document , slightly closer to the distribution of golden reference .
Intuitively , this improvement might result from the document rotation transformation of BART pre-training , which shuffles the sentences on the encoder side for the same decoder .
We leave the verification to future work , which requires re-training of BART without document rotation transformation .
Analysis of Evaluation Methods
The main goal of this paper is to investigate the differences between summarization systems , rather than to promote a human evaluation metric .
Nonetheless , our dataset gives us a testbed to calculate the correlation between automatic and human evaluation methods .
In this section , we report a contrast between ROUGE and PolyTope quantitatively , and between PolyTope and other human evaluation metrics qualitatively to demonstrate why we used PolyTope for our research goal .
First , research has shown that ROUGE is inconsistent with human evaluation for summary quality ( Liu and Liu , 2008 ; Zopf , 2018 ; Kryscinski et al. , 2019 ; Maynez et al. , 2020 ) .
We evaluate ROUGE using PolyTope from the perspective of both instance -level and system-level performances .
On the instance level , the individual 1500 outputs from the 10 models are adopted to calculate the Pearson correlation coefficients between ROUGE and PolyTope .
Additionally , we select test instances that only make Accuracy or Fluency errors to better understand the correlation between ROUGE and Accuracy / Fluency aspects .
On the system level , the overall scores of each model are adopted to calculate the Pearson correlation coefficients between ROUGE and PolyTope .
The results are summarized in Table 4 .
For the instance - level comparison , we find a weak correlation between ROUGE and human judgement .
In addition , with respect to Accuracy and Fluency , ROUGE can measure Accuracy to a certain extent , and ROUGE - 2 is better than ROUGE - 1/ L in terms of evaluating Fluency .
For the system-level comparison , the Pearson correlation coefficient is 0.78 , 0.73 , 0.52 for ROUGE -1 , ROUGE - 2 , and ROUGE -L , respectively , much higher than 0.40 , 0.32 , 0.32 on the instance level .
This confirms that ROUGE is useful for ranking systems after aggregation of samples but is relatively weak for assessing single summary quality , where the fine grained PolyTope could help ( Peyrard et al. , 2017 ) .
Second , Figure 3 shows results of two models on one test document by ROUGE , Pyramid , ranking , scaling , QA and PolyTope evaluation metrics .
As can be seen from the figure , PolyTope offers more fine- grained information in quality evaluation .
Sun et al. ( 2019 ) warned that human evaluation prefers to give higher scores to longer and more informative summaries .
Under the setting of PolyTope , there was relatively little influence from the sentence length .
Taking BertSumExt and BertSumExtAbs models as examples , the Pearson correlation coefficients between length of their out-puts and the corresponding scores is 0.25 and 0.27 , respectively , suggesting that PolyTope is more objective and meaningful for current models that produce summaries without pre-specified length .
Finally , we also evaluate the reference summaries of our 150 test trials by means of PolyTope , obtaining a general score of 96.41 , with 63 errors in the Accuracy aspect and 0 errors in the Fluency aspect .
Gold summaries did not receive full marks in the PolyTope evaluation , mainly because of hallucinating content .
For example , a news article describes an event as happening " on Wednesday " in a summary although the original document has " on April 1 " .
The human summary requires external knowledge beyond the document and thus suffers penalization .
Another common hallucination involves rhetorical but irrelevant sentences , e.g. , " Click here for more news " .
In addition , there are a few grammatical issues that affect the accuracy .
For example , in " Piglet was born in China with only two front legs has learned to walk . " , there is a missing conjunction between two verb phrases .
Conclusion
We empirically compared 10 representative text summarizers using a fine- grained set of human evaluation metrics designed according to MQM for human writing , aiming to achieve a better understanding on neural text summarization systems and the effect of milestone techniques investigated recently .
Our observations suggest that extractive summarizers generally outperform abstractive summarizers by human evaluation , and more details are also found about the unique advantages gained by copy , coverage , hybrid and especially pre-training technologies .
The overall conclusions are largely in line with existing research , while we provide more details in an error diagnostics aspect .
Score Card
This sheet automatically calculates error numbers and scores ( Figure 4 ) , and demonstrates the whole performance of the tested model .
Error Log
This sheet is the annotation interface designed for annotators ( Figure 5 ) .
It is filled with source articles in column C and output summaries in column D in advance , and allows annotators to select segments that are deemed incorrect in column E to H .
Upon one selection , annotators are asked to make a selection among 8 issue types ( column F ) and 8 syntactic labels ( column G ) .
A severity is then generated automatically in column J , and a quality score is calculated automatically in the Scores per Segment sheet individually and in the Score Card sheet overall .
Scores per Segment
This sheet calculates word count , error count and score for each tested sample ( Figure 6 ) .
Severity Matrix
This sheet is the predefined severity matrix ( Table 2 ) embedded in the Excel workbook by Macros .
