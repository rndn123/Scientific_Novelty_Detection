title
Timeline Generation through Evolutionary Trans-Temporal Summarization
abstract
We investigate an important and challenging problem in summary generation , i.e. , Evolutionary Trans -Temporal Summarization ( ETTS ) , which generates news timelines from massive data on the Internet .
ETTS greatly facilitates fast news browsing and knowledge comprehension , and hence is a necessity .
Given the collection of time -stamped web documents related to the evolving news , ETTS aims to return news evolution along the timeline , consisting of individual but correlated summaries on each date .
Existing summarization algorithms fail to utilize trans-temporal characteristics among these component summaries .
We propose to model trans-temporal correlations among component summaries for timelines , using inter-date and intra-date sentence dependencies , and present a novel combination .
We develop experimental systems to compare 5 rival algorithms on 6 instinctively different datasets which amount to 10251 documents .
Evaluation results in ROUGE metrics indicate the effectiveness of the proposed approach based on trans-temporal information .
Introduction Along with the rapid growth of the World Wide Web , document floods spread throughout the Internet .
Given a large document collection related to a news subject ( for example , BP Oil Spill ) , readers get lost in the sea of articles , feeling confused and powerless .
General search engines can rank these * Corresponding author .
news webpages by relevance to a user specified aspect , i.e. , a query such as " first relief effort for BP Oil Spill " , but search engines are not quite capable of ranking documents given the whole news subject without particular aspects .
Faced with thousands of news documents , people usually have a myriad of interest aspects about the beginning , the development or the latest situation .
However , traditional information retrieval techniques can only rank webpages according to their understanding of relevance , which is obviously insufficient ( Jin et al. , 2010 ) .
Even if the ranked documents could be in a satisfying order to help users understand news evolution , readers prefer to monitor the evolutionary trajectories by simply browsing rather than navigate every document in the overwhelming collection .
Summarization is an ideal solution to provide an abbreviated , informative reorganization for faster and better representation of news documents .
Particularly , a timeline ( see Table 1 ) can summarize evolutionary news as a series of individual but correlated component summaries ( items in Table 1 ) and offer an option to understand the big picture of evolution .
With unique characteristics , summarizing timelines is significantly different from traditional summarization methods which are awkward in such scenarios .
We first study a manual timeline of BP Oil Spill in Mexico Gulf in Table 1 from Reuters News 1 to understand why timelines generation is observably different from traditional summarization .
No traditional method has considered to partition corpus into subsets by timestamps for trans-temporal correlations .
However , we discover two unique trans -
The Deepwater Horizon rig , valued at more than $ 560 million , sinks and a five mile long ( 8 km ) oil slick is seen .
April 25 , 2010
The Coast Guard approves a plan to have remote underwater vehicles activate a blowout preventer and stop leak .
Efforts to activate the blowout preventer fail .
April 28 , 2010
The Coast Guard says the flow of oil is 5,000 barrels per day ( bpd ) ( 210,000 gallons / 795,000 litres ) - five times greater than first estimated .
A controlled burn is held on the giant oil slick .
April 29 , 2010 U.S. President Barack Obama pledges " every single available resource , " including the U.S. military , to contain the spreading spill .
Obama also says BP is responsible for the cleanup .
Louisiana declares state of emergency due to the threat to the state 's natural resources .
April 30 , 2010
An Obama aide says no drilling will be allowed in new areas , as the president had recently proposed , until the cause of the Deepwater Horizon accident is known .
temporal characteristics of component summaries from the handcrafted timeline .
Individuality .
The component summaries are summarized locally : the component item on date t is constituted by sentences with timestamp t. Correlativeness .
The component summaries are correlative across dates , based on the global collection .
To the best of our knowledge , no traditional method has examined the relationships among these timeline items .
Although it is profitable , summarizing timeline faces with new challenges : ?
The first challenge for timeline generation is to deliver important contents and avoid information overlaps among component summaries under the trans-temporal scenario based on global / local source collection .
Component items are individual but not completely isolated due to the dynamic evolution .
?
As we have individuality and correlativeness to evaluate the qualities of component summaries , both locally and globally , the second challenge is to formulate the combination task into a balanced optimization problem to generate the timelines which satisfy both standards with maximum utilities .
We introduce a novel approach for the web mining problem Evolutionary Trans -Temporal Summarization ( ETTS ) .
Taking a collection relevant to a news subject as input , the system automatically outputs a timeline with items of component summaries which represent evolutionary trajectories on specific dates .
We classify sentence relationships as interdate and intra-date dependencies .
Particularly , the inter-date dependency calculation includes temporal decays to project sentences from all dates onto the same time horizon ( Figure 1 ( a ) ) .
Based on intra- / inter-date sentence dependencies , we then model affinity and diversity to compute the saliency score of each sentence and merge local and global rankings into one unified ranking framework .
Finally we select top ranked sentences .
We build an experimental system on 6 real datasets to verify the effectiveness of our methods compared with other 4 rivals .
Related Work Multi-document summarization ( MDS ) aims to produce a summary delivering the majority of information content from a set of documents and has drawn much attention in recent years .
Conferences such as ACL , SIGIR , EMNLP , etc. , have advanced the technology and produced several experimental systems .
Generally speaking , MDS methods can be either extractive or abstractive summarization .
Abstractive summarization ( e.g. NewsBlaster 2 ) usually needs information fusion , sentence compression and reformulation .
We focus on extraction - based methods , which usually involve assigning saliency scores to some units ( e.g. sentences , paragraphs ) of the documents and extracting the units with highest scores .
To date , various extraction - based methods have been proposed for generic multi-document summarization .
The centroid- based method MEAD ) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids , position , and TF.IDF , etc. NeATS ( Lin and Hovy , 2002 ) adds new features such as topic signature and term clustering to select important content , and use MMR ( Goldstein et al. , 1999 ) to remove redundancy .
Graph - based ranking methods have been proposed to rank sentences / passages based on " votes " or " recommendations " between each other .
Tex -tRank ( Mihalcea and Tarau , 2005 ) and LexPageRank ( Erkan and Radev , 2004 ) use algorithms similar to PageRank and HITS to compute sentence importance .
Wan et al .
have improved the graph-ranking algorithm by differentiating intra-document and inter-document links between sentences ( 2007 b ) , and have proposed a manifold - ranking method to utilize sentence - to-sentence and sentence - to- topic relationships ( Wan et al. , 2007a ) .
ETTS seems to be related to a very recent task of " update summarization " started in DUC 2007 and continuing with TAC .
However , update summarization only dealt with a single update and we make a novel contribution with multi-step evolutionary updates .
Further related work includes similar timeline systems proposed by ( Swan and Allan , 2000 ) using named entities , by ( Allan et al. , 2001 ) measured in usefulness and novelty , and by ( Chieu and Lee , 2004 ) measured in interest and burstiness .
We have proposed a timeline algorithm named " Evolutionary Timeline Summarization ( ETS ) " in ( Yan et al. , 2011 b ) but the refining process based on generated component summaries is time consuming .
We aim to seek for more efficient summarizing approach .
To the best of our knowledge , neither update summarization nor traditional systems have considered the relationship among " component summaries " , or have utilized trans-temporal properties .
ETTS approach can also naturally and simultaneously take into account global / local summarization with biased information richness and information novelty , and combine both summarization in optimization .
Trans-temporal Summarization
We conduct trans-temporal summarization based on the global biased graph using inter-date dependency and local biased graph using intra-date dependency .
Each graph is the complementary graph to the other .
Global Biased Summarization
The intuition for global biased summarization is that the selected summary should be correlative with sentences from neighboring dates , especially with those informative ones .
To generate the component summary on date t , we project all sentences in the collection onto the time horizon of t to construct a global affinity graph , using temporal decaying kernels .
Temporal Proximity Based Projection Clearly , a major technical challenge in ETTS is how to define the temporal biased projection function ?( ? t ) , where ?t is the distance between the 1 . Gaussian kernel ?( ?t ) = exp [ ?t 2 2 ?
2 ] 2 . Triangle kernel ?(?t ) = 1 ? ?t ? if ?t ? ? 0 otherwise 3 . Cosine ( Hamming ) kernel ?(?t ) = 1 2 [ 1 + cos ( ?t? ? ) ] if ?t ? ?
0 otherwise 4 . Circle kernel ?(?t ) = 1 ? ( ?t ? ) 2 if ?t ? ? 0 otherwise 435 5 . Window kernel ?( ?t ) = 1 if ?t ? ?
0 otherwise
All kernels have one parameter ? to tune , which controls the spread of kernel curves , i.e. , it restricts the projection scope of each sentence .
In general , the optimal setting of ?
may vary according to the news set because sentences presumably would have wider semantic scope in certain news subjects , thus requiring a higher value of ? and vice versa .
Modeling Global Affinity
Given the sentence collection C partitioned by the timestamp set T , C = { C 1 , C 2 , . . . , C |T | } , we ob- tain C t = {s t i | 1 ? i ?
| C t | } where s i is a sentence with the timestamp t = t s i .
When we generate component summary on t , we project all sentences onto time horizon t.
After projection , all sentences are weighted by their influence on t .
We use an affinity matrix M t with the entry of the inter-date transition probability on date t.
The sum of each row equals to 1 .
Note that for the global biased matrix , we measure the affinity between local sentences from t and global sentences from other dates .
Therefore , intradate transition probability between sentences with the timestamp t is set to 0 for local summarization .
M t i , j is the transition probability of s i to s j based on the perspective of date t , i.e. , p( s i ? s j | t ) : p( s i ? s j | t ) = f ( s i ?s j | t ) | C | f ( s i ?s k | t ) if f = 0 0 if t s i = t s j = t ( 1 ) f ( s i ? s j |t ) is defined as the temporal weighted cosine similarity between two sentences : f ( s i ? s j | t ) = w?s i ?s j ?( w , s i |t ) ? ?( w , s j | t ) ( 2 ) where the weight ?
associated with term w is calculated with the temporal weighted tf.isf formula : ?( w , s|t ) = ?|t ? t s | ? tf ( w , s ) ( 1 + log ( | C| Nw ) ) | s | ( tf ( w , s ) ( 1 + log ( | C| Nw ) ) ) 2 . ( 3 ) where t s is the timestamp of sentence s , and tf ( w , s ) is the term frequency of w in s. t s can be any date from T . | C | is the sentences set size and N w is the number of sentences containing term w .
We let p( s i ? s i | t ) =0 to avoid self transition .
Note that although f ( . ) is a symmetric function , p( s i ? s j | t ) is usually not equal to p(s j ? s i | t ) , depending on the degrees of nodes s i and s j .
Now we establish the affinity matrix M t i , j and by using the general form of PageRank , we obtain : ? = ?M ?1 ? + 1 ? ? | C| e ( 4 ) where ? is the selective probability of all sentence nodes and e is a column vector with all elements equaling to 1 . ? is the damping factor set as 0.85 .
Usually the convergence of the iteration algorithm is achieved when difference between the scores computed at two successive iterations for any sentences falls below a given threshold ( 0.0001 in this study ) .
Modeling Diversity
Diversity is to reflect both biased information richness and sentence novelty , which aims to reduce information redundancy .
However , using standard PageRank of Equation ( 4 ) will not result in diversity .
The aggregational effect of PageRank assigns high salient scores to closely connected node communities ( Figure 3 ( b ) ) .
A greedy vertex selection algorithm may achieve diversity by iteratively selecting the most prestigious vertex and then penalizing the vertices " covered " by the already selected ones , such as Maximum Marginal Relevance and its applications in Wan et al . ( 2007 b ; 2007 a ) .
Most recently diversity rank DivRank is another solution to diversity penalization in ( Mei et al. , 2010 ) .
We incorporate DivRank in our general ranking framework , which creates a dynamic M during each iteration , rather than a static one .
After z times of iteration , the matrix M becomes : M ( z ) = ?M ( z?1 ) ? ? ( z?1 ) + 1 ? ? | C| e ( 5 ) Equation ( 5 ) raises the probability for nodes with higher centrality and nodes already having high weights are likely to " absorb " the weights of its neighbors directly , and the weights of neighbors ' neighbors indirectly .
The process is to iteratively adjust matrix M according to ? and then to update ?
according to the changed M .
As iteration increases there emerges a rich- gets - richer phenomenon ( Figure 3 ( c ) and ( d ) ) .
By incorporating DivRank , we obtain rank r ?
i and the global biased ranking score G i for sentence s i from date t to summarize C t .
Local Biased Summarization
Naturally , the component summary for date t should be informative within C t .
Given the sentence collection C t = {s t i | 1 ? i ?
| C t |} , we build an affinity matrix for Figure 1 ( b ) , with the entry of intradate transition probability calculated from standard cosine similarity .
We incorporate DivRank within local summarization and we obtain the local biased rank and ranking score for s i , denoted as r ? i and L i .
Optimization of Global / Local Combination
We do not directly add the global biased ranking score and local biased ranking score , as many previous works did ( Wan et al. , 2007 b ; Wan et al. , 2007a ) , because even the same ranking score gap may indicate different rank gaps in two ranking lists .
Given subset C t , let R = {r i }( i = 1 , . . . ,| C t | ) , r i is the final ranking of s i to estimate , optimize the following objective cost function O ( R ) , O( R ) =?
| C t | i=1 G i r i ? i ? r ? i G i 2 + ? | C t | i=1 L i r i ? i ? r ? i L i 2 ( 6 ) where G i is the global biased ranking score while L i is the local biased ranking score .
?
i is expected to be the merged ranking score , namely sentence importance , which will be defined later .
Among the two components in the objective function , the first component means that the refined rank should not deviate too much from the global biased rank .
We use r i ?
i ? r ?
i G i 2 instead of r i ?
r ?
i 2 in order to distinguish the differences between sentences from the same rank gap .
The second component is similar by refining rank from local biased summarization .
Our goal is to find R = R * to minimize the cost function , i.e. , R * = argmin { O ( R ) } .
R * is the final rank merged by our algorithm .
To minimize O( R ) , we compute its first-order partial derivatives .
?O( R ) ?r i = 2 ? ? i ( G i ? i r i ? r ? i ) + 2 ? ? i ( L i ? i r i ? r ? i ) ( 7 ) Let ?O ( R ) ?r i = 0 , we get r * i = ?
i r ? i + ? i r ? i ?G i + ?L i ( 8 ) Two special cases are that if ( 1 ) ? = 0 , ? = 0 : we obtain r i = ?
i r ?
i / L i , indicating we only use the local ranking score .
( 2 ) ? = 0 , ? = 0 , indicating we ignore local ranking score and only consider global biased summarization using inter-date dependency .
There can be many ways to calculate the sentence importance ?
i .
Here we define ?
i as the weighted combination of itself with ranking scores from global biased and local biased summarization : ? ( z ) i = ?G i + ?L i + ? ( z?1 ) i ? + ? + ? . ( 9 ) To save one parameter we let ?+? +? = 1 . In the zth iteration , r i is dependent on ? ( z?1 ) i and ? ( z ) i is indirectly dependent on r ( z ) i via ? ( z?1 ) i . ? ( 0 ) i = 0 .
We iteratively approximate final ?
i for the ultimate rank list R * .
The expectation of stable ?
i is obtained when ?
( z ) i = ? ( z?1 ) i . Final ?
i is expected to satisfy ?
i = ?G i + ?L i + ? i : ? i = ?G i + ?L i 1 ? ? = ?G i + ?L i ? + ? ( 10 ) Final ?
i is dependent only on original global / local biased ranking scores .
Equation ( 8 ) becomes more concise with no ? or ? : r * is a weighted combination of global and local ranks by ? ? (? = 0 , ? = 0 ) : r * i = ? ? + ? r ? i + ? ? + ? r ? i = 1 1 + ?/? r ? i + 1 1 + ?/? r ? i ( 11 ) 4 Experiments and Evaluation
Datasets
There is no existing standard test set for ETTS methods .
We randomly choose 6 news subjects with special coverage and handcrafted timelines by editors from 10 selected news websites : these 6 test sets consist of news datasets and golden standards to evaluate our proposed framework empirically , which amount to 10251 news articles .
As shown in Table 2 , three of the sources are in UK , one of them is in China and the rest are in the US .
We choose these sites because many of them provide timelines edited by professional editors , which serve as reference summaries .
The news belongs to different categories of Rule of Interpretation ( ROI ) ( Kumaran and Allan , 2004 ) .
More detailed statistics are in Table 3 .
Experimental System Setups ?
Preprocessing .
As ETTS faces with much larger corpus compared with traditional MDS , we apply further data preprocessing besides stemming and stop-word removal .
We extract text snippets representing atomic " events " from all documents with a toolkit provided by Yan et al . ( 2010 ; 2011a ) , by which we attempt to assign more fine- grained and accurate timestamps for every sentence within the text snippets .
After the snippet extraction procedure , we filter the corpora by discarding non-event texts .
? Compression Rate and Date Selection .
After preprocessing , we obtain numerous snippets with fine- grained timestamps , and then decompose them into temporally tagged sentences as the global collection C .
We partition C according to timestamps of sentences , i.e. , C = C 1 ? C 2 ? ? ? ? ? C |T | .
Each component summary is generated from its corresponding sub-collection .
The sizes of component summaries are not necessarily equal , and moreover , not all dates may be represented , so date selection is also important .
We apply a simple mechanism that users specify the overall compression rate ? , and we extract more sentences for important dates while fewer sentences for others .
The importance of dates is measured by the burstiness , which indicates probable significant occurrences ( Chieu and Lee , 2004 ) .
The compression rate on t i is set as ? i = | C i | | C| .
Evaluation Metrics
The ROUGE measure is widely used for evaluation ( Lin and Hovy , 2003 ) : the DUC contests usually officially employ ROUGE for automatic summarization evaluation .
In ROUGE evaluation , the summarization quality is measured by counting the number of overlapping units , such as N-gram , word sequences , and word pairs between the candidate timelines CT and the reference timelines RT .
There are several kinds of ROUGE metrics , of which the most important one is ROUGE -N with 3 sub-metrics : 1 ROUGE -N-R is an N-gram recall metric : ROUGE -N-R = I?RT N-gram ?
I Count match ( N- gram ) I?RT N-gram ? I Count ( N- gram ) 2 ROUGE -N-P is an N-gram precision metric : ROUGE -N-P = I?CT N-gram ?
I Count match ( N- gram ) I?CT N-gram ? I Count ( N- gram ) 3 ROUGE -N-F is an N-gram F 1 metric : ROUGE -N-F = 2 ? ROUGE-N-P ? ROUGE-N-R ROUGE -N-P + ROUGE -N-R
I denotes a timeline .
N in these metrics stands for the length of N-gram and N-gram ?
RT denotes the N-grams in reference timelines while N-gram ?
CT denotes the N-grams in the candidate timeline .
Count match ( N- gram ) is the maximum number of Ngram in the candidate timeline and in the set of reference timelines .
Count ( N- gram ) is the number of Ngrams in reference timelines or candidate timelines .
According to ( Lin and Hovy , 2003 ) , among all sub-metrics , unigram- based ROUGE ( ROUGE - 1 ) has been shown to agree with human judgment most and bigram- based ROUGE ( ROUGE - 2 ) fits summarization well .
We report three ROUGE F-measure scores : ROUGE -1 , ROUGE - 2 , and ROUGE -W , where ROUGE - W is based on the weighted longest common subsequence .
The weight W is set to be 1.2 in our experiments by ROUGE package ( version 1.55 ) .
Intuitively , the higher the ROUGE scores , the similar the two summaries are .
Algorithms for Comparison
We implement the following widely used summarization algorithms as baseline systems .
They are designed for traditional summarization without trans-temporal dimension .
The first intuitive way to generate timelines by these methods is via a global summarization on collection C and then distribution of selected sentences to their source dates .
The other one is via an equal summarization on all local sub-collections .
For baselines , we average both intuitions as their performance scores .
For fairness we conduct the same preprocessing for all baselines .
Random :
The method selects sentences randomly for each document collection .
Centroid :
The method applies MEAD algorithm to extract sentences according to the following three parameters : centroid value , positional value , and first-sentence overlap .
GMDS : The graph- based MDS proposed by ( Wan and Yang , 2008 ) first constructs a sentence connectivity graph based on cosine similarity and then selects important sentences based on the concept of eigenvector centrality .
Chieu : ( Chieu and Lee , 2004 ) present a similar timeline system with different goals and frameworks , utilizing interest and burstiness ranking but neglecting trans-temporal news evolution .
ETTS : ETTS is an algorithm with optimized combination of global / local biased summarization .
RefTL : As we have used multiple human timelines as references , we not only provide ROUGE evaluations of the competing systems but also of the human timelines against each other , which provides a good indicator as to the upper bound ROUGE score that any system could achieve .
Overall Performance Comparison
We use a cross validation manner among 6 datasets , i.e. , train parameters on one subject set and examine the performance on the others .
After 6 trainingtesting processes , we take the average F-score performance in terms of ROUGE -1 , ROUGE - 2 , and ROUGE - W on all sets .
The overall results are shown in Figure 4 and details are listed in Tables 4?6 .
From the results , we have following observations : ?
Random has the worst performance as expected . ?
The results of Centroid are better than those of Random , mainly because the Centroid method takes Table 4 : Overall performance comparison on Influenza A ( ROI * category : Science ) and Financial Crisis ( ROI category : Finance ) . ?=0.4 , kernel =Gaussian , ?=60 . ?
In general , the result of Chieu is better than Centroid but unexpectedly , worse than GMDS .
The reason may be that Chieu does not capture sufficient timeline attributes .
The " interest " modeled in the algorithms actually performs flat clusteringbased summarization which is proved to be less useful ( Wang and Li , 2010 ) .
GMDS utilizes sentence linkage , and partly captures " correlativeness " .
1 . Influenza A 2 . Financial Crisis Systems R-1 R-2 R-W R-1 R -2 R-W RefTL 0 .
Jackson Death 6 . Obama Presidency Systems R-1 R-2 R-W R-1 R-2 R-W RefTL 0 . ? ETTS under our proposed framework outperforms baselines , indicating that the properties we use for timeline generation are beneficial .
We also add a direct comparison between ETTS and ETS ( Yan et al. , 2011 b ) .
We notice that both balanced algorithms achieve comparable performance ( 0.386 v.s. 0.412 : a gap of 0.026 in terms of ROUGE - 1 ) , but ETTS is much faster than ETS .
It is understandable that ETS refines timelines based on neighboring component summaries iteratively while for ETTS neighboring information is incorporated in temporal projection and hence there is no such procedure .
Furthermore , ETS has 8 free parameters to tune while ETTS has only 2 parameters .
In other words , ETTS is more simple to control .
?
The performance on intensive focused news within short time range ( | last timestamp ?
first timestamp |< 1 year ) is better than on long lasting news .
Having proved the effectiveness of our proposed methods , we carry the next move to identity how global ? local combination ratio ?/?
and projection kernels take effects to enhance the quality of a summary in parameter tuning .
Parameter Tuning Each time we tune one parameter while others are fixed .
To identify how global and local biased summarization combine , we provide experiments on the performance of varying ?/? in Figure 5 . Results indicate that a balance between global and local biased summarization is essential for timeline generation because the performance is best when ? ? ? [ 10 , 100 ] and outperforms global and local summarization in isolation , i.e. , when ?=0 or ? = 0 in Figure 5 . Interestingly , we conclude an opposite observation compared with ETS .
Different approaches might lead to different optimum of global / local combination .
Another key parameter ? measures the temporal projection influence from global collection to local collection and hence the size of neighboring sentence set .
6 datasets are classified into two groups .
Subject 1 , 2 , 6 are grouped as long news with a time span of more than one year and the others are short news .
The effect of ?
varies on long news sets and short news sets .
In Figure 6 ? is best around 60 and in Figure 7 it is best at about 20 ? 40 , indicating long news has relatively wider semantic scope .
We then examine the effect of different projection kernels .
Generally , Gaussian kernel outperforms others and window kernel is the worst , probably because Gaussian kernel provides the best smoothing effect with no arbitrary cutoffs .
Window kernel fails to distinguish different weights of neighboring sets by temporal proximity , so its performance is as expected .
Other 3 kernels are comparable .
Sample Output and Case Study Sample output is presented in Table 7 and it shares major information similarity with the human timeline in Table 1 . Besides , we notice that a dynamic ?
i is reasonable .
Important burstiness is worthy of more attention .
Fewer sentences are selected on the dates when nothing new occurs .
Interesting Findings .
We notice that humans have biases to generate timelines for they have ( 1 ) preference on local occurrences and ( 2 ) different writing styles .
For instance , news outlets from United States tend to summarize reactions by US government while UK websites tend to summarize British affairs .
Some editors favor statistical reports while others prefer narrative style , and some timelines have detailed explanations while others are extremely concise with no more than two sentences for each entry .
Our system - generated timelines have a large variance among all golden standards .
Probably a new evaluation metric should be introduced to measure the quality of human generated timelines to mitigate the corresponding biases .
A third interesting observation is that subjects have different volume patterns , e.g. , H1N 1 has a slow start and a bursty evolution and BP Oil has a bursty start and a quick decay .
Obama is different in nature because the report volume is temporally stable and scattered .
Conclusion
We present a novel solution for the important web mining problem , Evolutionary Trans -Temporal Summarization ( ETTS ) , which generates trajectory timelines for news subjects from massive data .
We formally formulate ETTS as a combination of global and local summarization , incorporating affinity and April 20 , 2010 s 1 : An explosion on the Deepwater Horizon offshore oil drilling rig in the Gulf of Mexico , around 40 miles south east of Louisiana , causing several kills and injuries .
s 2 : The rig was drilling in about 5,000 ft ( 1,525 m ) of water , pushing the boundaries of deepwater drilling technology .
s 3 : The rig is owned and operated by Transocean , a company hired by BP to carry out the drilling work .
s 4 : Deepwater Horizon oil rig fire leaves 11 missing .
April 22 , 2010 s 1 : The US Coast Guard estimates that the rig is leaking oil at the rate of up to 8,000 barrels a day .
s 2 : The Deepwater Horizon sinks to the bottom of the Gulf after burning for 36 hours , raising concerns of a catastrophic oil spill .
s 3 : Deepwater Horizon rig sinks in 5,000 ft of water .
April 23 , 2010 s 1 : The US coast guard suspends the search for missing workers , who are all presumed dead .
s 2 : The Coast Guard says it had no indication that oil was leaking from the well 5,000 ft below the surface of the Gulf .
s 3 : Underwater robots try to shut valves on the blowout preventer to stop the leak , but BP abandons that failed effort two weeks later .
s 4 : The US Coast Guard estimates that the rig is leaking oil at the rate of up to 8,000 barrels a day .
s 5 : Deepwater Horizon clean- up workers fight to prevent disaster .
April 24 , 2010 s 1 : Oil is found to be leaking from the well .
April 26 , 2010 s 1 : BP 's shares fall 2 % amid fears that the cost of cleanup and legal claims will hit the London - based company hard .
s 2 : Roughly 15,000 gallons of dispersants and 21,000 ft of containment boom are placed at the spill site .
April 27 , 2010 s 1 : BP reports a rise in profits , due in large part to oil price increases , as shares rise again .
s 2 : The US departments of interior and homeland security announce plans for a joint investigation of the explosion and fire .
s 3 : Minerals Management Service ( MMS ) approves a plan for two relief wells .
s 4 : BP chairman Tony Hayward says the company will take full responsibility for the spill , paying for legitimate claims and cleanup cost .
April 28 , 2010 s 1 : The coast guard says the flow of oil is 5,000bpd , five times greater than first estimated , after a third leak is discovered .
s 2 : BP 's attempts to repair a hydraulic leak on the blowout preventer valve are unsuccessful .
s 3 : BP reports that its first-quarter profits more than double to ?3.65 billion following a rise in oil prices .
s 4 : Controlled burns begin on the giant oil slick .
diversity into a unified ranking framework .
We implement a system under such framework for experiments on real web datasets to compare all approaches .
Through our experiment we notice that the combination plays an important role in timeline generation , and global optimization weights slightly higher ( ? /? ? [ 10 , 100 ] ) , but auxiliary local information does help to enhance performance in ETTS .
Figure 1 : 1 Figure 1 : Construct global / local biased graphs .
Solid circles denote intra-date sentences on the pending date t and dash ones represent inter-date sentences from other dates .
