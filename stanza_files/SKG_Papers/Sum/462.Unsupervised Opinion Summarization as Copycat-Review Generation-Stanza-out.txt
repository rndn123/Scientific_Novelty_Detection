title
Unsupervised Opinion Summarization as Copycat-Review Generation
abstract
Opinion summarization is the task of automatically creating summaries that reflect subjective information expressed in multiple documents , such as product reviews .
While the majority of previous work has focused on the extractive setting , i.e. , selecting fragments from input reviews to produce a summary , we let the model generate novel sentences and hence produce abstractive summaries .
Recent progress in summarization has seen the development of supervised models which rely on large quantities of document-summary pairs .
Since such training data is expensive to acquire , we instead consider the unsupervised setting , in other words , we do not use any summaries in training .
We define a generative model for a review collection which capitalizes on the intuition that when generating a new review given a set of other reviews of a product , we should be able to control the " amount of novelty " going into the new review or , equivalently , vary the extent to which it deviates from the input .
At test time , when generating summaries , we force the novelty to be minimal , and produce a text reflecting consensus opinions .
We capture this intuition by defining a hierarchical variational autoencoder model .
Both individual reviews and the products they correspond to are associated with stochastic latent codes , and the review generator ( " decoder " ) has direct access to the text of input reviews through the pointergenerator mechanism .
Experiments on Amazon and Yelp datasets , show that setting at test time the review 's latent code to its mean , allows the model to produce fluent and coherent summaries reflecting common opinions .
Introduction Summarization of user opinions expressed in online resources , such as blogs , reviews , social media , or internet forums , has drawn much attention due to its potential for various information access applications , such as creating digests , search , and report
Summary
This restaurant is a hidden gem in Toronto .
The food is delicious , and the service is impeccable .
Highly recommend for anyone who likes French bistro .
Reviews
We got the steak frites and the chicken frites both of which were very good ... generation ( Hu and Liu , 2004 ; Angelidis and Lapata , 2018 ; Medhat et al. , 2014 ) .
Although there has been significant progress recently in summarizing non-subjective context ( Rush et al. , 2015 ; Nallapati et al. , 2016 ; Paulus et al. , 2017 ; See et al. , 2017 ; Liu et al. , 2018 ) , modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion -summarization domain and expensive to produce .
Moreover , annotation efforts would have to be undertaken for multiple domains as online reviews are inherently multi-domain ( Blitzer et al. , 2007 ) and summarization systems highly domain-sensitive ( Isonuma et al. , 2017 ) .
Thus , perhaps unsurprisingly , there is a long history of applying unsupervised and weakly - supervised methods to opinion summarization ( e.g. , Mei et al.
2007 ; Titov and McDonald 2008 ; Angelidis and Lapata 2018 ) , however , these approaches have primarily focused on extractive summarization , i.e. , producing summaries by copying parts of the input reviews .
In this work , we instead consider abstractive summarization which involves generating new phrases , possibly rephrasing or using words that were not in the original text .
Abstractive summaries are often preferable to extractive ones as they can synthesize content across documents avoiding redundancy ( Barzilay et al. , 1999 ; Carenini and Cheung , 2008 ; Di Fabbrizio et al. , 2014 ) .
In addition , we focus on the unsupervised setting and do not use any summaries for training .
Unlike aspect-based summarization ( Liu , 2012 ) , which rewards the diversity of opinions , we aim to generate summaries that represent consensus ( i.e. , dominant opinons in reviews ) .
We argue that such summaries can be useful for quick decision making , and to get an overall feel for a product or business ( see the example in Table 1 ) .
More specifically , we assume we are provided with a large collection of reviews for various products and businesses and define a generative model of this collection .
Intuitively , we want to design such a model that , when generating a review for a product 1 relying on a set of other reviews , we can control the " amount of novelty " going into the new review or , equivalently , vary the extent to which it deviates from the input .
At test time , we can force the novelty to be minimal , and generate summaries representing consensus opinions .
We capture this intuition by defining a hierarchical variational autoencoder ( VAE ) model .
Both products and individual reviews are associated with latent representations .
Product representations can store , for example , overall sentiment , common topics , and opinions expressed about the product .
In contrast , latent representations of reviews depend on the product representations and capture the content of individual reviews .
While at training time the latent representations are random variables , we fix them to their respective means at test time .
As desired for summarization , these ' average ' ( or ' copycat ' ) reviews differ in writing style from a typical review .
For example , they do not contain irrelevant details that are common in customer reviews , such as mentioning the occasion or saying how many family members accompanied the reviewer .
In order to encourage the summaries to include spe-cific details , the review generator ( ' decoder ' ) has direct access to the text of input reviews through the pointer - generator mechanism ( See et al. , 2017 ) .
In the example in Table 1 , the model included specific information about the restaurant type and its location in the generated summary .
As we will see in ablation experiments , without this conditioning , model performance drops substantially , as the summaries become more generic .
We evaluate our approach on two datasets , Amazon product reviews and Yelp reviews of businesses .
The only previous method dealing with unsupervised multi-document opinion summarization , as far as we are aware of , is MeanSum ( Chu and Liu , 2019 ) .
Similarly to our work , they generate consensus summaries and consider the Yelp benchmark .
Whereas we rely on continuous latent representations , they treat the summary itself as a discrete latent representation of a product .
Although this captures the intuition that a summary should relay key information about a product , using discrete latent sequences makes optimization challenging ; ( Miao and Blunsom , 2016 ; Baziotis et al. , 2019 ; Chu and Liu , 2019 ) all have to use an extra training loss term and biased gradient estimators .
Our contributions can be summarized as follows : ? we introduce a simple end-to- end approach to unsupervised abstractive summarization ; ? we demonstrate that the approach substantially outperforms the previous method , both when measured with automatic metrics and in human evaluation ; ? we provide a dataset of abstractive summaries for Amazon products .
2
Model and Estimation
As discussed above , we approach the summarization task from a generative modeling perspective .
We start with a high level description of our model , then , in Sections 2.2 and 2.3 , we describe how we estimate the model and provide extra technical details .
In Section 3 , we explain how we use the model to generate summaries .
Overview of the Generative Model
Our text collection consists of groups of reviews , with each group corresponding to a single product .
z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T We visited this place last week .
M = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t >
z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = " r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = " > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = " r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = " > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = " n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = " > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 x b o 2 g < / l a t e x i t >
z N < l a t e x i t s h a 1 _ b a s e 6 4 = " n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = " > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d
The waiters were friendly , and the food was great !
z i < l a t e x i t s h a 1 _ b a s e 6 4 = " C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c +
N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d +
J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > z i < l a t e x i t s h a 1 _ b a s e 6 4 = " C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c +
N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d +
J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = "
6 e j v q f W e T m l k h + x J P T v H F G h m p p k = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m t w u / + 4 R K 8 1 g + m l m C f k T H k o e c U W O l B z X k w 3 L V r b l L k E 3 i 5 a Q K O V r D 8 t d g F L M 0 Q m m Y o F r 3 P T c x f k a V 4 U z g v D R I N S a U T e k Y + 5 Z K G q H 2 s + W p c 3 J p l R E J Y 2 V L G r J U f 0 9 k N N J 6 F g W 2 M 6 J m o t e 9 h f i f 1 0 9 N e O N n X C a p Q c l W i 8 J U E B O T x d 9 k x B U y I 2 a W U K a 4 v Z W w C V W U G Z t O y Y b g r b +
8 S T r 1 m n d d q 9 9 7 1 W Y l j 6 M I F 1 C B K / C g A U 2 4 g x a 0 g c E Y n u E V 3 h z h v D j v z s e q t e D k M + f w B 8 7 n D 0 4 q j b M = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = "
6 e j v q f W e T m l k h + x J P T v H F G h m p p k = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m t w u / + 4 R K 8 1 g + m l m C f k T H k o e c U W O l B z X k w 3 L V r b l L k E 3 i 5 a Q K O V r D 8 t d g F L M 0 Q m m Y o F r 3 P T c x f k a V 4 U z g v D R I N S a U T e k Y + 5 Z K G q H 2 s + W p c 3 J p l R E J Y 2 V L G r J U f 0 9 k N N J 6 F g W 2 M 6 J m o t e 9 h f i f 1 0 9 N e O N n X C a p Q c l W i 8 J U E B O T x d 9 k x B U y I 2 a W U K a 4 v Z W w C V W U G Z t O y Y b g r b +
8 S T r 1 m n d d q 9 9 7 1 W Y l j 6 M I F 1 C B K / C g A U 2 4 g x a 0 g c E Y n u E V 3 h z h v D j v z s e q t e D k M + f w B 8 7 n D 0 4 q j b M = < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = " + u 2
We visited this place last week .
Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P C A J H 8 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s v C A J H 8 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s v r 5 N 2 v e Z d 1 u r 3 X r V R 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t >
z 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
H t n N Q 8 7 l 4 s U B 8 7 7 3 B 8 j R e w 1 t l T M = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G 3 q B c d W v u A m S d e D m p Q o 7 m o P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y b p U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D K / 9 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u 1 7 z L W v 3 O q z Y q e R x F O I M K X I A H V 9 C A W 2 h C C x i M 4 B l e 4 c 2 R z o v z 7 n w s W w t O P n M K f + B 8 / g A F e o 2 D < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = " r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = " > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > c < l a t e x i t s h a 1 _ b a s e 6 4 = " r W O 4 a M m 2 s R s k t Q X w 3 x G E o I X J 3 E w = " > A A A B 6 H i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W U C 5 g O S I + x t 5 p I 1 e 3 v H 7 p 4 Q j v w C G w t F b P 1 J d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D N Y h G r X k A 1 C i 6 x b b g R 2 E s U 0 i g Q 2 A 2 m d w u / + 4 R K 8 1 g + m F m C f k T H k o e c U W O l F h u W q 2 7 N X Y J s E i 8 n V c j R H J a / B q O Y p R F K w w T V u u + 5 i f E z q g x n A u e l Q a o x o W x K x 9 i 3 V N I I t Z 8 t D 5 2 T S 6 u M S B g r W 9 K Q p f p 7 I q O R 1 r M o s J 0 R N R O 9 7 i 3 E / 7 x + a s J b P + M y S Q 1 K t l o U p o K Y m C y + J i O u k B k x s 4 Q y x e 2 t h E 2 o o s z Y b E o 2 B G / 9 5 U 3 S q d e 8 6 1 q 9 5 V U b l T y O I l x A B a 7 A g x t o w D 0 0 o Q 0 M E J 7 h F d 6 c R + f F e X c + V q 0 F J 5 8 5 h z 9 w P n 8 A v S O M y A = = < / l a t e x i t > z N < l a t e x i t s h a 1 _ b a s e 6 4 = " n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = " > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d 4 c 6 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 x b o 2 g < / l a t e x i t >
z N < l a t e x i t s h a 1 _ b a s e 6 4 = " n 3 B r e B z d C g o 8 W f + t 9 N v W 0 Z O V f F o = " > A A A B 6 n i c b V A 9 S w N B E J 3 z M 8 a v q K X N k i B Y h b t Y a B m w s Z K I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j / w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 a y t b 2 x u b R d 2 i r t 7 + w e H p a P j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D 3 z 2 4 9 c G x G r B 5 w k 3 I / o U I l Q M I p W u n / q 3 / Z L F b f q z k F W i Z e T C u R o 9 E t f v U H M 0 o g r Z J I a 0 / X c B P 2 M a h R M 8 m m x l x q e U D a m Q 9 6 1 V N G I G z + b n z o l Z 1 Y Z k D D W t h S S u f p 7 I q O R M Z M o s J 0 R x Z F Z 9 m b i f 1 4 3 x f D K z 4 R K U u S K L R a F q S Q Y k 9 n f Z C A 0 Z y g n l l C m h b 2 V s B H V l K F N p 2 h D 8 J Z f X i W t W t W 7 q N b u v E q 9 n M d R g F M o w z l 4 c A l 1 u I E G N I H B E J 7 h F d
The waiters were friendly , and the food was great !
Our latent summarization model ( which we call COPYCAT ) captures this hierarchical organization and can be regarded as an extension of the vanilla text - VAE model ( Bowman et al. , 2016 ) . COPYCAT uses two sets of latent variables as shown in Figure 1a .
Namely , we associate each review group ( equivalently , each product ) with a continuous variable c , which captures the group 's ' latent semantics ' .
In addition , we associate each individual review ( r i ) with a continuous variable z i , encoding the semantics of that review .
The information stored in z i is used by the decoder p ? ( r i |z i ) to produce review text r i .
The marginal log-likelihood of one group of reviews r 1:N = ( r 1 , . . . , r N ) is given by z i < l a t e x i t s h a 1 _ b a s e 6 4 = " C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c +
N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d +
J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > z i < l a t e x i t s h a 1 _ b a s e 6 4 = " C R 7 L 5 4 o 1 Y e U 9 L s N Q E 4 0 W 9 U q 2 l J 0 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j W V E 8 w H J E f Y 2 e 8 m S v b 1 j d 0 6 I R 3 6 C j Y U i t v 4 i O / + N m + Q K T X w w 8 H h v h p l 5 Q S K F Q d f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k b e J U M 9 5 i s Y x 1 N 6 C G S 6 F 4 C w V K 3 k 0 0 p 1 E g e S e Y 3 M z 9 z i P X R s T q A a c J 9 y M 6 U i I U j K K V 7 p 8 G Y l C u u j V 3 A b J O v J x U I U d z U P 7 q D 2 O W R l w h k 9 S Y n u c m 6 G d U o 2 C S z 0 r 9 1 P C E s g k d 8 Z 6 l i k b c +
N n i 1 B k 5 t 8 q Q h L G 2 p Z A s 1 N 8 T G Y 2 M m U a B 7 Y w o j s 2 q N x f / 8 3 o p h t d +
J l S S I l d s u S h M J c G Y z P 8 m Q 6 E 5 Q z m 1 h D I t 7 K 2 E j a m m D G 0 6 J R u C t / r y O m n X a 9 5 l r X 7 n V R u V P I 4 i n E E F L s C D K 2 j A L T S h B Q x G 8 A y v 8 O Z I 5 8 V 5 d z 6 W r Q U n n z m F P 3 A + f w B a W o 2 7 < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = "
F H L u u s i R u g f X q 9 y L D H J X e o R y D T w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F I p Y B G 8 u I 5 g O S I + x t 9 p I l e 3 v H 7 p w Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k Q K g 6 7 7 7 R S 2 t n d 2 9 4 r 7 p Y P D o + O T 8 u l Z x 8 S p Z r z N Y h n r X k A N l 0 L x N g q U v J d o T q N A 8 m 4 w v V 3 4 3 S e u j Y j V I 8 4 S 7 k d 0 r E Q o G E U r P e i h N y x X 3 Z q 7 B N k k X k 6 q k K M 1 L H 8 N R j F L I 6 6 Q S W p M 3 3 M T 9 D O q U T D J 5 6 V B a n h C 2 Z S O e d 9 S R S N u / G x 5 6 p x c W m V E w l j b U k i W 6 u + J j E b G z K L A d k Y U J 2 b d W 4 j / e f 0 U w x s / E y p J k S u 2 W h S m k m B M F n + T k d C c o Z x Z Q p k W 9 l b C J l R T h j a d k g 3 B W 3 9 5 k 3 T q N e + 6 V r / 3 q s 1 K H k c R L q A C V + B B A 5 p w B y 1 o A 4 M x P M M r v D n S e X H e n Y 9 V a 8 H J Z 8 7 h D 5 z P H / k 7 j X s = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 C g G L J g q b L W V C p 8 / r I 9 t i O 4 P 6 n I = " > A A A B + X i c b V C 7 T s M w F L 3 h W c o r w M h i t U J i q p I y w F i J h b F I 9 C G 1 U e Q 4 b m v V s S P b q V R F / R M W B h B i 5 U / Y + B u c N g O 0 H M n y 0 T n 3 y s c n S j n T x v O + n a 3 t n d 2 9 / c p B 9 f D o + O T U P T v v a p k p Q j t E c q n 6 E d a U M 0 E 7 h h l O + 6 m i O I k 4 7 U X T + 8 L v z a j S T I o n M 0 9 p k O C x Y C N G s L F S 6 L r D S P J Y z x N 7 5 S p k i 9 C t e w 1 v C b R J / J L U o U Q 7 d L + G s S R Z Q o U h H G s 9 8 L 3 U B D l W h h F O F 9 V h p m m K y R S P 6 c B S g R O q g 3 y Z f I G u r B K j k V T 2 C I O W 6 u + N H C e 6 C G c n E 2 w m e t 0 r x P + 8 Q W Z G d 0 H O R J o Z K s j q o V H G k Z G o q A H F T F F i + N w S T B S z W R G Z Y I W J s W V V b Q n + + p c 3 S b f Z 8 G 8 a z U e / 3 q q V d V T g E m p w D T 7 c Q g s e o A 0 d I D C D Z 3 i F N y d 3 X p x 3 5 2 M 1 u u W U O x f w B 8 7 n D z 4 j k / E = < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 C g G L J g q b L W V C p 8 / r I 9 t i O 4 P 6 n I = " > A A A B + X i c b V C 7 T s M w F L 3 h W c o r w M h i t U J i q p I y w F i J h b F I 9 C G 1 U e Q 4 b m v V s S P b q V R F / R M W B h B i 5 U / Y + B u c N g O 0 H M n y 0 T n 3 y s c n S j n T x v O + n a 3 t n d 2 9 / c p B 9 f D o + O T U P T v v a p k p Q j t E c q n 6 E d a U M 0 E 7 h h l O + 6 m i O I k 4 7 U X T + 8 L v z a j S T I o n M 0 9 p k O C x Y C N G s L F S 6 L r D S P J Y z x N 7 5 S p k i 9 C t e w 1 v C b R J / J L U o U Q 7 d L + G s S R Z Q o U h H G s 9 8 L 3 U B D l W h h F O F 9 V h p m m K y R S P 6 c B S g R O q g 3 y Z f I G u r B K j k V T 2 C I O W 6 u + N H C e 6 C G c n E 2 w m e t 0 r x P + 8 Q W Z G d 0 H O R J o Z K s j q o V H G k Z G o q A H F T F F i + N w S T B S z W R G Z Y I W J s W V V b Q n + + p c 3 S b f Z 8 G 8 a z U e / 3 q q V d V T g E m p w D T 7 c Q g s e o A 0 d I D C D Z 3 i F N y d 3 X p x 3 5 2 M 1 u u W U O x f w B 8 7 n D z 4 j k / E = < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = " + u 2 Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P C A J H 8 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B C A J H 8 = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t l g T B K t z F Q s u A j Z V E N B + Q H G F v M 5 c s 2 d s 7 d v e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h Y 3 N r e 6 e 4 W 9 r b P z g 8 K h + f t H W c K o Y t F o t Y d Q O q U X C J L c O N w G 6 i k E a B log p ? ( r 1:N ) = log p ? ( c ) N i=1 p ? ( r i |z i ) p ? ( z i | c ) dz i dc , where we marginalize over variables c and z 1:N .
When generating a new review r i , given the set of previous reviews r 1 : i , the information about these reviews has to be conveyed through the latent representations c and z i .
This bottleneck is undesirable , as it will make it hard for the model to pass fine -grain information .
For example , at generation time , the model should be reusing named entities ( e.g. , product names or technical characteristics ) from other reviews rather than ' hallucinating ' or avoiding generating them at all , resulting in generic and non-informative text .
We alleviate this issue by letting the decoder directly access other reviews .
We can formulate this as an autoregressive model : p ? ( r 1:N |c ) = N i=1 p ? ( r i |r 1 , ... , r i?1 , c ) . ( 1 ) As we discuss in Section 2.3 , the conditioning is instantiated using the pointer - generator mechanism ( See et al. , 2017 ) and , thus , will specifically help in generating rare words ( e.g. , named entities ) .
We want our summarizer to equally rely on every review , without imposing any order ( e.g. , temporal ) on the generation process .
Instead , as shown in Figure 1 b , when generating r i , we let the decoder access all other reviews within a group , r ?i = ( r 1 , . . . , r i?1 , r i+ 1 , . . . , r N ) .
This is closely related to pseudolikelihood estimation ( Besag , 1975 ) or Skip- Thought 's objective ( Kiros et al. , 2015 ) .
The final objective that we maximize for each group of reviews r 1:N : log p ? ( c ) N i=1 p ? ( r i |z i , r i ) p ? ( z i | c ) dz i dc ( 2 )
We will confirm in ablation experiments that both hierarchical modeling ( i.e. , using c ) and the direct conditioning on other reviews are beneficial .
Model Estimation
As standard with VAEs and variational inference in general ( Kingma and Welling , 2013 ) , instead of directly maximizing the intractable marginal likelihood in Equation 2 , we maximize its lower bound : 3 L ( ? , ? ; r 1:N ) = E c?q ? ( c|r 1:N ) N i=1 E z i ?q ? ( z i |r i , c ) [ log p ? ( r i |z i , r i ) ] ? N i=1 D KL [ q ? ( z i |r i , c ) | |p ? ( z i | c ) ] ? D KL [ q ? ( c|r 1:N ) ||p ? ( c ) ] .
3 See the derivations in Appendix A.1. concat ? review 1 h T1 1 h T1 1 w T1 1 w T1 1 w 2 1 w 2 1 w 1 1 w 1 1 h 1 1 h 1 1 h 2 1 h 2 1 concat w TN N w TN N ? h 1 N h 1 N h TN N h TN N review N w 1 N w 1 N word embeddings ?
GRU hidden states ? ? q ( c|r 1:N ) q ( c|r 1:N ) c c sampling z N z N sampling q ( z|r N , c ) q ( z|r N , c ) p ? ( z|c ) p ? ( z| c ) Figure 2 : Production of latent code z N for review r N .
The lower bound includes two ' inference networks ' , q ? ( c|r 1:N ) and q ?
( z i |r i , c ) , which are neural networks parameterized with ? and will be discussed in detail in Section 2.3 .
They approximate the corresponding posterior distributions of the model .
The first term is the reconstruction error : it encourages the quality reconstruction of the reviews .
The other two terms are regularizers .
They control the amount of information encoded in the latent representation by penalizing the deviation of the estimated posteriors from the corresponding priors , the deviation is measured in terms of the Kullback - Leibler ( KL ) divergence .
The bound is maximized with respect to both the generative model 's parameters ? and inference networks ' parameters ?.
Due to Gaussian assumptions , the Kullback - Leibler ( KL ) divergence terms are available in closed form , while we rely on the reparameterization trick ( Kingma and Welling , 2013 ) to compute gradients of the reconstruction term .
The inference network predicting the posterior for a review-specific variable q ?
( z i |r i , c ) is needed only in training and is discarded afterwards .
In contrast , we will exploit the inference network q ? ( c|r 1:N ) when generating summaries , as discussed in Section 3 .
Design of Model Components
Text Representations
A GRU encoder ( Cho et al. , 2014 ) embeds review words w to obtain hidden states h .
Those representations are reused across the system , e.g. , in the inference networks and the decoder .
The full architecture used to produce the latent codes c and z i is shown in Figure 2 .
We make Gaussian assumptions for all distributions ( i.e. posteriors and priors ) .
As in Kingma and Welling ( 2013 ) , we use separate linear projections ( LPs ) to compute the means and diagonal log-covariances .
2.3.2 Prior p( c ) and posterior q ? ( c|r 1:N )
We set the prior over group latent codes to the standard normal distribution , p( c ) = N ( c ; 0 , I ) .
In order to compute the approximate posterior q ? ( c|r 1:N ) , we first predict the contribution ( ' importance ' ) of each word in each review ?
t i to the code of the group : ? t i = exp ( f ? ? ( m t i ) )
N j=1 T j k exp ( f ? ? ( m k j ) ) , where T i is the length of r i and f ? ? is a feed-forward neural network ( FFNN ) 4 which takes as input concatenated word embeddings and hidden states of the GRU encoder , m t i = [ h t i ? w t i ] , and returns a scalar .
Next , we compute the intermediate representation with the weighted sum : ? = N i=1 T i t ?
t i m t i .
Finally , we compute the Gaussian 's parameters using the affine projections : ? ? ( r 1:N ) = L ? + b L log ? ? ( r 1:N ) = G ? + b G 2.3.3 Prior p ? ( z i |c ) and posterior q ?
( z i |r i , c )
To compute the prior on the review code z i , p ? ( z i | c ) = N ( z i ; ? ? ( c ) , I ? ? ( c ) ) , we linearly project the product code c.
Similarly , to compute the parameters of the approximate posterior q ?
( z|r i , c ) = N ( z ; ? ? ( r i , c ) , I ? ? ( r i , c ) ) , we concatenate the last encoder 's state h T i i of the review r i and c , and perform affine transformations .
Decoder p ? ( r i |z i , r i )
To compute the distribution p ?
( r i |z i , r i ) , we use an auto-regressive GRU decoder with the attention mechanism ( Bahdanau et al. , 2015 ) and a pointergenerator network .
We compute the context vector c t i = att ( s t i , h i ) by attending to all the encoder 's hidden states h i of the other reviews r i of the group , where the decoder 's hidden state s t i is used as a query .
The hidden state of the decoder is computed using the GRU cell as s t i = GRU ? ( s t?1 i , [ w t i ? c t?1 i ? z i ] ) .
( 3 )
The cell inputs the previous hidden state s t?1 i , as well as concatenated word embedding w t i , context vector c t?1 i , and latent code z i .
Finally , we compute the word distributions using the pointer- generator network g ? : p ? ( r i |z i , r i ) = T t=1 g ?
( r t i |s t i , c t i , w t i , r i ) ( 4 )
The pointer - generator network computes two internal word distributions that are hierarchically aggregated into one distribution ( Morin and Bengio , 2005 ) .
One distribution assigns probabilities to words being generated using a fixed vocabulary , and another one probabilities to be copied directly from the other reviews r i .
In our case , the network helps to preserve details and , especially , to generate rare tokens .
Summary Generation Given reviews r 1:N , we generate a summary that reflects common information using trained components of the model .
Formally , we could sample a new review from p ? ( r|r 1:N ) = E c?q ? ( c|r 1:N ) E z?p ? ( z|c ) [ p ? ( r|z , r 1:N ) ] .
As we argued in the introduction and will revisit in experiments , a summary or summarizing review , should be generated relying on the mean of the reviews ' latent code .
Consequently , instead of sampling z from p ? ( z| c ) = N ( z ; ? ? ( c ) , I ? ? ( c ) ) , we set it to ? ? ( c ) .
We also found beneficial , in terms of evaluation metrics , not to sample c but instead to rely on the mean predicted by the inference network q ? ( c|r 1:N ) .
Experimental Setup
Datasets
Our experiments were conducted on business customer reviews from the Yelp Dataset Challenge and Amazon product reviews ( He and McAuley , 2016 ) .
These were pre-processed similarly to Chu and Liu ( 2019 ) shown in Table 2 . Details of the pre-processing are available in Appendix A.2 .
These datasets present different challenges to abstractive summarization systems .
Yelp reviews contain much personal information and irrelevant details which one may find unnecessary in a summary .
Our summarizer , therefore , needs to distill important information in reviews while abstracting away from details such as a listing of all items on the menu , or mentions of specific dates or occasions upon which customers visited a restaurant .
On the contrary , in Amazon reviews , we observed that users tend to provide more objective information and specific details that are useful for decision making ( e.g. , the version of an electronic product , its battery life , its dimensions ) .
In this case , it would be desirable for our summarizer to preserve this information in the output summary .
For evaluation , we used the same 100 humancreated Yelp summaries released by Chu and Liu ( 2019 ) .
These were generated by Amazon Mechanical Turk ( AMT ) workers , who summarized 8 input reviews .
We created a new test for Amazon reviews following a similar procedure ( see Appendix A.6 for details ) .
We sampled 60 products and 8 reviews for each product , and they were shown to AMT workers who were asked to write a summary .
We collected three summaries per product , 28 products were used for development and 32 for testing .
Experimental Details
We used GRUs ( Cho et al. , 2014 ) for sequential encoding and decoding we used GRUs .
We randomly initialized word embeddings that were shared across the model as a form of regularization ( Press and Wolf , 2017 ) .
Further , optimization was performed using Adam ( Kingma and Ba , 2014 ) .
In order to overcome the " posterior collapse " ( Bowman et al. , 2016 ) , both for our model and the vanilla VAE baseline , we applied cyclical annealing ( Fu et al. , 2019 )
Baseline Models Opinosis is a graph- based abstractive summarizer ( Ganesan et al. , 2010 ) designed to generate short opinions based on highly redundant texts .
Although it is referred to as abstractive , it can only select words from the reviews .
LexRank is an unsupervised algorithm which selects sentences to appear in the summary based on graph centrality ( sentences represent nodes in a graph whose edges have weights denoting similarity computed with tf - idf ) .
A node 's centrality can be measured by running a ranking algorithm such as PageRank ( Page et al. , 1999 ) .
MeanSum 5 is the unsupervised abstractive summarization model ( Chu and Liu , 2019 ) discussed in the introduction .
We also trained a vanilla text VAE model ( Bowman et al. , 2016 ) with our GRU encoder and decoder .
When generating a summary for r 1 , ... , r N , we averaged the means of q ? ( z i |r i ) .
Finally , we used a number of simple summarization baselines .
We computed the clustroid review for each group as follows .
We took each review from a group and computed ROUGE -L with respect to all other reviews .
The review with the highest ROUGE score was selected as the clustroid review .
Furthermore , we sampled a random review from each group as the summary , and constructed the summary by selecting the leading sentences from each review of a group .
Additionally , as an upper bound , we report the performance of an oracle review , i.e. , the highest - scoring review in a group when computing ROUGE -L against reference summaries .
5 Evaluation Results
Automatic Evaluation
As can be seen in Tables 3 and 4 , our model , Copycat , yields the highest scores on both Yelp and Amazon datasets .
We observe large gains over the vanila VAE .
We conjecture that the vanilla VAE struggles to properly represent the variety of categories under a single prior p ( z ) .
For example , reviews about a sweater can result in a summary about socks ( see example summmaries in Appendix ) .
This contrasts with our model which allows each group to have its own prior p ? ( z|c ) and access to other reviews during decoding .
The gains are especially large on the Amazon dataset , which is very broad in terms of product categories .
Our model also substantially outperforms Mean-Sum .
As we will confirm in human evaluation , MeanSum 's summaries are relatively fluent at the sentence level but often contain hallucinations , i.e. , information not present in the input reviews .
Human Evaluation Best-Worst Scaling
We performed human evaluation using the AMT platform .
We sampled 50 businesses from the human-annotated Yelp test set and used all 32 test products from the Amazon set .
We recruited 3 workers to evaluate each tuple containing summaries from MeanSum , our model , LexRank , and human annotators .
The reviews and summaries were presented to the workers in random order and were judged using Best-Worst Scaling ( Louviere and Woodworth , 1991 ; Louviere et al. , 2015 ) .
BWS has been shown to produce more reliable results than ranking scales ( Kiritchenko and Mohammad , 2016 criteria listed below ( we show an abridged version below , the full set of instructions is given in Appendix A.5 ) .
The non-redundancy and coherence criteria were taken from Dang ( 2005 ) .
Fluency : the summary sentences should be grammatically correct , easy to read and understand ;
Coherence : the summary should be well structured and well organized ;
Non-redundancy : there should be no unnecessary repetition in the summary ; Opinion consensus : the summary should reflect common opinions expressed in the reviews ; Overall : based on your own criteria ( judgment ) please select the best and the worst summary of the reviews .
For every criterion , a system 's score is computed as the percentage of times it was selected as best minus the percentage of times it was selected as worst ( Orme , 2009 ) .
The scores range from - 1 ( unanimously worst ) to + 1 ( unanimously best ) .
On Yelp , as shown in Table 5 , our model scores higher than the other models according to most criteria , including overall quality .
The differences with other systems are statistically significant for all the criteria at p < 0.01 , using post-hoc HD Tukey tests .
The difference in fluency between our system and gold summaries is not statistically significant .
The results on Amazon are shown in Table 6 .
Our system outperforms other methods in terms of fluency , coherence , and non-redundancy .
As with Yelp , it trails LexRank according to the opinion consensus criterion .
Additionally , LexRank is slightly preferable overall .
All pairwise differences between our model and comparison systems are statistically significant at p < 0.05 .
Opinion consensus ( OC ) is a criterion that cap-tures the coverage of common opinions , and it seems to play a different role in the two datasets .
On Yelp , LexRank has better coverage compared to our model , as indicated by the higher OC score , but is not preferred overall .
In contrast , on Amazon , while the OC score is on the same par , LexRank is preferred overall .
We suspect that presenting a breadth of exact details on Amazon is more important than on Yelp .
Moreover , LexRank tends to produce summaries that are about 20 tokens longer than ours resulting in better coverage of input details .
Content Support
The ROUGE metric relies on unweighted n-gram overlap and can be insensitive to hallucinating facts and entities ( Falke et al. , 2019 ) .
For example , referring to a burger joint as a veggie restaurant is highly problematic from a user perspective but yields only marginal differences in ROUGE .
To investigate how well the content of the summaries is supported by the input reviews , we performed a second study .
We used the same sets as in the human evaluation in Section 5.2 , and split MeanSum and our system 's summaries into sentences .
Then , for each summary sentence , we assigned 3 AMT workers to assess how well the sentence is supported by the reviews .
Workers were advised to read the reviews and rate sentences using one of the following three options .
Full support : all the content is reflected in the reviews ; Partial support : only some content is reflected in the reviews ;
No support : content is not reflected in the reviews .
The results in
Analysis Ablations
To investigate the importance of the model 's individual components , we performed ablations by removing the latent variables ( z i and c , one at a time ) , and attention over the other reviews .
The models were re-trained on the Amazon dataset .
The results are shown in Table 8 .
They indicate that all components play a role , yet the most significant drop in ROUGE was achieved when the variable z was removed , and only c remained .
Summaries obtained from the latter system were wordier and looked more similar to reviews .
Dropping the attention ( w/ o r i ) results in more generic summaries as the model cannot copy details from the input .
Finally , the smallest quality drop in terms of ROUGE -L was observed when the variable c was removed .
In the introduction , we hypothesized that using the mean of latent variables would result in more " grounded " summaries reflecting the content of the input reviews , whereas sampling would yield texts with many novel and potentially irrelevant details .
To empirically test this hypothesis , we sampled the latent variables during summary generation , as opposed to using mean values ( see Section 3 ) .
We indeed observed that the summaries were wordier , less fluent , and less aligned to the input reviews , as is also reflected in the ROUGE scores ( Table 8 ) .
Copy Mechanism Finally , we analyzed which words are copied by the full model during summary generation .
Generally , the model copies around 3 - 4 tokens per summary .
We observed a tendency to copy product - type specific words ( e.g. , shoes ) as well as brands and names .
Related Work Extractive weakly - supervised opinion summarization has been an active area of research .
A recent example is Angelidis and Lapata ( 2018 ) .
First , they learn to assign sentiment polarity to review segments in a weakly - supervised fashion .
Then , they induce aspect labels for segments relying on a small sample of gold summaries .
Finally , they use a heuristic to construct a summary of segments .
Opinosis ( Ganesan et al. , 2010 ) does not use any supervision .
The model relies on redundancies in opinionated text and PoS tags in order to generate short opinions .
This approach is not well suited for the generation of coherent long summaries and although it can recombine fragments of input text , it cannot generate novel words and phrases .
LexRank ( Erkan and Radev , 2004 ) is an unsupervised extractive approach which builds a graph in order to determine the importance of sentences , and then selects the most representative ones as a summary .
Isonuma et al. ( 2019 ) introduce an unsupervised approach for single review summarization , where they rely on latent discourse trees .
Other earlier approaches ( Gerani et al. , 2014 ; Di Fabbrizio et al. , 2014 ) relied on text planners and templates , while our approach does not require rules and can produce fluent and varied text .
Finally , conceptually related methods were applied to unsupervised single sentence compression ( West et al. , 2019 ; Baziotis et al. , 2019 ; Miao and Blunsom , 2016 ) .
The most related approach to ours is MeanSum ( Chu and Liu , 2019 ) which treats a summary as a discrete latent state of an autoencoder .
In contrast , we define a hierarchical model of a review collection and use continuous latent codes .
Conclusions
In this work , we presented an abstractive summarizer of opinions , which does not use any summaries in training and is trained end -to - end on a large collection of reviews .
The model compares favorably to the competitors , especially to the only other unsupervised abstractive multi-review summarization system .
Furthermore , human evaluation of the generated summaries ( by considering their alignment with the reviews ) shows that those created by our model better reflect the content of the input .
Alexander M Rush , Sumit Chopra , and Jason Weston . 2015 .
A neural attention model for abstractive sentence summarization .
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 379-389 .
Abigail See , Peter J Liu , and Christopher D Manning . 2017 .
Get to the point : Summarization with pointergenerator networks .
In Proceedings of Association for Computational Linguistics ( ACL ) .
Ivan Titov and Ryan McDonald .
2008 . Modeling online reviews with multi-grain topic models .
In Proceedings of the 17th international conference on World Wide Web , pages 111-120 .
ACM .
Peter West , Ari Holtzman , Jan Buys , and Yejin Choi. 2019 .
Bottlesum : Unsupervised and self-supervised sentence summarization using the information bottleneck principle .
arXiv preprint arXiv:1909.07405 .
A Appendices
A.1 Derivation of the Lower Bound
To make the notation below less cluttered , we make a couple of simplifications : q ? ( c| ? ) = q ? ( c|r 1:N ) and q ? ( z|i ) = q ? ( z|r i , c ) .
log p ? ( c ) N i=1 p ? ( r i |c , r i ) dc = log p ? ( c ) N i=1 p ? ( r i , z|c , r i ) dz dc = log p ? ( c ) q ? ( c| ? ) q ? ( c| ? ) N i=1 p ? ( r i , z|c , r i ) q ? ( z|i ) q ? ( z| i ) dz dc = log E c?q ? ( c| ? ) p ? ( c ) q ? ( c| ? ) N i=1 E z?q ? ( z|i ) p ? ( r i , z|c , r i ) q ? ( z|i ) ? E c?q ? ( c| ? ) N i=1 log E z?q ? ( z|i ) p ? ( r i , z|c , r i ) q ? ( z|i ) ? D KL [ q ? ( c| ? ) ) ||p ? ( c ) ] ? E c?q ? ( c| ? ) N i=1 E z?q ? ( z|i ) log p ? ( r i , z|c , r i ) q ? ( z|i ) ? D KL [ q ? ( c| ? ) ) ||p ? ( c ) ] = E c?q ? ( c| ? ) N i=1 E z?q ? ( z|i ) [ log p ? ( r i |z , r i ) ] ? N i=1 D KL [ q ? ( z|i ) | |p ? ( z| c ) ] ? D KL [ q ? ( c| ? ) ||p ? ( c ) ] ( 5 ) A.2 Dataset Pre-Processing
We selected only businesses and products with a minimum of 10 reviews , and thee minimum and maximum length of 20 and 70 words respectively , popular groups above the 90 th percentile were removed .
And each group was set to contain 8 reviews during training .
From the Amazon dataset we selected 4 categories : Electronics ; Clothing , Shoes and Jewelry , Home and Kitchen ; Health and Personal Care .
A.3 Hyperparameters
For sequential encoding and decoding , we used GRUs ( Cho et al. , 2014 ) with 600 - dimensional hidden states .
The word embeddings dimension was set to 200 , and they were shared across the model ( Press and Wolf , 2017 ) .
The vocabulary size was set to 50,000 most frequent words , and an extra 30,000 were allowed in the extended vocabulary , the words were lower -cased .
We used the Moses ' ( Koehn et al. , 2007 ) reversible tokenizer and truecaser .
Xavier uniform initialization ( Glorot and Bengio , 2010 ) of 2D weights was used , and 1D weights were initialized with the scaled normal noise ( ? = 0.1 ) .
We used the Adam optimizer ( Kingma and Ba , 2014 ) , and set the learning rate to 0.0008 and 0.0001 on Yelp and Amazon , respectively .
For summary decoding , we used lengthnormalized beam search of size 5 , and relied on latent code means .
In order to overcome " posterior collapse " ( Bowman et al. , 2016 ) we applied cycling annealing ( Fu et al. , 2019 ) with r = 0.8 for both the z and c related KL terms , with a new cycle over approximately every 2 epochs over the training set .
The maximum annealing scalar was set to 1 for z-related KL term in on both datasets , and 0.3 and 0.65 for c-related KL -term on Yelp and Amazon , respectively .
The reported ROUGE scores are based on F1 .
The dimensions of the variables c and z were set to 600 , and the c posterior 's scoring neural network had a 300 - dimensional hidden layer and the tanh non-linearity .
The decoder 's attention mechanism used a single layer neural network with a 200 - dimensional hidden layer , and the tanh non-linearity .
The copy gate in the pointer - generator network was computed with a 100 - dimensional single - hidden layer network , with the same non-linearity .
A.4 Human Evaluation Setup
To perform the human evaluation experiments described in Sections 5.2 and 5.2 we combined both tasks into single Human Intelligence Tasks ( HITs ) .
Namely , the workers needed to mark sentences as described in Section 5.2 , and then proceed to the task in Section 5.2 .
We explicitly asked then to re-read the reviews before each task .
For worker requirements we set 98 % approval rate , 1000 +
HITS , Location : USA , UK , Canada , and the maximum score on a qualification test that we designed .
The test was asking if the workers are native English speakers , and verifying that they correctly understand the instructions of both tasks by completing a mini version of the actual HIT .
A.5 Full Human Evaluation Instructions ?
Fluency :
The summary sentences should be grammatically correct , easy to read and understand .
?
Coherence :
The summary should be well structured and well organized .
The summary should not just be a heap of related information , but should build from sentence to sentence to a coherent body of information about a topic . ?
Non-redundancy :
There should be no unnecessary repetition in the summary .
Unnecessary repetition might take the form of whole sentences that are repeated , or repeated facts , or the repeated use of a noun or noun phrase ( e.g. , " Bill Clinton " ) when a pronoun ( " he " ) would suffice .
?
Opinion consensus :
The summary should reflect common opinions expressed in the reviews .
For example , if many reviewers complain about a musty smell in the hotel 's rooms , the summary should include this information .
?
Overall : Based on your own criteria ( judgment ) please select the best and the worst summary of the reviews .
A.6 Amazon Summaries Creation First , we sampled 15 products from each of the Amazon review categories : Electronics ; Clothing , Shoes and Jewelry ; Home and Kitchen ; Health and Personal Care .
Then , we selected 8 reviews from each product to be summaries .
We used the same requirements for workers as for human evaluation in A.4 .
We assigned 3 workers to each product , and instructed them to read the reviews and produce a summary text .
We followed the instructions provided in ( Chu and Liu , 2019 ) , and used the following points in our instructions : ?
The summary should reflect common opinions about the product expressed in the reviews .
Try to preserve the common sentiment of the opinions and their details ( e.g. what exactly the users like or dislike ) .
For example , if most reviews are negative about the sound quality , then also write negatively about it .
Please make the summary coherent and fluent in terms of sentence and information structure .
Iterate over the written summary multiple times to improve it , and re-read the reviews whenever necessary .
?
Please write your summary as if it were a review itself , e.g . ' This place is expensive ' instead of ' Users thought this place was expensive ' .
Keep the length of the summary reasonably close to the average length of the reviews .
?
Please try to write the summary using your own words instead of copying text directly from the reviews .
Using the exact words from the reviews is allowed , but do not copy more than 5 consecutive words from a review .
A.7 Latent Codes Analysis
We performed a qualitative analysis of the latent variable z to shed additional light on what it stores and sensitivity of the decoder with respect to its input .
Specifically , we computed the mean value for the variable c using the approximate posterior q ? ( c|r 1 , ... , r N ) , and then sampled z from the prior p ? ( z| c ) .
First , we observed that the summaries produced using the mean of z are more fluent .
For example , in Table 9 , the z 1 based summary states : " The picture quality is very good , but it does n't work aswell as the picture . " , where the second phrase could be rewritten in a more fluent matter .
Also , we found that mean based summaries contain less details that are partially or not supported by the reviews .
For example , in the table , z 1 based summary mentions Kindle Fire HD 8.9 ' , while the dimension is never mentioned in the reviews .
Finally , different samples were observed to result in texts that contain different details about the reviews .
For example , z 1 sample results in the summary that captures the picture quality , while z 3 that the item is good for its price .
Overall , we observed that the latent variable z stores content based information , that results in syntactically diverse texts , yet reflecting information about the same businesses or product .
A.8 Repetitions
We observed an increase in the amount of generated repetitions both in the reconstructed reviews and summaries when the z-related KL term is low and beam search is used .
Intuitively , the initial input to the decoder becomes less informative , and it starts relying on learned local statistics to perform reconstruction .
When the KLD vanishes to zero , the decoder essentially becomes a uncoditional language model , for which beam search was shown to lead to generation of repetitions ( Holtzman et al. , 2019 ) .
Ours
This place is the best Mexican restaurant i have ever been to .
The food was delicious and the staff was very friendly and helpful .
Our server was very attentive and made sure we were taken care of .
We 'll be back for sure .
MeanSum
A little on the pricey side but I was pleasantly surprised .
We went there for a late lunch and it was packed with a great atmosphere , food was delicious and the staff was super friendly .
Very friendly staff .
We had the enchiladas with a few extra veggies and they were delicious !
Will be back for sure !
LexRank
We will definitely be going back for more great food !
Everything we had so far was great .
The staff was great and so nice !
Good food !
Great atmosphere !
Gold
This place is simply amazing !
Its the best Mexican spot in town .
Their tacos are delicious and full of flavor .
They also have chips and salsa that is to die for !
The salsa is just delectable !
It has a sweet , tangy flavor that you ca n't find anywhere else .
I highly recommend !
Rev 1 Classic style Mexican food done nicely !
Yummy crispy cheese crisp with a limey margarita will will win my heart any day of the week !
The classic frozen with a chambord float is my favorite and they do it well here .
The salad carbon was off the chain-served on a big platter and worked for me as 2 full dinners .
Rev 2 For delicious Mexican food in north Phoenix , try La Pinata .
This was our visit here and we were so stunned by the speed in which our food was prepared that we were sure it was meant for another table .
The food was hot and fresh and well within our budget .
My husband got a beef chimichanga and I got bean and cheese burrito , which we both enjoyed .
Chips and salsa arrived immediately ; the salsa tastes sweeter than most and is equally flavorful .
We will be back !
Rev 3 Good food !
Great atmosphere !
Great patio .
Staff was super friendly and accommodating !
We will definately return !
Rev 4
This place was very delicious !
I got the ranchero burro and it was so good .
The plate could feed at least two people .
The staff was great and so nice !
I also got the fried ice cream it was good .
I would recommend this place to all my friends .
Rev 5
We arrive for the first time , greeted immediately with a smile and seated promptly .
Our server was fantastic , he was funny and fast .
Gave great suggestions on the menu and we both were very pleased with the food , flavors , speed and accuracy of our orders .
We will definitely be going back for more great food !
Rev 6
Well was very disappointed to see out favorite ice cream parlor closed but delightfully surprised at how much we like this spot !!
Service was FANTASTIC TOP notch !!
Taco was great lots of cheese .
Freshly deep fried shell not like SO MANY Phoenix mex restaurants use !
Enchilada was very good .
My wife really enjoyed her chimichanga .
My moms chilli reanno was great too .
Everything we had so far was great .
We will return .
Highly recommended .
Rev 7 I 'm only on the salsa and it 's just as fabulous as always .
I love the new location and the decor is beautiful .
Open 5 days and the place is standing room only .
To the previous negative commentor , they are way took busy to fill an order for beans .
Go across the street .... you 'll be angry lol .
Rev 8 I just tried to make a reservation for 15 people in March at 11 am on a Tuesday and was informed by a very rude female .
She said " we do not take reservations " and I asked if they would for 15 people and she said " I told you we do n't take reservations " and hung up on me .
Is that the way you run a business ?
Very poor customer service and I have no intentions of ever coming there or recommending it to my friends .
Ours
This place is the worst service I 've ever had .
The food was mediocre at best .
The service was slow and the waiter was very rude .
I would not recommend this place to anyone who wants to have a good time at this location .
MeanSum
I love the decor , but the food was mediocre .
Service is slow and we had to ask for refills .
They were not able to do anything and not even charge me for it .
It was a very disappointing experience and the service was not good at all .
I had to ask for a salad for a few minutes and the waitress said he did n't know what he was talking about .
All I can say is that the staff was nice and attentive .
I would have given 5 stars if I could .
LexRank Food was just okay , server was just okay .
The atmosphere was great , friendly server .
It took a bit long to get a server to come over and then it took our server a while to get our bread and drinks .
However there was complementary bread served .
The Pizza I ordered was undercooked and had very little sauce .
Macaroni Grill has unfortunately taken a dive .
Went to dinner with 4 others and had another bad experience at the Macaroni Grill .
Gold I 'm really not a fan of Macaroni Grill , well , at least THIS Macaroni Grill .
The staff is slow and really does n't seem to car about providing quality service .
It took well over 30 minutes to get my food and the place was n't even packed with people .
I ordered pizza and it did n't taste right .
I think it was n't fully cooked .
I wo n't be coming back .
Rev 1 10/22/2011 was the date of our visit .
Food was just okay , server was just okay .
The manager climbed up on the food prep counter to fix a light .
We felt like that was the most unsanitary thing anyone could do -he could have just come from the restroom for all we knew .
Needless to say , lackluster service , mediocre food and lack of concern for the cleanliness of the food prep area will guarantee we will NEVER return .
Rev 2
We like the food and prices are reasonable .
Our biggest complaint is the service .
It took a bit long to get a server to come over and then it took our server a while to get our bread and drinks .
They really need to develop a better sense of teamwork .
While waiting for things there were numerous servers standing around gabbing .
It really gave us the impression of " Not my table . " " Not my problem . "
Only other complaint is they need to get some rinse aid for the dishwasher .
I had to dry our bread plates when the hostess gave them to us .
Rev 3
Not enough staff is on hand the two times I have been in to properly pay attention to paying customers .
I agree that the portions have shrunk over the years , and the effort is no longer there .
It is convenient to have nearby but not worth my time when other great restaurants are around .
Wish I could rate it better but it 's just not that good at all .
Rev 4
Went to dinner with 4 others and had another bad experience at the Macaroni Grill .
When will we ever learn ?
The server was not only inattentive , but p o'd when we asked to be moved to another table .
When the food came it was at best , luke warm .
They had run out of one of our ordered dishes , but did n't inform us until 20 minutes after we had ordered .
Running out at 6:00 p.m. : Really ?
More delay and no apologies .
There is no excuse for a cold meal and poor service .
We will not go back since the Grill seems not to care and there are plenty of other restaurants which do .
Rev 5
The service is kind and friendly .
However there was complementary bread served .
The Pizza I ordered was undercooked and had very little sauce .
Macaroni Grill has unfortunately taken a dive .
Best to avoid the place or at the very least this location .
Rev 6 I know this is a chain , but Between this and Olive Garden , I would def pick this place .
Service was great at this location and food not bad at all , although not excellent , I think it still deserves a good 4 stars Rev 7 I had a 2 for 1 $ 9.00 express dinner coupon so we order up 2 dinners to go .
The deal was 9 min or its free , it took 20 , but since I was getting 2 meals for $ 9.00 I did not make a fuss .
The actual pasta was fine and amount was fair but it had maybe a 1/4 of a chicken breast .
The chicken tasted like it came from Taco Bell , VERY processed .
The sauce straight from a can .
I have had much better frozen dinners .
My husband and I used to like Macaroni Grill it sad too see its food go so down hill .
Rev 8
The atmosphere was great , friendly server .
Although the food I think is served from frozen .
I ordered mama trio .
The two of three items were great .
Plate came out hot , could n't touch it .
Went to eat lasagna and was ice cold in the center , nit even warm .
The server apologized about it offered new one or reheat this one .
I chose a new one to go .
I saw her go tell manager .
The manager did n't even come over and say anything .
I was not even acknowledged on my way out and walked past 3 people .
I will not be going back .
Over priced for frozen food .
Table 11 : Yelp summaries produced by different models .
Ours
My wife and i have been here several times now and have never had a bad meal .
The service is impeccable , and the food is delicious .
We had the steak and lobster , which was delicious .
I would highly recommend this place to anyone looking for a good meal .
MeanSum
Our first time here , the restaurant is very clean and has a great ambiance .
I had the filet mignon with a side of mashed potatoes .
They were both tasty and filling .
I 've had better at a chain restaurant , but this is a great place to go for a nice dinner or a snack .
Have eaten at the restaurant several times and have never had a bad meal here .
LexRank
Had the filet ...
Really enjoyed my filet and slobster .
In addition to excellent drinks , they offer free prime filet steak sandwiches .
I have had their filet mignon which is pretty good , calamari which is ok , scallops which are n't really my thing , sour dough bread which was fantastic , amazing stuffed mushrooms .
Very good steak house .
Gold
The steak is the must have dish at this restaurant .
One small problem with the steak is that you want to order it cooked less than you would at a normal restaurant .
They have the habit of going a bit over on the steak .
The drinks are excellent and the stuffed mushrooms as appetizers were amazing .
This is a classy place that is also romantic .
The staff pays good attention to you here .
Rev 1
The ambiance is relaxing , yet refined .
The service is always good .
The steak was good , although not cooked to the correct temperature which is surprising for a steakhouse .
I would recommend ordering for a lesser cook than what you normally order .
I typically order medium , but at donovan 's would get medium rare .
The side dish menu was somewhat limited , but we chose the creamed spinach and asparagus , both were good .
Of course , you have to try the creme brulee - Yum !
Rev 2 Had n't been there in several years and after this visit I remember why , I do n't like onions or shallots in my macaroni and cheese .
The food is good but not worth the price just a very disappointing experience and I probably wo n't go back Rev 3 My wife and I come here every year for our anniversary ( literally every year we have been married ) .
The service is exceptional and the food quality is top-notch .
Furthermore , the happy hour is one of the best in the Valley .
In addition to excellent drinks , they offer free prime filet steak sandwiches .
I highly recommend this place for celebrations or a nice dinner out .
Rev 4 I get to go here about once a month for educational dinners .
I have never paid so do n't ask about pricing .
I have had their filet mignon which is pretty good , calamari which is ok , scallops which are n't really my thing , sour dough bread which was fantastic , amazing stuffed mushrooms .
The vegetables are perfectly cooked and the mashed potatoes are great .
At the end we get the chocolate mousse cake that really ends the night well .
I have enjoyed every meal I have eaten there .
Rev 5
Very good steak house .
Steaks are high quality and the service was very professional .
Attentive , but not hovering .
Classic menus and atmosphere for this kind of restaurant .
No surprises .
A solid option , but not a clear favorite compared to other restaurants in this category .
Rev 6
Had a wonderful experience here last night for restaurant week .
Had the filet ...
Which was amazing and cooked perfectly with their yummy mashed potatoes and veggies .
The bottle of red wine they offered for an additional $ 20 paired perfectly with the dinner .
The staff were extremely friendly and attentive .
Ca n't wait to go back !
Rev 7
The seafood tower must change in selection of seafood , which is good , which is also why mine last night was so fresh fresh delicious .
Its good to know that you can get top rate seafood in Phoenix .
Bacon wrapped scallops were very good , and I sacrificied a full steak ( opting for the filet medallion ) to try the scallops .
I asked for medium rare steak , but maybe shouldve asked for rare ... my cousin had the ribeye and could not have been any happier than he was :) yum for fancy steak houses .
Its an ultra romantic place to , fyi . the wait staff is very attentive .
Rev 8 Donovans , how can you go wrong .
Had some guests in town and some fantastic steaks paired with some great cabernets .
Really enjoyed my filet and lobster .
Table 12 : Yelp summaries produced by different models .
Great service ... ||
I really love this place ... C?te de Boeuf ...
A Jewel in the big city ... ||
French jewel of Spadina and Adelaide , Jules ...
They are super accommodating ... moules and frites are delicious ... ||
Food came with tons of greens and fries along with my main course , thumbs uppp ... ||
Chef has a very cool and fun attitude ... || Great little French Bistro spot ...
Go if you want French bistro food classics ... ||
Great place ... the steak frites and it was amazing ... Best Steak Frites ... in Downtown Toronto ... ||
Favourite french spot in the city ... cr?me brule for dessert
8 u I 5 g O S I + x t 5 p I l e 3 v H 7 p 4 Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D
8 u I 5 g O S I + x t 5 p I l e 3 v H 7 p 4 Q j v w E G w t F b P 1 F d v 4 b N 8 k V m v h g 4 P H e D D P z g k R w b V z 3 2 y l s b e / s 7 h X 3 S w e H R 8 c n 5 d O z j o 5 T x b D
r 5 N 2 v e Z d 1 u r 3 X r V R y e M o w h l U 4 A I 8 u I I G 3 E I T W s B g B M / w C m + O c F 6 c d + d j 2 V p w 8 p l T + A P n 8 w c l P o 2 Y < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = " + u 2 Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P
y e M o w h l U 4 A I 8 u I I G 3 E I T W s B g B M / w C m + O c F 6 c d + d j 2 V p w 8 p l T + A P n 8 w c l P o 2 Y < / l a t e x i t >
Conditional independence of the reviews given the group representation c .
z
4 c 6 b w 4 7 8 7 H o n X N y W d O 4 A + c z x 8 x b o 2 g < / l a t e x i t >
w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 mo P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s vr 5 N 2 v e Z d 1 u r 3 X r V R y e M o w h l U 4 A I 8 u I I G 3 E I T W s B g B M / w C m + O c F 6 c d + d j 2 V p w 8 p l T + A P n 8 w c l P o 2 Y < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = " + u 2 Z E i 3 F 1 8 m U Z u F 5 m e 4 Z 7 P
w E 4 w u Z n 7 n S d U m s f y 0 U w T 9 C M 6 k j z k j B o r P a j B 3 a B c d W v u A m S d e D m p Q o 7 mo P z V H 8 Y s j V A a J q j W P c 9 N j J 9 R Z T g T O C v 1 U 4 0 J Z R M 6 w p 6 l k k a o / W x x 6 o y c W 2 V I w l j Z k o Y s 1 N 8 T G Y 2 0 n k a B 7 Y y o G e t V b y 7 + 5 / V S E 1 7 7 G Z d J a l C y 5 a I w F c T E Z P 4 3 G X K F z I i p J Z Q p b m 8 l b E w V Z c a m U 7 I h e K s vr 5 N 2 v e Z d 1 u r 3 X r V R y e M o w h l U 4 A I 8 u I I G 3 E I T W s B g B M / w C m + O c F 6 c d + d j 2 V p w 8 p l T + A P n 8 w c l P o 2 Y < / l a t e x i t >
The ri's decoder accesses other reviews of the group ( r1 , ... , ri?1 , ri +1 , ... , rN ) .
Figure 1 : 1 Figure 1 : Unfolded graphical representation of the model .
Table 1 : 1 A summary produced by our model ; colors encode its alignment to the input reviews .
The reviews are truncated , and delimited with the symbol ' | |' .
Table 2 : 2 , and the corresponding data statistics are Data statistics after pre-processing .
The format in the cells is Businesses / Reviews and Products / Reviews for Yelp and Amazon , respectively .
Dataset Training Validation Yelp 38,776/1,012,280 4,311/113,373 Amazon
183,103/4,566,519 9,639/240,819
Table 3 : 3 ROUGE scores on the Yelp test set . .
The reported ROUGE scores are based on F1 ( see Appendix A.3 for details on hy- perparameters ) .
Table 4 : 4 ROUGE scores on the Amazon test set .
R1 R2 RL Copycat 0.3197 0.0581 0.2016 MeanSum 0.2920 0.0470 0.1815 LexRank 0.2874 0.0547 0.1675 Opinosis 0.2842 0.0457 0.1550 VAE 0.2287 0.0275 0.1446 Clustroid 0.2928 0.0441 0.1778 Lead 0.3032 0.0590 0.1578 Random 0.2766 0.0472 0.1695 Oracle 0.3398 0.0788 0.2160
Table 5 : 5 Human evaluation results in terms of the Best-Worst scaling on the Yelp dataset . ) .
Crowdwork -
Table 6 : 6 Human evaluation results in terms of the Best-Worst scaling on the Amazon dataset .
Table 7 : 7 Table 7 indicate that our model is better at preserving information than MeanSum .
Content support on Yelp and Amazon datasets , percentages .
Yelp Amazon Copycat MeanSum Copycat MeanSum Full 44.50 28.41 38.23 24.41 Partial 32.48 30.66 33.95 31.23 No 23.01 40.92 27.83 44.36
Table 8 : 8 Ablations , ROUGE scores on Amazon .
R1 R2 RL w/o r i 0.2866 0.0454 0.1863 w/o c 0.2767 0.0507 0.1919 w/o z 0.2926 0.0416 0.1739 Sampling 0.2563 0.0434 0.1716 Full 0.3197 0.0581 0.2016
Table 10 : 10
Yelp summaries produced by different models .
For simplicity , we refer to both products ( e.g. , i Phone X ) and businesses ( e.g. , a specific Starbucks branch ) as products .
Data and code : https://github.com/ixlan/ Copycat-abstractive-opinion-summarizer .
We use FFNNs with the tanh non-linearity in several model components .
Whenever a FFNN is mentioned in the subsequent discussion , this architecture is assumed .
For experiments on Yelp , we used the checkpoint provided by the authors , as we obtained very similar ROUGE scores when retraining the model .
