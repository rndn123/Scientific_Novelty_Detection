title
Examining the State-of- the - Art in News Timeline Summarization
abstract
Previous work on automatic news timeline summarization ( TLS ) leaves an unclear picture about how this task can generally be approached and how well it is currently solved .
This is mostly due to the focus on individual subtasks , such as date selection and date summarization , and to the previous lack of appropriate evaluation metrics for the full TLS task .
In this paper , we compare different TLS strategies using appropriate evaluation frameworks , and propose a simple and effective combination of methods that improves over the stateof - the - art on all tested benchmarks .
For a more robust evaluation , we also present a new TLS dataset , which is larger and spans longer time periods than previous datasets .
The dataset will be made available at https://github.
com/complementizer/news-tls .
Introduction
Timelines of news events can be useful to condense long- ranging news topics and can help us understand how current major events follow from prior events .
Timeline summarization ( TLS ) aims to automatically create such timelines , i.e. , temporally ordered time -stamped textual summaries of events focused on a particular topic .
While TLS has been studied before , most works treat it as a combination of two individual subtasks , 1 ) date selection and 2 ) date summarization , and only focus on one of these at a time ( Tran et al. , 2013a ( Tran et al. , , b , 2015 b .
However , these subtasks are almost never evaluated in combination , which leaves an unclear picture of how well TLS is being solved in general .
Furthermore , previously used evaluation metrics for the date selection and timeline summarization tasks are not appropriate since they do not consider the temporal alignment in the evaluation .
Just until recently , there were no established experimental settings and appropriate metrics for the full TLS task Markert , 2017 , 2018 ) .
In this paper , we examine existing strategies for the full TLS task and how well they actually work .
We identify three high- level approaches :
1 ) Direct summarization treats TLS like text summarization , e.g. , by selecting a small subset of sentences from a massive collection of news articles ;
2 ) The date- wise approach first selects salient dates and then summarizes each date ; 3 ) Event detection first detects events , e.g. , via clustering , selects salient events and summarizes these individually .
The current state - of - the - art method is based on direct summarization ( Martschat and Markert , 2018 ) .
We therefore focus on testing the two remaining strategies , which have not been appropriately evaluated yet and allow for better scalability .
We propose a simple method to improve date summarization for the date-wise approach .
The method uses temporal expressions ( textual references to dates ) to derive date vectors , which in turn help to filter candidate sentences to summarize particular dates .
With this modification , the date- wise approach obtains improved state - of - the - art results on all tested datasets .
We also propose an event-based approach via clustering , which outperforms ( Martschat and Markert , 2018 ) on one of three tested datasets .
We use purpose- build evaluation metrics for evaluating timelines introduced by Martschat and Markert ( 2017 ) .
For a more robust evaluation , we also present a new dataset for TLS , which is significantly larger than previous datasets in terms of the number of individual topics and time span .
We summarize our contributions as follows : 1 . We compare different TLS strategies side-byside using suitable evaluation metrics to provide a better picture for how well the full TLS task for news is solved so far .
2 . We propose a simple addition to existing methods to significantly improve date- wise TLS , achieving new state - of - the - art results .
3 . We present a new TLS dataset that is larger than previous datasets and spans longer time ranges ( decades of news timelines ) .
Related Work Timeline summarization for news articles has received some attention in the last two decades ( Swan and Allan , 2000 ; Allan et al. , 2001 ; Chieu and Lee , 2004 ; Yan et al. , 2011a , b ; Kessler et al. , 2012 ; Tran et al. , 2013a , b ; Li and Li , 2013 ; Tran et al. , 2015 a , b ; Wang et al. , 2015 Wang et al. , , 2016 Markert , 2017 , 2018 ; Steen and Markert , 2019 ) .
The task is commonly split into date selection and date summarization subtasks .
Date Selection Supervised machine learning has been proposed to predict whether dates appear in ground - truth timelines ( Kessler et al. , 2012 ; Tran et al. , 2013a ) .
Tran et al. ( 2015 b ) use graph - based ranking of dates , which is reported to outperform supervised methods 1 .
Date Summarization Several approaches construct date summaries by picking sentences from ranked lists .
The ranking is based on regression or learning - to - rank to predict ROUGE scores between the sentence and a ground - truth summary ( Tran et al. , 2013 a , b) et al. , 2015 ) , or images ( Wang et al. , 2016 ) .
Full Timeline Summarization Chieu and Lee ( 2004 ) produce timelines by ranking sentences from an entire document collection .
The ranking is based on summed up similarities to other sentences in an n-day window .
marized ( Wang et al. , 2014 ; Li and Cardie , 2014 ) .
We explore a similar framework for evaluating clustering - based TLS .
Strategies for Timeline Summarization Problem Definition
We define the TLS setup and task as follows .
Given is a set of news articles A , a set of query keyphrases Q , and a ground- truth ( reference ) timeline r , with l dates that are associated with k sentences on average , i.e. , m = k * l sentences in total .
The task is to construct a ( system ) timeline s that contains m sentences , assigned to an arbitrary number of dates .
A simpler and stricter setting can also be used , in which s must contain exactly l dates with k sentences each .
Approach Types
A number of different high- level approaches can be used to tackle this task : 1 . Direct Summarization :
A is treated as one set of sentences , from which a timeline is directly extracted , e.g. , by optimizing a sentence combination ( Martschat and Markert , 2018 ) , or by sentence ranking ( Chieu and Lee , 2004 ) .
Among these , Martschat and Markert ( 2018 ) 's solution for the full TLS task has state - of - theart accuracy but does not scale well .
2 . Date-wise Approach :
This approach selects l dates and then constructs a text summary of k sentences on average for each date .
3 . Event Detection :
This approach first detects events in A , e.g. , by clustering similar articles , and then identifies the l most important events and summarizes these separately .
Since no prior work has analyzed the latter two categories for the full TLS task , we discuss and develop such approaches next .
Date-wise Approach
The approach described here mostly consists of existing building blocks , with a few but important modifications proposed from our side .
Defining the Set of Dates First , we identify the set of possible dates to include in a timeline .
We obtain these from ( i ) the publication dates of all articles in A and ( ii ) textual references of dates in sentences in A , such as ' last Monday ' , or ' 12 April ' .
We use the tool Hei-delTime 2 ( Str?tgen and Gertz , 2013 ) to detect and resolve textual mentions of dates .
2 https://github.com/HeidelTime/ heideltime
Date Selection Next , we select the l most important dates .
We compare the following date selection methods introduced by Tran et al . ( 2013 a ) : ? PUBCOUNT : Ranking dates by the number of articles published on a date .
?
MENTIONCOUNT : Ranking dates by the number of sentences that mention the date .
?
SUPERVISED : Extracting date features and using classification or regression to predict whether a date appears in a ground -truth timeline .
These features mostly include the publication count and different variants of counting date mentions .
Our experiments show that SUPERVISED works best , closely followed by MENTIONCOUNT ( Appendix A.1 ) .
Figure 1 shows an example of publication and date mention counts and ground -truth dates over time .
Two challenges are evident that date selection methods face : 1 ) These count signals usually do not perfectly correlate with groundtruth dates , and 2 ) high values often cluster around important dates , i.e. , a " correct " date is often surrounded by other , " incorrect " dates with similarly strong signals .
Candidate Sentences for Dates
To summarize a particular date d , we first need to decide which articles or sentences we use as a source to create a summary from .
Previous research has not explored this aspect much due to the separated treatment of subtasks .
We propose a simple but effective heuristic to do this .
We consider the following two sets to be the primary source of suitable candidate sentences : ?
P d : Sentences published on or closely after d .
These often contain initial reports of events occurring on d. ?
M d : Sentences that mention d .
These sentences are from articles published at any point in time , and may retrospectively refer to d , or announce events on d beforehand 3 .
We evaluate these two options in our experiments , and propose an heuristic that combines these , which we call PM - MEAN .
We aim to find a subset of sentences in P d ?
M d that are likely to mention important events happening on d .
We convert all the sentences in the collection A to sparse bag-of-words ( unigram ) vectors with sentence - level TF - IDF weighting .
We represent the sets of sentences P d and M d using the mean of their respective sentence vectors , x P d and x M d .
The core assumption of the method is that the content shared between P d and M d is a good source for summarizing events on d .
To capture this content , we build a date vector x d , so that we can compare sentence vectors against it to rank sentences .
We set the value of x d for each dimension i in the feature space as follows : x i d = 1 | P d | x i P d + 1 | M d | x i M d if x i P d > 0 and x i M d > 0 0 otherwise ( 1 ) Thus the date vector x d is an average of x P d and x M d weighted by the sizes of P d and M d , with any features zeroed out if they are missing in either P d or M d .
To rank sentences , we compute the cosine similarity between the vector x s of each candidate sentence s ?
( P d ? M d ) to x d .
We select the best-scoring candidate sentences by defining a threshold on this similarity .
To avoid tuning this threshold , we use a simple knee point detection method ( Satopaa et al. , 2011 ) to dynamically identify a threshold that represents the " knee " ( or elbow ) in the similarity distribution .
This set of best-scoring sentences is then used as the input for the final date summarization step .
Date Summaries
To construct the final timeline , we separately construct a summary for the l highest ranked dates .
Prior to our main experiments , we test several multi-document summarization algorithms : ? TEXTRANK : Runs PageRank on a graph of pairwise sentences similarities to rank sentences ( Mihalcea and Tarau , 2004 ) . ? CENTROID-RANK : Ranks sentences by their similarity to the centroid of all sentences ( Radev et al. , 2004 ) . ? CENTROID-OPT : Greedily optimises a summary to be similar to the centroid of all sentences ( Ghalandari , 2017 ) .
? SUBMODULAR : Greedily optimizes a summary using submodular objective functions that represent coverage and diversity ( Lin and Bilmes , 2011 ) .
The only modification to these algorithms in our TLS pipeline is that we prevent sentences not containing any topic keyphrases from query Q to be included in the summary .
CENTROID -OPT has the best results ( Appendix A.1 ) and is used in the main experiments .
Timeline Construction
The date- wise approach constructs a timeline as follows : first , rank all potential dates using one of the date selection approaches described , then pick the l highest ranked ones , pick candidate sentences for each date , and summarize each date individually from the according candidate set , using k sentences .
We might not be able to summarize a particular date due to the keyword constraint in the summarization step .
Whenever this is the case , we skip to the next date in the ranked list , until l is reached .
Event Detection Approach
When humans are tasked with constructing a timeline , we expect that they reason over important events rather than dates .
Conceptually , detecting and selecting events might also be more appropriate than selecting dates because multiple events can happen on the same day , and an event can potentially span multiple days .
To explore this , we test a TLS approach based on event detection by means of article clustering .
The general approach can be summarized as follows : ( 1 ) Group articles into clusters ; ( 2 ) Rank and select the l most important clusters ; ( 3 ) Construct a summary for each cluster .
Similarly to the date- wise approach , this mostly consists of existing building blocks that we adapt for TLS .
Clustering
For each input collection A , we compute sparse TF - IDF unigram bag-of-words vectors for all articles in A .
We apply clustering algorithms to these vectors .
To cluster articles , we use Markov Clustering ( MCL ) with a temporal constraint .
MCL ( Van Dongen , 2000 ) is a clustering algorithm for graphs , i.e. , a community detection algorithm .
It is based on simulating random walks along nodes in a graph .
Ribeiro et al. ( 2017 ) use this approach for clustering news articles .
We convert A into a graph where nodes correspond to articles so that we can cluster the articles using MCL , with the following temporal constraint : Articles a 1 , a 2 are assigned an edge if their publication dates are at most 1 day apart from each other .
The edge weight is set to the cosine similarity between the TF -IDF bag-of-words vectors of a 1 and a 2 .
The constraint on the publication dates ensures that clusters do not have temporal gaps .
Furthermore , it reduces the number of similarity computations between pairs of articles considerably .
We run MCL on this graph and obtain clusters by identifying the connected components in the resulting connectivity matrix 4 .
Assigning Dates to Clusters
We define the cluster date as the date that is most frequently mentioned within articles of the cluster .
We identify date mentions using the HeidelTime tool .
Cluster Ranking
To construct a timeline , we only need the l most important clusters .
We obtain these by ranking and retaining the top -l clusters of the ranked list .
We test the following scores to rank clusters by : ? SIZE : Rank by the numbers of articles in a cluster .
? DATEMENTIONCOUNT : Rank by how often the cluster date is mentioned throughout the input collection .
? REGRESSION : Rank using a score by a regression model trained to predict importance scores of clusters .
For the regression - based ranking method , we represent clusters using the following features : number of articles in a cluster ; number of days between the publication dates of the first and last article in the cluster ; maximum count of publication dates of articles within a cluster ; maximum mention count of dates mentioned in articles in a cluster ; sum of mention counts of dates mentioned in articles in a cluster .
We test two approaches to label clusters with target scores to predict .
?
Date-Accuracy :
This is 1 if the cluster date appears in the ground- truth , else 0 . ? ROUGE : The ROUGE -1 F1 - score 5 between the summary of the cluster and the groundtruth summary of the cluster date .
If the cluster date does not appear in the ground - truth , the score is set to 0 .
We evaluate these different options ( Appendix A.2 ) and observe that ranking by DATEMENTION - COUNT works better than the supervised methods , showing that predicting the suitability of clusters for timelines is difficult .
Cluster Summarization
We use the same multi-document summarization method that works best for the date- wise approach ( CENTROID - OPT ) .
Timeline Construction
In summary , the clustering approach builds a timeline as follows : 1 ) cluster all articles , 2 ) rank clusters , 3 ) build a summary with k sentences for the top -l clusters , skipping clusters if a summary cannot be constructed due to missing keywords .
Furthermore , we skip clusters if the date assigned to the cluster is already " used " by a previously picked cluster .
Conceptually , this implies that we can only recognize one event per day .
In initial experiments , this leads to better results than alternatives , e.g. , allowing multiple summaries of length k per day .
Dataset Tran et al. introduced the 17 Timelines ( T17 ) ( Tran et al. , 2013a ) and the CRISIS ( Tran et al. , 2015a ) datasets for timeline summarization from news articles .
However , we see the need for better benchmarks due to 1 ) a small number of topics in the T17 and CRISIS datasets ( 9 and 4 topics respectively ) , and 2 ) relatively short time span , ranging from a few months to 2 years .
Therefore , we build a new TLS dataset , called ENTITIES , that contains more topics ( 47 ) and longer time - ranges per topic , e.g. , decades of news articles .
In the following , we describe how we obtain ground -truth timelines and input article collections for this dataset .
Ground -Truth Timelines :
We obtain groundtruth timelines from CNN Fast Facts 6 , which has a collection of several hundred timelines grouped in categories , e.g. , ' people ' or ' disasters ' .
We pick all timelines of the ' people ' category and a small number from other categories .
Queries :
For each ground - truth timeline , we define a set of query keyphrases Q. By default , we use the original title of the timeline as the keyphrase .
For people entities , we use the last token of the title to capture surnames only , which increases the coverage .
We manually inspect the resulting sets of keyphrases and correct these if necessary .
Input Articles :
For each entity from the groundtruth timelines , we search for news articles using TheGuardian API 7 .
We use this source because it provides access to all published articles starting from 1999 .
We search for articles that have exact matches of the queries in the article body .
The timespan for the article search is set so that it extends the ground - truth timeline by 10 % of its days before its first and after its last date .
Adjustments and Filtering :
The ground - truth timelines are modified to be usable for TLS and to ensure they do not contain data not present in the document collection : ?
We remove entries in the ground - truth timelines if they do not specify year , month , and day of an event .
?
Ground - truth timelines are truncated to the first and last date of the input articles .
?
Entries in the ground - truth timeline are removed if there is no input article published within ?
2 days .
Afterwards , we remove all topics from the dataset that do not fulfill the following criteria : ?
The timeline must have at least 5 entries .
?
For at least 50 % of the dates present in the ground - truth timeline , textual references have to be found in the article collection ( e.g. , 'on Wednesday ' or 'on 1 August ' . ) .
This is done to ensure that the content of the timelines is reflected to some degree in the article collection .
?
There are at least 100 and less than 3000 articles containing the timeline-entity in the input articles .
This is done to reduce the running time of experiments .
Dataset Characteristics :
Tables 2 and 3 give an overview of properties of the two existing datasets and our new dataset , and mostly show averaged values over tasks in a dataset .
An individual task corresponds to one ground - truth timeline that a TLS algorithm aims to simulate .
# P ubDates refers to the number of days in an article collection A on which any articles are published .
The compression ratio w.r.t. sentences ( " comp. ratio ( sents ) " ) is m divided by the total number of sentences in A , and the compression ratio w.r.t dates is l divided by # P ubDates .
" Avg. date cov " refers to the average coverage of dates in the ground - truth timeline r by the articles in A .
This can be counted by using publication dates in A , ( " published " ) , or by textual date references to dates within articles in A ( " mentioned " ) .
The fact that there are generally more ground - truth dates covered in textual date references compared to publication dates suggests making use of these date mentions .
T17 has longer ( l ) , and more detailed ( k ) timelines than the other datasets , CRISIS has more articles per task , and ENTITIES has more topics , publication dates and longer time periods per task .
Experiments
Evaluation Metrics
In our experiments , we measure the quality of generated timelines with the following two evaluation metrics , which are also used by Martschat and Markert ( 2018 ) : ? Alignment - based ROUGE F1 -score :
This metric compares the textual overlap between a system and a ground - truth timeline , while also considering the assignments of dates to texts .
? Date F1 - score :
This metric compares only the dates of a system and a ground -truth timeline .
We denote the alignment - based ROUGE -1 F1score as AR1 - F and Date F1 -score as Date - F1 .
Experimental Settings
Concerning the datasets and task , we follow the experimental settings of Martschat and Markert ( 2018 ) : ?
Each dataset is divided into multiple topics , each having at least one ground -truth timeline .
If a topic has multiple ground - truth timelines , we split the topic into multiple tasks .
The final results in the evaluation are based on averages over tasks / ground - truth timelines , not over topics .
?
Each task includes a set of news articles A , a set of keyphrases Q , a ground - truth timeline r , with number of dates ( length ) l , average number of summary sentences per date k , and total number of summary sentences m = l * k. ?
In each task , we remove all articles from A whose publication dates are outside of the range of dates of the ground - truth timeline r of the task .
Article headlines are not used . ?
We run leave- one - out cross-validation over all tasks of a dataset . ?
We test for significant differences using an approximate randomization test ( Marcus et al. , 1993 ) with a p-value of 0.05 .
We use the following configurations for our methods : ?
A stricter and simpler version of the output size constraint :
We produce timelines with the number of dates l and k sentences per date . ?
In the summarization step of our methods , we only allow a sentence to be part of a summary if it contains any keyphrase in Q .
As opposed to Martschat and Markert ( 2018 ) , we still keep sentences not matching Q , e.g. , for TF - IDF computation , clustering , and computing date vectors .
Methods Evaluated
We compare the following types of methods to address the full news TLS task .
Direct summarization approaches : ? CHIEU2004 : Chieu and Lee ( 2004 )
An unsupervised baseline based on direct summarization .
We use the reimplementation from Martschat and Markert ( 2018 ) . ? MARTSCHAT2018 : Martschat and Markert ( 2018 ) State - of - the - art method on the CRISIS and T17 datasets .
It greedily selects a combination of sentences from the entire collection A maximizing submodular functions for content coverage , textual and temporal diversity , and a high count of date references 8 .
Date-wise approaches : ? TRAN 2013 ( Tran et al. , 2013a ) :
The original date-wise approach , using regression for both date selection and summarization , and using all sentences of a date as candidate sentences .
? PUBCOUNT : A simple date- wise baseline that uses the publication count to rank dates , and all sentences published on a date for candidate selection .
We use CENTROID -OPT for summarization .
? DATEWISE : Our date- wise approach after testing different building blocks ( see Appendix A.1 ) .
It uses supervised date selection , PM - MEAN for candidate selection and CENTROID -OPT for summarization .
Event detection approach based on clustering : ? CLUST : We use DATEMENTIONCOUNT to rank clusters , and CENTROID -OPT for summarization , which are the best options according to our tests ( see Appendix A.2 ) .
Note that all methods apart from DATEWISE and CLUST have been proposed previously .
Oracles :
To interpret the alignment - based ROUGE scores better and to approximate their upper bounds , we measure the performance of three different oracle methods : ? DATE ORACLE : Selects the correct ( groundtruth ) dates and uses CENTROID -OPT for date summarization .
?
TEXT ORACLE : Uses regression to select dates , and then constructs a summary for each date by optimizing the ROUGE to the groundtruth summaries .
?
FULL ORACLE : Selects the correct dates and constructs a summary for each date by optimizing the ROUGE to the ground -truth summaries .
We give more detail about these in Appendix A.3 .
Results
Table 4 shows the final evaluation results .
We reproduced the results of CHIEU2004 and MARTSCHAT2018 reported by Martschat and Markert ( 2018 ) using their provided code 9 .
The other results are based on our implementations .
Table 10 in Appendix A.6 shows several output examples across different methods .
6 Analysis and Discussion
Performance of TLS Strategies
Among the methods evaluated , DATEWISE consistently outperforms all other methods on all tested datasets in the alignment - based ROUGE metrics .
The Date - F1 metric for this method is close to other methods , and not always better , which shows that the advantage of DATEWISE is due to the sentence selection ( based on our heuristic date vectors ) and summarization .
Note that the date selection method is identical to TRAN2013 .
We conclude from these results that the expensive combinatorial optimization used in MARTSCHAT2018 is not necessary to achieve high accuracy for news TLS .
CLUST performs worse than DATEWISE and MARTSCHAT2018 , except on ENTITIES , where it outperforms MARTSCHAT2018 .
We find that for the other two datasets , CLUST often merges articles from close dates together that would belong to separate events on ground - truth timelines , which may suggest that a different granularity of clusters is required depending on the task .
DATE ORACLE and FULL ORACLE should theoretically have a 100 % Date -F1 .
In practice , their Date - F1 scores turn out lower because , for some dates , no candidate sentences that match query Q 9 With the exception of CRISIS due to memory issues .
can be found , which causes the dates to be omitted from the oracle timelines .
Based on the performance of different systems , the hardest dataset is ENTITIES , followed by CRI-SIS .
What makes TLS difficult ?
While the ranking of methods is fairly stable , the performance of all methods varies a lot across the datasets and across individual tasks within datasets .
To find out what makes individual tasks difficult , we measure the Spearman correlation between AR1 - F and several dataset statistics .
The details are included in Appendix A.5 .
The correlations show that a high number of articles and publication dates and a low compression ratio w.r.t to dates generally decreases performance .
This implies that highly popular topics are harder to summarize .
The duration of a topic also corresponds to lower performance , but in a less consistent pattern .
The generally low performance across tasks and methods is likely influenced by the following factors : ?
The decision for human editors to include particular events in a timeline and to summarise these in a particular way can be highly subjective .
Due to the two -stage nature of TLS , this problem is amplified in comparison to regular text summarization .
?
Article collections can be insufficient to cover every important event of a topic , e.g. , due to the specific set of news sources or the search technique used .
Running Time DATEWISE and CLUST are up to an order of magnitude faster to run than MARTSCHAT2018 ( Appendix A.4 ) since their date summarization steps only involve a small subset of sentences in an article collection .
Adjacent Dates and Redundancy Automatically constructed timelines often contain a high amount of multiple adjacent dates , while this is not the case in ground - truth timelines .
Summaries of such adjacent dates often tend to refer to the same event and introduce redundancy into a timeline .
To quantify this , we count the proportion of those " date bigrams " in a chronologically ordered timeline , which are only 1 day apart .
The results ( see Table 5 ) show that this is an issue for MARTSCHAT2018 and DATEWISE , but less so for CLUST , which is designed to avoid this behavior .
Note that MARTSCHAT2018 includes an objective function to reward diversity within a timeline , while DATEWISE has no explicit mechanism against redundancy among separate dates .
Interestingly , when forcing DATEWISE to avoid selecting adjacent dates ( by skipping such dates in the ranked list ) , the performance in all metrics decreases .
In this case , high redundancy is a safer strategy for optimizing TLS metrics compared to enforcing a more balanced spread over time .
Because of such effects , we advise to use automated evaluation metrics for TLS with care and to conduct qualitative analysis and user studies where possible .
Use of Titles
While using article titles can make timelines more readable and understandable ( Tran et al. , 2015a ) , we do not involve titles in our main experiments , in order to directly compare to MARTSCHAT2018 , and due to the lack of titles in T17 .
The last row in Table 4 shows the results of a separate experiment with DATEWISE in which we build date summaries using titles only .
Using only titles generally increases AR Precision at the cost of Recall .
AR - F is negatively affected in CRISIS but does not change in ENTITIES .
Figure 1 shows parts of a title - based timeline produced by DATEWISE .
Conclusion
In this study , we have compared and proposed different strategies to construct timeline summaries of long- ranging news topics : the previous stateof - the - art method based on direct summarization , a date - wise approach , and a clustering - based approach .
By exploiting temporal expressions , we have improved the date- wise approach and yielded new state - of - the - art results on all tested datasets .
Hence , we showed that an expensive combinatorial search over all sentences in a document collection is not necessary to achieve good results for news TLS .
For a more robust and diverse evaluation , we have constructed a new TLS dataset with a much larger number of topics and with longer time - spans than in previous datasets .
Most of the generated timelines are still far from oracle timeline extractors and leave large gaps for improvements .
Potential future directions include a more principled use of our proposed heuristic for detecting content relevant to specific dates , the use of abstractive techniques , a more effective treatment of the redundancy challenge , and extending the new dataset with multiple sources .
