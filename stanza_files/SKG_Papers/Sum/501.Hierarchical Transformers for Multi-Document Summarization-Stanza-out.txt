title
Hierarchical Transformers for Multi-Document Summarization
abstract
In this paper , we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries .
Our model augments a previously proposed Transformer architecture with the ability to encode documents in a hierarchical manner .
We represent cross-document relationships via an attention mechanism which allows to share information as opposed to simply concatenating text spans and processing them as a flat sequence .
Our model learns latent dependencies among textual units , but can also take advantage of explicit graph representations focusing on similarity or discourse relations .
Empirical results on the WikiSum dataset demonstrate that the proposed architecture brings substantial improvements over several strong baselines .
1
Introduction Automatic summarization has enjoyed renewed interest in recent years , thanks to the popularity of neural network models and their ability to learn continuous representations without recourse to preprocessing tools or linguistic annotations .
The availability of large-scale datasets ( Sandhaus , 2008 ; Hermann et al. , 2015 ; Grusky et al. , 2018 ) containing hundreds of thousands of documentsummary pairs has driven the development of neural architectures for summarizing single documents .
Several approaches have shown promising results with sequence - to-sequence models that encode a source document and then decode it into an abstractive summary ( See et al. , 2017 ; Celikyilmaz et al. , 2018 ; Paulus et al. , 2018 ; Gehrmann et al. , 2018 ) .
Multi-document summarization - the task of producing summaries from clusters of themati-cally related documents - has received significantly less attention , partly due to the paucity of suitable data for the application of learning methods .
High-quality multi-document summarization datasets ( i.e. , document clusters paired with multiple reference summaries written by humans ) have been produced for the Document Understanding and Text Analysis Conferences ( DUC and TAC ) , but are relatively small ( in the range of a few hundred examples ) for training neural models .
In an attempt to drive research further , tap into the potential of Wikipedia and propose a methodology for creating a large-scale dataset ( WikiSum ) for multidocument summarization with hundreds of thousands of instances .
Wikipedia articles , specifically lead sections , are viewed as summaries of various topics indicated by their title , e.g. , " Florence " or " Natural Language Processing " .
Documents cited in the Wikipedia articles or web pages returned by Google ( using the section titles as queries ) are seen as the source cluster which the lead section purports to summarize .
Aside from the difficulties in obtaining training data , a major obstacle to the application of end-to - end models to multi-document summarization is the sheer size and number of source documents which can be very large .
As a result , it is practically infeasible ( given memory limitations of current hardware ) to train a model which encodes all of them into vectors and subsequently generates a summary from them .
propose a two -stage architecture , where an extractive model first selects a subset of salient passages , and subsequently an abstractive model generates the summary while conditioning on the extracted subset .
The selected passages are concatenated into a flat sequence and the Transformer ( Vaswani et al. , 2017 ) , an architecture well -suited to language modeling over long sequences , is used to decode the summary .
Although the model of takes an important first step towards abstractive multidocument summarization , it still considers the multiple input documents as a concatenated flat sequence , being agnostic of the hierarchical structures and the relations that might exist among documents .
For example , different web pages might repeat the same content , include additional content , present contradictory information , or discuss the same fact in a different light ( Radev , 2000 ) .
The realization that cross-document links are important in isolating salient information , eliminating redundancy , and creating overall coherent summaries , has led to the widespread adoption of graph - based models for multi-document summarization ( Erkan and Radev , 2004 ; Christensen et al. , 2013 ; Wan , 2008 ; Parveen and Strube , 2014 ) .
Graphs conveniently capture the relationships between textual units within a document collection and can be easily constructed under the assumption that text spans represent graph nodes and edges are semantic links between them .
In this paper , we develop a neural summarization model which can effectively process multiple input documents and distill abstractive summaries .
Our model augments the previously proposed Transformer architecture with the ability to encode multiple documents in a hierarchical manner .
We represent cross-document relationships via an attention mechanism which allows to share information across multiple documents as opposed to simply concatenating text spans and feeding them as a flat sequence to the model .
In this way , the model automatically learns richer structural dependencies among textual units , thus incorporating well - established insights from earlier work .
Advantageously , the proposed architecture can easily benefit from information external to the model , i.e. , by replacing inter-document attention with a graph-matrix computed based on the basis of lexical similarity ( Erkan and Radev , 2004 ) or discourse relations ( Christensen et al. , 2013 ) .
We evaluate our model on the WikiSum dataset and show experimentally that the proposed architecture brings substantial improvements over several strong baselines .
We also find that the addition of a simple ranking module which scores documents based on their usefulness for the target summary can greatly boost the performance of a multi-document summarization system .
Related Work Most previous multi-document summarization methods are extractive operating over graph - based representations of sentences or passages .
Approaches vary depending on how edge weights are computed e.g. , based on cosine similarity with tf -idf weights for words ( Erkan and Radev , 2004 ) or on discourse relations ( Christensen et al. , 2013 ) , and the specific algorithm adopted for ranking text units for inclusion in the final summary .
Several variants of the PageRank algorithm have been adopted in the literature ( Erkan and Radev , 2004 ) in order to compute the importance or salience of a passage recursively based on the entire graph .
More recently , Yasunaga et al. ( 2017 ) propose a neural version of this framework , where salience is estimated using features extracted from sentence embeddings and graph convolutional networks ( Kipf and Welling , 2017 ) applied over the relation graph representing cross-document links .
Abstractive approaches have met with limited success .
A few systems generate summaries based on sentence fusion , a technique which identifies fragments conveying common information across documents and combines these into sentences ( Barzilay and McKeown , 2005 ; Filippova and Strube , 2008 ; Bing et al. , 2015 ) .
Although neural abstractive models have achieved promising results on single-document summarization ( See et al. , 2017 ; Paulus et al. , 2018 ; Gehrmann et al. , 2018 ; Celikyilmaz et al. , 2018 ) , the extension of sequence - to-sequence architectures to multi-document summarization is less straightforward .
Apart from the lack of sufficient training data , neural models also face the computational challenge of processing multiple source documents .
Previous solutions include model transfer ( Zhang et al. , 2018 ; Lebanoff and Liu , 2018 ) , where a sequence - to-sequence model is pretrained on single-document summarization data and finetuned on DUC ( multi-document ) benchmarks , or unsupervised models relying on reconstruction objectives ( Ma et al. , 2016 ; Chu and Liu , 2018 ) . propose a methodology for constructing large-scale summarization datasets and a two -stage model which first extracts salient information from source documents and then uses a decoder-only architecture ( that can attend to very long sequences ) to generate the summary .
We follow their setup in viewing multi-document summarization as a supervised machine learning prob - lem and for this purpose assume access to large , labeled datasets ( i.e. , source documents - summary pairs ) .
In contrast to their approach , we use a learning - based ranker and our abstractive model can hierarchically encode the input documents , with the ability to learn latent relations across documents and additionally incorporate information encoded in well - known graph representations .
Model Description
We follow in treating the generation of lead Wikipedia sections as a multidocument summarization task .
The input to a hypothetical system is the title of a Wikipedia article and a collection of source documents , while the output is the Wikipedia article 's first section .
Source documents are webpages cited in the References section of the Wikipedia article and the top 10 search results returned by Google ( with the title of the article as the query ) .
Since source documents could be relatively long , they are split into multiple paragraphs by line - breaks .
More formally , given title T , and L input paragraphs { P 1 , ? ? ? , P L } ( retrieved from Wikipedia citations and a search engine ) , the task is to generate the lead section D of the Wikipedia article .
Our summarization system is illustrated in Figure 1 .
Since the input paragraphs are numerous and possibly lengthy , instead of directly applying an abstractive system , we first rank them and summarize the L - best ones .
Our summarizer follows the very successful encoder-decoder architecture ( Bahdanau et al. , 2015 ) , where the encoder encodes the input text into hidden representations and the decoder generates target summaries based on these representations .
In this paper , we focus exclusively on the encoder part of the model , our decoder follows the Transformer architecture in-troduced in Vaswani et al . ( 2017 ) ; it generates a summary token by token while attending to the source input .
We also use beam search and a length penalty ( Wu et al. , 2016 ) in the decoding process to generate more fluent and longer summaries .
Paragraph Ranking
Unlike who rank paragraphs based on their similarity with the title ( using tf -idfbased cosine similarity ) , we adopt a learningbased approach .
A logistic regression model is applied to each paragraph to calculate a score indicating whether it should be selected for summarization .
We use two recurrent neural networks with Long- Short Term Memory units ( LSTM ; Hochreiter and Schmidhuber 1997 ) to represent title T and source paragraph P : {u t1 , ? ? ? , u tm } = lstm t ( { w t1 , ? ? ? , w tm } ) ( 1 ) { u p1 , ? ? ? , u pn } = lstm p ( { w p1 , ? ? ? , w pn } ) ( 2 ) where w ti , w pj are word embeddings for tokens in T and P , and u ti , u pj are the updated vectors for each token after applying the LSTMs .
A max-pooling operation is then used over title vectors to obtain a fixed - length representation ?t : ?t = maxpool ( { u t1 , ? ? ? , u tm } ) ( 3 )
We concatenate ?t with the vector u pi of each token in the paragraph and apply a non-linear transformation to extract features for matching the title and the paragraph .
A second max-pooling operation yields the final paragraph vector p : p i = tanh ( W 1 ( [ u pi ; ?t ] ) ) ( 4 ) p = maxpool ( { p 1 , ? ? ? , p n } ) ( 5 ) Finally , to estimate whether a paragraph should be selected , we use a linear transformation and a sigmoid function : s = sigmoid ( W 2 ( p ) ) ( 6 ) where s is the score indicating whether paragraph P should be used for summarization .
All input paragraphs { P 1 , ? ? ? , P L } receive scores {s 1 , ? ? ? , s L }.
The model is trained by minimizing the cross entropy loss between s i and ground - truth scores y i denoting the relatedness of a paragraph to the gold standard summary .
We adopt ROUGE - 2 recall ( of paragraph P i against gold target text D ) as y i .
In testing , input paragraphs are ranked based on the model predicted scores and an ordering { R 1 , ? ? ? , R L } is gener - ated .
The first L paragraphs { R 1 , ? ? ? , R L } are selected as input to the second abstractive stage .
Paragraph Encoding Instead of treating the selected paragraphs as a very long sequence , we develop a hierarchical model based on the Transformer architecture ( Vaswani et al. , 2017 ) to capture inter-paragraph relations .
The model is composed of several local and global transformer layers which can be stacked freely .
Let t ij denote the j-th token in the i-th ranked paragraph R i ; the model takes vectors x 0 ij ( for all tokens ) as input .
For the l-th transformer layer , the input will be x l?1 ij , and the output is written as x l ij .
Embeddings
Input tokens are first represented by word embeddings .
Let w ij ?
R d denote the embedding assigned to t ij .
Since the Transformer is a nonrecurrent model , we also assign a special positional embedding pe ij to t ij , to indicate the position of the token within the input .
To calculate positional embeddings , we follow Vaswani et al . ( 2017 ) and use sine and cosine functions of different frequencies .
The embedding e p for the p-th element in a sequence is : e p [ i ] = sin( p/10000 2i /d ) ( 7 ) e p [ 2 i + 1 ] = cos( p/10000 2i /d ) ( 8 ) where e p [ i ] indicates the i-th dimension of the embedding vector .
Because each dimension of the positional encoding corresponds to a sinusoid , for any fixed offset o , e p+o can be represented as a linear function of e p , which enables the model to distinguish relative positions of input elements .
In multi-document summarization , token t ij has two positions that need to be considered , namely i ( the rank of the paragraph ) and j ( the position of the token within the paragraph ) .
Positional embedding pe ij ?
R d represents both positions ( via concatenation ) and is added to word embedding w ij to obtain the final input vector x 0 ij : pe ij = [ e i ; e j ] ( 9 ) x 0 ij = w ij + pe ij ( 10 )
Local Transformer Layer
A local transformer layer is used to encode contextual information for tokens within each paragraph .
The local transformer layer is the same as the vanilla transformer layer ( Vaswani et al. , 2017 ) , and composed of two sub-layers : h = LayerNorm ( x l?1 + MHAtt ( x l?1 ) ) ( 11 ) x l = LayerNorm ( h + FFN ( h ) ) ( 12 ) where LayerNorm is layer normalization proposed in Ba et al . ( 2016 ) ; MHAtt is the multihead attention mechanism introduced in Vaswani et al . ( 2017 ) which allows each token to attend to other tokens with different attention distributions ; and FFN is a two -layer feed -forward network with ReLU as hidden activation function .
Global Transformer Layer
A global transformer layer is used to exchange information across multiple paragraphs .
As shown in Figure 2 , we first apply a multi-head pooling operation to each paragraph .
Different heads will encode paragraphs with different attention weights .
Then , for each head , an inter-paragraph attention mechanism is applied , where each paragraph can collect information from other paragraphs by selfattention , generating a context vector to capture contextual information from the whole input .
Finally , context vectors are concatenated , linearly transformed , added to the vector of each token , and fed to a feed-forward layer , updating the representation of each token with global information .
Multi-head Pooling a z ij = W z a x l?1 ij ( 13 ) b z ij = W z b x l?1 ij ( 14 ) ?z ij = exp ( a z ij ) / n j=1 exp ( a z ij ) ( 15 ) where W z a ?
R 1 * d and W z b ?
R d head * d are weights .
d head = d/n head is the dimension of each head .
n is the number of tokens in R i .
We next apply a weighted summation with another linear transformation and layer normalization to obtain vector head z i for the paragraph : head z i = LayerNorm ( W z c n j=1 a z ij b z ij ) ( 16 ) where W z c ?
R d head * d head is the weight .
The model can flexibly incorporate multiple heads , with each paragraph having multiple attention distributions , thereby focusing on different views of the input .
Inter-paragraph Attention
We model the dependencies across multiple paragraphs with an inter-paragraph attention mechanism .
Similar to self-attention , inter-paragraph attention allows for each paragraph to attend to other paragraphs by calculating an attention distribution : q z i = W z q head z i ( 17 ) k z i = W z k head z i ( 18 ) v z i = W z v head z i ( 19 ) context z i = m i=1 exp ( q z i T k z i ) m o=1 exp ( q z i T k z o ) v z i ( 20 ) where q z i , k z i , v z i ?
R d head * d head are query , key , and value vectors that are linearly transformed from head z i as in Vaswani et al . ( 2017 ) ; context z i ?
R d head represents the context vector generated by a self-attention operation over all paragraphs .
m is the number of input paragraphs .
Figure 2 provides a schematic view of inter-paragraph attention .
Feed-forward Networks
We next update token representations with contextual information .
We first fuse information from all heads by concatenating all context vectors and applying a linear transformation with weight W c ? R d * d : We then add c i to each input token vector x l?1 ij , and feed it to a two -layer feed - forward network with ReLU as the activation function and a highway layer normalization on top : c i = W c [ context 1 i ; ? ? ? ; context n head i ] ( 21 ) g ij = W o2 ReLU ( W o1 ( x l?1 ij + c i ) ) ( 22 ) x l ij = LayerNorm ( g ij + x l?1 ij ) ( 23 ) where W o1 ?
R d f f * d and W o2 ?
R d * d f f are the weights , d f f is the hidden size of the feed -forward later .
This way , each token within paragraph R i can collect information from other paragraphs in a hierarchical and efficient manner .
Graph -informed Attention
The inter-paragraph attention mechanism can be viewed as learning a latent graph representation ( self-attention weights ) of the input paragraphs .
Although previous work has shown that similar latent representations are beneficial for downstream NLP tasks ( Liu and Lapata , 2018 ; Kim et al. , 2017 ; Williams et al. , 2018 ; Niculae et al. , 2018 ; Fernandes et al. , 2019 ) , much work in multi-document summarization has taken advantage of explicit graph representations , each focusing on different facets of the summarization task ( e.g. , capturing redundant information or representing passages referring to the same event or entity ) .
One advantage of the hierarchical transformer is that we can easily incorporate graphs external to the model , to generate better summaries .
We experimented with two well - established graph representations which we discuss briefly below .
However , there is nothing inherent in our model that restricts us to these , any graph modeling relationships across paragraphs could have been used instead .
Our first graph aims to capture lexical relations ; graph nodes correspond to paragraphs and edge weights are cosine similarities based on tf-idf representations of the paragraphs .
Our second graph aims to capture discourse relations ( Christensen et al. , 2013 ) ; it builds an Approximate Discourse Graph ( ADG ) ( Yasunaga et al. , 2017 ) over paragraphs ; edges between paragraphs are drawn by counting ( a ) co-occurring entities and ( b ) discourse markers ( e.g. , however , nevertheless ) connecting two adjacent paragraphs ( see the Appendix for details on how ADGs are constructed ) .
We represent such graphs with a matrix G , where G ii is the weight of the edge connecting paragraphs i and i .
We can then inject this graph into our hierarchical transformer by simply substituting one of its ( learned ) heads z with G. Equation ( 20 ) for calculating the context vector for this head is modified as : context z i = m i =1 G ii m o=1 G io v z i ( 24 ) 4 Experimental Setup WikiSum Dataset
We used the scripts and urls provided in to crawl Wikipedia articles and source reference documents .
We successfully crawled 78.9 % of the original documents ( some urls have become invalid and corresponding documents could not be retrieved ) .
We further removed clone paragraphs ( which are exact copies of some parts of the Wikipedia articles ) ; these were paragraphs in the source documents whose bigram recall against the target summary was higher than 0.8 .
For both ranking and summarization stages , we encode source paragraphs and target summaries using subword tokenization with Sentence - Piece ( Kudo and Richardson , 2018 ) .
Our vocabulary consists of 32 , 000 subwords and is shared for both source and target .
Paragraph Ranking
To train the regression model , we calculated the ROUGE - 2 recall ( Lin , 2004 ) of each paragraph against the target summary and used this as the ground - truth score .
The hidden size of the two LSTMs was set to 256 , and dropout ( with dropout probability of 0.2 ) was used before all linear layers .
Adagrad ( Duchi et al. , 2011 ) with learning rate 0.15 is used for optimization .
We compare our ranking model against the method proposed in who use the tf-idf cosine similarity between each paragraph and the article title to rank the input paragraphs .
We take the first L paragraphs from the ordered paragraph set produced by our ranker and the similarity - based method , respectively .
We concatenate these paragraphs and calculate their ROUGE -L recall against the gold target text .
The results are shown in Table 1 .
We can see that our ranker effectively extracts related paragraphs and produces more informative input for the downstream summarization task .
Training Configuration
In all abstractive models , we apply dropout ( with probability of 0.1 ) before all linear layers ; label smoothing ( Szegedy et al. , 2016 ) with smoothing factor 0.1 is also used .
Training is in traditional sequence - to-sequence manner with maximum likelihood estimation .
The optimizer was Adam ( Kingma and Ba , 2014 ) with learning rate of 2 , ? 1 = 0.9 , and ? 2 = 0.998 ; we also applied learning rate warmup over the first 8 , 000 steps , and decay as in ( Vaswani et al. , 2017 ) .
All transformer - based models had 256 hidden units ; the feed-forward hidden size was 1 , 024 for all layers .
All models were trained on 4 GPUs ( NVIDIA TITAN Xp ) for 500 , 000 steps .
We used gradient accumulation to keep training time for all models approximately consistent .
We selected the 5 best checkpoints based on performance on the validation set and report averaged results on the test set .
During decoding we use beam search with beam size 5 and length penalty with ? = 0.4 ( Wu et al. , 2016 ) ; we decode until an end-of-sequence token is reached .
Comparison Systems
We compared the proposed hierarchical transformer against several strong baselines : Lead is a simple baseline that concatenates the title and ranked paragraphs , and extracts the first k tokens ; we set k to the length of the ground -truth target .
LexRank ( Erkan and Radev , 2004 ) is a widelyused graph - based extractive summarizer ; we build a graph with paragraphs as nodes and edges weighted by tf- idf cosine similarity ; we run a PageRank - like algorithm on this graph to rank and select paragraphs until the length of the ground - truth summary is reached .
Flat Transformer ( FT ) is a baseline that applies a Transformer - based encoder - decoder model to a flat token sequence .
We used a 6 - layer transformer .
The title and ranked paragraphs were concatenated and truncated to 600 , 800 , and 1 , 200 tokens .
T-DMCA is the best performing model of and a shorthand for Transformer Decoder with Memory Compressed Attention ; they only used a Transformer decoder and compressed the key and value in selfattention with a convolutional layer .
The model has 5 layers as in .
Its hidden size is 512 and its feed -forward hidden size is 2 , 048 .
The title and ranked paragraphs were concatenated and truncated to 3,000 tokens .
Hierarchical Transformer ( HT ) is the model proposed in this paper .
The model architecture is a 7 - layer network ( with 5 localattention layers at the bottom and 2 global attention layers at the top ) .
The model takes the title and L = 24 paragraphs as input to produce a target summary , which leads to approximately 1 , 600 input tokens per instance .
Results Automatic Evaluation
We evaluated summarization quality using ROUGE F 1 ( Lin , 2004 ) .
We report unigram and bigram overlap ( ROUGE - 1 and ROUGE - 2 ) as a means of assessing informativeness and the longest common subsequence ( ROUGE -L ) as a means of assessing fluency .
forms FT , and even T-DMCA when the latter is presented with 3 , 000 tokens .
Adding an external graph also seems to help the summarization process .
The similarity graph does not have an obvious influence on the results , while the discourse graph boosts ROUGE -L by 0.16 .
We also found that the performance of the Hierarchical Transformer further improves when the model is presented with longer input at test time .
2
As shown in the last row of Table 2 , when testing on 3 , 000 input tokens , summarization quality improves across the board .
This suggests that the model can potentially generate better summaries without increasing training time .
Table 3 summarizes ablation studies aiming to assess the contribution of individual components .
Our experiments confirmed that encoding paragraph position in addition to token position within each paragraph is beneficial ( see row w/o PP ) , as well as multi-head pooling ( w/ o MP is a model where the number of heads is set to 1 ) , and the global transformer layer ( w/ o GT is a model with only 5 local transformer layers in the encoder ) .
Human Evaluation
In addition to automatic evaluation , we also assessed system performance by eliciting human judgments on 20 randomly selected test instances .
Our first evaluation study quantified the degree to which summarization models retain key information from the documents following a question - answering ( QA ) paradigm ( Clarke and Lapata , 2010 ; Narayan et al. , 2018 ) .
We created a set of questions based on the gold summary under the assumption that it contains the most important information from the input paragraphs .
We then examined whether participants were able to answer these questions by reading system summaries alone without access to the gold summary .
The more questions a system can answer , the better it is at summarization .
We created 57 questions in total varying from two to four questions per gold summary .
Examples of questions and their answers are given in Table 5 .
We adopted the same scoring mechanism used in Clarke and Lapata ( 2010 ) , i.e. , correct answers are marked with 1 , partially correct ones with 0.5 , and 0 otherwise .
A system 's score is the average of all question scores .
Our second evaluation study assessed the overall quality of the summaries by asking participants to rank them taking into account the following criteria : Informativeness ( does the summary convey important facts about the topic in question ? ) , Fluency ( is the summary fluent and grammatical ? ) , and Succinctness ( does the summary avoid repetition ? ) .
We used Best-Worst Scaling ( Louviere et al. , 2015 ) , a less labor-intensive alternative to paired comparisons that has been shown to produce more reliable results than rating scales ( Kiritchenko and Mohammad , 2017 ) .
Participants were presented with the gold summary and summaries generated from 3 out of 4 systems and were asked to decide which summary was the best and which one was the worst in relation to the gold standard , taking into account the criteria mentioned above .
The rating of each system was computed as the percentage of times it was chosen as best minus the times it was selected as worst .
Ratings range from ?1 ( worst ) to 1 ( best ) .
Both evaluations were conducted on the Amazon Mechanical Turk platform with 5 responses per hit .
Participants evaluated summaries produced by the Lead baseline , the Flat Transformer , T-DMCA , and our Hierarchical Transformer .
All evaluated systems were variants that achieved the best performance in automatic evaluations .
As shown in Table 4 , on both evaluations , participants overwhelmingly prefer our model ( HT ) .
All pairwise comparisons among systems are statistically significant ( using a one - way ANOVA with posthoc Tukey HSD tests ; p < 0.01 ) .
Examples of system output are provided in Table 5 .
Pentagoet Archeological District
GOLD
The Pentagoet Archeological District is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Castine , Maine .
It is the site of Fort Pentagoet , a 17th - century fortified trading post established by fur traders of French Acadia .
From 1635 to 1654 this site was a center of trade with the local Abenaki , and marked the effective western border of Acadia with New England .
From 1654 to 1670 the site was under English control , after which it was returned to France by the Treaty of Breda .
The fort was destroyed in 1674 by Dutch raiders .
The site was designated a National Historic Landmark in 1993 .
It is now a public park .
QA
What is the Pentagoet Archeological District ? [ a National Historic Landmark District ]
Where is it located ?
[ Castine , Maine ]
What did the Abenaki Indians use the site for ?
[ trading center ]
LEAD
The Pentagoet Archeological District is a National Historic Landmark District located in Castine , Maine .
This district forms part of the traditional homeland of the Abenaki Indians , in particular the Penobscot tribe .
In the colonial period , Abenakis frequented the fortified trading post at this site , bartering moosehides , sealskins , beaver and other furs in exchange for European commodities .
" Pentagoet Archeological district " is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda .
The Australian golden whistler ( Pachycephala pectoralis ) is a species of bird found in forest , woodland , mallee , mangrove and scrub in Australia ( except the interior and most of the north )
Most populations are resident , but some in south - eastern Australia migrate north during the winter .
FT
The Melanesian whistler ( P. Caledonica ) is a species of bird in the family Muscicapidae .
It is endemic to Melanesia .
T-DMCA
The Australian golden whistler ( Pachycephala chlorura ) is a species of bird in the family Pachycephalidae , which is endemic to Fiji .
HT
The Melanesian whistler ( Pachycephala chlorura ) is a species of bird in the family Pachycephalidae , which is endemic to Fiji .
Table 5 : GOLD human authored summaries , questions based on them ( answers shown in square brackets ) and automatic summaries produced by the LEAD - 3 baseline , the Flat Transformer ( FT ) , T-DMCA and our Hierachical Transformer ( HT ) .
Conclusions
In this paper we conceptualized abstractive multidocument summarization as a machine learning problem .
We proposed a new model which is able to encode multiple input documents hierarchically , learn latent relations across them , and additionally incorporate structural information from well - known graph representations .
We have also demonstrated the importance of a learning - based approach for selecting which documents to summarize .
Experimental results show that our model produces summaries which are both fluent and in-formative outperforming competitive systems by a wide margin .
In the future we would like to apply our hierarchical transformer to question answering and related textual inference tasks .
Figure 1 : 1 Figure1 : Pipeline of our multi-document summarization system .
L source paragraphs are first ranked and the L - best ones serve as input to an encoder-decoder model which generates the target summary .
FTthe Pentagoet Archeological district is a National Historic Landmark District located at the southern edge of the Bagaduce Peninsula in Treaty Of Breda .
It was listed on the national register of historic places in 1983 .
T-DMCA
The Pentagoet Archeological District is a national historic landmark district located in castine , maine . this district forms part of the traditional homeland of the abenaki indians , in particular the Penobscot tribe .
The district was listed on the national register of historic places in 1982 .
HT
The Pentagoet Archeological district is a National Historic Landmark District located in Castine , Maine .
This district forms part of the traditional homeland of the Abenaki Indians , in particular the Penobscot tribe .
In the colonial period , Abenaki frequented the fortified trading post at this site , bartering moosehides , sealskins , beaver and other furs in exchange for European commodities .
Melanesian Whistler GOLD
The Melanesian whistler or Vanuatu whistler ( Pachycephala chlorura ) is a species of passerine bird in the whistler family Pachycephalidae .
It is found on the Loyalty Islands , Vanuatu , and Vanikoro in the far southeastern Solomons .
QA
What is the Melanesian Whistler ?
[ a species of passerine bird in the whistler family Pachycephalidae ]
Where is it found ?
[ Loyalty Islands , Vanuatu , and Vanikoro in the far south - eastern Solomons ] LEAD
R d denote the output vector of the last transformer layer for token t ij , which is used as input for the current layer .
For each paragraph R i , for head z ? { 1 , ? ? ? , n head } , we first transform the input vectors into attention scores a z
To obtain fixed - length paragraph representations , we apply a weighted - pooling operation ; instead of using only one rep- resentation for each paragraph , we introduce a multi-head pooling mechanism , where for each paragraph , weight distributions over tokens are calculated , allowing the model to flexibly encode paragraphs in different representation subspaces by attending to different words .
Let x l?1 ij ? ij and value vectors b z ij .
Then , for each head , we calculate a probability distribution ?z ij over tokens within the paragraph based on attention scores :
Table 1 : 1 ROUGE -L recall against target summary for L - best paragraphs obtained with tf - idf cosine similarity and our ranking model .
Methods ROUGE -L Recall L = 5 L = 10 L = 20 L = 40 Similarity 24.86 32.43 40.87 49.49 Ranking 39.38 46.74 53.84 60.42
On average , each input has 525 paragraphs , and each paragraph has 70.1 tokens .
The average length of the target summary is 139.4 tokens .
We split the dataset with 1 , 579 , 360 instances for training , 38 , 144 for validation and 38 , 205 for test .
Table 2 : 2 Test set results on the WikiSum dataset using ROUGE F 1 . Model ROUGE-1 ROUGE -2 ROUGE -L Lead 38.22 16.85 26.89 LexRank 36.12 11.67 22.52 FT ( 600 tokens , no ranking ) 35.46 20.26 30.65 FT ( 600 tokens ) 40.46 25.26 34.65 FT ( 800 tokens ) 40.56 25.35 34.73 FT ( 1,200 tokens ) 39.55 24.63 33.99 T-DMCA ( 3000 tokens ) 40.77 25.60 34.90 HT ( 1,600 tokens ) 40.82 25.99 35.08 HT ( 1,600 tokens ) + Similarity Graph 40.80 25.95 35.08 HT ( 1,600 tokens ) + Discourse Graph 40.81 25.95 35.24 HT ( train on 1,600 tokens / test on 3000 tokens ) 41.53 26.52 35.76
Table 2 summarizes our results .
The first block in the table includes extractive systems ( Lead , LexRank ) , the second block includes several variants of Flat Transformer - based models ( FT , T - DMCA ) , while the rest of the table presents the results of our Hierarchical Transformer ( HT ) .
As can be seen , abstractive models generally outperform extractive ones .
The Flat Transformer , achieves best results when the input length is set to 800 tokens , while longer input ( i.e. , 1 , 200 tokens ) actually hurts performance .
The Hierarchical Transformer with 1 , 600 input tokens , outper - Model R1 R2 RL HT 40.82 25.99 35.08 HT w/o PP 40.21 24.54 34.71 HT w/o MP 39.90 24.34 34.61 HT w/o GT 39.01 22.97 33.76
Table 3 : Hierarchical Transformer and versions thereof without ( w/ o ) paragraph position ( PP ) , multi-head pooling ( MP ) , and global transformer layer ( GT ) .
Table 4 : 4 System scores based on questions answered by AMT participants and summary quality rating .
Model QA Rating Lead 31.59 -0.383 FT 35.69 0.000 T-DMCA 43.14 0.147 HT 54.11 0.237
Our code and data is available at https://github.
com/nlpyang / hiersumm .
This was not the case with the other Transformer models .
