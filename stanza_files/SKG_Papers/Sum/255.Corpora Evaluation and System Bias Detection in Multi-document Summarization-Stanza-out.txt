title
Corpora Evaluation and System Bias Detection in Multi-document Summarization
abstract
Multi-document summarization ( MDS ) is the task of reflecting key points from any set of documents into a concise text paragraph .
In the past , it has been used to aggregate news , tweets , product reviews , etc. from various sources .
Owing to no standard definition of the task , we encounter a plethora of datasets with varying levels of overlap and conflict between participating documents .
There is also no standard regarding what constitutes summary information in MDS .
Adding to the challenge is the fact that new systems report results on a set of chosen datasets , which might not correlate with their performance on the other datasets .
In this paper , we study this heterogeneous task with the help of a few widely used MDS corpora and a suite of state - of- theart models .
We make an attempt to quantify the quality of summarization corpus and prescribe a list of points to consider while proposing a new MDS corpus .
Next , we analyze the reason behind the absence of an MDS system which achieves superior performance across all corpora .
We then observe the extent to which system metrics are influenced , and bias is propagated due to corpus properties .
The scripts to reproduce the experiments in this work are available at https://github.com/ LCS2-IIITD/summarization_bias.git.
* Equal contribution ; listed alphabetically .
1 https://duc.nist.gov/
Introduction Multi-document summarization ( MDS ) deals with compressing more than one document into a textual summary .
It has a wide range of applications - gaining insights from tweets related to similar hashtags , understanding product features amongst e-commerce reviews , summarizing live blogs related to an ongoing match , etc .
Most studies on MDS were performed during the DUC 1 and TAC 2 challenges starting in the early 2000s .
Each version of the challenges released a new dataset .
Most of the MDS systems submitted to these challenges were unsupervised and extractive in nature .
Gradually , the data released in these challenges became the de facto for MDS .
These datasets were manually curated and had less than a hundred instances each .
The recent development of deep neural architecture has led to a significant increase in the number of supervised document summarization systems .
Large labeled corpora which are mostly crowd-sourced have been introduced to meet the training requirements of the supervised systems .
However , the crowd-sourced corpora widely differ in quality based on factors like genre , size of the community , presence of moderation in the community , etc .
This is further aggravated by the complexity of the task , the hardness of accumulating labeled data , or more so in the definition of what constitutes a multi-document summary .
Recently , a few large datasets for MDS have been introduced ( Fabbri et al. , 2019 ; Chowdhury and Chakraborty , 2019a ) .
However , there has been no study to measure the relative complexity of these datasets .
We observe that existing MDS systems behave differently on different corpora .
For example , a system achieving state - of - the - art performance on one corpus fails to achieve reasonable performance on another .
Although the ROUGE points of MDS systems are increasing day-by - day , manual inspection reveals an increased presence of bias in generated summaries .
New systems are being introduced and evaluated on a few selected corpora , leading to difficulty in understanding whether the bias is introduced by the system or it is present in the corpus used for training .
Our research questions are as follows : Q1 .
How should one model the quality of a MDS corpus as a function of its intrinsic properties ?
Q2 .
Why do the ROUGE - based ranks of different MDS systems differ across different corpora ?
How should an MDS system which intends to achieve high ROUGE scores across all corpora , look like ? Q3 .
Why do systems show bias on different metrics , and which other system and corpus attributes are the reason behind it ?
Q4 .
Is the task of MDS almost solved , or is there still scope for improvement ?
We study five MDS corpora - DUC ( DUC , 2002 ) , TAC ( TAC , 2008 ) , Opinosis ( Ganesan et al. , 2010 ) , Multinews ( Fabbri et al. , 2019 ) , and CQA - Summ ( Chowdhury and Chakraborty , 2019 b ) .
We consider eight popular summarization systems - LexRank ( Erkan and Radev , 2004 ) , TextRank ( Mihalcea and Tarau , 2004 ) , MMR ( Carbinell and Goldstein , 2017 ) , ICSISumm ( Gillick et al. , 2008 ) , PG ( See et al. , 2017 ) , PG -MMR ( Lebanoff et al. , 2018 ) , Hi-Map ( Fabbri et al. , 2019 ) , and Copy-Transformer ( Gehrmann et al. , 2018 ) .
Our major contributions are four-fold : ?
We propose a suite of metrics to model the quality of an MDS corpus in terms of - Abstractness , Inter Document Similarity ( IDS ) , Redundancy , Pyramid Score , Layout Bias and Inverse -Pyramid Score . ?
We develop an interactive web portal for imminent corpora to be uploaded and evaluated based on our proposed metrics .
?
We explore different biases that the MDS systems exhibit over different corpora and provide insight into properties that a universal MDS system should display to achieve reasonable performance on all types of corpora . ?
We look into metrics to capture bias shown by MDS systems and explore the extent to which corpus properties influence them .
To the best of our knowledge , the current study is the first of its kind .
Background and Proposed Metrics
Throughout the paper , we use the term candidate documents for the documents participating in summarization , and the term reference to indicate the ground - truth summary .
Oracle summary is the extractive set of sentences selected from the candidate documents , exhibiting maximum ROUGE -N score w.r.t. the reference summary .
It is an NP - hard problem ( Hirao et al. , 2017 ) , and approximate solutions can be found greedily or using ILP solvers .
Here , we briefly introduce a suite of corpus and system metrics proposed by us to better understand the MDS task .
These metrics are further explained in detail in Supplementary .
Corpus Metrics ?
Abstractness :
It is defined as the percentage of non-overlapping higher order n-grams between the reference summary and candidate documents .
A high score highlights the presence of more distinctive phrases in reference summary .
The intuition behind quantifying the number of new words is to sync with the basic human nature of paraphrasing while summarizing .
? Inter Document Similarity ( IDS ) :
It is an indicator of the degree of overlap between candidate documents .
Inspired by the theoretical model of relevance ( Peyrard , 2019a ) , we calculate IDS of a set of documents as follows : IDS ( Di ) = D j ?S Relevance ( Dj , Di ) | S| ( 1 ) where D i is the i th candidate document , and S is the set of all documents other than D i . Here , Relevance ( . , . ) is defined as : Relevance ( A , B ) = ? i PA ( ?i ) . log ( PB ( ? i ) ) ( 2 ) where P A ( ? i ) represents the probability distribution of the i th semantic unit 3 in document A .
The further this score is from 0 , the lesser inter document overlap there is in terms of semantic unit distribution .
As shown in Equation 1 , the numerator calculates relevance which can be interpreted as the average surprise of observing one distribution while expecting another .
This score is small if the distributions are similar i.e. , P A ? P B from Equation 2 . ? Pyramid Score :
We propose the metric Corpus Pyramid score to measure how well important information across documents is represented in the ground truth .
As introduced by ( Nenkova and Passonneau , 2004 ) , Pyramid score is a metric to evaluate system summaries w.r.t. the pool of ground -truth summaries .
We instead use this metric to quantitatively analyze the ground- truth summary w.r.t. candidate documents .
The entire information set is split into Summarization Content Units ( SCUs 4 ) , and each SCU is assigned a weight based on the number of times it occurs in the text .
A pyramid of SCUs is constructed with an SCU 's weight , denoting its level , and a score is assigned to a text , based on the number of SCUs it contains .
Pyramid score is defined as the ratio of a reference summary score and an optimal summary score .
Higher values indicate that the reference summary covers the SCUs at the top of the pyramid better .
SCUs present at the top are the ones occurring in most articles and thus can be deemed as important .
?
Redundancy :
The amount of information in a text can be measured as the negative of Shannon 's entropy ( H ) ( Peyrard , 2019a ) . H ( D ) = ? ? i PD ( ?i ) . log ( PD ( ? i ) ) ( 3 ) where P D represents the probability distribution of documents D , and ?
i represents the i th semantic unit 3 in the distribution .
H( D ) would be maximized for a uniform probability distribution when each semantic unit is present only once .
The farther this score is from 0 , the better a document is distributed over its semantic units in the distribution , hence lesser the redundancy .
As evident from Equation 5 , redundancy is maximized if all semantics units have equal distribution i.e. , P ( ? i ) = P ( ? j ) .
The idea behind using redundancy is to quantify how well individual documents cover sub-topics , which might not be the core content but important nonetheless .
Thus Redundancy ( D ) = Hmax ? H ( D ) ( 4 ) Since H max is constant , we obtain Redundancy ( D ) = ? i PD ( ?i ) . log ( PD ( ? i ) ) ( 5 ) ? Layout Bias :
We define Layout Bias across a document as the degree of change in importance w.r.t. the ground - truth over the course of candidate documents .
We divide the document into k segments , calculate the importance of each segment w.r.t. the ground - truth by a similarity score , and average over the sentences in the segment .
Positional importance of D j , the j th sentence in the document is denoted by : P ositionalImportance ( Dj ) = max 1 ?i? n sim ( ? ? Dj , ? ? Ri ) ( 6 ) where , ? ?
R i is the vector representation of the i th sentence in the reference , sim is a similarity metric between two sentences , and n is the total number of sentences in the reference summary .
A lower shift indicates that while generating reference summaries , all segments have been given similar importance within any 3 - fold segmented article .
? Inverse-Pyramid Score ( Inv Pyr ) :
We propose Inverse - Pyramid score to quantify the bias that a reference summary exhibits w.r.t. its set of candidate documents .
It measures the importance given to each document in the candidate set by the reference summary as : InvP yr ( D , S ) = V arj Dj ? Su ( 7 )
Here , D and S are the set of candidate documents for MDS and their summary respectively , V ar is the variance , D j and S u are the sets of SCUs 4 in the j th document of the candidate set and the reference summary respectively .
Higher Inv
Pyr scores suggest the difference in importance given to each document while generating the summary is higher .
As evident from Equation 7 , Variance across the similarities is high if the similarity scores across the document-summary pairs are uneven .
System Metrics ?
ROUGE ( Lin , 2004 ) is a metric which computes the n-gram overlap recall value for the generated summary w.r.t. the reference summary .
? F1 Score with Oracle Summaries :
Oracle summaries reflect the extractive selection of sentences that achieve the highest ROUGE score over the candidate documents given a reference summary .
Similar to ROUGE - 1 , this metric also combines both precision and recall between the oracle and system summaries to calculate F1 Score .
It is a better indicator of the presence of non-essential ngrams than ROUGE as it also takes precision into account .
?
System Abstractness :
Analogous to corpus abstractness , we compute the percentage of novel higher order n-grams in the generated summary w.r.t. the candidate documents .
System abstractness is calculated using Coverage ( D , S ) = i?1..n ( D ? S i ) C n ( S ) where D represents the set of n-grams in the candidate document , and S represents the set of n-grams in the i th system summary .
The denominator denotes the total count of n-grams in a system summary .
Finally , the values of all articles is normalized to get the score for the system ?
Layout Bias :
We propose this metric to capture which sections of the candidate documents comprise a majority of the information in the generated summary .
For neural abstractive systems , we concatenate candidate documents to form one large document and feed it to the neural model .
We study two variations of this metric - The first variation involves segmenting this large document into k parts and then computing the similarity of n-gram tokens of system summaries w.r.t. the candidate document segment .
The second variation includes shuffling the candidate documents before concatenating and then computing the n-gram similarity with the generated summary .
? Inter Document Distribution ( IDD ) :
We propose this metric to quantify the extent of contribution of each candidate document to form the generated summary .
The relevance for system summaries is calculated by , Relevance ( A , B ) = ? i P A ( ? i ) . log ( P B ( ? i ) ) where P A represents the probability distribution of system summary S , and ?
i represents the i th semantic unit in the distribution .
IDD ( D i ) = D j ?S Relevance ( D j , D i ) Cardinality ( S ) ?
Redundancy :
It measures the degree to which system summaries can cover the distribution across semantic units generated from the candidate documents .
Redundancy for candidate documents is given by Eq. , Redundancy ( D ) = ? i S D ( ? i ) . log ( S D ( ? i ) ) where S D represents the probability distribution of a system summary D. ?
i represents the i th semantic unit in the distribution .
3 Experimental Setup
MDS Systems
To identify bias in system- generated summaries , we study a few non-neural extractive and neural abstractive summarization systems , which are extensively used for multi-document summarization .
?
LexRank ( Erkan and Radev , 2004 ) is a graph based algorithm that computes the importance of a sentence using the concept of eigen vector centrality in a graphical representation of text .
?
TextRank ( Mihalcea and Tarau , 2004 ) runs a modified version of PageRank ( Brin and Page , 1998 ) on a weighted graph , consisting of nodes as sentences and edges as similarities between sentences .
? Maximal Marginal Relevance ( MMR ) ( Carbinell and Goldstein , 2017 ) is an extractive summarization system that ranks sentences based on higher relevance while considering the novelty of the sentence to reduce redundancy .
? ICSISumm ( Gillick et al. , 2008 ) optimizes the summary coverage by adopting a linear optimization framework .
It finds a globally optimal summary using the most important concepts covered in the document .
? Pointer Generator ( PG ) network ( See et al. , 2017 ) is a sequence - to-sequence summarization model which allows both copying words from the source by pointing or generating words from a fixed vocabulary .
?
Pointer Generator-MMR : PG -MMR ( Lebanoff et al. , 2018 ) uses MMR along with PG for better coverage and redundancy mitigation .
?
Hi-Map : Hierarchical MMR -Attention PG model ( Fabbri et al. , 2019 ) extends the work of PG and MMR .
MMR scores are calculated at word level and incorporated in the attention weights for a better summary generation .
? Bottom - up Abstractive Summarization ( CopyTransformer ) ( Gehrmann et al. , 2018 ) uses transformer parameters proposed by ( Vaswani et al. , 2017 ) ; but one of the attention heads chosen randomly acts as a copy distribution .
Inferences from Corpus Metrics ?
News derived corpora show a strong layout bias where significant reference information is contained in the introductory sentences of the candidate documents ( Fig. 2 ) . ?
Different MDS corpora vary in compression factors with DUC at 56.55 , TAC at 54.68 , Multinews at 8.18 and CQASumm at 5.65 .
A high compression score indicates an attempt to pack candidate documents to a shorter reference summary .
?
There has been a shift in the size and abstractness of reference summaries in MDS corpora over timewhile DUC and TAC were small in size and mostly extractive ( 11 % novel unigrams ) ; crowd-sourced corpora like CQASumm are large enough to train neural models and highly abstractive ( 41.4 % novel unigrams ) .
?
Candidate documents in Opinosis , TAC and DUC feature a high degree of redundant information as compared to Opinosis and CQASumm , with instances of the former revolving around a single key entity while that of the latter tending to show more topical versatility .
?
MDS corpora present a variation in interdocument content overlap as well : while Multinews shows the highest degree of overlap , CQA - Summ shows the least and the rest of the corpora show moderate overlap ( see Fig. 1 ) . ?
Pyramid Score , the metric which evaluates if the important and redundant SCUs 4 from the candidate documents have been elected to be part of the reference summary , shows considerably positive values for DUC , TAC and Multinews as compared to crowdsourced corpora like CQASumm ( Fig. 3 .b ) .
? Inverse-Pyramid Score , the metric which evaluates how well SCUs 4 of the reference summary are distributed amongst candidate documents , also shows better performance on human-annotated corpora compared to crowd-sourced ones ( Fig. 3 ( b ) ) . ?
A comparison amongst corpus metrics presents a
Inferences from System Metrics ?
MDS systems under consideration are ranked differently in terms of ROUGE on different corpora ; leading to a dilemma whether to declare a system superior to others without testing on all types of datasets ( Fig. 4 ( c ) ) and Table 2 ) . ?
MDS systems under consideration outperform abstractive summarization systems by up to ?
Hi-Map and CopyTransformer generate more abstract summaries ( 17.5 % and 16 % novel unigrams respectively ) in comparison to PG and PG - MMR ( Fig. 4 ( a ) ) . ?
Averaging over systems and comparing corpora , we notice that Multinews and CQASumm achieve the highest abstractness ( 27 % and 7 % respectively ) , which might be a result of these two corpora having the most abstract reference summaries ( Fig. 4 ( a ) and ( Table 2 ) ) .
?
Abstractive systems exhibit a 55 % shift in importance between the first and the second segments of generated summaries , whereas extractive systems show an average shift of only 40 % , implying that 2 ) . ?
In terms of Topic Coverage , extractive systems show better coverage than abstractive systems ( Table 2 ) , which might be a result of extractive systems being based on sentence similarity algorithms which find important sentences , reduce redundancy and increase the spread of information from different segments of the candidate document .
( Fig. 5 ) .
Discussion on Research Questions Q1 .
How should one model the quality of an MDS corpus as a function of its intrinsic metrics ?
What guidelines should be followed to propose MDS corpora for enabling a fair comparison with existing datasets ?
The quality of an MDS corpus is a function of two independent variables : the quality of the candidate documents and the quality of the reference summary .
Our findings suggest that a framework for future MDS datasets should provide scores measuring their standing w.r.
CQASumm .
Hence as of today , no summarization system strictly outperforms others on every corpus .
We also see that CopyTransformer which achieves state - of - the - art performance on Multinews achieves 10 points less than the best system on DUC .
Similarly , LexRank , the state- of- the - art performer on CQASumm , achieves almost 12 points less than the best system on TAC .
Therefore , a system that performs reasonably well across all corpora , is also missing .
This is because different corpora are high on various bias metrics , and summarization systems designed for a particular corpus take advantage and even aggravate these biases .
For example , summarization systems proposed on news based corpora are known to feed only the first few hundred tokens to neural models , thus taking advantage of the layout bias .
Feeding entire documents to these networks have shown relatively lower performance .
Systems such as LexRank are known to perform well on candidate documents with high inter-document similarity ( e.g. , Opinosis ) .
Solving the summarization problem for an unbiased corpus is a harder problem , and for a system to be able to perform reasonably well on any test set , it should be optimized to work on such corpora . Q3 .
Why do systems show bias on different metrics , and which other system and corpus attributes are the reason behind it ?
We begin by studying how abstractness of generated summaries is related to the abstractness of corpora the system is trained on .
For this , we calculate the Pearson correlation coefficient between the abstractness of generated summaries and references across different datasets .
From correlation which implies that they are likely to generate more abstract summaries if the datasets on which they are trained have more abstract references .
Lastly , we infer how Layout Bias in system- generated summaries is dependent on the layout bias of reference summaries .
The last three highlighted columns of Table 3 infer that the abstractive systems such as PG , PG - MMR , Hi-Map and CopyTransformer show a high negative correlation for the end segments while maintaining a strongly positive one with the starting segment .
On the other hand , extractive systems such as LexRank , TextRank , MMR and ICSISumm maintain a strongly positive correlation throughout the segments .
On shuffling the source segments internally , we observe that extractive systems tend to retain their correlation with corpora while abstractive systems show no correlation at all ( Fig. 2 ) , proving that in supervised systems , the layout bias in system summaries propagates from the layout bias present in corpora .
Q4 .
Is the task of MDS almost solved , or there is still plenty of scope remaining for improvement ?
In the previous sections , we computed the oracle extractive upper bound summary using greedy approaches to find the summary that obtains the highest ROUGE score given the candidate documents and references .
We observe that the best summarization system on each corpus today obtains a score which is 39.6 % of the extractive oracle upper bound on DUC , 47.8 % on TAC , 75.02 % on Opinosis , 54.5 % on Multinews and 49.9 % on CQASumm .
This shows that there is enough scope for MDS systems to achieve double the ROUGE scores obtained by the best system to date on each corpus except Opinosis .
Therefore , we believe that the task of MDS is only partially solved and considerable efforts need to be devoted to improving the systems .
Related Work Previous attempts to evaluate the quality of the benchmark summarization corpora are few in number and mostly from the time when corpora were manually accumulated .
( Hirao et al. , 2004 ) primarily used the intrinsic metrics of precision and recall to evaluate corpus quality .
In addition , the authors proposed an extrinsic metric , called ' Pseudo Question Answering ' .
This metric evaluates whether a summary has an answer to a question that is otherwise answerable by reading the documents or not .
Although effective , the cost of such an evaluation is enormous and is not scalable to modern day corpora sizes .
For such corpora where multiple references are available , ( Benikova et al. , 2016 ) used an inter-annotator agreement to model the quality of the corpora .
They also used non-redundancy , focus , structure , referential clarity , readability , coherence , length , grammaticality , spelling , layout , and overall quality as quantitative features for an MDS corpus .
Recently , ( Chowdhury et al. , 2020 ) proposed an MDS system that used the baseline PG model along with Hierarchical structural attention to take into account long-term dependencies for superior results compared to baseline models .
There have been a series of very recent studies that look into how to strengthen the definition and discover system biases in single -document summarization .
Very recently , ( Jung et al. , 2019 ) studied how position , diversity and importance are significant metrics in analyzing the toughness of singledocument summarization corpora .
Another recent work ( Kryscinski et al. , 2019 ) extensively studied the Layout Bias in news datasets that most single -document summarization systems seem to exploit .
Two seminal works , namely ( Peyrard , 2019a ) and ( Peyrard , 2019 b ) , exploited the theoretical complexity of summarization on the ground of importance , analyzing in - depth what makes for a good summary .
( Peyrard , 2019a ) mathematically modeled the previously intuitive concepts of Redundancy , Relevance and Informativeness to define importance in single-document summarization .
( Grusky et al. , 2018 ) proposed a new singledocument summarization corpus and quantified how it compares to other datasets in terms of di-versity and difficulty of the data .
They introduced metrics such as extractive fragment density and extractive fragment coverage to plot the quality of SDS corpus .
To the best of our knowledge , no comparative work exists for either corpora or systems in MDS , and the current paper is the first in this direction .
Conclusion
In this paper , we aimed to study the heterogeneous task of multi-document summarization .
We analyzed interactions between widely used corpora and several state - of - the - art systems to arrive at a line of conclusions .
We defined MDS as a mapping from a set of non-independent candidate documents to a synopsis that covers important and redundant content present in the source .
We proposed intrinsic metrics to model the quality of an MDS corpus and introduced a framework for future researches to consider while proposing a new corpus .
We analyzed how ROUGE ranks of different systems vary differently on different corpora and described what a system that achieves reasonable performance on all corpora would look like .
We evaluated how different systems exhibit bias and how their behavior is influenced by corpus properties .
We also commented on the future scope for the task of MDS .
Future directions to take forward this work would include a causal analysis of how corpus bias is responsible for bias in model prediction across different corpora and systems .
This might bring forward measures to de-bias NLP algorithms with / without de-biasing the corpora .
Figure 1 : 1 Figure 1 : Heatmap depicting the corpus metric : Inter document similarity .
We explain with a single instance of ( a ) DUC - 2004 , ( b) DUC - 2003 , ( c ) TAC - 2008 , and ( d ) CQASumm , and highlight inter-document overlap .
