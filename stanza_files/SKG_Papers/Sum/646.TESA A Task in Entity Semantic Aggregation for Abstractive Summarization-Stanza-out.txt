title
TESA : A Task in Entity Semantic Aggregation for Abstractive Summarization
abstract
Human-written texts contain frequent generalizations and semantic aggregation of content .
In a document , they may refer to a pair of named entities such as ' London ' and ' Paris ' with different expressions : " the major cities " , " the capital cities " and " two European cities " .
Yet generation , especially , abstractive summarization systems have so far focused heavily on paraphrasing and simplifying the source content , to the exclusion of such semantic abstraction capabilities .
In this paper , we present a new dataset and task aimed at the semantic aggregation of entities .
TESA contains a dataset of 5.3 K crowd-sourced entity aggregations of PERSON , ORGANIZATION , and LO - CATION named entities .
1
The aggregations are document - appropriate , meaning that they are produced by annotators to match the situational context of a given news article from the New York Times .
We then build baseline models for generating aggregations given a tuple of entities and document context .
We finetune on TESA an encoder-decoder language model and compare it with simpler classification methods based on linguistically informed features .
Our quantitative and qualitative evaluations show reasonable performance in making a choice from a given list of expressions , but free-form expressions are understandably harder to generate and evaluate .
Introduction
Abstractly speaking , abstraction can be defined as the process of deriving general concepts from specific instances .
In automatic summarization , however , " abstractive " summarization often means any type of rewriting of words in some source document into an output summary .
Concretely , recent summarization datasets including XSum ( Narayan
Input Entities Franc ?ois Bayrou , Nicolas Sarkozy , S?gol?ne Royal
Document Context Franc ?ois Bayrou , Nicolas Sarkozy , and S?gol?ne Royal are the main contenders in the French presidential elections .
Possible Aggregations ? the French politicians ? the French presidential candidates ? the politicians Table 1 : An example of semantic entity aggregation .
The input consists of a tuple of named entities , a situational ( document ) context , and background information about the entities ( not shown here ) .
The expected output is an aggregation of the tuple of entities .
et al. , 2018 ) and NEWSROOM ( Grusky et al. , 2018 ) quantify the degree of abstractiveness of a summary in terms of its novel N-grams .
While such a surface - level definition of abstractiveness is certainly useful and convenient , it is nevertheless only a proxy for abstraction in the broader sense which concerns semantic generalization .
We argue that it is important to also focus explicitly on semantic abstraction , as this capability is required for more difficult types of summarization which are out of reach of current methods .
For example , generating a plot summary of a novel might require describing sequences of events using one sentence .
Writing a survey of a scientific field would require categorizing papers and ideas , and being able to refer to them as a whole .
Outside of domain-specific settings such as opinion summarization ( Ganesan et al. , 2010 ; Gerani et al. , 2014 , inter alia ) , and tasks such as sentence fusion ( Barzilay and McKeown , 2005 ) , there has been little work focusing on semantic generalization and abstraction .
In this paper , we start to tackle this issue by focusing on the specific task of semantic aggregation of entities ; i.e. , how to refer to a tuple of named entities using a noun phrase instead of enumerating them ( See Table 1 for an example ) .
We define a task to evaluate summarization models on semantic entity aggregation , which we call TESA ( A Task in Entity Semantic Aggregation ) .
In TESA , a system is presented with a list of named entities in an original textual context , and it must produce a non-enumerating noun phrase which refers to the designated entities .
Solving this task requires finding a semantic link between all the entities in the list ( e.g. , London and Paris are cities of considerable sizes ) , then using this information to generate a noun phrase ( e.g. , " the major cities " ) .
We introduce an accompanying dataset of entities in context drawn from the New York Times corpus ( Sandhaus , 2008 ) , and their aggregations which were written by crowd workers .
Our dataset contains 5.3 K aggregation expressions .
Each example , contains a tuple of PERSON , ORGANIZATION or LOCATION named entities , a paragraph context from an NYT article discussing the entities , and background information about entities in the form of summary snippets from Wikipedia .
We also introduce the first models for the TESA task which are based on an encoder-decoder system pretrained for abstractive summarization , BART ( Lewis et al. , 2019 ) .
We present two ways of fine-tuning BART to TESA , either in a discriminative or in a generative fashion , and compare them against simpler statistical and frequency - based methods .
The simple classifier achieves decent results on TESA .
It is however outperformed by a wide margin by BART , when fine - tuned on our task in a discriminative manner .
When fine-tuned as a generative model , BART yields similar performance as the simple classifier .
Yet , the generative model is able to freely generate entity aggregations with diversity and quality , despite some factual inconsistencies .
Related work Abstractive summarizers have gained prominence with the popularization of RNNs ( Sutskever et al. , 2014 ; Nallapati et al. , 2016 ) , and more recently Transformers ( Vaswani et al. , 2017 ) like BERT ( Devlin et al. , 2019 ) .
Several abstractive models have achieved state - of - the - art performances on benchmark summarization datasets in terms of ROUGE , including ProphetNet ( Yan et al. , 2020 ) , PEGA -SUS ( Zhang et al. , 2019 ) and BART ( Lewis et al. , 2019 ) .
Recent work has also focused on specific issues such as preventing inappropriate repetition ( Kry?ci?ski et al. , 2018 ) , word-level rewriting , and evaluating factual consistency ( Kry?ci?ski et al. , 2019 ; Maynez et al. , 2020 ) .
Abstraction is critical for certain domains and applications , but has not been thoroughly explored in many .
For example , in scientific article summarization the particular structure and length of scientific articles make extractive techniques much easier to apply ( Agarwal et al. , 2011 ) , therefore abstractive summarizers ( Lloret et al. , 2013 ) remain a minority .
In opinion summarization , there have been abstractive systems that leverage cues specific to this task , such as redundancy in opinions ( Ganesan et al. , 2010 ) and specific discourse structures ( Gerani et al. , 2014 ) .
As abstractive systems have become strong in terms of generation capabilities , the time is apt to examine issues in semantic abstraction that could be useful in many summarization domains and tasks .
Our work is a step in this direction .
Our proposed entity aggregation task is related to referring expression generation ( REG ) .
REG is concerned with determining the form and content that entity references should take during generation ( Krahmer and van Deemter , 2012 ; Castro Ferreira et al. , 2018 ; Cao and Cheung , 2019 ) .
It emphasizes finding the right distinguishing characteristics of the intended referent or referents .
Our work can be seen as a specific REG task that focuses on semantically abstracting multiple named entities .
Our work is also related to coreference resolution , especially those that examine multi-antecedent resolution ( Burga et al. , 2016 ; Vala et al. , 2016 ) , an inverse problem to ours .
To the best of our knowledge , no previous work has directly addressed entity aggregation .
The TESA dataset
We used the New York Times ( NYT ) Annotated Corpus ( Sandhaus , 2008 ) to extract tuples of named entities and their document context .
The NYT corpus contains high-quality metadata listing the salient named entities mentioned in each article .
We form our tuples from entities tagged in the metadata for the same article .
We refer to a tuple of entities and its associated information as an aggregatable instance .
We first describe the components of an aggregatable instance in more detail .
Then , we describe our data extraction and crowd-sourcing experiments .
An aggregatable instance
The starting point of an aggregatable instance is the tuple of named entities which should be aggregated and the type of its entities ( e.g. , PERSON ) .
As we aim for contextual entity aggregations , an aggregatable instance also contains a document context ; i.e. , a passage from a document in which all the entities are mentioned .
To provide additional background knowledge , we also include introductory summaries for the entities taken from Wikipedia .
An example of aggregatable instance , as presented to the annotators , is in Figure 1 .
For more examples , see Table 8 in the appendix .
Data extraction
While we could have gathered naturally occurring entity aggregations , work on multi-antecedent coreference resolution is still nascent , and our initial attempts to define heuristic methods to extract entity aggregations were very noisy .
We instead used crowd-sourcing to gather human-generated aggregations from sets of entities .
We used the 2006 and 2007 portions of the New York Times corpus .
We started with the editorial metadata which tags salient named entities in each article .
These are entities we believe are likely to be included in a summary .
We filtered the entity tuples to remove those that are unlikely to be naturally aggregatable using the following two constraints .
First , the entities should have the same type ( PERSON , LOCATION , or ORGANIZATION in this corpus ) .
Second , the entities should be mentioned close together , within a span of consecutive sentences of the same length as the size of the tuple of entities ( e.g. , three consecutive sentences for three entities ) .
We also selected those entity tuples that are mentioned together in the abstract of an article .
To extract the document context , we extracted both the title of the article and the span of sentences which mentions the entities .
If the same entity tuple is mentioned in different qualifying sentence spans in the same article , they would be extracted as different aggregatable instances .
As for the background information , we extracted an excerpt of each entity 's Wikipedia article , using the first paragraph of the article if it exists , up to 600 tokens .
We used the entity name to identify its Wikipedia page 2 , and , in case of ambiguous or incorrect linking , we corrected it manually when possible , or discarded it .
After extraction , we sampled 2,100 instances uniformly at random for annotation .
A tuple contains between 2 and 6 entities , for an average of 2.4 .
Data Annotation
We used Amazon Mechanical Turk to collect entity aggregations .
Annotators were asked to generate aggregations given information about an aggregatable instance .
For each instance , we showed the same information as described above , including the mentions of the entities in context , and a link to the Wikipedia pages of the entities .
Some of the instructions given to the annotators and examples of the annotation layout are in Figures 1 , 2 The entity tuple , document context , Wikipedia background information are presented to annotators , alongside a prompt ( see Figure 1 ) .
For the PERSON entities in our example , this prompt is " In this article , Franc ?ois Bayrou , Nicolas Sarkozy and S?gol ?ne Royal are discussed .
The three people ... "
Annotators were asked to replace the phrase " The three people " with a relevant one referring to the entities .
The prompt serves to prime the annotator to produce a fluent and comprehensive aggregation covering all the entities .
For other named entity types , the prompt is changed accordingly .
While simple , we found this prompt to be rather effective in the collection process .
We also presented detailed examples ( see Figure 2 ) explaining the desired aggregations .
Annotators were asked not to use generic aggregations involving only the entities ' type ( e.g. , " the three people " ) and to avoid using " and " , as it would often imply an enumeration .
For each of the 2,100 aggregatable instances , three different annotators were asked to provide an annotation .
In each annotation , an annotator could provide between zero ( meaning the instance is not aggregatable ) and two aggregations .
The aggregations produced for the example of Figure 1 by the three annotators are below : Annotator 1 ? french politicians Annotator 2 ? the French politicians ? the French presidential candidates Annotator 3 ? the politicians
We discarded instances that at least two of the three annotators considered as ' not aggregatable ' .
In addition , we discarded those annotations that did not conform to our instructions , and annotations from workers who performed less than five annotations .
Finally , we post-processed the aggregations , removing determiners , numerical expressions and standardized the casing ( e.g. , " The two cities " became " cities " ) .
Table 2 presents statistics on the size of the data collected and the final dataset .
Data Splits
We split the dataset into training , validation , and test sets using a 2:1:1 ratio , resulting in 858/430/430 aggregatable instances in each set , respectively ( corresponding to 20592/10320/10320 ranking candidates , respectively ) .
The entities in our dataset are quite diverse .
In the validation and test sets , 29 % and 30 % of the aggregatable instances respectively have a set of input entities which do not overlap with entities in the training set at all .
On average , each aggregatable instance has 2.7 different aggregations .
4
The TESA task
Task Definition
We frame TESA as a ranking task where , given an aggregatable instance as input , models must rank a list of candidates according to their plausibility as an aggregation of the input entities ( in context ) .
We choose a discriminative approach to avoid relying on word-overlap metrics , and we opt for a ranking task set - up to avoid classification between heavily imbalanced classes , as the number of gold standards remains limited .
In this set-up , generative models can also be evaluated .
In our experiments , the list of candidate aggregations contains 24 candidates in total , including the gold-standard , correct aggregations generated by the human annotators , as well as a list of negative candidates which serve as distractors .
The candidates ' number is chosen to yield approximately 10 times more negative candidates than gold standards .
Negative candidates are sampled uniformly at random from other aggregatable instances sharing the same named entity type .
An example of TESA 's tasks is available in Table 3 ; for more examples , see Table 9 in the appendix .
Evaluation Measures
We evaluate the models ' performances using three widely used ranking performance measures .
Let rank ( i ) be the rank of candidate i , G be the set of gold -standard candidates in a ranking and R ( n ) be the set of candidates retrieved up to and including position n.
Then , for an aggregatable instance : Average precision .
AP = 1 | G | i?G |G ? R( rank ( i ) ) | | R( rank ( i ) ) | ( 1 ) Recall at 10 .
R@10 = 1 | G | |G ? R ( 10 ) |
( 2 ) Reciprocal rank .
RR = 1 min i?G rank ( i ) ( 3 )
We report the mean of these values across all instances in the test set ( MAP , R@10 , MRR ) .
We chose these measures because they provide different perspectives on the evaluation results .
Recall at 10 captures the models ' ability to rank correct aggregations as promising or neutral at worst .
Reciprocal rank focuses solely on the best ranked correct aggregation .
Models
We tested several simple baselines as well as models adapted from current work on abstractive summarization on TESA .
Simple Baselines
All the baselines and models are given as input an aggregatable instance and a list of candidates to rank with the same entity type as the aggregatable instance .
The first two baselines are agnostic to the aggregatable instance : Random .
This baseline produces a random ordering of the candidate entities .
Frequency .
This baseline ranks the candidates according to their frequency as a correct aggregation in the training set .
Logistic Regression
We defined a number of statistical and linguistically informed features , which we extracted from each candidate aggregation and its aggregatable instance 's context and background information .
These 15 features include : ? the count of the " frequency " baseline , ? the number of common tokens ( with repetition ) between the candidate and the union of the background information , ? the size of the word overlap between a candidate and the intersection of the entities ' background information , ? the cosine similarity between the average word embeddings of the candidate and of the context .
We detail these features in Appendix C .
We trained a binary logistic regression using this representation , to discriminate between the goldstandard aggregations and the negative candidates .
We used the model 's predictive probability for the gold -standard class to produce a ranking over the candidate list .
BART - based models
We tested BART ( Lewis et al. , 2019 ) as a representative model of recent high - performance abstractive summarization systems based on an encoderdecoder architecture with a Transformer backbone .
We compared three versions of BART , which differ based on whether and how they are fine-tuned on TESA .
Pre-trained BART .
We applied an existing pretrained version of BART in a generative set - up without fine-tuning .
We formatted each aggregatable instance into a single sequence of tokens by concatenating the fields of the aggregatable instances in the following order : background information , context ( title of the article and excerpt ) , and entity names .
An example of such input can be seen in Table 3 .
We fed this as input to BART 's encoder , and we evaluated the probability of each candidate aggregation to be generated autoregressively by the decoder .
We used these probabilities to rank the candidates .
Generative BART .
This version is similar to the above , but we fine- tuned BART on TESA , considering each correct aggregation as a separate target , and training the model to generate each target given the corresponding aggregatable instance .
For the aggregatable instances , we used the same input format as above .
We did not add any form of separation tokens , as our initial experiments showed that they slightly hurt the performance .
Discriminative BART .
Finally , we fine- tuned BART discriminatively as a classifier .
During finetuning , we consider each candidate and its aggregatable instance as a separate sample , and the model was trained on these samples to discriminate the correct aggregations from the negative candidates .
At test time , we rank the candidates by their probability of being the correct aggregation according to the classifier .
Again , we did not add any separation tokens , as it did not improve the performance .
The main advantage of this approach over the previous one is that it leverages the set- up of TESA as a ranking task , and the model is exposed to both correct and incorrect aggregations during training ( which , on the other hand , makes it more computationally expensive ) .
By contrast , generative BART only sees correct ones .
We thus expect the discriminative model to produce higher performance .
However , this comes at a cost , as this approach cannot generate freely an aggregation , but only retrieve one from a set of candidates .
For all three versions above , we built upon code that is available through fairseq ( Ott et al. , 2019 ) .
We use the version of BART pre-trained on the CNN / DailyMail dataset .
The choice of hyperparameters is described in Appendix D.
Results
The results of the models on TESA 's test set are presented in Table 4 .
We see that most models out - Method MAP R@10 MRR Random baseline 0.222 0.442 0.289
Frequency baseline 0.570 0.655 0.761 Logistic regression 0.700 0.863 0.840 Pre-trained BART 0.389 0.682 0.505 Generative BART 0.701 0.903 0.840 Discriminative BART 0.895 0.991 0.954
Table 5 : Results of the ablation study .
We report the mean average precision differences between the ablated system and the full model 's performance ( in parentheses ) on TESA .
Negative numbers mean the performance of the full model is higher .
perform the frequency baseline , except pre-trained BART .
Fine-tuning BART on TESA increased its performance significantly , especially if done discriminatively .
Discriminative BART achieves the best results .
Its high performance can be mitigated by our choice of ranking only 24 candidates , which makes unlikely confusing negative candidates .
Ablation Study
To understand the importance of the different components of the input for this task , we performed an ablation study , where we removed selected parts of the input : without the background information ( context , entities ) , without context ( info. , entities ) and with only the names of the entities ( entities ) .
We fine- tuned generative and discriminative BART on these modified datasets .
The hyperparameters used are described in Appendix D .
We report the mean average precision results , which are representative of the other measures , in Table 5 .
Models perform best when all information is available , which validates our choice of input format .
The background information seems to be more important than the context , as removing the context leads to the smallest drop in average precision .
Interestingly , models perform quite well when given only the entities ' names , though the performance gap is still quite significant .
7 : Aggregations generated by generative BART on the running example .
The model 's encoder is fed an aggregatable instance , and the decoder generates autoregressivly the aggregations without constraint .
We show the input entities , and the 10 aggregations retrieved by the beam search , ranked according to their likelihoods .
If a generated aggregation matches a gold standard ( except for capital letters ) , it is in bold ; the generated examples probabilities are in brackets .
Qualitative analysis
We compare the two best-performing models : generative and discriminative BART .
In Table 6 , we present an example of their results on a ranking task from TESA 's test set .
In general , the discriminative approach performs well , is robust and the negative candidates ranked at high positions are quite coherent ( e.g. , " former presidents " and " police officers " ) .
On the other hand , generative BART performs quite well on the ranking task , but is far less robust and its negative candidates ranked at high positions are more intriguing ( e.g. , " new york mafiosos " and " american men " ) , which seems to indicate a poorer understanding of the aggregatable instance .
Besides , we show some aggregations generated by the generative approach in Table 7 .
Qualitatively speaking , the generated samples are quite interesting as many of them are accurate and have a diverse vocabulary ( e.g. , " politicians " , " figures " , " candidates " , " leader " ) .
However , some samples are factually inconsistent ( e.g. , " american politicians " ) which seems to indicate that the model does not have a deep understanding of relevant semantic concepts ( e.g. , nationalities cannot be substituted for each other ) .
For other examples , including some specifically chosen as the models failed on them , see Tables 10 - 13 in the appendix .
Conclusion and future work
We have proposed TESA , a novel task and an accompanying dataset of crowd-sourced entity aggregations in context .
TESA directly measures the ability of summarizers to abstract at a semantic level .
We have compared several baseline models and models adapted from existing abstractive summarizers on TESA , and find that a discriminative fine-tuning achieves the best performance , though this model inherently cannot generate aggregations .
In future work , we would like to expand the domains covered by our dataset , which is biased towards topics found in the source corpus , such as politics .
Another important direction is to investigate how to integrate the ability to aggregate entities derived from training on TESA into an abstractive summarizer .
This would require models to tackle another challenging issue which we have not addressed : which set of entities should a model aggregate in the first place ?
inspired by BART 's finetuning on summarization datasets described here .
We kept the model 's parameters of the experiment and the epoch maximizing the average precision of the validation set .
We performed a grid search on the following hyperparameters : ? lr in { 3e - 6 , 5e- 6 , 1e-5 , 2e-5 , 3e-5 } , ? max-tokens in { 1024 , 2048 } .
We used the following final hyperparameters : ? lr=5e-06 , ? max-tokens =1024 , ? max-epochs=6 , ? update-freq=1 , ? total-num-updates =4974 , ? warmup-updates =149 .
total-num-updates was determined empirically as max-epochs ? updates -per-epoch update-freq and warmup-updates was chosen as 3 % of total - num-updates .
During the hyperparameter search we used total- num-updates = 4974 , 375 and warmup-updates = 149 , 67 for max-tokens = 1024 , 2048 respectively .
To evaluate candidates ' likelihood and to generate aggregations , we modified slightly the code of Ott et al . ( 2019 ) to compute all hypotheses of the beam search ( not only the most probable one ) and we used the same parameters as in Appendix D.2 .
We ran our experiments on a single V100 GPU with 32 GB of memory with the fp16 option , and an experiment took typically 1 hour .
This model had 401 million parameters , all of them being trained .
For the ablation study , we used the final hyperparameters , except for total - num-updates and warmup-updates which were determined empirically as above .
We added max-sentences = 16 for the " entities " ablation experiment .
