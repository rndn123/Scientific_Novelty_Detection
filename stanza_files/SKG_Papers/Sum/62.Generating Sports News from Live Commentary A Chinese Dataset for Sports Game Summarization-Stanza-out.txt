title
Generating Sports News from Live Commentary : A Chinese Dataset for Sports Game Summarization
abstract
Sports game summarization focuses on generating news articles from live commentaries .
Unlike traditional summarization tasks , the source documents and the target summaries for sports game summarization tasks are written in quite different writing styles .
In addition , live commentaries usually contain many named entities , which makes summarizing sports games precisely very challenging .
To deeply study this task , we present SPORTSSUM 1 , a Chinese sports game summarization dataset which contains 5,428 soccer games of live commentaries and the corresponding news articles .
Additionally , we propose a two-step summarization model consisting of a selector and a rewriter for SPORTSSUM .
To evaluate the correctness of generated sports summaries , we design two novel score metrics : name matching score and event matching score .
Experimental results show that our model performs better than other summarization baselines on ROUGE scores as well as the two designed scores .
Introduction
There are a large number of sports games playing every day .
Apparently , manually writing sports news articles to summarize every game is laborintensive and infeasible .
How to automatically generate sports summaries , therefore , becomes a popular and demanding task .
Recently , generating news from live commentaries has gradually attracted attention in the academic community ( Zhang et al. , 2016 ; Yao et al. , 2017 ) .
At the same time , several trials have been done in the industry such as sports news from Toutiao 's Xiaoming Bot 2 , Sohu Ruibao 3 and AI football news 4 .
The ball flew into the goal through the lower right corner .
The ball went in !
Muller gave the assist .
Bayern Munich 1 - 0 Dortmund .
71 ' 1 - 0 ? ? , ?.?.
Bayern Munich 's player Ribery tried to shoot with his right foot from the penalty area 's left side , but the ball was higher than the crossbar .
Lahm passed the ball to him .
Sports News Ariticle ?3 ? ?8 ? ?12?13 ? ?27 ?(...)
In the 3rd minutes , Kroos 's free kick on the left was tipped to the back , and Ribery 's shot from the penalty area missed .
In the 8th minute , Muller and Mandzukic had teamwork , and Muller 's shot from the 12 meters ahead the goal line was touched out by Suboti ? .
In the 13th minute , Ribery passed the ball from the right , and Kroos 's shot near the 27 meters ahead the goal line missed .
(...)
Table 1 : An example of SPORTSSUM dataset .
Unlike traditional text summarization tasks ( Hermann et al. , 2015 ; Rush et al. , 2015 ) , the source documents and the target summaries for sports game summarization tasks are written in quite different styles .
Live commentaries are the real-time transcripts of the commentators .
Accordingly , commentary sentences are more colloquial and informal .
In contrast , news summaries are usually more narrative and well - organized since they are written after the games .
In addition , commentaries contain a large number of player names .
One player can be referred to multiple times in the whole game , and one commentary sentence may mention multiple player names simultaneously .
Those properties make sports games summarization tasks very challenging .
In this paper , we present SPORTSSUM , a Chinese dataset for studying sports game summarization tasks .
We collect 5,428 pairs of live commentaries and news articles from seven famous soccer leagues .
To the best of our knowledge , SPORTSSUM is the largest Chinese sports game summarization dataset .
In addition , we propose a two-step summarization model for SPORTSSUM , which learns a selector to extract important commentary sentences and trains a rewriter to convert the selected sentences to a news article .
To encourage the model to capture the relations between players and actions better , we replace all the player names in the training sentences with a special token and train the proposed model on the modified template - like sentences .
The proposed model performs better than existing extractive and abstractive summarization baseline models in ROUGE scores ( Lin , 2004 ) .
However , we observe that ROUGE scores cannot evaluate the correctness of generated summaries very well .
Therefore , we design two new scores , name matching score and event matching score , as the auxiliary metrics for SPORTSSUM .
Our experimental results demonstrate that the proposed model is superior to the baseline models in all the metrics .
Summarizing documents between two articles written in different styles and involving many named entities is not limited to the sports game summarization tasks .
There are many possible applications , such as summarizing events from tweets and summarizing trends from forum comments .
We hope that SPORTSSUM provides a potential research platform to develop advanced techniques for this type of summarization tasks .
Dataset
We present SPORTSSUM , a sports game summarization dataset in Chinese .
Data collection .
We crawl the records of soccer games from Sina Sports Live 5 .
The collected records contain soccer games in seven different leagues ( Bundesliga , CSL , Europa , La Liga , Ligue 1 , PL , Series A , UCL ) from 2012 to 2018 .
For each game , we have a live commentary document C and a news article R , as illustrated in sists of a series of tuples ( t i , s i , c i ) , where t i is the timeline information , s i represents the current scores , and c i denotes the commentary sentence .
The news article R consists of several news sentences r i .
In addition to commentaries and news reports , we also include some metadata , such as rosters , starting lineups , and player positions , which is potentially helpful for sports game summarization tasks .
Data cleaning .
The crawled live commentary documents and news articles are quite noisy .
Therefore , we apply multiple steps of data cleaning to improve the quality of the dataset .
We first remove all the HTML tags from the commentary documents and the news articles .
Then , we observe that there are usually some descriptions that cannot be directly inferred from the commentaries at the beginning of news articles , such as matching history .
Hence , we design a heuristic rule to remove those descriptions .
We identify several starting keywords which can indicate the start of a game , such as " ?
?( at the beginning of the game ) " and " ?
?( after the game started ) " .
The full list of starting keywords can be found in Appendix A.
Once we see a starting keyword appearing in a news report , we remove all the sentences before the starting keyword .
Finally , we discard those games with the number of news sentences being less than 5 and the number of commentary sentences being less than 20 .
After data cleaning , we have 5,428 games remaining ( detailed numbers of games are shown in Table 2 ) .
Notice that SPORTSSUM ( 5,428 games ) is much larger than the only public sports game summariza-tion dataset ( 150 games ) ( Zhang et al. , 2016 ) . Statistics and properties .
Table 3 shows the statistics of SPORTSSUM .
On average , there are 193.77 sentences per commentary document and 23.80 sentences per news article .
After applying word segmentation by pyltp tool 6 , the average numbers of words for commentary documents and news reports are 1825.63 and 427.98 , respectively .
As mentioned in Section 1 , commentary sentences and news sentences are in quite different writing styles .
Commentary sentences are more colloquial and informal , while news sentences are more narrative and well -organized .
Also , commentaries contain a large number of player names , which makes the model easy to generate news reports with incorrect facts , as shown in Section 3 .
Sports Game Summarization
The goal of sports game summarization is to generate a sports news report R = {r 1 , r2 , .. , rn } from a given live commentary document C = { ( t 1 , s 1 , c 1 ) , ... , ( t m , s m , c m ) }.
The generated news report R is expected to cover most of the important events in the games and describe those events correctly .
In this paper , we propose a twostep model for SPORTSSUM .
The proposed model first learns a selector to extract important commentary sentences and then utilizes a rewriter to convert the selected sentences to a news article .
Sentence mapping .
To train the selector and rewriter , we need some labels to indicate the importance of commentary sentences and the corresponding news sentences .
To obtain the labels , we consider the timeline information and BERTScore ( Zhang et al. , 2020 ) , a metric to measure the sentence similarity , and map each news sentence to a commentary sentence .
Although we have no explicit timeline information for news sentences , we observe that many news sentences start with " in the n-th minute " and thus we can extract the timeline information for some news sentences .
We map sentences by the following steps : 1 ) For each news sentence r i , we extract the timeline information h i if possible .
Otherwise , we do not map this news sentence .
2 ) We consider those commentary sentences c j with t j being close to h i .
More specifically , we consider C ( i ) = { c k , c k +1 , ...c k+l } , where c j is the commentary sentence with timeline information t j ? [ h i , h i + 3 ]
6 https://github.com/HIT-SCIR/pyltp for k ? j ? k + l. 3 ) We compute BERTScore of the news sentence r i and all the commentary sentences in C ( i ) .
The commentary sentence c j ? C ( i ) with the highest score is considered to be mapped with the news sentences r i .
With the above mapping process , we obtain a set of mapped commentary sentences and news sentences D = { ( c 1 , r1 ) , ( c 2 , r2 ) , ... , ( c s , rs ) } , which can be used for training our selector and rewriter .
Selector .
There are many commentary sentences in a live commentary document , but only few of them contain valuable information and should be reported in the news article .
Therefore , we learn a selector to pick up those important sentences .
More specifically , Given a commentary document C = { ( t 1 , s 1 , c 1 ) , ... , ( t m , s m , c m ) } , the selector outputs a set C select = { c 1 , c2 , ... , cn } which contains only important commentary sentences .
We train a binary classifier as the selector to choose important commentary sentences .
When training , for each commentary sentence c i in C , we assign a positive label if c i can be mapped with a news sentence by the aforementioned mapping process .
Otherwise , we give a negative label .
Rewriter .
The rewriter converts the selected commentary sentences C select = { c 1 , c2 , ... , cn } to a news report R = {r 1 , r2 , .. , rn }.
We focus on the sentence - level rewriter .
That is , we convert each selected commentary sentence ci to a news sentence ri .
An intuitive way to learn the sentencelevel rewriter is training a sequence - to-sequence ( seq2seq ) model , such as LSTM ( Hochreiter and Schmidhuber , 1997 ) and Transformer ( Vaswani et al. , 2017 ) , on the mapped sentences D. However , as illustrated in Table 4 , we observe that the seq2seq model tends to generate high - frequency player names rather than the correct player names even though the high - frequency player names do not appear in the commentary sentences .
We call this situation name mismatch problem .
To solve the name mismatch problem , we train the rewriter in a template - to-template ( tem2tem ) way instead of in a seq2seq way .
We first build a dictionary of player names from the lineup data ( metadata ) .
Next , for each ( c i , ri ) in D , we replace all the player names in ci and ri with a special token " [ player ] " so that the new sentence is like a template .
If there are multiple player names in a sentence , we append a number to the special token to distinguish them , as shown in Table 5 .
Live Commentary Sentence ? ? ? ? ? ? ? , ?.? ? ? ? ?.
Ribery tried to shoot with his right foot from the left side of the penalty area , but the ball was higher than the crossbar .
Lahm passed the ball to him .
Gound Truth News Sentence ? ? ? ? ? ? ? ? ? ? ?12 ?
Lahm passed the ball to the left , and Ribery cut in the left penalty area and shot from 12 meters ahead the goal line .
The shot was too high .
News Sentence Generated by Seq2seq Model ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?
Ribery passed the ball and Mandzukic 's shot from the left side of the penalty area was out of the goalpost .
Lahm made a low pass on the right and Ribery 's shot from the front was blocked by Adler .
After converting ci and ri to the template sentences , we train a seq2seq model on the template sentences .
By training models in a tem2tem way , the model focuses more on the relations between players and actions and is less influenced by the high- frequency player names .
Tem2tem ? ?!!![ player1 ] ? ? ? ? ? ? ? ? ?[ player2 ] ?. ?[ player3 ] .
[ player3 ] ? ? ? ? ?[ player1 ] ?
? ? ? ?[ player2 ] ?
Shoot !!!
[ player1 ] 's right
When predicting , for each commentary sentence ci in C select , we use the aforementioned way to convert ci to a commentary template sentence .
Then , we generate a news template sentence by the rewriter and replace all the special tokens in the sentence with the original player names .
Experiments SPORTSSUM contains 5,428 games and we split them into three sets : training ( 4,828 games ) , validation ( 300 games ) , and testing ( 300 games ) sets .
Evaluation .
We consider ROUGE scores ( Lin , 2004 ) , which are standard metrics for summarization tasks .
More precisely , we focus on ROUGE -1 , ROUGE - 2 , and ROUGE -L .
However , we observe that ROUGE scores cannot accurately evaluate the correctness of summaries .
Some summaries may get high ROUGE scores but contain many incorrect facts .
Therefore , we design two metrics : name matching score ( NMS ) and event matching score ( EMS ) .
The name matching score evaluates the closeness of the player names in the ground truth news article R and the generated summaries R. Let N g and N p denote the set of the player names appearing in R and R , respectively .
We define the name matching score as NMS ( R , R ) = F-score ( N g , N p ) .
Similarly , the event matching score evaluates the closeness of the events in R and R .
We define an event as a pair ( subject , verb ) in the sentence .
Two pairs ( subject 1 , verb 1 ) and ( subject 2 , verb 2 ) are viewed as equivalent if and only if 1 ) subject 1 is the same as subject 2 and 2 ) verb 1 and verb 2 are synonym 7 to each other .
Let E g and E p represent the set of events in R and R , respectively , the event matching score is defined as EMS ( R , R ) = F-score( E g , E p ) .
Implementations and Models .
We consider the convolutional neural network ( Kim , 2014 ) as the selector .
For the rewriter , we consider the following : ( 1 ) LSTM : a bidirectional LSTM with attention mechanism ( Bahdanau et al. , 2015 ) .
( 2 ) Transformer .
( Vaswani et al. , 2017 ) ( 3 ) PGNet : pointergenerator network , an encoder-decoder model with copy mechanism ( See et al. , 2017 ) .
For comparison , we consider two extractive summarization baselines : ( 1 ) RawSent : the raw sentences selected by the selector without rewriting .
( 2 ) LTR : the learning - to - rank approach for sports game summarization proposed by the previous work ( Zhang et al. , 2016 ) .
In addition , we train a bidirectional LSTM with attention mechanism ( Abs - LSTM ) and a pointergenerator network ( Abs - PGNet ) on the paired commentaries and news articles as two simple abstractive summarization baselines .
More implementation details can be found in Appendix C. Results .
Table 6 shows the experimental results .
We observe that the extractive models ( RawSent and LTR ) get low ROUGE scores but high NMS and EMS .
That means the extractive models can generate summaries with correct information , but the writing style is different from the ground truth .
On the contrary , the abstractive models get higher ROUGE scores but lower NMS and EMS .
That implies the summaries generated by the abstractive models usually contain incorrect facts .
Our proposed two -step model performs better than the extractive models and the abstractive models on ROUGE scores , NMS , and EMS .
This verifies our design of the selector and the rewriter .
In addition , we observe that when training the model in a tem2tem way , we can get better NMS and EMS , which implies that training by tem2tem can improve the correctness of summaries .
Related Work Text summarization .
Existing approaches can be grouped into two families : extractive models and abstractive models .
Extractive models select a part of sentences from the source document as the summary .
Traditional approaches ( Carbonell and Goldstein , 1998 ; Erkan and Radev , 2004 ; Mc- Donald , 2007 ) utilize graph or optimization techniques .
Recently , neural models achieve good performance ( Cheng and Lapata , 2016 ; Nallapati et al. , 2017 ; Jadhav and Rajan , 2018 ) .
Abstractive summarization models aim to rephrase the source document .
Most work applies neural models for this task .
( Rush et al. , 2015 ; Chopra et al. , 2016 ; Nallapati et al. , 2016 ; Zeng et al. , 2016 ; See et al. , 2017 ; Gehrmann et al. , 2018 ) .
Factual correctness of summaries .
There is a lot of work focusing on evaluation and improvement of the factual correctness of summaries ( Falke et al. , 2019 ; Kryscinski et al. , 2019 ; Wang et al. , 2020 ; Maynez et al. , 2020 ; Zhu et al. , 2020 ) .
Conclusion
We present SPORTSSUM , a Chinese dataset for sports game summarization , as well as a model that consists of a selector and a rewriter .
To improve the quality of generated news , we train the model in a tem2tem way .
We design two metrics to evaluate the correctness of generated summaries .
The experimental results demonstrate that the proposed model performs well on ROUGE scores and the two designed scores .
