title
Predicting Salient Updates for Disaster Summarization
abstract
During crises such as natural disasters or other human tragedies , information needs of both civilians and responders often require urgent , specialized treatment .
Monitoring and summarizing a text stream during such an event remains a difficult problem .
We present a system for update summarization which predicts the salience of sentences with respect to an event and then uses these predictions to directly bias a clustering algorithm for sentence selection , increasing the quality of the updates .
We use novel , disaster -specific features for salience prediction , including geo-locations and language models representing the language of disaster .
Our evaluation on a standard set of retrospective events using ROUGE shows that salience prediction provides a significant improvement over other approaches .
Introduction
During crises , information is critical for first responders , crisis management organizations , and those caught in the event .
When the event is significant , as in the case of Hurricane Sandy , the amount of content produced by traditional news outlets , government agencies , relief organizations , and social media can vastly overwhelm those trying to monitor the situation .
Crisis informatics ( Palen et al. , 2010 ) is dedicated to finding methods for sharing the right information in a timely fashion during such an event .
Research in this field has focused on human- in- the -loop approaches ranging from on the ground information gathering to crowdsourced reporting and disaster management ( Starbird and Palen , 2013 ) .
Multi-document summarization has the potential to assist the crisis informatics community .
Automatic summarization could deliver relevant and salient information at regular intervals , even when human volunteers are unable to .
Perhaps more importantly it could help filter out unnecessary and irrelevant detail when the volume of incoming information is large .
While methods for identifying , tracking , and summarizing events from text based input have been explored extensively ( Allan et al. , 1998 ; Filatova and Hatzivassiloglou , 2004 ; Wang et al. , 2011 ) , these experiments were not developed to handle streaming data from a heterogeneous environment at web scale .
These methods also rely heavily on redundancy which is suboptimal for time sensitive domains where there is a high cost in delaying information .
In this paper , we present an update summarization system to track events across time .
Our system predicts sentence salience in the context of a large-scale event , such as a disaster , and integrates these predictions into a clustering based multidocument summarization system .
We demonstrate that combining salience with clustering produces more relevant summaries compared to baselines using clustering or relevance alone .
Our experiments suggest that this is because our system is better able to adapt to dynamic changes in input volume that adversely affect methods that use redundancy as a proxy for salience .
In addition to the tight integration between clustering and salience prediction , our approach also exploits knowledge about the event to determine salience .
Thus , salience represents both how typical a sentence is of the event type ( e.g. , industrial accident , hurricane , riot ) and whether it specifies information about this particular event .
Our feature representation includes a set of language models , one for each event type , to measure the typicality of the sentence with regard to the current event , the distance of mentioned locations from the center of the event , and the change in word frequencies over the time of the event .
While we evaluate these features in the domain of disasters , this approach is generally applicable to many update summarization tasks .
Our approach achieves a statistically significant improvement in ROUGE scores compared to multiple baselines .
Additionally , we introduce novel methods for estimating the average information gain each update provides and how completely the update summary covers the event it is tracking ; our system 's updates contain more relevant information on average than the competing baselines .
The remainder of the paper is organized as follows .
We begin with a review of related work in the information retrieval and multi-document summarization literature .
Section 3 outlines the details of our salience and summarization models .
Next we describe our data ( Section 4 ) and experiments ( Section 5 ) .
Finally , we discuss our results ( Section 6 ) and conclude the paper .
Related Work
A principal concern in extractive multi-document summarization is the selection of salient sentences for inclusion in summary output ( Nenkova and McKeown , 2012 ) .
Existing approaches generally fall into one of three categories , each with specific trade - offs with respect to update summarization .
First , centrality - focused approaches ( including graph ( Erkan and Radev , 2004 ) , cluster ( Hatzivassiloglou et al. , 2001 ) , and centroid ( Radev et al. , 2004 ) methods ) are very natural for retrospective analysis in the sense that they let the data " speak for itself . "
These methods equate salience with centrality , either to the input or some other aggregate object ( i.e. a cluster center or input centroid ) .
However , they rely chiefly on redundancy .
When applied to an unfolding event , there may not exist enough redundant content at the event onset for these methods to exploit .
Once the event onset has passed , however , the redundancy reduction of these methods is quite beneficial .
The second category , predictive approaches , includes ranking and classification based methods .
Sentences have been ranked by the average word probability , average TF*IDF score , and the number of topically related words ( topic-signatures in the summarization literature ) ( Nenkova and Vanderwende , 2005 ; Hovy and Lin , 1998 ; Lin and Hovy , 2000 ) .
The first two statistics are easily computable from the input sentences , while the third only requires an additional , generic background corpus .
In classification based methods , model features are usually derived from human generated summaries , and are non-lexical in nature ( e.g. , sentence starting position , number of topic-signatures , number of unique words ) .
Seminal work in this area has employed na?ve Bayes and logistic regression classifiers to identify sentences for summary inclusion ( Kupiec et al. , 1995 ; Conroy et al. , 2001 ) .
While these methods are less dependent on redundancy , the expressiveness of their features is limited .
Our model expands on these basic features to account for geographic , temporal , and language model features .
The last category includes probabilistic ( Haghighi and Vanderwende , 2009 ) , information theoretic , and set cover ( Lin and Bilmes , 2011 ) approaches .
While these methods are focused on producing diverse summaries , they are difficult to adapt to the streaming setting , where we do not necessarily have a fixed summary length and the corpus to be summarized contains many irrelevant sentences , i.e. there are large portions of the corpora that we specifically want to avoid .
Several researchers have recognized the importance of summarization during natural disasters .
( Guo et al. , 2013 ) developed a system for detecting novel , relevant , and comprehensive sentences immediately after a natural disaster .
( Wang and Li , 2010 ) present a clustering - based approach to efficiently detect important updates during natural disasters .
The algorithm works by hierarchically clustering sentences online , allowing the system to output a more expressive narrative structure than ( Guo et al. , 2013 ) .
Our system attempts to unify these system 's approaches ( predictive ranking and clustering respectively ) .
Method
Our update summarization system takes as input a ) a short query defining the event to be tracked ( e.g. ' Hurricane Sandy ' ) , b ) an event category defining the type of event to be tracked ( e.g. ' hurricane ' ) , c ) a stream of time -stamped documents presented in temporal order , and d ) an evaluation time period of interest .
While processing documents throughout the time period of interest , the system outputs sentences from these documents likely to be useful to the query issuer .
We refer to these selected sentences as updates .
In order to measure the usefulness of a system 's updates , we consider the degree to which the system output reflects the different aspects of - hurricane force wind warnings are in effect from Rhode Island Sound to Chincoteague Bay - over 5000 commercial airline flights scheduled for October 28 and October 29 were cancelled an event .
Events are often composed of a variety of sub-events .
For example , the Hurricane Sandy event includes sub-events related to the storm making landfall , the ensuing flooding , the many transportation issues , among many others .
An ideal system would update the user about each of these sub-events as they occur .
We refer to these sub-events as the nuggets associated with an event .
A nugget is defined as a fine-grained atomic sub-event associated with an event .
We present 2 example nuggets associated with the Hurricane Sandy event in Figure 1 . Each event has anywhere from 50 to several hundred nuggets in total in our gold dataset .
We describe how these nuggets are found in Section 4 .
Throughout our treatment of our algorithm , the salience of an update captures the degree to which it reflects an event 's unobserved nuggets .
Assuming that we have a text representation for each of our nuggets , the salience of an update u with respect to a set of nuggets N is defined as , salience ( u ) = max n?N sim ( u , n ) ( 1 ) where sim ( ? ) is the semantic similarity such as the cosine similarity of latent vectors associated with the update and nugget text ( Guo and Diab , 2012 ) .
Update Summarization
Our system architecture follows a simple pipeline design where each stage provides an additional level of processing or filtering of the input sentences .
We begin with an empty update summary U .
At each hour we receive a new batch of sentences b t from the stream of event relevant documents and perform the following actions : 1 . predict the salience of sentences in b t ( Section 3.2 ) , 2 . select a set of exemplar sentences in b t by combining clustering with salience predictions ( Section 3.3 ) , 3 . add the most novel and salient exemplars to U ( Section 3.4 ) .
The resultant list of updates U is our summary of the event .
Salience Prediction
Features
We want our model to be predictive of sentence salience across different event instances so we avoid event-specific lexical features .
Instead , we extract features such as language model scores , geographic relevance , and temporal relevance from each sentence .
Basic Features
We employ several basic features that have been used previously in supervised models to rank sentence salience ( Kupiec et al. , 1995 ; Conroy et al. , 2001 ) .
These include sentence length , the number of capitalized words normalized by sentence length , document position , and number of named entities .
The data stream comprises text extracted from raw html documents ; these features help to downweight sentences that are not content ( e.g. web page titles , links to other content ) or more heavily weight important sentences ( e.g. , that appear in prominent positions such as paragraph initial or article initial ) .
Query Features
Query features measure the relationship between the sentence and the event query and type .
These include the number of query words present in the sentence in addition to the number of event type synonyms , hypernyms , and hyponyms using WordNet ( Miller , 1995 ) .
For example , for event type earthquake , we match sentence terms " quake " , " temblor " , " seism " , and " aftershock " .
Language Model Features
Language models allow us to measure the likelihood of producing a sentence from a particular source .
We consider two types of language model features .
The first model is estimated from a corpus of generic news articles ( we used the 1995 - 2010 Associated Press section of the Gigaword corpus ( Graff and Cieri , 2003 ) ) .
This model is intended to assess the general writing quality ( grammaticality , word usage ) of an input sentence and helps our model to select sentences written in the newswire style .
The second model is estimated from text specific to our event types .
For each event type we create a corpus of related documents using pages and subcategories listed under a related Storm Earthquake Meteor Impact Accident Riot Protest Hostages Shooting Bombing Figure 2 : TREC TS event types .
Wikipedia category .
For example , the language model for event type ' earthquake ' is estimated from Wikipedia pages under the category Category : Earthquakes .
Fig. 2 lists the event types found in our dataset .
These models are intended to detect sentences similar to those appearing in summaries of other events in the same category ( e.g. most earthquake summaries are likely to include higher probability for ngrams including the token ' magnitude ' ) .
While we focus our system on the language of news and disaster , we emphasize that the use of language modeling can be an effective feature for multi-document summarization for other domains that have related text corpora .
We use the SRILM toolkit ( Stolcke and others , 2002 ) to implement a 5 - gram Kneser - Ney model for both the background language model and the event specific language models .
For each sentence we use the average token log probability under each model as a feature .
Geographic Relevance Features
The disasters in our corpus are all phenomena that affect some part of the world .
Where possible , we would like to capture a sentence 's proximity to the event , i.e. when a sentence references a location , it should be close to the area of the disaster .
There are two challenges to using geographic features .
First , we do not know where the event is , and second , most sentences do not contain references to a location .
We address the first issue by extracting all locations from documents relevant to the event at the current hour and looking up their latitude and longitude using a publicly available geo-location service .
Since the documents that are at least somewhat relevant to the event , we assume in aggregate the locations should give us a rough area of interest .
The locations are clustered and we treat the resulting cluster centers as the event locations for the current time .
The second issue arises from the fact that the majority of sentences in our data do not contain explicit references to locations , i.e. a sequence of tokens tagged as location named entities .
Our intuition is that geographic relevance is important in the disaster domain , and we would like to take ad-vantage of the sentences that do have location information present .
To make up for this imbalance , we instead compute an overall location for the document and derive geographic features based on the document 's proximity to the event in question .
These features are assigned to all sentences in the document .
Our method of computing document- level geographic relevance features is as follows .
Using the locations in each document , we compute the median distance to the nearest event location .
Because document position is a good indicator of importance we also compute the distance of the first mentioned location to the nearest event location .
All sentences in the document take as features these two distance calculations .
Because some events can move , we also compute these distances to event locations from the previous hour .
Temporal Relevance Features
As we track events over time , it is likely that the coverage of the event may die down , only to spike back up when there is a breaking development .
Identifying terms that are " bursty , " i.e. suddenly peaking in usage , can help to locate novel sentences that are part of the most recent reportage and have yet to fall into the background .
We compute the IDF for each hour in our data stream .
For each sentence , the average TF*IDF for the current hour t is taken as a feature .
Additionally , we use the difference in average TF*IDF from time t to t ?
i for i = { 1 , . . . , 24 } to measure how the TF*IDF scores for the sentence have changed over the last 24 hours , i.e. we keep the sentence term frequencies fixed and compute the difference in IDF .
Large changes in IDF value indicate the sentence contains bursty terms .
We also use the time ( in hours ) since the event started as a feature .
Model Given our feature representation of the input sentences , we need only target salience values for model learning .
For each event in our training data , we sample a set of sentences and each sentence 's salience is computed according to Equation 1 .
This results in a training set of sentences , their feature representations , and their target salience values to predict .
We opt to use a Gaussian process ( GP ) regression model ( Rasmussen and Williams , 2006 ) with a Radial Basis Function ( RBF ) kernel for the salience prediction task .
Our features fall naturally into five groups and we use a separate RBF kernel for each , using the sum of each feature group RBF kernel as the final input to the GP model .
Exemplar Selection
Once we have predicted the salience for a batch of sentences , we must now select a set of update candidates , i.e. sentences that are both salient and representative of the current batch .
To accomplish this , we combine the output of our salience prediction model with the affinity propagation algorithm .
Affinity propagation ( AP ) is a clustering algorithm that identifies a subset of data points as exemplars and forms clusters by assigning the remaining points to one of the exemplars ( Frey and Dueck , 2007 ) .
AP attempts to maximize the net similarity objective S = n i :i =e i sim ( i , e i ) + n i:i=e i salience ( e i ) where e i is the exemplar of the i-th data point , and functions sim and salience express the pairwise similarity of data points and our predicted apriori preference of a data point to be an exemplar respectively .
AP differs from other k-centers algorithms in that it simultaneously considers all data points as exemplars , making it less prone to finding local optima as a result of poor initialization .
Furthermore , the second term in S incorporates the individual importance of data points as candidate exemplars ; most other clustering algorithms only make use of the first term , i.e. the pairwise similarities between data points .
AP has several useful properties and interpretations .
Chiefly , the number of clusters k is not a model hyper-parameter .
Given that our task requires clustering many batches of streaming data , searching for an optimal k would be computationally prohibitive .
With AP , k is determined by the similarities and preferences of the data .
Generally lower preferences will result in fewer clusters .
Recall that salience ( s ) is a prediction of the semantic similarity of s to information about the event be summarized , i.e. the set of event nuggets .
Intuitively , when maximizing objective function S , AP must balance between best representing the input data and representing the most salient input .
Additionally , when the level of input is high but the salience predictions are low , the preference term will guide AP toward a solution with fewer clusters ; vice-versa when input is very salient on average but the volume of input is low .
The adaptive nature of our model differentiates our method from most other update summarization systems .
Update Selection
The exemplar sentences from the exemplar selection stage are the most salient and representative of the input for the current hour .
However , we need to reconcile these sentences with updates from the previous hour to ensure that the most salient and least redundant updates are selected .
To ensure that only the most salient updates are selected we apply a minimum salience threshold ; after exemplar sentences have been identified , any exemplars whose salience is less than ? sal are removed from consideration .
Next , to prevent adding updates that are redundant with previous output updates , we filter out exemplars that are too similar to previous updates .
The exemplars are examined sequentially in order of decreasing salience and a similarity threshold is applied , where the exemplar is ignored if its maximum semantic similarity to any previous updates in the summary is greater than ? sim .
Exemplars that pass these thresholds are selected as updates and added to the summary .
Data
For the document stream , we use the news portion of the 2014 TREC KBA Stream Corpus ( Frank et al. , 2012 ) .
The documents from this corpus come from hourly crawls of the web covering October 2011 through February 2013 .
Our experiments also make use of the TREC Temporal Summarization ( TS ) Track data from 2013 and 2014 ( Aslam et al. , 2013 ) .
This data includes 25 events and event metadata ( e.g. , a user search query for the event , the event type , and event evaluation time frame ) .
All events occurred during the time span of the TREC KBA Stream Corpus .
For each event we create a stream of relevant documents by selecting only documents that contain the complete set of query words .
Along with the metadata , NIST assessors constructed a set of ground truth nuggets for each event .
Nuggets are brief and important text snippets that represent sub-events that should be conveyed by an ideal update summary .
In order to accomplish this , for each event , assessors were provided with the revision history of the Wikipedia page associated with the event .
For example , the revision history for the Wikipedia page for ' Hurricane Sandy ' will contain text additions including those related to individual nuggets .
The assessment task involves reviewing the Wikipedia revisions in the evaluation time frame and marking the text additions capturing a new , unique nugget .
More detail on this process can be found in the track description ( Aslam et al. , 2013 ) .
Experiments
We evaluate our system on two metrics : ROUGE ( Lin , 2004 ) , an automatic summarization method and an evaluation of system expected gain and comprehensiveness -metrics adapted from the TREC TS track ( Aslam et al. , 2013 ) .
Training and Testing Of the 25 events in the TREC TS data , 24 are covered by the news portion of the TREC KBA Stream Corpus .
From these 24 , we set aside three events to use as a development set .
All system salience and similarity threshold parameters are tuned on the development set to maximize ROUGE - 2 F1 scores .
We train a salience model for each event using 1000 sentences randomly sampled from the event 's document stream .
We perform a leave- one - out evaluation of each event .
At test time , we predict a sentence 's salience using the average predictions of the 23 other models .
ROUGE Evaluation ROUGE measures the ngram overlap between a model summary and an automatically generated system summary .
Model summaries for each event were constructed by concatenating the event 's nuggets .
Generally , ROUGE evaluation assumes both model and system summaries are of a bounded length .
Since our systems are summarizing events over a span of two weeks time , the total length of our system output is much longer than the model .
To address this , for each system / event pair , we sample with replacement 1000 random summaries of length less than or equal to the model summary ( truncating the last sentence when neccessary ) .
The final ROUGE scores for the system are the average scores from these 1000 samples .
Because we are interested in system performance over time , we also evaluate systems at 12 hour intervals using the same regime as above .
The model summaries in this case are retrospective , and this evaluation reveals how quickly systems can cover information in the model .
Expected Gain and Comprehensiveness NIST developed metrics for evaluating update summarization systems as part of the TREC TS track .
We present results on two of these metrics , the expected gain and comprehensiveness .
Expected Gain
We treat the event 's nuggets as unique units of information .
When a system adds an update to its summary , it is potentially adding some of this nugget information .
It would be instructive to know how much unique and novel information each update is adding on average to the summary .
To that end , we define E[ Gain ] = | S n | | S | where S is the set of system updates , S n is the set of nuggets contained in S , and | ? | is the number of elements in the set .
To compute the set S n we match each system update to 0 or more nuggets , where an update matches a nugget if their semantic similarity is above a threshold .
S n results from the unique set of nuggets matched .
Because an update can map to more than one nugget , it is possible to receive an expected gain greater than 1 .
An expected gain of 1 would indicate that every sentence was both relevant and contained a unique piece of information .
Comprehensiveness
Additionally , we can use the nuggets to measure the completeness of an update summary .
We define Comprehensiveness = | S n | |N | where N is the set of event nuggets .
A comprehensiveness of 1 indicates that the summary has covered all nugget information for the event ; the maximum attainable comprehensiveness is 1 .
Update-nugget matches are computed automatically ; a match exists if the semantic similarity of the update / nugget pair is above a threshold .
Determining an optimal threshold to count matches is difficult so we evaluate at threshold values ranging from .5 to 1 , where values closer to 1 are more conservative estimates of performance .
A manual inspection of matches suggests that semantic similarity values around .7 produce reasonable matches .
The average semantic similarity of manual matches performed by NIST assessors was much lower at approximately .25 , increasing our confidence in the automatic matches in the .5 - 1 range .
Model Comparisons
We refer to our complete model as AP + SALIENCE .
We compare this model against several variants and baselines intended to measure the contribution of different components .
All thresholds for all runs are tuned on the development set .
Affinity Propagation only ( AP )
The purpose of this model is to directly measure the effect of integrating salience and clustering by providing a baseline that uses the identical clustering component , but without the salience information .
In this model , input sentences are apriori equally likely to be exemplars ; the salience values are uniformly set as the median value of the input similarity scores , as is commonly used in the AP literature ( Frey and Dueck , 2007 ) .
After clustering a sentence batch , the exemplars are examined in order of increasing time since event start and selected as updates if their maximum similarity to the previous updates is less than ? sim , as in the novelty filtering stage of AP + SALIENCE .
Hierarchichal Agglomerative Clustering ( HAC )
We provide another clustering baseline , single - linkage hierarchichal agglomerative clustering .
We include this baseline to show that AP +SALIENCE is not just an improvement over AP but centrality driven methods in general .
HAC was chosen over other clustering approaches because the number of clusters is not an explicit hyper-parameter .
To produce flat clusters from the hierarchical clustering , we flatten the HAC dendrogram using the cophenetic distance criteria , i.e. observations in each flat cluster have no greater a cophenetic distance than a threshold .
Cluster centers are determined to be the sentence with highest cosine similariy to the flat cluster mean .
Cluster centers are examined in time order and are added to the summary if their similarity to previous updates is below a similarity threshold ? sim as is done in the AP model .
Rank by Salience ( RS )
We also isolate the impact of our salience model in order to demonstrate that the fusion of clustering and salience prediction improves over predicting salience alone .
In this model we predict the salience of sentences as in step 1 for AP + SALIENCE .
We omit the cluster-ing phase ( step 2 ) .
Updates are selected identically to step 3 of AP + SALIENCE , proceeding in order of decreasing salience , selecting updates that are above a salience threshold ? sal and below a similarity threshold ? sim with respect to the previously selected updates .
6 Results
Table 1 shows our results for system output samples against the full summary of nuggets using ROUGE .
This improvement is statistically significant for all ngram precision , recall , and Fmeasures at the ? = .01 level using the Wilcoxon signed - rank test .
AP +SALIENCE maintains its performance above the baselines over time as well .
Figure 3 shows the ROUGE - 1 scores over time .
We show the difference in unigram precision ( bigram precision is not shown but it follows similar curve ) .
Within the initial days of the event , AP +SALIENCE is able to take the lead over the over systems in ngram precision .
The AP+SALIENCE model is better able to find salient updates earlier on ; for the disaster domain , this is an especially important quality of the model .
Moreover , the AP+SALIENCE 's recall is not diminished by the high precision and remains competitive with AP .
Over time AP +SALIENCE 's recall also begins to pull away , while the other models start to suffer from topic drift .
to 1 are more conservative estimates .
The ranking of the systems remains constant across the sweep with AP + SALIENCE beating all baseline systems .
Expected Gain and Comprehensiveness
Predicting salience in general is helpful for keeping a summary on topic as the RS approach out performs the clustering only approaches on expected gain .
When looking at the comprehensiveness of the summaries AP outperforms AP + SALIENCE .
The compromise encoded in the AP +SALIENCE objective function , between being representative and being salient , is seen clearly here where the performance of the AP +SALIENCE methods is lower bounded by the salience focused RS system and upper bounded by the clustering only AP system .
Overall , AP +SALIENCE achieves the best balance of these two metrics .
Feature Ablation
Table 2 shows the results of our feature ablation tests .
Removing the language models yields a statistically significant drop in both ngram recall and F-measure .
Interestingly , removing the basic features leads to an increase in both unigram and bigram precision ; in the bigram case this is enough to cause a statistically significant increase in F-measure over the full model .
In other words , the generic features actually lead to an inferior model when we can incorporate more appropriate domain specific features .
The result mirrors Sparck Jones ' claim that generic approaches to summarization cannot produce a useful summary ( Sparck - Jones , 1998 ) .
Removing the language model and geographic relevance features leads to a statistically significant drop in ROUGE -1 F1 scores .
Unfortunately , this is not the case for the temporal relevance features .
We surmise that these features are too strongly correlated with each other , i.e. the differences in TF*IDF between hours are definitely not i.i.d. variables .
Conclusion
In this paper , we have presented an update summarization system for the disaster domain , and demonstrated improved system performance by integrating sentence salience with clustering .
We also have shown that features specifically targeted to the domain of disaster yield better summaries .
We developed novel features that capture the language typical of different event types and that identify sentences specific to the particular disaster based on location .
In the future we would like to explore the appli -
