title
Pre-training for Abstractive Document Summarization by Reinstating Source Text
abstract
Abstractive document summarization is usually modeled as a sequence-to-sequence ( SEQ2SEQ ) learning problem .
Unfortunately , training large SEQ2SEQ based summarization models on limited supervised summarization data is challenging .
This paper presents three sequence - to-sequence pre-training ( in shorthand , STEP ) objectives which allow us to pre-train a SEQ2SEQ based abstractive summarization model on unlabeled text .
The main idea is that , given an input text artificially constructed from a document , a model is pre-trained to reinstate the original document .
These objectives include sentence reordering , next sentence generation and masked document generation , which have close relations with the abstractive document summarization task .
Experiments on two benchmark summarization datasets ( i.e. , CNN / DailyMail and New York Times ) show that all three objectives can improve performance upon baselines .
Compared to models pre-trained on large-scale data ( ? 160GB ) , our method , with only 19GB text for pre-training , achieves comparable results , which demonstrates its effectiveness .
Code and models are public available at https://github.com/ zoezou2015 / abs_pretraining .
Introduction Automatic document summarization is the task of condensing a document into its shorter form with important content preserved , which requires widecoverage understandings of the document , rather than specific words or phrases .
This task can be typically classified into two categories : extractive and abstractive document summarization .
Extractive summarization ( Cheng and Lapata , 2016 ; Nallapati et al. , 2017 ; Narayan et al. , 2018a ) aims to extract important sentences from the input document and concatenates such extracted sentences as the corresponding output summary .
Thus , the relative orders of the selected sentences in the summary is the same as their relative orders in the input document .
Differently , abstractive summarization ( Nallapati et al. , 2016 ; See et al. , 2017 ; Paulus et al. , 2018 ) rewrites the source text and generates the corresponding summary which may contain novel words and phrases not featured in the input .
The output summary is closely related to the input document .
Also , summary sentences , paraphrased from the input by the abstractive summarizers , might have a different relative order compared to the source text .
In other words , contents of the original document may be reordered in its summary .
Such a phenomena is defined as content reordering ( see Section 3.2 for detailed definition ) .
Statistically , we observed that around 40 % instances of the training split of our summarization dataset have this content reordering phenomena .
Therefore , it is necessary to design a model that is capable of reordering content .
However , as far as we know , relatively rare prior work has studied this for abstractive summarization .
Abstractive summarization is usually framed as a sequence-to-sequence ( SEQ2SEQ ) learning problem ( Nallapati et al. , 2016 ; See et al. , 2017 ) .
In this paper , we adopt the SEQ2SEQ Transformer ( Vaswani et al. , 2017 ) , which has been demonstrated to be the state - of - the - art for SEQ2SEQ modeling ( Vaswani et al. , 2017 ; . Recent studies ( Song et al. , 2019 ; Dong et al. , 2019 ; Zhang et al. , 2019a ; Raffel et al. , 2019 ) have proven effectiveness of pretrained SEQ2SEQ Transformer models on the natural language generation tasks , such as abstractive summarization .
Based on the above observations , with regard to abstractive summarization , this work proposes three sequence - to-sequence pre-training ( in shorthand , STEP ) objectives which can be used to pretrain a SEQ2SEQ model on unlabeled text , namely Sentence Reordering ( SR ) , Next Sentence Generation ( NSG ) , and Masked Document Generation ( MDG ) .
All three objectives are designed to reinstate the original source text .
SR learns to recover a document with randomly shuffled sentences .
Given the first segment of a document , NSG generates the next segment of the original document .
MDG learns to recover a masked document to its original form .
After pre-training a model with our proposed objective ( s ) on unlabeled documents , we finetune it on supervised summarization datasets ( i.e. , CNN / DailyMail and New York Times ) .
Experiments show that , even pre-training on documents from the training split of a summarization dataset , our method can improve performance upon a heavily tuned large SEQ2SEQ
Transformer model which already includes a strong pre-trained encoder by a large margin .
By involving more data ( 19GB ) for pre-training , the performance is further improved .
Compared to models pre-trained with much more data ( ? 160GB ) , we can still achieve comparable or even higher ROUGE scores .
Related Work Extractive Summarization
This task aims to find the informative sentences in a document as its summary .
This task is usually viewed as a sentence ranking problem ( Kupiec et al. , 1995 ; Conroy and O'leary , 2001 ) using scores from a binary ( sequence ) classification model , which predicts whether a sentence is in the summary or not .
Extractive neural models ( Cheng and Lapata , 2016 ; Nallapati et al. , 2017 ; Narayan et al. , 2018 b ; Zhang et al. , 2018 ) employ hierarchical LSTMs / CNNs as the feature learning part of the binary ( sequence ) classifier , which largely outperform discrete feature based models ( Radev et al. , 2004 ; Filatova and Hatzivassiloglou , 2004 ; Nenkova et al. , 2006 ) .
Very recently , the feature learning part was replaced again with pre-trained Transformer encoders ( Zhang et al. , 2019 b ; Liu and Lapata , 2019 ) that lead to another huge performance gain .
However , extractive models have their own limitations .
For example , the extracted sentences might be too long and redundant .
Besides , manually written summaries in their nature are abstractive .
Therefore , we focus on abstractive summarization in this paper .
Abstractive Summarization
This task aims to generate a summary by rewriting a document , which is a SEQ2SEQ learning problem .
SEQ2SEQ attentive LSTMs ( Hochreiter and Schmidhuber , 1997 ; Bahdanau et al. , 2015 ) are employed in Nallapati et al . ( 2016 ) that have been extended with copy mechanism ( Gu et al. , 2016 ) , coverage model ( See et al. , 2017 ) and reinforcement learning ( Paulus et al. , 2018 ) . Liu and Lapata ( 2019 ) used a SEQ2SEQ
Transformer model with only its encoder initialized with a pre-trained Transformer encoder ( i.e. , BERT ; Devlin et al. 2019 ) .
This work proposes to pre-train the decoder together with the encoder and then initialize both the encoder and decoder of a summarization model with the pre-trained Transformer model .
There is also a line of work that bridges extractive and abstractive models with attention mechanisms ( Gehrmann et al. , 2018 ; Hsu et al. , 2018 ) and reinforcement learning ( Chen and Bansal , 2018 ) , while our model is simpler .
Pre-training Pre-training methods draw a lot of attentions recently .
Peters et al. ( 2018 ) and Radford et al . ( 2019 ) pre-trained LSTM and Transformer using language modeling objectives .
To leverage the context in both directions , BERT ( Devlin et al. , 2019 ) is trained with the masked language modeling and next sentence prediction objectives .
SpanBERT ( Joshi et al. , 2020 ) applied only the masked language modeling objective that masks contiguous random spans , rather than random tokens .
XLNet ( Yang et al. , 2019 ) proposed a permutation language modeling objective that removes the independence assumption of masked tokens in BERT .
RoBERTa extends BERT with more training data and better training strategies .
The above models focus on pre-training an encoder or a decoder , while we propose methods to pre-train a SEQ2SEQ model ( i.e. , the encoder together with the decoder ) for abstractive summarization .
Dong et al. ( 2019 ) ( UniLM ) proposed a unified language model that can be used for both natural language understanding and generation tasks , which is pre-trained using masked , unidirectional and SEQ2SEQ language modeling objectives .
The encoder and decoder parameters are shared .
By contrast , we pre-train a SEQ2SEQ Transformer with separate parameters for the encoder and decoder .
Song et al. ( 2019 ) ( MASS ) proposed a method to pre-train a SEQ2SEQ
Transformer by masking a span of text and then predicting the masked tokens .
Their pre-training task is similar to our MDG task , but we apply a different masking strategy and predict the original text .
Song et al. ( 2019 ) tested their model on sentence - level tasks ( e.g. , machine translation and sentence compression ) , while we aim to solve document - level tasks ( e.g. , abstractive document summarization ) .
( BART ) adopted the combination of text infilling and sentence permutation as a single objective for SEQ2SEQ Transformer pretraining .
Differently , we propose three objectives and use them individually .
Specifically , MDG replaced each selected token with a masked token in the input sequence .
Raffel et al. ( 2019 ) ( T5 ) studies different pre-training objectives , model architectures , and unlabeled datasets .
ProphetNet ( Yan et al. , 2020 ) predicts the next n tokens simultaneously .
Zhang et al. ( 2019a ) ( PEGASUS ) proposed to remove / mask sentences from an input document and learn to generate such removed / masked sentences for pre-training , while NSG predicts the following sentences of the input sequence and MDG masks randomly selected tokens .
3 Proposed Method
Sequence-to-Sequence Learning
In this work , the task of abstractive document summarization is modeled as a SEQ2SEQ learning problem .
We adopt the SEQ2SEQ Transformer architecture ( Vaswani et al. , 2017 ) .
Given a document X = ( x 1 , x 2 , . . . , x | X| ) paired with its summary Y = (y 1 , y 2 , . . . , y | Y | ) , we aim to learn the model parameters ? and estimate the conditional probability : P ( Y |X ; ? ) = | Y | t=1 p(y t |y <t ; X ; ? ) ( 1 ) where y <t stands for all tokens before position t ( i.e. , y <t = ( y 1 , y 2 , . . . , y t?1 ) ) .
Given the whole training set ( X , Y ) , this model can be trained by maximizing the log-likelihood of the training document -summary pairs : L ( ? ; X , Y ) = ( X , Y ) ? ( X , Y ) log P ( Y |X ; ? ) ( 2 ) We first pre-train the SEQ2SEQ Transformer model on the unlabeled text using our proposed pre-training objectives ( see Section 3.2 ) and then fine -tune it on the document-summary dataset .
Pre-training Objectives Automatic abstractive summarization requires comprehensive understanding of the input document and rewrites the source text into its shorter form , where the summary is closely related to the input , retaining important contents .
Also , rewriting the document may result in content reordering .
Now , we define content reordering as follows .
For each document -summary pair , we first map each sentence in the summary to its corresponding sentence in the document by maximizing the ROUGE score ( see Appendix A more details ) .
If the relative orders of sentences in the summary are different from the relative orders of their mapped sentences in the original document , we count this as one content reordering .
According to the statistics on the training split of our summarization dataset , contents of the original documents are reordered in their summaries for 40 % of cases , approximately .
The above observations motivate us to propose sequence - to-sequence pre-training objectives that are capable of pre-training a SEQ2SEQ model serving the abstractive summarization task .
Sentence Reordering
In sentence reordering ( SR ) , we first divide an unlabeled document into multiple sentences based on full stops .
Let us change the notation of a document slightly in this paragraph .
Let X = ( S 1 ||S 2 || . . . || S m ) denote a document , where S i is a sentence , m is the number of sentences , and | | refers to sentence concatenation .
The sentence index order in X can be represented as O = ( 1 , 2 , . . . , m ) .
We then shuffle the document by sentences .
In other words , the items in the order O are rearranged and we obtain a shuffled order O S = ( a 1 , a 2 , . . . , a m ) , where 1 ? a i ? m , 1 ? a j ? m , and a i = a j for any i , j ? [ 1 , m ] and i = j.
Concatenating sentences following O S , we obtain a shuffled document XS = ( S a 1 ||S a 2 || . . . || S am ) .
A SEQ2SEQ model takes as input the shuffled document XS and is pre-trained to reinstate the original one X , as demonstrated in Figure 1 .
The training objective is calculated as : L ( ? ; X ) = X?X log P ( X| XS ; ?)
There are several reasons why we design this objective .
First , a summary of a document usually consists of multiple sentences .
We expect that the model is pre-trained to learn to generate long and coherent summaries ( across sentences ) .
The output of the objective ( i.e. , the original document ) also contains multiple sentences .
Second , as we discussed earlier , sentence reordering ( or content reordering ) is necessary for summarization .
Third , abstractive summary requires reproducing factual details ( e.g. , named entities , figures ) from the source document .
We also expect the model to learn to copy tokens .
Note that document rotation 1 is a special case of sentence reordering with a significant amount of partially ordered sentences , which we believe is a simpler objective .
In this work , we only consider the general case of sentence reordering .
Next Sentence Generation Next Sentence Generation ( NSG ) uses one span of text in a document to predict its next span of text , which leverages the natural order of text , as shown in Figure 1 . Specifically , we split a document into two segments ( i.e. , XG 1 and XG 2 ) .
Note that each segment might contain multiple sentences .
Intuitively , in a document , sentences are highly correlated with their 1 A document is randomly divided into two fragments X = ( F1 | | F2 ) using full stops .
The rotated document is XR = ( F2 | | F1 ) .
Document rotation recovers X using XR .
preceding sentences due to the context dependent nature of documents or language .
Our intention is to learn to generate multiple sentences and also learn to focus on input text , which fits the document summarization task , since either a document or its summary usually includes multiple sentences and they are closely related .
The training objective is calculated as : L ( ? ; X ) = X =( XG 1 || XG 2 ) , X?X log P ( XG 2 | XG 1 ; ?)
We do not make constraints that the split point must be the position right after a full-stop symbol , which ensures full sentences for each segment .
Instead , the split point can be at any position within the document , which may lead to incomplete sentences in segments .
We intend to force the model to understand input text without complete information .
Similarly , as a common wisdom in abstractive summarization , documents , as input , are truncated to a fixed number of tokens , which may also contain incomplete sentences .
This setting allows to reduce mismatches between the pretraining and fine-tuning input .
Masked Document Generation
The third objective is Masked Document Generation ( MDG ) that learns to reinstate a document with a masked span of tokens ( see Figure 1 ) .
A document is denoted as X = ( x 1 , x 2 , ? ? ? , x | X | ) .
We randomly sample the length of the span l from a discrete uniform distribution U ( a , b ) ( a and b are distribution parameters ) and the span starting position k from another discrete uniform distribution U( 1 , | X | ? l + 1 ) .
Thus , M = ( x k , x k+1 , ? ? ? , x k+l?1 ) is the text span to be masked .
Let XM denote the document after the application of our masking strategy .
The training objective is calculated as : L ( ? ; X ) = X?X log P ( X| XM ; ?)
One straightforward masking strategy is to replace each token residing in M with a special [ MASK ] token .
However , we refrain from doing so because of the following two reasons .
Usually , [ MASK ] tokens will not appear in downstream tasks .
Second , similar to SR , avoiding replacing every token with [ MASK ] also helps our model learn the ability of copying tokens from the input while preserving the ability of generating novel tokens .
Thus , in the sub-sequence M , each token is processed with one of the three strategies : 1 ) replaced with the [ MASK ] token ; 2 ) replaced with a random token ; 3 ) remains unchanged .
Inspired by BERT ( Devlin et al. , 2019 ) , for 80 % of selected tokens , we follow strategy 1 ) .
In 10 % of cases , we employ strategy 2 ) and we use strategy 3 ) for the remaining 10 % of cases .
During pre-training , we consider two settings .
Setting one : pre-training a model with one single objective , i.e. , SR , NSG or MDG , resulting in three different pre-trained models .
Setting two : employing all three objectives .
For each training batch , we randomly choose one objective and each objective is used for 1/3 of the training time , obtaining one model ( i.e. , ALL , see Section 5 ) .
For better reference , we name our model as STEP ( i.e. , sequence - to- sequence pre-training ) that can be used to denote a SEQ2SEQ model pretrained using our proposed objective ( s ) .
Fine-tuning After a SEQ2SEQ model is pre-trained , we finetune the model on abstractive document summarization datasets .
In other words , we continue to train the model on the document-summary pairs .
( Hermann et al. , 2015 ) .
Following previous work ( See et al. , 2017 ; Liu and Lapata , 2019 ) , we use the non-anonymized version of CNNDM .
Specifically , we preprocessed the dataset with the publicly available scripts 3 provided by See et al . ( 2017 ) and obtained 287,226 document - summary pairs for training , 13,368 for validation and 11,490 for test .
NYT
The NYT dataset ( Sandhaus , 2008 ) is a collection of articles along with multi-sentence summaries written by library scientists .
Following the preprocessing procedures described in ( Durrett et al. , 2016 ; Liu and Lapata , 2019 ) , the test set is constructed by including all articles published on January 1 , 2007 or later , which contains 9,076 articles .
The remaining 100,834 articles are split into a training set of 96,834 examples and a validation set of 4,000 examples .
Following ( Durrett et al. , 2016 ) , we also removed articles whose summaries contain less than 50 words from the test set , and the resulting test set contains 3,452 examples .
GIGA -CM
To pre-train our model with the objectives introduced in Section 3.2 , following the procedures in Zhang et al . ( 2019 b ) , we created the GIGA - CM dataset , which contains only unlabeled documents .
The training set of GIGA - CM is composed of 6,521,658 documents sampled from the English Gigaword dataset 4 and the training documents in CNNDM , resulting in 19GB text for pretraining .
We used the 13,368 documents in the validation split of CNNDM as the validation set .
Note that the Gigaword dataset overlaps with the NYT dataset and we therefore excluded the test set of NYT from the training set of GIGA - CM .
Table 1 lists the number of document-summary pairs ( for CNNDM and NYT ) and unlabeled documents ( for GIGA - CM ) .
For CNNDM , NYT and GIGA - CM datasets , we segmented and tokenized documents and / or summaries ( GIGA - CM only contains documents ) using the Stanford CoreNLP toolkit ( Manning et al. , 2014 ) .
We further applied the UTF8 based BPE ( Sennrich et al. ; Radford et al. , 2019 ) to reduce the vocabulary size .
As a common wisdom in abstractive summarization , documents and summaries in CNNDM and NYT are usually truncated to 512 and 256 tokens , respectively .
We leverage unlabeled documents differently for different pre-training objectives .
We first split each document into 512 - token pieces if it contains more than 512 tokens ( pieces or documents with less than 512 tokens are removed ) .
In SR and MDG , we use the piece after transformation to predict its original form .
We set the minimum and maximum masked length a = 100 and b = 256 in MDG individually .
In NSG , each piece is used to predict its next 256 tokens .
Implementation Details
As mentioned in Section 3 , we adopt the SEQ2SEQ Transformer model ( Vaswani et al. , 2017 ) as our backbone architecture .
The purpose of releasing large pre-trained models is to reuse so that the community can avoid high computational costs .
Hence , similar to previous work ( Liu and Lapata , 2019 ) , our encoder is initialized with a pretrained model , i.e. , RoBERTa LARGE model 5 , and therefore they share the same architecture .
Specifically , the encoder is a 24 - layer Transformer .
Each layer has 16 attention heads and its hidden size and feed -forward filter size are 1,024 and 4,096 , respectively .
The decoder is shallower with 6 layers and is randomly initialized .
The number of total trainable model parameters is 585M .
The hidden size and number of attention head of the decoder are identical to those of the encoder , but the feed -forward filter size is 2,048 .
We use a smaller filter size in the decoder to re-duce the computational and memory cost .
The dropout rates of all layers in the encoder are set to 0.1 and all dropout rates in the decoder are set to 0.3 .
Our models are optimized using Adam ( Kingma and Ba , 2015 ) with ?
1 = 0.9 , ? 2 = 0.98 .
The other optimization hyper-parameters for pretraining and fine-tuning are different .
In the pretraining stage , the encoder is initialized with a pre-trained model while the decoder is randomly initialized .
Therefore , similar to Liu and Lapata ( 2019 ) , we used two separate optimizers for the encoder and decoder .
The peak learning rates of the encoder and decoder are set to 2e ? 5 and 1e ? 4 with 10,000 warmup steps , respectively .
We also adopted the same learning rate schedule strategies as in Vaswani et al . ( 2017 ) .
We used smaller batch sizes for datasets with less examples ( i.e. , 1,024 for GIGA - CM , 256 for CNNDM and 128 for NYT ) to ensure each epoch has sufficient number of model updates .
We trained our models until their convergence of validation perplexities ( around 30 epochs on GIGA - CM , 60 epochs on CNNDM and 40 epochs on NYT ) .
One epoch on GIGA - CM takes around 24 hours with 8 Nvidia Tesla V100 GPUs .
The time costs for different pre-training objectives are close .
We highlight the parameters used in the finetuning stage that are different from the pre-training stage .
Others remain the same .
The learning rates for both the encoder and decoder are set to 2e - 5 with 4,000 warmup steps , since both the encoder and decoder are already pre-trained .
We trained our models for 30 epochs on CNNDM and 50 epochs on NYT , respectively .
We selected the best model with regard to ROUGE score on the validation set .
During decoding , similar to Liu and Lapata ( 2019 ) ; Dong et al. ( 2019 ) , we applied beam search with beam size of 5 .
We also conducted experiments on the validation set of CN - NDM with different beam sizes ( i.e. , 1 to 10 ) .
According to ROUGE -L , beam=5 is indeed optimal .
Detailed results with different beam sizes are included in the Appendix B. Following Paulus et al. ( 2018 ) , we also blocked repeated trigrams during beam search and tuned the minimum summary length on the validation set in the range of [ 30 , 80 ] .
The search range of minimum summary length was empirically set according to the summaries of training split of CNNDM , where the average and medium minimum lengths are both around 55 .
We used step size of 5 to get quick feedback .
Similar to the pre-training process , the datasets with less instances were fine-tuned with smaller batch sizes ( i.e. , 64 for NYT and 768 for CNNDM ) .
Results
Automatic Evaluation
We used ROUGE ( Lin , 2004 ) to measure the quality of different summarization model outputs .
We reported full- length F1 based ROUGE -1 , ROUGE - 2 and ROUGE -L scores on CN - NDM , while we used the limited - length recall based ROUGE -1 , ROUGE - 2 and ROUGE -L on NYT , following Durrett et al . ( 2016 ) .
The ROUGE scores are computed using the ROUGE -1.5.5.pl script 6 .
Models in Comparison Lead3 is a baseline which simply takes the first three sentences of a document as its summary .
BERTExt ( Liu and Lapata , 2019 ) is an extractive model fine-tuned on BERT ( Devlin et al. , 2019 ) that outperforms other extractive systems .
PTGen ( See et al. , 2017 ) , DRM ( Paulus et al. , 2018 ) , and DCA ( Celikyilmaz et al. , 2018 ) are SEQ2SEQ learning based models extended with copy and coverage mechanism , reinforcement learning , as well as deep communicating agents individually .
Bot-tomUp ( Gehrmann et al. , 2018 ) assisted summary generation with a word prediction model .
BERTAbs ( Liu and Lapata , 2019 ) and UniLM ( Dong et al. , 2019 ) are both pre-training based models and are trained based on BERT ( Devlin et al. , 2019 ) .
We also implemented four abstractive models as our baselines .
Transformer -S2S is a 12 - layer SEQ2SEQ
Transformer with random initialization .
When we replaced the encoder of Transformer - S2S with ROBERTA BASE or ROBERTA LARGE , we obtain two baselines , ROBERTA BASE - S2S and ROBERTA - S2S , respectively .
Following , we further train the ROBERTA LARGE on the documents of training split of CNNDM for 60 epochs , same as the number of epochs for our models ( indicated as " In-domain " ) .
We replaced the encoder of Transformer - S2S with this further trained model , resulting in ROBERTA CONT - S2S .
( Paulus et al. , 2018 ) 39.87 15.82 36.90 BottomUp ( Gehrmann et al. , 2018 ) 41.22 18.68 38.34 DCA ( Celikyilmaz et al. , 2018 ) 41.69 19.47 37.92 BERTAbs ( Liu and Lapata , 2019 ) 42.13 19.60 39.18 UniLM
( Dong et al. , 2019 ) 43 with ROBERTA CONT - S2S , although the encoders of such two models are pre-trained on the same corpus for the same epochs , our model achieves better performance .
This shows that the performance gains mainly result from our proposed objectives for pre-training the decoder together with the encoder .
Training RoBERTa longer may imunderstanding tasks , but no evidence shows longer training time for RoBERTa may improve generation performance .
Results on CNNDM
The results on the CN - NDM are listed in When we pre-train the SEQ2SEQ model on even larger dataset ( i.e. , GIGA - CM in the size of 19GB ) , indicated as STEP ( GIGA - CM ) , the results are further improved and our method outperforms all models under comparison , as listed in the bottom part of Table 2 . 3 presents results on NYT dataset .
Following the same evaluation protocol as Durrett et al . ( 2016 ) , we adopted the limited - length recall based ROUGE , where we truncated the predicted summaries to the length of the gold ones .
Again , the first and second blocks show results of previous extractive and abstractive models , respectively .
Results of our models are listed in the third block .
Similar to the trends in CNNDM , our method leads to significant performance gains ( with p < 0.05 ) .
better than the other two objectives ( i.e. , NSG and MDG ) .
We also tried to randomly use all the three objectives during training with 1/3 probability each ( indicated as ALL ) .
Interestingly , we observed that , in general , ALL outperforms all three objectives when employing unlabeled documents of training splits of CNNDM or NYT , which might be due to limited number of unlabeled documents of the training splits .
After adding more data ( i.e. , GIAG - CM ) for pre-training , SR consistently achieves the highest ROUGE - 2 on both CNNDM and NYT .
We conclude that SR is the most effective pre-training objective for abstractive summarization since sentence reordering objective fits content reordering and it requires comprehensively understanding a document in a wide coverage , going beyond individual words and sentences , which is highly close to the essence of abstractive document summarization .
Results on NYT Table
Comparisons among Objectives
We put the performance of our models on the validation splits of CNNDM and NYT in the Appendix B. Comparison to Models Pre-trained with Largescale Corpora
It is worth noting that several models have been released recently , which are pretrained using various corpora much larger than ours , as listed in Table 4 ( top part ) .
T5 ( Raffel et al. , 2019 ) introduced C4 ( 750GB ) as its pretraining corpus .
PEGASUS LARGE has two versions that are pre-trained on C4 and HugeNews ( 3,800 GB ) , respectively .
Both BART and ProphetNet ( 160GB ) ( Yan et al. , 2020 ) are pre-trained on a 160GB corpus introduced by .
We compare our best preforming model STEP ( i.e. , pre-training on the GIGA - CM dataset using SR objective ) with such models and focus on the performance on the CN - NDM which is the well - known benchmark for abstractive summarization .
We highlight the high-est ROUGE scores in Table 4 using bold font and use the symbol * to indicate the models that perform significantly different from STEP .
Both T5 and PEGASUS ( HugeNews ) achieve significantly higher ROUGE - 2 scores than our model .
However , we obtain higher ROUGE - 1 and ROUGE -L scores .
On the other hand , we also consider models pre-trained on the relatively small -scale corpus .
Following BERT ( Devlin et al. , 2019 ) , both ProphetNet ( 16GB ) ( Yan et al. , 2020 ) and UniLM ( Dong et al. , 2019 ) use the same 16GB text for pretraining .
As listed in Table 4 ( bottom part ) , our model significantly outperforms such two models .
Human Evaluation
Since summaries generated by abstractive models may produce disfluent or ungrammatical outputs , we also evaluated abstractive systems by eliciting human judgements .
We compared our best preforming model ( i.e. , pre-training on the GIGA - CM dataset using SR objective ) with human references ( denoted as Gold ) , as well as several strong baselines whose system outputs are available to us , including RoBERTa - S2S , and two pre-training based models , i.e. , BERTAbs ( Liu and Lapata , 2019 ) and UniLM ( Dong et al. , 2019 ) 9 . 50 documents are randomly sampled from the test split of CNNDM .
10 participants are presented with a document and a list of outputs generated by different abstractive summarization systems .
Then they are asked to rank the outputs of these systems from best to worst according to informativeness ( does the summary capture the informative part of the document ? ) , fluency ( is the summary grammatical ? ) , and succinctness ( does the summary express the document clearly in a few words ? )
We report the proportions of system rankings and mean rank ( lower is better ) in Table 5 .
The output of STEP is selected as the best for the 23 % of cases and we obtained lower mean rank than all systems except for Gold , which shows the participants ' preference for our model .
We further converted ranking numbers into ratings ( i.e. , rank i is converted into 6 ? i ) and applied the student t-test on the ratings .
Ours is significantly better than all other systems ( except for Gold ) in comparison with p < 0.05 .
However , it still lags behind human .
One possible reason is that our system ( as well as other systems ) only takes the first 512 tokens of a long document
Conclusion
We proposed three sequence- to-sequence pretraining objectives , including sentence reordering , next sentence generation , and masked document generation .
All those objectives have relations with abstractive summarization task and are designed based on reinstating the source text .
A SEQ2SEQ model for abstractive document summarization can be pre-trained using such objectives and then fine-tuned on the summarization dataset .
Compared to models pre-training on the even larger corpora ( ? 160GB ) , our method , with only 19GB for pre-training , can still achieve comparable and even better performance .
In the future , we would like to investigate other objectives to pre-train SEQ2SEQ models for abstractive summarization .
