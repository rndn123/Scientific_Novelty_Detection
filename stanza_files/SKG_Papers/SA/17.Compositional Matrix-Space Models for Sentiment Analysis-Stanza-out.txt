title
Compositional Matrix -Space Models for Sentiment Analysis
abstract
We present a general learning - based approach for phrase - level sentiment analysis that adopts an ordinal sentiment scale and is explicitly compositional in nature .
Thus , we can model the compositional effects required for accurate assignment of phrase -level sentiment .
For example , combining an adverb ( e.g. , " very " ) with a positive polar adjective ( e.g. , " good " ) produces a phrase ( " very good " ) with increased polarity over the adjective alone .
Inspired by recent work on distributional approaches to compositionality , we model each word as a matrix and combine words using iterated matrix multiplication , which allows for the modeling of both additive and multiplicative semantic effects .
Although the multiplication - based matrix-space framework has been shown to be a theoretically elegant way to model composition ( Rudolph and Giesbrecht , 2010 ) , training such models has to be done carefully : the optimization is nonconvex and requires a good initial starting point .
This paper presents the first such algorithm for learning a matrix -space model for semantic composition .
In the context of the phrase-level sentiment analysis task , our experimental results show statistically significant improvements in performance over a bagof-words model .
Introduction Sentiment analysis has been an active research area in recent years .
Work in the area ranges from identifying the sentiment of individual words to determining the sentiment of phrases , sentences and doc-uments ( see Pang and Lee ( 2008 ) for a survey ) .
The bulk of previous research , however , models just positive vs.
negative sentiment , collapsing positive ( or negative ) words , phrases and documents of differing intensities into just one positive ( or negative ) class .
For word- level sentiment , therefore , these methods would not recognize a difference in sentiment between words like " good " and " great " , which have the same direction of polarity ( i.e. , positive ) but different intensities .
At the phrase level , the methods will fail to register compositional effects in sentiment brought about by intensifiers like " very " , " absolutely " , " extremely " , etc . " Happy " and " very happy " , for example , will both be considered simply " positive " in sentiment .
In real-world settings , on the other hand , sentiment values extend across a polarity spectrum - from very negative , to neutral , to very positive .
Recent research has shown , in particular , that modeling intensity at the phrase level is important for real-world natural language processing tasks including question answering and textual entailment ( de Marneffe et al. , 2010 ) .
This paper describes a general approach for phrase - level sentiment analysis that takes these realworld requirements into account : we adopt a fivelevel ordinal sentiment scale and present a learningbased method that assigns ordinal sentiment scores to phrases .
Importantly , our approach will also be explicitly compositional 1 in nature so that it can accurately account for critical interactions among the words in each sentiment - bearing phrase .
Consider , for example , combining an adverb like " very " with a polar adjective like " good " .
" Good " has an a priori positive sentiment , so " very good " should be considered more positive even though " very " , on its own , does not bear sentiment .
Combining " very " with a negative adjective , like " bad " , produces a phrase ( " very bad " ) that should be characterized as more negative than the original adjective .
Thus , it is convenient to think of the effect of combining an intensifying adverb with a polar adjective as being multiplicative in nature , if we assume the adjectives ( " good " and " bad " ) to have positive and a negative sentiment scores , respectively .
Next , let us consider adverbial negators like " not " combined with polar adjectives .
When modeling only positive and negative labels for sentiment , negators are generally treated as flipping the polarity of the adjective it modifies ( Choi and Cardie , 2008 ; Nakagawa et al. , 2010 ) .
However , recent work ( Taboada et al. , 2011 ; Liu and Seneff , 2009 ) suggests that the effect of the negator when ordinal sentiment scores are employed is more akin to dampening the adjective 's polarity rather than flipping it .
For example , if " perfect " has a strong positive sentiment , then the phrase " not perfect " is still positive , though to a lesser degree .
And while " not terrible " is still negative , it is less negative than " terrible " .
For these cases , it is convenient to view " not " as shifting polarity to the opposite side of polarity scale by some value .
There are , of course , more interesting examples of compositional semantic effects on sentiment : e.g. , prevent cancer , ease the burden .
Here , the verbs prevent and ease act as content- word negators ( Choi and Cardie , 2008 ) in that they modify the negative sentiment of their direct object arguments so that the phrase as a whole is perceived as somewhat positive .
Nonetheless , the vast majority of methods for phrase - and sentence - level sentiment analysis do not tackle the task compositionally : they , instead , employ a bag-of-words representation and , at best , incorporate additional features to account for negators , intensifiers , and for contextual valence shifters , which can change the sentiment over neighboring words ( e.g. , Polanyi and Zaenen ( 2004 ) , , Kennedy and Inkpen ( 2006 ) , Shaikh et al . ( 2007 ) ) .
One notable exception is Moilanen and Pulman ( 2007 ) , who propose a compositional semantic approach to assign a positive or negative sentiment to newspaper article titles .
However , their knowledgebased approach presupposes the existence of a sentiment lexicon and a set of symbolic compositional rules .
But learning - based compositional approaches for sentiment analyis also exist .
Choi and Cardie ( 2008 ) , for example , propose an algorithm for phrase - based sentiment analysis that learns proper assignments of intermediate sentiment analysis decision variables given the a priori ( i.e. , out of context ) polarity of the words in the phrase and the ( correct ) phrase - level polarity .
As in Moilianen and Pulman ( 2007 ) , semantic inference is based on ( a small set of ) hand -written compositional rules .
In contrast , Nakagawa et. al ( 2010 ) use a dependency parse tree to guide the learning of compositional effects .
Each of the above , however , uses a binary rather than an ordinal sentiment scale .
In contrast , our proposed method for phraselevel sentiment analysis is inspired by recent work on distributional approaches to compositionality .
In particular , Baroni and Zamparelli ( 2010 ) tackle adjective -noun compositions using a vector representation for nouns and learning a matrix representation for each adjective .
The adjective matrices are then applied as functions over the meanings of nouns - via matrix -vector multiplication - to derive the meaning of adjective -noun combinations .
Rudolph and Giesbrecht ( 2010 ) show theoretically , that multiplicative matrix-space models are a general case of vector-space models and furthermore exhibit desirable properties for semantic analysis : they take into account word order and are algebraically , neurologically and psychologically plausible .
This work , however , does not present an algorithm for learning such models ; nor does it provide empirical evidence in favor of matrix -space models over vector-space models .
In the sections below , we propose a learningbased approach to assign ordinal sentiment scores to sentiment - bearing phrases using a general compositional matrix -space model of language .
In contrast to previous work , all words are modeled as matrices , independent of their part- of-speech , and compositional inference is uniformly modeled as ma-trix multiplication .
To predict an ordinal scale sentiment value , we employ Ordered Logistic Regression , introducing a novel training algorithm to accommodate our compositional matrix -space representations ( Section 2 ) .
To our knowledge , this is the first such algorithm for learning matrix -space models for semantic composition .
We evaluate the approach on a standard sentiment corpus ( Section 3 ) , making use of its manually annotated phrase - level annotations for polarity and intensity , and compare our approach to the more commonly employed bag-of-words model .
We show ( Section 4 ) that our matrix -space model significantly outperforms a bag-of-words model for the ordinal scale sentiment prediction task .
The Model for Ordinal Scale Sentiment Prediction
As described above , our task is to predict an ordinal scale sentiment value for a phrase .
We have chosen OLogReg , as opposed to say PRanking ( Crammer and Singer , 2001 ) , because optimization of the former is more attractive : the objective ( likelihood ) is smooth and the gradients are continuous .
As will become clear shortly , learning our models is not trivial and it is important to use sophisticated off - the-shelf optimizers such as L-BFGS .
For a bag-of-words model , OLogReg learns one weight for each word and a set of thresholds by maximizing the likelihood of the training data .
Typically , this is accomplished by using an optimizer like L-BFGS whose interface needs the value and gradient of the likelihood with respect to the parameters at their current values .
In the next subsections , we instantiate OLogReg for our sentiment prediction task using a matrix -space word model ( 2.1 and 2.2 ) and a bag-of-words model ( 2.3 ) .
The learning formulation of bag-of- words OLogReg is convex therefore we will get the global optimum ; in contrast , the optimization problem for matrix -space model is nonconvex , it is important to initialize the model well .
Initialization of the matrix-space model is discussed in Section 2.4 .
Notation
In the subsequent subsections we will use the following notation .
Let n be the number of phrases in the training set and let d be the number of words in the dictionary .
Let x i be the i-th phrase and y i would be the label of x i , where y i takes r different values y i ?
{ 0 , . . . , r ? 1 } . Then |x i | will denote the length of the phrase x i , and the words in i-th phrase are : x i = x i 1 , x i 2 , . . . , x i |x i | ; x i j , 1 ? j ?
|x i | is the j-th word of i-th phrase ; where x i j is from the dictionary : 1 ? x i j ? d.
In the case of the bag-of-words model , ?( x i ) ?
R d is the representation of the i-th phrase .
? j ( x i ) counts the number of times the j-th word from the dictionary appears in the i-th phrase .
Given a w ?
R d it assigns a score ?
i to a phrase x i by ?
i = w T ?( x i ) = |x i | j=1 w x i j ( 1 ) In the case of the matrix -space model the ?( x i ) ?
R |x i |?d is the representation of the i-th phrase .
? jk ( x i ) is 1 , if x i j is the k-th word in the dictionary , and zero otherwise .
Given u , v ?
R m and a set of matrices { W p ? R m?m } d p=1 , one for each word , it assigns a score ?
i to a phrase x i by ?
i = u T ? ? |x i | j=1 d k=1 W k ? jk ( x i ) ? ? v = u T ? ? |x i | j=1 W x i j ? ? v ( 2 ) where |x i | j=1 W x i j = W x i 1 W x i 2 ? ? ?
W x i |x i | in exactly this order .
We choose to map matrices to the real numbers by using vectors u and v from R m?1 ; so that ?
= u T M v , where M ? R m?m , which is sensitive to the order of matrices 2 , i.e. u T M 1 M 2 v = u T M 2 M 1 v. Modeling composition .
A m?m matrix , representing a word , can be considered as a linear function , mapping from R m to R m .
Composition of words is modeled by function composition , in our case composition of linear functions , i.e. matrix multiplication .
Note , that unlike bag-of-words model , the matrix -space model takes word order into account , since matrix multiplication is not commutative operation .
Ordered Logistic Regression
Now we will describe our objective function for OLogReg and its derivatives .
OLogReg has r ?
1 thresholds ( ?
0 , . . . ? r?2 ) , so introducing ? ?1 = ? and ? r?1 = ? leads to the unified expression for posterior probabilities for all values of k : P ( y i = k|x ) = P ( ? k?1 < ? i ? ? k ) = F (? k ? ? i ) ? F ( ? k?1 ? ? i ) F ( x ) is an inverse-logit function F ( x ) = e x 1 + e x this is its derivative : dF ( x ) dx = F ( x ) ( 1 ? F ( x ) )
Therefore the negative loglikelihood of the training data will look like the following ( Hardin and Hilbe , 2007 ) : L = ? n i=1 r?1 k=0 ln( F ( ? k ? ? i ) ? F ( ? k?1 ? ? i ) ) I(y i = k ) where r is the number of ordinal classes , ?
i is the score of i-th phrase , I is the indicator function that is equal to 1 - when y i = k , and zero otherwise .
We need to minimize the objective L with respect to the following constraints : ? k?1 ? ? k , 1 ? k ? r ? 2 ( 3 ) number .
For example , one other way to map matrices to the real numbers is to use the determinant of a matrix ; however , the determinant is not sensitive to the word order : det ( M1M2 ) = det ( M1 ) det ( M2 ) = det ( M2M1 ) ; which is not desirable for a model that needs to account for word order .
( The constraints are similar to the ones in PRank algorithm ) .
For ease of optimization we parametrize our model via ?
0 , and ? j , 1 ? j ? r ? 2 : ? ?1 = ? , ? 0 , ? 1 = ? 0 + ? 1 , ? 2 = ? 0 + 2 j=1 ? j , . . . , ? r?2 = ? 0 + r?2 j=1 ? j ? r?1 = ? , where ?
1 , . . . , ? r?2 are non-negative values , that represent how far the corresponding thresholds are from each other .
Then the constraints ( 3 ) would be : ? j ? 0 , 1 ? j ? r ? 2 ( 4 ) To simplify the equations we can rewrite the negative loglikelihood as follows : L = ? n i=1 r?1 k=0 ln( A ik ?
B ik ) I (y i = k ) ( 5 ) where A ik = F (?
0 + k j=1 ? j ? ? i ) , if k = 0 , . . . , r ? 2 1 , if k = r ?
1 B ik = 0 , if k = 0 F (? 0 + k?1 j=1 ? j ? ? i ) , if k = 1 , . . . , r ?
1 Let 's introduce L ik = ?
ln( A ik ?
B ik ) I(y i = k ) and then the derivative of L ik with respect to ?
0 will be : ?L ik ?
0 = ?[ A ik ( 1 ? A ik ) ?
B ik ( 1 ? B ik ) ]
A ik ?
B ik I(y i = k ) = ( A ik + B ik ?
1 ) I( y i = k)
For j = y i : ?L ik ? j = ?A ik ( 1 ? A ik ) A ik ?
B ik I(y i = k)
For all j < y i : ?L ik ? j = ( A ik + B ik ?
1 ) I( y i = k)
For all j > y i : ?L ik ? j = 0 .
The derivative with respect to the score ?
i is : ?L ik ? i = ( ? A ik ?
B ik + 1 ) I( y i = k ) ( 6 ) 2.2.1 Matrix -Space Word Model
Here we show the derivatives with respect to a word .
For the OLogReg model with matrix-space word representations , we have : ?L ?W x i j = ?L ? i ? ? i ?W x i j
The expression for ?L ?
i is given in ( 6 ) ; we will derive ?
i ?W x i j from ( 2 ) .
In the case of the Matrix - Space word model each word is represented as an m ?
m affine matrix W : W = A b 0 1 ( 7 ) We choose the class of affine matrices since for affine matrices matrix multiplication represents both operations : linear transformation and translation .
Linear transformation is important for modeling changes in sentiment - translation is also useful ( we make use of a translation vector during initialization , see Section 2.4 ) .
In this work we consider m ?
3 since we want the matrix A from ( 7 ) to represent rotation and scaling .
Applying the affine transformation W to vector [ x , 1 ] T is equivalent to applying linear transformation A and translation b to x .
3
Though vectors u and v can be learned together with word matrices W j , we choose to fix u and v .
The main intuition behind fixing u and v is to reduce the degrees of freedom of the model : different assignments of u , v and W j -s can lead to the same score ? , i.e. there exist ?
v and ?j -s different from u , v and W j -s respectively , such that ?( u , v , W ) would be equal to ?( ? , v , ? ) .
4 3 " A b 0 1 ? " x 1 ? = " Ax + b 1 ? where A is a linear transformation , b is a translation vector .
Also the product of affine matrices is an affine matrix .
4
The specific choice of u and v leads to an equivalent model for all ? and v such that ? = M T u , v = M ?1 v , where M is any invertible transformation ( i.e. ? , v are derived from u , v by applying linear transformations M T , M ?1 respectively ) : u T W1W2v = ( u T M ) ( M ?1 W1M ) ( M ?1 W2M ) ( M ?1 v ) = ?T ?1 ?2v
The derivative of the phrase ?
i with respect to j-th word W j would be ( for brevity we drop the phrase index and W j refers to W x i j and p refers to |x i | ) : ? i ?W j = ?u T W 1 W 2 . . . W p v ?W j = ( u T W 1 . . . W j?1 ) T ( W j+1 . . . W p v) T = ( W T j?1 . . . W T 1 ) ( uv T ) ( W T p . . . W T j+1 ) ( see Peterson and Pederson ( 2008 ) ) .
In case if a certain word appears multiple times in the phrase , the derivative with respect to that word would be a sum of derivatives with respect to each appearance of a word , while all other appearances are fixed .
For example , ?u T W W 1 W v ?W = u( W 1 W v) T + ( u T W W 1 ) T v T where W is a representation of a word that is repeated .
So given the expression ( 6 ) for ?L ? i , the derivative with respect to each word can be computed .
Notice that the update for the j-th word in a sentence depends on the order words , which is in line with our desire to account for word order .
Optimization
The goal of training procedure is for the i-th phrase with p words x 1 x 2 . . . x p to learn word matrices W 1 , W 2 , . . . , W p such that resulting ?
i -s will lead to the lowest negative loglikelihood .
The goal of training procedure is to find word matrices W 1 , W 2 , . . .
W p and thresholds ?
0 , ? 1 , . . . ? r?2 such that the negative loglikelihood is minimized .
So , given the negative loglikelihood and the derivatives with respect ?
0 and ?
j -s and word matrices W , we optimize objective ( 5 ) subject to ? j ? 0 . We use L-BFGS -B ( Large-scale Bound-constrained Optimization ) by Byrd et al. ( 1995 ) as an optimizer .
Regularization in Matrix -Space Model
In order to make sure that the L-BFGS - B updates do not cause numerical issues we perform the following regularization to the resulting matrices .
An m by m matrix W j that can be represented as : W j = A 11 a 12 a T 21 a 22 where A 11 ? R m?1?m?1 , a 12 , a 21 ? R m?1?1 , a 22 ? R. First make the matrix affine by updating the last row , then the updated matrix will look like : ?j = A 11 a 12 0 1 It can be proven that such a projection returns the closest affine matrix in Frobenius norm .
However , we also want to regularize the model to avoid ill-conditioned matrices .
Ill-conditioned matrices represent transformations whose output is very sensitive to small changes in the input and therefore they have a similar effect to having large weights in a bag-of-words model .
To perform such a regularization we " shrink " the singular values of A 11 towards one .
More specifically , we first use the Singular Value Decomposition ( SVD ) of the A 11 : U ?V
T = A 11 , where U and V are orthogonal matrices , ? is a matrix with singular values on the diagonal .
Then we update singular values in the following way to get ? : ? ii = ? h ii , where h is a parameter between 0 and 1 .
If h = 1 then ? ii remains the same .
In the extreme case h = 0 then ? h ii = 1 .
For intermediate values of h the singular values of A 11 would be brought closer to one .
Finally , we recompute ?11 : ?11 = U ?V T . So , W j would be : Wj = ?11 a 12 0 1
Learning in the Matrix -Space Model
We use Algorithm 1 to learn the matrix-space model .
What essentially happens is that we iterate two steps : optimizing the W matrices using L-BFGS -B and the projection step .
L-BFGS -B returns a solution that is not necessarily an affine matrix .
After projecting to the space of affine matrices we start L-BFGS - B from a better initial point .
In practice , the first few iterations lead to large decrease in negative loglikelihood .
Bag-Of-Words Model
In the bag-of-words model the score of the i-th phrase is given in ( 1 ) .
Therefore , the partial derivative with respect to j-th word in i-th phrase ? i ?w
x i j is equal to the number c j of times x j i appears in x i , so : ( W , ? 0 , ? j ) =minimize L using L-BFGS -B 7 : ?L ?w x i j = ?L ? i ? c j Algorithm 1 for i = 1 , . . . , d do 8 : W i = Project ( W i , h ) 9 : end for 10 : end for 11 : Return W , ? 0 , ? j Optimization .
We minimize negative loglikelihood using L-BFGS - B subject to ? j ?
0 . Regularization .
To prevent overfitting for bag-ofwords model we regularize w .
The L 2 -regularized negative loglikelihood will consist of the expression in ( 5 ) and an additional term ? 2 ||w|| 2 2 , where || ? || 2 is the L 2 -norm of a vector .
The derivative of the additional term with respect to w will be : ? ? 2 ||w|| 2 2 ?w = ?w
Hence the partial derivative with respect to w x i j will have an additional term ?w x i j .
Initialization Initialization of bag-of- words OLogReg .
We initialize the weight for each word with zero and ?
0 with a random number and ?
j -s with non-negative random numbers .
Since the learning problem for bag-of- words OLogReg is convex , we will get the global optimum .
Better Initialization of Matrix -Space Model .
Preliminary experiments showed that the Matrix - Space model needs a good initialization .
Initializing with different random matrices reaches different local minima and the quality of local minima depends on initialization .
Therefore , it is important to initialize the model with a good initial point .
One way to initialize the Matrix - Space model is to use the weights learned by the bag-of-words model .
We use the following intuition for initializing the Matrix - Space model .
As noted in Section 2.2.1 applying transformation
A of affine matrix W can model a linear transformation , while vector b represents a translation .
Since matrix-space model can encode a vectorspace model ( Rudolph and Giesbrecht , 2010 ) , we can initialize the matrices to exactly mimic the bagof-words model .
In order to do that we place the weight , learned by the bag-of-words model in the first component of b .
Let 's assume that w x 1 and w x 2 are the weights learned for two distinct words x 1 and x 2 respectively .
To compute the polarity score of a phrase x 1 , x 2 the bag-of-words model sums the weights of these two words : w x 1 and w x 2 .
Now we want to have the same effect in matrix -space model .
Here we assume m =
3 . Z = ? ? 1 0 w x 1 0 1 0 0 0 1 ? ? ? ? 1 0 w x 2 0 1 0 0 0 1 ? ? = ? ? 1 0 w x 1 + w x 2 0 1 0 0 0 1 ? ? Finally , there is a step of mapping matrix Z to a number using u and v , such that ?( Z ) = w x 1 + w x 2 .We also want vector u and v to be such that : u T ? ? 1 0 w x 1 + w x 2 0 1 0 0 0 1 ? ? v = w x 1 + w x 2 ( 8 ) The last equation can help us construct u and v .
We also set u and v to be orthogonal : u T v = 0 .
So , we arbitrarily choose two orthogonal vectors for which equation ( 8 ) holds : u = [ 1 , ? 2 , 1 ] T and v = [ 1 , ? ? 2 , 1 ] T . 5
Experimental Methodology
For experimental evaluation of the proposed method we use the publicly available Multi-Perspective Question Answering ( MPQA ) 6 corpus " medium " , " high " or " extreme " .
7
The schematic mapping of phrase polarity and intensity values on ordinal sentimental scale is shown in Table 1 .
Training Details
We perform 10 - fold cross-validation on phrases extracted from the MPQA corpus : eight folds for training ; one as a validation set ; and one as test set .
In total there were 8022 phrases .
Before training , we extract lemmas for each word .
For evaluation we use Ranking Loss : 1 n i |?
i ? y i | , where ?i is the prediction .
Choice of dimensionality m .
The reported experiments are done by setting m =
3 . Preliminary experiments with higher values of m ( 5 , 20 , 50 ) , did not lead to a better performance and increased the training time ; therefore we did not use those values in our final experiments .
Methods PRank .
For each of the folds , we run 500 iterations of PRank and choose an early stopping iteration using a model that led to the lowest ranking loss on the validation set ; afterwards report the average performance of on a test set .
Bag-of-words OLogReg .
To prevent overfitting we search for the best regularization parameter among the following values of ? : 10 i , from 10 ?4 to 10 4 .
The lowest negative log-likelihood value on the validation set is attained for 8 ? = 0.1 .
With this value of ? fixed , the final model is the one with the lowest negative loglikelihood on the training set .
Matrix-space OLogReg + RandInit .
First , we initialized matrices with with random numbers from normal distribution N ( 0 , 0.1 ) and set u and v as in section 2.4 , T is set to 25 .
We run with two different random seeds and three different values for the parameter h : [ 0.1 , 0.5 , 0.9 ] and report the performance of the model that had the lowest likelihood on the validation set .
The setting of h that lead to the best model was 0.9 .
Matrix-space OLogReg + BowInit .
For the matrixspace models we initialize the model with the output of the regularized Bag-of-words OLogReg as described in Section 2.4 , T is set to 25 .
Then we use the training procedure of Algorithm 1 .
We consider three different values for the parameter h [ 0.1 , 0.5 , 0.9 ] and choose as the model with the lowest validation set negative log-likelihood .
The best setting of h was 0.1 .
Method
Results and Discussion
We report Ranking Loss for the four models in Table 2 .
The worst performance ( denoted by the highest ranking loss value ) is obtained by PRank , followed by matrix-space OLogReg with random initialization .
Bag-of-words
OLogReg obtains quite good performance , and matrix-space OLogReg , initialized using the bag-of-words model performs the best , showing statistically significant improvements over the bag-of- words OLogReg model according to a paired t-test . .
To see what the bag-of- word and matrix -space models are learning we performed inference on a few examples .
In Table 3
Here we choose two popular adjectives like ' good ' and ' bad ' that appeared in the training data , and examine the effect of applying the intensifier ' very ' on the sentiment score .
As we can see , the matrix-space model learns a matrix that intensifies both ' bad ' and ' good ' in the correct sentiment scale , i.e. , ?( good ) < ?( very good ) and ?( bad ) < ?( very bad ) , while the bag-of- words model gets the sentiment of ' very bad ' wrong : it is more positive than ' bad ' .
We also looked at the effect of combining ' not ' with these adjectives .
The matrix-space model correctly encodes the effect of the negator for both positive and negative adjectives , such that ?( not good ) < ?( good ) and ?( bad ) < ?( not bad ) .
For the interesting case of applying a negator to a phrase with an intensifier , ?( not good ) should be less than ?( not very good ) and ?( not very bad ) should be less than ?( not bad ) .
9
As shown in Table 3 , these are predicted correctly by the matrixspace model , which the matrix -space model gets right , but the bag-of-words model misses in the case of " bad " .
Also notice that since in the matrix-space model each word is represented as a function , more specifically a linear operator , and the function composition defined as matrix multiplication , we can think of " not very " being an operator itself , that is a composition of operator " not " and operator " very " .
Related Work Sentiment Analysis .
There has been a lot of research in determining the sentiment of words and constructing polarity dictionaries ( Hatzivassiloglou and McKeown , 1997 ; Wiebe , 2000 ; Rao and Ravichandran , 2009 ; Mohammad et al. , 2009 ; . Some recent work is trying to identify the degree of sentiment of adjectives and adverbs from text using co-occurrence statistics .
Work by Taboada et. al ( 2011 ) and Liu and Seneff ( 2009 ) , suggest ways of computing the sentiment of adjectives from data , and computing the effect of combining adjective with adverb as multiplicative effect and combining adjective with negation as additive effect .
However these models require the knowledge of a part of speech of given words and the list of negators ( since the negator is an adjective as well ) .
In our work we propose a single unified model for handling all words of any part of speech .
On the other hand , there has been some research in trying to model compositional effects for sentiment at the phrase - and sentence - level .
Choi and Cardie ( 2008 ) hand -code compositional rules in order to model compositional effects of combining different words in the phrase .
The hand -coded rules are based on domain knowledge and used to learn the effects of combining words in the phrase .
Another recent work that tries to model the compositional semantics of combining different words is Nakagawa et. al. ( 2010 ) , which proposes a model that learns the effects of combining different words using phrase / sentence dependency parse trees and an initial polarity dictionary .
They present a learning method that employs hidden variables for sentiment classification : given the polarity of a sentence and the a priori polarities of its words , they learn how to model the interactions between words with headmodifier relations in the dependency tree .
Some of the previous work looked at MPQA phrase -level classification .
Wilson et al. ( 2004 ) tackles the problem of classifying clauses according to their subjective strength but not polarity ; classifies phrases according to their polarity / sentiment but not strength .
Our task is different : we classify phrases according to a single ordinal scale that combines both polarity and strength .
Task of predicting document- level star ratings was considered in ( Pang and Lee , 2005 ; Goldberg and Zhu , 2006 ) .
In the current work we look at finegrained sentiment analysis , more specifically we study word representations for use in true compositional semantic settings .
Distributional Semantics and Compositionality .
Research in the area of distributional semantics in NLP and Cognitive Science has looked at different word representations and different ways of combining words .
Mitchell and Lapata ( 2010 ) propose a framework for vector-based semantic composition .
They define composition as an additive or multiplicative function of two vectors and show that compositional approaches generally outperform non-compositional approaches that treat the phrase as the union of single lexical items .
Work by Baroni and Zamparelli ( 2010 ) models nouns as vectors in some semantic space and adjectives as matrices .
It shows that modeling adjectives as linear transformations and applying those linear transformations to nouns results in final vectors for adjective -noun compositions that are close in semantic space to other similar phrases .
The authors argue that modeling adjectives as a linear transformation is a better idea than using additive vector-space models .
In this work , a separate matrix for each adjective is learned using the Partial Least Squares method in a completely unsupervised way .
The recent paper by Rudolph and Giesbrecht ( 2010 ) , described in the introduction , argues for multiplicative matrix -space models .
In contrast to other work in this area , our work is concerned with a specific dimension of word meaning - sentiment .
Our techniques , however , are quite general and should be applicable to other problems in lexical semantics .
Conclusions and Future work
In the current work we present a novel matrix -space model for ordinal scale sentiment prediction and an algorithm for learning such a model .
The proposed model learns a matrix for each word ; the composition of words is modeled as iterated matrix multiplication .
The matrix-space framework with iterated matrix multiplication defines an elegant framework for modeling composition ; it is also quite general .
We use the matrix-space framework in the context of sentiment prediction , a domain where interesting compositional effects can be observed .
The main focus of this work was to study word representations ( represent as a single weight vs. as a matrix ) for use in true compositional semantic settings .
One of the benefits of the proposed approach is that by learning matrices for words , the model can handle unseen word compositions ( e.g. unseen bigrams ) when the unigrams involved have been seen .
However , it is not trivial to learn a matrix -space model .
Since the final optimization problem is nonconvex , the initialization has to be done carefully .
Here the weights learned in bag-of-words model come to rescue and provide good initial point for optimization procedure .
The final model outperforms the bag-of-words based model , which suggests that this research direction is very promising .
Though in our model the order of composition is the same as the word order , we believe that a linguistically informed order of composition can give us further performance gains .
For example , one can use the output of a dependency parser to guide the order of composition , similar to Nakagawa et al . ( 2010 ) .
Another possibility for improvement is to use the information about the scope of negation .
In the current work we assume the scope of negation to be the expression following the negation ; in reality , however , determining the scope of negation is a complex linguistic phenomenon ( Moilanen and Pulman , 2007 ) .
So the proposed model can benefit from identifying the scope of negation , similar to ( Councill et al. , 2010 ) .
Also we plan to consider other ways to initialize the matrix-space model .
One interesting direction to explore might be to use non-negative matrix factorization ( Lee and Seung , 2001 ) , co-clustering techniques ( Dhillon , 2001 ) to better initialize words that share similar contexts .
The other possible direction is to use existing sentiment lexicons and employing a " curriculum learning " strategy ( Bengio et al. , 2009 ; Kumar et al. , 2010 ) for our learning problem .
Training Algorithm for Matrix -Space OLogReg 1 : Input : {( x 1 , y 1 ) , . . . , ( x n , y n ) } //training data 2 : Input : h //projection parameter 3 : Input : T //number of iterations 4 : Input : W , ? 0 and ? j //initial values 5 : for t = 1 , . . . , T do 6 :
Table 1 : 1 Mapping of combination of polarities and intensities from MPQA dataset to our ordinal sentiment scale .
version 1.2 , which contains 535 newswire documents that are manually annotated with phrase - level subjectivity and intensity .
We use the expression - level boundary markings in MPQA to extract phrases .
We evaluate on positive , negative and neutral opinion expressions that have intensities
Table 2 : 2 Ranking loss for vector-space Ordered Logistic Regression and Matrix -Space Logistic Regression .
Ranking loss PRank 0.7808 Bag-of-words OLogReg 0.6665 Matrix-space OLogReg + RandInit 0.7417 Matrix-space OLogReg+BowInit 0.6375 ? ? Stands for a significant difference w.r.t. the Bag-Of-Words OLogReg model with p-value less than 0.001 ( p < 0.001 )
we show the sentiment scores of the best performing bag-of- words OLo - gReg model and the best performing model based Phrase Matrix-space Bag-of-words OLogReg+ BowInit OLogReg not -0.83 - 0.42 very 0.23 0.04 good 2.81 1.51 very good 3.53 1.55 not good - 0.16 1.09 not very good 0.66 1.13 bad - 1.67 - 1.42 very bad - 2.01 - 1.38 not bad - 0.54 - 1.85 not very bad - 1.36 - 1.80
Table 3 : 3 Phrase and the sentiment scores of the phrase for 2 models Matrix-space OLogReg + BowInit and Bag-ofwords OLogReg respectively .
Notice that relative ranking order what matters on matrices Matrix-space OLogReg + BowInit .
By sentiment score , we mean equation ( 1 ) of Bag-of - words OLogReg and equation ( 2 ) of Matrix -space OLogReg + BowInit .
The Principle of Compositionality asserts that the meaning of a complex expression is a function of the meanings of its constituent expressions and the rules used to combine them .
Care must be taken in choosing way to map matrix to a real
If m > 3 , u and v can be set using the same intuition .
6 http://www.cs.pitt.edu/mpqa/
We ignored low-intensity phrases similar to ( Choi and Cardie , 2008 ; Nakagawa et al. , 2010 ) .8
We pick single ?
that gives best average validation set performance , and then use it to compute the average test set performance .
See the detailed discussion in Taboada et al . ( 2011 ) and Liu and Seneff ( 2009 ) .
