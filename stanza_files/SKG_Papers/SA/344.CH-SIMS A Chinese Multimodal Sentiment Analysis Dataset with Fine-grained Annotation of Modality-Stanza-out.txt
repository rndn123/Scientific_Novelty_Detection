title
CH-SIMS : A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality
abstract
Previous studies in multimodal sentiment analysis have used limited datasets , which only contain unified multimodal annotations .
However , the unified annotations do not always reflect the independent sentiment of single modalities and limit the model to capture the difference between modalities .
In this paper , we introduce a Chinese single - and multimodal sentiment analysis dataset , CH - SIMS , which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations .
It allows researchers to study the interaction between modalities or use independent unimodal annotations for unimodal sentiment analysis .
Furthermore , we propose a multi-task learning framework based on late fusion as the baseline .
Extensive experiments on the CH-SIMS show that our methods achieve state - of - the - art performance and learn more distinctive unimodal representations .
The full dataset and codes are available for use at https://github.com/ thuiar / MMSA .
Introduction Sentiment analysis is an important research area in Natural Language Processing ( NLP ) .
It has wide applications for other NLP tasks , such as opinion mining , dialogue generation , and user behavior analysis .
Previous study ( Pang et al. , 2008 ; Liu and Zhang , 2012 ) mainly focused on text sentiment analysis and achieved impressive results .
However , using text alone is not sufficient to determine the speaker 's sentimental state , and text can be misleading .
With the booming of short video applications , nonverbal behaviors ( vision and audio ) are introduced to solve the above shortcomings ( Zadeh et al. , 2016 ; .
In multimodal sentiment analysis , intra-modal representation and inter-modal fusion are two im - * * Corresponding Author portant and challenging subtasks ( Baltru? aitis et al. , 2018 ; Guo et al. , 2019 ) .
For intra-modal representation , it is essential to consider the temporal or spatial characteristics in different modalities .
The methods based on Convolutional Neural Network ( CNN ) , Long Short-term Memory ( LSTM ) network and Deep Neural Network ( DNN ) are three representative approaches to extract unimodal features Zadeh et al. , , 2018a .
For inter-modal fusion , numerous methods have been proposed in recent years .
For example , concatenation , Tensor Fusion Network ( TFN ) , Lowrank Multimodal Fusion ( LMF ) ( Liu et al. , 2018 ) , Memory Fusion Network ( MFN ) ( Zadeh et al. , 2018a ) , Dynamic Fusion Graph ( DFG ) ( Zadeh et al. , 2018 b ) , and others .
In this paper , we mainly consider late-fusion methods that perform intramodal representation learning first and then employ inter-modal fusion .
An intuitive idea is that the greater the difference between inter-modal representations , the better the complementarity of intermodal fusion .
However , it is not easy for existing late-fusion models to learn the differences between different modalities , further limits the performance of fusion .
The reason is that the existing multimodal sentiment datasets only contain a unified multimodal annotation for each multimodal segment , which is not always suitable for all modalities .
In other words , all modalities share a standard annotation during intra-modal representation learning .
Further , these unified supervisions will guide intra-modal representations to be more consistent and less distinctive .
To validate the above analysis , in this paper , we propose a Chinese multimodal sentiment analysis dataset with independent unimodal annotations , CH -SIMS .
Figure 1 shows an example of the annotation difference between our proposed dataset and the other existing multimodal datasets .
SIMS has 2,281 refined video clips collected from different movies , TV serials , and variety shows with spontaneous expressions , various head poses , occlusions , and illuminations .
The CHEAVD ( Li et al. , 2017 ) is also a Chinese multimodal dataset , but it only contains two modalities ( vision and audio ) and one unified annotation .
In contrast , SIMS has three modalities and unimodal annotations except for multimodal annotations for each clip .
Therefore , researchers can use SIMS to do both unimodal and multimodal sentiment analysis tasks .
Furthermore , researchers can develop new methods for multimodal sentiment analysis with these additional annotations .
Based on SIMS , we propose a multimodal multitask learning framework using unimodal and multimodal annotations .
In this framework , the unimodal and multimodal tasks share the feature representation sub-network in the bottom .
It is suitable for all multimodal models based on late-fusion .
Then , we introduce three late-fusion models , including TFN , LMF , and Late-Fusion DNN ( LF - DNN ) , into our framework .
With unimodal tasks , the performance of multimodal task is significantly increased .
Furthermore , we make a detailed discus-sion on multimodal sentiment analysis , unimodal sentiment analysis and multi-task learning .
Lastly , we verify that the introduction of unimodal annotations can effectively expand the difference between different modalities and obtain better performance in inter-modal fusion .
In this work , we provide a new perspective for multimodal sentiment analysis .
Our main contributions in this paper can be summarized as follows : ?
We propose a Chinese multimodal sentiment analysis dataset with more fine- grained annotations of modality , CH -SIMS .
These additional annotations make our dataset available for both unimodal and multimodal sentiment analysis . ?
We propose a multimodal multi-task learning framework , which is suitable for all latefusion methods in multimodal sentiment analysis .
Besides , we introduce three late-fusion models into this framework as strong baselines for SIMS .
?
The benchmark experiments on the SIMS show that our methods learn more distinctive unimodal representations and achieve state- ofthe - art performance .
Related Work
In this section , we briefly review related work in multimodal datasets , multimodal sentiment analysis , and multi-task learning .
Multimodal Datasets
To meet the needs of multimodal sentiment analysis and emotion recognition , researchers have proposed various of multimodal datasets , including IEMOCAP ( Busso et al. , 2008 ) , YouTube ( Morency et al. , 2011 ) , MOUD ( P?rez-Rosas et al. , 2013 ) , ICT -MMMO ( W?llmer et al. , 2013 ) , MOSI ( Zadeh et al. , 2016 ) , CMU -MOSEI ( Zadeh et al. , 2018 b ) and so on .
In addition ,
Multi-task Learning Multi-task learning aims to improve the generalization performance of multiple related tasks by utilizing useful information contained in these tasks ( Zhang and Yang , 2017 ) .
A classical method is that different tasks share the first several layers and then have task -specific parameters in the subsequent layers ( Liu et al. , 2015 ; Zhang et al. , 2016 b ) .
Based on this method , we design a multimodal multi-task learning framework for verifying the practicality and feasibility of independent unimodal annotations .
CH-SIMS Dataset
In this section , we introduce a novel Chinese multimodal sentiment analysis dataset with independent unimodal annotations , CH -SIMS .
In the following subsections , we will explain the data acquisition , annotation , and feature extraction in detail .
Data Collection
Comparing with unimodal datasets , the requirements of multimodal datasets are relatively high .
A fundamental requirement is that the speaker 's face and voice must appear in the picture at the same time and remain for a specific period of time .
In this work , to acquire video clips as close to life as possible , we collect target fragments from movies , TV series , and variety shows .
After getting raw videos , we use video editing tools , Adobe Premiere Pro 1 , to crop target segments at the frame level , which is very time - consuming but accurate enough .
Moreover , during the data collection and cropping , we enforce the following constraints : ?
We only consider mandarin and are cautious with the selection of materials with the accent . ?
The length of clips is no less than one second and no more than ten seconds .
?
For each video clip , no other faces appear except for the speaker 's face .
Finally , we collect 60 raw videos and acquire 2,281 video segments .
SIMS has rich character background , wide age range , and high quality .
Table 1 shows the basic statistics for SIMS .
2
Annotation
We make one multimodal annotation and three unimodal annotations for each video clip .
In addition to the increase in workload , the mutual interference between different modalities is more confused .
To avoid this problem as much as possible , we claim every labeler can only see the information in the current modality when annotating .
Besides , conducting four annotations at the same time is not permitted .
More precisely , every labeler makes unimodal annotation first and then performs multimodal annotation , which of the order is text first , audio second , then silent video , and multimodal last .
For each clip , every annotator decides its sentimental state as - 1 ( negative ) , 0 ( neutral ) or 1 ( positive ) .
we have five independent students in this field making annotations .
Then , in order to do both regression and multi-classifications tasks , we average the five labeled results .
Therefore , the final labeling results are one of { -1.0 , -0.8 , -0.6 , -0.4 , -0.2 , 0.0 , 0.2 , 0.4 , 0.6 , 0.8 , 1.0 } .
We further divide these values into 5 classifications : negative { - 1.0 , - 0.8 } , weakly negative { - 0.6 , -0.4 , -0.2 } , neutral { 0.0 } , weakly positive { 0.2 , 0.4 , 0.6 } and positive { 0.8 , 1.0 } .
The histogram in the left of Figure 2 shows the distribution of sentiment over the entire dataset in four annotations .
We can see that negative segments are more than positive segments .
The main reason is that actors in film and television dramas are more expressive in negative sentiments than positive ones .
The confusion matrix in the right of Figure 2 indicates the annotations difference between different modalities , which is computed as : D ij = 1 N N n=1 ( A n i ?
A n j ) 2 ( 1 ) where i , j ?
{ m , t , a , v} , N is the number of all samples , A n i means the n th label value in modal i .
From the confusion matrix , we can see that the difference between A and M is minimal , and the difference between V and T is maximal , which is in line with expectations .
Because audio contains text information , closer to multimodal while the connection between video and text is sparse .
Furthermore , we provide the other attribute annotations , including speakers ' age and gender .
And we use sentimental annotations only in our following experiments .
Extracted Features
The extracted features for all modalities are as follows ( we use the same basic features in all experiments ) :
Text : All videos have manual transcription , including the Chinese and English versions .
We use Chinese transcriptions only .
We add two unique tokens to indicate the beginning and the end for each transcript .
And then , pre-trained Chinese BERTbase word embeddings are used to obtain word vectors from transcripts ( Devlin et al. , 2018 ) .
It is worth noting that we do not use word segmentation tools due to the characteristic of BERT .
Eventually , each word is represented as a 768 - dimensional word vector .
Audio :
We use LibROSA ( McFee et al. , 2015 ) speech toolkit with default parameters to extract acoustic features at 22050 Hz .
Totally , 33 dimensional frame- level acoustic features are extracted , including 1 - dimensional logarithmic fundamental frequency ( log F0 ) , 20 - dimensional Melfrequency cepstral coefficients ( MFCCs ) and 12dimensional Constant - Q chromatogram ( CQT ) .
These features are related to emotions and tone of speech according to ( Li et al. , 2018 ) .
Text SubNet Audio SubNet Video SubNet Feature Fusion Network FC FC FC FC ?
Y t < l a t e x i t s h a 1 _ b a s e 6 4 = " u B D p O 1 u e g T I N U k X e 5 q 6 J R p / L R v E = " > A A A B 6 n i c b V C 7 S g N B F L 0 b X z G + o o K N z W I Q r M J u L L Q M s b F M 0 D w k W c L s Z D Y Z M j u 7 z N w V w p J P s L F Q x N b W v / A L 7 G z 8 F i e P Q h M P X D i c c y / 3 3 u P H g m t 0 n C 8 r s 7 K 6 t r 6 R 3 c x t b e / s 7 u X 3 D x o 6 S h R l d R q J S L V 8 o p n g k t W R o 2 C t W D E S + o I 1 / e H V x G / e M 6 V 5 J G 9 x F D M v J H 3 J A 0 4 J G u n m r o v d f M E p O l P Y y 8 S d k 0 L 5 q P b N 3 y s f 1 W 7 + s 9 O L a B I y i V Q Q r d u u E 6 O X E o W c C j b O d R L N Y k K H p M / a h k o S M u 2 l 0 1 P H 9 q l R e n Y Q K V M S M T K E U M X N r T Y d E E U o m n R y J g R 3 8 e V l 0 i g V 3 f N i q W b S q M A M W T i G E z g D F y 6 g D N d Q h T p Q 6 M M D P M G z J a x H 6 8 V 6 n b V m r P n M I f y B 9 f Y D I f i R Z Q = = < / l a t e x i t >
Y m < l a t e x i t s h a 1 _ b a s e 6 4 = " a H T w e l 4 y S h E m O z j 3 9 M 1 Z J T F l T 5 Y = " > A A A B 6 3 i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b F M w D w k L m F 2 M p s M m Z l d Z m a F s O Q X b C w U s b X 0 L / w C O x u / x d k k h S Y M l W S 2 K E w 4 M h H K H k d 9 p i g x f G w J J o r Z W x E Z Y o W J s f F k I X i L L y + T V q X s n Z c r D Z t G D W b I w z G c w B l 4 c A F V u I Y 6 N I H A E B 7 g C Z 4 d 4 T w 6 L 8 7 r r D X n z G c O 4 Q + c t x 9 M k Z F y < / l a t e x i t >
Y a < l a t e x i t s h a 1 _ b a s e 6 4 = " V H X i 6 T X k O U Z j q Z L 7 g H K L V k 4 d y 1 A = " > A A A B 6 n i c b V C 7 S g N B F L 0 b X z G + o o K N z W A Q r M J u L L Q M s b F M 0 D w k W c L s Z D Y Z M j O 7 z M w K Y c k n 2 F g o Y m v r X / g F d j Z + i 5 N H o Y k H L h z O u Z F C v D C K f j X C f R N M Z k i P u 0 b a n E g m o / n Z 4 6 R q d W 6 a E w U r a k Q V P 1 9 0 S K h d Y j E d h O g c 1 A L 3 o T 8 T + v n Z j w 0 k + Z j B N D J Z k t C h O O T I Q m f 6 M e U 5 Q Y P r I E E 8 X s r Y g M s M L E 2 H R y N g R v 8 e V l 0 i g V v f N i q W b T q M A M W T i G E z g D D y 6 g D N d Q h T o Q 6 M M D P M G z w 5 1 H 5 8 V 5 n b V m n P n M I f y B 8 / Y D B S y R U g = = < / l a t e x i t >
Y v < l a t e x i t s h a 1 _ b a s e 6 4 = " b z T Z x d a t E N u 4 b X n W o p 0 7 y t Y U G f Q = " > A A A B 6 n i c b V C 7 S g N B F L 3 j M 8 Z X V L C x G Q y C V d i N h Z Y h N p Y J m o c k S 5 i d z C Z D Z m e X m d l A W P I J N h a K 2 N r 6 F 3 6 B n Y 3 f 4 u R R a O K B C 4 d z 7 u X e e / x Y c G 0 c 5 w u t r K 6 t b 2 x m t r L b O 7 t 7 + 7 m D w 7 q O E k V Z j U Y i U k 2 f a C a 4 Z D X D j W D N W D E S + o I 1 / M H 1 x G 8 M m d I 8 k n d m F D M v J D 3 J A 0 6 J s d L t f W f Y y e W d g j M F X i b u n O R L x 9 V v / l 7 + q H R y n + 1 u R J O Q S U M F 0 b r l O r H x U q I M p 4 K N s + 1 E s 5 j Q A e m x l q W S h E x 7 6 f T U M T 6 z S h Z O 4 B T O w Y V L K M E N V K A G F H r w A E / w j A R 6 R C / o d d a 6 g u Y z R / A H 6 O 0 H J Q C R Z w = = < / l a t e x i t >
Text Input Audio Input Video Input Vision : Frames extracted from the video segments at 30 Hz .
We use the MTCNN face detection algorithm ( Zhang et al. , 2016a ) to extract aligned faces .
Then , following Zadeh et al . ( 2018 b ) , we use MultiComp OpenFace 2.0 toolkit ( Baltrusaitis et al. , 2018 ) to extract the set of 68 facial landmarks , 17 facial action units , head pose , head orientation , and eye gaze .
Lastly , 709 - dimensional frame- level visual features are extracted in total .
Multimodal Multi-task Learning Framework
In this section , we describe our proposed multimodal multi-task learning framework .
Shown as Figure 3 , based on late-fusion multimodal learning framework , we add independent output units for three unimodal representations : text , audio , and vision .
Therefore , these unimodal representations not only participate in feature fusion but are used to generate their predictive outputs .
For the convenience in following introduction , in text , audio and vision , we assume that L u , D u i , D u r , where u ?
{t , a , v} , represent the sequence length , initial feature dimension extracted by section 3.3 and representation dimension learned by unimodal feature extractor , respectively .
The batch size is B .
Unimodal SubNets Unimodal subNets aim to learn intra-modal representations from initial feature sequences .
A universal feature extractor can be formalized as : R u = S u ( I u ) ( 2 ) where I u ?
R B?L u ?D u i , R u ? R B?D u r . S u ( ? ) is the feature extractor network for modal u .
In this work , following ; Liu et al . ( 2018 ) , we use a Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) network , a deep neural network with three hidden layers of weights W a and a deep neural network with three hidden layers of weights W v to extract textual , acoustic and visual embeddings , respectively .
Feature Fusion Network Feature fusion network aims to learn inter-modal representation with three unimodal representations , formulated as : R m = F ( R t , R a , R v ) ( 3 ) where R t , R a , R v ?
R B?D u r are the unimodal representations .
F ( ? ) is the feature fusion network and R m is the fusion representation .
In this work , for full comparison with existing works , we try three fusion methods : LF - DNN , TFN and LMF ( Liu et al. , 2018 ) .
Optimization Objectives Except for the training losses in different tasks , we sparse the sharing parameters via L2 norm , which aims to select intra-modal features .
Therefore , our optimization objectives is : min 1 N t Nt n=1 i ?
i L(y n i , ?n i ) + j ? j || W j || 2 2 ( 4 ) where N t is the number of training samples , i ?
{ m , t , a , v} , j ?
{t , a , v}. L(y n i , ?n i ) means the training loss of n th sample in modality i .
W j is the sharing parameters in modality j and multimodal tasks .
?
i is the hyperparameter to balance different tasks and ?
j represents the step of weight decay of subNet j , respectively .
Lastly , we use a three - layer DNN to generate outputs of different tasks .
In this work , we treat these tasks as regression models and use the L1 loss as training loss in Equation 4 .
Experiments
In this section , we mainly explore the following problems using SIMS : ( 1 ) Multimodal Sentiment Analysis :
We evaluate the performance of multimodal multi-task learning methods comparing with the other methods .
learning with unimodal annotations and set up multimodal baselines for SIMS .
( 2 ) Unimodal Sentiment Analysis :
We analyze the performance in unimodal tasks with unimodal or multimodal annotations only .
The aim is to validate the necessary of multimodal analysis and set unimodal baselines for SIMS .
( 3 ) Representations Differences :
We use t-SNE to visualize the unimodal representations of models with or without independent unimodal annotations .
The aim is to show that the learned unimodal representations are more distinctive after using unimodal annotations .
Baselines
In this section , we briefly review our baselines used in the following experiments .
Early Fusion LSTM .
The Early Fusion LSTM ( EF - LSTM ) ( Williams et al. , 2018 ) concatenates initial inputs of three modalities first and then use LSTM to capture long-distance dependencies in a sequence .
Later Fusion DNN .
In contrast with EF - LSTM , the Later Fusion DNN ( LF - DNN ) learns unimodal features first and then concatenates these features before classification .
Memory Fusion Network .
The Memory Fusion Network ( MFN ) ( Zadeh et al. , 2018a ) word -level alignment in three modalities .
However , this is not easy for SIMS because we have n't found a reliable alignment tool of Chinese corpus .
In this work , we follow Tsai et al . ( 2019 ) to use CTC ( Graves et al. , 2006 ) as an alternative .
Low- rank Multimodal Fusion .
The Low-rank Multimodal Fusion ( LMF ) ( Liu et al. , 2018 ) model learns both modality -specific and cross-modal interactions by performing efficient multimodal fusion with modality -specific factors .
Tensor Fusion Network .
The Tensor Fusion Network ( TFN ) explicitly models view-specific and cross-view dynamics by creating a multi-dimensional tensor that captures unimodal , bimodal and trimodal interactions across three modalities .
Multimodal Transformer .
The Multimodal Transformer ( MULT ) ( Tsai et al. , 2019 ) using the directional pairwise crossmodal attention to realize the interactions between multimodal sequences across distinct time steps and latently adapt streams from one modality to another .
Experimental Details
In this section , we introduce our experimental settings in detail , including dataset splits , hyperparameters selection , and our evaluation metrics .
Dataset Splits .
We shuffle all video clips in random first and then divide train , valid and , test splits by multimodal annotations .
The detailed split results are shown in Table 3 . Hyper-parameters Selection .
Due to the different sequence lengths in different segments , it is necessary that fixing sequence length for the specific modality .
Empirically , we choose the average length plus three times the standard deviation as the maximum length of the sequence .
Besides , for all baselines and our methods , we adjust their hyperparameters using grid search with binary classification accuracy .
For a fair comparison , in each experiment , we select five same random seeds ( 1 , 12 , 123 , 1234 , and 12345 )
Results and Discussion
In this section , we present and discuss the experimental results of the research questions introduced in Section 5 .
Comparison with Baselines .
We compare three new methods with the aforementioned baselines .
In this part , we only consider the multimodal evaluation results though new methods are multi-task .
Results are shown in Table 2 . Compared with single - task models , multi-task models have better performance in most of evaluation metrics .
In particular , all three improved models ( MLF - DNN , MLFM , and MTFN ) have promotion significantly compared to corresponding original models ( LF - DNN , LFM , and TFN ) in all evaluation metrics except for Acc - 5 .
The above results demonstrate that the introduction of independent unimodal annotations in multimodal sentiment analysis can significantly improve the performance of existing methods .
Also , we find that some methods , such as MULT , that perform well on existing public datasets while they are not satisfactory on SIMS .
It further illustrates that designing a robust , cross-lingual multimodal sentiment analysis model is still a challenging task , which is also one of our motivations for proposing this dataset .
Unimodal Sentiment Analysis .
Due to the independent unimodal annotations in SIMS , we conducted two sets of experiments for unimodal sentiment analysis .
In the first set of experiments , we use real unimodal labels to verify the model 's ability of performing unimodal sentiment analysis .
In the second set of experiments , we use multimodal labels instead of unimodal labels to verify the ability of predicting the true emotions of speakers when there is only unimodal information .
Results are shown in Table 4 . Firstly , in the same unimodal task , the results under unimodal labels are better than those under multimodal labels .
But the former cannot reflect the actual sentimental state of speakers .
Secondly , under multimodal annotations , the performance with unimodal information only is lower than using multimodal information in Table 2 .
Hence , it is inadequate to perform sentiment analysis using unimodal information only due to the inherent limitations of unimodal information .
Representations Differences .
Another motivation for us to propose CH -SIMS is that we think the unimodal representation differences will be greater with independent unimodal annotations .
We use t-SNE ( Maaten and Hinton , 2008 )
Ablation Study
In this section , we compare the difference in the effects of combining different unimodal tasks on multimodal sentiment analysis .
We aim to further explore the influence on multimodal sentiment analysis with different unimodal tasks .
Furthermore , we reveal the relationship between multi-task learning and multimodal sentiment analysis .
We conducted multiple combination experiments to analyze the effects of different unimodal subtasks on the main multimodal task .
In this part , we only report the results in MLF - DNN .
Results are shown in Table 5 .
The results show that in the case of partial absence of three unimodal subtasks , the performance of the multimodal task has not significantly improved , or even damaged .
Two factors may cause an adverse effect in multimodal learning , including the consistency between different unimodal representations and the asynchrony of learning in different tasks .
The former means that unified annotations guide the representations to be similar and lack complementarity in different modalities .
The latter means that the learning process in different tasks is inconsistent .
5 : ( % ) Results for multimodal sentiment analysis with different tasks using MLF - DNN .
" M " is the main task and " T , A , V " are auxiliary tasks .
Only the results of task " M " are reported .
" M , A " as an example , the sub-network of subtask " A " is supervised by multimodal loss and unimodal loss .
In contrast , subtask " T " and subtask " V " are supervised by their unimodal loss only .
It means the " A " is learned twice while the " T " and the " V " are learned once only during an training epoch .
Therefore , the introduction of unimodal tasks will reduce the consistency of the representation and strengthen the complementarity , but will also cause the asynchrony .
As more unimodal tasks are introduced , the positive effects of the former gradually increase , and the negative effects of the latter gradually decrease .
Finally , when all unimodal tasks are added , the negative effect of the latter is almost dis-appearing .
Finally , the performance of the model with tasks " M , T , A , V " reaches a peak .
Conclusion
In this paper , we propose a novel Chinese multimodal sentiment analysis dataset with independent unimodal annotations and a multimodal multi-task learning framework based on late-fusion methods .
We hope that the introduction of CH -SIMS will provide a new perspective for researches on multimodal analysis .
Furthermore , we conduct extensive experiments on discussing unimodal , multimodal , and multi-task learning .
Lastly , we summarize our overall findings as follows : ?
Multimodal labels cannot reflect unimodal sentimental states always .
The unified multimodal annotations may mislead the model to learn inherent characteristics of unimodal representations .
?
With the help of unimodal annotations , models can learn more differentiated information and improve the complementarity between modalities .
?
When performing multi-task learning , the asynchrony of learning in different subtasks may cause an adverse effect on multimodal sentiment analysis .
In the future , we will further explore the connection between multimodal analysis and multi-task learning and incorporate more fusion strategy , including early - and middle-fusion .
Figure 1 : 1 Figure 1 : An example of the annotation difference between CH -SIMS and other datasets .
For each multimodal clip , in addition to multimodal annotations , our proposed dataset has independent unimodal annotations .
M : Multimodal , T : Text , A : Audio , V : Vision .
