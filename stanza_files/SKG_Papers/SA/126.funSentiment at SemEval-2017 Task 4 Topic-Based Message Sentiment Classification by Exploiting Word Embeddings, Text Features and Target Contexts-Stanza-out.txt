title
funSentiment at SemEval-2017 Task 4 : Topic-Based Message Sentiment Classification by Exploiting Word Embeddings , Text Features and Target Contexts
abstract
This paper describes the approach we used for SemEval - 2017 Task 4 : Sentiment Analysis in Twitter .
Topic- based ( target - dependent ) sentiment analysis has become attractive and been used in some applications recently , but it is still a challenging research task .
In our approach , we take the left and right context of a target into consideration when generating polarity classification features .
We use two types of word embeddings in our classifiers : the general word embeddings learned from 200 million tweets , and sentiment -specific word embeddings learned from 10 million tweets using distance supervision .
We also incorporate a text feature model in our algorithm .
This model produces features based on text negation , tf.idf weighting scheme , and a Rocchio text classification method .
We participated in four subtasks ( B , C , D & E for English ) , all of which are about topic-based message polarity classification .
Our team is ranked # 6 in subtask B , # 3 by MAE u and # 9 by MAE m in subtask C , # 3 using RAE and # 6 using KLD in subtask D , and # 3 in subtask E.
Introduction
There have been many studies on message or sentence level sentiment classification ( Go et al. , 2009 ; Mohammand et al. , 2013 ; Pang et al. , 2002 ; Liu , 2012 ; , but there are few studies on target - dependent , or topic-based , sentiment prediction ( Jiang et al. , 2011 ; Dong et al. , 2014 ; Vo and Zhang , 2015 ) .
A target entity in a message does not necessarily have the same polarity type as the message , and different entities in the same message may have different polarities .
For example , in the tweet " Linux is better than Windows " , the two named entities , Linux and Windows , will have different sentiment polarities .
In this paper , we describe our approach for the subtask B , C , D & E of SemEval - 17 Task 4 : Sentiment Analysis in Twitter ( Sara Rosenthal and Noura Farra and Preslav Nakov , 2017 ) .
All the four subtasks are on topic-based message sentiment classification .
Task B and C are about topic-based message polarity classification .
Given a message and a topic , in task B , we classify the message on a two -point scale : positive or negative sentiment towards the topic .
And in task C , we classify the message on a five-point scale : sentiment conveyed by the tweet towards the topic on a five-point scale .
Task D and E are about Tweet quantification .
Given a set of tweets about a given topic , in task D , we want to estimate the distribution of the tweets across two -point scale - the positive and negative classes , and in task E , we estimate that on a five-point scale - the five classes of a fivepoint scale .
Our approach uses word embeddings ( WE ) learned from general tweets , sentiment specific word embeddings ( SSWE ) learned from distance supervised tweets , and a weighted text feature model ( WTM ) .
Learning features directly from tweet text has recently gained lot of attention .
One approach is to generate sentence representations from word embeddings .
Several word embedding generation algorithms have been proposed in previous studies ( Collobert et al. , 2011 ; Mikolov et al. , 2013 ) .
Using the general word embeddings directly in sentiment classification is not effective , since they mainly model a word 's semantic context , ignoring the sentiment clues in text .
Therefore , words with opposite polarity , such as worst and best , are mapped onto vectors embeddings that are close to each other in some dimensions .
propose a sentiment -specific word embedding ( SSWE ) method for sentiment analysis , by extending the word embedding algorithm .
SSWE encodes sentiment information in the word embeddings .
In our approach , we incorporate WE , SSWE and a weighted text feature model ( WTM ) together .
The WTM model generates two types of features .
The first type is a negation feature based on the negation words in a tweet .
The second set of features is created by computing the similarity between the tweet and each of the polarity types , using cosine similarity and the tf.idf word weighting scheme .
Each polarity category is represented by a pseudo centroid tweet learned from training data .
This is very similar to the Rocchio text classifier ( Christopher et al. , 2008 ) , but here all the similarity values with all the polarity types are used as features , and fed to the classification algorithm .
The rationale behind the second set of features is that the similarity values with the different polarity types will have some correlations , and using all of them as features will provide more information to the classifier .
For example , a positive tweet usually will have a higher similarity value with neutral type than with the negative type .
This will provide an additional signal to the classifier .
The context of an entity will affect its polarity value , and usually an entity has a left context and also a right one , unless it is at the beginning or end of a message .
Both the context information and the interaction between these two contexts are included in the classification features of our approach .
Our approach uses both SSWE and WE to represent these contexts , since WE and SSWE complement each other , and our experiment shows that using both increases the accuracy by more than 6 % , compared to using only one of them .
In the following sections , we present the related studies , our methodology and the experiments and results for subtask B , C , D and E .
Related Work Message Level Sentiment : Traditional sentiment classification approaches use sentiment lexicons ( Mohammad et al. , 2013 ; Thelwall et al. , 2012 ; Turney , 2002 ) to generate various features .
Pang et al. treat sentiment classification as a special case of text categorization , by applying learning algorithms ( 2002 ) .
Many studies follow Pang 's approach by designing features and applying different learning algorithms on them ( Feldman , 2013 ; Liu , 2012 ) .
Go et al. ( 2009 ) proposed a distance supervision approach to derive features from tweets obtained by positive and negative emotions .
Some studies ( Hu et al. , 2013 ; Liu , 2012 ; Pak and Paroubek 2010 ) follow this approach .
Feature engineering plays an important role in tweet sentiment classification ; Mohammad et al . ( 2013 ) implemented hundreds of hand -crafted features for tweet sentiment classification .
Deep learning has been used in the sentiment analysis tasks , mainly by applying word embeddings ( Collobert et al. , 2011 ; Mikolov et al. , 2013 ) .
Learning the compositionality of phrase and sentence and then using them in sentiment classification is also explored by some studies ( Hermann and Blunsom , 2013 ; Socher et al. , 2011 ; Socher et al. , 2013 ) .
Using the general word embeddings directly in sentiment classification may not be effective , since they mainly model a word 's semantic context , ignoring the sentiment clues in text .
propose a sentiment -specific word embedding method by extending the word embedding algorithm from ( Collobert et al. , 2011 ) and incorporating sentiment data in the learning of word embeddings .
Target-dependent Sentiment : Jiang et al. ( 2011 ) use both entity dependent and independent features generated based on a set of rules to assign polarity to entities .
By using POS features and the CRF algorithm , Mitchell et al . ( 2013 ) identify polarities for people and organizations in tweets .
Dong et al. ( 2014 ) apply adaptive recursive neural network on the entity level sentiment classification .
These two approaches use syntax parsers to parse the tweet to generate related features .
In our approach , we consider both the left and right contexts of a target when generating features .
Methodology
In this section , we describe the three main components used in our method , the WE , SSWE and WTM models , and how they are integrated together .
Word Embedding Word embedding is a dense , low-dimensional and real-valued vector for a word .
The embeddings of a word capture both the syntactic structure and semantics of the word .
Traditional bag-of-words and bag-of -n- grams hardly capture the semantics of words .
Word embeddings have been used in many NLP tasks .
The C&W model ( Collobert et al. , 2011 ) and the word2vec model ( Mikolov et al. , 2013 ) , which is used in this study to generate the WE embeddings , are the two popular word embedding models .
The embeddings are learned to optimize an objective function defined on the original text , such as likelihood for word occurrences .
One implementation is the word2vec from Mikolov et al . ( 2013 ) .
This model has two training options , continuous bag of words and the Skip-gram model .
The Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships .
This model is used in our method and here we briefly introduce it .
The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document .
Given a sequence of training words W 1 , W 2 , W 3 , . ., W N , the Skip-gram model aims to maximize the average log probability ? ? ? ? ? ? ? ? 0 , 1 ) | ( log 1 i m i m n i n N n W W p N where m is the size of the training context .
A larger m will result in more semantic information and can lead to a higher accuracy , at the expense of the training time .
Generating word embeddings from text corpus is an unsupervised process .
To get high quality embedding vectors , a large amount of training data is necessary .
After training , each word , including all hashtags in the case of tweet text , is represented by a lowdimensional , dense and real-valued vector .
Sentiment -Specific Word Embedding
The C&W model ( Collobert et al. , 2011 ) learns word embeddings based on the syntactic contexts of words .
It replaces the center word with a random word and derives a corrupted n-gram .
The training objective is that the original n-gram is expected to obtain a higher language model score than the corrupted n-gram .
The original and corrupted n-grams are treated as inputs of a feedforward neural network , respectively .
SSWE extends the C&W model by incorporating the sentiment information into the neural network to learn the embeddings ; it captures the sentiment information of sentences as well as the syntactic contexts of words .
Given an original ( or corrupted ) n-gram and the sentiment polarity of a tweet as input , it predicts a two -dimensional vector ( f 0 , f 1 ) , for each input n-gram , where ( f 0 , f 1 ) are the language model score and sentiment score of the input n-gram , respectively .
The training objectives are twofold : the original n-gram should get a higher language model score than the corrupted n-gram , and the polarity score of the original n-gram should be more aligned to the polarity label of the tweet than the corrupted one .
The loss function is the linear combination of two losses - loss 0 ( t , t ' ) is the syntactic loss and loss 1 ( t , t ' ) is the sentiment loss : loss ( t , t ' ) = ? * loss 0 ( t , t ' ) + ( 1 - ? ) * loss 1 ( t , t ' )
The SSWE model used in this study was trained from massive distant - supervised tweets , collected using positive and negative emotions .
ry is treated as a document , and tf is normalized by its category size .
?
A pseudo centroid tweet is generated for each sentiment type .
We define a centroid as a vector containing the tf.idf value for each term in this category .
Although we call it a " tweet " , its length is much longer than a regular tweet .
Similarity value :
For each training or test tweet , its similarity with a sentiment type is calculated as follows :
Weighted Text ?
The tweet text is pre-processed using the same steps mentioned above ?
A tf.idf value is calculated for each remaining term ?
A cosine similarity is calculated between this tweet and each sentiment type .
Feature Generation
Features
Given a tweet and the target entity , eight types of features are generated based on WE , SSWE , and WTM models .
They are integrated together to train the classifier .
Figure 1 shows the eight types of features .
Six types of features are generated from WE and SSWE embeddings for a target entity .
Two types of features are generated from the WTM model .
The red ones are SSWE embeddings , and the blue ones are WE embeddings .
The subscript letter L and R refer to the left and right side of an entity , respectively .
They are described below : WE L and WE R : These are the WE embeddings for the text on the left side and right side of the target entity , respectively .
In the four subtasks , occasionally , the given topic ( target ) is a paraphrase of the actual target entity in the tweet text , and it is not easy to match these two .
In this case , the whole tweet text is used for both the left and right contexts , and this case is handled in the same way when generating SSWE L and SSWE R described below .
SSWE L and SSWE R : These are the SSWE embeddings for the text on the left side and right side of the target entity , respectively .
WE and SSWE : these are the embeddings generated from the whole message text , which means they are entity independent features .
We use these two features to capture the whole message , which reflects the interaction between the left and right sides of the entity .
WTM features :
It has two types of features : the negation feature and the features corresponding to the cosine similarity values between the tweet and the pseudo centroid tweet of each of the polarity types .
We have described how to generate them in the previous section .
These eight types of features together capture different types of information we are interested : the entity 's left and right contexts , the interaction of the two sides , the sentiment specific word embedding information , the general word embedding information , and the sentiment affected by negation terms .
Text Representation from Term Embeddings
There are different ways to obtain the representation of a text segment , such as a whole tweet or the left / right context of an entity , from word embeddings .
In our approach , we use the concatenation convolution layer , which concatenates the layers of max , min and average of word embeddings , because this layer gives the best performance based in our pilot experiments .
Subtask
Experiments and Results
WE Model Construction
The tweets for building the WE model include tweets obtained through Twitter 's public streaming API and the Decahose data ( 10 % of Twitter 's streaming data ) obtained from Twitter .
Only English tweets are included in this study .
Totally there are about 200 million tweets .
Each tweet text is preprocessed to get a clean version , following similar steps described in the WTM model subsection , except the stop removal step .
Stop words are not removed , since they provide important information on how other words are used .
Totally , about 2.9 billion words were used to train the WE model .
Based on our pilot experiments , we set the embedding dimension size , word frequency threshold and window size as 300 , 5 and 8 , respectively .
There are about 1.9 million unique words in this model .
SSWE Model Construction
The SSWE model for Twitter was trained from massive distant - supervised tweets , collected using positive and negative emoticons , such as :) , = ) , :( and :-(.
A total of 10 million tweets were collected , where 5 million contain positive emotions and the other 5 million contain negative ones .
The embedding dimension size was set as 50 and the window size as 3 .
Data Set and Results for Task 4
For subtask B and D , the sentiment classification and quantification based on a 2 - point scale , the training data is from the related tasks of SemEval - 2015 and SEmEval - 2016 .
There are 20,538 tweets , but the actual tweet texts are not provided , due to privacy concerns .
So we crawled these tweets from Twitter 's REST API .
However , we were unable to obtain all these tweets because some of them were already deleted or not available due to authorization status change .
To build our classifier , we split this data set into three parts : 70 % as training data , 20 % as development data and 10 % for testing our classifier .
For these two subtasks , we applied several classification algorithms , such as SMO , LibLinear and logistic regression , to see which one performs the best .
The result we reported is based on logistic regression , which performed the best .
For subtask C and E , the sentiment classification and quantification based on a 5 - point scale , the training data is from the related tasks of SemEval - 2016 .
There are 30,632 tweets , and similarly to subtask B and D , we downloaded the tweets from Twitter 's API and split them into three parts .
The result we reported is based on SMO ( Keerthi et al. , 2001 ) , which performed the best among several classifiers we tested .
SMO is a sequential minimal optimization algorithm for training a support vector classifier .
Table 1 shows the results of our approach for the four subtasks .
It lists the scores and ranks of our team for all the performance metrics used for each subtask .
The subscript of each score is the rank of our team using that metric for that subtask .
The meanings of the measures used in Table 1 are explained below :
For task B : ? is the macro- averaged recall , which is macro- averaged over the positive and the negative class .
Accuracy and F1 measures are also used for subtask B .
As subtask B is topicbased , each metric is computed individually for each topic , and then the results are averaged across the topics to yield the final score .
This is the same for all the measures used in task C , D and E , which are all topic based tasks .
For task C : MAE M is the macro- averaged mean absolute error , which is an ordinal classification measure .
Note that MAE M is a measure of error , not accuracy , and thus lower values are better .
MAE ? is an extension of macro-averaged recall for ordinal regression .
More details about these two measures are described in ( Baccianella et al. , 2009 ; Nakov et al. , 2016 a ) .
For task D : KLD is the Kullback - Leibler Divergence measure , a measure of error , which means that lower values are better .
AE is the absolute error and RAE is the relative absolute error .
For task E : Subtask E is an ordinal quantification task .
As in binary quantification , the goal is to compute the distribution across classes , assuming a quantification setup .
EMD is Earth Mover 's Distance ( Rubner et al. , 2000 ) , which is currently the only known measure for ordinal quantification .
Like KLD and MAE , EMD is a measure of error , so lower values are better .
Conclusion
This paper describes the approach we used for subtask B , C , D and E of SemEval - 2017 Task 4 : Sentiment Analysis in Twitter .
We use two types of word embeddings in our classifiers : general word embeddings learned from 200 million tweets , and sentiment -specific word embeddings learned from 10 million tweets using distance supervision .
We also incorporate a weighted text feature model in our algorithm .
Our team is ranked # 6 in subtask B , # 3 using MAE u metric and # 9 using MAE m metric in subtask C , # 3 using RAE and # 6 using KLD in subtask D , and # 3 in subtask E. Figure 1 . 1 Figure 1 .
The features generated from different models .
