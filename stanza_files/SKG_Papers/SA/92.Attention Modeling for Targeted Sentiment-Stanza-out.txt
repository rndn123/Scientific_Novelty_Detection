title
Attention Modeling for Targeted Sentiment
abstract
Neural network models have been used for target - dependent sentiment analysis .
Previous work focus on learning a target specific representation for a given input sentence which is used for classification .
However , they do not explicitly model the contribution of each word in a sentence with respect to targeted sentiment polarities .
We investigate an attention model to this end .
In particular , a vanilla LSTM model is used to induce an attention value of the whole sentence .
The model is further extended to differentiate left and right contexts given a certain target following previous work .
Results show that by using attention to model the contribution of each word with respect to the target , our model gives significantly improved results over two standard benchmarks .
We report the best accuracy for this task .
Introduction Targeted sentiment analysis investigates the classification of opinions polarities towards specific target entity mentions in given sentences ( Jiang et al. , 2011 ; Dong et al. , 2014 ; Tang et al. , 2016 ; .
The input is a sentence with given target entity mentions , and the output consists of two -way or three - way sentimental classes on each target mention .
For example , the sentence " She began to love miley ray cyrus since 2013 :) " is marked with a positive sentiment label on the target " miley ray cyrus " .
One important problem of targeted sentiment classification is how to model the relation between targets and their context .
Earlier methods defined rich features by exploiting POS tags and syntactic structures ( Jiang et al. , 2011 ; Dong et al. , 2014 ) .
Compared with discrete manual features , embedding features are less sparse , and can be learnt from large raw texts , capturing distributional syntactic and semantic information .
Dong et al. ( 2014 ) use a target-specific recurrent neural network to represent a sentence .
use the rich pooling functions to extract the feature vector for a given target .
One important contribution of is that they split a sentence into three sections including the target , its left contexts and its right contexts , as shown in Figure 1 . represent words in the input using a bidirectional gated recurrent neural network , and then use three - way gated neural network structure to model the interaction between the target and its left and right contexts .
Tang et al. ( 2016 ) learn target-specific sentence representation by combining word embeddings with the corresponding targeted embeddings , and then using two recurrent neural networks to encode the left context and the right context , respectively .
The above methods use the different neural network structures to model the relation between contexts and targets , but they did not explicitly model the importance of each word in contributing to the sentiment polarity of the target .
For example , the sentence " # nowplaying [ lady gaga ] 0 - let love down " is neural for the target " lady gaga " , where the contribution of " love " is little , despite that the word " love " is a positive word .
To address this , we utilize the attention mechanism to calculate the contribution of each word towards targeted sentiment classes , as shown in Figure 1 , where the gray level in the spectrum means the contribution of words .
In particular , we build a vanilla model using a bidirectional LSTM to extract word embeddings over the sentence and then apply attention over the hidden nodes to estimate the importance of each word .
Furthermore , following , Tang et al. ( 2016 ) and , we differentiate the left and right contexts given a target .
Our final models give significantly improved results on two standard benchmarks compared to previous methods , resulting in best reported accuracy so far .
Our source code is released at https://github.com/LeonCrashCode/ AttentionTargetSentiment .
Related Work Traditional sentiment classification methods rely on manual discrete features ( Pang et al. , 2002 ; Go et al. , 2009 ; Mohammad et al. , 2013 ) .
Recently , distributed word representation ( Socher et al. , 2013 ; and neural network methods ( Irsoy and Cardie , 2013 ; dos Santos and Gatti , 2014 ; Dong et al. , 2014 ; Teng et al. , 2016 ; Ren et al. , 2016 ) have shown promising results on this task .
The success of such work suggests that using word embeddings and deep neural network structures can automatically exploit the syntactic and semantic structures .
Our work is in line with these methods .
The seminal work using the attention mechanism is neural machine translation ( Bahdanau et al. , 2015 ) , where different weights are assigned to source words to implicitly learn alignments for translation .
Subsequently , the attention mechanism has been applied into various other natural language processing tasks including parsing ( Vinyals et al. , 2015 ; Kuncoro et al. , 2016 ; Liu and Zhang , 2017 ) , document classification ( Yang et al. , 2016 ) , question answering ( He and Golub , 2016 ) and text understanding ( Kadlec et al. , 2016 ) .
For sentiment analysis , the attention mechanism has been applied to cross-lingual sentiment ( Zhou et al. , 2016 ) , aspect-level sentiment ( Wang et al. , 2016 ) and user-oriented sentiment ( Chen et al. , 2016 ) .
To our knowledge , we are the first to use the attention mechanism to model sentences with respect to targeted sentiments .
Models
We use a bidirectional LSTM to represent the input word sequence w 0 , w 1 , ... , w n as hidden nodes h 0 , h 1 , ... , h n : [ h 0 ; ... ; h n ] = BILSTM ( [ w 0 ; ... ; w n ] ) , where the target is denoted as h t , which is the average of word embeddings in the target phrase [ h t 0 ; ... ; h tm ] .
We propose three variants of attention to model the relation between context words and targets .
Vanilla Model
We build a vanilla attention model by calculating a weighted value ? over each word in sentences .
The final representation of the sentence s is then given by 1 : s = attention ( [ h 0 ; ... ; h n ] , h t ) = n i ?
i h i , where ? i = exp ( ?
i ) n j exp (? j ) and the weight scores ?
are calculated by using the target representation and the context word representation , ? i = U T tanh ( W 1 ? [ h i ; h t ] + b 1 ) .
The sentence representation s is then used to predict the probability distribution p of sentiment labels on the target by : p = sof tmax ( W 2 s + b 2 ) .
We refer to this vanilla model as BILSTM - ATT .
Contextualized Attention
We make two extensions to the vanilla attention method .
The first is a contextualized attention model ( BILSTM - ATT - C ) , where the sentence is divided into two segments with respect to the target , namely left context and right context Tang et al. , 2016 ; . Attention is applied on left and right contexts , respectively .
In particular , the representation of the left context is : s l = attention ( [ h 0 ; ... ; h t 0 ?1 ] , h t ) , and the representation of the right context is : s r = attention ( [ h tm +1 ; ... ; h n ] , h t ) .
Together with the vanilla representation s , the distribution of sentiment labels is predicted by : p = sof tmax ( W 1 s + W l s l + W r s r + b 1 ) .
Contextualized Attention with Gates A second extension is to add gates to control the flow of context information ( BILSTM - ATT -G ) .
This is motivated by the fact that sentiment signals can be dominated by the left context , the right context or the entire sentence .
The three gates , z , z l and z r , controlled by the target and the corresponding context , are used .
z ? exp ( W 1 s + U 1 h t + b 1 ) , z l ? exp ( W 2 s l + U 2 h t + b 2 ) , z r ? exp ( W 3 s r + U 3 h t + b 3 ) , where z + z l + z r = 1 .
The linear interpolation among s , s l and s r is formulated as s = z s + z l s l + z r s r .
Then the probability distribution of sentiment labels is predicted by : p = sof tmax ( W 4 s + b 4 ) .
Training our models are trained to minimize a cross-entropy loss object with a l 2 regularization term , defined by L ( ? ) = ?
i log p t i + ? 2 ||?|| 2 , where ? is the set of parameters , p t is the probability of the ith training example given by the model and ? is a regularization hyper-parameter , ? = 10 ?6 .
We use momentum stochastic gradient descent ( Sutskever et al. , 2013 ) with a learning rate of ? = 0.01 for optimization .
1 shows the corpus statistics .
Both dataset are three - way classification data .
Parameters & Metrics
The hyper-parameters are given in Table 2 4 .
We use GloVe vectors ( Pennington et al. , 2014 ) with 200 dimensions as pre-trained word embeddings , which are tuned during training .
Two metrics are used to evaluate model performance : the classification accuracy and macro F1 - measure over the three sentiment classes .
Development Experiments
We run three variants of targeted sentiment classification models on the development section of Z-Dataset to investigate the effectiveness of attention mechanism .
A simple BILSTM without attention is deployed as our baseline .
Table 3 shows the development results .
We find that BILSTM -C gives a 0.6 % accuracy improvement by differentiating the left and right contexts .
However , surprisingly , BILSTM -G does not give much improvement despite using gates to control the contexts .
This is different from the observation of , who find that gate mechanism improves accuracy without using attention .
Finally , compared to baseline models without attention , our models give an average 1.2 % accuracy improvement and a 1.8 % macro F1 improvement .
Our final model ( BILSTM - ATT -G ) gives a 2.3 % accuracy significant improvement ( p < 0.01 using ttest ) and a 3.0 % macro F1 improvement over the strongest baseline .
Final Results
We compare our models with previous work .
The final results are shown in
Analysis
We compare the performances of various models against OOV rates .
In particular , we split the test sentences into two sets , where one contains sentences that have no OOV and the other consist of sentences which have at least one OOV .
The results are shown in Figure 2 .
The BILSTM -ATT -G performs the best , especially on OOV sentences , which shows the robustness of the BILSTM - ATT - G .
We compare the performances of various models on each distinct polarity .
The results are shown in Figure 5 .
Interestingly , compared to BILSTM - ATT without contextualized attention , BILSTM -ATT - C loses accuracies on positive ( - 1.1 % ) .
However , BILSTM -ATT -G gives large improvements on positive ( + 4.2 % ) and neutral ( + 1.2 % ) targets but loses accuracy on negative ( - 2.4 % ) .
Overall , both BILSTM -ATT -C and BILSTM -ATT -G outperform BILSTM - ATT on neural cases , which account for 50 % of all targets .
The words " most " , " famous " , " history " , " XD " lead to a positive label , while the word " damn " leads to a negative label .
In Figure 3 ( c ) , although " haha " could be a positive word , here the sentimental class of the target is neutral .
This can be explained by the fact that the word " haha " shows the happiness of the speaker instead of the target " Nicolas Cage " .
Examples
Conclusion
Prior work on targeted sentiment analysis investigates sentence representation that are targetspecific but do not explicitly model the contribution of each word towards targeted sentiment .
We investigated various attentional neural networks for targeted sentiment classification .
Experiments demonstrated that attention over words is highly useful for targeted sentiment analysis .
Our model gives the best reported results on two different benchmarks .
Figure 1 : 1 Figure 1 : Structures of modeling target , left and right contexts and the attention over words .
