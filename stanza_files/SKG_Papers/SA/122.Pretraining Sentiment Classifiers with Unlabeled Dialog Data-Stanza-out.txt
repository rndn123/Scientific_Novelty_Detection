title
Pretraining Sentiment Classifiers with Unlabeled Dialog Data
abstract
The huge cost of creating labeled training data is a common problem for supervised learning tasks such as sentiment classification .
Recent studies showed that pretraining with unlabeled data via a language model can improve the performance of classification models .
In this paper , we take the concept a step further by using a conditional language model , instead of a language model .
Specifically , we address a sentiment classification task for a tweet analysis service as a case study and propose a pretraining strategy with unlabeled dialog data ( tweet- reply pairs ) via an encoder-decoder model .
Experimental results show that our strategy can improve the performance of sentiment classifiers and outperform several state - of - theart strategies including language model pretraining .
Introduction Sentiment classification is a task to predict a sentiment label , such as positive / negative , for a given text and has been applied to many domains such as movie / product reviews , customer surveys , news comments , and social media .
A common problem of this task is the lack of labeled training data due to costly annotation work , especially for social media without explicit sentiment feedback such as review scores .
To overcome this problem , Dai and Le ( 2015 ) recently proposed a semi-supervised sequence learning framework , where a sentiment classifier based on recurrent neural networks ( RNNs ) is trained with labeled data after initializing it with the parameters of an RNN - based language model pretrained with a large amount of unlabeled data .
The concept of their framework is simple but effective , and their work yielded many related studies of semi-supervised training based on sequence modeling , as described in Section 4 .
In this paper , we take their concept a step further by using a conditional language model with unlabeled dialog data ( i.e. , tweet - reply pairs ) instead of a language model with unpaired data 1 .
An important observation of the dialog data that underpins our strategy is that the sentiment or mood in a message often affects messages in reply to it .
People tend to write angry responses to angry messages , empathetic replies to sad remarks , or congratulatory phrases to good news .
Our contributions are listed as follows . ?
We propose a pretraining strategy with unlabeled dialog data ( tweet- reply pairs ) via an encoder-decoder model for sentiment classifiers ( Section 2 ) .
To the best of our knowledge , our proposal is the first such proposal , as clarified in Section 4 . ?
We report on a case study based on a costly labeled sentiment dataset of 99.5 K items and a large-scale unlabeled dialog dataset of 22.3 M , which were provided from a tweet analysis service ( Section 3.1 ) .
?
Experimental results of sentiment classification show that our method outperforms the current semi-supervised methods based on a language model , autoencoder , and distant supervision , as well as linear classifiers ( Section 3.4 ) .
Proposed Method
Our pretraining strategy simply consists of the following two steps : 1 . Training a dialog ( encoder-decoder ) model using unlabeled dialog data ( tweet - reply pairs ) as pretraining .
2 . Training a sentiment classifier ( encoderlabeler ) model using labeled sentiment data ( tweet - label pairs ) after initializing its encoder part with the encoder parameters of the encoder-decoder model .
The encoder- decoder model is a conditional language model that predicts a correct output sequence from an input sequence ( Sutskever et al. , 2014 ) .
This model consists of two RNNs : an encoder and decoder .
The encoder extracts a context of the input sequence as a real-valued vector , and the decoder predicts the output sequences from the context individually .
Our classifier forms an encoder-labeler structure , which consists of the above encoder and a labeler that predicts a sentiment label from the context .
Note that the encoder of the classifier is finetuned with labeled data , as in ( Dai and Le , 2015 ) .
The main difference between their approach and ours is that we examine paired ( dialog ) data for pretraining , while they only showed the usefulness of pretraining with unpaired data .
Experiments
Datasets
We used two datasets , a dialog dataset for pretraining the encoder-decoder model and a sentiment dataset for training ( fine-tuning ) the sentiment classifier , as shown in Table 1 .
Those datasets were provided by Yahoo !
JAPAN , which is the largest portal site in Japan .
The dialog dataset contains about 22.3 million tweet - reply pairs extracted from Twitter Firehose data .
In its preprocessing , we filtered out spam and bot posts by using user- level signals such as the follower count , the friend count , the favorite count , and whether a profile image is set or not .
Also , we replaced all the URLs in the text with " [ u ] " and all the user mentions with " [ m ] " , considering them as noise .
The sentiment dataset includes about 100K tweets with manually annotated three - class sentiment labels : positive , negative , and neutral .
The breakdown of positive , negative , and neutral in the training set was 15.0 , 18.6 , and 66.4 % , respectively .
Note that the tweets were sampled separately from those of the dialog dataset .
The procedure for text preprocessing was the same with that of the dialog dataset .
The average length of the tweets after preprocessing was 17 characters .
Each tweet was judged by a majority vote of three experienced editors in the company providing the sentiment -analysis service .
The inter-annotator agreement ratio assessed with Fleiss ' ? was 0.495 .
The overall annotation work took roughly 300 person-days .
This means that the cost is at least 24 K dollars , 8 hours ?
300 days ? legal minimum wage in Japan 10 dollars / hour .
Considering that the in-house annotators are well - educated , skilled proper employees , the actual cost would be much higher than this rough estimate and much more costly than collecting unlabeled dialog data .
In addition , the annotators had gone through a few days of training to become able to appropriately judge the sentiment before they got down to actual annotation work , but the number , 300 person-days , does not include the time for this training .
Model and Training
The settings of the dialog ( encoder-decoder ) model are as follows .
In both the encoder and decoder , the size of the word-embedding layer is 256 and that of the LSTM - RNN hidden layer is 1024 .
The size of the output layer is 4000 , which is the same as the ( character - based ) vocabulary size .
3 . The encoder and decoder share these hyperparameters as well as the parameters themselves ( that is , with regard to the embedding layer and recurrent layer ) .
The total number of parameters is 8.9 million .
The settings of the sentiment classifier ( encoder-labeler ) model are as follows .
The encoder part has the same structure and hyperparameters as that of the dialog model , making them compatible for transferring learned parameters .
We reused the dialog model 's dictionaries in the classifier model so that the two models could process tweet texts consistently .
The labeler consists of a fully connected layer and soft max nonlinearity .
The models were trained with ADADELTA ( Zeiler , 2012 ) with a mini-batch size of 64 .
The dialog model was trained in five epochs , and the classifier model was tuned with the early - stopping strategy , which stops training when the validation accuracy drops .
For ADADELTA 's parameters , we fixed the learning rate to 1.0 , decay rate ? to 0.95 , and smoothing constant ? to 10 ?6 for all training sessions .
We evaluated validation costs ten times per epoch and selected the model with the lowest validation cost .
The training took 15.9 days on 1 GPU with 7 TFLOPS computational power .
Compared Models
We compared the following eight models : nonpretrained ( Default ) , proposed dialog pretraining ( Dial ) , current pretraining with unpaired data ( Lang , SeqAE ) and pseudo labeled data ( Emo2 M , Emo6 M ) , and classical linear learners ( LogReg , LinSVM ) .
The details of these models are given below .
?
Default : Trained without pretraining by executing only Step 2 in Section 2 . ? Dial : Pretrained with the dialog model described in Section 2 . ? Lang , SeqAE : Pretrained with the language model and autoencoder model proposed in ( Dai and Le , 2015 ) .
The language model is the decoder part of the encoder-decoder model using a zero vector as the initial hidden layer value , and the autoencoder model is the same structure of the encoder- decoder model , where input and output are the same .
To make the comparison as fair as possible , we used the reply -side of the dialog dataset for pretraining Lang and SeqAE so that the same supervision information on the basis of the same tweet - reply pairs would be applied to Lang , SeqAE , and Dial .
The number of their pretraining epochs was also equal to that of Dial .
? Emo2 M , Emo6M : Pretrained with pseudo labeled data ( 2 M , 6 M ) based on manually collected emoticons , which consist of 120 positive emoticons and 116 negative ones .
This technique is also known as distant -supervision .
These pseudo labels were annotated by extracting tweets including one of those emoticons from our dialog data and another 92 M tweets .
Pretraining was conducted via a two -class sentiment classifier , which is a similar model to Default , since uncertain tweets without emoticons are not always neutral .
We confirmed that this two -class classifier can reach more than 90 % test accuracy on the emoticonbased test dataset .
After pretraining , the parameters of the encoder part were transfered to the final classifier model .
?
LogReg , LinSVM : Logistic regression and linear support vector machine ( SVM ) models of LIBLINEAR ( Fan et al. , 2008 ) with bag-ofwords features , which consist of 50 K unigrams ( w/ o stopwords ) , 50 K bigrams , and 233 emoticons .
These features are based on a state- ofthe - art system ( Mohammad et al. , 2013 ) that performed best in the SEMEVAL competition ( Nakov et al. , 2013 ) and was actually used in the tweet analysis service of the data- providing company .
The best parameters were found through a grid-search on the validation set .
Results
Table 2 shows the macro-average F-measure results of the compared models in Section 3.3 on the sentiment classification task when varying data size ( 5 K to 80K ) .
Each value is the average of five trials with different random seeds for each setting , and a value of a trial is the macro-average of F-measure values of three sentiment classes .
Comparing Dial with the other models , we can see that our pretraining strategy with dialog data consistently outperformed all the other models : state - of - the - art pretraining strategies with unpaired unlabeled data ( Lang , SeqAE ) and pseudo labeled data ( Emo2 M , Emo6 M ) , as well as linear learners ( LogReg , LinSVM ) .
This indicates that unlabeled dialog data ( tweet - reply pairs ) have useful information for sentiment classifiers , as expected in Section 1 .
In fact , we observed that the pretrained encoder-decoder model seems to generate an appropriate reply , on which the sentiment on the input tweet is well reflected .
For example , the reply " : ( " was generated for the input tweet " I 'm sorry to hear that " ( see supplementary material for more examples ) .
Lang also outperformed well but did not overtake Dial .
The differences between Dial and Lang are statistically significant 4 for all five training dataset sizes .
Interestingly , SeqAE was not so effective like Dial , despite their model structures are basically the same .
This implies that it is practically important to find appropriate data for pretraining , such as dialog data for sentiment classification .
As for the results of distant supervision with emoticons , both Emo2 M and Emo6 M performed worse than Default , and increasing the dataset size did not change the situation .
The reason why these models did not perform as well as other pretraining - based models is considered to be noisy labels , especially in negative ones .
We illustrate two instances in the Emo2 M training data that include an emoticon that is usually negative emoti - 4
Under the significance level of 0.05 with two -tailed t-test assuming unequal variances .
con but can be considered positive : ? ? ( ; ;) , " She is so beautiful , cute ( crying emoticon ) " ? ? orz , " I envy you .
Congratulations ( bow - theknee emoticon ) " Comparing Default with LogReg and LinSVM , we can see that the linear models performed better than the default RNN model without pretraining , when the labeled data size is less than or equal to 20K .
However , looking at the results of Dial , our method improved Default even for these cases ( 5 K to 20K ) , and Dial clearly outperformed the linear models .
This means that pretraining is useful especially on the situation where the labeled data size is limited .
Related Work After Dai and Le ( 2015 ) proposed the framework of semi-supervised sequence learning , there have been several attempts to extend sequence learning models for different tasks to semi-supervised settings .
Cheng et al. ( 2016 ) and Ramachandran et al . ( 2017 ) studied semi-supervised training of machine translation models via an autoencoder model and language model , respectively .
They also used paired data ( parallel corpora ) , but unsupervised training was conducted with reasonable monolingual corpora to compensate for costly parallel corpora , which is opposite to our setting .
Zhou et al . ( 2016 a , b) proposed to use parallel corpora for adapting the sentiment resources in a resource - rich language to a resource -poor language .
Their purpose was completely different from ours , since making parallel corpora is also costly .
The other studies include semi-supervised extensions for predicting the property values of Wikipedia ( Hewlett et al. , 2017 ) , detecting medical conditions from heart rate data ( Ballinger et al. , 2018 ) , and morphological reinflection of inflected words ( e.g. , " playing " to " played " ) .
They did not use paired - text data to leverage their tasks .
Our method can be regarded as a general version of distant supervision since we assume that a reply includes the label information of the corresponding tweet .
There have been many studies about distant supervision for sentiment analysis ( Read , 2005 ; Go et al. , 2009 ; Davidov et al. , 2010 ; Purver and Battersby , 2012 ; Mohammad et al. , 2013 ; Tang et al. , 2014 ; dos Santos and Gatti , 2014 ; Severyn and Moschitti , 2015 ; Deriu et al. , 2016 ; M?ller et al. , 2017 ) , but they basically focused on how to use emoticons and hashtags to leverage performance .
One exception is the study by ( Pool and Nissim , 2016 ) , in which Facebook reactions were used for distant supervision .
Their approach is similar to ours using tweet - reply pairs , but our method is more general since they only used six reply categories ( i.e. , like , love , haha , wow , sad , and angry ) , not text replies .
There have been a few studies on sentiment classification in dialogue data .
These studies involved sentiment classification based on dialog contexts , which means that they used labeled dialog data , while we used unlabeled dialog data .
For tweet data , several studies used reply -features for sentiment classification of tweets ( Barbosa and Feng , 2010 ; Jiang et al. , 2011 ; Vanzo et al. , 2014 ; Bamman and Smith , 2015 ; Ren et al. , 2016 ; Castellucci et al. , 2016 ) .
However , they used replies as labeled data for sentiment classification , not unlabeled data for pretraining .
Conclusion
We proposed a pretraining strategy with dialog data for sentiment classifiers .
The experimental results showed that our strategy clearly outperformed the existing pretraining with unpaired unlabeled data via language modeling and pseudo labeled data via distant supervision , as well as linear classifiers .
In the future , we will investigate whether or not we can use other paired data for pretraining of classification tasks .
For example , we expect that news article -comment pairs are useful for predicting fake news detection and that question - answer pairs of Q&A sites are useful for recommending questions for answering .
Table 1 : 1 Details of dialog and sentiment datasets as it was .
On average , source and target ( or reply ) tweets after preprocessing were 31.5 and 27.8 characters long , respectively .
While redistribution of tweets is prohibited , we are planning to publicize tweet IDs of this dataset for reproducibility .
2
The rest of the text was used Train Valid Test Dialog 22,300,000 10,000 50,000 Sentiment 80,591 4,000 15,000
