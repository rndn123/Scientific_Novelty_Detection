title
ConvSent at CLPsych 2019 Task A : Using Post-level Sentiment Features for Suicide Risk Prediction on Reddit
abstract
This work aims to infer mental health status from public text for early detection of suicide risk .
It contributes to Shared Task A in the 2019 CLPsych workshop by predicting users ' suicide risk given posts in the Reddit subforum r/SuicideWatch .
We use a convolutional neural network architecture to incorporate LIWC information at the Reddit post level about topics discussed , first-person focus , emotional experience , grammatical choices , and thematic style .
In sorting users into one of four risk categories , our best system 's macro- averaged F1 score was 0.50 on the withheld test set .
The work demonstrates the predictive power of the Linguistic Inquiry and Word Count dictionary , in conjunction with a convolutional network and holistic consideration of each post and user .
WHO . 2017 .
Policy options on mental health : a WHO - Gulbenkian mental health platform collaboration .
Introduction Psychological distress in the form of depression , anxiety , and other mental health issues can have serious consequences for individuals and society ( WHO , 2017 ) .
Unfortunately , stigma surrounding poor mental health may prevent disclosure of suicidal ideation .
For example , Oexle et al . ( 2017 ) found that perceived stigma and the associated secrecy around mental illness were positively linked with feelings of hopelessness and suicidal ideation .
McHugh et al. ( 2019 ) found that the standard practice of clinicians asking people about suicidal thoughts fails in many cases , as 80 % of patients who ultimately died of suicide reported no suicidal thoughts when prompted by their general practitioner . *
* These authors contributed equally
There is a need to supplement traditional methods for evaluating suicidality that minimize the need for direct disclosure from the individual .
Some of those suffering from mental health challenges have adopted social media outlets , such as Reddit 's r/SuicideWatch , as a means to cope ( Park et al. , 2012 ; Robinson et al. , 2016 ) .
Recent research finds promising links between an individual 's mental well - being and the linguistic content they share on social media ( Coppersmith et al. , 2014 ; De Choudhury et al. , 2016 ; Vioul ?s et al. , 2018 ; Shing et al. , 2018 ) .
The Sixth Annual Workshop on Computational Linguistics and Clinical Psychology ( CLPsych 2019 ) includes a shared task on predicting a Reddit user 's degree of suicide risk based on their posts in the r/SuicideWatch forum ( Zirikly et al. , 2019 ) .
The task involves assigning a degree of risk ( no , low , moderate , or severe ) to a user on Reddit based on content they have posted on Reddit .
For this task , researchers were given access to the University of Maryland Reddit Suicidality Dataset ( Shing et al. , 2018 ) , made available with assistance by the American Association of Suicidology .
This dataset consists of ?1000 users annotated with the four-level scale , and a larger set of 20,000 unannotated users .
Prior work
The baseline deep learning model for classifying suicide risk on Reddit , by Shing et al . ( 2018 ) , builds on the convolutional neural network ( CNN ) for language processing as laid out by Kim ( 2014 )
We additionally leverage prior social media work ( Braithwaite et al. , 2016 ; that finds suicidality can be predicted from a particular feature set , the Linguistic Inquiry and Word Count ( LIWC ) dictionary , as distributed by Tausczik and Pennebaker ( 2010 ) .
Methods
All modeling methods were applied to the deidentified Reddit data as part of Shared Task A. Approval from CMU IRB was obtained on March 11 2019 , and we adhered to the ethical review criteria laid out by Zirikly et al . ( 2019 ) .
Modeling with word embeddings Convolutional neural networks form the basic architecture for our models .
Following Shing et al. ( 2018 ) and Kim ( 2014 ) , we concatenate word embeddings for each word in a post , then concatenate these embedding sequences for all posts in order of occurrence .
Our implementation uses pretrained GloVe word embeddings by Pennington et al . ( 2014 ) and code snippets from Neubig et al . ( 2019 ) .
In both of these experiments , we transform all posts by a user into a two -dimensional array of dimension num total words ? embedding size .
For the CNN , filter parameters that must be trained are then window size ?
embedding size ? num f ilters .
Given the small size of the expertannotated dataset , we next explore ways to reduce the number of features a network needs to train .
Modeling with post-level features
We next consider post-level features .
In this dataset the post body field is often empty , presumably when the post comprises only an image or other embedded media , so features must be robust to this variation .
In all subsequent models , each post component ( title or body ) is represented as a one-dimensional vector of size num post f eatures .
Calling each such 1 - D vector x ij , we chronologically concatenate these vectors for each post title and non-empty body for user i into a longer 1 - D vector : x i = x i1 ? x i2 ? ... ? x in .
Thus we represent each user with the concatenated vector of all post features from posts 1 : n , where n is their total number of post titles and non-empty post bodies .
The resulting vector for user i has shape 1 ? ( n * num post f eatures ) .
Users are then batched for quicker training .
Each user vector is padded to the length of the longest one , resulting in a batch of k user vectors having shape k ?
( n max * num post f eatures ) .
Masking prevents back - propagation of weights to padding vectors .
Others ' prior work successfully incorporated LIWC features into suicidality detection ( e.g. Lightman et al . ( 2007 ) ) .
Thus , we experiment with sets of LIWC features as the summary of each post by a user , then concatenate these features from all of a user 's posts .
In order to maintain crosspost context while reducing the number of features , the first model considers only features from the ' affect ' category .
Using just these sentiments appeared likely to predict self- destructive mental state ( Kumar et al. , 2015 ) .
Subsequent models use all 45 features provided in the LIWC dictionary .
We next apply a convolutional neural network to this 1 - D sequence of LIWC features .
Our network uses the keras implementation of a one-dimensional CNN ( Chollet et al. , 2015 ) , setting both stride length and window size equal to num post f eatures and using num f ilters = 10 filters .
This structure means that each window looks at LIWC features from a single post title or body , and extracts relationships between these features into 10 filter representations .
The model forgoes pooling ( following Springenberg et al . ( 2014 ) ) in favor of maintaining independent information about each post .
Thus , after convolution , the batch of k users with max number of posts n max has shape k ?
( n max * num f ilters ) .
Convolution is followed by a dropout layer setting 30 % of input units to 0 at any given timestep , intended to reduce overfitting .
The next two layers are fully connected , with 250 and 100 nodes , respectively , and rectified linear activation functions ; thus , after passing through the second linear layer , the data has shape k ? 100 .
Finally , labels are generated by a softmax output layer .
Training seeks to minimize cross entropy , and uses 10 - fold cross-validation ( CV ) on the training set .
' Affect-only ' model
This model uses the four affect categories relating to negative sentiment : ' negative affect , ' ' anger , ' ' anxiety , ' and ' sadness ' .
We selected this subset as a reasonable approximation of negative valence , and to test its predictive performance without broader information .
' Primary ' model
The best- performing model on a set-aside development set serves as our primary model .
This model differes from the affect-only model in incorporating all 45 LIWC categories as post features .
' Balanced classes ' model Next , we provide our model with custom weights corresponding to the penalty incurred while misclassifying each class .
We provide larger weights for the underrepresented ' low risk ' and ' moderate risk ' classes to force the model to pay more attention to these categories while training .
' Leave none out ' model
This final model used all available data for training .
In the primary and balanced models , it was clear that while training set performance continues to improve , development set performance levels off somewhere around 150 epochs .
That is , crossvalidation results were optimized at epoch 235 for Our primary evaluation metric is the resulting macro- averaged F1 score of our models ; we report averages on a set-aside development set ( see Table 1 ) .
For three approaches , we also present macroaveraged F1 scores on an unseen test set .
Results
With our initial convolutional network model , using GloVe word embeddings in a convolutional neural net in the style of Kim ( 2014 ) , we confirm similar performance to Shing et al . ( 2018 ) with a macro-averaged F1 score of 0.42 .
We also find that this model strongly overfits the data ; it performs exceptionally well on the training data ( F1=0.95 ) but fails to generalize well on development data ( F1=0.42 ) .
This overfitting is expected , since the size of our dataset is not sufficient to successfully train large models .
The high overfitting and our model 's inability to further learn from the dataset encourage us to focus on simpler models , and to thoughtfully select our features .
The best- performing models all use LIWC features at the post level , concatenated by user , and run through a one-dimensional CNN with stride length and window size equal to the number of features .
When representing each post as a vector of LIWC affect features , we find that the base model achieves an F1 - score of 0.47 in cross-validation .
We still find a significant discrepancy between our model 's performance on seen / unseen data , indicating that it still suffers from overfitting .
We experiment with hyperparameters like dropout and number of filters , finding that a model with 10 filters and 0.3 dropout probability outperforms all our previous models with a macro-averaged CV F1score of 0.49 .
On studying the performance of our model , we find that its behaviour is not uniform across all classes : it does well in labeling ' no risk ' and ' severe risk , ' but performs poorly in trying to label the intermediate risk categories .
' Primary ' model
We next use variations to improve features provided while still minimizing parameters trained .
For our ' primary ' model , we provide all 45 LIWC category features to a CNN of the same structure .
In macro- averaging pairwise AUC scores on the development set , this model scores 0.76 .
On the test set , the model 's macro- averaged F1 is 0.37 .
A random guessing strategy weighted by label frequency would yield F1=0.25 .
' Balanced classes ' model
We find that this change boosts the model 's CV performance on our development set to an F1 score of 0.57 , with a macro-averaged AUC score on the development set of 0.78 .
We also find that this model performs more uniformly across the four classes than we see in the previous model , resulting in a slightly better score on the unseen test set , F1=0.40 .
' Leave none out ' model
With this final model and feature architecture , we train our model on the entire training dataset available for Task A , stopping after 150 epochs .
This model achieves our highest score on the test set , a macro-averaged F1 - score of 0.50 on this taskcomparing favorably with the best-scoring system , whose F1 - score is 0.53 .
We also note that our model achieves high F1 -scores ( 0.90 and 0.82 respectively ) for the ' flagged ' and ' urgent ' tasks .
This model 's final confusion matrix is shown in Figure 1 .
We find that our model is best at identifying the ' no risk ' and ' moderate risk ' users , while it miscategorizes 42 % of ' severe risk ' cases as ' moderate risk ' as well .
There are fewer ' low risk ' users , and about half of these are miscategorized as ' moderate risk ' as well .
Discussion
' Affect-only ' model
We can attribute this model 's difficulty with intermediate labels to our usage of only the negative ' affect ' category from LIWC .
This category extracts counts for words associated with ' negative affect , ' ' anger , ' ' anxiety , ' and ' sadness ' , i.e , words one would typically associate with severe suicidality conditions ; presence of ( a large number of ) these words may be common in Severe risk users , whereas their absence might be a strong indicator of No risk users .
Poorer performance in the intermediate categories may indicate inconsistent use of emotion terms by those users , or may suggest a smaller range of variation between those categories as opposed to variation within the extremes .
' Primary ' and ' balanced classes ' models
The ' primary ' and ' balanced classes ' models perform similarly , with a difference in F1 scores of about 0.03 .
We believe that the latter model is slightly more effective because its higher weights for the intermediate categories counteracted those labels ' lower representation in the training set .
This is borne out in the model 's slightly better performance on those classes : it categorizes 1 13 of ' low risk ' and 10 28 ' moderate risk ' users correctly , whereas the ' primary ' model is right about 0 13 and 8 28 of such users , respectively .
Macro-averaged F1 as the primary metric means that even this slight improvement is significant when comparing the two models .
It seems plausible that , because it was trained for longer , the ' primary ' model was more overfitted to the training data .
Because we use 10 fold cross-validation to train these models , we also note that both these models are trained using 90 % of the training data ; we hypothesize this missing 10 % of data to be the primary reason that our leave - none - out model outperforms both of these models .
A larger training dataset allows the model to " observe " more data , which helps both with getting more training data for under-represented classes ( e.g. low and moderate risk ) and with generalizing better on all unseen data .
' Leave none out ' model Difficulty identifying ' low risk ' users may be partially explained by the fact that fewer users from the training set were in that class than any otherjust 10 % of examples were labeled low risk , so there was less opportunity to learn these features .
In Figure 2a , we plot the learned convolutional layer weights from our final model with respect to the input LIWC feature categories , finding that each filter is activated ( or deactivated ) by a subset of LIWC features .
We hypothesize that each filter focuses on learning presence or absence of a particular character trait ( or ' sentiment ' ) from each post .
For instance , filter 9 is inversely associated with money , anxiety , and 'we , ' indicating that someone describing their stress around money would have a negative activation for Filter 9 .
Seeing a stronger association between Filter 9 and ' no risk , ' we can extrapolate that users who are not at risk are less likely to be preoccupied with their financial troubles on r/SW .
While not all subsets are clear , we can observe some patterns .
For instance , Filter 2 has the highest positive weights for ' hear , ' ' negative affect , ' ' death , ' ' percept , ' and ' see . '
We could hypothesize that a user activating this filter is preoccupied with how they are perceived , and is also considering death ( whether their own or that of a loved one ) .
This filter may indicate both a feeling of being observed , perhaps stigmatized , and an experience of suicidal ideation , as discussed by Oexle et al . ( 2017 ) .
Findings
Overall , this work demonstrates the power of combining human feature - engineering with deep learning in data-constrained situations .
The Linguistic Inquiry and Word Count dictionary , in conjunction with a convolutional network , leads to a holistic consideration of each post and each user , all while reducing the overall number of parameters the network needs to learn .
Within the constraints of a relatively small dataset , we find that our best model incorporates engineered features and all available data to outperform a ' baseline ' re-implementation of Shing et al . ( 2018 ) .
Acknowledgements
This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1252522 .
Any opinions , findings , and conclusions or recommendations expressed in this material are those of the author ( s ) and do not necessarily reflect the views of the National Science Foundation .
Dr. Krishnamurti 's time was supported by an Institutional K-award ( NIH KL2 TR001856 ) .
Figure 1 : 1 Figure 1 : Confusion matrix on the test set from the best-performing model
Strength of average alignment between filters and the four classes .
Figure 2 : 2 Figure 2 : Filters for the best-performing model indicate
Table 1 : 1 Shing et al . 's CNN makes use of unigram word embeddings , concatenated by post and then by user , then constructs an overall user score using Average performance of our models in 10 - fold cross-validation on the training set Model Precision Recall F1 CNN + GloVe vectors 0.55 0.43 0.42 Affect-only CNN + LIWC 0.53 0.47 0.49 Primary : CNN + all LIWC 0.65 0.55 0.56 Model Full F1 Flagged F1 Urgent F1 Primary 0.37 0.88 0.77
Leave none out 0.50 0.90 0.82 Balanced classes 0.41 0.90 0.80
Table 2 2 : Performance of our models by macro- averaged F1 on the test set .
' Full F1 ' indicates score across four classes , while ' flagged ' and ' urgent ' F1 reflect binary splits between no / some risk and non- severe / severe risk , respectively .
All three submitted models use a convolutional network plus all LIWC fea- tures .
sliding windows over that sequence .
In a sepa- rate approach , Shing et al.
use an SVM to con- sider post-level features but make an overall risk assessment based on the most concerning individ- ual post .
Neither method incorporates distinct in - sights from individual posts - where , for instance , a long series of moderately concerning posts might indicate more serious risk .
Our model incorpo- rates information from multiple posts within the CNN framework .
