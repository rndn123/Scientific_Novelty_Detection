title
New Word Detection for Sentiment Analysis
abstract
Automatic extraction of new words is an indispensable precursor to many NLP tasks such as Chinese word segmentation , named entity extraction , and sentiment analysis .
This paper aims at extracting new sentiment words from large-scale user-generated content .
We propose a fully unsupervised , purely data-driven framework for this purpose .
We design statistical measures respectively to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word .
The method is almost free of linguistic resources ( except POS tags ) , and requires no elaborated linguistic rules .
We also demonstrate how new sentiment word will benefit sentiment analysis .
Experiment results demonstrate the effectiveness of the proposed method .
Introduction
New words on the Internet have been emerging all the time , particularly in user-generated content .
Users like to update and share their information on social websites with their own language styles , among which new political / social / cultural words are constantly used .
However , such new words have made many natural language processing tasks more challenging .
Automatic extraction of new words is indispensable to many tasks such as Chinese word segmentation , machine translation , named entity extraction , question answering , and sentiment analysis .
New word detection is one of the most critical issues in Chinese word segmentation .
Recent studies ( Sproat and Emerson , 2003 ) ( Chen , 2003 ) have shown that more than 60 % of word segmentation errors result from new words .
Statistics show that more than 1000 new Chinese words appear every year ( Thesaurus Research Center , 2003 ) .
These words are mostly domain-specific technical terms and time -sensitive political / social / cultural terms .
Most of them are not yet correctly recognized by the segmentation algorithm , and remain as out of vocabulary ( OOV ) words .
New word detection is also important for sentiment analysis such as opinionated phrase extraction and polarity classification .
A sentiment phrase with complete meaning should have a correct boundary , however , characters in a new word may be broken up .
For example , in a sentence " ? ?/ n ? ?/ adv ?/ v ?/ n( artists ' performance is very impressive ) " the two Chinese characters " ? / v ?/n( cool ; powerful ) " should always be extracted together .
In polarity classification , new words can be informative features for classification models .
In the previous example , " ? ?( cool ; powerful ) " is a strong feature for classification models while each single character is not .
Adding new words as feature in classification models will improve the performance of polarity classification , as demonstrated later in this paper .
This paper aims to detect new word for sentiment analysis .
We are particulary interested in extracting new sentiment word that can express opinions or sentiment , which is of high value towards sentiment analysis .
New sentiment word , as exemplified in Table 1 , is a sub-class of multi-word expressions which is a sequence of neighboring words " whose exact and unambiguous meaning or connotation cannot be derived from the meaning or connotation of its components " ( Choueka , 1988 ) .
Such new words cannot be directly identified using grammatical rules , which poses a major challenge to automatic analysis .
Moreover , existing lexical resources never have adequate and timely coverage since new words appear constantly .
People thus resort to statistical methods such as Pointwise Mutual Information ( Church and Hanks , 1990 ) , Symmetrical Conditional Probability ( da Silva and Lopes , 1999 ) , Mutual Expectation ( Dias et al. , 2000 ) , Enhanced Mutual Information , and Multi-word Expression Distance ( Bu et al. , 2010
Our central idea for new sentiment word detection is as follows :
Starting from very few seed words ( for example , just one seed word ) , we can extract lexical patterns that have strong statistical association with the seed words ; the extracted lexical patterns can be further used in finding more new words , and the most probable new words can be added into the seed word set for the next iteration ; and the process can be run iteratively until a stop condition is met .
The key issues are to measure the utility of a pattern and to quantify the possibility of a word being a new word .
The main contributions of this paper are summarized as follows : ?
We propose a novel framework for new word detection from large-scale user-generated data .
This framework is fully unsupervised and purely data-driven , and requires very lightweight linguistic resources ( i.e. , only POS tags ) .
?
We design statistical measures to quantify the utility of a pattern and to quantify the possibility of a word being a new word , respectively .
No elaborated linguistic rules are needed to filter undesirable results .
This feature may enable our approach to be portable to other languages . ?
We investigate the problem of polarity prediction of new sentiment word and demonstrate that inclusion of new sentiment word benefits sentiment classification tasks .
The rest of the paper is structured as follows : we will introduce related work in the next section .
We will describe the proposed method in Section 3 , including definitions , the overview of the algorithm , and the statistical measures for addressing the two key issues .
We then present the experiments in Section 4 .
Finally , the work is summarized in Section 5 .
Related Work New word detection has been usually interweaved with word segmentation , particularly in Chinese NLP .
In these works , new word detection is considered as an integral part of segmentation , where new words are identified as the most probable segments inferred by the probabilistic models ; and the detected new word can be further used to improve word segmentation .
Typical models include conditional random fields proposed by ( Peng et al. , 2004 ) , and a joint model trained with adaptive online gradient descent based on feature frequency information ( Sun et al. , 2012 ) .
Another line is to treat new word detection as a separate task , usually preceded by part- of-speech tagging .
The first genre of such studies is to leverage complex linguistic rules or knowledge .
For example , Justeson and Katz ( 1995 ) extracted technical terminologies from documents using a regular expression .
Argamon et al. ( 1998 ) segmented the POS sequence of a multi-word into small POS tiles , counted tile frequency in the new word and non-new-word on the training set respectively , and detected new words using these counts .
Chen and Ma ( 2002 ) employed morphological and statistical rules to extract Chinese new word .
The second genre of the studies is to treat new word detection as a classification problem .
Zhou ( 2005 ) proposed a discriminative Markov Model to detect new words by chunking one or more separated words .
In ( Li et al. , 2005 ) , new word detection was viewed as a binary classification problem .
However , these supervised models requires not only heavy engineering of linguistic features , but also expensive annotation of training data .
User behavior data has recently been explored for finding new words .
Zheng et al. ( 2009 ) explored user typing behaviors in Sogou Chinese Pinyin input method to detect new words .
Zhang et al. ( 2010 ) proposed to use dynamic time warping to detect new words from query logs .
However , both of the work are limited due to the public unavailability of expensive commercial resources .
Statistical methods for new word detection have been extensively studied , and in some sense exhibit advantages over linguistics - based methods .
In this setting , new word detection is mostly known as multi-word expression extraction .
To measure multi-word association , the first model is Pointwise Mutual Information ( PMI ) ( Church and Hanks , 1990 ) .
Since then , a variety of statistical methods have been proposed to measure bi-gram association , such as Log-likelihood ( Dunning , 1993 ) and Symmetrical Conditional Probability ( SCP ) ( da Silva and Lopes , 1999 ) .
Among all the 84 bi-gram association measures , PMI has been reported to be the best one in Czech data ( Pecina , 2005 ) .
In order to measure arbitrary ngrams , most common strategies are to separate ngram into two parts X and Y so that existing bigram methods can be used ( da Silva and Lopes , 1999 ; Dias et al. , 2000 ; Schone and Jurafsky , 2001 ) . proposed Enhanced Mutual Information ( EMI ) which measures the cohesion of n-gram by the frequency of itself and the frequency of each single word .
Based on the information distance theory , Bu et al . ( 2010 ) proposed multi-word expression distance ( MED ) and the normalized version , and reported superior performance to EMI , SCP , and other measures .
Methodology
Definitions Definition 3.1 ( Adverbial word ) .
Words that are used mainly to modify a verb or an adjective , such as " ?( too ) " , " ?( very ) " , " ?( very ) " , and " ? ?( specially ) " .
Definition 3.2 ( Auxiliary word ) .
Words that are auxiliaries , model particles , or punctuation marks .
In Chinese , such words are like " ? , ? , ? , ? , ? " , and punctuation marks include " ? " and so on .
Definition 3.3 ( Lexical Pattern ) .
A lexical pattern is a triplet < AD , * , AU > , where AD is an adverbial word , the wildcard * means an arbitrary number of words 1 , and AU denotes an auxiliary word .
Table 2 gives some examples of lexical patterns .
In order to obtain lexical patterns , we can define regular expressions with POS tags 2 and apply the regular expressions on POS tagged texts .
Since the tags of adverbial and auxiliary words are relatively static and can be easily identified , such a method can safely obtain lexical patterns .
Pattern Frequency <" ? " , * , " ? " > 562,057 <" ? " , * , " ? " > 387,649 <" ? " , * , " ? " > 380,470 <" ? " , * , " ? " >
369,702
Table 2 : Examples of lexical pattern .
The frequency is counted on 237,108,977 Weibo posts .
The Algorithm Overview
The algorithm works as follows : starting from very few seed words ( for example , a word in Table 1 ) , the algorithm can find lexical patterns that have strong statistical association with the seed words in which the likelihood ratio test ( L- RT ) is used to quantify the degree of association .
Subsequently , the extracted lexical patterns can be further used in finding more new words .
We design several measures to quantify the possibility of a candidate word being a new word , and the topranked words will be added into the seed word set for the next iteration .
The process can be run iteratively until a stop condition is met .
Note that we do not augment the pattern set ( P ) at each iteration , instead , we keep a fixed small number of patterns during iteration because this strategy produces optimal results .
From linguistic perspectives , new sentiment words are commonly modified by adverbial words and thus can be extracted by lexical patterns .
This is the reason why the algorithm will work .
Our algorithm is in spirit to double propagation ( Qiu et al. , 2011 ) , however , the differences are apparent in that : firstly , we use very lightweight linguistic information ( except POS tags ) ; secondly , our major contributions are to propose statistical measures to address the following key issues : first , to measure the utility of lexical patterns ; second , to measure the possibility of a candidate word being a new word .
Measuring the Utility of a Pattern
The first key issue is to quantify the utility of a pattern at each iteration .
This can be measured by the association of a pattern to the current word set used in the algorithm .
The likelihood ratio tests ( Dunning , 1993 ) is used for this purpose .
This association model has also been used to model association between opinion target words by ( The LRT is well known for not relying critically on the assumption of normality , instead , it uses the asymptotic assumption of the generalized likelihood ratio .
In practice , the use of likelihood ratios tends to result in significant improvements in text-analysis performance .
In our problem , LRT computes a contingency table of a pattern p and a word w , derived from the corpus statistics , as given in Table 3 , where k 1 ( w , p ) is the number of documents that w matches pattern p , k 2 ( w , p ) is the number of documents that w occurs while p does not , k 3 ( w , p ) is the number of documents that p occurs while w does not , and k 4 ( w , p ) is the number of documents containing neither p nor w .
Based on the statistics shown in Table 3 , the likelihood ratio tests ( LRT ) model captures the statistical association between a pattern p and a word w by employing the following formula : Statistics p p w k 1 ( w , p ) k 2 ( w , p ) w k 3 ( w , p ) k 4 ( w , p ) LRT ( p , w ) = log L ( ? 1 , k1 , n1 ) * L ( ? 2 , k2 , n2 ) L ( ? , k1 , n1 ) * L (? , k2 , n2 ) ( 1 ) where : L ( ? , k , n ) = ? k * ( 1 ? ? ) n?k ; n 1 = k 1 + k 3 ; n 2 = k 2 + k 4 ; ? 1 = k 1 /n 1 ; ? 2 = k 2 /n 2 ; ? = ( k 1 + k 2 ) /( n 1 + n 2 ) .
Thus , the utility of a pattern can be measured as follows : U ( p ) = ? w i ?W LRT ( p , w i ) ( 2 ) where W is the current word set used in the algorithm ( see Algorithm 1 ) .
Measuring the Possibility of Being New Words
Another key issue in the proposed algorithm is to quantify the possibility of a candidate word being a new word .
We consider several factors for this purpose .
Likelihood Ratio Test
Very similar to the pattern utility measure , L- RT can also be used to measure the association of a candidate word to a given pattern set , as follows : LRT ( w ) = ? p i ?P LRT ( w , p i ) ( 3 ) where P is the current pattern set used in the algorithm ( see Algorithm 1 ) , and p i is a lexical pattern .
This measure only quantifies the association of a candidate word to the given pattern set .
It tells nothing about the possibility of a word being a new word , however , a new sentiment word , should have close association with the lexical patterns .
This has linguistic interpretations because new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns .
This measure is proved to be an influential factor by our experiments in Section 4.3 .
Left Pattern Entropy
If a candidate word is a new word , it will be more commonly used with diversified lexical patterns since the non-compositionality of new word means that the word can be used in many different linguistic scenarios .
This can be measured by information entropy , as follows : LP E( w ) = ? ? l i ?L ( Pc , w ) c( li , w ) N ( w ) * log c( li , w ) N ( w ) ( 4 ) where L( P c , w ) is the set of left word of all patterns by which word w can be matched in P c , c( l i , w ) is the count that word w can be matched by patterns whose left word is l i , and N ( w ) is the count that word w can be matched by the patterns in P c .
Note that we use P c , instead of P , because the latter set is very small while computing entropy needs a large number of patterns .
Tuning the size of P c will be further discussed in Section 4.4 .
New Word Probability
Some words occur very frequently and can be widely matched by lexical patterns , but they are not new words .
For example , " ?( love to eat ) " and " ?( love to talk ) " can be matched by many lexical patterns , however , they are not new words due to the lack of non-compositionality .
In such words , each single character has high probability to be a word .
Thus , we design the following measure to favor this observation .
N W P ( w ) = n ? i=1 p( w i ) 1 ? p( w i ) ( 5 ) where w = w 1 w 2 . . . w n , each w i is a single character , and p(w i ) is the probability of the character w i being a word , as computed as follows : p( w i ) = all ( w i ) ? s( w i ) all ( w i ) where all ( w i ) is the total frequency of w i , and s( w i ) is the frequency of w i being a single character word .
Obviously , in order to obtain the value of s( w i ) , some particular Chinese word segmentation tool is required .
In this work , we resort to ICTCLAS ( Zhang et al. , 2003 ) , a widely used tool in the literature .
Non-compositionality Measures
New words are usually multi-word expressions , where a variety of statistical measures have been proposed to detect multi-word expressions .
Thus , such measures can be naturally incorporated into our algorithm .
The first measure is enhanced mutual information ( EMI ) : EM I ( w ) = log 2 F /N ? n i=1 F i ?F N ( 6 ) where F is the number of posts in which a multiword expression w = w 1 w 2 . . . w n occurs , F i is the number of posts where w i occurs , and N is the total number of posts .
The key idea of EMI is to measure word pair 's dependency as the ratio of its probability of being a multi-word to its probability of not being a multi-word .
The larger the value , the more possible the expression will be a multi-word expression .
The second measure we take into account is normalized multi-word expression distance ( Bu et al. , 2010 ) , which has been proposed to measure the non-compositionality of multi-word expressions .
N M ED ( w ) = log | ?( w ) | ? log |?( w ) | logN ? log |?( w ) | ( 7 ) where ?( w ) is the set of documents in which all single words in w = w 1 w 2 . . . w n co-occur , ?( w ) is the set of documents in which word w occurs as a whole , and N is the total number of documents .
Different from EMI , this measure is a strict distance metric , meaning that a smaller value indicates a larger possibility of being a multi-word expression .
As can be seen from the formula , the key idea of this metric is to compute the ratio of the co-occurrence of all words in a multi-word expressions to the occurrence of the whole expression .
Configurations to Combine Various Factors
Taking into account the aforementioned factors , we have different settings to score a new word , as follows : FLRT ( w ) = LRT ( w ) ( 8 ) FLP E ( w ) = LRT ( w ) * LP E ( w ) ( 9 )
Experiment
In this section , we will conduct the following experiments : first , we will compare our method to several baselines , and perform parameter tuning with extensive experiments ; second , we will classify polarity of new sentiment words using two methods ; third , we will demonstrate how new sentiment words will benefit sentiment classification .
Data Preparation
We crawled 237,108,977 Weibo posts from http://www.weibo.com, the largest social website in China .
These posts range from January of 2011 to December of 2012 .
The posts were then part-ofspeech tagged using a Chinese word segmentation tool named ICTCLAS ( Zhang et al. , 2003 ) .
Then , we asked two annotators to label the top 5,000 frequent words that were extracted by lexical patterns as described in Algorithm 1 .
The annotators were requested to judge whether a candidate word is a new word , and also to judge the polarity of a new word ( positive , negative , and neutral ) .
If there is a disagreement on either of the two tasks , discussions are required to make the final decision .
The annotation led to 323 new words , among which there are 116 positive words , 112 negative words , and 95 neutral words 3 .
Evaluation Metric
As our algorithm outputs a ranked list of words , we adapt average precision to evaluate the performance of new sentiment word detection .
The metric is computed as follows : AP ( K ) = ? K k=1 P ( k ) * rel( k ) ? K k=1 rel( k ) where P ( k ) is the precision at cut-off k , rel ( k ) is 1 if the word at position k is a new word and 0 otherwise , and K is the number of words in the ranked list .
A perfect list ( all top K items are correct ) has an AP value of 1.0 .
Evaluation of Different Measures and Comparison to Baselines First , we assess the influence of likelihood ratio test , which measures the association of a word to the pattern set .
As can be seen from Table 4 , the association model ( LRT ) remarkably boosts the performance of new word detection , indicating L-RT is a key factor for new sentiment word extraction .
From linguistic perspectives , new sentiment words are commonly modified by adverbial words and thus should have close association with lexical patterns .
Second , we compare different settings of our method to two baselines .
The first one is enhanced mutual information ( EMI ) where we set F ( w ) = EM I ( w ) and the second baseline is normalized multi-word expression distance ( NMED ) ( Bu et al. , 2010 ) where we set F ( w ) = N M ED ( w ) .
The results are shown in Figure 1 .
As can be seen , all the proposed measures outperform the two baselines ( EM I and N M ED ) remarkably and consistently .
The setting of F N M ED produces the best performance .
Adding N M ED or EM I leads to remarkable improvements because of their capability of measuring non-compositionality of new words .
Only using LRT can obtain a fairly good results when K is small , however , the performance drops sharply because it 's unable to measure non-compositionality .
Comparison between LRT + LP E ( or LRT + LP E + N W P ) and LRT shows that inclusion of left pattern entropy also boosts the performance apparently .
However , the new word probability ( N W P ) has only marginal contribution to improvement .
In the above experiments , we set k p = 5 ( the number of patterns chosen at each iteration ) and k w = 10 ( the number of words added at each iteration ) , which is the optimal setting and will be discussed in the next subsection .
And only one seed word " ?( reverse one 's expectation ) " is used .
Parameter Tuning Firstly , we will show how to obtain the optimal settings of k p and k w .
The measure setting we take here is F N M ED ( w ) , as shown in Formula ( 12 ) .
Again , we choose only one seed word " ?
?( reverse one 's expectation ) " , and the number of words returned is set to K = 300 .
Results in Table 5 show that the performance drops consistently across different k w settings when the number of patterns increases .
Note that at the early stage of Algorithm 1 , larger k p ( perhaps with noisy patterns ) may lead to lower quality of new words ; while larger k w ( perhaps with noisy seed words ) may lead to lower quality of lexical patterns .
Therefore , we choose the optimal setting to small numbers , as k p = 5 , k w = 10 .
Secondly , we justify whether the proposed algorithm is sensitive to the number of seed words .
We set k p = 5 and k w = 10 , and take F N M ED as the weighting measure of new word .
We experimented with only one seed word , two , three , and four seed words , respectively .
The results in Table 6 show very stable performance when different numbers of seed words are chosen .
It 's interesting that the performance is totally the same with different numbers of seed words .
By looking into the pattern set and the selected words at each iteration , we found that the pattern set ( P ) converges soon to the same set after a few iterations ; and at the beginning several iterations , the selected words are almost the same although the order of adding the words is different .
Since the algorithm will finally sort the words at step ( 11 ) and P is the same , the ranking of the words becomes all the same .
Lastly , we need to decide the optimal number of patterns in P c ( that is , k c in Algorithm 1 ) because the set has been used in computing left pattern entropy , see Formula ( 4 ) .
Too small size of P c may lead to insufficient estimation of left pattern entropy .
Results in Table 7 shows that larger P c decrease the performance , particularly when the number of words returned ( K ) becomes larger .
Therefore , we set | P c | = 100 .
Polarity Prediction of New Sentiment Words
In this section , we attempt to classifying the polarity of the annotated 323 new words .
Two methods are adapted with different settings for this purpose .
The first one is majority vote ( MV ) , and the second one is pointwise mutual information , similar to ( Turney and Littman , 2003 ) .
The majority vote method is formulated as below : M V ( w ) = ? wp?P W #( w , wp ) | P W | ? ? wn?N W #( w , wn ) | N W | where P W and N W are a positive and negative set of emoticons ( or seed words ) respectively , and # ( w , w p ) is the co-occurrence count of the input word w and the item w p .
The polarity is judged according to this rule : if M V ( w ) > th 1 , the word w is positive ; if M V ( w ) < ? th 1 the word negative ; otherwise neutral .
The threshold th 1 is manually tuned .
And PMI is computed as follows : where P M I ( x , y ) = log 2 ( P r( x , y ) P r( x ) * P r(y ) ) , and P r( ? ) denotes probability .
The polarity is judged according to the rule : if P M I ( w ) > th 2 , w is positive ; if P M I ( w ) < ? th 2 negative ; otherwise neutral .
The threshold th 2 is manually tuned .
P M I ( w ) = ?
As for the resources P W and N W , we have three settings .
The first setting ( denoted by 1 . Large_Emo ) is a set of most frequent 36 emoticons in which there are 21 positive and 15 negative emoticons respectively .
The second one ( denoted by Small_Emo ) is a set of 10 emoticons , which are chosen from the 36 emoticons , as shown in Table 8 .
The third one ( denoted by Opin_Words ) is two sets of seed opinion words , where P W ={ ? ?( happy ) , ? ?( generous ) , ? ?( beautiful ) , ? ?( kind ) , ?( smart ) } and N W ={?( sad ) , ? ?( mean ) , ?( ugly ) , ?( wicked ) , ?( stupid ) } .
The performance of polarity prediction is shown in Table 9 .
In two -class polarity classification , we remove neutral words and only make prediction with positive / negative classes .
The first observation is that the performance of using emoticons is much better than that of using seed opinion words .
We conjecture that this may be because new sentiment words are more frequently co-occurring with emoticons than with these opinion words .
The second observation is that threeclass polarity classification is much more difficult than two -class polarity classification because many extracted new words are nouns such as " ? ?( gay ) " , " ? ?( girl ) " , and " ? ?( friend ) " .
Such nouns are more difficult to classify sentiment orientation .
Application of New Sentiment Words to Sentiment Classification
In this section , we justify whether inclusion of new sentiment word would benefit sentiment classification .
For this purpose , we randomly sampled and annotated 4,500 Weibo posts that contain at least one opinion word in the union of the Hownet 4 opinion lexicons and our annotated new words .
We apply two models for polarity classification .
The first model is a lexicon- based model ( denoted by Lexicon ) that counts the number of positive and negative opinion words in a post respectively , and classifies a post to be positive if there are more positive words than negative ones , and to be negative otherwise .
The second model is a SVM model in which opinion words are used as feature , and 5 - fold cross validation is conducted .
We experiment with different settings of Hownet lexicon resources : ?
Hownet opinion words ( denoted by Hownet ) :
After removing some obviously inappropriate words , the left lexicons have 627 positive opinion words and 1,038 negative opinion words , respectively .
?
Compact Hownet opinion words ( denoted by cptHownet ) : we count the frequency of the above opinion words on the training data and remove words whose document frequency is less than 2 .
This results in 138 positive words and 125 negative words .
Then , we add into the above resources the labeled new polar words ( denoted by N W , including 116 positive and 112 negative words ) and the top 100 words produced by the algorithm ( denoted by T 100 ) , respectively .
Note that the lexicon- based model requires the sentiment orientation of each dictionary entry 5 , we thus manually label the po- larity of all top 100 words ( we did NOT remove incorrect new word ) .
This results in 52 positive and 34 negative words .
Results in Table 10 show that inclusion of new words in both models improves the performance remarkably .
In the setting of the original lexicon ( Hownet ) , both models obtain 2 - 3 % gains from the inclusion of new words .
Similar improvement is observed in the setting of the compact lexicon .
Note , that T 100 is automatically obtained from Algorithm 1 so that it may contain words that are not new sentiment words , but the resource also improves performance remarkably .
Conclusion
In order to extract new sentiment words from large-scale user- generated content , this paper proposes a fully unsupervised , purely data-driven , and almost knowledge - free ( except POS tags ) framework .
We design statistical measures to quantify the utility of a lexical pattern and to measure the possibility of a word being a new word , respectively .
The method is almost free of linguistic resources ( except POS tags ) , and does not rely on elaborated linguistic rules .
We conduct extensive experiments to reveal the influence of different statistical measures in new word finding .
Comparative experiments show that our proposed method outperforms baselines remarkably .
Experiments also demonstrate that inclusion of new sentiment words benefits sentiment classification definitely .
From linguistic perspectives , our framework is capable to extract adjective new words because the lexical patterns usually modify adjective words .
As future work , we are considering how to extract other types of new sentiment words , such as nounal new words that can express sentiment .
7P = { top k p patterns } ; 8 Use P to extract new words and if the words are in W c , score them with F ( w ) ; 9 W = W ?
{ top k w words } ; W c = W c - W ; Sort words in W with F ( w ) ; Output the ranked list of words in W ; al. , 2012 ) .
