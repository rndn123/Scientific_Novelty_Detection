title
NRC - Canada-2014 : Recent Improvements in the Sentiment Analysis of Tweets
abstract
This paper describes state - of- the - art statistical systems for automatic sentiment analysis of tweets .
In a Semeval - 2014 shared task ( Task 9 ) , our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets .
In the message- level sentiment classification task , our submissions obtained highest scores on the Live- Journal blog posts test set , sarcastic tweets test set , and the 2013 SMS test set .
These systems build on our SemEval - 2013 sentiment analysis systems ( Mohammad et al. , 2013 ) which ranked first in both the termand message - level subtasks in 2013 .
Key improvements over the 2013 systems are in the handling of negation .
We create separate tweet -specific sentiment lexicons for terms in affirmative contexts and in negated contexts .
Introduction Automatically detecting sentiment of tweets ( and other microblog posts ) has attracted extensive interest from both the academia and industry .
The Conference on Semantic Evaluation Exercises ( SemEval ) organizes a shared task on the sentiment analysis of tweets with two subtasks .
In the message - level task , the participating systems are to identify whether a tweet as a whole expresses positive , negative , or neutral sentiment .
In the term-level task , the objective is to determine the sentiment of a marked target term ( a single word or a multi-word expression ) within the tweet .
Our submissions stood first in both subtasks in 2013 .
This paper describes improvements over that sys- tem and the subsequent submissions to the 2014 shared task ( Rosenthal et al. , 2014 ) .
The training data for the SemEval - 2014 shared task is same as that of SemEval - 2013 ( about 10,000 tweets ) .
The 2014 test set has five subcategories : a tweet set provided newly in 2014 ( Twt14 ) , the tweet set used for testing in the 2013 shared task ( Twt13 ) , a set of tweets that are sarcastic ( Sarc14 ) , a set of sentences from the blogging website LiveJournal ( LvJn14 ) , and the set of SMS messages used for testing in the 2013 shared task ( SMS13 ) .
Instances from these categories were interspersed in the provided test set .
The participants were not told about the source of the individual messages .
The objective was to determine how well a system trained on tweets generalizes to texts from other domains .
Our submissions to SemEval - 2014
Task 9 , ranked first in five out of the ten subtask - dataset combinations .
In the other evaluation sets as well , our submissions performed competitively .
The results are summarized in Table 1 .
As we will show , automatically generated tweet -specific lexicons were especially helpful in all subtask - dataset combinations .
The results also show that even though our models are trained only on tweets , they generalize well to data from other domains .
Our systems are based on supervised SVMs and a number of surface -form , semantic , and sentiment features .
The major improvement in our 2014 system over the 2013 system is in the way it handles negation .
Morante and Sporleder ( 2012 ) define negation to be " a grammatical category that allows the changing of the truth value of a proposition " .
Negation is often expressed through the use of negative signals or negators , words such as isnt and never , and it can significantly affect the sentiment of its scope .
We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts .
That is , we automatically determine the average sentiment of a term when occurring in an affirmative context , and separately the average sentiment of a term when occurring in a negated context .
Our Systems Our SemEval - 2014 systems are based on our SemEval - 2013 systems ( Mohammad et al. , 2013 ) .
For completeness , we briefly revisit our previous approach , which uses support vector machine ( SVM ) as the classification algorithm and leverages the following features .
Lexicon features
These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons .
The manually constructed lexicons include the NRC Emotion Lexicon ( Mohammad and Turney , 2010 ; Mohammad and Yang , 2011 ) , the MPQA Lexicon ( Wilson et al. , 2005 ) , and the Bing Liu Lexicon ( Hu and Liu , 2004 ) .
The two automatically constructed lexicons , the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon , were created specifically for tweets ( Mohammad et al. , 2013 ) .
The sentiment score of each term ( e.g. , a word or bigram ) in the automatically constructed lexicons is computed by measuring the PMI ( pointwise mutual information ) between the term and the positive or negative category of tweets using the formula : SenScore ( w ) = P M I ( w , pos ) ? P M I ( w , neg ) ( 1 ) where w is a term in the lexicons .
P M I ( w , pos ) is the PMI score between w and the positive class , and P M I ( w , neg ) is the PMI score between w and the negative class .
Therefore , a positive Sen-Score ( w ) suggests a stronger association of word w with positive sentiment and vice versa .
The magnitude indicates the strength of association .
Note that the sentiment class of the tweets used to construct the lexicons was automatically identified either from hashtags or from emoticons as described in ( Mohammad et al. , 2013 ) .
With these lexicons available , the following features were extracted for a text span .
Here a text span can be a target term , its context , or an entire tweet , depending on the task .
The lexicon features include : ( 1 ) the number of sentiment tokens in a text span ; sentiment tokens are word tokens whose sentiment scores are not zero in a lexicon ; ( 2 ) the total sentiment score of the text span : w?textSpan SenScore ( w ) ; ( 3 ) the maximal score : max w?textSpan SenScore ( w ) ; ( 4 ) the total positive and negative sentiment scores of the text span ; ( 5 ) the sentiment score of the last token in the text span .
Note that all these features are generated , when applicable , by using each of the sentiment lexicons mentioned above .
Ngrams
We employed two types of ngram features : word ngrams and character ngrams .
The former reflect the presence or absence of contiguous or non-contiguous sequences of words , and the latter are sequences of prefix / suffix characters in each word .
These features are same as in our last year 's submission .
Negation
The number of negated contexts .
Our definition of a negated context follows Pang et al . ( 2002 ) , which will be described in more details below in Section 2.1 .
POS
The number of occurrences of each partof-speech tag .
We tokenized and part-of-speech tagged the tweets with the Carnegie Mellon University ( CMU ) Twitter NLP tool ( Gimpel et al. , 2011 ) .
Cluster features
The CMU POS - tagging tool provides the token clusters produced with the Brown clustering algorithm from 56 million Englishlanguage tweets .
These 1,000 clusters serve as an alternative representation of tweet content , reducing the sparsity of the token space .
Encodings
The encoding features are derived from hashtags , punctuation marks , emoticons , elongated words , and uppercased words .
For the term-level task , all the above features are extracted for target terms and their context , where a context is a window of words surrounding a target term .
For the message - level task , the features are extracted from the whole tweet .
In the term-level task , we used the LIB -SVM ( Chang and Lin , 2011 ) tool with the following parameters : -t 0 -b 1 -m 1000 .
The total number of features is about 115,000 .
In the messagelevel task , we used an in-house implementation of SVM with a linear kernel .
The parameter C was set to 0.005 .
The total number of features was about 1.5 million .
Improving Lexicons and Negation Models
An important advantage of our SemEval - 2013 systems comes from the use of the two highcoverage tweet -specific sentiment lexicons .
In the SemEval - 2014 submissions , we improve these lexicons by incorporating negation modeling into the lexicon generation process .
Improving Sentiment Lexicons
A word in a negated context has a different evaluative nature than the same word in an affirmative ( non- negated ) context .
We have proposed a lexicon- based approach to determining the sentiment of words in these two situations by automatically creating separate sentiment lexicons for the affirmative and negated contexts .
In this way , we do not need to employ any explicit assumptions to model negation .
To achieve this , a tweet corpus is split into two parts : Affirmative Context Corpus and Negated Context Corpus .
Following the work of Pang et al . ( 2002 ) , we define a negated context as a segment of a tweet that starts with a negation word ( e.g. , no , should n't ) and ends with one of the punctuation marks : ' , ' , '. ' , ' : ' , ' ; ' , '! ' , '?'.
The list of negation words was adopted from Christopher Potts ' sentiment tutorial .
1
Thus , part of a tweet that is marked as negated is included into the negated context corpus while the rest of the tweet becomes part of the affirmative context corpus .
The sentiment label for the tweet is kept unchanged in both corpora .
Then , we generate an affirmative context lexicon from the affirmative context corpus and a negated context lexicon from the negated context corpus using the technique described in .
Furthermore , we refined the method of constructing the negated context lexicons by splitting a negated context into two parts : the immediate context consisting of a single token that directly follows a negation word , and the distant 1 http://sentiment.christopherpotts.net/lingstruc.html context consisting of the rest of the tokens in the negated context .
This has two benefits .
Intuitively , negation affects words directly following the negation words more strongly than more distant words .
Second , immediate - context scores are less noisy .
Our simple negation scope identification algorithm can at times fail and include parts of a tweet that are not actually negated ( e.g. , if a punctuation mark is missing ) .
Overall , a sentiment word can have up to three scores , one for affirmative context , one for immediate negated context , and one for distant negated context .
We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval - 2014 systems .
Discriminating Negation Words Different negation words , e.g. , never and did n't , can have different effects on sentiment Taboada et al. , 2011 ) .
In our SemEval - 2014 submission , we discriminate negation words in the term-level models .
For example , the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever , while in the sentence this is not acceptable , it is marked as acceptable beNot .
In this way , different negators ( e.g. , be not and be never ) are treated differently .
Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness ( e.g. , was not and am not are treated in the same way ) .
This new representation is used to extract ngrams and lexicon - based features .
Results Overall performance
The evaluation metric used in the competition is the macro-averaged Fmeasure calculated over the positive and negative categories .
Table 2 presents the overall performance of our models .
NRC13 and NRC14 are the systems we submitted to SemEval - 2013 and SemEval - 2014 , respectively .
The integers in the brackets are our official ranks in SemEval - 2014 under the constrained condition .
In the term-level task , our submission ranked first on the two Tweet datasets among 14 teams .
The results show that we achieved significant improvements over our last year 's submission : the Fscore improves from 85.19 to 86.63 on the Twt14 data and from 89.10 to 90.14 on the Twt13 data .
More specifically , on the Twt14 data , the approach described in Section 2.1.1 improved our F-score
Our system ranked second on the LvJn14 and SMS13 dataset .
Note that the term-level system that ranked first on LvJn14 performed worse than our system on SMS13 and the system that ranked first on SMS13 showed worse results than ours on LvJn14 , indicating that our term-level models in general have good generalizability on these two out - of- domain datasets .
On the message - level task , again the NRC14 system showed significant improvements over the last year 's system on all five datasets .
It achieved the second best result on the Twt13 data and the fourth result on the Twt14 data among 42 teams .
It was also the best system to predict sentiment in sarcastic tweets ( Sarc14 ) .
Furthermore , the system proved to generalize well to other types of short informal texts ; it placed first on the two out-ofdomain datasets : SMS13 and LvJn14 .
We observe a major improvement of our message - level model on Sarc14 over our last year 's model , but as the size of Sarc14 is small ( 86 tweets ) , more data and analysis would be desirable to help better understand this phenomenon .
Contribution of features
Table 3 presents the results of ablation experiments on all five test sets for the term-level task .
The features derived from the manual and automatic lexicons proved to be useful on four datasets .
The only exception is the Sarc14 data where removing lexicon features results in no performance improvement .
Considering that this test set is very small ( only about 100 test terms ) , further investigation would be desirable if a larger dataset becomes available .
Also , in sarcasm the real sentiment of a text span may be different from its literal sentiment .
In such a situation , a system that correctly recognizes the literal sentiment may actually make mistakes in capturing the real sentiment .
The last two rows in Table 3 show the results obtained when the features are extracted only from the target ( and not from its context ) and when they are extracted only from the context of the target ( and not from the target itself ) .
Observe that even though the context may influence the polarity of the target , using target features alone is substantially more useful than using context features alone .
Nonetheless , adding context features improves the F-scores in general .
On the message - level task ( Table 4 ) , the features derived from the sentiment lexicons and , in particular , from our large- coverage tweet -specific lexicons turned out to be the most influential .
The use of the lexicons provided consistent gains of 9 - 11 percentage points not only on tweet datasets , but also on out-of- domain SMS and LiveJournal data .
Note that removing the features derived from the manual lexicons as well as removing the ngram features improves the performance on the Twt14 dataset .
However , this effect is not observed on the Twt13 and the out-of- domain test sets .
The possible explanation of this phenomenon is minor overfitting on the tweet data .
Conclusions
We presented supervised statistical systems for message -level and term-level sentiment analysis of tweets .
They incorporate many surface -form , semantic , and sentiment features .
Among submissions from over 40 teams in the Semeval - 2014 shared task " Sentiment Analysis in Twitter " , our submissions ranked first in five out of the ten subtask - dataset combinations .
The single most useful set of features are those obtained from automatically generated tweet -specific lexicons .
We obtained significant improvements over our previous system ( which ranked first in the 2013 shared task ) notably by estimating the sentiment of words in affirmative and negated contexts separately .
Also , since different negation words impact sentiment differently , we modeled different negation words separately in our term-level system .
This too led to an improvement in F-score .
The results on different kinds of evaluation sets show that even though our systems are trained only on tweets , they generalize well to text from other domains such as blog posts and SMS messages .
Many of the resources we created and used are made freely available .
Table 1 : 1 Overall rank of NRC - Canada sentiment analysis models in Semeval - 2014
Task 9 under the constrained condition .
The rows are five evaluation datasets and the columns are the two subtasks .
Evaluation Set Term-level Task Message-level Task Twt14 1 4 Twt13 1 2 Sarc14 3 1 LvJn14 2 1 SMS13 2 1
Table 2 : 2 Overall performance of the NRC - Canada sentiment analysis systems .
from 85.19 to 86.37 , and discriminating nega- tion words ( discussed in Section 2.1.2 ) further im - proved the F-score from 86.37 to 86.63 .
Table 3 : 3 2 Term-level Task :
The macro-averaged F-scores obtained on the 5 test sets with one of the feature groups removed .
Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13 all features 86.63 90.14 77.13 85.49 88.03 all - lexicons 81.98 86.25 80.74 80.00 83.91 all - manu. lex . 86.08 89.25 75.32 84.13 87.69 all - auto. lex . 86.05 88.32 80.38 83.96 86.18 all - ngrams 83.31 86.67 72.95 81.58 82.41 all - target 72.93 74.19 63.09 72.21 69.34 all - context 84.40 88.83 77.22 82.99 87.97 Experiment Twt14 Twt13 Sarc14 LvJn14 SMS13 all features 69.85 70.75 58.16 74.84 70.28 all - lexicons 60.59 60.04 47.17 65.80 60.56 all - manu. lex. 71.84 69.84 53.34 73.41 66.60 all - auto. lex. 63.40 65.08 47.57 71.76 66.94 all - ngrams 70.02 67.90 44.58 74.43 68.45
Table 4 : 4 Message-level
Task :
The macro-averaged F-scores obtained on the 5 test sets with one of the feature groups removed .
www.purl.com/net/sentimentoftweets
