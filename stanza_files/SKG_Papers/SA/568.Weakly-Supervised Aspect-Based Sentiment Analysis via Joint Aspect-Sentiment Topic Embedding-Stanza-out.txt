title
Weakly - Supervised Aspect - Based Sentiment Analysis via Joint Aspect-Sentiment Topic Embedding
abstract
Aspect - based sentiment analysis of review texts is of great value for understanding user feedback in a fine-grained manner .
It has in general two sub-tasks : ( i ) extracting aspects from each review , and ( ii ) classifying aspectbased reviews by sentiment polarity .
In this paper , we propose a weakly - supervised approach for aspect- based sentiment analysis , which uses only a few keywords describing each aspect / sentiment without using any labeled examples .
Existing methods are either designed only for one of the sub-tasks , neglecting the benefit of coupling both , or are based on topic models that may contain overlapping concepts .
We propose to first learn sentiment , aspect joint topic embeddings in the word embedding space by imposing regularizations to encourage topic distinctiveness , and then use neural models to generalize the word- level discriminative information by pre-training the classifiers with embedding - based predictions and self-training them on unlabeled data .
Our comprehensive performance analysis shows that our method generates quality joint topics and outperforms the baselines significantly ( 7.4 % and 5.1 % F1 - score gain on average for aspect and sentiment classification respectively ) on benchmark datasets 1 .
Introduction
With the vast amount of reviews emerging on platforms like Amazon and Yelp , aspect- based sentiment analysis , which extracts opinions about certain facets of entities from text , becomes increasingly essential and benefits a wide range of downstream applications ( Bauman et al. , 2017 ; Nguyen et al. , 2015 ) .
Aspect- based sentiment analysis contains two sub-tasks : Aspect extraction and sentiment polarity classification .
The former identifies the aspect S1 : Eye-pleasing with semiprivate booths , place for a date .
S2 : Mermaid
Inn is an overall good restaurant with really good seafood .
( good , ambience ) ( good , food ) Figure 1 : Two sample restaurant reviews .
Pure aspect words are in red and wavy - underlined , and general opinion words are in blue and framed in boxes .
Words implying both aspects and opinions ( which we define as joint topics ) are underlined and in purple .
covered in the review , whereas the latter decides its sentiment polarity .
Various methods have been proposed for the task .
Neural network models Xu et al. , 2018 ) have outperformed rule- based models ( Hu and Liu , 2004 ; Zhuang et al. , 2006 ) , but they require large-scale fine- grained labeled data to train , which can be difficult to obtain .
Some other studies leverage word embeddings to solve the aspect extraction problem in an unsupervised ( He et al. , 2017 ; Liao et al. , 2019 ) or weaklysupervised setting ( Angelidis and Lapata , 2018 ; Karamanolakis et al. , 2019 ) , without using any annotated documents .
In this work , we study the weakly - supervised setting , where only a few keywords are provided for each aspect and sentiment .
We show two sample restaurant reviews in Fig. 1 together with their expected output-aspect and sentiment labels .
With a closer look at these two example reviews , we observe that S2 includes a general opinion word " good " and a pure aspect word " seafood " , which are separate hints for sentiment and aspect classification respectively .
S1 , on the other hand , does not address the target with plain and general words , but instead use more specific words like " semi-private " and " date " which are uniquely used when people feel good about the ambience instead of other aspects .
Humans can interpret these unique and fine- grained terms as hints for a joint topic of good , ambience , but this is hard for models that are solely trained for one sub-task .
If a model can automatically learn the semantics of each joint topic of sentiment , aspect , it will be able to identify representative terms of the joint topics such as " semi-private " which provide information for aspect and sentiment simultaneously , and will consequently benefit both aspect extraction and sentiment classification .
Therefore , leveraging more fine- grained information by coupling the two subtasks will enhance both .
Several LDA - based methods consider learning joint topics ( Zhao et al. , 2010 ; Wang et al. , 2015 ; Xu et al. , 2012 ) , but they rely on external resources such as part-of-speech ( POS ) tagging or opinion word lexicons .
A recent LDA - based model ( Garc? a - Pablos et al. , 2018 ) uses pre-trained word embedding to bias the prior in topic models to jointly model aspect words and opinion words .
Though working fairly well , topic models are generative models and do not enforce topic distinctivenesstopic -word distribution can largely overlap among different topics , allowing topics to resemble each other .
Besides , topic models yield unstable results , causing large variance in classification results .
We propose the JASen model for Joint Aspect-Sentiment Topic Embedding .
Our general idea is to learn a joint topic representation for each sentiment , aspect pair in the shared embedding space with words so that the surrounding words of topic embeddings nicely describe the semantics of a joint topic .
This is accomplished by training topic embeddings and word embeddings on in-domain corpora and modeling the joint distribution of usergiven keywords on all the joint topics .
After learning the joint topic vectors , embedding - based predictions can be derived for any unlabeled review .
However , these predictions are sub-optimal for sentiment analysis where word order plays an important role .
To leverage the expressive power of neural models , we distill the knowledge from embeddingbased predictions to convolutional neural networks ( CNNs ) ( Krizhevsky et al. , 2012 ) which perform compositional operations upon local sequences .
A self-training process is then conducted to refine CNNs by using their high -confident predictions on unlabeled data .
We demonstrate the effectiveness of JASen by conducting experiments on two benchmark datasets and show that our model outperforms all the baseline methods by a large margin .
We also show that our model is able to describe joint topics with coherent term clusters .
Our contributions can be summarized as follows : ( 1 ) We propose a weakly - supervised method JASen to enhance two sub-tasks of aspect- based sentiment analysis .
Our method does not need any annotated data but only a few keywords for each aspect / sentiment .
( 2 ) We introduce an embedding learning objective that is able to capture the semantics of fine- grained joint topics of sentiment , aspect in the word embedding space .
The embedding - based prediction is effectively leveraged by neural models to generalize on unlabeled data via self-training .
( 3 ) We demonstrate that JASen generates high-quality joint topics and outperforms baselines significantly on two benchmark datasets .
Related Work
The problem of aspect-based sentiment analysis can be decomposed into two sub-tasks : aspect extraction and sentiment polarity classification .
Most previous studies deal with them individually .
There are various related efforts on aspect extraction ( He et al. , 2017 ) , which can be followed by sentiment classification models ( He et al. , 2018 ) .
Other methods ( Garc? a - Pablos et al. , 2018 ) jointly solve these two sub-tasks by first separating target words from opinion words and then learning joint topic distributions over words .
Below we first review relevant work on aspect extraction ( Sec 2.1 ) and then turn to studies that jointly extract aspects and sentiment polarity ( Sec 2.2 ) .
Aspect Extraction Early studies towards aspect extraction are mainly based on manually defined rules ( Hu and Liu , 2004 ; Zhuang et al. , 2006 ) , which have been outperformed by supervised neural approaches that do not need labor-intensive feature engineering .
While CNN ( Xu et al. , 2018 ) and RNN based models have shown the powerful expressiveness of neural models , they can easily consume thousands of labeled documents thus suffer from the label scarcity bottleneck .
Various unsupervised approaches are proposed to model different aspects automatically .
LDAbased methods ( Brody and Elhadad , 2010 ; Chen et al. , 2014 ) model each document as a mixture of aspects ( topics ) and output a word distribution for each aspect .
Recently , neural models have shown to extract more coherent topics .
ABAE ( He et al. , 2017 ) uses an autoencoder to reconstruct sentences through aspect embedding and removes irrelevant words through attention mechanisms .
CAt ( Tulkens and van Cranenburgh , 2020 ) introduces a single head attention calculated by a Radial Basis Function ( RBF ) kernel to be the sentence summary .
The unsupervised nature of these algorithms is hindered by the fact that the learned aspects often do not well align with user 's interested aspects , and additional human effort is needed to map topics to certain aspects , not to mention some topics are irrelevant of interested aspects .
Several weakly - supervised methods address this problem by using a few keywords per aspect as supervision to guide the learning process .
MATE ( Angelidis and Lapata , 2018 ) extends ABAE by initializing aspect embedding using weighted average of keyword embeddings from each aspect .
ISWD ( Karamanolakis et al. , 2019 ) co-trains a bagof-word classifier and an embedding - based neural classifier to generalize the keyword supervision .
Other text classification methods leverage pre-trained language model ( Meng et al. , 2020 b ) to learn the semantics of label names or metadata to propagate document labels .
The above methods do not take aspect-specific opinion words into consideration .
The semantic meaning captured by a sentiment , aspect joint topic preserves more fine- grained information to imply the aspect of a sentence and thus can be used to improve the performance of aspect extraction .
Joint Extraction of Aspect and Sentiment Most previous studies that jointly perform aspect and sentiment extraction are LDA - based methods .
Zhao et al. ( 2010 ) include aspect-specific opinion models along with aspect models in the generative process .
Wang et al. ( 2015 ) propose a restricted Boltzmann machine - based model that treats aspect and sentiment as heterogeneous hidden units .
Xu et al . ( 2012 ) adapt LDA by introducing sentimentrelated variables and integrating sentiment prior knowledge .
All these methods rely on external resources such as part-of-speech ( POS ) tagging or opinion word lexicons .
A more recent study that shares similar weakly - supervised setting with ours is W2VLDA ( Garc? a - Pablos et al. , 2018 ) .
They apply Brown clustering ( Brown et al. , 1992 ) to separate aspect-terms from opinion -terms and construct biased hyperparameters ? and ? by embedding similarity .
Despite the effectiveness of topic models , they suffer from the drawback of not imposing discriminative constraints among topics -topic-word distribution can largely overlap among different topics , allowing redundant topics to appear and making it hard to classify them .
We empirically show the advances of our method by capturing discriminative joint topic representations in the embedding space .
Problem Definition
Our weakly - supervised aspect-based sentiment analysis task is defined as follows .
The input is a training corpus D = {d 1 , d 2 , . . . , d | D| } of text reviews from a certain domain ( e.g. , restaurant or laptop ) without any label for aspects or sentiment .
A list of keywords l a for each aspect topic ( denoted as a ?
A ) and l s for each sentiment polarity ( denoted as s ? S ) are provided by users as guidance .
For each unseen review in the same domain , our model outputs a set of s , a labels .
Model Figure 2 shows the workflow of JASen .
Our goal is to generate a set of sentiment , aspect predictions for each review .
We first learn an embedding space to explicitly represent the semantics of the topics ( including both pure aspect / sentiment and joint sentiment , aspect ones ) as embedding vectors , which are surrounded by the embeddings of the representative words of the topics .
We also impose discriminative regularization on the embedding space to push different topics apart .
To model the local sequential information which is crucial for sentiment analysis , we use CNN as the classifier by pre-training it on pseudo labels given by the cosine similarity between document embeddings and topic embeddings , and self-training it on unlabeled data to iteratively refine its parameters .
Below we introduce the details of JASen .
Joint-Topic Representation Learning
We learn the representations of words and topics on the in-domain corpus by following two principles : ( 1 ) distributional hypothesis ( Sahlgren , 2008 ) and ( 2 ) topic distinctiveness .
The first principle is achieved by an adaptation of the Skip - Gram model ( Mikolov et al. , 2013 ) Mermaid Inn is an overall good restaurant with really good seafood .
d : < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 t H 4 2 Y N G c g w Y / B k K X 8 f f S f K X 0 d M = " > A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L L a C p 5 L 0 o n g q e v F Y w X 5 A G 8 p m s 2 m X 7 m 7 C 7 k Y o o X / B i w d F v P q H v P l v 3 L Q 5 a O u D g c d 7 M 8 z M C x L O t H H d b 6 e 0 s b m 1 v V P e r e z t H x w e V Y 9 P u j p O F a E d E v N Y 9 Q O s K W e S d g w z n P Y T R b E I O O 0 F 0 7 v c 7 z 1 R p V k s H 8 0 s o b 7 A Y 8 k i R r D J p X p Y v x l V a 2 7 D X Q C t E 6 8 g N S j Q H l W / h m F M U k G l I R x r P f D c x P g Z V o Y R T u e V Y a p p g s k U j + n A U o k F 1 X 6 2 u H W O L q w S o i h W t q R B C / X 3 R I a F 1 j M R 2 E 6 B z U S v e r n 4 n z d I T X T t Z 0 w m q a G S L B d F K U c m R v n j K G S K E s N n l m C i m L 0 V k Q l W m B g b T 8 W G 4 K 2 + v E 6 6 z Y b n N r y H Z q 1 1 W 8 R R h j M 4 h 0 v w 4 A p a c A 9 t 6 A C B C T z D K 7 w 5 w n l x 3 p 2 P Z W v J K W Z O 4 Q + c z x / 8 a o 2 G < / l a t = < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > food good , food bad , food = < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " D m N M k 2 Y X Y y T v + z N 0 r W X Y w O / Y u L g = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 I t Q 9 O K x B f s B b S i b 7 a R d u 9 m E 3 Y 1 Q Q n + B F w + K e P U n e f P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F y S C a + O 6 3 0 5 h b X 1 j c 6 u 4 X d r Z 3 d s / K B 8 e t X S c K o Z N F o t Y d Q K q U X C J T c O N w E 6 i k E a B w H Y w v p v 5 7 S d U m s f y w U w S 9 C M 6 l D z k j B o r N W 7 6 5 Y p b d e c g q 8 T L S Q V y 1 P v l r 9 4 g Z m m E 0 j B B t e 5 6 b m L 8 j C r D m c B p q Z d q T C g b 0 y F 2 L Z U 0 Q u 1 n 8 0 O n 5 M w q A x L G y p Y 0 Z K 7 + n s h o p P U k C m x n R M 1 I L 3 s z 8 T + v m 5 r w 2 s + 4 T F K D k i 0 W h a k g J i a z r 8 m A K 2 R G T C y h T H F 7 K 2 E j q i g z N p u S D c F b f n m V t C 6 q n l v 1 G p e V 2 m 0 e R x F O 4 B T O w Y M r q M E 9 1 K E J D B C e 4 R X e n E f n x X l 3 P h a t B S e f O Y Y / c D 5 / A I z N j M E = < / l a t e x i t > + < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " a W O c H J r c b r s x J S L O 3 n 8 0 z 0 6 8 C G E = " > A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j y 2 Y D + g D W W z n b R r N 5 u w u x F K 6 C / w 4 k E R r / 4 k b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g k R w b V z 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o 5 T x b D J Y h G r T k A 1 C i 6 x a b g R 2 E k U 0 i g Q 2 A 7 G d z O / / Y R K 8 1 g + m E m C f k S H k o e c U W O l x k W / X H G r 7 h x k l X g 5 q U C O e r / 8 1 R v E L I 1 Q G i a o 1 l 3 P T Y y f U W U 4 E z g t 9 V K N C W V j O s S u p Z J G q P 1 s f u i U n F l l Q M J Y 2 Z K G z N X f E x m N t J 5 E g e 2 M q B n p Z W 8 m / u d 1 U x P e + B m X S W p Q s s W i M B X E x G T 2 N R l w h c y I i S W U K W 5 v J W x E F W X G Z l O y I X j L L 6 + S 1 m X V c 6 t e 4 6 p S u 8 3 j K M I J n M I 5 e H A N N b i H O j S B A c I z v M K b 8 + i 8 O O / O x 6 K 1 4 O Q z x / A H z u c P c Y W M r w = = < / l a t e x i t > Topic Distribution P ( w j |w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " Q X B g f K Y j q H u 2 Q v I i d T X e 1 9 m e L M s = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h X k o i g h 6 L X j x G s B / Y h r D Z b t q 1 m 0 3 Y 3 V h K 7 L / w 4 k E R r / 4 b b / 4 b t 2 0 O 2 v p g 4 P H e D D P z g o Q z p W 3 7 2 y q s r K 6 t b x Q 3 S 1 v b O 7 t 7 5 f 2 D p o p T S W i D x D y W 7 Q A r y p m g D c 0 0 p + 1 E U h w F n L a C 4 f X U b z 1 S q V g s 7 v Q 4 o V 6 E + 4 K F j G B t p H u 3 O v I f n k Y + O / X L F b t m z 4 C W i Z O T C u R w / f J X t x e T N K J C E 4 6 V 6 j h 2 o r 0 M S 8 0 I p 5 N S N 1 U 0 w W S I + 7 R j q M A R V V 4 2 u 3 i C T o z S Q 2 E s T Q m N Z u r v i Q x H S o 2 j w H R G W A / U o j c V / / M 6 q Q 4 v v Y y J J N V U k P m i M O V I x 2 j 6 P u o x S Y n m Y 0 M w k c z c i s g A S 0 y 0 C a l k Q n A W X 1 4 m z b O a Y 9 e c 2 / N K / S q P o w h H c A x V c O A C 6 n A D L j S A g I B n e I U 3 S 1 k v 1 r v 1 M W 8 t W P n M I f y B 9 f k D B Z e Q e g = = < / l a t e x i t > P ( d|w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " F Z 5 y / M b H R v v D U W k u G C b 4 D d 8 9 w 6 c = " > A A A B 7 3 i c b V B N S w M x E J 3 U r 1 q / q h 6 9 B I t Q L 2 V X B D 0 W v X i s Y D + g X U o 2 m 2 1 D s 9 k 1 y S p l 7 Z / w 4 k E R r / 4 d b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z / E R w b R z n G x V W V t f W N 4 q b p a 3 t n d 2 9 8 v 5 B S 8 e p o q x J Y x G r j k 8 0 E 1 y y p u F G s E 6 i G I l 8 w d r + 6 H r q t x + Y 0 j y W d 2 a c M C 8 i A 8 l D T o m x U q d R D Z 4 e + / y 0 X 6 4 4 N W c G v E z c n F Q g R 6 N f / u o F M U 0 j J g 0 V R O u u 6 y T G y 4 g y n A o 2 K f V S z R J C R 2 T A u p Z K E j H t Z b N 7 J / j E K g E O Y 2 V L G j x T f 0 9 k J N J 6 H P m 2 M y J m q B e 9 q f i f 1 0 1 N e O l l X C a p Y Z L O F 4 W p w C b G 0 + d x w B W j R o w t I V R x e y u m Q 6 I I N T a i k g 3 B X X x 5 m b T O a q 5 T c 2 / P K / W r P I 4 i H M E x V M G F C 6 j D D T S g C R Q E P M M r v K F 7 9 I L e 0 c e 8 t Y D y m U P 4 A / T 5 A 2 e a j 4 o = < / l a t e x i t > P ( t|w i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = " h Y + B T + h x 1 p Z t A o X C N V J t K h N O H z w = " > A A A B 7 3 i c b V D L S g N B E O y N r x h f U Y 9 e B o M Q L 2 F X B D 0 G v X i M Y B 6 Q L G F 2 M p s M m Z 1 d Z 3 q V E P M T X j w o 4 t X f 8 e b f O E n 2 o I k F D U V V N 9 1 d Q S K F Q d f 9 d n I r q 2 v r G / n N w t b 2 z u 5 e c f + g Y e J U M 1 5 n s Y x 1 K 6 C G S 6 F 4 H Q V K 3 k o 0 p 1 E g e T M Y X k / 9 5 g P X R s T q D k c J 9 y P a V y I U j K K V W r U y P j 1 2 x W m 3 W H I r 7 g x k m X g Z K U G G W r f 4 1 e n F L I 2 4 Q i a p M W 3 P T d A f U 4 2 C S T 4 p d F L D E 8 q G t M / b l i o a c e O P Z / d O y I l V e i S M t S 2 F Z K b + n h j T y J h R F N j O i O L A L H p T 8 T + v n W J 4 6 Y + F S l L k i s 0 X h a k k G J P p 8 6 Q n N G c o R 5 Z Q p o W 9 l b A B 1 Z S h j a h g Q / A W X 1 4 m j b O K 5 1 a 8 2 / N S 9 S q L I w 9 H c A x l 8 O A C q n A D N a g D A w n P 8 A p v z r 3 z 4 r w 7 H / P W n J P N H M I f O J 8 / g C q P m g = = < / l a t e x i t > objectives .
Fig. 3 provides the overview of our embedding learning objectives with an example .
Modeling Local and Global Contexts .
We learn word embeddings based on the assumption that words with similar contexts have similar meanings , and define contexts to be a combination of location contexts ( Mikolov et al. , 2013 ) and global contexts ( Meng et al. , 2019 ; Liao et al. , 2019 ; Meng et al. , 2020a ) .
The local context of a word w i refers to other words whose distances are h words or less from w i .
To maximize the probability of seeing the local context of a word w i , we use the following objective : L l = ? w i 0 <|j?i|?h log P ( w j |w i ) , ( 1 ) where P ( w j | w i ) ? exp( v j u i ) , and u i , v j are the center and context word embeddings .
The global context ( Meng et al. , 2019 ; Liao et al. , 2019 ) of a word w i refers to the document d where a word appears , based on the motivation that similar documents contain similar-meaning words .
We use the following objective for global context : L g = ?
d?D w i ?d log P ( d|w i ) , ( 2 ) where P ( d|w i ) ? exp ( d u i ) .
Regularizing Pure Aspect / Sentiment Topics .
To endow the embedding space with discriminative power over the aspect / sentiment categories for better classification performance , we regularize the aspect topic embeddings t a and sentiment topic embeddings t s so that different topics are pushed apart .
For example , the word " good " in Fig .
3 is a keyword for the sentiment topic good , and we aim to place t good close to the word embedding of " good " in the embedding space while away from other topic embeddings ( i.e. , t bad ) .
To achieve this , we maximize the probability of using each topic keyword to predict its representing topic : L A reg = ?
a?A w i ? la log P ( t a | w i ) , ( 3 ) L S reg = ?
s?S w i ?ls log P ( t s |w i ) , ( 4 ) where l a , l s are the keyword lists for aspect a and sentiment s , respectively ; P ( t|w i ) ? exp( u i t ) .
Eqs. ( 3 ) and ( 4 ) empower the embedding space for classification purpose , that is , words can be " classified " into topics based on embedding similarity .
For good initializations of t a and t s , we use the average word embedding of user-provided keywords for each aspect and sentiment .
Regularizing Joint Sentiment , Aspect Topics .
Now we examine the joint case , where | S| ?
| A | topics are regularized .
We connect the learning of joint topic embeddings with pure aspect / sentiment topics by exploring the relationship between marginal distribution and joint distribution : P ( t a | w i ) = s?S
P t s, a w i , ( 5 ) P ( t s |w i ) = a?A P t s, a w i . ( 6 ) As an example , Fig. 3 shows that the marginal probability of the keyword " good " belonging to the sentiment topic " good " is equal to the probability sum of it belonging to good , food , good , ambience and good , service .
The objective for regularizing joint topics L joint can be derived by replacing P ( t a | w i ) in Eq. ( 3 ) with Eq. ( 5 ) and P ( t s |w i ) in Eq. ( 4 ) with Eq. ( 6 ) .
We also notice that general opinion words such as " good " ( or pure aspect words such as " seafood " ) are equally irrelevant to the aspect ( or sentiment ) dimension , so we use a uniform distribution U to regularize their distribution over all the classes on the irrelevant dimension : L A cross = s?S w i ?ls KL ( U , P ( t a | w i ) ) , ( 7 ) L S cross = a?A w i ?la KL ( U , P ( t s | w i ) ) . ( 8 ) Putting the above objectives altogether , our final embedding learning objective is : L = L l +?
g L g +?
r ( L reg + L joint + L cross ) , ( 9 ) where L reg = L A reg +L S reg , and the same for L joint and L cross .
For all the regularization terms , we treat them equally by using the same weight ?
r , which shows to be effective in practice .
Training CNNs for Classification
Word ordering information is crucial for sentiment analysis .
For example , " Any movie is better than this one " and " this one is better than any movie " convey opposite sentiment polarities but have the exactly same words .
The trained embedding space mainly captures word-level discriminative signals but is insufficient to model such sequential information .
Therefore , we propose to train convolutional neural networks ( CNNs ) to generalize knowledge from the preliminary predictions given by the embedding space .
Specifically , we first pre-train CNNs with soft predictions given by the cosine similarity between document embeddings and topic embeddings , and then adopt a self-training strategy to further refine the CNNs using their highconfident predictions on unlabeled documents .
Neural Model Pre-training .
For each unlabeled review , we can ( 1 ) derive one distribution over the joint topics by calculating the cosine similarity between the document representation d and t s , a , ( 2 ) derive separate distributions over sentiment and aspect variables using cosine similarity with marginal topics t a and t s , or ( 3 ) combine ( 1 ) and ( 2 ) by adding the two sets of cosine scores .
We find empirically that the last method achieves the best result , i.e. , the distribution of a test review d over the aspect and sentiment categories is computed as : P ( a |d ) ? exp T ? cos( ta , d ) + s?S cos( t s , a , d ) | S | , ( 10 ) P ( s|d ) ? exp T ? cos( ts , d ) + a?A cos( t s , a , d ) | A | , ( 11 ) where d is obtained by averaging the word embeddings in d , and T is the temperature to control how greedy we want to learn from the embedding - based prediction .
We train two CNN models separately for aspect and sentiment classification by learning from the two distributions in Eqs. ( 10 ) and ( 11 ) .
We leverage the knowledge distillation objective ( Hinton et al. , 2015 ) to minimize the cross entropy between the embedding - based prediction p d and the output prediction q d of the CNNs : H( p d , q d ) = ? t P ( t |d ) log Q ( t | d ) .
( 12 ) Neural Model Refinement .
The pre-trained CNNs only copy the knowledge from the embedding space .
To generalize their current knowledge to the unlabeled corpus , we adopt a self-training technique to bootstrap the CNNs .
The idea of self-training is to use the model 's high - confident predictions on unlabeled samples to refine itself .
Specifically , we compute a target score ( Xie et al. , 2016 ) for each unlabeled document based on the predictions of the current model by enhancing highconfident predictions via a squaring operation : target ( P ( a| d ) ) = P ( a |d ) 2 /f a a ?A P ( a |d ) 2 /f a , where f a = d?D P ( a|d ) .
Since self- training updates the target scores at each epoch , the model is gradually refined by its most recent high - confident predictions .
The self- training process is terminated when no more samples change label assignments after the target scores are updated .
The resulting model can be used to classify any unseen reviews .
Evaluation
We conduct a series of quantitative and qualitative evaluation on benchmark datasets to demonstrate the effectiveness of our model .
Experimental Setup Datasets :
The following two datasets are used for evaluation : ?
Restaurant :
For in - domain training corpus , we collect 17,027 unlabeled reviews from Yelp Dataset Challenge 2 . For evaluation , we use the benchmark dataset in the restaurant domain in SemEval - 2016 ( Pontiki et al. , 2016 ) and SemEval - 2015 ( Pontiki et al. , 2015 ) , where each sentence is labeled with aspect and sentiment polarity .
We remove sentences with multiple labels or with a neutral sentiment polarity to simplify the problem ( otherwise a set of keywords can be added to describe it ) .
?
Laptop :
We leverage 14,683 unlabeled Amazon reviews under the laptop category collected by ( He and McAuley , 2016 ) as in- domain training corpus .
We also use the benchmark dataset in the laptop domain in SemEval - 2016 and SemEval - 2015 for evaluation .
Detailed statistics of both datasets are listed in Table 1 , and the aspects along with their keywords are in Table 2 . Preprocessing and Hyperparameter Setting .
To preprocess the training corpus D , we use the word tokenizer provided by NLTK 3 .
We also use a phrase mining tool , AutoPhrase ( Shang et al. , 2017 ) , to extract meaningful phrases such as " great wine " and " numeric keypad " such that they can capture complicated semantics in a single text unit .
We use the benchmark validation set to fine- tune the hyperparameters : embedding dimension = 100 , local context window size h = 5 , ? g = 2.5 , ?
Quantitative Evaluation
We conduct quantitative evaluation on both aspect extraction and sentiment polarity classification .
Compared Methods .
Our model is compared with several previous studies .
A few of them are specifically designed for aspect extraction but do not perform well on sentiment classification .
So we only report their results on aspect extraction .
For fair comparison , we use the same training corpus and test set for each baseline method .
For weaklysupervised methods , they are fed with the same keyword list as ours .
?
CosSim :
The topic representation is averaged by the embedding of seed words trained by Word2Vec on training corpus .
Cosine similarity is computed between a test sample and the topics to classify the sentence .
? ABAE ( He et al. , 2017 ) :
An attention - based model to unsupervisedly extract aspects .
An autoencoder is trained to reconstruct sentences through aspect embeddings .
The learned topics need to be manually mapped to aspects .
? CAt ( Tulkens and van Cranenburgh , 2020 ) :
A recent method for unsupervised aspect extraction .
A single head attention is calculated by a Radio Basis Function kernel to be the sentence summary .
? W2VLDA ( Garc?a- Pablos et al. , 2018 ) :
A stateof - the- art topic modeling based method that leverages keywords for each aspect / sentiment to jointly do aspect / sentiment classification .
? BERT ( Devlin et al. , 2019 ) : A recent proposed deep language model .
We utilize the pre-trained BERT ( 12 - layer , 768 dimension , uncased ) and implement a simple weakly - supervised method that fine- tunes the model by providing pseudo
Effect of Number of Keywords
We study the effect of the number of keywords .
In Figure 4 we show the macro- F1 score of aspect extraction using different number of keywords for each aspect on Laptop dataset .
The trend clearly shows the model performance increases when more keywords are provided .
Moreover , when only one keyword is provided ( only the label name ) , JASen still has a stable performance and a large gap over the ablation without learning joint topic embedding , implying that learning joint topic semantics is especially powerful in low resource setting .
Joint Topic Representation Visualization
To understand how the joint topics are distributed in the embedding space , along with the aspect and sentiment topics , we use PCA ( Jolliffe , 2011 ) for dimension reduction to visualize topic embedding trained on the Restaurant corpus in Fig.
5 .
An interesting observation is that , some aspect topics ( e.g. , ambience ) lie approximately in the middle of their joint topics ( " good , ambience " and " bad , ambience " ) , showing that our embedding learning objective understands the joint topics as decomposition of their " marginal " topics , which fits with our goal to learn fine - grained topics .
Case Studies
We list several test samples along with their ground truth and model predictions in Table 6 and Table 7 .
Some conflicting cases between ours and the ground truth are rather ambiguous .
For example , the ground truth of the second example in Table 6 is ( good , location ) , but we still think given that the review mentions " the outdoor atmosphere " and uses terms like " sitting on the sidewalk " and " cool evening " , it is more relevant to ambience than location , as is output by our full model .
The gold aspect label for the second and the third reviews in Table 7 are both " company " , but apparently these two sentences are talking about two different aspects : the product itself and the service of the company .
Though the output of our model , " os " and " support " for these two sentences may not be the most precise prediction , at least our model treats them as two different aspects .
Conclusion
In this paper we propose to enhance weaklysupervised aspect-based sentiment analysis by learning the representation of sentiment , aspect joint topic in the embedding space to capture more fine - grained information .
We introduce an Figure 2 : 2 Figure 2 : Overview of our model JASen .
We first leverage the in-domain training corpus and user-given keywords to learn joint topic representation in the word embedding space .
The marginal probability of keywords belonging to an aspect / sentiment can be summed up by the joint distribtution over sentiment , aspect joint topics .
Embeddingbased prediction on unlabeled data are then leveraged by neural models for pre-training and self-training .
