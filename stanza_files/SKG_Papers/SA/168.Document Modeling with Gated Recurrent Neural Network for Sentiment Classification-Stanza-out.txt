title
Document Modeling with Gated Recurrent Neural Network for Sentiment Classification
abstract
Document level sentiment classification remains a challenge : encoding the intrinsic relations between sentences in the semantic meaning of a document .
To address this , we introduce a neural network model to learn vector-based document representation in a unified , bottom - up fashion .
The model first learns sentence representation with convolutional neural network or long short -term memory .
Afterwards , semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network .
We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge .
Experimental results show that : ( 1 ) our neural model shows superior performances over several state - of - the - art algorithms ; ( 2 ) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification .
1 * Corresponding author .
1 Codes and datasets are publicly available at http://ir.hit.edu.cn/ ?dytang .
Introduction Document level sentiment classification is a fundamental task in sentiment analysis , and is crucial to understand user generated content in social networks or product reviews ( Manning and Sch?tze , 1999 ; Jurafsky and Martin , 2000 ; Pang and Lee , 2008 ; Liu , 2012 ) .
The task calls for identifying the overall sentiment polarity ( e.g. thumbs up or thumbs down , 1 - 5 stars on review sites ) of a document .
In literature , dominant approaches follow ( Pang et al. , 2002 ) and exploit machine learn-ing algorithm to build sentiment classifier .
Many of them focus on designing hand -crafted features ( Qu et al. , 2010 ; Paltoglou and Thelwall , 2010 ) or learning discriminate features from data , since the performance of a machine learner is heavily dependent on the choice of data representation .
Document level sentiment classification remains a significant challenge : how to encode the intrinsic ( semantic or syntactic ) relations between sentences in the semantic meaning of document .
This is crucial for sentiment classification because relations like " contrast " and " cause " have great influences on determining the meaning and the overall polarity of a document .
However , existing studies typically fail to effectively capture such information .
For example , Pang et al . ( 2002 ) and Wang and Manning ( 2012 ) represent documents with bag-of-ngrams features and build SVM classifier upon that .
Although such feature - driven SVM is an extremely strong performer and hardly to be transcended , its " sparse " and " discrete " characteristics make it clumsy in taking into account of side information like relations between sentences .
Recently , Le and Mikolov ( 2014 ) exploit neural networks to learn continuous document representation from data .
Essentially , they use local ngram information and do not capture semantic relations between sentences .
Furthermore , a person asked to do this task will naturally carry it out in a sequential , bottom - up fashion , analyze the meanings of sentences before considering semantic relations between them .
This motivates us to develop an end-to-end and bottom - up algorithm to effectively model document representation .
In this paper , we introduce a neural network approach to learn continuous document representation for sentiment classification .
The method is on the basis of the principle of compositionality ( Frege , 1892 ) , which states that the meaning of a longer expression ( e.g. a sentence or a docu - ment ) depends on the meanings of its constituents .
Specifically , the approach models document representation in two steps .
In the first step , it uses convolutional neural network ( CNN ) or long short - term memory ( LSTM ) to produce sentence representations from word representations .
Afterwards , gated recurrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations .
These representations are naturally used as features to classify the sentiment label of each document .
The entire model is trained end-to - end with stochastic gradient descent , where the loss function is the cross-entropy error of supervised sentiment classification 2 .
We conduct document level sentiment classification on four large-scale review datasets from IMDB 3 and Yelp Dataset Challenge 4 .
We compare to neural network models such as paragraph vector ( Le and Mikolov , 2014 ) , convolutional neural network , and baselines such as feature - based SVM ( Pang et al. , 2002 ) , recommendation algorithm JMARS ( Diao et al. , 2014 ) ?
We present a neural network approach to encode relations between sentences in document representation for sentiment classification .
?
We report empirical results on four large-scale datasets , and show that the approach outperforms state - of - the - art methods for document level sentiment classification .
?
We report empirical results that traditional recurrent neural network is weak in modeling document composition , while adding neural gates dramatically improves the classification performance .
The Approach
We introduce the proposed neural model in this section , which computes continuous vector representations for documents of variable length .
These representations are further used as features to classify the sentiment label of each document .
An overview of the approach is displayed in Figure 1 .
Our approach models document semantics based on the principle of compositionality ( Frege , 1892 ) , which states that the meaning of a longer expression ( e.g. a sentence or a document ) comes from the meanings of its constituents and the rules used to combine them .
Since a document consists of a list of sentences and each sentence is made up of a list of words , the approach models document representation in two stages .
It first produces continuous sentence vectors from word represen-tations with sentence composition ( Section 2.1 ) .
Afterwards , sentence vectors are treated as inputs of document composition to get document representation ( Section 2.2 ) .
Document representations are then used as features for document level sentiment classification ( Section 2.3 ) .
Sentence Composition
We first describe word vector representation , before presenting a convolutional neural network with multiple filters for sentence composition .
Each word is represented as a low dimensional , continuous and real-valued vector , also known as word embedding ( Bengio et al. , 2003 ) .
All the word vectors are stacked in a word embedding matrix L w ?
R d?|V | , where d is the dimension of word vector and | V | is vocabulary size .
These word vectors can be randomly initialized from a uniform distribution ( Socher et al. , 2013 b ) , or be pre-trained from text corpus with embedding learning algorithms ( Mikolov et al. , 2013 ; Pennington et al. , 2014 ; Tang et al. , 2014 ) .
We adopt the latter strategy to make better use of semantic and grammatical associations of words .
We use convolutional neural network ( CNN ) and long short - term memory ( LSTM ) to compute continuous representations of sentences with semantic composition .
CNN and LSTM are stateof - the - art semantic composition models for sentiment classification ( Kim , 2014 ; Johnson and Zhang , 2015 ; Li et al. , 2015a ) .
They learn fixed - length vectors for sentences of varying length , captures words order in a sentence and does not depend on external dependency or constituency parse results .
One could also use tree - based composition method such as Recursive Neural Tensor Network ( Socher et al. , 2013 b ) or Tree-Structured LSTM ( Tai et al. , 2015 ; Zhu et al. , 2015 ) as alternatives .
Specifically , we try CNN with multiple convolutional filters of different widths ( Tang et al. , 2015 ) to produce sentence representation .
Figure 2 displays the method .
We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities , which have been proven effective for sentiment classification .
For example , a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence .
In this work , we use three convolutional filters whose widths are 1 , 2 and 3 to encode the semantics of unigrams , bigram - s and trigrams in a sentence .
Each filter consists of a list of linear layers with shared parameters .
Formally , let us denote a sentence consisting of n words as {w 1 , w 2 , ...w i , ...w n } , let l c be the width of a convolutional filter , and let W c , b c be the shared parameters of linear layers in the filter .
Each word w i is mapped to its embedding representation e i ?
R d .
The input of a linear layer is the concatenation of word embeddings in a fixed - length window size l c , which is denoted as I c = [ e i ; e i + 1 ; ... ; e i+lc?1 ] ? R d?lc .
The output of a linear layer is calculated as O c = W c ?
I c + b c ( 1 ) where W c ?
R loc?d?lc , b c ?
R loc , l oc is the output length of linear layer .
To capture global semantics of a sentence , we feed the outputs of linear layers to an average pooling layer , resulting in an output vector with fixed - length .
We further add hyperbolic tangent ( tanh ) to incorporate pointwise nonlinearity , and average the outputs of multiple filters to get sentence representation .
We also try lstm as the sentence level semantic calculator , the performance comparison between these two variations is given in Section 3 .
Document Composition with Gated Recurrent Neural Network
The obtained sentence vectors are fed to a document composition component to calculate the document representation .
We present a gated recurrent neural network approach for document composition in this part .
Given the vectors of sentences of variable length as input , document composition produces a fixed - length document vector as output .
To this end , a simple strategy is ignoring the order of sen- tences and averaging sentence vectors as document vector .
Despite its computational efficiency , it fails to capture complex linguistic relations ( e.g. " cause " and " contrast " ) between sentences .
Convolutional neural network ( Denil et al. , 2014 ) is an alternative for document composition , which models local sentence relations with shared parameters of linear layers .
Standard recurrent neural network ( RNN ) can map vectors of sentences of variable length to a fixed - length vector by recursively transforming current sentence vector s t with the output vector of the previous step h t?1 .
The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh .
GNN Softmax GNN GNN ? 1 ? 2 ? ? Softmax Average ( a ) GatedNN ( b) GatedNN Avg GNN GNN GNN ? 1 ? 2 ? ? ? ? h t = tanh ( W r ? [ h t?1 ; s t ] + b r ) ( 2 ) where W r ?
R l h ?( l h + loc ) , b r ?
R l h , l h and l oc are dimensions of hidden vector and sentence vector , respectively .
Unfortunately , standard RNN suffers the problem of gradient vanishing or exploding ( Bengio et al. , 1994 ; Hochreiter and Schmidhuber , 1997 ) , where gradients may grow or decay exponentially over long sequences .
This makes it difficult to model long-distance correlations in a sequence .
To address this problem , we develop a gated recurrent neural network for document composition , which works in a sequential way and adaptively encodes sentence semantics in document representations .
The approach is analogous to the recently emerged LSTM ( Graves et al. , 2013 ; Zaremba and Sutskever , 2014 ; Xu et al. , 2015 ) and gated neural network ( Cho et al. , 2014 ; Chung et al. , 2015 ) .
Specifically , the transition function of the gated RNN used in this work is calculated as follows .
i t = sigmoid ( W i ? [ h t?1 ; s t ] + b i ) ( 3 ) f t = sigmoid ( W f ? [ h t?1 ; s t ] + b f ) ( 4 ) g t = tanh ( W r ? [ h t?1 ; s t ] + b r ) ( 5 ) h t = tanh ( i t g t + f t h t?1 ) ( 6 ) where stands for element - wise multiplication , W i , W f , b i , b f adaptively select and remove history vector and input vector for semantic composition .
The model can be viewed as a LSTM whose output gate is alway on , since we prefer not to discarding any part of the semantics of sentences to get a better document representation .
Figure 3 ( a ) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification .
We can make further extensions such as averaging hidden vectors as document representation , which takes considerations of a hierarchy of historical semantics with different granularities .
The method is illustrated in Figure 3 ( b ) , which shares some characteristics with ( Zhao et al. , 2015 ) .
We can go one step further to use preceding histories and following evidences in the same way , and exploit bidirectional ( Graves et al. , 2013 ) gated RNN as the calculator .
The model is embedded in Figure 1 .
Sentiment Classification
The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering .
Specifically , we first add a linear layer to transform document vector to real-valued vector whose length is class number C.
Afterwards , we add a sof tmax layer to convert real values to conditional probabilities , which is calculated as follows .
P i = exp( x i ) C i =1 exp ( x i ) ( 7 )
We conduct experiments in a supervised learning setting , where each document in the training data is accompanied with its gold sentiment label .
Corpus P g i ( d ) ? log ( P i ( d ) ) ( 8 ) where T is the training data , C is the number of classes , d represents a document .
P g ( d ) has a 1 - of - K coding scheme , which has the same dimension as the number of classes , and only the dimension corresponding to the ground truth is 1 , with all others being 0 .
We take the derivative of loss function through back - propagation with respect to the whole set of parameters ? = [ W c ; b c ; W i ; b i ; W f ; b f ; W r ; b r ; W sof tmax , b sof tmax ] , and update parameters with stochastic gradient descent .
We set the widths of three convolutional filters as 1 , 2 and 3 , output length of convolutional filter as 50 .
We learn 200 - dimensional word embeddings with SkipGram ( Mikolov et al. , 2013 ) on each dataset separately , randomly initialize other parameters from a uniform distribution U ( ?0.01 , 0.01 ) , and set learning rate as 0.03 .
Experiment
We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification .
We describe experimental settings and report empirical results in this section .
Experimental Setting
We conduct experiments on large-scale datasets consisting of document reviews .
Specifically , we use one movie review dataset from IMDB ( Diao et al. , 2014 ) and three restaurant review datasets from Yelp Dataset Challenge in 2013 , 2014 and 2015 .
Human labeled review ratings are regarded as gold standard sentiment labels , so that we do not need to manually annotate sentiment labels of documents .
We do not consider the cases that rating does not match with review texts ( Zhang et al. , 2014 ) .
Statistical information of these datasets are given in Table 1 .
We use the same dataset split as in ( Diao et al. , 2014 ) on IMDB dataset , and split Yelp datasets into training , development and testing sets with 80/10/10 .
We run tokenization and sentence splitting with Stanford CoreNLP ( Manning et al. , 2014 ) on all these datasets .
We use accuracy ( Manning and Sch?tze , 1999 ; Jurafsky and Martin , 2000 ) and M SE ( Diao et al. , 2014 ) as evaluation metrics , where accuracy is a standard metric to measure the overall sentiment classification performance .
We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths ( e.g. one star means strong negative and five star means strong positive ) .
M SE = N i ( gold i ? predicted i ) 2 N ( 9 )
Baseline Methods
We compare our methods ( Conv- GRNN and LSTM - GRNN ) with the following baseline methods for document level sentiment classification .
( 1 ) Majority is a heuristic baseline , which assigns the majority sentiment label in training set to each document in test set .
( 2 ) In SVM + Ngrams , we use bag-of-unigrams and bag-of - bigrams as features and train SVM classifier with LibLinear ( Fan et al. , 2008 ) 5 . ( 3 ) In TextFeatures , we implement sophisticated features ( Kiritchenko et al. , 2014 ) ( 4 ) In AverageSG , we learn 200 - dimensional word vectors with word2vec 6 ( Mikolov et al. , 2013 ) , average word embeddings to get document representation , and train a SVM classifier .
( 5 ) We learn sentiment -specific word embeddings ( SSWE ) , and use max / min / average pooling ( Tang et al. , 2014 ) to get document representation .
( 6 ) We compare with a state - of- the - art recommendation algorithm JMARS ( Diao et al. , 2014 ) , which utilizes user and aspects of a review with collaborative filtering and topic modeling .
( 7 ) We implement a convolutional neural network ( CNN ) baseline as it is a state - of - the - art semantic composition method for sentiment analysis ( Kim , 2014 ; Denil et al. , 2014 ) . ( 8 ) We implement a state - of - the - art neural network baseline Paragraph Vector ( Le and Mikolov , 2014 ) because its codes are not officially provided .
Window size is tuned on the development set .
Comparison to Other Methods Experimental results are given in Table 2 .
We evaluate each dataset with two metrics , namely accuracy ( higher is better ) and MSE ( lower is better ) .
The best method in each dataset and each evaluation metric is in bold .
From Table 2 , we can see that majority is the worst method because it does not capture any textual semantics .
SVM classifiers with unigram and bigram features ( Pang et al. , 2002 ) are extremely strong , which are almost the strongest performers among all baseline methods .
Designing complex features are also effective for document level sentiment classification , however , it does not surpass the bag-of-ngram features significantly as on Twitter corpora ( Kiritchenko et al. , 2014 ) .
Furthermore , the aforementioned bag-of-features are discrete and sparse .
For example , the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899 K and 4.81 M after we filter out low frequent features .
Based on them , we try to concatenate several discourse-driven features , but the classification performances remain unchanged .
AverageSG is a straight forward way to compose document representation without feature engineering .
Unfortunately , we can see that it does not work in this scenario , which appeals for powerful semantic composition models for document level sentiment classification .
We try to make better use of the sentiment information to learn a better SSWE ( Tang et al. , 2014 ) , e.g. setting a large window size .
However , its performance is still worse than context - based word embedding .
This stems from the fact that there are many sentiment shifters ( e.g. negation or contrast words ) in document level reviews , while Tang et al . ( 2014 ) learn SSWE by assigning sentiment label of a text to each phrase it contains .
How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work .
Since JMARS outputs real-valued outputs , we only evaluate it in terms of M SE .
We can see that sophisticated baseline methods such as JMARS , paragraph vector and convolutional NN obtain significant performance boosts over AverageSG by capturing deeper semantics of texts .
Comparing between CNN and AverageSG , we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents .
However , it is somewhat disappointing that these models do not significantly outperform discrete bag-of-ngrams and bag-of-features .
The reason might lie in that semantic meanings of documents , e.g. relations between sentences , are not well captured .
We can see that the proposed method Conv-GRNN and LSTM - GRNN yield the best performance on all four datasets in two evaluation metrics .
Compared with CNN , Conv-GRNN shows its superior power in document composition component , which encodes semantics of sentences and their relations in document representation with gated recurrent neural network .
We also find that LSTM ( almost ) consistently performs better than CNN in modeling the sentence representation .
Model Analysis
As discussed before , document composition contributes a lot to the superior performance of Conv-GRNN and LSTM - GRNN .
Therefore , we take Conv-GRNN as an example and compare different neural models for document composition in this part .
Specifically , after obtaining sentence vectors with convolutional neural network as described in Section 2.1 , we carry out experiments in following settings .
( 1 ) Average .
Sentence vectors are averaged to get the document vector .
( 2 ) Recurrent / GatedNN .
Sentence vectors are fed to standard ( or gated ) recurrent neural network in a sequential way from the beginning of the input document .
The last hidden vector is regarded as document representation .
( 3 ) Recurrent Avg / GatedNN Avg .
We extend setting ( 2 ) by averaging hidden vectors of recurrent neural network as document vector .
( 4 ) Bi Recurrent Avg / Bi GatedNN Avg .
We extend setting ( 3 ) by calculating hidden vectors from both preceding histories and following contexts .
Bi-directional hidden vectors are averaged as document representation .
Table 3 shows the experimental results .
We can see that standard recurrent neural network ( RN - N ) is the worst method , even worse than the simple vector average .
This is because RNN suffers from the vanishing gradient problem , stating that the influence of a given input on the hidden layer decays exponentially over time on the network output .
In this paper , it means that document representation encodes rare semantics of the beginning sentences .
This is further justified by the great improvement of Recurrent Avg over Recurrent .
Bi Recurrent Avg and Recurrent Avg perform comparably , but disappointingly both of them fail to transcend Average .
After adding neural gates , GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings .
The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend , and it is practical to adaptively model sentence semantics in document representation .
GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN .
Related Work Document level sentiment classification is a fundamental problem in sentiment analysis ( Pang and Lee , 2008 ; Liu , 2012 ) , which aims at identifying the sentiment label of a document ( Pang et al. , 2002 ; Turney , 2002 ) . Pang and Lee ( 2002 ; 2005 ) cast this problem as a classification task , and use machine learning method in a supervised learning framework .
Turney ( 2002 ) introduces an unsupervised approach by using sentiment words / phrases extracted from syntactic patterns to determine the document polarity .
Goldberg and Zhu ( 2006 ) place this task in a semi-supervised setting , and use unlabelled reviews with graph - based method .
Dominant studies in literature follow Pang et al . ( 2002 ) and work on designing effective features for building a powerful sentiment classifier .
Representative features include word ngrams ( Wang and Manning , 2012 ) , text topic ( Ganu et al. , 2009 ) , bag-of-opinions ( Qu et al. , 2010 ) , syntactic relations ( Xia and Zong , 2010 ) , sentiment lexicon features ( Kiritchenko et al. , 2014 ) .
Despite the effectiveness of feature engineering , it is labor intensive and unable to extract and organize the discriminative information from data .
Recently , neural network emerges as an effective way to learn continuous text representation for sentiment classification .
Existing studies in this direction can be divided into two groups .
One line of research focuses on learning continuous word embedding .
Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way ( Bengio et al. , 2003 ; Mikolov et al. , 2013 ; Baroni et al. , 2014 ) .
Since these methods typically map words with similar contexts but opposite polarity ( e.g. " good " and " bad " ) to neighboring vectors , several studies ( Maas et al. , 2011 ; Labutov and Lipson , 2013 ; Tang et al. , 2014 ) learn sentiment -specific word embeddings by taking sentiment of texts into account .
Another line of research concentrates on semantic composition ( Mitchell and Lapata , 2010 ) . Yessenalina and Cardie ( 2011 ) represent each word as a matrix and use iterated matrix multiplication as phrase -level composition function .
Socher et al. ( 2013 b ) introduce a family of recursive neural networks for sentence - level semantic composition .
Recursive neural network is extended with global feedbackward ( Paulus et al. , 2014 ) , feature weight tuning ( Li , 2014 ) , deep recursive layer ( Irsoy and Cardie , 2014 ) , adaptive composition functions ( Dong et al. , 2014 ) , combined with Combinatory Categorial Grammar ( Hermann and Blunsom , 2013 ) , and used for opinion relation detection .
Glorot et al. ( 2011 ) use stacked denoising autoencoder .
Convolutional neural networks are widely used for semantic compo-sition ( Kim , 2014 ; Denil et al. , 2014 ; Johnson and Zhang , 2015 ) by automatically capturing local and global semantics .
Le and Mikolov ( 2014 ) introduce Paragraph Vector to learn document representation from semantics of words .
Sequential model like recurrent neural network or long short - term memory ( LSTM ) are also verified as strong approaches for semantic composition ( Li et al. , 2015 a ) .
In this work , we represent document with convolutional - gated recurrent neural network , which adaptively encodes semantics of sentences and their relations .
A recent work in ( Li et al. , 2015 b ) also investigate LSTM to model document meaning .
They verify the effectiveness of LSTM in text generation task .
Conclusion
We introduce neural network models ( Conv- GRNN and LSTM - GRNN ) for document level sentiment classification .
The approach encodes semantics of sentences and their relations in document representation , and is effectively trained end-to -end with supervised sentiment classification objectives .
We conduct extensive experiments on four review datasets with two evaluation metrics .
Empirical results show that our approaches achieve state - of - the - art performances on all these datasets .
We also find that ( 1 ) traditional recurrent neural network is extremely weak in modeling document composition , while adding neural gates dramatically boosts the performance , ( 2 ) LSTM performs better than a multi-filtered CNN in modeling sentence representation .
We briefly discuss some future plans .
How to effectively compose sentence meanings to document meaning is a central problem in natural language processing .
In this work , we develop neural models in a sequential way , and encode sentence semantics and their relations automatically without using external discourse analysis results .
From one perspective , one could carefully define a set of sentiment - sensitive discourse relations ( Zhou et al. , 2011 ) , such as " contrast " , " condition " , " cause " , etc .
Afterwards , relation -specific gated RNN can be developed to explicitly model semantic composition rules for each relation ( Socher et al. , 2013a ) .
However , defining such a relation scheme is linguistic driven and time consuming , which we leave as future work .
From another perspective , one could compose document representation over discourse tree structures rather than in a sequential way .
Accordingly , Recursive Neural Network ( Socher et al. , 2013 b ) and Structured LSTM ( Tai et al. , 2015 ; Zhu et al. , 2015 ) can be used as composition algorithms .
However , existing discourse structure learning algorithms are difficult to scale to massive review texts on the web .
How to simultaneously learn document structure and composition function is an interesting future work .
Figure 2 : 2 Figure 2 : Sentence composition with convolutional neural network .
