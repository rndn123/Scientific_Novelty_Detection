title
SwissCheese at SemEval-2016 Task 4 : Sentiment Classification Using an Ensemble of Convolutional Neural Networks with Distant Supervision
abstract
In this paper , we propose a classifier for predicting message - level sentiments of English micro-blog messages from Twitter .
Our method builds upon the convolutional sentence embedding approach proposed by ( Severyn and Moschitti , 2015a ; Severyn and Moschitti , 2015 b ) .
We leverage large amounts of data with distant supervision to train an ensemble of 2 - layer convolutional neural networks whose predictions are combined using a random forest classifier .
Our approach was evaluated on the datasets of the SemEval - 2016 competition ( Task 4 ) outperforming all other approaches for the Message Polarity Classification task .
Introduction Sentiment analysis is a fundamental problem aiming to give a machine the ability to understand the emotions and opinions expressed in a written text .
This is an extremely challenging task due to the complexity of human language , which makes use of rhetorical devices such as sarcasm or irony .
Deep neural networks have shown great promises at capturing salient features for these complex tasks ( Mikolov et al. , 2013 b ; Severyn and Moschitti , 2015a ) .
Particularly successful for sentiment classification were Convolutional Neural Networks ( CNN ) ( Kim , 2014 ; Kalchbrenner et al. , 2014 ; Severyn and Moschitti , 2015a ; Severyn and Moschitti , 2015 b ; Johnson and Zhang , 2015 ) , on which our work builds upon . *
These authors contributed equally to this work
These networks typically have a large number of parameters and are especially effective when trained on large amounts of data .
In this work , we use a distant supervision approach to leverage large amounts of data in order to train a 2 - layer CNN 1 , extending the 1 - layer architecture proposed by ( Severyn and Moschitti , 2015a ) .
More specifically , we train a neural network using the following three - phase procedure : i ) creation of word embeddings for initialization of the first layer ; ii ) distant supervised phase , where the network weights and word embeddings are trained to capture aspects related to sentiment ; and iii ) supervised phase , where the network is trained on the provided supervised training data .
We also combine the predictions of several neural networks using a random forest meta-classifier .
The proposed approach was evaluated on the datasets of the SemEval - 2016 competition , Task 4 ( Nakov et al. , 2016 ) 2 for which it reaches state - of - the - art results .
System Description
Convolutional Neural Networks
We combine the outputs of two 2 - layer CNNs having similar architectures but differing in the choice of certain parameters ( such as the number of convolutional filters ) .
These two networks were also initialized using different word embeddings and used slightly different training data for the distant supervised phase .
The common architecture of the two CNNs is shown in Figure 1 and described in details below .
Sentence model .
Each word is associated to a vector representation , which consists in a d-dimensional vector .
A sentence ( or tweet ) is represented by the concatenation of the representations of its n constituent words .
This yields a matrix X ? R d?n , which is used as input to the convolutional neural network .
Convolutional layer .
In this layer , a set of m filters is applied to a sliding window of length h over each sentence .
Let X [ i:i+h ] denote the concatenation of word vectors x i to x i+h .
A feature c i is generated for a given filter F by : c i := k , j ( X [ i:i+h ] ) k , j ? F k , j ( 1 ) A concatenation of all vectors in a sentence produces a feature vector c ? R n?h +1 .
The vectors c are then aggregated over all m filters into a feature map matrix C ? R m? ( n?h + 1 ) .
The filters are learned during the training phase of the neural network using a procedure detailed in the next section .
Max pooling .
The output of the convolutional layer is passed through a non-linear activation function , before entering a pooling layer .
The latter aggregates vector elements by taking the maximum over a fixed set of non-overlapping intervals .
The resulting pooled feature map matrix has the form : C pooled ?
R m? n?h+1 s , where s is the length of each interval .
In the case of overlapping intervals with a stride value s t , the pooled feature map matrix has the form C pooled ?
R where w j denotes the weights vector of class j and a j the bias of class j .
Network parameters .
Training the neural network consists in learning the set of parameters ? = { X , F 1 , b 1 , F 2 , b 2 , W , a} , where X is the sentence matrix , with each row containing the d-dimensional embedding vector for a specific word ; F i , b i ( i = { 1 , 2 } ) the filter weights and biases of the first and second convolutional layers ; W the concatenation of the weights w j for every output class in the softmax layer ; and a the bias of the soft-max layer .
Ensemble of classifiers
We combine the results of the two 2 - layer CNN described in Section 2.1 with the intent of increasing the generalizability of the final classifier .
This is achieved relying on two systems trained using different procedures as well as different word embeddings .
The network parameters of the two CNNs are summarized in Table 1 .
The preprocessing and training phases of the two systems are described below .
System I Preprocessing and word embeddings .
The word embeddings are initialized using word2vec ( Mikolov et al. , 2013a ) and then trained using an unlabelled corpus of 200M tweets .
We apply a skipgram model of window size 5 and filter words that occur less than 5 times ( Severyn and Moschitti , 2015 b ) .
The dimensionality of the vector representation is set to d = 52 .
Training .
During a first distant - supervised phase , we use emoticons to infer the polarity of a balanced set of 90M tweets ( Read , 2005 ; Go et al. , 2009 ) .
The resulting dataset contains 45 M tweets for both the positive and negative class .
The neural network is trained on these 90M tweets for one epoch , before training for 10 to 15 epochs on the labelled data provided by SemEval - 2016 .
The word-embeddings X ? R d?n , are updated during both the distant and the supervised training phases , as back - propagation is applied through the entire network .
System II Preprocessing and word embeddings .
A corpus of 90M tweets 3 ( 30 M contain positive emoticons , 30 M negative ones and 30 M contain none ) is employed to create embedding vectors of d = 50 dimensions using GloVe ( Pennington et al. , 2014 ) .
Words which appear less than 5 times are discarded .
Additionally , special flags ? { 0 , 1 } are assigned to some words , by appending a flag vector to their word embeddings .
Four different flags can be set , marking ( i ) words that belong to hashtags , ( ii ) words that have been elongated ( e.g. ' hellooo ' , which is mapped to the same vector as ' hello ' ) , ( iii ) words in which all characters are capitalized , and ( iv ) punctuations that are repeated more than three times ( e.g. '!!!!' and ' !!!' being mapped to the same vector ) .
Training .
In the distant supervised phase , the network is trained for one epoch on a set of 60 M tweets , containing an equal amount of samples with positive and negative emoticons .
Similarly to System I , this pre-trained network is further refined by supervised training for about 15 epochs on the SemEval - 2016 data .
We apply L 2 regularization to reduce overfitting to the cost function ( negative log likelihood ) by adding a penalty of the form of ? ? 2 2 , with regularization strength 4 ? , where ? ? ? are the network parameters of each layer .
Optimization
The network parameters are learned using AdaDelta ( Zeiler , 2012 ) , which adapts the learning rate for each dimension using only first order information .
We used the hyper-parameters = 1e?6 and ? = 0.95 as suggested by ( Zeiler , 2012 ) .
Meta-Classifier
Each aforementioned system outputs three real values ? corresponding to the three sentiment classes .
In addition , it outputs the categorical value for the predicted sentiment class .
The meta-classifier uses these values ( sentiment class and categorical value of systems I and II ) as input features .
We trained a random forest using the Weka ( Hall et al. , 2009 ) library on the training data .
We selected the number of trees ( 300 ) , maximum depth of the forest ( 2 ) and the number of features used per random selection ( 18 ) as to obtain the best overall performance over the previous years ' test sets .
Computing Resources
The core routines are written in Python , making heavy use of mathematical routines in Theano ( Bergstra et al. , 2010 ) that exploits GPU acceleration .
For further performance improvement , we used the CuDNN library ( Chetlur and Woolley , 2014 ) .
The framework requires approximately 10 hours for the distant supervised phase and only 20 - 30 minutes for the supervised phase .
Experiments were conducted on g2.2 xlarge instances of Amazon Web Services ( AWS ) , which offer a GRID K520 GPU with 3072 CUDA cores and 8 GB of GDDR5 RAM .
Data
The training and development datasets used in our experiments were provided by the SemEval - 2016 competition .
A fraction of the tweets ( 10 - 15 % ) from the period 2013 - 2015 were no longer available on Twitter , which made the results of this year competition not directly comparable to the ones of previous years .
For testing , in addition to last year 's data ( tweets , SMS , LiveJournal ) , new tweets were accessible .
An overview of the data available for download is given in Table 2 . Data preparation .
Before extracting features , the tweets were preprocessed using the following procedure : ? URLs and usernames were substituted by a replacement token ?
The text was lowercased ?
The NLTK twitter tokenizer was employed in System I and a customized version of the CMU ARK Twitter Part-of - Speech Tagger ( Gimpel et al. , 2011 ) in System II .
Results
The F- 1 score was computed by the competition organizers as evaluation measure .
As a result , the presented system was ranked 1 st out of 34 participants , with an F1 - score of 63.30 on the Twitter - 2016 test set .
See ( Nakov et al. , 2016 ) for further details .
Table 4 summarizes the results of individual subsystems , as well as the final system on each test set .
For each test set the best score is marked in bold face .
In case of the Twitter 2016 and Twitter 2015 test sets we marked the best performing subsystem in italics .
For System I ( S1 ) , we observed that during the supervised phase , the F - 1 scores measured on the different test sets presented large deviations .
Hence , to improve robustness , we considered six different models of S1 ( S1a , ... , S1 f ) , varying the number of epochs between 12 and 25 during the supervised phase , stopping determined by the validation score on different sets .
For System II ( S2 ) , the number of epochs , equal to 12 , is determined by the one achieving the highest score on the DevTest2016 set .
The final system ( FS ) using the meta-classifier trained on the outputs of systems S1 a -f and S2 , achieves the highest accuracy on the 2016 test set with the competition - winning F1 - score of 63.30 % .
These results improve the score of the best performing subsystem ( S1 b ) by 0.57 points .
For the 2015 test set , FS shows an improvement of 0.35 points with respect to the score of the best subsystem ( S1 f ) .
Table 3 : Overall results of the proposed subsystems .
S1 : System ( s ) I ; S2 : System II ; FS : Final system , using the metaclassifier .
Best ( second- best ) results are highlighted in bold ( underlined ) face .
Conclusion
We described a deep learning framework to predict the sentiment polarity of short phrases , such as tweets .
The proposed approach is based on an ensemble of Convolution Neural Networks and relies on a significantly large amount of data for the distant - supervised phase .
The final random forest classifier resulted in state - of - the - art performance , ranking 1 st in the SemEval - 2016 competition for the task of Message Polarity Classification .
Figure 1 : 1 Figure 1 : The architecture of the CNNs used in our approach .
