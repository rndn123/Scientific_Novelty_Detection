title
Dimensional Sentiment Analysis Using a Regional CNN-LSTM
Model
abstract
Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal ( VA ) space .
Compared to the categorical approach that focuses on sentiment classification such as binary classification ( i.e. , positive and negative ) , the dimensional approach can provide more fine - grained sentiment analysis .
This study proposes a regional CNN - LSTM model consisting of two parts : regional CNN and LSTM to predict the VA ratings of texts .
Unlike a conventional CNN which considers a whole text as input , the proposed regional CNN uses an individual sentence as a region , dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction .
Such regional information is sequentially integrated across regions using LSTM for VA prediction .
By combining the regional CNN and LSTM , both local ( regional ) information within sentences and long-distance dependency across sentences can be considered in the prediction process .
Experimental results show that the proposed method outperforms lexicon- based , regression - based , and NN - based methods proposed in previous studies .
Introduction Sentiment analysis has been useful in the development of online applications for customer reviews and public opinion analysis ( Pang and Lee 2008 ; Calvo and D'Mello 2010 ; Liu 2012 ; Feldman 2013 ) .
In sentiment representation , the cate-gorical approach represents emotional states as several discrete classes such as binary ( i.e. , positive and negative ) or as multiple categories such as Ekman 's ( 1992 ) six basic emotions ( anger , happiness , fear , sadness , disgust , and surprise ) .
Classification algorithms can then be used to identify sentiment categories from texts .
The dimensional approach represents emotional states as continuous numerical values in multiple dimensions such as the valence -arousal ( VA ) space ( Russell , 1980 ) .
The dimension of valence refers to the degree of positive and negative sentiment , whereas the dimension of arousal refers to the degree of calm and excitement .
Both dimensions range from 1 ( highly negative or calm ) to 9 ( highly positive or excited ) based on the self-assessment manikin ( SAM ) annotation scheme ( Bradley et al. 1994 ) .
For example , the following passage consisting of three sentences is associated with a valence - arousal rating of ( 2.5 , 7.8 ) , which displays a high degree of negativity and arousal .
Such high- arousal negative ( or high - arousal positive ) texts are usually of interest and could prioritized in product review systems .
Dimensional sentiment analysis can accomplish this by recognizing the VA ratings of texts and rank them accordingly , thus providing more intelligent and fine- grained sentiment applications .
Research on dimensional sentiment analysis has addressed VA recognition at both the wordlevel ( Wei et al. , 2011 ; Malandrakis et al. , 2011 ; Yu et al. , 2015 ) and the sentence - level ( Paltoglou et al. , 2013 ; Malandrakis et al. , 2013 ) .
At the word- level , Wei et al . ( 2011 ) used linear regression to transfer VA ratings from English affective words to Chinese words .
Malandrakis et al. ( 2011 ) used a kernel function to combine the similarity between words for VA prediction .
Yu et al . ( 2015 ) used a weighted graph model to iteratively determine the VA ratings of affective words .
At the sentence level , Paltoglou et al . ( 2013 ) adopted a lexicon - based method to calculate the VA ratings of texts by averaging the VA ratings of affective words in the texts using a weighted arithmetic / geometric mean .
Malandrakis et al. ( 2013 ) proposed a regression method that extracted n-gram with affective ratings as features to predict VA values for texts .
Recently , word embedding ( Mikolov et al. , 2013a ; Mikolov et al. , 2013 b ) and deep neural networks ( NN ) such as convolutional neural networks ( CNN ) ( Kim , 2014 ; Kalchbrenner et al. , 2014 ) , recurrent neural networks ( RNN ) ( Graves , 2012 ; Irsoy and Cardie , 2014 ) and long shortterm memory ( LSTM ) have been successfully employed for categorical sentiment analysis .
In general , CNN is capable of extracting local information but may fail to capture long-distance dependency .
LSTM can address this limitation by sequentially modeling texts across sentences .
Such NN - based and word embedding methods have not been well explored for dimensional sentiment analysis .
This study proposes a regional CNN - LSTM model consisting of two parts , regional CNN and LSTM , to predict the VA ratings of texts .
We first construct word vectors for vocabulary words using word embedding .
The regional CNN is then used to build text vectors for the given texts being predicted based on the word vectors .
Unlike a conventional CNN which considers a whole text as input , the proposed regional CNN uses individual sentences as regions , dividing an input text into several regions such that the useful affective information in different regions can be extracted and weighted according to their contribution to the VA prediction .
For example , in the aforementioned example text , it would be useful for the system to emphasize the two sentences / regions ( r2 ) and ( r3 ) containing negative affective information .
Finally , such regional information is sequentially integrated across regions using LSTM for VA prediction .
By combining the regional CNN and LSTM , both local ( regional ) information within sentences and longdistance dependency across sentences can be considered in the prediction process .
The rest of this paper is organized as follows .
Section 2 describes the proposed regional CNN - LSTM model .
Section 3 reports the evaluation results of the proposed method against lexiconbased , regression - based , and NN - based methods .
Conclusions are finally drawn in Section 4 .
Regional CNN-LSTM Model Figure 1 shows the overall framework of the proposed regional CNN - LSTM model .
First , the word vectors of vocabulary words are trained from a large corpus using the word2vec toolkit .
For each given text , the regional CNN model uses a sentence as a region to divide the given text into R regions , i.e. r1 , ? , ri , rj , rk , ? , rR .
In each region , useful affective features can be extracted once the word vectors sequentially pass through a convolutional layer and max pooling layer .
Such local ( regional ) features are then sequentially integrated across regions using LSTM to build a text vector for VA prediction .
Convolutional Layer
In each region , a convolutional layer is first used to extract local n-gram features .
All word embeddings are stacked in a region matrix d V M ? ? ? , where | V | is the vocabulary size of a region , and d is the dimensionality of word vectors .
For example , in Fig .
1 l l l n n n y f W b ? + ? = + x ? ( 1 ) where ? is a convolutional operator , Given varying text lengths in the regions , y l may have different dimensions for different texts .
Therefore , we define the maximum length of the CNN input in the corpora as the dimension N .
If the input length is shorter than N , then several random vectors with a uniform distribution U ( - 0.25 , 0.25 ) will be appended .
Max-pooling Layer Max-pooling subsamples the output of the convolutional layer .
The most common way to do pooling it to apply a max operation to the result of each filter .
There are two reasons to use a max-pooling layer here .
First , by eliminating non-maximal values , it reduces computation for upper layers .
Second , it can extract the local dependency within different regions to keep the most salient information .
The obtained region vectors are then fed to a sequential layer .
Sequential Layer
To capture long-distance dependency across regions , the sequential layer sequentially integrates each region vector into a text vector .
Due to the problem of gradients vanishing or exploding in RNN ( Bengio et al. , 1994 ) , LSTM is introduced in the sequential layer for vector composition .
After the LSTM memory cell sequentially traverses through all regions , the last hidden state of the sequential layer is regarded as the text representation for VA prediction .
Linear Decoder
Since the values in both the valence and arousal dimensions are continuous , the VA prediction task requires a regression .
Instead of using a softmax classifier , a linear activation function ( also known as a linear decoder ) is used in the output layer , defined as , d t d y W b = + x ( 2 ) where xt is the text vector learned from the sequential layer , y is the degree of valence or arousal of the target text , and Wd and bd respectively denote the weight and bias associated with the linear decoder .
The regional CNN - LSTM model is trained by minimizing the mean squared error between the predicted y and actual y. Given a training set of text matrix X= {x ( 1 ) , x ( 2 ) , ? , x ( m ) } , and their VA ratings set y={y ( 1 ) , y ( 2 ) , ? , y ( m ) } , the loss function is defined as 2 ( ) ( ) 1 1 ( , ) ( ) 2 m i i i L h y m = = ? ? X y x ( 3 ) In the training phase , a back propagation ( BP ) algorithm with stochastic gradient descent ( SGD ) is used to learn model parameters .
Details of the BP algorithm can be found in ( LeCun et al. , 2012 ) .
Experiments
This section evaluates the performance of the proposed regional CNN - LSTM model against lexicon- based , regression - based , and NN - based methods .
Datasets .
This experiment used two affective corpora .
i ) Stanford Sentiment Treebank ( SST ) ( Socher et al. , 2013 ) contains 8,544 training texts , 2,210 test texts , and 1,101 validation texts .
Each text was rated with a single dimension ( valence ) in the range of ( 0 , 1 ) .
ii ) Chinese Valence -Arousal Texts ( CVAT ) ( Yu et al. , 2016 ) consists of 2,009 texts collected from social forums , manually rated with both valence and arousal dimensions in the range of ( 1 , 9 ) using the SAM annotation scheme ( Bradley et al. 1994 ) .
The word vectors for English and Chinese were respectively trained using the Google News and Chinese wiki dumps ( zhwiki ) datasets .
The dimensionality for both word vectors are 300 .
Experimental Settings .
Two lexicon- based methods were used for comparison : weighted arithmetic mean ( wAM ) and weighted geometric mean ( wGM ) ( Paltoglou et al. , 2013 ) , along with two regression - based methods : average values regression ( AVR ) and maximum values regression ( MVR ) ( Malandrakis et al. , 2013 ) .
The valence ratings of English and Chinese words were respectively taken from the Extended ANEW ( Warriner et al. , 2013 ) and Chinese Valence -Arousal Words ( CVAW ) lexicons ( Yu et al. , 2016 ) .
A conventional CNN , RNN and LSTM were also implemented for comparison .
Metrics .
Performance was evaluated using the root mean square error ( RMSE ) , mean absolute error ( MAE ) , and Pearson correlation coefficient ( r ) , defined as ?
Root mean square error ( RMSE ) ( ) 2 1 n i i i RMSE A P n = = ? ? ( 4 ) ? Mean absolute error ( MAE ) 1 1 | | n i i i MAE A P n = = ? ? ( 5 ) ? Pearson correlation coefficient ( r) 1 1 ( ) ( )
1 n i i i A P A A P P r n ? ? = ? ? = ? ? ( 6 ) where Ai is the actual value , Pi is the predicted value , n is the number of test samples , A and P respectively denote the arithmetic mean of A and P , and ? is the standard deviation .
A lower RMSE or MAE and a higher r value indicates better prediction performance .
A t-test was used to determine whether the performance difference was statistically significant .
Comparative Results .
Tables 1 and 2 respectively present the comparative results of the regional CNN - LSTM against several methods for VA prediction of texts in both English and Chinese corpora .
For the lexicon- based methods , wGM outperformed wAM , which is consistent with the results presented in ( Paltoglou et al. , 2013 ) .
Instead of using the VA ratings of words to directly measure those of texts , the regressionbased methods learned the correlations between the VA ratings of words and texts , thus yielding better performance .
Once the word embedding and deep learning techniques were introduced , the performance of NN - based methods ( except RNN ) jumped dramatically .
In addition , the proposed regional CNN - LSTM outperformed the other NN - based methods , indicating the effectiveness of sequentially integrating the regional information across regions .
Another observation is that the Pearson correlation coefficient of prediction in arousal is lower than that for the valence prediction , indicating that arousal is more difficult to predict .
SST ( English
Conclusion
This study presents a regional CNN - LSTM model to predict the VA ratings of texts .
By capturing both local ( regional ) information within sentences and long-distance dependency across sentences , the proposed method outperformed regression - and conventional NN - based methods presented in previous studies .
Future work will focus on the use of a parser to identify regions so the structural information can be further incorporated to improve the prediction performance . ( r1 ) A few days ago I checked into a franchise hotel .
( r2 )
The front desk service was terrible , and they did n't know much about local attractions . ( r3 ) I would not recommend this hotel to a friend .
