title
LyS : Porting a Twitter Sentiment Analysis Approach from Spanish to English
abstract
This paper proposes an approach to solve message - and phrase -level polarity classification in Twitter , derived from an existing system designed for Spanish .
As a first step , an ad-hoc preprocessing is performed .
We then identify lexical , psychological and semantic features in order to capture different dimensions of the human language which are helpful to detect sentiment .
These features are used to feed a supervised classifier after applying an information gain filter , to discriminate irrelevant features .
The system is evaluated on the SemEval 2014 task 9 : Sentiment Analysis in Twitter .
Our approach worked competitively both in message - and phraselevel tasks .
The results confirm the robustness of the approach , which performed well on different domains involving short informal texts .
Introduction Millions of opinions , conversations or just trivia are published each day in Twitter by users of different cultures , countries and ages .
This provides an effective way to poll how people praise , complain or discuss about virtually any topic .
Comprehending and analysing all this information has become a new challenge for organisations and companies , which aim to find out a way to make quick and more effective decisions for their business .
In particular , identifying the perception of the public with respect to an event , a service or an entity are some of their main goals in a short term .
In this respect , sentiment analysis , and more specifically polarity classification , is playing an important role
This work is licensed under a Creative Commons Attribution 4.0 International Licence .
Page numbers and proceedings footer are added by the organisers .
Licence details : http://creativecommons.org/licenses/by/4.0/ in order to automatically analyse subjective information in texts .
This paper describes our participation at Sem-Eval 2014 task 9 : Sentiment Analysis in Twitter .
Specifically , two subtasks were presented : ( A ) contextual polarity disambiguation and ( B ) message polarity classification .
The first subtask consists on determining the polarity of words or phrases extracted from short informal texts , the scope of extracts being provided by the Se-mEval organisation .
Subtask B focusses on classifying the content of the whole message .
In both cases , three possible sentiments are considered : positive , negative and neutral ( which involves mixed and non-opinionated instances ) .
Although the training set only contains tweets , the test set also includes short informal texts from other domains , in order to measure cross-domain portability .
You can test the model for subtask B at miopia.grupolys.org .
SemEval 2014 - Task 9 : Sentiment Analysis in Twitter
Our contribution is a reduced version of a Spanish sentiment classification system ( Vilares et al. , 2013a ; Vilares et al. , 2013 b ) that participated in TASS 2013 ( Villena - Rom ? n et al. , 2014 ) , achieving the 5th place on the global sentiment classification task and the 1st place on topic classification on tweets .
In this section we describe how we have ported to English this system originally designed for Spanish .
Tasks
A and B are addressed from the same perspective , which is described below .
Preprocessing
We implement a naive preprocessing algorithm which seeks to normalise some of the most common ungrammatical elements .
It is intended for Twitter , but many of the issues addressed would also be valid in other domains : ?
Replacement of frequent abbreviations
The list of the most frequent ones was extracted from the training set , taking the Penn Treebank ( Marcus et al. , 1993 ) as our dictionary .
A term is considered ungrammatical if it does not appear in our dictionary .
We then carry out a manual review to distinguish between unknown words and abbreviations , providing a correction in the latter case .
For example , ' c' mon ' becomes ' come on ' and ' Sat ' is replaced by ' Saturday ' .
?
Emoticon normalisation :
We employ the emoticon collection published in ( Agarwal et al. , 2011 ) .
Each emoticon is replaced with one of these five labels : strong positive ( ESP ) , positive ( EP ) , neutral ( ENEU ) , negative ( EN ) or strong negative ( ESN ) .
?
Laughs : Multiple forms used in social media to reflect laughs ( e.g. ' hhahahha ' , ' HHEHE - HEH ' ) are preprocessed in a homogeneous way to obtain a pattern of the form ' hxhx ' where x ? { a , e , i , o , u} . ? URL normalisation : External links are replaced by the string ' url ' .
? Hashtags ( '# ' ) and usernames ( ' @ ' ) :
If the hashtag appears at the end or beginning of the tweet , we remove the hashtag .
Based on other participant approaches at SemEval 2013 ( Nakov et al. , 2013 ) , we realized maybe this is not the best option , although we believe hashtags will not be useful in most of cases , since they refer to very specific events .
Otherwise , only the '# ' is removed , hypothesising the hashtag is used to emphasise a term ( e.g. ' Matthew # Mcconaughey has won the Oscar ' ) .
Feature Extraction
Our approach only takes into account information extracted from the text , without considering any kind of meta-data .
Extracted features combine lexical , psychological and semantic knowledge in order to build a linguistic model able to analyse tweets , but also other kinds of messages .
These features can be divided into two types : corpusextracted features and lexicon-extracted features .
All of them take the total number of occurrences of the respective feature as the weighting factor to then feed the supervised classifier .
Corpus-extracted features
Given a corpus , we use it to extract the following set of features : ?
Word forms : A model based on this type of features is our baseline .
Each single word is considered as a feature in order to feed the supervised classifier .
This often becomes a simple and acceptable start point which obtains a decent performance .
?
Part-of-speech ( PoS ) information : some coarse- grained PoS- tags such as adjective or adverb are usually good indicators of subjective texts while some fine- grained PoS tags such as third person personal pronoun provide evidence of non-opinionated messages ( Pak and Paroubek , 2010 ) .
Lexicon-extracted features
We also consider information obtained from external lexicons in order to capture linguistic information that can not be extracted from a training corpus by means of bag-of-words and PoS- tag models .
We rely on two manually - build lexicons : ?
Pennebaker et al. ( 2001 ) psychometric dictionaries .
Linguistic Inquiry and Word Count 1 ( LIWC ) is a software which includes a semantic dictionary to measure how people use different kinds of words over a wide number of texts .
It categorises terms into psychometric properties , which correspond to different dimensions of the human language .
The dictionary relates terms with psychological properties ( e.g. anger or anxiety ) , but also with topics ( e.g. family , friends , religion ) or even morphological features ( e.g. future time , past time or exclamations ) .
?
Hu and Liu ( 2004 ) opinion lexicon .
It is a collection of positive and negative words .
Many of the occurrences are misspelled , since they often come from web environments .
Syntactic features
We also parsed the tweets using MaltParser ( Nivre et al. , 2007 ) in order to obtain dependency triplets of the form ( w i , arc ij , w j ) , where w i is the head word w j , the dependent one and arc ij the existing syntactic relation between them .
We tried to incorporate generalised dependency triplets ( Joshi and Penstein-Ros ? , 2009 ) , following an enriched perspective presented in Vilares et al . ( 2014 ) .
A generalisation consists on backing off the words to more abstracted terms .
For example , a valid dependency triplet for the phrase ' awesome villain ' is ( villain , modifier , awesome ) , which could be generalised into ( anger , modifier , assent ) by means of psychometric properties .
However , experimental results over the development corpus using these features decreased performance with respect to our best model , probably due to the small size of the training corpus , since dependency triplets tend to suffer from sparsity , so a larger training corpus is needed to exploit them in a proper way ( Vilares et al. , 2014 ) .
Feature Selection
For a machine learning approach , sparsity could be an issue .
In particular , due to the size of the corpus , many of the terms extracted from the training set only appear a few times in it .
This makes it impossible to properly learn the polarity of many tokens .
Thus , we carry out a filtering step before feeding our classifier .
In particular , we rely on the information gain ( IG ) method to then rank the most relevant features .
Information gain measures the relevance of an attribute with respect to a class .
It takes values between 0 and 1 , where a higher value implies a higher relevance .
Classifier
We have trained our runs with a SVM LibLINEAR classifier ( Fan et al. , 2008 ) taking the implementation provided in WEKA ( Hall et al. , 2009 ) .
The selection was motivated by the acceptable results that some of the participants in SemEval 2013 , e.g. Becker et al. ( 2013 ) , obtained using this imple-mentation .
We configured the multi-class support vector machine by Crammer and Singer ( 2002 ) as the SVMtype .
Since the corpus was unbalanced , we tuned the weights for the classes using the development corpus : 1 for the positive class , 2 for negative and 0.5 for neutral .
The rest of parameters were set to default values .
Experimental Results
The SemEval 2014 organisation provides a standard training corpus for both tasks A an B .
For task A , each tweet is marked with a list of the words and phrases to analyse , and for each one its sentiment label is provided .
In addition , a development corpus was released for tuning the system parameters .
The training and the development corpus can be used jointly ( constrained runs ) to train models that are then evaluated over the test corpus .
2 Some participants used external annotated corpora ( unconstrained runs ) to build their models .
With respect to the test corpus , it contains texts from tweets but also from LiveJournal texts , which we are abbreviating as LJ , and SMS messages .
Table 2 contains the statistics of the corpora we used .
Sharing data is a violation of Twitter 's terms of service , so we had to download them .
Unfortunately , some of the tweets were no longer available for several reasons , e.g. , user or a tweet does not exist anymore or the privacy settings of a user have changed .
As a result , the size of our training and development corpora may be different from those of other participant 's corpora .
Evaluation Metrics F-measure is the official score to measure how systems behave on each class .
In order to rank participants , the SemEval 2014 organisation proposed the averaged F-measure of positive and negative tweets .
Performance on Sets Tables 3 and 4 show performance on the test set of different combinations of the proposed features .
Table 5 shows the performance of our run on task A .
The results over the corresponding sets for task B are illustrated in Table 6 .
They are significant lower than in task A .
This suggests that when a message involves more than one of two tokens , a lexical approach is not enough .
Improving performance should involve taking into account context and linguistic phenomena that appear in sentences to build a model based on the composition of linguistic information .
Conclusions
This papers describes the participation of the LyS Research Group ( http://www.grupolys. org ) at the SemEval 2014 task 9 : Sentiment Analysis in Twitter , with a system that attained competitive performance both in message and phrase - level tasks , as can be observed in Table 7 .
This system is a reduced version of a sentiment classification model for Spanish texts that performed well in the TASS ( Villena et al. , 2013 .
The official results show how our approach works competitively both on tasks A and B without needing large and automatically - built resources .
The approach is based on a bag-of-words that includes word-forms and PoS-tags .
We also extract psychometric and sentiment information from external lexicons .
In order to reduce sparsity problems , we firstly apply an information gain filter to select only the most relevant features .
Experiments on the development set showed a significant improvement on the same model with respect to skipping it on subtask B. Table 1 1 shows
Table 1 : 1 Most relevant features for task B. '# ' must be read this table as ' the number of ' and not as a hashtag .
Table 2 : 2 SemEval 2014 corpus statistics .
Table 3 : 3 Performance on the test set for task A .
The model marked with a * was our official run .
W stands for features obtained from a bag-of-words approach , L from Hu and Liu ( 2004 ) , P from Pennebaker et al. ( 2001 ) and T for fine- grained PoStags .
They can be combined , e.g. , a model named WP use both words and psychometric properties .
Model LJ SMS Twitter Twitter 2013 2014 Twitter Sarcasm WPLT ( no IG ) 82.21 82.32 84.82 81.69 71.19 WPL 83.55 81.04 84.85 80.64 68.79 WPLT * 83.96 81.46 85.63 79.93 71.98 WP 78.53 80.97 80.34 73.35 74.18 P 75.70 78.74 73.58 65.75 71.82 W 61.58 65.45 64.56 59.16 62.93 L 66.04 64.11 62.96 53.81 61.26 T 47.07 51.37 71.82 43.64 49.37 Model LJ SMS Twitter Twitter 2013 2014 Twitter Sarcasm WPLT * 69.79 60.45 66.92 64.92 42.40 WPL 70.19 61.41 66.71 64.51 45.72 WP 66.84 60.22 65.29 63.90 45.90 WPLT ( no IG ) 66.38 57.01 61.96 62.84 43.71 W 65.12 56.00 62.87 62.64 48.75 P 63.42 54.80 60.05 57.66 54.20 T 45.99 35.85 46.53 45.99 48.58 L 57.53 45.14 48.80 44.48 49.14
Table 4 : 4 Performance on the test set for task B .
Table 5 : 5 Performance on different sets for our model on task A .
The model evaluated on the development set was only built using the training set .
Test set Positive Negative Neutral DEV 86.30 81.60 4.30 TWITTER 2013 ( full ) 88.70 81.90 17.60 TWITTER 2013 ( progress subset ) 88.81 82.57 20.75 LJ 84.34 83.56 13.84 SMS 80.31 82.56 7.10 TWITTER 2014 89.02 70.82 4.44 TWITTER SARCASM 85.71 57.63 28.57
Test set Positive Negative Neutral DEV 69.80 60.40 66.70 TWITTER 2013 ( full ) 72.50 64.30 72.30 TWITTER 2013 ( progress subset )
71.92 61.92 71.22 LJ 71.94 67.65 66.23 SMS 63.83 57.06 73.76 TWITTER 2014 74.26 55.58 66.76 TWITTER SARCASM 55.17 29.63 51.61
Table 6 : 6 Performance on different sets for our model on task B. Test set Task A Task B LiveJournal 2014 4 / 27 13 / 50 SMS 2013 12 / 27 19 / 50 Twitter 2013 9 / 27 10 / 50 Twitter 2014 11 / 27 18 / 50 Twitter 2014 Sarcasm 10 / 27 33 / 50
Table 7 : 7 Position of our submission on each corpus and task , according to results provided by the organization on April 22 , 2014 .
http://www.liwc.net/
We followed this angle .
