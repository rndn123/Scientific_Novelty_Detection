title
Connotation Lexicon : A Dash of Sentiment Beneath the Surface Meaning
abstract
Understanding the connotation of words plays an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text , as seemingly objective statements often allude nuanced sentiment of the writer , and even purposefully conjure emotion from the readers ' minds .
The focus of this paper is drawing nuanced , connotative sentiments from even those words that are objective on the surface , such as " intelligence " , " human " , and " cheesecake " .
We propose induction algorithms encoding a diverse set of linguistic insights ( semantic prosody , distributional similarity , semantic parallelism of coordination ) and prior knowledge drawn from lexical resources , resulting in the first broad-coverage connotation lexicon .
Introduction
There has been a substantial body of research in sentiment analysis over the last decade ( Pang and Lee , 2008 ) , where a considerable amount of work has focused on recognizing sentiment that is generally explicit and pronounced rather than implied and subdued .
However in many real-world texts , even seemingly objective statements can be opinion - laden in that they often allude nuanced sentiment of the writer ( Greene and Resnik , 2009 ) , or purposefully conjure emotion from the readers ' minds ( Mohammad and Turney , 2010 ) .
Although some researchers have explored formal and statistical treatments of those implicit and implied sentiments ( e.g. Wiebe et al. ( 2005 ) , Esuli and Sebastiani ( 2006 ) , Greene and Resnik ( 2009 ) , Davidov et al. ( 2010 ) ) , automatic analysis of them largely remains as a big challenge .
In this paper , we concentrate on understanding the connotative sentiments of words , as they play an important role in interpreting subtle shades of sentiment beyond denotative or surface meaning of text .
For instance , consider the following : Geothermal replaces oil-heating ; it helps reducing greenhouse emissions .
1 Although this sentence could be considered as a factual statement from the general standpoint , the subtle effect of this sentence may not be entirely objective : this sentence is likely to have an influence on readers ' minds in regard to their opinion toward " geothermal " .
In order to sense the subtle overtone of sentiments , one needs to know that the word " emissions " has generally negative connotation , which geothermal reduces .
In fact , depending on the pragmatic contexts , it could be precisely the intention of the author to transfer his opinion into the readers ' minds .
The main contribution of this paper is a broadcoverage connotation lexicon that determines the connotative polarity of even those words with ever so subtle connotation beneath their surface meaning , such as " Literature " , " Mediterranean " , and " wine " .
Although there has been a number of previous work that constructed sentiment lexicons ( e.g. , Esuli and Sebastiani ( 2006 ) , Wilson et al . ( 2005 a ) , Kaji and Kitsuregawa ( 2007 ) , Qiu et al. ( 2009 ) ) , which seem to be increasingly and inevitably expanding over words with ( strongly ) connotative sentiments rather than explicit sentiments alone ( e.g. , " gun " ) , little prior work has directly tackled this problem of learning connotation , 2 and much of the subtle connotation of many seemingly objective words is yet to be determined .
1 Our learned lexicon correctly assigns negative polarity to emission .
2 A notable exception would be the work of Feng et al .
A central premise to our approach is that it is collocational statistics of words that affect and shape the polarity of connotation .
Indeed , the etymology of " connotation " is from the Latin " com - " ( " together or with " ) and " notare " ( " to mark " ) .
It is important to clarify , however , that we do not simply assume that words that collocate share the same polarity of connotation .
Although such an assumption played a key role in previous work for the analogous task of learning sentiment lexicon ( Velikovich et al. , 2010 ) , we expect that the same assumption would be less reliable in drawing subtle connotative sentiments of words .
As one example , the predicate " cure " , which has a positive connotation typically takes arguments with negative connotation , e.g. , " disease " , when used as the " relieve " sense .
3
Therefore , in order to attain a broad coverage lexicon while maintaining good precision , we guide the induction algorithm with multiple , carefully selected linguistic insights : [ 1 ] distributional similarity , [ 2 ] semantic parallelism of coordination , [ 3 ] selectional preference , and [ 4 ] semantic prosody ( e.g. , Sinclair ( 1991 ) , Louw ( 1993 ) , Stubbs ( 1995 ) , Stefanowitsch and Gries ( 2003 ) ) ) , and also exploit existing lexical resources as an additional inductive bias .
We cast the connotation lexicon induction task as a collective inference problem , and consider approaches based on three distinct types of algorithmic framework that have been shown successful for conventional sentiment lexicon induction : Random walk based on HITS / PageRank ( e.g. , Kleinberg ( 1999 ) Label / Graph propagation ( e.g. , Zhu and Ghahra - ( 2011 ) but with practical limitations .
See ?3 for detailed discussion .
3 Note that when " cure " is used as the " preserve " sense , it expects objects with non-negative connotation .
Hence wordsense- disambiguation ( WSD ) presents a challenge , though not unexpectedly .
In this work , we assume the general connotation of each word over statistically prevailing senses , leaving a more cautious handling of WSD as future work .
mani ( 2002 ) , Velikovich et al . ( 2010 ) ) Constraint optimization ( e.g. , Roth and Yih ( 2004 ) , Choi and Cardie ( 2009 ) , Lu et al . ( 2011 ) ) .
We provide comparative empirical results over several variants of these approaches with comprehensive evaluations including lexicon - based , human judgments , and extrinsic evaluations .
It is worthwhile to note that not all words have connotative meanings that are distinct from denotational meanings , and in some cases , it can be difficult to determine whether the overall sentiment is drawn from denotational or connotative meanings exclusively , or both .
Therefore , we encompass any sentiment from either type of meanings into the lexicon , where non-neutral polarity prevails over neutral one if some meanings lead to neutral while others to non-neutral .
4
Our work results in the first broad-coverage connotation lexicon , 5 significantly improving both the coverage and the precision of Feng et al . ( 2011 ) .
As an interesting by -product , our algorithm can be also used as a proxy to measure the general connotation of real-world named entities based on their collocational statistics .
Table 1 highlights some example proper nouns included in the final lexicon .
The rest of the paper is structured as follows .
In ?2 we describe three types of induction algorithms followed by evaluation in ?3 .
Then we revisit the induction algorithms based on constraint optimization in ?4 to enhance quality and scalability .
?5 presents comprehensive evaluation with human judges and extrinsic evaluations .
Related work and conclusion are in ?6 and ?7 .
Connotation Induction Algorithms
We develop induction algorithms based on three distinct types of algorithmic framework that have been shown successful for the analogous task of sentiment lexicon induction : HITS & PageRank ( ?2.1 ) , Label / Graph Propagation ( ?2.2 ) , and Constraint Optimization via Integer Linear Programming ( ?2.3 ) .
As will be shown , each of these approaches will incorporate additional , more diverse linguistic insights .
HITS & PageRank
The work of Feng et al . ( 2011 ) explored the use of HITS ( Kleinberg , 1999 ) and PageRank ( Page et al. , 1999 ) to induce the general connotation of words hinging on the linguistic phenomena of selectional preference and semantic prosody , i.e. , connotative predicates influencing the connotation of their arguments .
For example , the object of a negative connotative predicate " cure " is likely to have negative connotation , e.g. , " disease " or " cancer " .
The bipartite graph structure for this approach corresponds to the left-most box ( labeled as " pred-arg " ) in Figure 1 .
Label Propagation
With the goal of obtaining a broad-coverage lexicon in mind , we find that relying only on the structure of semantic prosody is limiting , due to relatively small sets of connotative predicates available .
6
Therefore , we extend the graph structure as an overlay of two sub-graphs ( Figure 1 ) as described below : Sub-graph # 1 : Predicate-Argument Graph
This sub-graph is the bipartite graph that encodes the selectional preference of connotative predicates over their arguments .
In this graph , connotative predicates p reside on one side of the graph and their co-occurring arguments a reside on the other side of the graph based on Google Web 1T corpus .
7
The weight on the edges between the predicates p and arguments a are defined using Point- wise Mutual Information ( PMI ) as follows : w( p ? a ) :=
P M I ( p , a ) = log 2 P ( p , a ) P ( p ) P ( a ) PMI scores have been widely used in previous studies to measure association between words ( e.g. , Turney ( 2001 ) , Church and Hanks ( 1990 ) ) .
Sub-graph # 2 : Argument-Argument Graph
The second sub-graph is based on the distributional similarities among the arguments .
One possible way of constructing such a graph is simply connecting all nodes and assign edge weights proportionate to the word association scores , such as PMI , or distributional similarity .
However , such a completely connected graph can be susceptible to propagating noise , and does not scale well over a very large set of vocabulary .
We therefore reduce the graph connectivity by exploiting semantic parallelism of coordination ( Bock ( 1986 ) ( 1997 ) , Pickering and Branigan ( 1998 ) ) .
In particular , we consider an undirected edge between a pair of arguments a 1 and a 2 only if they occurred together in the " a 1 and a 2 " or " a 2 and a 1 " coordination , and assign edge weights as : w( a1 ? a2 ) = CosineSim ( ? ? a1 , ? ? a2 ) = ? ? a1 ? ? ? a2 || ? ? a1 || || ? ? a2 || where ? ? a 1 and ? ? a 2 are co-occurrence vectors for a 1 and a 2 respectively .
The co-occurrence vector for each word is computed using PMI scores with respect to the top n co-occurring words .
8 n ( =50 ) is selected empirically .
The edge weights in two sub-graphs are normalized so that they are in the comparable range .
9
Limitations of Graph- based Algorithms
Although graph - based algorithms ( ?2.1 , ?2.2 ) provide an intuitive framework to incorporate various lexical relations , limitations include : 1 . They allow only non-negative edge weights .
Therefore , we can encode only positive ( supportive ) relations among words ( e.g. , distributionally similar words will endorse each other with the same polarity ) , while missing on exploiting negative relations ( e.g. , antonyms may drive each other into the opposite polarity ) .
2 . They induce positive and negative polarities in isolation via separate graphs .
However , we expect that a more effective algorithm should induce both polarities simultaneously .
Constraint Optimization Addressing limitations of graph- based algorithms ( ?2.2 ) , we propose an induction algorithm based on Integer Linear Programming ( ILP ) .
Figure 2 provides the pictorial overview .
In comparison to Figure 1 , two new components are : ( 1 ) dictionarydriven relations targeting enhanced precision , and ( 2 ) dictionary - driven words ( i.e. , unseen words with respect to those relations explored in Figure 1 ) targeting enhanced coverage .
We formulate insights in Figure 2 using ILP as follows : Definition of sets of words : 1 . P + : the set of positive seed predicates .
P ? : the set of negative seed predicates .
2 . S : the set of seed sentiment words .
Definition of variables :
For each word i , we define binary variables x i , y i , z i ? { 0 , 1 } , where x i = 1 ( y i = 1 , z i = 1 ) if and only if i has a positive ( negative , neutral ) connotation respectively .
For every pair of word i and j , we define binary variables d pq ij where p , q ? {+ , ? , 0 } and d pq ij = 1 if and only if the polarity of i and j are p and q respectively .
Objective function :
We aim to maximize : F = ? prosody + ? coord + ? neu where ?
prosody is the scores based on semantic prosody , ? coord captures the distributional similarity over coordination , and ? neu controls the sensitivity of connotation detection between positive ( negative ) and neutral .
In particular , ? prosody = R pred i , j w pred i , j ( d ++ i , j + d ? i , j ? d +? i , j ? d ?+ i , j ) ? coord = R coord i , j w coord i , j ( d ++ i , j + d ? i , j + d 00 i , j ) ? neu = ?
R pred i , j w pred i , j ? zj
Soft constraints ( edge weights ) :
The weights in the objective function are set as follows : w pred ( p , a ) = f req ( p , a ) ( p , x ) ?
R pred f req( p , x ) w coord ( a1 , a2 ) = CosSim ( ? ? a1 , ? ? a2 ) = ? ? a1 ? ? ? a2 || ? ? a1 || || ? ? a2 ||
Note that the same w coord ( a 1 , a 2 ) has been used in graph propagation described in Section 2.2 . ? controls the sensitivity of connotation detection such that higher value of ? will promote neutral connotation over polar ones .
Hard constrains for variable consistency : 1 . Each word i has one of { + , ? , ?} as polarity : ?i , x i + y i + z i = 1 2 .
Variable consistency between d pq ij and x i , y i , z i : x i + x j ?
1 ? 2d ++ i , j ? x i + x j y i + y j ?
1 ? 2d ? i , j ? y i + y j z i + z j ?
1 ? 2d 00 i , j ? z i + z j x i + y j ?
1 ? 2d +? i , j ? x i + y j y i + x j ? 1 ? 2d ?+ i , j ? y i + x j Hard constrains for WordNet relations : 1 . C ant : Antonym pairs will not have the same positive or negative polarity : ?( i , j ) ?
R ant , x i + x j ?
1 , y i + y j ?
1 For this constraint , we only consider antonym pairs that share the same root , e.g. , " sufficient " and " insufficient " , as those pairs are more likely to have the opposite polarities than pairs without sharing the same root , e.g. , " east " and " west " .
2 . C syn : Synonym pairs will not have the opposite polarity : ?( i , j ) ?
R syn , x i + y j ?
1 , x j + y i ?
1 3 Experimental Result I
We provide comprehensive comparisons over variants of three types of algorithms proposed in ?2 .
We use the Google Web 1T data ( Brants and Franz ( 2006 ) ) , and POS - tagged ngrams using Stanford POS Tagger ( Toutanova and Manning ( 2000 ) ) .
We filter out the ngrams with punctuations and other special characters to reduce the noise .
Comparison against Conventional Sentiment Lexicon
Note that we consider the connotation lexicon to be inclusive of a sentiment lexicon for two practical reasons : first , it is highly unlikely that any word with non-neutral sentiment ( i.e. , positive or negative ) would carry connotation of the opposite , i.e. , conflicting 10 polarity .
Second , for some words with distinct sentiment or strong connotation , it can be difficult or even unnatural to draw a precise distinction between connotation and sentiment , e.g. , " efficient " .
Therefore , sentiment lexicons can serve as a surrogate to measure a subset of connotation words induced by the algorithms , as shown in ranks based on the frequency of words for ILP .
Because of this issue , the performance of top ?1 k words of ILP should be considered only as a conservative measure .
Importantly , when evaluated over more than top 5 k words , ILP is overall the top performer considering both precision ( shown in Table 3 ) and coverage ( omitted for brevity ) .
12 4 Precision , Coverage , and Efficiency
In this section , we address three important aspects of an ideal induction algorithm : precision , coverage , and efficiency .
For brevity , the remainder of the paper will focus on the algorithms based on constraint optimization , as it turned out to be the most effective one from the empirical results in ?3 .
Precision
In order to see the effectiveness of the induction algorithms more sharply , we had used a limited set of seed words in ?3 .
However to build a lexicon with substantially enhanced precision , we will use as a large seed set as possible , e.g. , entire sentiment lexicons 13 . Broad coverage Although statistics in Google 1T corpus represent a very large amount of text , words that appear in pred-arg and coordination relations are still limited .
To substantially increase the coverage , we will leverage dictionary words ( that are not in the corpus ) as described in ?2.3 and Figure 2 . Efficiency
One practical problem with ILP is efficiency and scalability .
In particular , we found that it becomes nearly impractical to run the ILP formulation including all words in WordNet plus all words in the argument position in Google Web 1T .
We therefore explore an alternative approach based on Linear Programming in what follows .
Induction using Linear Programming One straightforward option for Linear Programming formulation may seem like using the same Integer Linear Programming formulation introduced in ?2.3 , only changing the variable definitions to be real values ? [ 0 , 1 ] rather than integers .
However , because the hard constraints in ?2.3 are defined based on the assumption that all the variables are binary integers , those constraints are not as meaningful when considered for real numbers .
Therefore we revise those hard constraints to encode various semantic relations ( WordNet and semantic coordination ) more directly .
Definition of variables :
For each word i , we define variables x i , y i , z i ? [ 0 , 1 ] .
i has a positive ( negative ) connotation if and only if the x i ( y i ) is assigned the greatest value among the three variables ; otherwise , i is neutral .
Objective function :
We aim to maximize : F = ? prosody + ? coord + ? syn + ? ant + ? neu ? prosody = R pred + i , j w pred + i , j ? xj + R pred ?
i , j w pred ?
i , j ? yj ? coord = R coord i , j w coord i , j ? ( dc ++ i , j + dc ? i , j ) ? syn = W syn R syn i , j ( ds ++ i , j + ds ? i , j ) ? ant = W ant R ant i , j ( da ++ i , j + da ? i , j ) ? neu = ?
R pred i , j w pred i , j ? zj Hard constraints
We add penalties to the objective function if the polarity of a pair of words is not consistent with its corresponding semantic relations .
For example , for synonyms i and j , we introduce a penalty W syn ( a positive constant ) for ds ++ i , j , ds ? i , j ? [ ?1 , 0 ] , where we set the upper bound of ds ++ i , j ( ds ? i , j ) as the signed distance of x i and x j ( y i and y j ) as shown below : For ( i , j ) ?
R syn , ds ++ i , j ? x i ? x j , ds ++ i , j ? x j ?
x i ds ?
i , j ? y i ?
y j , ds ?
i , j ? y j ?
y i Notice that ds ++ i , j , ds ?
i , j satisfying above inequalities will be always of negative values , hence in order to maximize the objective function , the LP solver will try to minimize the absolute values of ds ++ i , j , ds ?
i , j , effectively pushing i and j toward the same polarity .
Constraints for semantic coordination R coord can be defined similarly .
Lastly , following constraints encode antonym relations : For ( i , j ) ?
R ant , da ++ i , j ? x i ? ( 1 ? x j ) , da ++ i , j ? ( 1 ? x j ) ?
x i da ?
i , j ? y i ?
( 1 ? y j ) , da ? i , j ? ( 1 ? y j ) ?
y i Interpretation Unlike ILP , some of the variables result in fractional values .
We consider a word has positive or negative polarity only if the assignment indicates 1 for the corresponding polarity and 0 for the rest .
In other words , we treat all words with fractional assignments over different polarities as neutral .
Because the optimal solutions of LP correspond to extreme points in the convex polytope formed by the constraints , we obtain a large portion of words with non-fractional assignments toward non-neutral polarities .
Alternatively , one can round up fractional values .
Empirical Comparisons : ILP v.s. LP
To solve the ILP / LP , we run ILOG CPLEX Optimizer ( CPLEX , 2009 ) ) on a 3.5 GHz 6 core CPU machine with 96GB RAM .
Efficiency - wise , LP runs within 10 minutes while ILP takes several hours .
Table 4 shows the results evaluated against MPQA for different variations of ILP and LP .
We find that LP variants much better recall and F-score , while maintaining comparable precision .
Therefore , we choose the connotation lexicon by LP ( C- LP ) in the following evaluations in ?5 .
Experimental Results II
In this section , we present comprehensive intrinsic ?5.1 and extrinsic ?5.2 evaluations comparing three representative lexicons from ?2 & ?4 : C-LP , OVERLAY , PRED -ARG ( CP ) , and two popular sentiment lexicons : SentiWordNet ( Baccianella et al. , 2010 ) and GI + MPQA .
14
Note that C-LP is the largest among all connotation lexicons , including ?70,000 polar words .
15
Intrinsic Evaluation : Human Judgements
We evaluate 4000 words 16 using Amazon Mechanical Turk ( AMT ) .
Because we expect that judging a connotation can be dependent on one 's cultural background , personality and value systems , we gather judgements from 5 people for each word , from which we hope to draw a more general judgement of connotative polarity .
About 300 unique Turkers participated the evaluation tasks .
We gather gold standard only for those words for which more than half of the judges agreed on the same polarity .
Otherwise we treat them as ambiguous cases .
17 Figure 3 shows a part of the AMT task , where Turkers are presented with questions that help judges to determine the subtle connotative polarity of each word , then asked to rate the degree of connotation on a scale from - 5 ( most negative ) and 5 ( most positive ) .
To draw 14 GI +MPQA is the union of General Inquirer and MPQA .
The GI , we use words in the " Positiv " & " Negativ " set .
For SentiWordNet , to retrieve the polarity of a given word , we sum over the polarity scores over all senses , where positive ( negative ) values correspond to positive ( negative ) polarity .
15 ?13 k adj , ?6 k verbs , ?28 k nouns , ?22 k proper nouns .
16
We choose words that are not already in GI + MPQA and obtain most frequent 10,000 words based on the unigram frequency in Google - Ngram , then randomly select 4000 words .
17
We allow Turkers to mark words that can be used with both positive and negative connotation , which results in about 7 % of words that are excluded from the gold standard set .
the gold standard , we consider two different voting schemes : ? ?
V ote : The judgement of each Turker is mapped to neutral for ?1 ? score ?
1 , positive for score ?
2 , negative for score ?
2 , then we take the majority vote .
? ? Score : Let ?( i ) be the sum ( weighted vote ) of the scores given by 5 judges for word i .
Then we determine the polarity label l ( i ) of i as : l( i ) = ? ? ? positive if ?( i ) > 1 negative if ?( i ) < ?1 neutral if ?1 ? ?( i ) ?
1
The resulting distribution of judgements is shown in Table 5 & 6 .
Interestingly , we observe that among the relatively frequently used English words , there are overwhelmingly more positively connotative words than negative ones .
In Table 7 , we show the percentage of words with the same label over the mutual words by the two lexicon .
The highest agreement is 77 % by C-LP and the gold standard by AMT V ote .
How good is this ?
It depends on what is the natural degree of agreement over subtle connotation among people .
Therefore , we also report the degree of agreement among human judges in Table 7 , where we compute the agreement of one Turker with respect to the gold standard drawn from the rest of the Turkers , and take the average across over all five Turkers 18 .
Interestingly , the performance of 18
In order to draw the gold standard from the 4 remaining Turkers , we consider adjusted versions of ?
V ote and ?
Score schemes described above .
POS NEG NEU UNDETERMINED ? V ote 50.4 14.6 24.1 10.9 ? Score 67 .
9 20.6 11.5 n/a
Turkers is not as good as that of C-LP lexicon .
We conjecture that this could be due to generally varying perception of different people on the connotative polarity , 19 while the corpus-driven induction algorithms focus on the general connotative polarity corresponding to the most prevalent senses of words in the corpus .
Extrinsic Evaluation
We conduct lexicon- based binary sentiment classification on the following two corpora .
SemEval From the SemEval task , we obtain a set of news headlines with annotated scores ( ranging from - 100 to 87 ) .
The positive / negative scores indicate the degree of positive / negative polarity orientation .
We construct several sets of the positive and negative texts by setting thresholds on the scores as shown in Table 8 . " ?
n " indicates that the positive set consists of the texts with scores ?
n and the negative set consists of the texts with scores ? ?n.
Emoticon tweets
The sentiment Twitter data 20 consists of tweets containing either a smiley emoticon ( positive sentiment ) or a frowny emoticon ( negative sentiment ) .
We filter out the tweets with question marks or more than 30 words , and keep the ones with at least two words in the union of all polar words in the five lexicons in Table 8 , and then randomly select 10000 per class .
We denote the short text ( e.g. , content of tweets or headline texts from SemEval ) by t. w represents the word in t. W + /W ? is the set of posi- tive / negative words of the lexicon .
We define the weight of w as s ( w ) .
If w is adjective , s ( w ) = 2 ; otherwise s ( w ) = 1 .
Then the polarity of each text is determined as follows : pol ( t ) = ? ? ? ? ? ? ? ? ? positive if W + w?t s ( w ) ? W ? w?t s ( w ) negative if W + w?t s ( w ) < W ? w?t s ( w )
As shown in Table 8 , C-LP generally performs better than the other lexicons on both corpora .
Considering that only very simple classification strategy is applied , the result by the connotation lexicon is quite promising .
Finally ,
Table 1 highlights interesting examples of proper nouns with connotative polarity , e.g. , " Mandela " , " Google " , " Hawaii " with positive connotation , and " Monsanto " , " Halliburton " , " Enron " with negative connotation , suggesting that our algorithms could potentially serve as a proxy to track the general connotation of real world entities .
Table 2 shows example common nouns with connotative polarity .
Practical Remarks on WSD and MWEs
In this work we aim to find the polarity of most prevalent senses of each word , in part because it is not easy to perform unsupervised word sense disambiguation ( WSD ) on a large corpus in a reliable way , especially when the corpus consists primarily of short n-grams .
Although the resulting lexicon loses on some of the polysemous words with potentially opposite polarities , per-word connotation ( rather than per-sense connotation ) does have a practical value : it provides a convenient option for users who wish to avoid the burden of WSD before utilizing the lexicon .
Future work includes handling of WSD and multi-word expressions ( MWEs ) , e.g. , " Great Leader " ( for Kim Jong - Il ) , " Inglourious Basterds " ( a movie title ) .
Related Work A very interesting work of Mohammad and Turney ( 2010 ) uses Mechanical Turk in order to build the lexicon of emotions evoked by words .
In contrast , we present an automatic approach that infers the general connotation of words .
Velikovich et al. ( 2010 ) use graph propagation algorithms for constructing a web-scale polarity lexicon for sentiment analysis .
Although we employ the same graph propagation algorithm , our graph construction is fundamentally different in that we integrate stronger inductive biases into the graph topology and the corresponding edge weights .
As shown in our experimental results , we find that judicious construction of graph structure , exploiting multiple complementing linguistic phenomena can enhance both the performance and the efficiency of the algorithm substantially .
Other interesting approaches include one based on min-cut ( Dong et al. , 2012 ) or LDA ( Xie and Li , 2012 ) .
Our proposed approaches are more suitable for encoding a much diverse set of linguistic phenomena however .
But our work use a few seed predicates with selectional preference instead of relying on word similarity .
Some recent work explored the use of constraint optimization framework for inducing domain-dependent sentiment lexicon ( Choi and Cardie ( 2009 ) , Lu et al . ( 2011 ) ) .
Our work differs in that we provide comprehensive insights into different formulations of ILP and LP , aiming to learn the much different task of learning the general connotation of words .
Conclusion
We presented a broad-coverage connotation lexicon that determines the subtle nuanced sentiment of even those words that are objective on the surface , including the general connotation of realworld named entities .
Via a comprehensive evaluation , we provided empirical insights into three different types of induction algorithms , and proposed one with good precision , coverage , and efficiency . , Page et al. ( 1999 ) , Feng et al . ( 2011 ) Heerschop et al. ( 2011 ) , Montejo - R?ez et al. ( 2012 ) )
