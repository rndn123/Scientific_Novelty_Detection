title
Multi-Task Stance Detection with Sentiment and Stance Lexicons
abstract
Stance detection aims to detect whether the opinion holder is in support of or against a given target .
Recent works show improvements in stance detection by using either the attention mechanism or sentiment information .
In this paper , we propose a multi-task framework that incorporates target-specific attention mechanism and at the same time takes sentiment classification as an auxiliary task .
Moreover , we used a sentiment lexicon and constructed a stance lexicon to provide guidance for the attention layer .
Experimental results show that the proposed model significantly outperforms state - of - the - art deep learning methods on the SemEval - 2016 dataset .
Introduction
With the rapid growth of social media , user opinions towards various targets , e.g. , politicians and religion , are abundant .
These opinions can help optimize management systems and can gain insight into important events , e.g. , presidential elections .
The stance detection task aims to determine whether people are in favor of , against , or neutral towards a specific target .
This task is similar to the three - way aspect-level sentiment analysis that determines sentiment polarity towards aspect terms .
However , different from aspect-level sentiment analysis , the target in stance detection might not be explicitly mentioned in a given sentence .
Consider the following example tweet : @realDonaldTrump is the only honest voice of the @GOP and that should scare the shit out of everyone !
# SemST .
Target : Hillary Clinton ; Stance : Against ; Sentiment : Positive .
Observe that even though target Hillary Clinton does not appear in this tweet , we can still infer that opinion holder is less likely to be in favor of Hillary Clinton .
Therefore , identifying target related information is of vital importance to stance detection .
Previous studies to stance detection used feature engineering ( Mohammad et al. , 2016 b ) , Convolutional Neural Networks ( CNNs ) ( Vijayaraghavan et al. , 2016 ; Wei et al. , 2016 ) and Recurrent Neural Networks ( RNNs ) ( Zarrella and Marsh , 2016 ) .
However , they failed to take target information into considerations .
In order to address this issue , several target-specific attention mechanisms ( Du et al. , 2017 ; Zhou et al. , 2017 ; Sun et al. , 2018 ) have been proposed to embed target information into sentence representations .
Sentiment information has also been proven to be beneficial to stance detection .
For example , Sobhani et al. ( 2016 ) found that sentiment lexicon features are useful for stance detection when combined with other features .
Instead of directly integrating sentiment features into vector representations , Sun et al . ( 2018 ) proposed a hierarchical attention network that learns the importance of sentiment information automatically .
Later , Sun et al. ( 2019 ) proposed a joint model that determines stance and sentiment simultaneously .
However , the attention mechanism is not considered in this model .
Motivated by recent advances in multi-task learning ( Liu et al. , , 2017 Balikas et al. , 2017 ; Yu et al. , 2018 ; Cohan et al. , 2019 ) and the effectiveness of sentiment information in stance detection ( Sobhani et al. , 2016 ; Sun et al. , 2018
Sun et al. , , 2019 , we propose an attention - based multi-task framework that takes sentiment classification as an auxiliary task for stance detection .
Specifically , we first encode words of input sentences in fixed - length dense vectors ( Mikolov et al. , 2013 ; Bojanowski et al. , 2017 ) and then feed them as input to Bidirectional Long Short - Term Memory networks ( BiLSTM ) ( Hochreiter and Schmidhuber , 1997 ; Schuster and Paliwal , 1997 ) .
After that , the attention mechanism ( Bahdanau et al. , 2015 ) is used to extract important words for sentiment representation .
Besides , a target-specific attention layer is designed to identify important words related to a given target .
The main task incorporates sentiment information by concatenating the outputs of the two attention layers .
Moreover , in order to provide guidance to the attention mechanism , sentiment and stance lexicon features are integrated into the final loss .
We show that the proposed model , called AT - JSS - Lex , achieves competitive results on the SemEval - 2016 dataset ( Mohammad et al. , 2016a ( Mohammad et al. , , b , 2017 .
Our contributions are summarized as follows :
First , we propose a joint sentiment and stance model ( AT - JSS - Lex ) based on multi-task learning that improves stance detection with the help of sentiment information and integrates both sentiment attention and target-specific attention .
Second , we propose a novel formulation of the loss function that uses an existing sentiment lexicon and a stance lexicon , that we specifically constructed for this task , to guide the attention mechanism .
As part of our contributions , we will make the stance lexicon publicly available .
Finally , we show that the proposed AT - JSS - Lex model achieves remarkable improvements in performance over strong baselines and prior works on the SemEval - 2016 stance detection dataset .
Model
Multi-Task Learning Architecture
The overall architecture of the proposed model is shown in Figure 1 .
For sentiment classification ( the auxiliary task ) , the input sentence ( S = s 1 , s 2 , ... , s n ) is first sent to an embedding layer and each word is represented by a dense vector ( X = x d 1 1 , x d 1 2 , ... , x d 1 n ) , where n is the sentence length and d 1 is the dimension of the word embedding .
Then , a BiLSTM is used for feature extraction .
At time step i , the hidden vector of forward LSTM is calculated based on previous hidden vector h i?1 and current input vector x i , ? ? h i = LST M ( x i , ? ? ? h i?1 ) .
The hidden vector ? ? h i of backward LSTM is defined in a similar way .
After concatenating the two hidden vectors , we obtain h i = [ ? ? h i ; ? ? h i ] .
We adopt the attention mechanism ( Bahdanau et al. , 2015 ) to improve the three - class sentiment classification .
Attention weight ?
1 i is computed as : where e i is calculated based on sentence summary vector s , the final hidden vector of BiLSTM , and hidden vector h i : ?
1 i = exp( e i ) n j=1 exp ( e j ) ( 1 ) e i = v T s tanh ( W s s + W i h i + b s ) ( 2 ) where W s , W i ?
R 2d 2 ?2d 2 and v s , b s ?
R 2d 2 . d 2 is the dimension of hidden units of LSTM .
The final vector representation is the weighted sum of hidden vectors : r 1 = n i=1 ? 1 i h i ( 3 )
At last , a fully - connected layer and a softmax layer are applied to get label distribution .
Different from the attention layer of auxiliary task , the attention layer of main task integrates target embedding t , similar to ( Zhou et al. , 2017 ) .
The target embedding t is the word embedding of the target word .
For example , for target " Abortion , " t is the word vector of the word " Abortion . "
For multi-word targets ( e.g. , Hillary Clinton ) , we use the average of the constituent word vectors ( e.g. , the average of the vectors corresponding to " Hillary " and " Clinton " ) .
Then e i can be written as : e i = v T t tanh ( W t t + W i h i + b t ) ( 4 ) where W t ?
R 2d 2 ?d 1 , W i ?
R 2d 2 ?2d 2 , t ?
R d 1 , v t and b t ?
R 2d 2 . Then , ? 2 and r 2 are computed in the same way as Eq. ( 1 ) and Eq. ( 3 ) , respectively .
The final vector representation of main task is the concatenation of r 1 and r 2 .
In the training stage , cross-entropy loss is used to train the model and the loss function L is defined as follows : where ? is a hyper-parameter to be tuned , determining the weight of stance detection task .
L main and L aux are loss functions of main task and auxiliary task , respectively .
L = ?L main + ( 1 ? ?) L aux ( 5 )
Lexicon Loss Previous works showed that the attention mechanism is beneficial for stance detection .
However , the attention mechanism does not always work well due to the size of training data and inability to identify target information .
In order to address these issues , we propose a reformulation of the loss in Eq. ( 5 ) that employs both sentiment and stance lexicons to improve stance detection .
Our full model is called AT - JSS - Lex .
We use the sentiment lexicon 1 by Hu and Liu ( 2004 ) and construct a stance lexicon 2 ( of almost 2,000 words ) with words related to the five targets in the SemEval - 2016 dataset .
Specifically , we construct a stance lexicon for each target from the training data available from the Se-mEval 2016 dataset and from an extra 1,000 tweets for each target that we collected from Twitter using specific hashtags .
For example , for the target " Hillary Clinton , " hashtags such as " # Crooked - Hillary , " " # hillaryforprison , " and " # Hillary 2016 " are used to collect more tweets .
After data collection , we manually extracted the directly related words ( e.g. , Killary , Shillary , and Hilly ) and indirectly related words ( e.g. , Trump , Benghazi , and abortion ) from each tweet .
We ended up with around 400 lexicon words for each target .
The intuition of using sentiment and stance lexicons is to provide guidance to the attention layer .
Specifically , given an input sentence , we mark sentiment and stance lexicon words as 1 and mark as 0 the remaining words that are not present in any of the two lexicons in order to obtain a lexicon vector lex .
For example , the lexicon vector for " Celebrity atheism is beginning to irk me # init - 1 https://www.cs.uic.edu/ ? liub/FBS/sentiment-analysis.
html#lexicon 2 https://github.com/chuchun8/EMNLP19-Stance forthemoney # SemST " is [ 0 1 0 0 0 1 0 0 0 ] .
Note that the words " atheism " and " irk " are from the stance and sentiment lexicons , respectively .
The final loss from Eq. ( 5 ) is then defined as : L =?L main + ( 1 ? ?) L aux + ?(? norm ? lex ) 2 ( 6 ) where ? is a hyperparameter that determines the importance of lexicon loss and ? norm is the normalization of ? sum . ? sum is the summation of attention weights ?
1 and ?
2 . We normalize the ? sum by dividing with the maximum value .
Through the minimization of the loss function , ideally , ? norm gets closer to lex , i.e. , the vector components of ?
norm get closer to 1 when the corresponding components in lex are 1 ( and closer to 0 otherwise ) , which enforces the model to learn higher attention weights for important words .
3 Experiments and Analysis
Datasets and Experimental Settings
We use SemEval - 2016
Task 6.A to test the performance of our proposed model .
This dataset contains five different targets : " Atheism , " " Climate Change is a Real Concern " ( " Climate " ) , " Feminist Movement " ( " Feminism " ) , " Hillary Clinton " ( " Hillary " ) and " Legalization of Abortion " ( " Abortion " ) .
Table 1 shows the distribution of these targets in the dataset .
Each tweet has a stance label ( " Favor , " " Against " or " None " ) and a sentiment label ( " Positive , " " Negative " or " Other " ) .
We sampled about 15 % of the training data as validation data to tune the parameters .
Word vectors are initialized using fastText word embeddings 3 ( Bojanowski et al. , 2017 ) main task is 50 and 100 , respectively .
? is 0.7 and ? is 0.025 for all targets .
L2 regularization is applied to the loss function and the regularization parameter is set to 0.01 .
Evaluation Metrics
F avg , macro- average of F1 score ( M acF avg ) and micro-average of F1 score ( M icF avg ) are adopted to evaluate the performance of proposed model .
Firstly , the F1 score of label " Favor " and " Against " is calculated as follows : F f avor = 2 P f avor R f avor P f avor + R f avor ( 7 ) F against = 2 P against R against P against + R against ( 8 ) where P and R are precision and recall respectively .
Then the F1 average is calculated as : F avg = F f avor + F against 2 ( 9 ) Note that the label " None " is not discarded during training .
However , the label " None " is not considered in the evaluation because we are only interested in labels " Favor " and " Against " in this task .
We average the F avg on each target to get M acF avg .
Moreover , we get M icF avg by calculating F f avor and F against across all targets .
Results First , an ablation experiment is used to determine the importance of each component of our proposed model for stance detection : ? AT - JSS - Lex is a lexicon integrated multi-task model with attention mechanisms .
? AT - JSS is a model that shares the same architecture with AT - JSS - Lex , but has no lexicon loss .
?
JSS is a joint sentiment and stance model similar to AT - JSS , but without the attention mechanisms .
? AT - BiLSTM is a BiLSTM model with stance attention .
?
BiLSTM is a single task model that only exploits BiLSTM to detect stance .
Table 2 ( top ) shows the results of this ablation study .
As we can see from the table , AT - JSS - Lex performs best on " Climate , " " Feminism , " " Hillary , " and " Abortion . "
Moreover , AT - JSS - Lex has the best M acF avg and M icF avg scores when compared with the other models .
Experimental results show that M acF avg and M icF avg drop by 0.75 % and 0.27 % when we remove the lexicon component , indicating that lexicon information contributes to stance detection ( except on " Atheism " ) .
Note that AT - JSS - Lex also outperforms AT - BiLSTM model , which shows that the proposed multi-task framework has better performance than single task with attention mechanism .
Second , we compare the proposed model with the following baseline methods ( all experimental results of baseline methods are retrieved from original papers ) : ? SVM -ngram ( Mohammad et al. , 2016 b ) is trained by using word n-grams and character n-grams features , surpassing the best model in SemEval - 2016 competition .
? JOINT
( Sun et al. , 2019 ) is a joint model that exploits sentiment information to improve stance detection task without attention mechanism .
? TAN ( Du et al. , 2017 ) is an attention - based LSTM model that extracts important part of given text .
? AS - biGRU-CNN
( Zhou et al. , 2017 ) is another attention - based model that adds CNN layer after attention - based LSTM model to extract target specific features .
? HAN ( Sun et al. , 2018 ) is a hierarchical attention model leveraging various linguistic features .
? TGMN - CR ( Wei et al. , 2018 ) uses attention and memory modules to extract important information for detecting stance .
Table 2 ( bottom ) shows the results of this comparison as well .
We can observe that AT - JSS - Lex model outperforms all baseline models except on " Atheism . "
Specifically , the proposed model outperforms JOINT model by 5.17 % and 3.11 % in M acF avg and M icF avg , demonstrating the effectiveness of attention mechanism and lexicon information .
In addition , our AT - JSS - Lex model also performs better than attention - based models ( TAN , HAN , AS - biGRU -CNN and TGMN - CR ) , showing that multi-task learning can benefit the stance detection task .
Attention Visualizations
In Figure 2 , we list two input sentences and visualize the attention weights learned during the training process .
The color indicates the importance of a word in given sentences , the darker the more important .
We can observe that the words " global " and " warming " that are closely related to the target " Climate , " have high attention weights in the first sentence .
Likewise , in the second sentence , the words " Mary , " " Mother , " " God , " and " sinners , " are highlighted and show that the proposed model can pay attention to target- related words .
Error Analysis Mislabeled data and compound hashtags are two challenging factors that increase the difficulty of further improving the stance classification .
For example , consider the following tweet : Watch @ Dame Lillard bring the Blazers to the playoff # Beast # SemST .
Target : Atheism ; Stance : Against ; Sentiment : Positive .
Even though the stance label of this tweet is " Against , " however , we can observe that the content of this sentence is nothing related to the target .
For SemEval 2016 dataset , sometimes it is hard to predict the stance without considering the compound hashtags .
Here is an example : @ Reince
This is very credible !
Good work !
America is desperately in need of good leadership .
# Vote-GOP # NoHillary # SemST .
Target : Hillary Clinton ; Stance : Against ; Sentiment : Positive .
It would be very difficult to infer the correct stance label if we do not consider the hashtags " # NoHillary " and " # VoteGOP . "
Therefore , inability to separate compound hashtags results in the loss of important target information .
Conclusion and Future Work
In this paper , we propose an attention - based multitask learning framework and integrate lexicon information to achieve better performance .
Experimental results show that our model outperforms state - of - the - art deep learning methods for this task .
Moreover , visualization results indicate the capability of our model to capture essential information .
Future work includes exploiting unsupervised learning to generate target-related lexicon and incorporating more labels ( e.g. , emotion classification ) for multi-task learning .
Figure 1 : 1 Figure 1 : Our multi-task model for stance detection .
