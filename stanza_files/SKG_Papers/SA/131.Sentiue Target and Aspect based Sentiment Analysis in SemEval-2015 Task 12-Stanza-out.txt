title
Sentiue : Target and Aspect based Sentiment Analysis in SemEval - 2015 Task 12
abstract
This paper describes our participation in SemEval - 2015 Task 12 , and the opinion mining system sentiue .
The general idea is that systems must determine the polarity of the sentiment expressed about a certain aspect of a target entity .
For slot 1 , entity and attribute category detection , our system applies a supervised machine learning classifier , for each label , followed by a selection based on the probability of the entity / attribute pair , on that domain .
The target expression detection , for slot 2 , is achieved by using a catalog of known targets for each entity type , complemented with named entity recognition .
In the opinion sentiment slot , we used a 3 class polarity classifier , having BoW , lemmas , bigrams after verbs , presence of polarized terms , and punctuation based features .
Working in unconstrained mode , our results for slot 1 were assessed with precision between 57 % and 63 % , and recall varying between 42 % and 47 % .
In sentiment polarity , sentiue 's result accuracy was approximately 79 % , reaching the best score in 2 of the 3 domains .
Introduction Social networks and other online platforms are an important communication mechanism in current lifestyle .
These platforms aggregate user- generated content , such as opinions that people write and publish freely on the Web , and are now valued for market research and trend analysis .
Natural language processing ( NLP ) helps to automatically extract information from these written opinions .
This paper describes a participation in SemEval - 2015 Task 12 1 , Aspect Based Sentiment Analysis ( Pontiki et al. , 2015 ) , with the sentiue system , from Universidade de ?vora .
In previous editions of SemEval , we participated in Sentiment Analysis ( SA ) tasks , but in terms of overall polarity , over Twitter messages ( Rosenthal et al. , 2014 ) , not being aspect oriented .
The general idea for this challenge , is that , for a text , the system must determine the polarity of the sentiment expressed about a certain aspect of a particular target entity .
Our sentiue system is an evolution from our previous work ( Saias and Fernandes , 2013 ; Saias , 2014 ) , for target oriented SA .
Task 12 was run in two phases .
In phase A systems are tested for aspect detection with one slot to aspect category , and a second slot for the opinion target expression on the text .
Test data includes review texts for two domains : restaurants and laptops .
In phase B , aspect category is provided , and systems must assign a polarity ( positive , negative , or neutral ) for each opinion .
In this phase , systems received also texts from a third domain , hotels , for which no sentiment training data was given .
We used a supervised machine learning classifier combined with a probability based selection process , for entity and attribute category detection , on slot 1 .
Target expression detection was performed with an entity catalog , filled with known targets for each entity type , and named entity recognition ( NER ) .
For the sentiment polarity slot , we used a a supervised machine learning classifier , having bagof-words ( BoW ) , lemmas , bigrams after verbs , and punctuation based features , along with sentiment lexicon based features .
The detailed procedure is explained in section 3 .
Related Work Many SA related publications , originating both in industry and in academia , have appeared , and it is notorious the growing interest by companies .
Popular scientific forums and events include activities and workshops on this area , such as RepLab ( Amig ?
et al. , 2014 ) at CLEF 2 , for online reputation , or ABSA and Twitter SA tasks in SemEval .
In last year 's edition of this SemEval task ( Pontiki et al. , 2014 ) , there were 26 systems participating in the polarity subtask .
The two systems with better polarity classification accuracy were from NRC - Canada and DCU teams .
NRC - Canada system ( Kiritchenko et al. , 2014 ) was trained with the data provided in the task , and complemented with lexicons generated from other corpora of customer reviews , to help feature extraction in machine learning .
Stanford CoreNLP was used to tokenize , POS tagging , and dependency parse trees .
They address polarity classification with a linear SVM classifier , with features for : the target , and its surrounding words ; POS based features ; dependency tree based features ; unigrams and bigrams ; lexicon based features .
The DCU system ( Wagner et al. , 2014 ) also uses SVM for aspect and for polarity classification , combining bag-of - n- gram features with rule- based features .
N-grams ( with size from 1 to 5 ) in a window around the aspect term , are used as features , as well as features derived from a sentiment lexicon .
The rule- based approach to predict the polarity of an aspect term , generated features considering all words score and their distance to the aspect term .
Method
Our participation involved the adaptation of our previous real-time system , for text overall sentiment classification , into a target oriented SA system .
The next subsections explain how the system works , for each part of Task 12 challenge .
2 http://clef2014.clef-initiative.eu/
Aspect Entity and Attribute
The first annotation task focuses on aspect category .
This category is an entity and attribute pair , each chosen from an inventory with possible values , in each domain , for entity types and attributes .
Since the possible category types are known and limited , we decided to use a classifier for each entity type ( e.g. food , laptop ) and for each attribute label ( e.g. price , quality ) .
Our approach comprises two stages .
The first processes each review sentence assigning to it zero , one , or more entity types and attribute labels .
The second stage chooses and combines identified entities and attributes , forming the aspect annotation .
Analyzing the training data , we found that in the same sentence , there may be opinions on various types of entity ( e.g. CPU , battery ) or attributes .
Thus , we have chosen to train a classifier for each entity type , and a classifier for each attribute label .
We set a supervised machine learning text classifier , using MALLET ( McCallum , 2002 ) , a Javabased tool for NLP , with machine learning applications to text .
For the purpose of this stage , it was necessary to prepare the training data for each binary classifier , that would determine whether a sentence contains an opinion on its tag ( entity type or attribute label ) .
The train process was the same for all tags , entity type or attribute label , of each domain .
We created a dataset where each instance is a sentence text , and its class is tag , if the sentence had at least one opinion with that tag , or no tag otherwise .
Text preprocessing includes tokenization , POS tagging and lemmatization , all performed with Stanford CoreNLP ( Toutanova et al. , 2003 ; Manning et al. , 2014 ) tool .
The classifier algorithm was Maximum Entropy 3 , and the classifier model features were text words and lemmas .
Second stage starts with each sentence annotated with a set and tags , some for entity type and some for attribute label .
When a sentence has no annotations , the system assumes that there is no opinion .
In case of 1 tag on entity type and 1 tag in the attribute label , then it is the trivial case where the junction of the two results in the aspect annotation .
For sentences with 1 tag on entity type and 0 tags for the attribute , our system searches for the most frequent aspect annotation , within the sentence domain , that includes that entity type .
The equivalent is applied in the case of 0 tags to entity and 1 tag for the attribute label .
If both sides have one or more tags , the system applies a cycle , where each loop iteration forms the more frequent pair ( entity , attribute ) in that domain , and removes these two tags from the sentence tag set .
This is repeated until the first , entity or attribute side , exhausts the tags provided by the previous stage classifier .
And if some tags are left , on the opposite side of the pair , the system applies , for each , the same process already explained for case 0 - 1 or 1- 0 .
Opinion Target Expression
At this point , sentences are already marked as having ( or not ) opinions on certain aspect category .
For each opinion on restaurants domain , the system needed to identify the entity mention on the sentence text , referred to as the opinion target expression ( OTE ) .
We collected the opinion targets for each entity type , from the training data , forming a catalog .
If any of the targets already known ( e.g. restaurant name , or meal ) appears in the sentence text , next to a verb or adjective , it is chosen as the OTE .
If this does not lead to any OTE candidate , our system applies named entity recognition , looking for references to organization and location entities , using Stanford NER tool ( Finkel et al. , 2005 ; Manning et al. , 2014 ) .
Having found one OTE , through the catalog or by NER , its text and position are marked in slot 2 .
If no mention is found , OTE slot is filled with the NULL value .
Sentiment Polarity
Phase
B was held in a subsequent period , and the input given to the systems is a little different , having the correct annotations on the aspect category , in restaurants , laptops and hotels domains .
For each opinion , the participating systems must assign a sentiment polarity ( positive , negative or neutral ) , considering the opinion aspect .
For training , there were 1654 opinions on restaurants domain , and 1974 more opinions about laptops , all annotated for polarity .
No sentiment training data was given for hotels domain .
Considering the available data , and the objective of this phase , we used a supervised machine learning classifier to predict each opinion polarity .
Instead of multiple classi-fiers , such as implemented for slot 1 , we prepared a single classifier , thought , as before , for text but tuned with a different model , so that it can choose between positive , negative or neutral polarity .
Sentences without opinion are not considered in the training , because here the polarity is associated with opinions .
Further , a single sentence may have several opinions about different aspects , and each may have a different and independent polarity .
To train the classifier , for each opinion we created a polarity data instance , containing the sentence text , its domain , its aspect entity and attribute , OTE ( if available , in restaurants ) , and the opinion polarity to be learned .
As before , MALLET was used with a Maximum Entropy classifier .
The sentence text preprocessing was the same we did for aspect category classification .
The features to represent each instance were : ?
BoW with a feature for each token text ; ? lemmas for verbs and adjectives ; ? bigram after verb ( lemmatized ) ; ? presence of negation terms ; ? bigram after negation term ; ? presence of exclamation / question mark ; ? presence of polarized terms ( positive or negative ) , according to each sentiment lexicon ; ? whether there are polarized terms before exclamation mark and question mark ; ? bigram before , and after , any polarized term ; ? polarity inversion , by negation detection before some polarized term ; ? presence of polarized terms in the last 5 tokens ; ? a feature for the domain , and two features for the entity type and the attribute label .
To see whether a term is polarized , each token text is verified in each sentiment lexicon .
These polarity support resources are AFINN lexicon ( Nielsen , 2011 ) , Bing Liu 's opinion lexicon ( Liu et al. , 2005 ) and MPQA subjectivity clues ( Wiebe et al. , 2005 ) .
Domain Precision Recall F-measure restaurants
0,633 0,472 0,541 laptops 0,577 0,441 0,500
After some experimentation , we decided to use a single full train , joining the instances of restaurants and laptops as a whole training set .
The resulting model was used to classify the opinion polarity for the three domains .
Because we used sentiment lexicons , our system operates in unconstrained mode .
These additional resources served as support for features extraction .
No supplementary training texts were used .
In our development testing , we obtained an 80 % accuracy for polarity .
this , the result is written in XML format for submission .
Results
The phase A test data had 685 sentences on restaurants domain and 761 on laptops domain .
With the method described above , the sentiue system extracted 596 opinion categories for restaurants domain and 751 other for laptops domain .
Table 1 shows the evaluation for slot 1 .
Among the 15 submissions evaluated in the first domain , the best system F-score value was 0,627 , while our result F-score was 0,541 .
For laptops aspect category , sentiue 's scores were lower , but improving in the comparison with other systems , achieving the second best F-measure , out of 9 evaluated submissions .
The evaluation of our result in opinion target expression in given in Table 2 .
This was a poor result , when compared with the 0,524 average F-measure of the 21 submissions for this slot .
In phase B systems had to fill the slot 3 , with sentiment polarity to 845 opinions on restaurants domain , 949 opinions on laptops domain , and 339 opinions on hotels domain .
On Table 3 we find our system 's result accuracy , in the two trained domains plus the untrained hotels do - main .
In this slot we got the most satisfactory result , with the best accuracy in restaurants and laptops , and an above average score , in the hotels domain .
The detailed evaluation is shown in Table 4 , with values for precision , recall and f-measure , per domain and polarity class .
Conclusions
By participating in this SemEval edition , we sought to develop our previous work , in order to achieve SA results focused on the opinion targets .
Our results were poor for OTE detection , but we think it will be easy to correct the implementation problems for that part .
As example , while checking if a sentence contained a known target , from the catalog , the system did not require whole words to be matched , and this led to some misidentification of word substrings as target .
Our result was more satisfactory for slot 1 , with a F-measure slightly above average between the 15 evaluated submissions for restaurants domain , and 4.5 % better than submissions average for laptops domain .
The distribution of opinions for each aspect category is not uniform .
For example , for attribute label classification , we already know that QUALITY and GENERAL have much more instances than other labels .
This analysis inspired our approach in the second stage , explained in section 3.1 .
To improve this part , we think to introduce a cascade classifier .
After the classification obtained in the current first stage , other machine learning classifier will decide how to pair entity + attribute , based on the wording of the sentence .
Another future work idea is to use more corpora for training the aspect classifiers , as other systems ( Kiritchenko et al. , 2014 ) have tried .
In phase B sentiue achieved good results .
This , perhaps , is justified by our previous experience in overall SA .
Many of the polarity classifier features are inherited from our former system .
SemEval challenge is always a motivation to test our system and an opportunity to learn from other participants .
Table 1 : 1 sentiue 's evaluation on aspect category .
Domain Precision Recall F-measure restaurants
0,488 0,336 0,398
Table 2 : 2 sentiue 's evaluation on target detection .
Table 3 : 3 sentiue accuracy on sentiment polarity .
Table 4 : 4 sentiue 's SA evaluation per domain .
http://alt.qcri.org/semeval2015/task12/
MALLET class : cc.mallet.classify .MaxEnt
Domain , PolarityPrecision Recall F-measure restaurants , positive 0,767 0,914 0,834 restaurants , negative 0,825 0,708 0,762 restaurants , neutral 0,714 0,111 0,192 laptops , positive 0,831 0,891 0,860 laptops , negative 0,766 0,787 0,777 laptops , neutral 0,387 0,152 0,218 hotels , positive 0,887 0,840 0,863 hotels , negative 0,608 0,738 0,667 hotels , neutral 0,143 0,083 0,105
