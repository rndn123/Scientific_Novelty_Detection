title
ECNU at SemEval - 2016 Task 5 : Extracting Effective Features from Relevant Fragments in Sentence for Aspect- Based Sentiment Analysis in Reviews
abstract
This paper describes our systems submitted to the Sentence-level and Text-level Aspect - Based Sentiment Analysis ( ABSA ) task ( i.e. , Task 5 ) in SemEval - 2016 .
The task involves two phases , namely , Aspect Detection phase and Sentiment Polarity Classification phase .
We participated in the second phase of both subtasks in laptop and restaurant domains , which focuses on the sentiment analysis based on the given aspect .
In this task , we extracted four types of features ( i.e. , Sentiment Lexicon Features , Linguistic Features , Topic Model Features and Word2vec Feature ) from certain fragments related to aspect rather than the whole sentence .
Then the proposed features are fed into supervised classifiers for sentiment analysis .
Our submissions rank above average .
Introduction Aspect- Based Sentiment Analysis task ( ABSA ) , i.e. , task 5 in SemEval - 2016 , is an interesting task , which focuses on the sentiment analysis based on the target and certain categories .
The organizers set up three subtasks , i.e. , Sentence- level ABSA ( i.e , Subtask 1 ) , Text-level ABSA ( i.e. , Subtask 2 ) , and Out - of domain ABSA ( i.e. , Subtask 3 ) .
For subtask 1 and 2 , the training data in each domain is provided , while no labeled data is provided for subtask 3 . Given an opinionated document in a domain , both subtask 1 and subtask 2 are grouped into two phases , i.e. , Aspect Detection phase and Sentiment Polarity Classification phase .
We participated in the second phase of these two subtasks , aiming to identify the sentiment polarity for each given aspect which is made up of < E# A , OTE > in two domains .
Specifically , for Sentence- level ABSA , focusing on identifying all the opinion tuples ( i.e. , < E# A , OTE , polarity > ) in each sentence of the reviews , the Aspect Detection phase contains two slots .
The Slot1 is to identify every entity ( i.e. , E ) and attribute ( i.e. , A ) pair ( also named as category , e.g. , RESTAURANT - PRICES ) according to given sentence , and the Slot2 focuses on detecting Opinion Target Expression ( i.e. , OTE or target in short ) .
For example , in " Pizza here is consistently good " , the participants are required to recognize Pizza as OTE and FOOD # QUALITY as E#A .
The second phase , i.e. , Sentiment Polarity Classification ( Slot3 ) , is to determine the sentiment polarity ( i.e. , positive , negative , or neutral ) for each aspect ( i.e. , < E# A , OTE > ) .
As for Text-level ABSA , aiming at identifying the opinion tuples ( i.e. , < E# A , polarity > ) expressed in each review , the Aspect Detection phase is to identify the E#A pairs and the second phase is to assign the sentiment label ( positive , negative , neutral or conflict ) for each detected E#A .
The conflict label is assigned when the dominant sentiment of the opinion is not clear .
In previous work , ( Kim et al. , 2013 ) presented a hierarchical aspect sentiment model to classify the polarity of aspect terms from unlabeled online reviews .
( Jim?nez - Zafra et al. , 2015 ) proposed a syntactic approach for identifying the words that modify each aspect .
( Branavan et al. , 2009 ; He et al. , 2012 ; Mei et al. , 2007 ) used topic or category information .
( Saias , 2015 ) used a 3 - class classifier and some handcrafted features to perform ABSA .
( Lin and He , 2009 ; Jo and Oh , 2011 ) presented LDAbased models , which incorporated aspect and sentiment analysis together to model sentiments towards different aspects .
Unlike these work , which try to extract features from the whole sentence , we propose a method which just takes certain fragments related to the given aspect from the sentence into consideration to perform feature engineering for the ABSA task .
The rest of this paper is structured as follows .
In Section 2 , we describe our system in details , including motivation , preprocessing , feature engineering , evaluation metric and algorithm , etc .
Section 3 reports data sets , experiments and result discussion .
Finally , Section 4 concludes our work .
System Description
Motivation
At sentence - level ABSA , generally , a review consists of several sentences and one single sentence may contain mixed opinion tuples ( i.e. , < OTE , E#A , polarity >) towards different OTE or E#A .
The goal of our system is to identify the polarity for each opinion tuple .
We found that the given aspect is just related to certain fragments in corresponding sentence .
Therefore , in order to extract features from the relevant fragments , we proposed a two-step method to acquire potential words related to given aspect as pending words for future feature extraction .
This approach consists of two steps , i.e. , Segmentation step , which is to split each sentence into several fragments , and Selection Step , which selects out one or more fragments from sentence for each aspect .
Specifically , in the Segmentation Step , we used punctuation marks ( i.e. , { , .?!} ) and conjunctions ( i.e. , { and , but } ) to split the sentence into several candidate fragments .
It is worth noting that the OTE is the entity or attribute words in reviews the users explicitly indicated .
When there is no explicit mention of the entity , the OTE takes the value NULL .
In restaurant domain , both E#A and OTE are provided in reviews , whereas only E#A are annotated and provided in the laptop domain , we assumed its OTE is NULL .
Therefore , we adopted two strategies for Selection Step .
In the case that the targets are provided , we located the fragment which contains the target as target fragment and selected the words ranging from the prior target fragment ( not include ) to the current target fragment ( included ) as pending words .
In another case that the targets are NULL , we automatically assigned a target fragment for it as follows .
We firstly divided all sentences in training data into several subsets according to their attributes ( i.e. , A in E# A ) .
If multiple attributes exist in the same sentence then the sentence is shared in corresponding subsets .
Then we calculated the tfidf score for each word in each subset .
Finally , we summed up the tfidf scores of all words in each fragment according to the attribute in given opinion .
The fragment with top score is set as target fragment .
After locating the target fragment , the approach to select the pending words is the same as the case that the targets are provided .
As for text - level ABSA , the opinion tuples ( i.e. , < E# A , polarity > ) are endowed with each review rather than the sentence .
Based on the statistic of the training data , we found that the labels of E#A in textlevel are consistent with the most frequent polarity of the corresponding E# A in one review at sentencelevel .
Thus , for subtask 2 ( i.e. , text- level ) , we counted the number of positive , negative and neutral labels for each E# A in each review from the results of subtask 1 .
Then the most frequent polarity of each E# A is set as the label for corresponding E#A in each review in subtask 2 .
For each domain , the participants are required to submit two runs , ( 1 ) constrained : only the provided data can be used ; ( 2 ) unconstrained : any additional resources can be used .
In this task , we adopted external resources , i.e. , 8 sentiment lexicons and 100 billion words from Google News , to train the Sentiment Lexicon features and the Word2vec ( Mikolov et al. , 2013 ) feature .
Thus , the difference between our two systems lies in these two features .
For both systems , we also extracted many traditional types of features to build classifiers for classification .
Data Preprocessing
The original data is provided in XML format .
So we first removed the XML tags from data and then transformed the abbreviations to their normal for-mat .
We used Stanford Parser tools 1 for tokenization , POS tagging and parsing .
Then , the WordNetbased Lemmatizer implemented in NLTK 2 was adopted to lemmatize words to their base forms with the aid of their POS tags .
Feature Engineering
Four types of features extracted from the pending words are adopted to build the classifiers , i.e. , Linguistic features , Sentiment Lexicon features , Topic Model features and Word2vec feature .
Linguistic Features Word N-grams :
For all pending words , after transforming them into lowercase , we extracted the unigram , bigram , trigram and 4 - gram as word Ngrams features .
Lemmatized Word N-grams :
Pending words were lemmatized by NLTK firstly , then we extracted four types of N-grams from the lemmatized form as Lemmatized Word N-grams , i.e. , unigram L , bigram L , trigram L and 4 - gram L. Word Nchars :
We recorded presence or absence of contiguous sequences of 3 , 4 , and 5 characters from pending words as N-chars features .
POS : We counted the number of nouns ( the corresponding POS tags were NN , NNP , NNS and NNPS ) , verbs ( VB , VBD , VBG , VBN , VBP and VBZ ) , adjectives ( JJ , JJR and JJS ) and adverbs ( RB , RBR and RBS ) in pending words as the pos feature .
Allcaps :
It indicated the number of uppercase words in pending words .
Elongated :
We recorded the number of the words contained the repeating characters ( e.g. , slowwwwww ) as the elongated feature .
Punctuation : Customers often use exclamation mark ( ! ) and question mark ( ? ) to express surprise or emphasis , so we recorded the number of exclamation and question marks in pending words as the punctuation features .
Negation : Negation comprised various kinds of devices to reverse the truth value of a proposition , thus the identification of negations is very essential .
In our work , we collected 29 negations from Internet and designed this binary feature to indicate whether there is negation in pending words .
Sentiment Lexicon Features
Giving the pending words , we first converted them into lowercase and then calculated five sentiment scores for each sentiment lexicon to construct Sentiment Lexicon Features ( SentiLexi ) ( 1 ) the ratio of positive words to pending words , ( 2 ) the ratio of negative words to pending words , ( 3 ) the maximum sentiment score , ( 4 ) the minimum sentiment score , ( 5 ) the sum of sentiment scores .
We transformed the sentiment scores in all sentiment lexicons to the range of [ ?1 , 1 ] , where " ? " denotes negative sentiment .
If the pending word does not exist in one sentiment lexicon , its corresponding score is set to zero .
The following 8 sentiment lexicons are adopted in our systems : Bing Liu opinion lexicon 3 , General Inquirer lexicon 4 , IMDB 5 , MPQA 6 , AFINN 7 , Senti-WordNet 8 , NRC Hashtag Sentiment Lexicon 9 , NRC Sentiment140 Lexicon 10 .
Topic Model Features
With the aid of LDA -C tool 11 with default parameter setting , we generate topic-related features from all training data as follows .
Sent2Topic :
The LDA could generate the document distribution among predefined topics .
We extracted this distribution as Sent2Topic feature .
Top Topic word ( TopTopic ) :
Since the topic probability of each word indicates its significance in corresponding topic , we set 20 topics and collect the top 25 words in each topic to build TopTopic feature .
Word2vec Feature Google Word2vec ( GoogleW2V ) :
We used the publicly available word2vec tool 12 to get word vectors with dimensionality of 300 , which is trained on 100 billion words from Google News as GoogleW2V .
Evaluation Measure and Algorithm
To evaluate the performance of different systems , the official evaluation measure accuracy is adopted .
We employ the Logistic Regression algorithm with the default parameter implemented in liblinear tools 13 to build the classifiers for its good performance in our preliminary experiments .
The 5 - fold cross validation is adopted for system development .
Experiment
Datasets
In restaurant domain , the opinion tuple is composed of target , category and polarity ( i.e. , < OTE , E#A , Polarity > ) .
And in laptop domain , the OTE is not taken into account in opinion tuple ( i.e. , < E# A , Polarity > ) .
The restaurant domain contains 6 entities ( e.g. , AMBIENCE , DRINKS , FOOD , RESTAURANT , etc ) and 5 attributes ( i.e. , GENERAL , PRICES , STYLE OPTIONS , QUALITY , PRICES , etc ) .
While in laptop , 22 entities ( e.g. , BATTERY , SUPPORT , CPU , COMPANY , etc. ) and 9 attributes ( e.g. , USABILITY , GENERAL , QUALI - TY , etc ) are tagged .
Table 1 shows the statistics of the data sets used in our experiments .
Experiments on Training data
For both laptop and restaurant domains , we adopted similar methods , i.e , employing rich features to build classifiers , and performed constrained systems and unconstrained systems respectively .
Since Sentiment lexicon feature and GoogleW2V feature utilized the external data , we did not use these two types of features in the constrained system .
As for 13 https://www.csie.ntu.edu.tw/ cjlin / liblinear / unconstrained systems , all features were employed .
As for feature selection , a hill climbing algorithm is adopted to find out the contributions of different features , which is described as : keeping adding one type of feature at a time until no further improvement can be achieved .
Table 2 shows the results of feature selection experiments for unconstrained and constrained systems in restaurant and laptop domains .
According to Table 2 , it is interesting to find that ( 1 ) 3 - char , 4 - char and negation are beneficial to this task .
The possible reason may be that there exists a lot of derivations in training data , e.g. , relax and relaxing .
Besides , the negator always reverses the sentiment polarity of corresponding review , which results in the good contribution of negation feature .
( 2 ) SentiLexi features are effective in two domains .
In our preliminary experiments , we found that the SentiLexi features made great contribution to sentiment analysis task , which indicates that this type of features are indeed significant .
( 3 ) POS features are not quite effective in all systems .
The possible reason may be that POS aims at identifying the subjective instances from objective ones , but the objective records just occupy a small proportion .
( 4 ) The majority of features are more valid in unconstrained system than that in constrained system .
The possible reason may be that there are certain overlapped information between the SentiLexi features , the Word2vec feature and the other features .
In our preliminary experiments , we conducted the baseline system where features are extracted from the whole sentence without the consideration of OTE and E#A .
The result showed that the method described in section 2.1 outperformed the baseline .
Thus , we used the strategy that extracting features from certain relevant fragments rather than the whole sentence for this task .
Results and Discussion on test data Using the optimum feature set shown in Table 2 and the algorithm described in section 2.4 , we trained separate models for each domain and evaluated them against the test set in SemEval - 2016 Task 5 .
For both subtask 1 and 2 , we constructed 4 systems for unconstrained and constrained systems in restaurant and laptop domains respectively .
From the
Conclusion
In this paper , we extracted several types of features , i.e. , Linguistic features , SentiLexi features , Topic Model features and Word2vec feature , and employed the Logistic Regression classifier to detect the sen-timent polarity in given aspect for reviews .
Moreover , we have demonstrated a two-step approach to acquire the pending words from the relevant fragments instead of the whole sentences for feature extraction .
This enables the system to capture the relationship between the sentiment of the sentence and its opinion adherent .
The results on test and training data showed the effectiveness of our method for the ABSA task .
For the future work , it would be interesting to explore domain-specific sentiment lexicons to improve the performance and examine more advanced ways of using sentiment lexicons and word embedding features .
Table 1 : 1 Statistics of training and test dataset of two subtasks in laptop and restaurant domains .
Positive , Negative , Neural , Conflict stand for the number of corresponding labels .
Data Reviews Sentences Opinions Positive Negative Neutral Conflict Restaurant ( SB1 ) : train 350 test 90 2,000 676 2,506 859 1,657 611 751 204 98 44 0 0 Laptop ( SB1 ) : train 450 test 80 2,500 808 2,908 801 1,634 481 1,086 274 188 46 0 0 Restaurant ( SB2 ) : train 335 test 90 1,950 676 1,435 404 1012 286 327 84 55 23 41 11 Laptop ( SB2 ) : train 395 test 80 2,373 808 2,082 545 1,210 338 708 162 123 31 41 14
