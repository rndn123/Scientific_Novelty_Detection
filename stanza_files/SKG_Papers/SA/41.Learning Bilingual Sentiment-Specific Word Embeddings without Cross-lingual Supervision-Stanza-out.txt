title
Learning Bilingual Sentiment -Specific Word Embeddings without Cross-lingual Supervision
abstract
Word embeddings learned in two languages can be mapped to a common space to produce Bilingual Word Embeddings ( BWE ) .
Unsupervised BWE methods learn such a mapping without any parallel data .
However , these methods are mainly evaluated on tasks of word translation or word similarity .
We show that these methods fail to capture the sentiment information and do not perform well enough on cross-lingual sentiment analysis .
In this work , we propose UBiSE ( Unsupervised Bilingual Sentiment Embeddings ) , which learns sentiment -specific word representations for two languages in a common space without any cross-lingual supervision .
Our method only requires a sentiment corpus in the source language and pretrained monolingual embeddings of both languages .
We evaluate our method on three language pairs for cross-lingual sentiment analysis .
Experimental results show that our method outperforms previous unsupervised BWE methods and even supervised BWE methods .
Our method succeeds for a distant language pair English - Basque .
Introduction
Lack of annotated corpora degrades the quality of sentiment analysis in low-resource languages .
Cross-lingual sentiment analysis tackles this problem by adapting the sentiment resource in a resource - rich language ( the source language ) to a resource - poor language ( the target language ) .
Bilingual Word Embeddings ( BWE ) provide a way to transfer the sentiment information from the source language to the target language .
There has been an increasing interest in BWE methods in recent years , including both supervised methods and unsupervised methods .
Supervised BWE methods map the word vectors of the two languages in a common space by exploiting either a bilingual seed dictionary or other parallel data , while unsupervised BWE methods do not utilize any form of bilingual supervision .
Yet , these methods are mostly evaluated on tasks of word translation or word similarity , and do not perform well enough on cross-lingual sentiment analysis as shown in Section 4 .
Consider the case where we want to perform sentiment analysis on the target language with merely an annotated sentiment corpus in the source language .
We assume pretrained monolingual embeddings of both languages are available to us .
One solution is to first align the embeddings of both languages in a common space using unsupervised BWE methods , then train a classifier based on the source sentiment corpus .
In this solution , no sentiment information is utilized to learn the alignment .
In this paper , we propose to exploit the sentiment information and learn sentiment -specific alignment .
The sentiment information is gradually incorporated into the BWE through an iterative constraint relaxation procedure .
Unlike previous work which performed alignment in a single direction by linearly mapping the source vectors to the target vector space , we propose an alignment model that maps the vectors of the two languages to a new shared space with two non-linear transformations .
Our model is able to separate positive vectors from negative vectors in the bilingual space and allow such sentiment information to be transferred to the target language .
Our main contributions are as follows : 1 . We propose a novel approach to learn bilingual sentiment -specific word embeddings without any cross-lingual supervision and perform cross-lingual sentiment analysis with minimum resource requirement .
We propose an iterative constraint relaxation pro-cedure that gradually incorporates the sentiment information into the BWE .
Our proposed approach achieves state - of- the - art results .
2 . We introduce a novel sentiment -specific objective without having to explicitly build a classifier .
Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches .
3 . We introduce an alignment-specific objective and a simple re-normalization trick .
Unlike previous BWE methods that learn orthogonal mappings , we introduce non-orthogonal mappings which enable the transfer of sentiment information from the source language to the target language .
Related Work Cross-Lingual Sentiment Analysis
Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories : ( i ) approaches that rely on machines translation ( MT ) systems ( ii ) approaches that rely on cross-lingual word embeddings .
Standard MT - based approaches perform crosslingual sentiment analysis by translating the sentiment data into a selected language ( e.g. English ) .
More sophisticated algorithms including co-training ( Wan , 2009 ; Demirtas and Pechenizkiy , 2013 ) and multi-view learning ( Xiao and Guo , 2012 ) have been shown to improve performance .
Zhou et al . ( 2015
Zhou et al . ( , 2016 b performed crosslingual sentiment analysis by learning bilingual document representations .
These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version .
Bilingual Word Embeddings
Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings ( BWE ) , which support many NLP tasks including machine translation , cross-lingual sentiment analysis ( Barnes et al. , 2018 ; Zhou et al. , 2015 ) and crosslingual dependency parsing ( Guo et al. , 2015 ) .
BWE can be obtained in a supervised way using a seed dictionary Artetxe et al. , 2016 ) , or in an unsupervised way without any bilingual data .
Adversarial training was the first successful attempt to learn unsupervised BWE ( Zhang et al. , 2017 ; . Selflearning was proposed by ( Artetxe et al. , 2017 ) to learn BWE with minimum bilingual resources , which was later extended into a fully unsupervised framework by adding an unsupervised dictionary initialization step ( Artetxe et al. , 2018 ) . Multilingual Word Embeddings BWE methods can be extended to the case of multiple languages by simply mapping all the languages to the vector space of a selected language .
However , directly learning multilingual word embeddings ( MWE ) in a shared space has been shown to improve performance ( Ammar et al. , 2016 ; Duong et al. , 2017 ; Chen and Cardie , 2018 ; Alaux et al. , 2018 ) .
Yet , all these approaches are mainly evaluated on word translation and their effectiveness on cross-lingual sentiment analysis have not been empirically compared .
Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity .
This drawback makes them hard to distinguish words with similar syntactic context but opposite sentiment polarity ( e.g. good and bad ) , resulting in unsatisfactory performance on sentiment analysis .
Tang et al. ( 2014 ) learned word representations that encode both syntactic context and sentiment polarity by adding an objective to classify the polarity of an n-gram .
This method can be generalized to the cross-lingual setting by training monolingual sentimental embeddings on both languages then aligning them in a common space .
However , it requires sentiment resources in the target language thus is impractical for low-resource languages .
There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language .
Barnes et al. ( 2018 ) jointly minimized an alignment objective based on a seed dictionary , and a classification objective based on the sentiment corpus .
Its performance is compared to our method in Section 4 .
Xu and Wan ( 2017 ) learned multilingual sentimental embeddings by extending the BiSkip model ( Luong et al. , 2015 ) .
However , their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not included in our experiments .
3 Proposed Method
The Overall Framework
This subsection first introduces the proposed mappings for aligning the monolingual embeddings in the bilingual space , then describes the general selflearning algorithm used to learn these bilingual mappings .
The details of our algorithm are explained in Section 3.2 - Section 3.6 .
The Alignment Model
We assume we have normalized monolingual embeddings S ? R v?d and T ? R v?d , where the i-th row of S is the vector representation of word i in the source language .
The normalization procedure is as follows : ( i ) l 2 - normalize each vector ( ii ) center the vectors ( iii ) l 2 - normalize each vector again ( Artetxe et al. , 2018 ) .
Given these monolingual embeddings , existing BWE methods typically learn a projection matrix W ? R d?d from the source vector space to the target vector space .
However , these methods are unsuitable in our setting for two reasons : ( i ) most methods constrain W to be orthogonal or near-orthogonal , thus preserving distances between word vectors ; ( ii ) word vectors in the target language space remain unchanged .
These two properties prevent us from separating words with opposite sentiment polarity in the bilingual space .
In this work , we propose to align the monolingual embeddings with two non-linear mappings : f s ( x ) = W s x W s x f t ( x ) = W t x W t x where ? denotes the l 2 - norm , W s ( W t ) is the projection matrix for the source ( target ) embeddings , and x is a d-dimension word vector .
Each mapping can be seen as a linear projection followed by a re-normalization step .
We propose the following convex domain D = { W ? R d?d | W 2 ? r} as an alternative for the orthogonal constraint , where ?
2 denotes the spectral norm and r is a hyperparameter that determines to what extent we want to preserve word distances .
This is inspired by the unit spectral norm constraint proposed by .
The Self-learning Procedure Given a bilingual seed dictionary , we can learn the projection matrices W s and W t by forcing the word pairs in the dictionary to have similar representations in the bilingual space .
In the unsupervised case , such a dictionary can be induced from the monolingual embeddings S and T ( Artetxe et al. , 2018 ) .
However , the quality of this dictionary is usually not good , which in turn degrades the quality of the projection matrices learned from this dictionary .
Previous work ( Artetxe et al. , 2017 ( Artetxe et al. , , 2018 showed that an iterative self- learning procedure can induce a good bilingual dictionary and hence good projection matrices .
Given an initial dictionary D bi , this procedure iterates over two steps : ( i ) it aligns the monolingual embeddings in a common space based on D bi , yielding S and T ; ( ii ) it computes a new dictionary D bi using nearest neighbour retrieval over the approximately aligned embeddings S and T .
In our method , there are three objects W s , W t and D bi to update through the self-learning procedure .
Thus we iterates over the following three steps : 1 . Solve W s by minimizing a sentimentspecific objective L s over D , as described in Section 3.3 ; 2 . Solve W t by minimizing an alignmentspecific objective L t over D , as described in Section 3.4 ; 3 . Derive a new bilingual dictionary D bi based on S = SW s and T = TW t , as described in Section 3.5 .
Re-normalization is applied as a final step after we have obtained W s and W t .
Preliminaries
Unsupervised Bilingual Dictionary Initialization
The normalized embeddings S and T are not aligned along the first axis , i.e. , the i-th row of S does not correspond to the i-th row of T .
Therefore , an initial bilingual dictionary is required in order to access the correspondence between the two languages .
Following ( Artetxe et al. , 2018 ) , we first compute the similarity matrices M s = ?
SS and M t = ?
TT , sort them along the second axis and normalize the rows , yielding M s and M t .
For each row in M s , we apply nearest neighbour retrieval over the rows of M t to find its corresponding translation , yielding a dictionary D s?t = { ( 1 , T s?t ( 1 ) ) , ( 2 , T s?t ( 2 ) ) , . . . , ( v , T s?t ( v ) ) } , where T s?t ( i ) is the translation of the source word i .
The same procedure is repeated in the other direction , yielding D t?s .
The two dictionaries are then concatenated to produce the initial bilingual dictionary D bi = D s?t ? D t?s .
Learning Sentiment -Specific Vectors
In order to incorporate the sentiment information into the bilingual word embeddings , we need a set of d-dimension vectors with known sentiment polarity .
We propose a neural network based approach to learn these sentiment -specific vectors .
Let the training corpus in the source language be C = {( z 1 , y 1 ) , ( z 2 , y 2 ) , . . . , ( z | C | , y | C | ) } , where z i is a text and y i is its corresponding label .
A ddimension vector with sentiment polarity y i can be obtained by calculating the weighted average of the word vectors in z i : h i = j?z i exp ( ?
j ) S j? j?z i exp ( ? j ) ( 1 ) where S j? is the vector representation of the word j in the source language ( corresponding to the jth row of S ) and ?
j is a scalar that scores the importance of word j on the sentiment polarity .
? j is computed by ? j = max ( AS j? + b ) , where A ? R h?d and b ?
R h are the parameters to learn .
This function can be seen as a convolution layer with h filters followed by a max pooling layer .
The number of filters h is set to 4 .
Each h i is then forwarded to a linear classifier to predict the sentiment label y i .
Once we have trained the model by minimizing the cross-entropy loss , we re-compute h i for each training example z i .
We denote the set of vectors ( i.e. , h i ) with positive labels as P = {h p 1 , h p 2 , . . . , h p | P | } and the set of vectors with negative labels as N = {h n 1 , h n 2 , . . . , h n | N | }.
In the 4 - class setup , we have four sets : P , N , SP ( the set of strongly positive vectors ) , SN ( the set of strongly negative vectors ) .
Solving W s Given a set of positive d-dimension vectors P = {h p 1 , h p 2 , . . . , h p | P | } and a set of negative ddimension vectors N = {h n 1 , h n 2 , . . . , h n |N | } ( or four sets in the 4 - class setup ) , our goal is to distinguish the positive vectors from the negative vectors in the bilingual space , i.e. , to separate W s h p i from W s h n j for any pair of i , j .
We introduce a new d-dimension vector a p ?
O = {x ?
R d | x ? 1 } to represent the " positive direction " , which is to be learned .
In order to separate positive vectors from negative vectors in the bilingual space , we try to make W s h p i ( i = 1 , . . . , | P | ) to be close to a p and W s h n i ( i = 1 , . . . , | N | ) to be distant from a p .
For a given a p , we first compute a p W s h p i for i = 1 , 2 , . . . , | P | and denote the set of i with ?|P | smallest values as Q p + , where ? ? [ 0 , 1 ] is a hyperparameter 1 . These W s h p i are least similar with a p ( dot product is used as the similarity metric ) , hence we maximize the average of a p W s h p i over Q p + .
Likewise , we denote the set of i ? { 1 , 2 , . . . , |N | } with ?|N
| largest values of a p W s h n i as Q p ? .
These W s h n i are most similar to a p , hence we minimize the average of a p W s h n i over Q p ? .
The overall objective is as follows : min Ws ?D a p ?O L s ( W s , a p ) = L ( W s , a p , P , N ) ? = ? 1 ?|P| i?Q p + a p W s h p i +
1 ?|N | i?Q p ? a p W s h n i ( 2 ) where D is the convex set defined in Section 3.1.1 .
The rationale for this objective is that , instead of forcing every W s h p i to be close to a p , we only focus on a fraction of positive vectors that are most distant from a p , and vice versa for those negative vectors .
We observe that this objective can be rewritten as : min Ws ?
D a p ?O L s ( W s , a p ) = 1 ?|P | max Q?S ?|P | ( | P | ) ? i?Q a p W s h p i + 1 ?|N | max Q?S ?|N | ( |N | ) i?Q a p W s h n i ( 3 ) where S ?|P | ( | P | ) represents all subsets of { 1 , 2 , . . . , | P| } of size ?| P | , and S ?|N | ( |N | ) is similarly defined .
2
This formulation shows that both terms of this objective can be seen as a maximum of linear functions of either W s or a p .
Therefore , our objective is convex with respect to either W s or a p , thus can be efficiently minimized by using the projected gradient descent algorithm .
We first minimize this objective with respect to a p over O , then minimize it with respect to W s over D .
While this objective is useful in the binary setup , it does not separate a strongly positive vector in SP from a weakly positive vector in P ( similarly for SN and N ) .
In order to achieve better performance in the 4 - class setup , we adopt the one-versus-rest strategy to write L s as the sum of four terms : min Ws ?
D a p ?O a sp ?O a n ?O a sn ?O L s ( W s , a p , a sp , a n , a sn ) = L ( W s , a p , P , N ? SP ? SN ) + L ( W s , a sp , SP , P ? N ? SN ) + L ( W s , a n , N , P ? SP ? SN ) + L ( W s , a sn , SN , P ? SP ? N ) ( 4 ) where L is defined in Eq. ( 2 ) and a c is a ddimension vector representing the " direction " of class c.
Solving W t Based on the current bilingual dictionary D bi , we construct two sets of vectors { x s 1 , x s 2 , . . . , x s 2v } and { x t 1 , x t 2 , . . . , x t 2 v } , where x s i and x t i are the vector representations of the i-th word pair in D bi .
With W s fixed , we can solve W t by minimizing : min Wt ?D L t ( W t ) = 2 v i=1 W s x s i ?
W t x t i 2 ( 5 ) where D is the convex set defined in Section 3.1.1 .
This objective is convex with respect to W t , thus can be minimized efficiently by using the projected gradient descent algorithm .
Bilingual Dictionary Induction
Once we have computed W s and W t , we can obtain the aligned embeddings S = SW s and T = TW t .
Then we induce a new dictionary D bi using nearest neighbour retrieval over the rows of S and T .
We perform the induction in two directions to produce D s?t and D t?s , then concatenate them to produce D bi .
In this work , we propose a modified version of CSLS to be used as the similarity metric to preform nearest neighbour retrieval : CSLS ( x , y ) = x y ?
1 k y ?N Y ( x ) x y ?
1 k x ?N X ( y ) x y ( 6 ) where N Y ( x ) is the set of k nearest neighbours of x in the set of vectors Y ( in our case Y is the set of rows of T ) .
We set k to 10 following the original paper .
Iterative Constraint Relaxation
As mentioned in Section 3.1.1 , we introduce a hyperparameter r to define the convex domain D .
There is a trade- off to make for r : a large r better incorporates sentimental similarity but significantly harms the quality of the alignment , while a small r constrains W s to be near-orthogonal thus prevents it to capture the sentimental similarity .
In order to address this problem , we propose to first set r to 1 , letting the the monolingual embeddings to be properly aligned .
Then r is iteratively increased by ?r , causing the positive vectors in the bilingual space to be gradually moved further away from the negative vectors .
The training process stops when r reaches a maximum value r max , where r max is a hyperparameter 3 .
The pseudo code of UBISE in the binary setup is shown in Algorithm 1 .
For the 4 - class UBISE , lines 3,6,7 are replaced by their counterparts in the 4 - class setup .
Experiments
Datasets
We use the multilingual sentiment dataset provided by ( Barnes et al. , 2018 ) .
It contains annotated hotel reviews in English ( EN ) , Spanish ( ES ) , Catalan ( CA ) and Basque ( EU ) .
In our experiment , we use EN as the source language and ES , CA , EU as the target languages .
For each target language , the dataset is divided into a target development set and a target test set .
We also combine the strong and weak labels to produce a binary setup .
Algorithm 1 binary UBISE Input : ? , r max , ?r , S , T , C Output : S , T , W s , W t 1 : r ? 1 2 : Initialize W s and W t to identity matrices 3 : Learn P , N from S and C , according to Section 3.2.2 4 : Compute the initial bilingual dictionary D bi from S and T , according to Section 3.2.1 5 : while r ?
r max do 6 : a p ? argmin a p ?O
L s ( a p , W s ) 7 : W s ? argmin Ws ?D L s ( a p , W s ) 8 : W t ? argmin Wt ?D L t ( W t ) 9 : S ? SW s 10 : T ? TW t 11 : Derive a new bilingual dictionary D bi from S and T , according to Section 3.5 12 : r ? r + ?r 13 : end while 14 : Normalize the rows of S , T to unit length 15 : return S , T , W s , W t The normalized 300 - dimension fastText vectors ( Bojanowski et al. , 2017 ) are used by all methods .
The MUSE dataset ) is used by approaches that require bilingual supervision 4 . Each dictionary contains 5000 unique source words .
Implementation details
We empirically set ?r = 0.01 and v = 10000 .
The vocabulary of each language is limited to the v most frequent words so that the embedding matrix has shape v ? d. Hyper parameters ? and r max are tuned on the target development set via a grid search .
We apply stochastic dictionary induction by randomly setting the elements of the similarity matrix used for nearest neighbour retrieval to zero with probability 1 ? p , as described in ( Artetxe et al. , 2018 ) . p is initialized to 0.1 and increased by 0.005 at each iteration .
We empirically stop updating the dictionary when r exceeds 3 .
Baselines
We compare our method with the following baselines , including state - of - the - art BWE methods that are originally evaluated on the word translation task , as well as bilingual sentimental embed-dings methods that are optimized for cross-lingual sentiment analysis .
The bilingual word embeddings learned by each method are later evaluated on cross-lingual sentiment analysis using the same classifier for fairness .
ADVERSARIAL Conneau et al. ( 2017 ) proposed an unsupervised BWE method based on adversarial training .
After a near-orthogonal projection matrix is learned through adversarial training , a refinement procedure is applied to improve the quality of the alignment .
Unsupervised BWE Methods VECMAP
Artetxe et al. ( 2018 ) proposed an unsupervised BWE learning framework .
It consists of an unsupervised dictionary initialization step and the self-learning procedure mentioned in Section 3.1.2 .
PROCRUSTES
Artetxe et al. ( 2016 ) proposed a simple and effective supervised BWE method that requires a seed dictionary .
It computes the optimal projection matrix by taking singular value decomposition ( SVD ) .
Supervised BWE Methods RCSLS proposed an supervised BWE method that also requires a seed dictionary .
They proposed a training objective that is consistent with the retieval criterion that can be minimized by using gradient descent .
It achieves state - of - the - art results on the word translation task .
Bilingual Sentimental Embedding Methods BLSE
Barnes et al. ( 2018 ) exploited both bilingual supervision and the sentiment corpus to learn bilingual sentimental embeddings .
They jointly minimize an alignment -specific objective and a classification objective to learn the projection matrices .
The trade- off between the two objectives is controlled by a hyperparameter ? ? [ 0 , 1 ] .
We tune ? on the target development set as described in the original paper .
Once the projection matrices have been learned , the classifier in this model is abandoned .
The quality of the resulting BWE is evaluated using the classifier mentioned in Section 4.4 .
Evaluation
We use DAN ( Iyyer et al. , 2015 ) as the classifier to preform cross-lingual sentiment analysis .
The loss of each instance is weighted by its inverse class frequency to address the class imbalance problem .
For each method , the dropout rate is fixed at 0.3 and the l 2 - regularization strength is tuned on the target development set 5 .
We train five classifiers for each method and report the average macro - F1 on the target test set .
Results and Analysis Table 1 presents the results of different BWE methods .
UBISE outperforms all unsupervised methods on all six tasks and outperforms all baselines on four out of six tasks .
All methods , especially unsupervised methods , suffer from distant language pairs , which is consistent with the observation of ( S?gaard et al. , 2018 ) . VECMAP and ADVERSARIAL perform significantly worse on EN - EU compared to supervised methods .
Yet , UBISE outperforms the strongest baseline by 2.1 % on EN - EU , indicating that incorporating sentiment is vital for crosslingual sentiment analysis on distant languages .
Despite the similar performance across different BWE methods in the binary setup , UBISE outperform all baselines in the 4 - class setup by a large margin ( average of + 2.2 % ) .
This may indicate that the original monolingual embeddings are able to distinguish positive words from negative words ( e.g. , good and bad ) , but bad at distinguishing strongly positive words from weakly positive words ( e.g. , good and perfect ) .
The performance of BLSE is merely comparative with other baselines .
6
We suspect that this is due to the classifier we use to perform crosslingual sentiment analysis .
The original paper used SVM or logistic regression to perform classification , in which case BLSE achieved better performance due to the utilization of sentiment information .
But if we use a deeper neural network to perform cross-lingual sentiment analysis , preserving the original semantic similarity is more important .
A qualitative comparison between BLSE and UBISE is presented in Section 4.8 .
Effect of the Sentiment Information
We perform an ablation test to demonstrate the effect of the sentiment information provided by L s .
We create a new model UBISE MIN that does not utilize the sentiment information by eliminating lines 6,7,12 in Algorithm 1 . UBISE MIN runs 500 iterations for every language pairs .
The comparative results in Table 2 show that utilizing the sentiment information leads to an average improvement of + 3.1 % in the binary setup or + 4.1 % in the 4 - class setup .
Effect of Re-Normalization Re-normalization is useful in the sense that it leads to better alignment by constraining all the bilingual vectors to be on the unit sphere .
While this property does not matter for word translation as long as cosine-similarity is used as the retrieval criterion , it matters for cross-lingual sentiment analysis .
Another effect of re-normalization is that it introduces non-linearity between the linear projection and the classifier , which is vital for separating words with opposite sentiment polarity .
Without non-linearity the linear projection and the first layer of the classifier would collapse into a single linear projection , thus eliminating the effect of W s .
Figure 2 illustrates how this nonlinearity helps separating positive words from negative words in the bilingual space .
This effect is demonstrated in Section 4.8 .
Visualization of the Bilingual Space
To illustrate how UBISE transfers sentiment information from the source language to the target language , we visualize six categories of words in the bilingual space of UBISE and BLSE using t-SNE ( Maaten and Hinton , 2008 ) .
As shown in Figure 1 , both methods manage to separate positive words from negative words without any annotated data in Spanish .
However , Barnes et al. ( 2018 ) abandon the original semantic similarity , which degrades its performance as shown in Section 4.5 .
In contrast , our method preserves semantic similarity by limiting the largest singular values of W s and W t to be smaller than r max .
The trade- off between semantic similarity and sentimental similarity is made by choosing an appropriate r max .
Conclusion
This paper presents a method to learn bilingual sentiment -specific word embeddings without any cross-lingual supervision .
We propose a novel sentiment -specific objective that separates words with opposite sentiment polarity in the bilingual space , and an alignment objective that enables the transfer of sentiment information from the source language to the target language .
An iterative constraint relaxation procedure is applied to gradually incorporate the sentiment information into the bilingual word embeddings .
We empirically evaluate our method on three language pairs for cross-lingual sentiment analysis and demonstrate its effectiveness .
Experimental results show that incorporating sentiment information significantly improves the performance on fine- grained crosslingual sentiment analysis .
Figure 2 : 2 Figure 2 : Illustration of the effect of re-normalization .
( a ) the original normalized embeddings ( b ) embeddings after linear projection ( c ) embeddings after renormalization
