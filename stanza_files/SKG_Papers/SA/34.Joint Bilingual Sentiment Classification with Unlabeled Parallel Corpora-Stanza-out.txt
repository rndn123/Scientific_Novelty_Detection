title
Joint Bilingual Sentiment Classification with Unlabeled Parallel Corpora
abstract
Most previous work on multilingual sentiment analysis has focused on methods to adapt sentiment resources from resource - rich languages to resource -poor languages .
We present a novel approach for joint bilingual sentiment classification at the sentence level that augments available labeled data in each language with unlabeled parallel data .
We rely on the intuition that the sentiment labels for parallel sentences should be similar and present a model that jointly learns improved monolingual sentiment classifiers for each language .
Experiments on multiple data sets show that the proposed approach ( 1 ) outperforms the monolingual baselines , significantly improving the accuracy for both languages by 3.44 % -8.12 % ; ( 2 ) outperforms two standard approaches for leveraging unlabeled data ; and ( 3 ) produces ( albeit smaller ) performance gains when employing pseudo- parallel data from machine translation engines .
Introduction
The field of sentiment analysis has quickly attracted the attention of researchers and practitioners alike ( e.g. Pang et al. , 2002 ; Turney , 2002 ; Hu and Liu , 2004 ; Wiebe et al. , 2005 ; Breck et al. , 2007 ; Pang and Lee , 2008 ) .
1 Indeed , sentiment analysis systems , which mine opinions from textual sources ( e.g. news , blogs , and reviews ) , can be used in a wide variety of applications , including interpreting product reviews , opinion retrieval and political polling .
Not surprisingly , most methods for sentiment classification are supervised learning techniques , which require training data annotated with the appropriate sentiment labels ( e.g. document - level or sentence - level positive vs. negative polarity ) .
This data is difficult and costly to obtain , and must be acquired separately for each language under consideration .
Previous work in multilingual sentiment analysis has therefore focused on methods to adapt sentiment resources ( e.g. lexicons ) from resourcerich languages ( typically English ) to other languages , with the goal of transferring sentiment or subjectivity analysis capabilities from English to other languages ( e.g. Mihalcea et al. ( 2007 ) ; Banea et al. ( 2008 ; ; Wan ( 2008 ; 2009 ) ; Prettenhofer and Stein ( 2010 ) ) .
In recent years , however , sentiment - labeled data is gradually becoming available for languages other than English ( e.g. Seki et al . ( 2007 ; 2008 ) ; Nakagawa et al. ( 2010 ) ; Schulz et al . ( 2010 ) ) .
In addition , there is still much room for improvement in existing monolingual ( including English ) sentiment classifiers , especially at the sentence level ( Pang and Lee , 2008 ) .
This paper tackles the task of bilingual sentiment analysis .
In contrast to previous work , we ( 1 ) assume that some amount of sentimentlabeled data is available for the language pair under study , and ( 2 ) investigate methods to simultaneously improve sentiment classification for both languages .
Given the labeled data in each language , we propose an approach that exploits an unlabeled parallel corpus with the following intuition : two sentences or documents that are parallel ( i.e. translations of one another ) should exhibit the same sentimenttheir sentiment labels ( e.g. polarity , subjectivity , intensity ) should be similar .
The proposed maximum entropy - based EM approach jointly learns two monolingual sentiment classifiers by treating the sentiment labels in the unlabeled parallel text as unobserved latent variables , and maximizes the regularized joint likelihood of the language -specific labeled data together with the inferred sentiment labels of the parallel text .
Although our approach should be applicable at the document- level and for additional sentiment tasks , we focus on sentence - level polarity classification in this work .
We evaluate our approach for English and Chinese on two dataset combinations ( see Section 4 ) and find that the proposed approach outperforms the monolingual baselines ( i.e. maximum entropy and SVM classifiers ) as well as two alternative methods for leveraging unlabeled data ( transductive SVMs ( Joachims , 1999 b ) and cotraining ( Blum and Mitchell , 1998 ) ) .
Accuracy is significantly improved for both languages , by 3.44 % -8.12 % .
We furthermore find that improvements , albeit smaller , are obtained when the parallel data is replaced with a pseudo-parallel ( i.e. automatically translated ) corpus .
To our knowledge , this is the first multilingual sentiment analysis study to focus on methods for simultaneously improving sentiment classification for a pair of languages based on unlabeled data rather than resource adaptation from one language to another .
The rest of the paper is organized as follows .
Section 2 introduces related work .
In Section 3 , the proposed joint model is described .
Sections 4 and 5 , respectively , provide the experimental setup and results ; the conclusion ( Section 6 ) follows .
Related Work Multilingual Sentiment Analysis .
There is a growing body of work on multilingual sentiment analysis .
Most approaches focus on resource adaptation from one language ( usually English ) to other languages with few sentiment resources .
Mihalcea et al. ( 2007 ) , for example , generate subjectivity analysis resources in a new language from English sentiment resources by leveraging a bilingual dictionary or a parallel corpus .
Banea et al. ( 2008 ; instead automatically translate the English resources using automatic machine translation engines for subjectivity classification .
Prettenhofer and Stein ( 2010 ) investigate crosslingual sentiment classification from the perspective of domain adaptation based on structural correspondence learning ( Blitzer et al. , 2006 ) .
Approaches that do not explicitly involve resource adaptation include Wan ( 2009 ) , which uses co-training ( Blum and Mitchell , 1998 ) with English vs .
Chinese features comprising the two independent - views ?
to exploit unlabeled Chinese data and a labeled English corpus and thereby improves Chinese sentiment classification .
Another notable approach is the work of Boyd - Graber and Resnik ( 2010 ) , which presents a generative model --- supervised multilingual latent Dirichlet allocation --- that jointly models topics that are consistent across languages , and employs them to better predict sentiment ratings .
Unlike the methods described above , we focus on simultaneously improving the performance of sentiment classification in a pair of languages by developing a model that relies on sentimentlabeled data in each language as well as unlabeled parallel text for the language pair .
Semi-supervised Learning .
Another line of related work is semi-supervised learning , which combines labeled and unlabeled data to improve the performance of the task of interest ( Zhu and Goldberg , 2009 ) .
Among the popular semisupervised methods ( e.g. EM on Na?ve Bayes ( Nigam et al. , 2000 ) , co-training ( Blum and Mitchell , 1998 ) , transductive SVMs ( Joachims , 1999 b ) , and co-regularization ( Sindhwani et al. , 2005 ; Amini et al. , 2010 ) ) , our approach employs the EM algorithm , extending it to the bilingual case based on maximum entropy .
We compare to co-training and transductive SVMs in Section 5 .
Multilingual NLP for Other Tasks .
Finally , there exists related work using bilingual resources to help other NLP tasks , such as word sense disambiguation ( e.g. Ido and Itai ( 1994 ) ) , parsing ( e.g. Burkett and Klein ( 2008 ) ; Zhao et al . ( 2009 ) ; Burkett et al. ( 2010 ) ) , information retrieval ( Gao et al. , 2009 ) , named entity detection ( Burkett et al. , 2010 ) ; topic extraction ( e.g. Zhang et al. , 2010 ) , text classification ( e.g. Amini et al. , 2010 ) , and hyponym-relation acquisition ( e.g. Oh et al. , 2009 .
In these cases , multilingual models increase performance because different languages contain different ambiguities and therefore present complementary views on the shared underlying labels .
Our work shares a similar motivation .
A Joint Model with Unlabeled Parallel Text
We propose a maximum entropy - based statistical model .
Maximum entropy ( MaxEnt ) models 1 have been widely used in many NLP tasks ( Berger et al. , 1996 ; Ratnaparkhi , 1997 ; Smith , 2006 ) .
The models assign the conditional probability of the label given the observation as follows : ( 1 ) where is a real-valued vector of feature weights and is a feature function that maps pairs to a nonnegative real-valued feature vector .
Each feature has an associated parameter , , which is called its weight ; and is the corresponding normalization factor .
Maximum likelihood parameter estimation ( training ) for such a model , with a set of labeled examples , amounts to solving the following optimization problem : ( 2 )
Problem Definition
Given two languages and , suppose we have two distinct ( i.e. not parallel ) sets of sentimentlabeled data , and written in and respectively .
In addition , we have unlabeled ( w.r.t. sentiment ) bilingual ( in and ) parallel data that are defined as follows .
where denotes the polarity of the - th instance ( positive or negative ) ; and are respectively the numbers of labeled instances in and ; and are parallel instances in and , respectively ( i.e. they are supposed to be translations of one another ) , whose labels and are unobserved , but according to the intuition outlined in Section 1 , should be similar .
Given the input data and , our task is to jointly learn two monolingual sentiment classifiers one for and one for .
With MaxEnt , we learn from the input data : where and are the vectors of feature weights for and , respectively ( for brevity we denote them as and in the remaining sections ) .
In this study , we focus on sentence - level sentiment classification , i.e. each is a sentence , and and are parallel sentences .
The Joint Model Given the problem definition above , we now present a novel model to exploit the correspondence of parallel sentences in unlabeled bilingual text .
The model maximizes the following joint likelihood with respect to and : ( 3 ) where denotes or ; the first term on the right - hand side is the likelihood of labeled data for both and ; and the second term is the likelihood of the unlabeled parallel data .
If we assume that parallel sentences are perfect translations , the two sentences in each pair should have the same polarity label , which gives us : ( 4 ) where is the unobserved class label for the - th instance in the unlabeled data .
This probability directly models the sentiment label agreement between and .
However , there could be considerable noise in real- world parallel data , i.e. the sentence pairs may be noisily parallel ( or even comparable ) instead of fully parallel ( Munteanu and Marcu , 2005 ) .
In such noisy cases , the labels ( positive or negative ) could be different for the two monolingual sentences in a sentence pair .
Although we do not know the exact probability that a sentence pair exhibits the same label , we can approximate it using their translation probabilities , which can be computed using word alignment toolkits such as Giza + + ( Och and Ney , 2003 ) or the Berkeley word aligner ( Liang et al. , 2006 ) .
The intuition here is that if the translation probability of two sentences is high , the probability that they have the same sentiment label should be high as well .
Therefore , by considering the noise in parallel data , we get : ( 5 ) where is the translation probability of the - th sentence pair in ; 2 is the opposite of ; the first term models the probability that and have the same label ; and the second term models the probability that they have different labels .
By further considering the weight to ascribe to the unlabeled data vs.
the labeled data ( and the weight for the L2- norm regularization ) , we get the following regularized joint log likelihood to be maximized : ( 6 ) where the first term on the right - hand side is the log likelihood of the labeled data from both and the second is the log likelihood of the unlabeled parallel data , multiplied by , a constant that controls the contribution of the unlabeled data ; and is a regularization constant that penalizes model complexity or large feature weights .
When is 0 , the algorithm ignores the unlabeled data and degenerates to two MaxEnt models trained on only the labeled data .
The EM Algorithm on MaxEnt
To solve the optimization problem for the model , we need to jointly estimate the optimal parameters for the two monolingual classifiers by finding : ( 7 )
This can be done with an EM algorithm , whose steps are summarized in Algorithm 1 .
First , the MaxEnt parameters , and , are estimated from just the labeled data .
Then , in the E-step , the classifiers , based on current values of and , compute for each labeled example and assign probabilistically - weighted class labels to each unlabeled example .
Next , in the M-step , the parameters , and , are updated using both the original labeled data ( and ) and the newly labeled data .
These last two steps are iterated until convergence or a predefined iteration limit .
In the M-step , we can optimize the regularized joint log likelihood using any gradient - based optimization technique ( Malouf , 2002 ) .
The gradient for Equation 3 based on Equation 4 is shown in Appendix A ; those for Equations 5 and 6 can be derived similarly .
In our experiments , we use the L-BFGS algorithm ( Liu et al. , 1989 ) and run EM until the change in regularized joint log likelihood is less than 1e - 5 or we reach 100 iterations .
3
Pseudo-Parallel Labeled and Unlabeled Data
We also consider the case where a parallel corpus is not available : to obtain a pseudo-parallel corpus ( i.e. sentences in one language with their corresponding automatic translations ) , we use an automatic machine translation system ( e.g. Google machine translation 4 ) to translate unlabeled indomain data from to or vice versa .
Since previous work ( Banea et al. , 2008 ; Wan , 2009 ) has shown that it could be useful to automatically translate the labeled data from the source language into the target language , we can further incorporate such translated labeled data into the joint model by adding the following component into Equation 6 : ( 8 ) where is the alternative class of , is the automatically translated example from ; and is a constant that controls the weight of the translated labeled data .
Experimental Setup
Data Sets and Preprocessing
The following labeled datasets are used in our experiments .
MPQA ( Labeled English Data ) : The Multi-Perspective Question Answering ( MPQA ) corpus ( Wiebe et al. , 2005 ) consists of newswire documents manually annotated with phrase -level subjectivity information .
We extract all sentences containing strong ( i.e. intensity is medium or higher ) , sentiment - bearing ( i.e. polarity is positive or negative ) expressions following Choi and Cardie ( 2008 ) .
Sentences with both positive and negative strong expressions are then discarded , and the polarity of each remaining sentence is set to that of its sentiment - bearing expression ( s ) .
NTCIR-EN ( Labeled English Data ) and NTCIR -CH ( Labeled Chinese Data ) :
The NTCIR Opinion Analysis task ( Seki et al. , 2007 ; 2008 ) provides sentiment - labeled news data in Chinese , Japanese and English .
Only those sentences with a polarity label ( positive or negative ) agreed to by at least two annotators are extracted .
We use the Chinese data from NTCIR - 6 as our Chinese labeled data .
Since far fewer sentences in the English data pass the annotator agreement filter , we combine the English data from NTCIR - 6 and NTCIR -7 .
The Chinese sentences are segmented using the Stanford Chinese word segmenter ( Tseng et al. , 2005 ) .
The number of sentences in each of these datasets is shown in Table 1 .
In our experiments , we evaluate two settings of the data : ( 1 ) MPQA + NTCIR -CH , and ( 2 ) NTCIR-EN + NTCIR -CH .
In each setting , the English labeled data constitutes and the Chinese labeled data , .
We also try to remove neutral sentences from the parallel data since they can introduce noise into our model , which deals only with positive and negative examples .
To do this , we train a single classifier from the combined Chinese and English labeled data for each data setting above by concatenating the original English and Chinese feature sets .
We then classify each unlabeled sentence pair by combining the two sentences in each pair into one .
We choose the most confidently predicted 10,000 positive and 10,000 negative pairs to constitute the unlabeled parallel corpus for each data setting .
Baseline Methods
In our experiments , the proposed joint model is compared with the following baseline methods .
MaxEnt :
This method learns a MaxEnt classifier for each language given the monolingual labeled data ; the unlabeled data is not used .
SVM : This method learns an SVM classifier for each language given the monolingual labeled data ; the unlabeled data is not used .
SVM - light ( Joachims , 1999a ) is used for all the SVM - related experiments .
Monolingual TSVM ( TSVM - M ) :
This method learns two transductive SVM ( TSVM ) classifiers given the monolingual labeled data and the monolingual unlabeled data for each language .
Bilingual TSVM ( TSVM - B ) :
This method learns one TSVM classifier given the labeled training data in two languages together with the unlabeled sentences by combining the two sentences in each unlabeled pair into one .
We expect this method to perform better than TSVM - M since the combined ( bilingual ) unlabeled sentences could be more helpful than the unlabeled monolingual sentences .
Co-Training with SVMs ( Co - SVM ) :
This method applies SVM - based co-training given both the labeled training data and the unlabeled parallel data following Wan ( 2009 ) .
First , two monolingual SVM classifiers are built based on only the corresponding labeled data , and then they are bootstrapped by adding the most confident predicted examples from the unlabeled data into the training set .
We run bootstrapping for 100 iterations .
In each iteration , we select the most confidently predicted 50 positive and 50 negative sentences from each of the two classifiers , and take the union of the resulting 200 sentence pairs as the newly labeled training data .
( Examples with conflicting labels within the pair are not included . )
Results and Analysis
In our experiments , the methods are tested in the two data settings with the corresponding unlabeled parallel corpus as mentioned in Section 4 .
6 We use 6
The results reported in this section employ Equation 4 . Preliminary experiments showed that Equation 5 does not significantly improve the performance in our case , which is reasonable since we choose only sentence pairs with the highest translation probabilities to be our unlabeled data ( see Section 4.1 ) .
5 - fold cross-validation and report average accuracy ( also MicroF1 in this case ) and MacroF1 scores .
Unigrams are used as binary features for all models , as Pang et al . ( 2002 ) showed that binary features perform better than frequency features for sentiment classification .
The weights for unlabeled data and regularization , and , are set to 1 unless otherwise stated .
Later , we will show that the proposed approach performs well with a wide range of parameter values .
7
Method Comparison
We first compare the proposed joint model ( Joint ) with the baselines in Table 2 .
As seen from the table , the proposed approach outperforms all five baseline methods in terms of both accuracy and MacroF1 for both English and Chinese and in both of the data settings .
8
By making use of the unlabeled parallel data , our proposed approach improves the accuracy , compared to MaxEnt , by 8.12 % ( or 33.27 % error reduction ) on English and 3.44 % ( or 16.92 % error reduction ) on Chinese in the first setting , and by 5.07 % ( or 19.67 % error reduction ) on English and 3.87 % ( or 19.4 % error reduction ) on Chinese in the second setting .
Among the baselines , the best is Co-SVM ; TSVMs do not always improve performance using the unlabeled data compared to the standalone SVM ; and TSVM -B outperforms TSVM -M except for Chinese in the second setting .
The MPQA data is more difficult in general compared to the NTCIR data .
Without unlabeled parallel data , the performance on the Chinese data is better than on the English data , which is consistent with results reported in NTCIR - 6 ( Seki et al. , 2007 ) .
Overall , the unlabeled parallel data improves classification accuracy for both languages when using our proposed joint model and Co-SVM .
The joint model makes better use of the unlabeled parallel data than Co-SVM or TSVMs presumably because of its attempt to jointly optimize the two monolingual models via soft ( probabilistic ) assignments of the unlabeled instances to classes in each iteration , instead of the hard assignments in Co-SVM and TSVMs .
Although English sentiment classification alone is more difficult than Chinese for our datasets , we obtain greater performance gains for English by exploiting unlabeled parallel data as well as the Chinese labeled data .
Varying the Weight and Amount of Unlabeled Data Figure 1 shows the accuracy curve of the proposed approach for the two data settings when varying the weight for the unlabeled data , , from 0 to 1 .
When is set to 0 , the joint model degenerates to two MaxEnt models trained with only the labeled data .
We can see that the performance gains for the proposed approach are quite remarkable even when is set to 0.1 ; performance is largely stable after reaches 0.4 .
Although MPQA is more difficult in general compared to the NTCIR data , we still see steady improvements in performance with unlabeled parallel data .
Overall , the proposed approach performs quite well for a wide range of parameter values of .
Figure 2 shows the accuracy curve of the proposed approach for the two data settings when varying the amount of unlabeled data from 0 to 20,000 instances .
We see that the performance of the proposed approach improves steadily by adding more and more unlabeled data .
However , even with only 2,000 unlabeled sentence pairs , the proposed approach still produces large performance gains .
Results on Pseudo-Parallel Unlabeled Data
As discussed in Section 3.4 , we generate pseudoparallel data by translating the monolingual sentences in each setting using Google 's machine translation system .
Figures 3 and 4 show the performance of our model using the pseudoparallel data versus the real parallel data , in the two settings , respectively .
The EN ->CH pseudoparallel data consists of the English unlabeled data and its automatic Chinese translation , and vice versa .
Although not as significant as those with parallel data , we can still obtain improvements using the pseudo- parallel data , especially in the first setting .
The difference between using parallel versus pseudo- parallel data is around 2 - 4 % in Figures 3 and 4 , which is reasonable since the quality of the pseudo- parallel data is not as good as that of the parallel data .
Therefore , the performance using pseudo- parallel data is better with a small weight ( e.g. = 0.1 ) in some cases .
Adding Pseudo-Parallel Labeled Data
In this section , we investigate how adding automatically translated labeled data might influence the performance as mentioned in Section 3.4 .
We use only the translated labeled data to train classifiers , and then directly classify the test data .
The average accuracies in setting 1 are 66.61 % and 63.11 % on English and Chinese , respectively ; while the accuracies in setting 2 are 58.43 % and 54.07 % on English and Chinese , respectively .
This result is reasonable because of the language gap between the original language and the translated language .
In addition , the class distributions of the English labeled data and the Chinese are quite different ( 30 % vs. 55 % for positive as shown in Table 1 ) .
Figures 5 and 6 show the accuracies when varying the weight of the translated labeled data vs.
the labeled data , with and without the unlabeled parallel data .
From Figure 5 for setting 1 , we can see that the translated data can be helpful given the labeled data and even the unlabeled data , as long as is small ; while in Figure 6 , the translated data decreases the performance in most cases for setting 2 .
One possible reason is that in the first data setting , the NTCIR English data covers the same topics as the NTCIR Chinese data and thus direct translation is helpful , while the English and Chinese topics are quite different in the second data setting , and thus direct translation hurts the performance given the existing labeled data in each language .
Discussion
To further understand what contributions our proposed approach makes to the performance gain , we look inside the parameters in the MaxEnt models learned before and after adding the parallel unlabeled data .
Table 3 shows the features in the model learned from the labeled data that have the largest weight change after adding the parallel data ; and Table 4 shows the newly learned features from the unlabeled data with the largest weights .
From Table 3 10 we can see that the weight changes of the original features are quite reasonable , e.g. the top words in positive class are obviously positive and the proposed approach gives them higher weights .
The new features also seem reasonable given the knowledge that the labeled and unlabeled data includes negative news about for specific topics ( e.g. Germany , Taiwan ) , .
We also examine the process of joint training by checking the performance on test data and the agreement of the two monolingual models on the unlabeled parallel data in both settings .
The average agreement across 5 folds is 85.06 % and 73.87 % in settings 1 and 2 , respectively , before the joint training , and increases to 100 % and 99.89 % , respectively , after 100 iterations of joint training .
Although the average agreement has already increased to 99.50 % and 99.02 % in settings 1 and 2 , respectively , after 30 iterations , the performance on the test set steadily improves in both settings until around 50 - 60 iterations , and then becomes relatively stable after that .
Examination of those sentence pairs in setting 2 for which the two monolingual models still 9
This is an abbreviation for the Organization of African Unity .
10
The features and weights in Tables 3 and 4 are extracted from the English model in the first fold of setting 1 . disagree after 100 iterations of joint training often produces sentences that are not quite parallel , e.g. : English :
The two sides attach great importance to international cooperation on protection and promotion of human rights .
Chinese : ? , ?-? , ?
?( Both sides agree that double standards on the issue of human rights are to be avoided , and are opposed to using pressure on human rights issues in international relations . )
Since the two sentences discuss human rights from very different perspectives , it is reasonable that the two monolingual models will classify them with different polarities ( i.e. positive for the English sentence and negative for the Chinese sentence ) even after joint training .
Conclusion
In this paper , we study bilingual sentiment classification and propose a joint model to simultaneously learn better monolingual sentiment classifiers for each language by exploiting an unlabeled parallel corpus together with the labeled data available for each language .
Our experiments show that the proposed approach can significantly improve sentiment classification for both languages .
Moreover , the proposed approach continues to produce ( albeit smaller ) performance gains when employing pseudo- parallel data from machine translation engines .
In future work , we would like to apply the joint learning idea to other learning frameworks ( e.g. SVMs ) , and to extend the proposed model to handle word- level parallel information , e.g. bilingual dictionaries or word alignment information .
Another issue is to investigate how to improve multilingual sentiment analysis by exploiting comparable corpora .
Figure 3 .Figure 5 . 35 Figure 3 . Accuracy with Pseudo-Parallel Unlabeled Figure 4 . Accuracy with Pseudo-Parallel Unlabeled Data in Setting 1 Data in Setting 2
