title
Aspect Sentiment Classification with Aspect-Specific Opinion Spans
abstract
Aspect sentiment classification , predicting the sentiment polarity of given aspects , has drawn extensive attention .
Previous attention - based models emphasize using aspect semantics to help extract opinion features for classification .
However , these works are either not able to capture opinion spans as a whole or capture variable - length opinion spans .
In this paper , we present a neat and effective multiple CRFs based structured attention model that is capable of extracting aspect-specific opinion spans .
The sentiment polarity of the target is then classified based on the extracted opinion features and contextual information .
The experimental results on four datasets demonstrate the effectiveness of the proposed model , and our further analysis shows that our model can capture aspect-specific opinion spans .
1
Introduction Aspect Based Sentiment Analysis ( ABSA ) ( Pang and Lee , 2008 ; Liu , 2012 ) is an extensively studied sentiment analysis task on a fine- grained semantic level , i.e. , opinion targets explicitly mentioned in sentences .
Previous ABSA studies focused on a few sub-tasks , such as Aspect Sentiment Classification ( ASC ) ( Wang et al. , 2016 ; Ma et al. , 2018 ) , Aspect Term Extraction ( ATE ) ( Li et al. , 2018 b ; He et al. , 2017 ) , Aspect and Opinion Co-Extraction ( Liu et al. , 2013 ; Xu et al. , 2018 ; Dai and Song , 2019 ) , E2E-ABSA ( a joint task of ASC and ATE ) ( Li et al. , 2019a ;
He et al. , 2019 ; Li et al. , 2019 b ) , Aspect Sentiment Triplet Extraction ( ASTE ) ( Peng et al. , 2019 ; Xu et al. , 2020 ) , etc. ASC analyzes the sentiment polarity of given aspects / targets in a review .
For example , consider the review sentence " Food is usually very good , though occasionally I worry about freshness of raw vegetables in side orders . "
This review mentions two aspects : Food and raw vegetables , and for ASC , the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables .
Most of the previous works ( Wang et al. , 2016 ; Liu and Zhang , 2017 ; Li et al. , 2018c ; He et al. , 2018 ; Li and Lu , 2019 ; Hu et al. , 2019 ) adopt attention mechanism ( Bahdanau et al. , 2015 ) to capture the semantic relatedness among the context words and the aspect , and learn aspect-specific features for sentiment classification .
However , it is challenging for attention - based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations .
Previous work ( Wang and Lu , 2018 ) engage structured attention networks ( Kim et al. , 2017 ) , which extend the previous attention mechanism to incorporate structure dependencies , to model the interaction among context words , and perform softselections of word spans .
In particular , they introduce two hand-coded regularizers to constrain the soft-selection process to attend to few short opinion spans .
However , such regularizers disturb the structure dependencies , and their method is not capable of emphasizing aspect-specific opinion spans for sentiment classification .
To better capture opinion features for aspect sentiment classification , we propose the MCRF - SA model , which introduces multiple conditional random fields ( CRF ) ( Lafferty et al. , 2001 ) to structured attention model .
While exploiting the advantages of structured attention mechanisms , our model avoids the regularizers by the complementarity among multiple CRFs .
We also improve the previous position decay function ( Li et al. , 2018a ; Tang et al. , 2019 ) to reduce the importance of context words that are further away from the aspect so as to emphasize aspect-specific opinion spans .
Our multi-CRF layer with the effective decay function extracts aspect-specific features from different representation sub-spaces to overcome the previous limitations .
The experimental results on the four datasets demonstrate the effectiveness of our model , and the analysis shows that the behaviors are in alignment with our intuition .
Model Description Given a context sequence w c = {w 1 , w 2 , . . . , w n } and a aspect sequence w a = {w i , ... , w j } ( 1 ? i ? j ? n ) which is a sub-sequence of w c , the goal of ASC is to predict sentiment polarity y ?
{ positive , negative , neutral } over the given aspect .
Our model is mainly constructed with a few neural layers , including an input layer , an aspect-specific contextualized representation layer , a position decay layer , a multi- CRF structured attention layer , and a sentiment classification layer .
Figure 1 presents the architecture of our MCRF - SA model .
Input Layer
The input of our model consists of word embedding w word t and aspect indicator embedding w as t .
The aspect indicator embedding is to differentiate aspect words and context words and is randomly initialized .
The input representation x t is as follows : x t = [ w word t ; w as t ] ( 1 )
Aspect-Specific Contextualized Representation
We employ a bi-directional GRU ( Cho et al. , 2014 ) to generate the contextualized representation .
Since the input representation has already contained the aspect information , the aspect-specific contextualized representation is obtained by concatenating the hidden states from both directions : h t = [ ? ? h t ; ? ? h t ] ( 2 ) where ? ? h t is the hidden state from the forward GRU and ? ? h t is from the backward .
Position Decay Following the previous work ( Li et al. , 2018a ; Zhang et al. , 2019 ; Tang et al. , 2019 ) , we also use a position decay function to reduce the influence of the context words on the aspect as it goes further away from the aspect .
We propose a higher - order decay function , which is more sensitive to distance , and the sensitivity can be tuned by ? on different datasets .
f ( t ) = ? ? ? ? ? ( L ? i + t L ) ? t < i 1 i ? t ? j ( L ? t + j L ) ? j < t ( 3 ) where i and j are the starting and ending position of an aspect , L is the maximum length of sentences across all datasets , ? is a hyper-parameter and a larger value enables more influence from the context words that are close to the aspect .
Then , the decayed contextual word representation is as follows : r t = f ( t ) h t ( 4 )
Multi-CRF Structured Attention
We use multiple linear-chain CRFs to intensively incorporate structure dependencies to capture the corresponding opinion spans of an aspect .
In particular , we create a latent label ( Wang and Lu , 2018 ) z ?
{ Y es , N o} to indicate whether each context word belongs to part of opinion spans .
Similar to ( Lample et al. , 2016 ) , given the sentence representation x , the CRF is defined as : P ( z|x ) = exp( score ( z , x ) ) z exp( score ( z , x ) ) ( 5 ) where score ( z , x ) is a score function that is defined as the summation of transition scores and emission scores from the Bi- GRU : score ( z , x ) = n t=0 T zt , z t+1 + n t=1 E t, zt ( 6 ) where T is a transition matrix and T zt , z t+1 denotes the transition score from label z t to z t +1 .
E t, zt denotes the emission score of label z t at the t-th position , and the score is obtained from a linear layer , which takes r t as input and returns a vector whose length is label size .
Marginal Inference
The latent labels introduced in the CRF layer show whether the word influences the given aspect 's sentiment .
Intuitively , we can understand that the marginal probabilities on the Y es label indicate the influence of the current context word on the aspect word 's sentiment .
By using the forward - backward algorithm , we calculate the marginal distribution of the latent label .
With the marginal distribution , the sentence representation s is obtained : s = n t=1 P ( z t = Y es | x ) r t ( 7 )
The final representation for classification is obtained by concatenating the sentence representations from all CRFs : q = [ s 1 ; s 2 ; ... ; s a ] ( 8 ) where a is the number of CRFs .
Sentiment Classification
The sentence representation q is passed to a sentiment classier to obtain the distribution of sentiment polarities : P ( y|q ) = Softmax ( W q + b ) ( 9 ) where W and b are learnable parameters for the sentiment classifier layer .
We learn model parameters by minimizing the negative log-likelihood .
3 Experiments
Experimental Setup Our proposed MCRF -SA model is evaluated on four benchmark datasets : SemEval 2014 Task4 ( Pontiki et al. , 2014 ) , SemEval 2015 Task12 ( Pontiki et al. , 2015 and SemEval 2016
Task 5 ( Pontiki et al. , 2016 ) .
Following the previous works ( Tang et al. , 2016 ; Wang and Lu , 2018 ; ( Alberto and Giacomo , 2018 ) .
We set the hidden size of GRU to 32 or 64 .
The batch size is set to 64 or 96 .
The dropout rate is selected from 0.3 to 0.8 , with a step size of 0.1 .
The dimension of the aspect indicator is selected from { 50 , 70 , 90 } .
The value of ? in the position decay function is selected from { 1,2,3 } .
The number of layer of GRU is selected from { 1,2,3 } .
We adopt Adam ( Kingma and Ba , 2014 ) to optimize our model with a learning rate of 0.008 .
All hyper- parameters are selected based on the best performance on the development set .
Baselines Our MCRF -SA model is compared with the following methods 2 . SVM ( Kiritchenko et al. , 2014 ) is a support vector machine based method that integrates surface , lexicon , and parse features .
ATAE -LSTM ( Wang et al. , 2016 ) is an LSTM ( Hochreiter and Schmidhuber , 1997 ) based model , which has an extra attention to perform soft-selection over the context words .
MemNet ( Tang et al. , 2016 ) introduces a deep memory network to implement attention mechanisms to learn the relatedness of context words towards the aspect .
IAN ( Ma et al. , 2017 ) utilizes two LSTM based attention models to learn both context and aspect representations interactively .
SA - LSTM -P ( Wang and Lu , 2018 ) employs structured attention networks with multiple regularizers to capture the opinion spans for ASC .
TNets ( Li et al. , 2018a ) ral Network ( CNN ) ( Lecun et al. , 1998 ) layer to obtain the sentence representation .
TNet- ATT ( Tang et al. , 2019 ) is an extension of TNet - LF , and it provides an attention supervision mining mechanism to improve the previous model .
ASCNN and ASGCN ( Zhang et al. , 2019 ) use CNN and Graph Convolutional Network ( GCN ) ( Kipf and Welling , 2017 ) to capture the long-range dependencies and syntactic information .
Experimental Results
Our proposed model shows significant improvements on the four datasets , Table 2 shows the performance comparisons .
Our method outperforms SVM ( Kiritchenko et al. , 2014 ) by 2.7 and 7.15 Acc. score on 14 Rest and 14 Lap , respectively .
This indicates that our neural approach extracts more effective features than hard -coded feature engineering .
Compared to the attention - based methods - ATAE ( Wang et al. , 2016 ) , MemNet ( Tang et al. , 2016 ) , IAN ( Ma et al. , 2017 ) , and TNet-ATT ( Tang et al. , 2019 ) , our MCRF - SA model pays more attention to the aspect-specific opinion spans , which bring significant performance improvement on the four datasets .
We also compare our model with methods that focus on word segmentations for sentiment classification .
Our method outperforms the previous regularizers guided structured attention model SA - LSTM -P ( Wang and Lu , 2018 ) by more than 1.2 Acc. score on 14 Rest and 14 Lap .
TNet -LF ( Li et al. , 2018a ) and ASCNN ( Zhang et al. , 2019 ) em-ploy CNN to evaluate word spans regarding how much it contributed to the sentiment , but the kernel size limits the length of the span .
ASGCN ( Zhang et al. , 2019 ) employs GCN over the dependency tree to capture syntactic and dependency information .
However , the performance heavily relies on the accuracy of the dependency trees .
Our proposed multi-CRF structured attention along with the position decay function allows MCRF - SA to perform soft-selection of multiple aspectspecific opinion spans that influence the aspect 's sentiment .
The large performance gaps between our model and baseline models confirm the effectiveness of our proposed architecture .
Such results also demonstrate that sentiment classification can benefit greatly from aspect-specific opinion spans .
Furthermore , we observe that the performance on 15 Rest is not as good as the other three datasets .
Such behavior is caused by the different distribution of positive , neutral , and negative sentiment between training and test set , shown in Table 1 .
Analysis
Effect of Number of CRFs
To fully investigate the effect of the number of CRFs , we conduct additional experiments on 14 Rest and 14Lap with the number of CRFs ? { 1 , 2 , 3 , ... , 16 }.
Figure 2 shows the experimental results .
The model achieves the best performance when the number of CRFs equals to 4 .
Particularly , the performance becomes relatively plateau when a large number of CRFs is adopted .
We believe this is because the sizes of the four benchmark datasets are relatively small , and an excessively large number of parameters may not be able to further extract effective features .
Case Study and Error Analysis Figure 3 shows the marginal distributions ( Equation 5 ) of SA - LSTM -P ( Wang and Lu , 2018 ) and our MCRF - SA model .
The aspect for the given example is " Indian food " with negative sentiment , and only our model predicts correct sentiment .
From Figure 3b heat map , the different marginal distributions on the four CRFs indicate that our model indeed captures different opinion features .
It can be observed that MCRF - SA is able to attend to the two major opinion spans : " real " and " n't " .
The SA -LSTM -P model returns positive sentiment as it focuses too much on wrong opinion words .
We also analyze some common errors from our MCRF - SA model , ASGCN , and TNet - ATT on the Lap14 dataset .
We observe two major types of errors , and Table 3 shows the examples for error analysis .
The first two sentences belong to the type 1 error and the last one presents a type 2 error .
The first type of errors appear frequently in neutral cases .
In general , the neural models cannot well differentiate if the negative expressions ( e.g. " cost " , " should n't " , etc. ) is associated with the target / aspect .
The second type typically involve complicated sentence structures with non-trivial semantics , which requires advanced language understanding capability .
Ablation Study
We examine the effectiveness of the major components of our MCRF - SA model , and Table 3 presents the ablation results on 14 Rest and 14Lap datasets .
Without the aspect indicator , our model becomes a sentence - level sentiment classification method which inevitably produces wrong predictions for sentences having multiple aspects with different sentiments .
Removing the position decay function hurts the performance by 2.84 and 1.11 F 1 score on 14 Rest and 14 Lap , respectively .
Lastly , without multi-CRF structured attention layer , the architecture becomes a simple Bi-GRU based model and the performance drops significantly by 4.89 and 10.49 F 1 points on 14 Rest and 14 Lap .
Conclusion
We propose a simple and effective MCRF - SA model to extract aspect-specific opinion span features .
In particular , with the proposed multi-CRF structured attention layer and the effective position decay function , our model is capable of extracting various aspect-specific opinion span features from different representation sub-spaces .
The experimental results demonstrate that our method effectively exploits the corresponding opinion features for sentiment classification .
One future direction is to investigate how to integrate the two different attention mechanisms , namely the standard attention and structured attention for NLP applications .
Figure 1 : 1 Figure 1 : MCRF -SA Architecture .
