title
Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification
abstract
In this paper , we study cross-domain sentiment classification with neural network architectures .
We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification .
We also propose to jointly learn this sentence embedding together with the sentiment classifier itself .
Experiment results demonstrate that our proposed joint model outperforms several state - of - theart methods on five benchmark datasets .
Introduction
With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews , sentiment classification has received continuous attention in the NLP community for over a decade ( Pang et al. , 2002 ; Pang and Lee , 2004 ; Hu and Liu , 2004 ; Choi and Cardie , 2008 ; Nakagawa et al. , 2010 ) .
One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain .
This is because sentiments are often expressed with domain-specific words and expressions .
For example , in the Movie domain , words such as moving and engaging are usually positive , but they may not be relevant in the Restaurant domain .
Since labeled data is expensive to obtain , it would be very useful if we could adapt a model trained on a source domain to a target domain .
Much work has been done in sentiment analysis to address this domain adaptation problem ( Blitzer et al. , 2007 ; Pan et al. , 2010 ; Bollegala et al. , 2011 ; Ponomareva and Thelwall , 2012 ; Bollegala et al. , 2016 ) .
Among them , an appealing method is the Structural Correspondence Learning ( SCL ) method ( Blitzer et al. , 2007 ) , which uses pivot feature prediction tasks to induce a projected feature space that works well for both the source and the target domains .
The intuition behind is that these pivot prediction tasks are highly correlated with the original task .
For sentiment classification , Blitzer et al . ( 2007 ) first chose pivot words which have high mutual information with the sentiment labels , and then set up the pivot prediction tasks to be the predictions of each of these pivot words using the other words .
However , the original SCL method is based on traditional discrete feature representations and linear classifiers .
In recent years , with the advances of deep learning in NLP , multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance ( Socher et al. , 2013 ; Dong et al. , 2014a ; Dong et al. , 2014 b ; Kim , 2014 ; Tang et al. , 2015 ) .
In these models , dense , real-valued feature vectors and non-linear classification functions are used .
By using real-valued word embeddings pre-trained from a large corpus , these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words .
And by using non-linear functions through multi-layer neural networks , these models represent a more expressive hypothesis space .
Therefore , it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification .
There has been some recent studies on neural network - based domain adaptation ( Glorot et al. , 2011 ; Chen et al. , 2012 ; Yang and Eisenstein , 2014 ) .
They use Stacked Denoising Auto-encoders ( SDA ) to induce a hidden representation that presumably works well across domains .
However , SDA is fully unsupervised and does not consider the end task we need to solve , i.e. , the sentiment classification task .
In contrast , the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation .
Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features ( Yang and Eisenstein , 2015 ; Bollegala et al. , 2015 ) .
Different from these methods , we aim to directly learn sentence embeddings that work well across domains .
In this paper , we aim to extend the main idea behind SCL to neural network - based solutions to sentiment classification to address the domain adaptation problem .
Specifically , we borrow the idea of using pivot prediction tasks from SCL .
But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights , which all relies on linear transformations , we introduce only two auxiliary binary prediction tasks and directly learn a non-linear transformation that maps an input to a dense embedding vector .
Moreover , different from SCL and the auto-encoderbased methods , in which the hidden feature representation and the final classifier are learned sequentially , we propose to jointly learn the hidden feature representation together with the sentiment classification model itself , and we show that joint learning works better than sequential learning .
We conduct experiments on a number of different source and target domains for sentence - level sentiment classification .
We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs .
Related Work Domain Adaptation : Domain adaptation is a general problem in NLP and has been well studied in recent years ( Blitzer et al. , 2006 ; Daum ?
III , 2007 ; Jiang and Zhai , 2007 ; Dredze and Crammer , 2008 ; Titov , 2011 ; Yu and Jiang , 2015 ) .
For sentiment classification , most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers .
One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features ( Blitzer et al. , 2007 ; Pan et al. , 2010 ; Pan et al. , 2011 ) .
Another line of work tries to derive domain-specific sentiment words ( Bollegala et al. , 2011 ; Li et al. , 2012 ) .
Our proposed method is similar to the first line of work in that we also aim to learn a general , cross-domain representation ( sentence embeddings in our case ) .
Neural Networks for Sentiment Classification :
A recent trend of deep learning enhances various kinds of neural network models for sentiment classification , including Convolutional Neural Networks ( CNNs ) , Recursive Neural Network ( ReNNs ) and Recurrent Neural Network ( RNNs ) , which have been shown to achieve competitive results across different benchmarks ( Socher et al. , 2013 ; Dong et al. , 2014a ; Dong et al. , 2014 b ; Kim , 2014 ; Tang et al. , 2015 ) .
Inspired by their success in standard indomain settings , it is intuitive for us to apply these neural network models to domain adaptation settings .
Denoising Auto-encoders for Domain Adaptation : Denoising Auto-encoders have been extensively studied in cross-domain sentiment classification , since the representations learned through multilayer neural networks are robust against noise during domain adaptation .
The initial application of this idea is to directly employ stacked denoising autoencoders ( SDA ) by reconstructing the original features from data that are corrupted with noise ( Glorot et al. , 2011 ) , and Chen et al . ( 2012 ) proposed to analytically marginalize out the corruption during SDA training .
Later Yang and Eisenstein ( 2014 ) further showed that their proposed structured dropout noise strategy can dramatically improve the efficiency without sacrificing the accuracy .
However , these methods are still based on traditional discrete representation and do not exploit the idea of using auxiliary tasks that are related to the end task .
In contrast , the sentence embeddings learned from our method are derived from real-valued feature vectors and rely on related auxiliary tasks .
Method
In this section we present our sentence embeddingbased domain adaptation method for sentiment classification .
We first introduce the necessary notation and an overview of our method .
we then delve into the details of the method .
Notation and Method Overview
We assume that each input is a piece of text consisting of a sequence of words .
For the rest of this paper , we assume each input is a sentence , although our method is general enough for longer pieces of text .
Let x = ( x 1 , x 2 , . . . ) denote a sentence where each x i ? { 1 , 2 , . . . , V } is a word in the vocabulary and V is the vocabulary size .
Let the sentiment label of x be y ? {+ , ?} where + denotes a positive sentiment and ?
a negative sentiment .
We further assume that we are given a set of labeled training sentences from a source domain , denoted by D s = {( x s i , y s i ) }
N s i=1 .
Also , we have a set of unlabeled sentences from a target domain , denoted by D t = { x t i } N t i=1 .
Our goal is to learn a good sentiment classifier from both D s and D t such that the classifier works well on the target domain .
A baseline solution without considering any domain difference is to simply train a classifier using D s , and with the recent advances in neural networkbased methods to sentence classification , we consider a baseline that uses a multi-layer neural network such as a CNN or an RNN to perform the classification task .
To simplify the discussion and focus on the domain adaptation ideas we propose , we will leave the details of the neural network model we use in Section 3.5 .
For now , we assume that a multilayer neural network is used to transform each input x into a sentence embedding vector z .
Let us use f ? to denote the transformation function parameterized by ? , that is , z = f ? ( x ) .
Next , we assume that a linear classifier such as a softmax classifier is learned to map z to a sentiment label y .
We introduce two auxiliary tasks which presumably are highly correlated with the sentiment classification task itself .
Labels for these auxiliary tasks can be automatically derived from unlabeled data in both the source and the target domains .
With the help of the two auxiliary tasks , we learn a non-linear transformation function f ? from unlabeled data and use it to derive a sentence embedding vector z from sentence x , which supposedly works better across domains .
Finally we use the source domain 's training data to learn a linear classifier on the representation z ?
z , where ? is the operator that concatenates two vectors .
Figure 1 gives the outline of our method .
Auxiliary Tasks
Our two auxiliary tasks are about whether an input sentence contains a positive or negative domainindependent sentiment word .
The intuition is the following .
If we have a list of domain-independent positive sentiment words , then an input sentence that contains one of these words , regardless of the domain the sentence is from , is more likely to contain an overall positive sentiment .
For example , a sentence containing the word good is likely to be overall positive .
Moreover , the rest of the sentence excluding the word good may contain domain-specific words or expressions that also convey a positive sentiment .
For example , in the sentence " The laptop is good and goes really fast , " we can see that the word fast is a domain-specific sentiment word , and its sentiment polarity correlates with that of the word good , which is domain-independent .
Therefore , we can hide the domain-independent positive words in a sentence and try to use the other words in the sentence to predict whether the original sentence contains a domain-independent positive word .
There are two things to note about this auxiliary task : ( 1 )
The label of the task can be automatically derived provided that we have the domain-independent positive word list .
( 2 ) The task is closely related to the original task of sentence - level sentiment classification .
Similarly , we can introduce a task to predict the existence of a domain-independent negative sentiment word in a sentence .
Formally , let us assume that we have two domainindependent sentiment word lists , one for the positive sentiment and the other for the negative sentiment .
Details of how these lists are obtained will be given in Section 3.5 .
Borrowing the term from SCL , we refer to these sentiment words as pivot words .
For each sentence x , we replace all the occurrences of these pivot words with a special token UNK .
Let g( ? ) be a function that denotes this procedure , that is , g( x ) is the resulting sentence with UNK tokens .
We then introduce two binary labels for g ( x ) .
The first label u indicates whether the original sentence x contains at least one domainindependent positive sentiment word , and the second label v indicates whether x contains at least one domain-independent negative sentiment word .
Figure 1 shows an example sentence x , its modified version g ( x ) and the labels u and v for x .
We further use D a = {( x i , u i , v i ) }
N a i=1 to denote a set of training sentences for the auxiliary tasks .
Note that the sentences in D a can be from the sentences in D s and D t , but they can also be from other unlabeled sentences .
Sentence Embeddings for Domain Adaptation
With the two auxiliary tasks , we can learn a neural network model in a standard way to produce sentence embeddings that work well for the auxiliary tasks .
Specifically , we still use ? to denote the parameters of the neural network that produces the sentence embeddings ( and f ? the corresponding transformation function ) , and we use ? + and ? ? to denote the parameters of two softmax classifiers for the two auxiliary tasks , respectively .
Using crossentropy loss , we can learn ? by minimizing the following loss function : J ( ? , ? + , ? ? ) = ? ( x , u , v ) ?
D a log p( u |f ? ( g ( x ) ) ; ? + ) + log p( v|f ? ( g ( x ) ) ; ? ? ) , where p( y|z ; ? ) is the probability of label y given vector z and parameter ? under softmax regression .
With the learned ? , we can derive a sentence embedding z from any sentence .
Although we could simply use this embedding z for sentiment classification through another softmax classifier , this may not be ideal because z is transformed from g ( x ) , which has the domain-independent sentiment words removed .
Similar to SCL and some other previous work , we concatenate the embedding vector z with the standard embedding vector z for the final classification .
Joint Learning
Although we can learn ? using D a as a first step , here we also explore a joint learning setting .
In this setting , ? is learned together with the neural network model used for the end task , i.e. , sentiment classification .
This way , the learning of ? depends not only on D a but also on D s , i.e. , the sentimentlabeled training data from the source domain .
Specifically , we use ? to denote the parameters for a neural network that takes the original sentence x and transforms it to a sentence embedding ( and f ? the corresponding transformation function ) .
We use ? to denote the parameters of a softmax classifier that operates on the concatenated sentence embedding z ?
z for sentiment classification .
With joint learning , we try to minimize the following loss func-tion : J ( ? , ? , ? , ? + , ? ? ) = ? ( x, y ) ?
D s log p( y |f ? ( x ) ? f ? ( g ( x ) ) ; ? ) ? ( x , u , v ) ?
D a log p( u |f ? ( g ( x ) ) ; ? + ) + log p( v|f ? ( g ( x ) ) ; ? ? ) .
We can see that this loss function contains two parts .
The first part is the cross-entropy loss based on the true sentiment labels of the sentences in D s .
The second part is the loss based on the auxiliary tasks and the data D a , which are derived from unlabeled sentences .
Finally , to make a prediction on a sentence , we use the learned ? and ? to derive a sentence embedding f ? ( x ) ? f ? ( g ( x ) ) , and then use the softmax classifier parameterized by the learned ? to make the final prediction .
Implementation Details
In this section we explain some of the model details .
Pivot Word Selection Recall that the two auxiliary tasks depend on two domain- independent sentiment word lists , i.e. , pivot word lists .
Different from Blitzer et al. ( 2007 ) , we employ weighted log-likelihood ratio ( WLLR ) to select the most positive and negative words in both domains as pivots .
The reason is that in our preliminary experiments we observe that mutual information ( used by Blitzer et al . ( 2007 ) ) is biased towards low frequency words .
Some high frequency words including good and great are scored low .
In comparison , WLLR does not have this issue .
The same observation was also reported previously by Li et al . ( 2009 ) .
More specifically , we first tokenize the sentences in D s and D t and perform part- of-speech tagging using the NLTK toolkit .
Next , we extract only adjectives , adverbs and verbs with a frequency of at least 3 in the source domain and at least 3 in the target domain .
We also remove negation words such as not and stop words using a stop word list .
We then measure each remaining candidate word 's relevance to the positive and the negative classes based on D s by computing the following scores : r( w , y ) = p( w|y ) log p( w|y ) p( w| ? ) , where w is a word , y ? {+ , ?} is a sentiment label , ? is the opposite label of y , and p( w|y ) is the empirical probability of observing w in sentences labeled with y .
We can then rank the candidate words in decreasing order of r(w , + ) and r(w , ? ) .
Finally , we select the top 25 % from each ranked list as the final lists of pivot words for the positive and the negative sentiments .
Some manual inspection shows that most of these words are indeed domain-independent sentiment words .
Neural Network Model
Our framework is general and potentially we can use any neural network model to transform an input sentence to a sentence embedding vector .
In this paper , we adopt a CNN - based approach because it has been shown to work well for sentiment classification .
Specifically , each word ( including the token UNK ) is represented by a word embedding vector .
Let W ?
R d?V denote the lookup table for words , where each column is a d-dimensional embedding vector for a word type .
Two separate CNNs are used to process x and g ( x ) , and their mechanisms are the same .
For a word x i in each CNN , the embedding vectors inside a window of size n centered at i are concatenated into a new vector , which we refer to as e i ?
R nd .
A convolution operation is then performed by applying a filter F ? R h?nd on e i to produce a hidden vector h i = m( Fe i + b ) , where b ?
R h is a bias vector and m is an elementwise non-linear transformation function .
Note that we pad the original sequence in front and at the back to ensure that at each position i we have n vectors to be combined into h i .
After the convolution operation is applied to the whole sequence , we obtain H = [ h 1 , h 2 , . . . ] , and we apply a max-over - time pooling operator to take the maximum value of each row of H to obtain an overall hidden vector , i.e. , z for x and z for g ( x ) .
It is worth noting that the two neural networks corresponding to f ? and f ? share the same word embedding lookup table .
This lookup table is initialized with word embeddings from word2vec 1 and is updated during our learning process .
Note that the token UNK is initialized as a zero vector and never updated .
Differences from SCL
Although our method is inspired by SCL , there are a number of major differences : ( 1 ) Our method is based on neural network models with continuous , dense feature representations and non-linear transformation functions .
SCL is based on discrete , sparse feature vectors and linear transformations .
( 2 ) Although our pivot word selection is similar to that of SCL , in the end we only use two auxiliary tasks while SCL uses much more pivot prediction tasks .
( 3 ) We can directly learn the transformation function f ? that produces the hidden representation , while SCL relies on SVD to learn the projection function .
( 4 ) We perform joint learning of the auxiliary tasks and the end task , i.e. , sentiment classification , while SCL performs the learning in a sequential manner .
To evaluate our proposed method , we conduct experiments using five benchmark data sets .
The data sets are summarized in Table 1 . Movie1 2 and Movie2 3 are movie reviews labeled by Pang and Lee ( 2005 ) and Socher et al . ( 2013 ) , respectively .
Camera 4 are reviews of digital products such as MP3 players and cameras ( Hu and Liu , 2004 ) . Laptop and Restaurant 5 are laptop and restaurant reviews taken from SemEval 2015 Task 12 .
Experiments
We consider 18 pairs of data sets where the two data sets come from different domains .
6
For neural network - based methods , we randomly pick 200 sentences from the target domain as the development set for parameter tuning , and the rest of the data from the target domain as the test data .
Baselines and Hyperparameters
We consider the following baselines : Naive is a non-domain- adaptive baseline based on bag-of - word representations .
SCL is our implementation of the Structural Correspondence Learning method .
We set the number of induced features K to 100 and rescale factor ? = 5 , and we use 1000 pivot words based on our preliminary experiments .
mDA is our implementation of marginalized Denoising Auto-encoders ( Chen et al. , 2012 ) , one of the state - of - the - art domain adaptation methods , which learns a shared hidden representation by reconstructing pivot features from corrupted inputs .
Following Yang and Eisenstein ( 2014 ) , we employ the efficient and effective structured dropout noise strategy without any parameter .
The top 500 features are chosen as pivots based on our preliminary experiments .
NaiveNN is a non-domain- adaptive baseline based on CNN , as described in Section 3.5 .
Aux -NN is a simple combination of our auxiliary tasks with NaiveNN , which treats the derived label of two auxiliary tasks as two features and then appends them to the hidden representation learned from CNN , followed by a softmax classifier .
SCL - NN is a naive combination of SCL with NaiveNN , which appends the induced representation from SCL to the hidden representation learned from CNN , followed by a softmax classifier .
mDA - NN is similar to SCL - NN but uses the hidden representation derived from mDA .
Sequential is our proposed method without joint learning , which first learns ? based on D a and then learns ? and ? based on D s with fixed ? . Joint is our proposed joint learning method , that is , we jointly learn ? and ? . For Naive , SCL and mDA , we use LibLinear 7 to train linear classifiers and use its default hyperparameters .
In all the tasks , we use unigrams and bigrams with a frequency of at least 4 as features for classification .
For the word embeddings , we set the dimension d to 300 .
For CNN , we set the window size to 3 .
Also , the size of the hidden representations z and z is set to 100 .
Following Kim ( 2014 ) , the non-linear activation function in CNN is Relu , the mini-batch size is 50 , the dropout rate ? equals 0.5 , and the hyperparameter for the l 2 norms is set to be 3 .
For Naive , SCL and mDA , we do not use the 200 sentences in the development set for tuning parameters .
Hence , for fair comparison , we also include settings where the 200 sentences are added to the training set .
We denote these settings by ++.
Results In Table 2 , we report the results of all the methods .
It is easy to see that the performance of Naive is very limited , and the incorporation of 200 reviews in the development set ( Naive + + ) brings in 4.3 % of improvement on average .
SCL ++ and mDA ++ can further improve the average accuracy respectively by 0.8 % and 1.9 % , which verifies the usefulness of these two domain adaptation methods .
However , we can easily see that the performance of these domain adaptation methods based on discrete , bag-of - word representations is even much lower than the nondomain - adaptive method on continuous representations ( NaiveNN ) .
This confirms that it is useful to develop domain adaptation methods based on embedding vectors and neural network models .
Moreover , we can find that the performance of simply appending two features from auxiliary tasks to NaiveNN ( i.e. , Aux - NN ) is quite close to that of NaiveNN on most data set pairs , which shows that it is not ideal for domain adaptation .
In addition , although the shared hidden representations derived from SCL and mDA are based on traditional bag-of - word representations , SCL - NN and mDA - NN can still improve the performance of NaiveNN on most data set pairs , which indicates that the derived shared hidden representations by SCL and by mDA can generalize better across domains and are generally useful for domain adaptation .
Finally , it is easy to see that our method with joint learning outperforms SCL - NN on almost all the data set pairs .
And in comparison with mDA - NN , our method with joint learning can also outper - form it on most data set pairs , especially when the size of the labeled data in the source domain is relatively large .
Furthermore , we can easily observe that for our method , joint learning generally works better than sequential learning .
All these observations show the advantage of our joint learning method .
In Table 3 , we also show the comparison between mDA - NN and our model under a setting some labeled target data is used .
Specifically , we randomly select 100 sentences from the development set and mix them with the training set .
We can observe that our method Joint outperforms NaiveNN and mDA - NN by 1.2 % and 0.6 % , respectively , which further confirms the effectiveness of our model .
But , in comparison with the setting where no target data is available , the average improvement of our method over NaiveNN is relatively small .
Hence , to give a deeper analysis , we further show the comparison of Joint and NaiveNN with respect to the number of labeled target data in Figure 2 .
Note that for space limitation , we only present the results on MV2 ? RT and MV2 ? CR .
Similar trends have been observed on other data set pairs .
As we can see from Figure 2 , the difference between the performance of NaiveNN and that of Joint gradually decreases with the increase of the number of labeled target data .
This indicates that our joint model is much more effective when no or small number of labeled target data is available .
Case Study
To obtain a better understanding of our method , we conduct a case study where the source is CR and the target is RT .
For each sentiment polarity , we try to extract the most useful trigrams for the final predictions .
Recall that our CNN models use a window size of 3 , which corresponds to trigrams .
By tracing the final prediction scores back through the neural network , we are able to locate the trigrams which have contributed the most through max-pooling .
In Table 4 , we present the most useful trigrams of each polarity extracted by NaiveNN and by the two components of our sequential and joint method .
Sequentialoriginal and Joint-original refer to the CNN corresponding to f ? while Sequential - auxiliary and Joint-auxiliary refer to the CNN corresponding to f ? , which is related to the auxiliary tasks .
In Table 4 , we can easily observe that for NaiveNN , the most important trigrams are domainindependent , which contain some general sentiment words like good , great and disappointing .
For our sequential model , the most important trigrams captured by Sequential - original are similar to NaiveNN , but due to the removal of the pivot words in each sentence , the most important trigrams extracted by Sequential - auxiliary are domain-specific , including target -specific sentiment words like oily , friendly and target-specific aspect words like flavor , atmosphere .
But since aspect words are irrelevant to our sentiment classification task , it might bring in some noise and affect the performance of our sequential model .
In contrast to Sequential - auxiliary , Joint-auxiliary is jointly learnt with the sentiment classification task , and it is easy to see that most of its extracted trigrams are target-specific sentiment words .
Also , for Jointoriginal , since we share the word embeddings of two components and do not remove any pivot , it is intuitive to see that the extracted trigrams contain both domain-independent and domain-specific sentiment words .
These observations agree with our motivations behind the model .
Finally , we also sample several sentences from the test dataset , i.e. , RT , to get a deeper insight of our joint model .
Although NaiveNN and Sequential correctly predict sentiments of the following two sentences : 1 . " I 've also been amazed at all the new additions in the past few years : A new Jazz Bar , the most fantastic Dining Garden , the Best Thin Crust Pizzas , and now a Lasagna Menu which is to die for ! "
2 . " The have a great cocktail with Citrus Vodka and lemon and lime juice and mint leaves that is to die for ! "
Both of them give wrong predictions on another three sentences containing to die for : 3 . " Try their chef 's specials - they are to die for . "
4 . " Their tuna tartar appetizer is to die for . "
5 . " It 's to die for ! " .
However , since to die for co-occurs with some general sentiment words like fantastic , best and great in previous two sentences , our joint model can implicitly learn that to die for is highly correlated with the positive sentiment via our auxiliary tasks , and ultimately make correct predictions for the latter three sentences .
This further indicates that our joint model can identify more domain-specific sentiment words in comparison with NaiveNN and Sequential , and therefore improve the performance .
Conclusions
We presented a domain adaptation method for sentiment classification based on sentence embeddings .
Our method induces a sentence embedding that works well across domains , based on two auxiliary tasks .
We also jointly learn the cross-domain sentence embedding and the sentiment classifier .
Experiment results show that our proposed joint method can outperform several highly competitive domain adaptation methods on 18 source - target pairs using five benchmark data sets .
Moreover , further analysis confirmed that our method is able to pick up domain-specific sentiment words .
Figure 2 : 2 Figure 2 : The influence of the number of labeled target data .
Sentiment Classification Sentence Embedding Word Embedding
The laptop is good and goes really fast
The laptop is UNK and goes really fast Auxiliary Tasks CNN / RNN CNN / RNN Shared Lookup Table Original Sentence New Sentence without Pivots Figure 1 : The Outline of our Proposed Method .
Table 1 : 1 Statistics of our data sets .
4.1 Data Sets and Experiment Settings Data Set # Sentences # Words Movie1 ( MV1 ) 10662 18765 Movie2 ( MV2 ) 9613 16186 Camera ( CR ) 3770 5340 Laptop ( LT ) 1907 2837 Restaurant ( RT ) 1572 2930
Table 2 : 2 Comparison of classification accuracies of different methods .
Task * indicates that our joint method is significantly better than NaiveNN , Aux-NN , SCL -NN and mDA - NN with p < 0.05 based on McNemar 's paired significance test .
Table 3 : 3 Comparison of our method Joint with NaiveNN and mDA - NN in a setting where some labeled target data is used .
Table 4 : 4 not go back , was not fresh , friendly * * , to die for , also very UNK , prices * * , inedible ! * , oily * * , overpriced * * of my favorite , food * * , * i highly , delicious !
* Comparison of the most useful trigrams chosen by our method and by NaiveNN on CR ? RT .
Here * denotes a " padding " , which we added at the beginning and the end of each sentence .
The domain-specific sentiment words are in bold .
Method Negative Sentiment Positive Sentiment disappointing * * , disgusting * * , it is not , * * great , good * * , * * best , * i love , NaiveNN slow * * , * too bad , * * terrible , place is not , was very good , * * excellent , unpleasant experience * , would not go , * the only wonderful * * , * * amazing , * * nice disgusting * * , disappointing * * , * * terrible * * great , good * * , * * best , * i love , Sequential - original expensive * * , it is not , unpleasant experience * , * highly recommended , * * excellent , slow * * , * too bad , probably would not , awful * * wonderful * * , is amazing * , is the perfect disgusting * * , never go back , money * * , delicious * * , friendly * * , food * * , Sequential - auxiliary rude * * , flavor * * , * this place , oily * * , food is UNK , * highly UNK , fresh * * , prices * * , inedible ! * , this place survives atmosphere * * , * i highly , nyc * * disgusting * * , soggy * * , disappointing * * , * * great , good * * , * * best , * i love , Joint-original * too bad , * would never , it is not , rude * * , * * amazing , delicious * * , * * terrible , place is not , disappointment * * back * * , * i highly , of my favorite soggy * * , disgusting * * , rude * * , delicious * * , go back * , is always fresh , Joint-auxiliary disappointment * * ,
https://code.google.com/p/word2vec/
https://www.cs.cornell.edu/people/pabo/ movie-review-data / 3 http://nlp.stanford.edu/sentiment/ 4 http://www.cs.uic.edu/ ?liub / FBS / sentiment-analysis.html5
Note that the original data set is for aspect-level sentiment analysis .
We remove sentences with opposite polarities towards different aspects , and use the consistent polarity as the sentencelevel sentiment of each remaining sentence .
Because Movie1 and Movie2 come from the same domain , we do not take this pair .
http://www.csie.ntu.edu.tw/cjlin/ liblinear /
