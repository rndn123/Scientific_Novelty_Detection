title
NileTMRG at SemEval-2016 Task 7 : Deriving Prior Polarities for Arabic Sentiment Terms
abstract
This paper presents a model that was developed to address SemEval Task 7 : " Determining Sentiment Intensity of English and Arabic Phrases " , with focus on ' Arabic Phrases ' .
The goal of this task is to determine the degree to which some given term is associated with positive sentiment .
The underlying premise behind the model that we have adopted is that determining the context ( positive or negative ) in which a term usually occurs can determine its strength .
Since the focus is on Twitter terms , Twitter was used to collect tweets for each term for which a strength value was to be derived .
An Arabic sentiment analyzer , was then used to assign a polarity to each of these tweets , thus defining their context .
We then experimented with normalized point wise mutual information with and without linear regression to assign intensity scores to input terms .
The output of the model that we 've adopted ranked at two out of the three presented systems for this task with a Kendall score of 0.475 .
Introduction
During the past few years , interest in sentiment analysis has surged .
Sentiment lexicons are often an essential component for building sentiment analysis systems .
Entries in sentiment lexicons can vary significantly in terms of how strongly they re-flect positive or negative sentiment .
For example , while both the terms " good " and " amazing " reflect a positive sentiment , the term " amazing " is stronger and more positive than " good " .
In this paper , we address early work that has been conducted to determine the intensity of Arabic twitter sentiment terms with respect to positive sentiment .
Since the desired task is to determine the " positive strength " of a word , the results can be interpreted as follows : the closer the term score is to 1 , the stronger it is as a positive indicator ; the closer it is to 0 , the stronger it is as a negative indicator .
Terms that can be used in both positive and negative contexts will usually fall somewhere in the middle .
While this task was introduced in SemEval - - 2015 as Task 10 - sub- task E ( Rosenthal et al. 2015 ) for English terms , this is the first time that it has been introduced for Arabic terms .
Addressing this task for Arabic has to take place in the absence of many resources that are available for English .
In this work we make of use an Arabic sentiment analyzer ) that was developed by our team as well as of a sentiment lexicon that was also developed within by same team and which is publicly available for research purposes ( El- Beltagy 2016 ) .
The main idea behind this work is to try to collect a representative number of tweets for each term for which a strength value is to be derived and to then classify those tweets in terms of polarity .
This classification would then be used for calculating the co-relation between positive sentiments and the original terms using normalized point wise mutual information ( nPMI ) ( Bouma 2009 ) .
However , as will be detailed in section 3.1 , there were cases when tweets were not available for input terms , or when they were too few .
A small sample of development data was also supplied with the task .
We have basically used this data set to test the developed model and to make adjustments when needed .
The rest of this paper is organized as follows : section 2 briefly outlines related work ; section 3 provides an overview of the developed model , section 4 presents the evaluation results and future directions , while section 5 concludes this paper .
Related Work Because sentiment lexicons are an integral part of many sentiment analysis systems , many such lexicons have been developed for the English language .
The most commonly used English lexicons include : SentiWordNet ( Baccianella et al. 2010 ) , Bing Liu 's opinion lexicon ( Liu 2010 ) , and the MPQA subjectivity lexicon ( Wilson et al. 2005 ) , Recently ,
Twitter specific lexicons have also come into existence and are increasing being used .
These include the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon ( Kiritchenko et al. 2014 ) .
However despite the availability of many English lexicons , only a few include a sentiment score , with work on automatically assigning such a score only recently starting to attract attention .
This particular research area was introduced as a subtask in SemEval - 2015 - task10 .
The top performing team for this sub-task , employed word embeddings to train a logistic regression model for assigning scores to sentiment terms ( Astudillo et al. 2015 ) .
The second best performing team , used 6 different sentiment lexicons to score input terms ( 2 manually created , and 4 automatically created ) .
Basically , input terms were compared against entries in the lexicons .
If a term was found in a manually constructed lexicon , it was assigned a value of 1 or - 1 , depending on its polarity .
If it was found in any of the automatically created lexicons , it was assigned the score found in those lexicons .
If it was not found in any of the used lexicons , it was assigned a default value ( Hamdan et al. 2015 ) .
Arabic lexicons are much more scarce than their English counterparts , and are often translated versions of an existing English lexicon .
An example of this is the Arabic translated version of the NRC word emotion association lexicon ( EmoLex ) ( Mohammad & Turney 2013 ) .
There have been some attempts to assign scores to Arabic lexicon terms .
( El- Beltagy & Ali 2013 ) presented a method for semi-automatically building a sentiment lexicon as well as two different approaches for assigning scores to sentiment terms .
The authors also demonstrated that the introduction of sentiment scores can increase the accuracy of sentiment analysis .
( Eskander & Rambow 2015 ) constructed a sentiment lexicon by devising a matching algorithm that tries to match entries in the lexicon of an Arabic morphological analyzer to entries in SentiWordNet ( Baccianella et al. 2010 ) .
When a match is found , a link is created between the lexicon entry and the matching entry in SentiWordNet and the scores of the matching term in SentiWordNet are assigned to that entry .
Model Overview
In order to assign strength scores to a given list of terms ( input terms ) , a number of steps are carried out .
These steps can be summarized as follows :
1 . Collect tweets for input terms 2 . Classify and index collected tweets 3 . Calculate a score for each term Each of the above steps is explained in the following subsections .
Data Collection
Since the goal of our work was to try to determine the correlation between a given term and positive or negative sentiments , we had to obtain a representative set of tweets for each term .
We have chosen to retrieve 500 tweets for each term using Twitter 's search API ( Twitter 2016 ) .
There were cases however , when the search API was unable to retrieve this number of tweets and cases where no tweets were retrieved at all .
It must also be noted that even though the flag that prevents retweets from being retrieved was set when invoking Twitter search , many tweets were in fact identical to other tweets in the tweet set .
To eliminate those from the dataset , they were filtered out using the Jaccard similarity measure ( Leskovec et al. 2014 ) .
For cases when only very few tweets ( less than 15 ) could be retrieved or when no tweets could be retrieved , an extra processing step was performed .
In this step , a check was used to determine if the term in question was a hashtag .
If it was , the hash symbol was removed from the term and so were any underscores .
The resulting phrase was then used to query Twitter .
If the term was not a hashtag or if the step just described also resulted in the retrieval of very few or no tweets , then the term was stemmed using a simple stemmer ( El - Beltagy & Rafea 2011 ) and re-sent to Twitter as a query .
For the supplied 1166 test phrases in the Arabic set , 141 had to undergo these extra processing steps and 15 failed to return any tweets .
In total , approximately 249 K tweets were collected for deriving scores for the 1166 test phrases .
This collection of tweets will henceforth be referred to as the twitter corpus .
Data Classification and Indexing After carrying out the data collection step described in the previous section , each of the collected tweets was classified using the sentiment analyzer described in .
The analyzer was built using a Complement Na?ve Bayes classifier ( Rennie et al. 2003 ) with a smoothing parameter of 1 and trained using 11,242 Arabic tweets of which 3759 were negative , 3725 were positive and 3758 were neutral .
The prediction accuracy using 10 fold cross validation on this dataset was 79.4 % .
Complement Na?ve Bayes was selected as a classifier based on the work presented in ( Khalil et al. 2015 ) .
Features related to the occurrence of positive and negative terms , were part of the feature -set used by the classifier .
These features were determined using the NileULex sentiment lexicon ( El - Beltagy 2016 ) which consists of 5953 single and compound MSA and Egyptian Arabic terms .
The overlap between the input SemEval test set and NileULex was as follows :
Out of the 1166 supplied test terms , 162 ( 13.8 % ) existed in the used lexicon , while 148( 12.6 % ) hash- tagged terms , existed in the lexicon , but without the hashtag .
Some of the terms in both the development set and the test set , were negated .
This was not really an issue when automatically assigning polarity labels , as the obtained label is one that it is relative to the entire phrase .
What had to be handled however , were cases when the retrieved tweets included the negated form of a non-negated term .
For example , when trying to assign a score to term ? , " ? " ?
a tweet with the following text : " @mention ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? " would be classified as negative .
In cases like this , negative labels were converted to positive ones and vice versa .
Each polarity classified tweet was then indexed using the Lucene ( Apache 2011 ) search engine .
The index was used to store the body of the tweet , the polarity class of the tweet , and the search term that was used to retrieve the tweet .
Term Scoring Following the indexing and classification step , it was easy to retrieve information needed to calculate the normalized pmi ( npmi ) scores for each term relative the positive class .
So for each term in the list for which strength values are to be derived , equation 1 was applied to calculate its npmi value relative to positive ( pos ) sentiment . ( , ) = ( , ) ( , ) ( 1 ) where ( , ) = ( , ) ( ) ( ) ( 2 ) ( ) = # ! " # $ % " $ $ & ' ( ' ) * + , - . ( 3 ) ( ) = $ $ / # ) 01 & ' ! " $ " 2 & $ % &&$ * + , - . ( 4 ) ( term , pos ) = # ! $ & ' 0 $ % && $ * + , - . ( 5 ) To calculate the probability of the occurrence of the i th term in the test set ( ( ) ) in the twitter corpus , the number of tweets classified as neutral for this term ( ) are subtracted from the total count of before applying equation 3 .
The total count of neutral tweets in the entire cor-pus is also subtracted from the size of the twitter corpus before applying equations 3 through 5 .
Normalized point wise mutual information returns values in the range of [ - 1,1 ] where - 1 indicates that a term would never occur with the positive class and 1 , indicates that it always will .
In our model , terms for which no tweets were retrieved received a npmi score of 0 .
The nmpi scores of terms that had to undergo extra processing as described in section 3.1 were penalized by multiplying their scores by a penalty factor < 1 .
We have experimented with various factors using the development dataset , and ended up with 0.75 as a penalty factor .
We also experimented with two methods for rescaling the npmi scores to values from 0 to 1 .
In the first model , we used the development dataset , which consisted of 200 terms , and then applied linear regression to map npmi scores to the gold standard scores .
This linear regression model , was then applied on the scores of the supplied test set .
In the second method , we applied simple re-scaling so that the scores would range from 0 to 1 .
When we experimented with both methods on the development dataset , simple re-scaling yielded slightly better results despite the fact that the regression model was built using that same set .
Accordingly , the results that we 've submitted , were the results obtained using simple rescaling rather the by applying linear regression .
Results and Discussion Based on the results supplied by the organizers of the task , the presented approach ranked at number 2 as shown in Table 1 .
We also ran the scoring script provided by the organizers on the results that were obtained using linear regression using the post submission test data with the gold labels .
The results were as follows : Kendall : 0.47524 , Spearman : 0.65756 .
The variation between these results and the submitted results are very minor .
By examining the results , it can be seen that the difference between our score and that achieved by the number 1 performer with respect to the Kendall metric is about 0.06 .
The difference in the Spearman metric is not quite as large .
This difference as well as the score itself , suggests that there is still a need for improving this model .
The following points summarize some of the ideas that we plan on pursuing for improving this model :
1 . Investigate alternative ways for handling the assignment of a score to terms for which no tweets were found .
2 . Some terms had a higher percentage of duplicates than other in their tweet sets .
Duplicate removal in cases like that resulted in those terms being under-represented .
In the future , we intend to make sure that we get a pre-defined number of tweets for each term whenever possible .
3 . We would like to repeat the experiment presented in this paper using tweets collected over a reasonable period of time , rather than in one go as we have done here .
This should counter - act any time sensitivity issues for any given term .
4 . We would like to examine the effect of replacing the sentiment analyzer for tweet labeling , with a method based on simple counting between the co-occurrence of the term for which we need to calculate a score , and known positive and negative terms .
5 . Make better use of the existing lexicon as well as other available lexicons by experimenting with a similar approach to that presented in ( Hamdan et al. 2015 ) .
Conclusion
This paper presented the approach followed by NileTMRG for addressing Sem - Eval task 7 , with respect to Arabic phrases .
While the presented work shows promising results , further work is needed to improve its performance .
In the previous section , we discussed several ideas that we believe might have a positive impact on the overall performance of the system .
Table 1 : 1 Official results ( official metric ) Rank Team name Kendall Spearman 1 iLab- Edinburgh 0.5362 0.67997 2 NileTMRG 0.47515 0.65763 3 LSIS 0.42431 0.58299
