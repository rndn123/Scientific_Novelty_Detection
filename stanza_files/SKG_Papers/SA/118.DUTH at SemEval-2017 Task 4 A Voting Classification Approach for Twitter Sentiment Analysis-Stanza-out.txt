title
DUTH at SemEval - 2017 Task 4 : A Voting Classification Approach for Twitter Sentiment Analysis
abstract
This report describes our participation to SemEval - 2017 Task 4 : Sentiment Analysis in Twitter , specifically in subtasks A , B , and C .
The approach for text sentiment classification is based on a Majority Vote scheme and combined supervised machine learning methods with classical linguistic resources , including bag-of-words and sentiment lexicon features .
Introduction
For millions of users , microblogging services such as Twitter , a popular service where users can post no more than 140 characters status messages , have become an elemental part of daily life .
By using tools and techniques from Natural Language Processing ( NLP ) and machine learning , Sentiment analysis is defined as the process to identify and analyze polarity from short texts , sentences , and documents ( Pang et al. , 2008 ) .
In the last few years , people from different research disciplines are interested in Sentiment Analysis , and the SemEval workshop offers an opportunity to compete and work in this field .
Our team has participated in SemEval - 2017 task 4 on Sentiment Analysis in Twitter , more specifically on subtasks A ( Message Polarity Classification ) , B , and C ( Tweet Classification in either two -point or five- point scale respectively ) ( Rosenthal et al. , 2017 ) .
In this report , we present an ensemble text sentiment classification scheme , based on an extensive empirical analysis of several classifiers and other related works , e.g. ( Balahur , 2013 ; Mart?nez-C?mara et al. , 2014 ; Balikas and Amini , 2016 ; Onan et al. , 2016 ) .
A voting scheme combines learning algorithms to identify and select an optimal set of base learning algorithms .
These com-ponents were carefully combined and optimized to create a separate version of the system for each of the tackled subtasks .
The rest of this report is organized as follows .
The description of proposed system we used and its feature extraction are presented in Section 2 .
Section 3 reports our experiments .
Conclusions and directions for further work / research are summarized in Section 4 .
System Description
The main objective of SemEval - 2017
Task 4 is sentiment classification .
The system we used is based on the bag-of-words representation , n-gram extraction , and usage of lexicons which have a predefined sentiment for every uni-gram and bi-gram .
For the implementation of the system we used Python's Scikit-Learn ( Pedregosa et al. , 2011 ) , as well as NLTK ( Natural Language Toolkit ) ( Bird et al. , 2009 ) .
Pre-processing
The pre-processing steps that we followed were to remove and replace strings from the tweets that do not show any sentiment , as well as to remove duplicates and unicode strings : ?
Removing duplicates : we found that some instances were duplicates , e.g. in Subtask A , so we removed them .
?
Replacing hashtags , URLs and usernames : we first removed the " # " character in front of the words and replaced the twitter oriented strings @usernames and the URLs with tags such as " AT USER " and " URL " respectively .
?
Removing unicode strings : there were many Unicode strings especially in the testing data , e.g. strings like " \u002 c " and " x96 " .
?
Removing numbers and punctuation : preliminary experiments showed better results when we removed all the numbers .
Before removing punctuation , we detected useful punctuation signs such as " ! " and " ? " and replaced them with labels .
?
Using lowercase and tokenization : the final tweets were lower - cased ( after detecting words that had all of their character capitalized which were retained ) and splitted into tokens .
?
Removing stop words : stopwords are common function words with very high frequency among sentences and low content , so we removed them .
?
Using stemming : stemming is the process of reducing a word to its base root form .
Preliminary tests showed that stemming improves a lot the results .
Previous studies ( Pak and Paroubek , 2010 ; Bakliwal et al. , 2013 ) have made references on the influence of pre-processing and proposed a set of features to extract the maximum sentimental information .
Feature Engineering
We extracted features based on the lexical content of each tweet and we also used lexicons .
Below we present all the features .
?
Word n-grams : the word level uni-grams and bi-grams are adopted .
? Number of capitalized words ?
Number of question marks , exclamation marks and the aggregation of them ?
Number of elongated words : it indicates the number of elongated words in the raw text of the tweet .
To identify the sentiment polarity of tweets , we used three different sentiment lexicons during our experiments .
Sentiment lexicons are lexical resources which are formed by a list of words without any additional information and are built by opinion words and some sentiment phrases ( Mart?nez-C?mara et al. , 2014 ) .
In our system we used sentiment lexicons such as Bing Liu 's lexicon ( Hu and Liu , 2004 ) , the NRC emotion lexicon ( Mohammad and Turney , 2010 ) , the MPQA lexicon ( Wilson et al. , 2005 ) and combinations of them .
The above lexicons have a sentiment tag for each word and in our approach we count the occurrences of each sentiment class for each tweet 's word .
Finally , we compute the overall sentiment of the tweet , by adding its words sentiments .
Experiments
In this section , after the feature extraction , we analyse the classification process with the learning methods and classification algorithms that used in our system .
Datasets
The datasets were provided by the organizers and contained all datasets of the previous years with the addition of a new .
For Subtask
A the available datasets were all the training , development , and testing data from the years 2013 to 2016 .
For Subtask
B the available datasets were from the years 2015 to 2016 , and for Subtask C from the year 2016 .
We used a portion of the data for development and the rest for training .
We present them in Tables 1 - 3 .
As we can observe from the tables , the testing data that were provided by the organizers have different ratio among the classes , especially between the positives and negatives .
Positive
Evaluation Metrics For Subtask
A , we use the macro- average recall , which is the recall averaged across the three classes R macro = Rpos+Rneu + Rneg 3 . Subtask
B maintains the same measure , but among the two classes R macro = Rpos+ Rneg 2 . For Subtask C , the official metrics are the macro- averaged mean absolute error and the extension of macro-averaged recall for ordinal regression ( Rosenthal et al. , 2017 ) among 5 predefined classes .
Learning Using all the features described above , we first trained several classifiers to the development data in order to tune the parameters of each classifier .
The main target of tuning was the metric of this specific task , which is the macro-average recall .
We tested a variety of classifiers that include the following : ? Ridge : an algorithm belonging to the Generalized Linear Models family that alleviates the multicollinearity amongst predictor variables .
? Logistic Regression : despite its name it is used for classification and fits a linear model .
It is also known as Maximum Entropy , and uses a logistic function to model the probabilities that describe the output prediction .
? Stochastic Gradient Descent : a simple and efficient algorithm to fit linear models .
It is suitable for very large number of features .
?
Nearest Centroid : an algorithm that uses the center of a class , called centroid , to represent it and has no parameters .
? Bernoulli Na?ve Bayes : an alternative of Na?ve Bayes , where each term is equal to 1 if it exists in the sentence and 0 if not .
Its difference from Boolean Na?ve Bayes is that it takes into account terms that do not appear in the sentence .
?
Linear SVC : an SVM algorithm , which tries to find a set of hyperplanes that separate space into dimensions representing classes .
The hyperplanes are chosen in a way to maximize the distance from the nearest data point of each class .
?
Passive - Aggressive : belongs to a family of algorithms for large-scale learning , which do not require a learning rate and includes a regularization parameter C ( Pedregosa et al. , 2011 ) .
In order to vectorize the collection of raw documents , we used a Python's Scikit-Learn ( Pedregosa et al. , 2011 ) tf -idf transformation with a max df parameter of 0.5 .
The value of this parameter was extracted by the tuning process and indicates that we ignore terms that have a frequency strictly higher than this threshold .
The next step was to use these parameters to test our model with the help of 10 - fold cross-validation on the training set .
Subtask A Subtask
A is a multi-class classification problem , where each tweet has to be classified in one among three classes .
We found that the best combination for this task was the use of stemming and the three lexicons .
Features like the number of exclamation marks , etc. , under-performed .
The three classifiers with the best results were the Bernoulli Na?ve Bayes , the Stochastic Gradient Descent ( SGD ) , and the Linear SVC .
The final step was to use the majority voting classification method that combines three different classifiers and outputs the class that the majority of them agreed .
Using all possible combinations of every three classifiers , the best result was with the Bernoulli Na?ve Bayes , SGD , and Nearest Centroid .
Note that Nearest Centroid was one of the weakest classifiers in isolation , but presented an excellent contribution when combined with other two .
Subtask B Subtask
B is a topic-based binary classification problem , where each tweet belongs to a topic , and one has to classify whether the tweet conveys a positive or negative sentiment towards the topic .
We used the same approach with Subtask A , with the addition of a weight for the topic which was added as a feature .
The best combination was the use of stemming and the three lexicons , like in subtask A .
The three best classifiers were the SGD , the Passive -Aggressive , and the Linear SVC .
The majority voting classifier outperformed all the single classifiers ; here , the best result was with the SGD , Logistic Regression , and Ridge classifiers , showing once again that weak classifiers can contribute significantly when combined with others .
Subtask C Subtask
C is also a topic-based classification problem , where each tweet belongs to a topic , and one has to estimate the sentiment conveyed by the tweet towards the topic on a five-point scale .
The same approach as with Subtask B was used , and the best result was achieved by the combination of the Logistic Regression , the Nearest Centroid , and the Bernoulli Na?ve Bayes classifiers .
Conclusions & Future work
By analyzing and classifying sentiments on Twitter , people can comprehend attitudes about particular topics , making Sentiment Analysis an attractive research area .
In this report we presented an approach for Twitter sentiment analysis on twopoint , three - point , and five-point scale , based on a voting classification method .
This was our first contact with the task of sentiment analysis and compared with the top-ranked participating systems , there seems to be for us much room for improvement .
In future work , we consider to focus on adding more pre-processing methods such as spelling correction and POS tagging .
We also consider adding more features such as emoticons , negation , character n-grams and more lexicons .
Table 1 : 1 Number of tweets in training ( train ) , development ( dev ) , and testing ( test ) data for subtask A. Positive Negative Neutral Total train 18377 ( 38 % ) 7442 ( 16 % ) 22012 ( 46 % ) 47831 dev 2412 ( 43 % ) 1056 ( 18 % ) 2185 ( 39 % ) 5653 test 2375 ( 19 % ) 3972 ( 32 % ) 5937 ( 49 % ) 12284
Table 2 : 2 Negative Total train 12812 ( 79 % ) 3410 ( 21 % ) 16222 dev 2139 ( 78 % ) 604 ( 22 % ) 2743 test 2463 ( 40 % ) 3722 ( 60 % ) 6185
Number of tweets in training ( train ) , development ( dev ) , testing ( test ) data for subtask B .
Table 3 : 3 Number of tweets in training ( train ) , development ( dev ) , testing ( test ) data for subtask C.
Table 4 : 4 DUTH 's results for SemEval - 2017
Task 4 on Sentiment Analysis in Twitter ( Rosenthal et al. , 2017 ) . ? F P N 1 Acc Task A 0.621 0.605 0.640 Task B 0.663 0.600 0.607 ( M AE M ) ( M AE ? ) Task C 0.895 0.544
