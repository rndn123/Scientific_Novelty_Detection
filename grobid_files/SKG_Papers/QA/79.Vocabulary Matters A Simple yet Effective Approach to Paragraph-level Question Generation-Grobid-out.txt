title
Vocabulary Matters: A Simple yet Effective Approach to Paragraph-level Question Generation

abstract
Question generation (QG) has recently attracted considerable attention. Most of the current neural models take as input only one or two sentences and perform poorly when multiple sentences or complete paragraphs are given as input. However, in real-world scenarios, it is very important to be able to generate high-quality questions from complete paragraphs. In this paper, we present a simple yet effective technique for answer-aware question generation from paragraphs. We augment a basic sequence-to-sequence QG model with dynamic, paragraph-specific dictionary and copy attention that is persistent across the corpus, without requiring features generated by sophisticated NLP pipelines or handcrafted rules. Our evaluation on SQuAD shows that our model significantly outperforms current state-of-theart systems in question generation from paragraphs in both automatic and human evaluation. We achieve a 6-point improvement over the best system on  from 16.38 to 22.62.

Introduction and Related work Automatic question generation (QG) from text aims to generate meaningful, relevant, and answerable questions from a given textual input. Owing to its applicability in conversational systems such as Cortana, Siri, chatbots, and automated tutoring systems, QG has attracted considerable interest in both academia and industry. Recent neural network-based approaches  (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018; Zhao et al., 2018; Song et al., 2018; Subramanian et al., 2018; Tang et al., 2017; Wang et al., 2017)  represent the state-of-the-art in question generation. Most of these techniques learn to generate questions from short text, i.e., one or two sentences  (Du et al., 2017; Kumar et al., 2018a,b; Du and Cardie, 2018) . On the other hand, the ability to generate high-quality questions from longer text such as from multiple sentences or from a paragraph in its entirety, is more useful in real-world settings. However, given that a paragraph contains a longer context and more information than a sentence, it is a significantly more challenging problem to generate questions around a longer context. In figure 1 we present one motivating example demonstrating why the model needs information more than just a single sentence for generating question a meaningful and relevant question. As we can see in figure  1 , question 2, question generated by our model use multiple sentences as context.  Du et al. (2017)  recently observed that 20% of the questions in the SQuAD dataset  (Rajpurkar et al., 2016 ) require paragraphlevel information to answer them. For the same reason, it is intuitive to conclude that the ability to consider the complete context; however long it may be, is critical for generating high-quality questions. Legislative power in Warsaw is vested in a unicameral Warsaw City Council ( Rada Miasta ) , which comprises 06 members . Council members are elected directly every four years . Like most legislative bodies , the City Council divides itself into committees which have the oversight of various functions of the city government . Bills passed by a simple majority are sent to the mayor ( the President of Warsaw ) , who may sign them into law . If the mayor vetoes a bill , the Council has 30 days to override the veto by a two-thirds majority vote . Human Generated: How many days does the Council have to override the mayor 's veto ? Our Model: How long does it take to override the veto ? Figure  1 : Examples of ground-truth questions and questions generated by our model from the same paragraph. Each question and its corresponding answer are highlighted using the same color.  Zhao et al. (2018)  very recently proposed a technique (referred to MPGSN here) for paragraph-level question generation using a max out pointer mechanism and a gated self-attention encoder. Their best model achieves BLEU-4 of 16.38 on SQuAD with paragraphs as input. Compared to  (Zhao et al., 2018) , our model has less number of parameters (making it more computationally efficient), is relatively easy to train and is somewhat deterministically biased toward the generation of important words in the input paragraph. In this paper, we propose a simple yet effective paragraph-level question generation technique. We augment the standard sequence-to-sequence model based on bidirectional LSTM with two components: (1) a dynamic, paragraph-specific dictionary and (2) a copy attention mechanism that is persistent across paragraphs. Our evaluation on SQuAD shows significant improvement over MPGSN in automatic evaluation. We achieve a 6-point increase with respect to BLEU-4 (from 16.38 to 22.62) over MPGSN's best system. We perform the human evaluation of our model with and without copy attention, and we observe that we obtain 27% more relevant questions when the copy attention is incorporated. For a given paragraph as input, we depict in Figure  1 , the ground-truth questions as well as the questions generated along with the answers highlighted in the paragraph. As can be seen from the example, while generating the second question(highlighted in green color), our model uses information not only from the sentence containing the answer, but also relevant context from the complete paragraph.  

 Problem Formulation & Approach Given a paragraph 'P ' and answer 'A', a question generation model iteratively samples question word q t ? V Q at every time step 't' from the probability distribution given by: Pr(Q|P,A;?) = |Q| t=1 Pr(q t |P,A;?) (1) Where V Q is the question vocabulary, ? is the set of parameters, and A is the answer. Our question generation model consists of a two-layer paragraph encoder and a one-layer question decoder, equipped with a dynamic dictionary and copy attention. In Figure  2 , we illustrate the overall architecture of our paragraph level question generation model. The dynamic dictionary allows every training instance (paragraph) to have its own vocabulary instead of relying on the preprocessed global vocabulary. Copy attention enables the model to predict question words from the extended vocabulary (complete vocabulary + paragraph vocabulary). Copy attention operates over the union of words in vocabulary and paragraph words. 

 Paragraph encoder We use a two-layer bidirectional long short-term memory (Bi-LSTM) network stack as the paragraph encoder. The paragraph encoder takes an answer-tagged paragraph as input and outputs a representation of the paragraph. Note that the Bi-LSTM network processes the input paragraph in both the forward and backward directions: ? ? h t = LST M (e t , ? ? ? h t?1 ) and ? ? h t = LST M (e t , ? ? ? h t+1 ), where ? ? h t (resp. ? ? h t ) is the forward (resp. backward) hidden state at time step t and e t is the vector representation of current input x t at time step t. The final hidden state for the current word input is the concatenation of the forward and backward hidden state vectors: h t = [ ? ? h t , ? ? h t ]. 

 Dynamic, shared dictionary In the traditional approach, a new/unknown word is typically replaced with the "<unk>" token. The copy mechanism  (Gu et al., 2016)  then unfortunately learns to copy this "<unk>" token instead of the actual (unknown) word from the source paragraph. Instead, we use a separate dynamic dictionary unique to each source paragraph, which includes all and only words that occur in the paragraph. This allows our model to copy source words that may not be in the target dictionary into the target (question). Using a dynamic dictionary consisting of the preprocessed vocabulary instead of a static one enables the copy mechanism to copy the exact words directly into the question, even if they are rare and unknown. Given a source paragraph p, we denote its dynamic vocabulary by V p . Our copy attention mechanism takes into account V p and the global vocabulary V to determine whether to copy a word from V p or to predict a word from question vocabulary V Q . As our model's source as well as target are in the same language, we work with a shared source and target vocabulary, though we learn different language models for the paragraph and the question. Sharing source and target vocabulary also decreases the memory requirement resulting from matrix multiplication (thus making faster training through larger batch size) possible. It also enables efficient question decoding, thus reducing the time for inference on the test data. 

 Question decoder Our question decoder is another Bi-LSTM that takes as input the last hidden state and context representation from the encoder and generates question words sequentially based on the previously generated words. The decoder hidden state (s t = [ ? ? s t , ? ? s t ]) at time step t is the concatenation of the forward and backward hidden state representations: ? ? s t = LST M (o t , ? ? ? s t?1 ) and ? ? s t = LST M (o t , ? ? ? s t+1 ) , where o t is the vector representation of decoder input (y t ) at time step t. During training time the vector representation of words from the ground-truth question is fed as decoder input, and during test time the vector representation of the vocabulary word with maximum probability is fed as input. We feed EOS symbol as input to decoder from both forward and backward dircetion at time t 0 . Bidirectional decoder factorizes the conditional decoding probabilities in both directions (left-to-right and right-to-left) into summation as: P y t |[y m ] m =t = ? ? ? logp y t |Y [1:t?1] + ? ? ? logP y t |Y [t+1:Ty] (2) The probability distribution over words in the vocabulary is calculated as: Pr(q t ) = sof tmax(W g ?(W s [s t ,h t ]+b s )+b g ) (3) where W g , W s , b s and b g are trainable model parameters. Probability distribution P (q t ) uses the standard softmax over the question vocabulary V Q . This is used to sample word with maximum probability while decoding a question. 

 Copy attention We know that a good question should be relevant to (answerable from) the paragraph. So we learn a probabilistic mixture model over the question vocabulary V Q and the current paragraph vocabulary V P . The current paragraph vocabulary is generated by a dynamic dictionary module. Our copy attention calculates two values: cs: a binary-valued variable which acts a switch between copying a word from the paragraph's dynamic vocabulary V P or generating from the question vocabulary V Q Pr .|V P : probability of copying a particular word from paragraph vocabulary V P . Therefore, the final probability distribution from which a word will be sampled while generating a question is calculated over the extended vocabulary V Q ? V p . Given a word from the extended vocabulary w ? V Q ?V P , its probability Pr(w) is computed as: Pr(w) =Pr(cs = 1)Pr w|V P + Pr(cs = 0)Pr w|V Q (4) The switch probability Pr(cs) is determined using the decoder hidden states as: Pr(cs = 1) = ?(W cs s t +b cs ) (5) where W cs and b cs are trainable model parameters. Pr w|V Q is the probability of predicting a word from complete vocabulary V Q . The copy attention weight a t is computed as: e t i = v T tanh(W h h i +W s s t +b attn ) (6) a t = sparsemax(e t ) (7) Where v, W h , W s and b attn are trainable model parameters. The probability of copying a word from the paragraph vocabulary V P is estimated as: Pr w|V P = ?(W a a t +b a ) (8) where W a and b a are trainable model parameters.  

 Experimental Setup We report the experimental result of our model (referred to as NQG dd ) and compare it with the current state of the art MPGSN  (Zhao et al., 2018) . We employ the widely-used metrics BLEU  (Papineni et al., 2002) , ROUGE-L and METEOR for automatic evaluation. We use evaluation script provided by  (Chen et al., 2015) . Similar to  (Kumar et al., 2018a)  we also report qualitative assessment on the syntax, semantics and relevance of the questions generated by our model. All experiments are performed on the SQuAD dataset  (Rajpurkar et al., 2016) , where complete paragraphs are taken as input instead of just one or two sentences. We reformat the SQuAD dataset such that during training time, each source instance is a (paragraph, question) pair annotated with the gold answers, and the target is a question. Following the exact setup from MPGSN  (Zhao et al., 2018) , we split the SQuAD train set into train and validation set containing 77,526 and 9,995 instances respectively, and take the separate SQuAD dev set containing 10,556 instances as our test set. 

 Results and Analysis Table  1  summarizes results of the automatic evaluation of the test set. As can be seen, our model significantly outperforms the state-of-the-art MPGSN on all metrics. The improvements on BLEU are especially substantial, the BLEU-4 score of MPGSN is 16.38, and ours (with copy incorporated) is 22.62, an improvement of 6.24, or 38%. This large performance difference demonstrates the effectiveness of our dynamic dictionary. In Table  2  we present human evaluation results. We evaluate the quality of questions generated in terms on syntactic correctness, semantic correctness and relevance to the paragraph. The evaluation is performed on a randomly selected subset of 100 sentences from the test set. Each of the three evaluators are presented the 100 paragraph-question pairs for two variants of our model (with and without copy) and asked for a binary responses for all three parameters. We averaged responses received by all three evaluators to compute the final scores. As can be seen, the incorporation of the copy attention improves performance, especially on relevance. We also measure the inter-rater agreement using Randolph's free-marginal multirater kappa  (Randolph, 2005) . It can be observed that our quality metrics for both our models are rated as substantial agreement  (Viera et al., 2005) . To explain how our model attends to different words in the source paragraph we visualize attention weights in Figure  3 , which shows attention weights between question 2 generated by our model and the corresponding paragraph in Figure  1 . We observe that the attention weight is high for words near the answer and the model attends to all relevant context rather that just the sentence containing the answer. Table  2 : Human evaluation results (columns "Score") as well as inter-rater agreement (columns "Kappa") for each of our two models on 100 questions from the test set. The scores are between 0 (worst) and 100 (best). Best results for each metric (column) are in bold. We also note that our training is faster atleast by a factor of 2. We expected this since we replace a slightly expensive self-attention mechanism in the decoder of  (Zhao et al., 2018)  with a simpler dynamic dictionary and reusable copy attention. 

 Conclusion Paragraph-level question generation (QG) is an important but challenging problem, mainly due to the challenge in effectively handling a longer context. We present a simple yet effective approach for automatic question generation from paragraphs. Besides using a standard global source dictionary, our RNN-based model incorporates a dynamic, paragraph-specific dictionary, and learns to switch between copying from the combined Figure 2 : 2 Figure 2: Overall architecture of our paragraph-level question generation model. 
