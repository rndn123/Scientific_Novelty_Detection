title
Partial Or Complete, That Is The Question

abstract
For many structured learning tasks, the data annotation process is complex and costly. Existing annotation schemes usually aim at acquiring completely annotated structures, under the common perception that partial structures are of low quality and could hurt the learning process. This paper questions this common perception, motivated by the fact that structures consist of interdependent sets of variables. Thus, given a fixed budget, partly annotating each structure may provide the same level of supervision, while allowing for more structures to be annotated. We provide an information theoretic formulation for this perspective and use it, in the context of three diverse structured learning tasks, to show that learning from partial structures can sometimes outperform learning from complete ones. Our findings may provide important insights into structured data annotation schemes and could support progress in learning protocols for structured tasks.

Introduction Many machine learning tasks require structured outputs, and the goal is to assign values to a set of variables coherently. Specifically, the variables in a structure need to satisfy some global properties required by the task. An important implication is that once some variables are determined, the values taken by other variables are constrained. For instance, in the temporal relation extraction problem in Fig.  1a , if met happened before leaving and leaving happened on Thursday, then we know that met must either be before Thursday ("met (1)") or has to happen on Thursday, too ("met (2)")  (Ning et al., 2018a) . Similarly, in the semantic frame of the predicate gave  (Kingsbury and Palmer, 2002)  in Fig.  1b , if the boy is ARG0 (short for argument 0), then it rules out the possibility of a frog The boy gave a frog to the girl. or to the girl taking the same role. Figure  1c  further shows an example of part-labeling of images  (Choi et al., 2018) ; given the position of FORE-HEAD and LEFT EYE of the cat in the picture, we roughly know that its NECK should be somewhere in the red solid box, while the blue dashed box is likely to be wrong. Data annotation for these structured tasks is complex and costly, thus requiring one to make the most of a given budget. This issue has been investigated for decades from the perspective of active learning for classification tasks  (Angluin, 1988; Atlas et al., 1990; Lewis and Gale, 1994)  and for structured tasks  (Roth and Small, 2006a,b, 2008; Hu et al., 2019) . While active learning aims at selecting the next structure to label, we try to investigate, from a different perspective, whether we should annotate each structure completely or partially. Conventional annotation schemes typically require complete structures, under the common perception that partial annotation could adversely affect the performance of the learning algorithm. But note that partial annotations will allow for more structures to be annotated (see Fig.  2 ). Therefore, a fair comparison should be done while maintaining a fixed annotation budget, which was not done before. Moreover, even if partial annotation leads to comparable learning performance to conventional complete schemes, it provides more flexibility in data annotation. Another potential benefit of partial annotation is that it imposes constraints on the remaining parts of a structure. As illustrated by Fig.  1 , with partial annotations, we already have some knowledge about the unannotated parts. Therefore, further annotations of these variables may use the available budget less efficiently; this effect was first discussed in  Ning et al. (2018c) . Motivated by the observations in Figs. 1-2, we think it is important to study partialness systematically, before we hastily assume that completeness should always be favored in data collection. To study whether the above benefits of partialness can offset its weakness for learning, our first contribution is the proposal of early stopping partial annotation (ESPA) scheme, which randomly picks up instances to label in the beginning, and stops before a structure is completed. We do not claim that ESPA should always be preferred; instead, it serves as an alternative to conventional, complete annotation schemes that we should keep in mind, because, as we show later, it can be comparable to (and sometimes even better than) complete annotation schemes. ESPA is straightforward to implement even in crowdsourcing; instances to annotate can be selected offline and distributed to crowdsourcers; this can be contrasted with the difficulties of implementing active learning protocols in these settings  (Ambati et al., 2010; Laws et al., 2011) . We think that ESPA is a good representative for a systematic study of par-tialness.  Our second contribution is the development of an information theoretic formulation to explain the benefit of ESPA (Sec. 2), which we further demonstrate via three structured learning tasks in Sec. 4: temporal relation (TempRel) extraction  (UzZaman et al., 2013) , semantic role classification (SRC), 1 and shallow parsing  (Tjong Kim Sang and Buchholz, 2000) . These tasks are chosen because they each represent a wide spectrum of structures that we will detail later. As a byproduct, we extend constraint-driven learning (CoDL)  (Chang et al., 2007)  to cope with partially annotated structures (Sec. 3); we call the algorithm Structured Selflearning with Partial ANnotations (SSPAN) to distinguish it from CoDL.  2  We believe in the importance of work in this direction. First, partialness is inevitable in practice, either by mistake or by choice, so our theoretical analysis can provide unique insight into understanding partialness. Second, it opens up opportunities for new annotation schemes. Instead of considering partial annotations as a compromise, we can in fact annotate partial data intentionally, allowing us to design favorable guidelines and collect more important annotations at a cheaper price. Many recent datasets that were collected via crowdsourcing are already partial, and this paper provides some theoretical foundations for them. Furthermore, the setting described here addresses natural scenarios where only partial, indirect supervision is available, as in Incidental Supervision  (Roth, 2017) , and this paper begins to provide theoretical understanding for this paradigm, too. Further discussions can be found in Sec. 5. It is important to clarify that we assume uniform cost over individual annotations (that is, all edges in Fig.  2  cost equally), often the default setting in crowdsourcing. We realize that the annotation difficulty can vary a lot in practice, sometimes incurring different costs. To address this issue, we randomly select instances to label so that on average, the cost is uniform. We agree that, even with this randomness, there could still be situations where the assumption does not hold, but we leave it for future studies, possibly in the context of active learning schemes. 

 ESPA: Early Stopping Partial Annotation In this section, we study whether the effect demonstrated by the examples in Fig.  1   It is necessary to model a structure as a set of random variables because when it is not completely annotated, there is still uncertainty in the annotation assignment. To study partial annotations, we introduce the following: Definition 2. A k-step annotation (0 ? k ? d) is a vector of RVs A k = [A k,1 , . . . , A k,d ] 2 (L [ u) d where u is a special character for null, such that d X i=1 1(A k,i 6 = u) = k, (1) P (Y|A k = a k ) = P (Y|Y j = a k,j , j 2 J ) , ( 2 ) where J is the set of indices that a k,j 6 = u. Eq. (1) means that, in total, k variables are already annotated at step k. Obviously, A 0 means that no variables are labeled, and A d means that all variables in Y are determined. A k is what we call a k-step ESPA, so hereafter we use k/d to represent annotation completeness. Eq. (  2 ) assumes no annotation mistakes, so if the i-th variable is labeled, then Y i must be the same as A k,i . To measure the theoretical benefit of A k , we propose the following quantity I k = log |C(L d )| E [log f (a k )] (3) for k = 0, . . . , d, where f (a k ) = |{y 2 C(L d ) : P (y|a k ) > 0}| is the total number of structures in C(L d ) that are still valid given A k = a k . Since we assume that the labeled variables in A k are se- lected uniformly randomly, E [?] is simply the av- erage of log f (a k ). When k = 0, f (a k ) ? C(L d ) and I 0 ? 0; as k increases, I k increases since the structure has more and more variables labeled; finally, when k = d, the structure is fully determined and I d ? log |C(L d )|. The first-order finite difference, I k I k 1 , is the benefit brought by annotating an additional variable at step k; if I k is concave (i.e., a decaying I k I k 1 ), the benefit from a new annotation attenuates, suggesting the potential benefit of the ESPA strategy. In an extreme case where the structure is so strong that it requires all individual variables to share the same label, then labeling any variable is sufficient for determining the entire structure. Intuitively, we do not need to annotate more than one variable. Our I k quantity can support this intuition: The structural constraint, C(L d ), contains only |L| elements: {[`i, `i, . . . , `i]} |L| i=1 , so I 0 = 0, and I 1 = ? ? ? = I d = log |L|. Since I k does not increase at all when k >= 1, we should adopt firststep annotation A 1 . Another extreme case is that of a trivial structure that has no constraints (i.e., C(Y d ) = Y d ). The annotation of all variables are independent and we gain no advantage from skipping any variables. This intuition can be supported by our I k analysis as well: Since I k = k log |L|, 8k = 0, 1, . . . , d, I k is linear and all steps contribute equally to improving I k by log |L|; therefore ESPA is not necessary. Real-world structures are often not as trivial as the two extreme cases above, but I k can still serve as a guideline to help determine whether it is beneficial to use ESPA. We next discuss three diverse types of structures and how to obtain I k for them. Example 1. The ranking problem is an important machine learning task and often depends on pairwise comparisons, for which the label set is L = {<, >}. For a ranking problem with n items, there are d = n(n 1)/2 pairwise comparisons in total. Its structure is a chain following the transitivity constraints, i.e., if A < B and B < C, then A < C.   3 ), were obtained through averaging 1000 experiments. We use base-2 logarithm and the unit on y-axis is thus "bit". A k-step ESPA A k for a chain means that only k (out of d) pairs are compared and labeled, resulting in a directed acyclic graph (DAG). In this case, f (a k ) is actually counting the number of linear extensions of the DAG, which is known to be #P-complete  (Brightwell and Winkler, 1991) , so we do not have a closed-form solution to I k . In practice, however, we can use the Kahn's algorithm and backtracking to simulate with a relatively small n, as shown by Fig.  3 , where n = 10 and I k was obtained through averaging 1000 random simulations. I k is concave, as reflected by the downward shape of I k I k 1 . Therefore, new annotations are less and less efficient for the chain structure, suggesting the usage of ESPA. Example 2. The general assignment problem requires assigning d agents to d 0 tasks such that the agent nodes and the task nodes form a bipartite graph (without loss of generality, assume d ? d 0 ). That is, an agent can handle exactly one task, and each task can only be handled by at most one agent. Then from the agents' point of view, the label set for each of them is L = {1, 2, . . . , d 0 }, denoting the task assigned to the agent. A k-step ESPA A k for this problem means that k agents are already assigned with tasks, and f (a k ) is to count the valid assignments of the remaining tasks to the remaining d k agents, to which we have closed-form solutions: f (a k ) = (d 0 k)! (d 0 d)! , 8a k . According to Eq. (3), I k = log d 0 ! (d 0 k)! regardless of d or the distribution of A k , and is concave (Fig.  4  shows an example of it when d = 4, d 0 = 10). Example 3. Sequence tagging is an important NLP problem, where the tags of tokens are interdependent. Take chunking as an example. A basic scheme is for each token to choose from three labels, B(egin), I(nside), and O(utside), to represent text chunks in a sentence. That is, L = {B, I, O}. Obviously, O cannot be immediately followed by I. Let d be the number of tokens in a sentence. A k-step ESPA A k for chunking means that k tokens are already labeled by B/I/O, and f (a k ) counts the valid BIO sequences that do not violate those existing annotations. Again, as far as we know, there is no closed-form solution to f (a k ) and I k , but in practice, we can use dynamic programming to obtain f (a k ) and then I k using Eq. (  3 ). We set d = 10 and show I k I k 1 for this task in Fig.  4 , where we observe the same effect we see in previous examples: The benefit provided by labeling a new token in the structure attenuates. Interestingly, based on Fig.  4 , we find that the slope of I k I k 1 may be a good measure of the "tightness" or "strength" of a structure. When there is no structure at all, the curve is flat (black). The BIO structure is intuitively simple, and it indeed has the flattest slope among the three structured tasks (purple). When the structure is a chain, the level of uncertainty goes down rapidly with every single annotation (think of standard sorting algorithms); the constraint is intuitively strong and in Fig.  4 , it indeed has a steep slope (blue). Finally, we want to emphasize that the definition of I k in Eq. (  3 ) is in fact backed by information theory. When we do not have prior information about Y, we can assume that Y follows a uniform distribution over C(L d ). Then, I k is essentially the mutual information between structure Y and annotation A k , I(Y; A k ): I(Y; A k ) = H(Y) H(Y|A k ) = log |C(L d )| E [H(Y|A k = a k )] = log |C(L d )| E [log f (a k )] , where H(?) is the entropy function. This is an im-portant discovery, since it points out a new way to view a structure and its annotations. It may be useful for studying active learning methods for structured tasks, and other annotation phenomena such as noisy annotations. The usage of mutual information also aligns well with the information bottleneck framework  (Shamir et al., 2010; Shwartz-Ziv and Tishby, 2017; Yu and Principe, 2018) , although a more recent paper challenges the interpretation of information bottleneck  (Saxe et al., 2018) .  

 Learning from Partial Structures So far, we have been advocating the ESPA strategy to maximize the information we can get from a fixed budget. Since early stopping leads to partial annotations, one missing component before we can benefit from it is an approach to learning from partial structures. In this study, we assume the existence of a relatively small but complete dataset that can provide a good initialization for learning from a partial dataset, which is very similar to semi-supervised learning (SSL). SSL, in its most standard form, studies the combined usage of a labeled set T = {(x i , y i )} i and an unlabeled set U = {x j } j , where the x's are instances and y's are the corresponding labels. SSL gains information about p(x) through U , which may improve the estimation of p(y|x). Specific algorithms range from self-training  (Scud-der, 1965; Yarowsky, 1995) , co-training  (Blum and Mitchell, 1998) , generative models  (Nigam et al., 2000) , to transductive SVM  (Joachims, 1999)  etc., among which one of the most basic algorithms is Expectation-Maximization (EM)  (Dempster et al., 1977) . By treating them as hidden variables, EM "marginalizes" out the missing labels of U via expectation (i.e., soft EM) or maximization (i.e., hard EM). For structured ML tasks, soft and hard EMs turn into posterior regularization (PR)  (Ganchev et al., 2010)  and constraintdriven learning (CoDL)  (Chang et al., 2007) , respectively. Unlike unlabeled data, the partially annotated structures caused by early stopping urge us to gain information not only about p(x), but also from their labeled parts. There have been many existing work along this line  (Tsuboi et al., 2008; Fernandes and Brefeld, 2011; Hovy and Hovy, 2012; Lou and Hamprecht, 2012) , but in this paper, we decide to extend CoDL to cope with partial annotations due to two reasons. First, CoDL, which itself can be viewed as an extension of self-training to structured learning, is a wrapper algorithm having wide applications. Second, as its name suggests, CoDL learns from U by guidance of constraints, so partial annotations in U are technically easy to be added as extra equality constraints. Algorithm 1 describes our Structured Selflearning with Partial ANnotations (SSPAN) algorithm that learns a model H. The same as CoDL, SSPAN is a wrapper algorithm requiring two components: LEARN and INFERENCE. LEARN attempts to estimate the local decision function for each individual instance regardless of the global constraints, while INFERENCE takes those local decisions and performs a global inference. Lines 3-9 are the procedure of self-training, which iteratively completes the missing annotations in P and learns from both T and the completed version of P (i.e., P). 3 Line 6 requires that the inference follows the structural constraints inherently in the task, turning the algorithm into CoDL; Line 7 enforces those partial annotations in a i , further turning it into SSPAN. In practice, IN-FERENCE can be realized by the Viterbi or beam search algorithm in sequence tagging, or more generally, by Integer Linear Programming (ILP)  (Punyakanok et al., 2005) ; either way, the partial constraints of Line 7 can be easily incorporated. Algorithm 1: Structured Self-learning with Partial Annotations (SSPAN) Input: T = {(xi, yi)} N i=1 , P = {(xi, ai)} N +M i=N +1 1 Initialize H = LEARN(T ) 2 while convergence criteria not satisfied do 3 P = ; 4 foreach (x i , a i ) 2 P do 5 ?i = INFERENCE(x i ; H), such that 6 ? ?i 2 C(Y d ) 7 ? ?i,j = a i,j , 8a i,j 6 = u 8 P = P [ {(x i , ?i )} 9 H = LEARN(T + P) 10 return H 

 Experiment In Sec. 2, we argued from an information theoretic view that ESPA is beneficial for structured tasks if we have a fixed annotation resource. We then proposed SSPAN in Sec. 3 to learn from the resulting partial structures. However, on one hand, there is still a gap between the I k analysis and the actual system performance; on the other hand, whether the benefit can be realized in practice also depends on how effective the algorithm exploits partial annotations. Therefore, it remains to be seen how ESPA works in practice. Here we use three NLP tasks: temporal relation (TempRel) extraction, semantic role classification (SRC), and shallow parsing, analogous to the chain, assignment, and BIO structures, respectively. For all tasks, we compare the following two schemes in Fig.  5 , where we use graph structures for demonstration. Initially, we have a relatively small but complete dataset T 0 , an unannotated dataset U 0 , and some budget to annotate U 0 . The conventional scheme I, also our baseline here, is to annotate each structure completely before randomly picking up the next one. Due to the limited budget, some U 0 remain untouched (denoted by U ). The proposed scheme II adopts ESPA so that all structures at hand are annotated but only partially. For fair comparisons, we use CoDL to incorporate U into scheme I as well. Finally, the systems trained on the dataset from I/II via CoDL/SSPAN are evaluated on unseen but complete testset T test . Note that because ESPA is a new annotation scheme, there exists no dataset collected this way. We use existing complete datasets and randomly throw out some annotations to mimic ESPA in the following. Due to the randomness in selecting which structures/instances to keep in scheme I/II, we repeat the whole process multiple times and report the mean F 1 . The budget, defined as the total number of individual instances that can be annotated, ranges from 10% to 100% with a stepsize of 10%, where x% means x% of all instances in U 0 can be annotated. (I) Complete (II) ESPA ! " same budget same budget ! # $ % % Training Phase ! &'(& 

 Testing Phase CoDL: ! " , ! # and $ SSPAN: ! " and % $ " $ " Figure  5 : The two annotation schemes we compare in Sec. 4. T , P, and U denote complete, partial, and empty structures, respectively. Both schemes start with a complete and relatively small dataset and an unannotated dataset (green). (I) Conventional complete annotation scheme (blue). (II) The proposed ESPA scheme (red). Finally, they are tested on an unseen and complete dataset (black). 

 Temporal Relation Extraction Temporal relations (TempRel) are a type of important relations representing the temporal ordering of events described by natural language text. That is to answer questions like which event happens earlier or later in time (see Fig.  1a ). Since time is physically one-dimensional, if A is before B and B is also before C, then A must be before C. In practice, the label set for TempRels can be more complex, e.g., with labels such as SIMULTA-NEOUS and VAGUE, but the structure can still be represented by transitivity constraints (see Table  1  of  (Ning et al., 2018a) ), which can be viewed as an analogy of the chain structure in Example 1. To avoid missing relations, annotators are required to exhaustively label every pair of events in a document (i.e., the complete annotation scheme), so it is necessary to study ESPA in this context. Here we adopt the MATRES dataset  (Ning et al., 2018b)  for its better inter-annotator agreement and relatively large size. Specifically, we use 35 documents as T 0 (the TimeBank-Dense section, 4 147 documents as U 0 (the TimeBank section minus those documents in T 0 ), and the Platinum section (a benchmark testset of 20 documents with 1K TempRels) as T test . Note that both schemes I and II are mimicked by downsampling the original annotations in MATRES, where the budget is defined as the total number of TempRels that are kept. Following CogComp-Time  (Ning et al., 2018d) , we choose the same features and sparse-averaged perceptron algorithm as the LEARN component and ILP as INFERENCE for SSPAN. 

 Semantic Role Classification (SRC) Semantic role labeling (SRL) is to represent the semantic meanings of language and answer questions like Who did What to Whom and When, Where, How  (Palmer et al., 2010) . Semantic Role Classification (SRC) is a subtask of which assumes gold predicates and argument chunks and only classifies the semantic role of each argument. We use the Verb SRL dataset provided by the CoNLL-2005 shared task  (Carreras and M?rquez, 2005) , where the semantic roles include numbered arguments, e.g., ARG0 and ARG1, and argument modifiers, e.g., location (AM-LOC), temporal (AM-TMP), and manner (AM-MNR) (see Prop-Bank  (Kingsbury and Palmer, 2002) ). The structural constraints for SRC is that each argument can be assigned to exactly one semantic role, and the same role cannot appear twice for a single verb, so SRC is an assignment problem as in Example 2. Specifically, we use the Wall Street Journal (WSJ) part of Penn TreeBank III  (Marcus et al., 1993) . We randomly select 700 sentences from the Sec. 24 of WSJ, among which 100 sentences as T 0 and 600 sentences as U 0 . Our T test is 5700 sentences (about 40K arguments) from Secs. 00, 01, 23. The budget here is defined as the total num-ber of the arguments. We adopt the SRL system in CogCompNLP  (Khashabi et al., 2018)  and uses the sparse averaged perceptron as LEARN and ILP as INFERENCE. 

 Shallow Parsing Shallow parsing, also referred as chunking, is a fundamental NLP task to identify constituents in a sentence, such as noun phrases (NP), verb phrases (VP), and adjective phrases (ADJP), which can be viewed as extending the standard BIO structure in Example 3 with different chunk types: B-NP, I-NP, B-VP, I-VP, B-ADJP, I-ADJP, . . . , O. We use the chunking dataset provided by the CoNLL-2000 shared task  (Tjong Kim Sang and Buchholz, 2000) . Specifically, we use 2K tokens' annotations as T 0 , 14K tokens as U 0 , and the benchmark testset (25K tokens) as T test . The budget here is defined as the total number of tokens' BIO labels. The algorithm we use here is the chunker provided in CogCompNLP, where the LEARN component is the sparse averaged perceptron and the INFERENCE is described in  (Punyakanok and Roth, 2001) . 

 Results We compare the F 1 performances of all three tasks in Fig.  6 , averaged from 50 experiments with different randomizations. As the budget increases, the system F 1 increases for both schemes I and II in all three tasks, which confirms the capability of the proposed SSPAN framework to learn from partial structures. When the budget is 100% (i.e., the entire U 0 is annotated), schemes I and II have negligible differences; when the budget is not large enough to cover the entire U 0 , scheme II is consistently better than I in all tasks, which follows our expectations based on the I k analysis. The strict improvement for all budget ratios indicates that the observation is definitely not by chance. Figure  7  further compares the improvement from I to II across tasks. When the budget goes down from 100%, the advantage of ESPA is more prominent; but when the budget is too low, the quality of P degrades and hurts the performance of SSPAN, leading to roughly hill-shaped curves in Fig.  7 . We have also conjectured based on Fig.  4  that the structure strength goes up from BIO chunks, to bipartite graphs, and to chains; interestingly, the improvement brought by ESPA is consistent with this order.   5 ) under three structured learning tasks (note the scale difference). Each F 1 value is the average of 50 experiments, and each curve is based on corresponding F 1 values smoothed by Savitzky-Golay filters. We can see that scheme II is consistently better than scheme I. Per the Wilcoxon rank-sum test, the significance levels at each given budget are shown on the x-axes, where + and ++ mean p < 5% and p < 1%, respectively. Admittedly, the improvement, albeit statistically significant, is small, but it does not diminish the contribution of this paper: Our goal is to remind people that the ESPA scheme (or more generally, partialness) is, at the least, comparable to (or sometimes even better than) complete annotation schemes. Also, the comparison here is in fact unfair to the partial scheme II, because we assume equal cost for both schemes, although it often costs less in a partial scheme as a large problem is decomposed into smaller parts. Therefore, the results shown here implies that the information theoretical benefit of partialness can possibly offset its disadvantages for learning. Figure  7 : The improvement of F 1 brought by ESPA for each task in Fig.  6 . Note that we conjectured earlier in Fig.  4  that the BIO structure is the weakest among the three, which is consistent with the fact that shallow parsing benefits the least from ESPA. 

 Dicussion and Conclusion In this paper, we investigate a less studied, yet important question for structured learning: Given a limited annotation budget (either in time or money), which strategy is better, completely an-notating each structure until the budget runs out, or annotating more structures at the cost of leaving some of them partially annotated?  Neubig and Mori (2010)  investigated this issue specifically in annotating word boundaries and pronunciations for Japanese. Instead of annotating full sentences, they proposed to annotate only some words in a sentence (i.e., partially) that can be chosen heuristically (e.g., skip those that we have seen or those low frequency words). Conceptually, Neubig and Mori (  2010 ) is an active learning work, with the understanding that if the order of annotation is deliberately designed, better learning can be achieved. The current paper addresses the problem from a different angle: Even without active learning, can we still answer the question above? The observation driving our questions is that when annotating a particular structure, the labels of the yet to be labeled variables may already be constrained by previous annotations and carry less information than those in a totally new structure. Therefore, we systematically study the ESPA scheme -stop annotating a given structure before it is completed and continue annotating another new structure. An important notion is annotation cost. Throughout the paper we have an ideal assumption that the cost is linear in the total number of annotations, but in practice the case can be more complicated. First, the actual cost of each individual annotation may vary across different instances. We try to eliminate this issue by enforcing random selection of annotation instances, rather than allowing the annotators to select arbitrarily by themselves. This strategy may be useful in practice as well, to avoid people only annotating easy cases. Second, even if we only require labeling partial structures, it is likely that the annotator still needs to comprehend the entire structure, incurring additional cost (usually in terms of time). This issue, however, is not addressed in this paper. Using this definition of cost, we provide a theoretical analysis for ESPA based on the mutual information between target structures and annotation processes. We show that for structures like chains, bipartite graphs, and BIO chunks, the information brought by an extra annotation attenuates as the annotation of the structure is more complete, suggesting to stop early and move to a new structure (although it still remains unclear when it is optimal to stop). This analysis is further supported by experiments on temporal relation extraction, semantic role classification, and shallow parsing, three tasks analogous to the three structures analyzed earlier, respectively. The ratio of the attenuation curve as in Fig.  4  is also shown to be an actionable metric to quantify the strength of a type of structure, which can be useful in various analysis, including judging whether ESPA is worthwhile for a particular task. For example, a more detailed I k -based analysis for SRC shows that predicates with more arguments are stronger structures than those with fewer arguments; we have investigated ESPA on those with more than 6 arguments and indeed, observed much larger improvement in SRC. More details on this analysis are put in the appendix. We think that the findings in this paper are very important. First, as far as we know, we are the first to propose the mutual information analysis that provides a unique view of structured annotation, that of the reduction in the uncertainty of a target of interest Y by another random variable/process. From this perspective, signals that have non-zero mutual information with Y can be viewed as "annotations". These can be partially labeled structures (studied here), partial labels (restricting the possible labels rather than determining a single one as in e.g.,  Hu et al. (2019) , noisy labels (e.g., generated by crowdsourcing or heuristic rules) or, generally, other indirect supervision signals that are correlated with Y. As we proposed, these can be studied within our mutual information framework as well. This paper thus provides a way to analyze the benefit of general incidental supervision signals  (Roth, 2017) ) and possibly even provides guidance in selecting good incidental supervision signals. Second, the findings here open up opportunities for new annotation schemes for structured learning. In the past, partially annotated training data have been either a compromise when completeness is infeasible (e.g., when ranking entries in gigantic databases), or collected freely without human annotators (e.g., based on heuristic rules). If we intentionally ask human annotators for partial annotations, the annotation tasks can be more flexible and potentially, cost even less. This is because annotating complex structures typically require certain expertise, and smaller tasks are often easier  (Fernandes and Brefeld, 2011) . It is very likely that some complex annotation tasks require people to read dozens of pages of annotation guidelines, but once decomposed into smaller subtasks, even laymen can handle them. Annotation schemes driven by crowdsourced questionanswering, known to provide only partial coverage are successful examples of this idea  (He et al., 2015; Michael et al., 2017) . Therefore, this paper is hopefully interesting to a broad audience. Figure 1 : 1 Figure 1: Due to the inherent structural constraints of each task, individual instances therein put restrictions on others. (a) The temporal relation between met and Thursday has to be BEFORE ("met (1)") or BE INCLUDED ("met (2)"). (b) The argument roles of a frog and to the girl cannot be ARG0 anymore. (c) Given the position of the cat's FOREHEAD and LEFT EYE, a rough estimate of its NECK can be the red solid box rather than the blue dashed box. 

 Figure 2 : 2 Figure2: If we need training data for a graph labeling task (assuming the gold values for the nodes are given) and our annotation budget allows us to annotate, for instance, 10 edges in total, we could (a) completely annotate one graph (and then we run out of budget), or (b) partially annotate two graphs. 

 Figure 3 : 3 Figure3: The mutual information between the chain structure and its k-step ESPA, I k , is concave, suggesting possible benefit of using ESPA. In the simulation, there are n = 10 items in the chain and thus d = 45 pairs, k of which are labeled. The values of I k 's, as defined by Eq. (3), were obtained through averaging 1000 experiments. We use base-2 logarithm and the unit on y-axis is thus "bit". 

 Figure 4 : 4 Figure 4: The I k I k 1 curves from several different structures. The curves are shifted to almost the same starting point for better visualization, so the Y-Axis grid is not shown. The curve for "Chain" was obtained via simulations, and the other curves all have closedform formulations. 

 Figure 6 : 6 Figure6: Comparison of the baseline, complete annotation scheme and the proposed ESPA scheme (See I & II in Fig.5) under three structured learning tasks (note the scale difference). Each F 1 value is the average of 50 experiments, and each curve is based on corresponding F 1 values smoothed by Savitzky-Golay filters. We can see that scheme II is consistently better than scheme I. Per the Wilcoxon rank-sum test, the significance levels at each given budget are shown on the x-axes, where + and ++ mean p < 5% and p < 1%, respectively. 

 exists in general. First, we formally define structure and annotation. Definition 1. A structure of size d is a vector of random variables (RV) Y = [Y 1 , . . . , Y d ] 2 C(L d ), where L = {`1, . . . , `|L| } is the label set for each variable and C(L d ) ? L d represents the constraints imposed by this type of structure. 

			 A subtask of semantic role labeling (SRL) (Palmer et al., 2010)  that only classifies the role of an argument.2 There has been many works on learning from partial annotations, which we review in Sec. 3. SSPAN is only an experimental choice in demonstrating ESPA. Whether SSPAN is better than other algorithms is out of the scope here, and a better algorithm for ESPA will only strengthen the claims in this paper. 

			 Line 9 can be interpreted in different ways, either as T [ P (adopted in this work) or as a weighted combination of LEARN(T ) and LEARN( P) (adopted by (Chang et al., 2007) ). 

			 The original TimeBank-Dense section contains 36 documents, but in collecting MATRES, one of the documents was filtered out because it contained no TempRels between mainaxis events.
