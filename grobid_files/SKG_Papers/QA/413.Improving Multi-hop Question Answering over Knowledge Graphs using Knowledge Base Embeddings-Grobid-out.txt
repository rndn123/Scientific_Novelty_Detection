title
Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings

abstract
Knowledge Graphs (KG) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over KG (KGQA) task is to answer natural language queries posed over the KG. Multi-hop KGQA requires reasoning over multiple edges of the KG to arrive at the right answer. KGs are often incomplete with many missing links, posing additional challenges for KGQA, especially for multi-hop KGQA. Recent research on multihop KGQA has attempted to handle KG sparsity using relevant external text, which isn't always readily available. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. Such KG embedding methods, even though highly relevant, have not been explored for multi-hop KGQA so far. We fill this gap in this paper and propose EmbedKGQA. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. EmbedKGQA also relaxes the requirement of answer selection from a prespecified neighborhood, a sub-optimal constraint enforced by previous multi-hop KGQA methods. Through extensive experiments on multiple benchmark datasets, we demonstrate EmbedKGQA's effectiveness over other stateof-the-art baselines.

Introduction Knowledge Graphs (KG) are multi-relational graphs consisting of millions of entities (e.g., San Jose, California, etc.) and relationships among them (e.g., San Jose-cityInState-California). Examples of a few large KGs include Wikidata  (Google, 2013) , DBPedia  (Lehmann et al., 2015) ,  Yago (Suchanek et al., 2007) , and NELL (Mitchell in the incomplete KG makes it much harder to answer the input NL question, as the KGQA model potentially needs to reason over a longer path over the KG (marked by bold edges). Existing multi-hop KGQA methods also impose heuristic neighborhood limits (shaded region in the figure), which often makes the true answer (Crime in this example) out of reach. EmbedKGQA, our proposed method, overcomes these limitations by utilizing embeddings of the input KG during multi-hop KGQA. For more details, please refer Figure  2  and Section 4.  et al., 2018) . Question Answering over Knowledge Graphs (KGQA) has emerged as an important research area over the last few years  Sun et al., 2019a) . In KGQA systems, given a natural language (NL) question and a KG, the right answer is derived based on analysis of the question in the context of the KG. In multi-hop KGQA, the system needs to perform reasoning over multiple edges of the KG to infer the right answer. KGs are often incomplete, which creates additional challenges for KGQA systems, especially in case of multi-hop KGQA. Recent methods have used an external text corpus to handle KG sparsity  (Sun et al., 2019a (Sun et al., , 2018 . For example, the method proposed in  (Sun et al., 2019a ) constructs a question-specific sub-graph from the KG, which is then augmented with supporting text documents. Graph  CNN (Kipf and Welling, 2016)  is then applied over this augmented sub-graph to arrive at the final answer. Unfortunately, availability and identification of relevant text corpora is a challenge on its own which limits broad-coverage applicability of such methods. Moreover, such methods also impose pre-specified heuristic neighborhood size limitation from which the true answer needs to be selected. This often makes the true answer out of reach of the model to select from. In order to illustrate these points, please consider the example shown in Figure  1 . In this example, Louis Mellis is the head entity in the input NL question, and Crime is the true answer we expect the model to select. If the edge has genre(Gangster No. 1, Crime) were present in the KG, then the question could have been answered rather easily. However, since this edge is missing from the KG, as is often the case with similar incomplete and sparse KGs, the KGQA model has to potentially reason over a longer path over the KG (marked by bolded edges in the graph). Moreover, the KGQA model imposed a neighborhood size of 3-hops, which made the true answer Crime out of reach. In a separate line of research, there has been a large body of work that utilizes KG embeddings to predict missing links in the KG, thereby reducing KG sparsity  (Bordes et al., 2013; Trouillon et al., 2016; Yang et al., 2014a; Nickel et al., 2011) . KG embedding methods learn high-dimensional embeddings for entities and relations in the KG, which are then used for link prediction. In spite of its high relevance, KG embedding methods have not been used for multi-hop KGQA -we fill this gap in this paper. In particular, we propose EmbedKGQA, a novel system which leverages KG embeddings to perform multi-hop KGQA. We make the following contributions in this paper: 1. We propose EmbedKGQA, a novel method for the multi-hop KGQA task. To the best of our knowledge, EmbedKGQA is the first method to use KG embeddings for this task. EmbedKGQA is particularly effective in performing multi-hop KGQA over sparse KGs. 2. EmbedKGQA relaxes the requirement of answer selection from a pre-specified local neighborhood, an undesirable constraint imposed by previous methods for this task. 3. Through extensive experiments on multiple real-world datasets, we demonstrate Embed-KGQA's effectiveness over state-of-the-art baselines. We have made EmbedKGQA's source code available to encourage reproducibility. 2 Related Work KGQA: In prior work  (Li et al., 2018)  TransE,  (Bordes et al., 2013)  embeddings have been used to answer factoid based questions. However, this requires ground truth relation labeling for each question and it does not work for multi-hop question answering. In another line of work  (Yih et al., 2015)  and  (Bao et al., 2016)  proposed extracting a particular sub-graph to answer the question. The method presented in  (Bordes et al., 2014a) , the sub-graph generated for a head entity is projected in a high dimensional space for question answering. Memory Networks have also been used to learn high dimensional embeddings of the facts present in the KG to perform QA  (Bordes et al., 2015) . Methods like  (Bordes et al., 2014b ) learn a similarity function between the question and the corresponding triple during training, and score the question with all the candidate triples at the test time.  (Yang et al., 2014b)  and  (Yang et al., 2015)  utilize embedding based methods to map natural language questions to logical forms. Methods like  (Dai et al., 2016; Dong et al., 2015; Hao et al., 2017; Lukovnikov et al., 2017; Yin et al., 2016)  utilize neural networks to learn a scoring functions to rank the candidate answers. Some works like  (Mohammed et al., 2017; Ture and Jojic, 2016)  consider each relation as a label and model QA task as a classification problem. Extending these kinds of approaches for multi-hop question answering is non-trivial. Recently, there has been some work in which text corpus is incorporated as a knowledge source in addition to KG to answer complex questions on KGs  (Sun et al., 2018 (Sun et al., , 2019a . Such approaches are useful in case the KG is incomplete. However, this leads to another level of complexity in the QA system, and text corpora might not always be available. KG completion methods: Link prediction in Knowledge Graphs using KG embeddings has become a popular area of research in recent years. The general framework is to define a score function for a set of triples (h, r, t) in a KG and constraining them in such a way that the score for a correct triple is higher than the score for an incorrect triple. RESCAL  (Nickel et al., 2011)  and DistMult  (Yang et al., 2015)  learn a score function containing a bi-linear product between head entity and tail entity vectors and a relation matrix. ComplEx  (Trouillon et al., 2016)  represents entity vectors and relation matrices in the complex space. SimplE  (Kazemi and Poole, 2018)  and TuckER  (Bala?evi? et al., 2019)  are based on Canonical Polyadic (CP) decomposition  (Hitchcock, 1927)  and Tucker  decomposition (Tucker, 1966)  respectively. TransE  (Bordes et al., 2013)  embeds entities in high dimensional real space and relation as translation between the head and the tail entities. RotatE  (Sun et al., 2019b)  on the other hand projects entities in complex space and relations are represented as rotations in the complex plane. ConvE  (Dettmers et al., 2018)  utilizes Convolutional Neural Networks to learn a scoring function between the head entity, tail entity and relation. InteractE  (Vashishth et al., 2019)  improves upon ConvE by increasing feature interaction. 

 Background In this section, we formally define a Knowledge Graph(KG) and then describe link prediction task on incomplete KGs. We then describe KG embeddings and explain the ComplEx embedding model. 

 Knowledge Graph Given a set of entities E and relations R, a Knowledge Graph G is a set of triples K such that K ? E ? R ? E . A triple is represented as (h, r, t), with h, t ? E denoting subject and object entities respectively and r ? R the relation between them. 

 Link Prediction In link prediction, given an incomplete Knowledge Graph, the task is to predict which unknown links are valid. KG Embedding models achieve this through a scoring function ? that assigns a score s = ?(h, r, t) ? R, which indicates whether a triple is true, with the goal of being able to score all missing triples correctly. 

 Knowledge Graph Embeddings For each e ? E and r ? R, Knowledge Graph Embedding (KGE) models generate e e ? R de and e r ? R dr , where e e and e r are d e and d r dimensional vectors respectively. Each of the embedding methods also has a scoring function ? : E ? R ? E ? R to assign some score ?(h, r, t) to a possible triple (h, r, t), h, t ? E and r ? R. Models are trained in a way such that for every correct triple (h, r, t) ? K and incorrect triple (h , r , t ) ? K the model assign scores such that ?(h, r, t) > 0 and ?(h , r , t ) < 0. A scoring function is generally a function of (e h , e r , e t ). 

 ComplEx Embeddings ComplEx  (Trouillon et al., 2016 ) is a tensor factorization approach that embeds relations and entities in complex space. Given h, t ? E and r ? R, ComplEx generates e h , e r , e t ? C d and defines a scoring function: ?(h, r, t) = Re( e h , e r , ?t ) = Re( d k=1 e (k) h e (k) r ?t (k) ) (1) such that ?(h, r, t) > 0 for all true triples, and ?(h, r, t) < 0 for false triples. Re denotes the real part of a complex number. 4 EmbedKGQA: Proposed Method In this section, we first define the problem of KGQA and then describe our model. 

 Problem Statement Let E and R be the set of all entities and relations respectively in a KG G, and K ? E ? R ? E is the set of all available KG facts. The problem in KGQA involves, given a natural language question q and a topic entity e h ? E present in the question, the task is to extract an entity e t ? E that correctly answers the question q. 

 EmbedKGQA Overview We work in a setting where there is no finegrained annotation present in the dataset, such as the question type or the exact logic reasoning steps. For example, co-actor is a combination of starred actor ?1 and starred actor relations, but our model does not require this annotation. EmbedKGQA uses Knowledge Graph embeddings to answer multi-hop natural language questions. First it learns a representation of the KG in an embedding space. Then given a question it learns a question embedding. Finally it combines these embedding to predict the answer. In the following sections, we introduce the Em-bedKGQA model.  

 Question Embedding Module This module embeds the natural language question q to a fixed dimension vector e q ? C d . This is done using a feed-forward neural network that first embeds the question q using RoBERTa  (Liu et al., 2019)  into a 768-dimensional vector. This is then passed through 4 fully connected linear layers with ReLU activation and finally projected onto the complex space C d . Given a question q, topic entity h ? E and set of answer entities A ? E, it learns the question embedding in a way such that ?(e h , e q , e a ) > 0 ?a ? A ?(e h , e q , e ?) < 0 ? / ? A where ? is the ComplEx scoring function (  1 ) and e a , e ? are entity embeddings learnt in the previous step. For each question, the score ?(.) is calculated with all the candidate answer entities a ? E. The model is learned by minimizing the binary crossentropy loss between the sigmoid of the scores and the target labels, where the target label is 1 for the correct answers and 0 otherwise. Label smoothing is done when the total number of entities is large. 

 Answer Selection Module At inference, the model scores the (head, question) pair against all possible answers a ? E. For relatively smaller KGs like MetaQA, we simply select the entity with the highest score. ?(e h , e q , e a ) However if the knowledge graph is large, pruning the candidate entities can significantly improve the performance of EmbedKGQA. The pruning strategy is described in the following section.  

 Relation matching Similar to PullNet  (Sun et al., 2019a)  we learn a scoring function S(r, q) which ranks each relation r ? R for a given question q. Let h r be the embedding of a relation r and q = (< s >, w 1 , .., w |q| , < /s >) be the sequence of words in question q which are input to RoBERTa. The scoring function is defined as the sigmoid of the dot product of the final output of the last hidden layer of RoBERTa (h q ) and the embedding of relation r (h r ). h q = RoBERTa(q ) S(r, q) = sigmoid(h T q h r ) Among all the relations, we select those relations which have score greater than 0.5 It is denoted as the set R a . For each candidate entity a that we have obtained so far (Section 4.4), we find the relations in the shortest path between head entity h and a . Let this set of relations be R a . Now the relation score for each candidate answer entity is defined as the size of their intersection. RelScore a = |R a ? R a | We use a linear combination of the relation score and ComplEx score to find the answer entity. e ans = arg max a ?N h ?(e h , e q , e a ) + ? * RelScore a where ? is a tunable hyperparameter. 

 Experimental Details In this section, we first describe the datasets that we evaluated our method on, and then explain the experimental setup and the results. 

 Datasets 1. MetaQA  dataset is a large scale multi-hop KGQA dataset with more than 400k questions in the movie domain. It has 1-hop, 2-hop, and 3-hop questions. In our experiments, we used the "vanilla" version of the questions. Along with the QA data, MetaQA also provides a KG with 135k triples, 43k entities, and nine relations. 2. WebQuestionsSP (tau Yih et al., 2016) is a smaller QA dataset with 4,737 questions. The questions in this dataset are 1-hop and 2-hop questions and are answerable through Freebase KG. For ease of experimentation, we restrict the KB to be a subset of Freebase which contains all facts that are within 2-hops of any entity mentioned in the questions of We-bQuestionsSP. We further prune it to contain only those relations that are mentioned in the dataset. This smaller KB has 1.8 million entities and 5.7 million triples. 

 Baselines We compare our model with the Key-Value Memory Network  (Miller et al., 2016) , the GraftNet  (Sun et al., 2018)  and the Pullnet  (Sun et al., 2019a)  for WebQuestionsSP dataset. For MetaQA dataset we also compare with the VRN . These methods implement multi-hop KGQA, and except VRN, use additional text corpus to mitigate the KG sparsity problem. ? VRN  uses variational learning algorithm to perform Multi-Hop QA over KG. ? Key-Value Memory Network (KVMem)  (Miller et al., 2016)  is one of the first models that attempts to do QA over incomplete KBs by augmenting it with text. It maintains a memory table which stores KB facts and text encoded into key-value pairs and uses this for retrieval. ? GraftNet  (Sun et al., 2018)  uses heuristics to create a question-specific subgraph containing KG facts, entities and sentences from the text corpora and then uses a variant of graph  CNN (Kipf and Welling, 2016)  to perform reasoning over it. ? PullNet  (Sun et al., 2019a ) also creates a question-specific sub-graph but instead of using heuristics, it learns to "pull" facts and sentences from the data to create a more relevant sub-graph. It also uses a graph CNN approach to perform reasoning. The complete KG setting is the easiest setting for QA because the datasets are created in such a way that the answer always exists in the KG, and there is no missing link in the path. However, it is not a realistic setting, and the QA model should also be able to work on an incomplete KG. So we simulate an incomplete KB by randomly removing half of the triples in the KB (we randomly drop a fact with probability = 0.5). We call this setting KG-50 and we call full KG setting KG-Full in the text. In the next section we will answer the following questions: Q1. Can Knowledge Graph embeddings be used to perform multi-hop KGQA? (Section 5.3) Q2. Can EmbedKGQA be used to answer questions when there is no direct path between the head entity and the answer entity? (Section 5.4) Q3. How much does the answer selection module help in the final performance of our model? (Section 5.5) 

 KGQA results In this section, we have compared our model with baseline models on MetaQA and WebQuestionsSP datasets. 

 Analysis on MetaQA MetaQA has different partitions of the dataset for 1-hop, 2-hop, and 3-hop questions. In the full KG setting (MetaQA KG-Full) our model is comparable to the state-of-the-art for 2-hop questions and establishes the state-of-the-art for 3-hop questions. EmbedKGQA performs similar to the state-of-the in case of 1-hop question which is expected because the answer node is directly connected to the head node and it is able to learn the corresponding relation embedding from the question. On the other hand performance on 2-hop and 3-hop questions suggest that EmbedKGQA is able to infer the correct relation from the neighboring edges because the KG embeddings can model composition of relations. Pullnet and GraftNet also perform similarly well because the answer entity lies in the question sub-graph most of the times. We have also tested our method on the incomplete KG setting, as explained in the previous section. Here we find that the accuracy of all baselines decreases significantly compared to the full KG setting, while EmbedKGQA achieves state-of-the-art performance. This is because MetaQA KG is fairly sparse, with only 135k triples for 43k entities. So when 50% of the triples are removed (as is done in MetaQA KG-50), the graph becomes very sparse with an average of only 1.66 links per entity node. This causes many head entity nodes of questions to have much longer paths (>3) to their answer node. Hence models that require question-specific sub-graph construction (GraftNet, PullNet) are unable to recall the answer entity in their generated sub-graph and therefore performs poorly. However, their performance improves only after including additional text corpora. On the other hand, Em-bedKGQA does not limit itself to a sub-graph and utilizing the link prediction properties the KG embeddings, EmbedKGQA is able to infer the relation on missing links. 

 Analysis on WebQuestionsSP WebQuestionsSP has a relatively small number of training examples but uses a large KG (Freebase) as background knowledge. This makes multi-hop KGQA much harder. Since all the entities of the KG are not covered in the training set, freezing the Table  3 : Performance on WebQuestionsSP dataset. All baseline results were taken from  Sun et al. (2019a) . The values reported are hits@1. Numbers in brackets correspond to a setting where text was used to augment the incomplete KG (WebQSP KG-50). For more details please refer Section 5.3.2. entity embeddings after learning them during KG embedding learning phase (Section 4.2) is necessary. Results on WebQuestionsSP (Table  3 ) highlight the fact that, even with a small number of training examples EmbedKGQA can learn good question embeddings that can infer the multi-hop path required to answer the questions. Our method on WebQSP KG-50 outperforms all baselines including PullNet, which uses extra textual information and is the state-of-the-art model. Even though WebQuestionsSP has fewer questions, EmbedKGQA able to learn good question embeddings that can infer mission links in KG. This can be attributed to the fact that relevant and necessary information is being captured through KG embeddings, implicitly. 

 QA on KG with missing links State-of-the-art KGQA models like PullNet and GraftNet require a path between the head entity and the answer entity to be present in the Knowledge Graph to answer the question. For example, in PullNet, the answer is restricted to be one of the entities present in the extracted question subgraph. For the incomplete KG case where only 50% of the original triples are present, PullNet  (Sun et al., 2019a)  reports a recall of 0.544 on the MetaQA 1hop dataset. This means that only for 54.4 percent of questions, all the answer entities are present in the extracted question subgraph, and this puts a hard limit on how many questions the model can answer in this setting. EmbedKGQA, on the other hand, uses Knowledge Graph Embeddings rather than a localized sub-graph to answer the question. It uses the head embedding and question embedding, which implicitly captures the knowledge of all observed and unobserved links around the head node. This is possible because of the link prediction property of Knowledge Graph Embeddings. So unlike other QA systems, even if there is no path between the head and answer entity, our model should be able to answer the question if there is sufficient information in the KG to be able to predict that path (See Fig.  1 ). We design an experiment to test this capability of our model. For all questions in the validation set of the MetaQA 1-hop dataset, we removed all the triples from the Knowledge Graph that can be directly used to answer the question. For example, given the question 'what language is [PK] in' in the validation set, we removed the triple (P K, in language, Hindi) from the KG. The dataset also contains paraphrases of the same question, for, e.g., 'what language is the movie [PK] in' and 'what is the language spoken in the movie [PK]'. We also removed all paraphrases of validation set questions from the training dataset since we only want to evaluate the KG completion property of our model and not a linguistic generalization. In such a setting, we expect models that rely only on sub-graph retrieval to achieve 0 hits@1. However, our model delivers a significantly better 29.9 hits@1 in this setting. This shows that our model can capture the KG completion property of ComplEx embeddings and apply it to answer questions which was otherwise impossible. Further, if we know the relation corresponding to each question, then the problem of 1-hop KG QA is the same as KG completion in an incomplete Knowledge Graph. Using the same training KG as above and using the removed triples as the test set, we do tail prediction using KG embeddings. Here we obtain 20.1 hits@1. The lesser score can be attributed to the fact that ComplEx embedding uses only the KG while our model uses the QA data as well -which in itself represents knowledge. Our model is first trained on the KG and then uses these embeddings to train the QA model, and thus it can leverage the knowledge present in both the KG and QA data. 

 Effect of Answer Selection Module We analyse the effect of the answer selection module (Section 4.4) on EmbedKGQA in the WebQues-tionsSP dataset by ablating the relation matching module. Furthermore, in order to compare with other methods that restrict the answer to a neighbourhood in the KG  (Sun et al. (2019a) ,  Sun et al. (2018) ), we experimented with restricting the candidate set of answer entities to only the 2-hop neighbourhood of the head entity. The results can be seen in Table  5 . As we can see, relation matching has a significant impact on the performance of Em-bedKGQA on both WebQSP KG-full and WebQSP KG-50 settings. Also, as mentioned earlier, WebQSP KG (Freebase subset) has an order of magnitude more entities than MetaQA (1.8M versus 134k in MetaQA) and the number of possible answers is large. So reducing the set of answers to a 2-hop neighbourhood of the head entity showed improved performance in the case of WebQSP KG-Full. However, this caused a degradation in performance on WebQSP KG-50. This is because restricting the answer to a 2-hop neighbourhood on an incomplete KG may cause the answer to not be present in the candidates (Please refer figure  1 ). In summary, we find that relation matching is an important part of EmbedKGQA. Morever, we suggest that n-hop filtering during answer selection may be included on top of EmbedKGQA for KGs which are reasonably complete. 

 Conclusion In this paper, we propose EmbedKGQA, a novel method for Multi-hop KGQA. KGs are often incomplete and sparse which poses additional challenges for multi-hop KGQA methods. Recent recent for this problem have tried to address the incompleteness problem by utilizing an additional text corpus. However, the availability of a relevant text corpus is often limited, thereby reducing broad-coverage applicability of such methods. In a separate line of research, KG embedding methods have been proposed to reduce KG sparsity by performing missing link prediction. EmbedKGQA utilizes the link prediction properties of KG embeddings to mitigate the KG incompleteness problem without using any additional data. It trains the KG entity embeddings and uses it to learn question embeddings, and during the evaluation, it scores (head entity, question) pair again all entities, and the highest-scoring entity is selected as an answer. EmbedKGQA also overcomes the shortcomings due to limited neighborhood size constraint imposed by existing multi-hop KGQA methods. Em-bedKGQA achieves state-of-the-art performance in multiple KGQA settings, suggesting that the link prediction properties of KG embeddings can be utilized to mitigate the KG incompleteness problem in Multi-hop KGQA. Figure 1 : 1 Figure 1: Challenges with Multi-hop QA over Knowledge Graphs (KGQA) in sparse and incomplete KGs: Absence of the edge has genre(Gangster No. 1, Crime) in the incomplete KG makes it much harder to answer the input NL question, as the KGQA model potentially needs to reason over a longer path over the KG (marked by bold edges). Existing multi-hop KGQA methods also impose heuristic neighborhood limits (shaded region in the figure), which often makes the true answer (Crime in this example) out of reach. EmbedKGQA, our proposed method, overcomes these limitations by utilizing embeddings of the input KG during multi-hop KGQA. For more details, please refer Figure 2 and Section 4. 
