title
Scalable Multi-Hop Relational Reasoning for Knowledge-Aware Question Answering

abstract
Existing work that augment question answering (QA) models with external knowledge (e.g., knowledge graphs) either struggle to model multi-hop relations efficiently, or lack transparency into the model's prediction rationale. In this paper, we propose a novel knowledge-aware approach that equips pretrained language models (PTLMs) with a multi-hop relational reasoning module, named multi-hop graph relation network (MHGRN). It performs multi-hop, multi-relational reasoning over subgraphs extracted from external knowledge graphs. The proposed reasoning module unifies path-based reasoning methods and graph neural networks and results in better interpretability and scalability. We also empirically show its effectiveness and scalability on CommonsenseQA and OpenbookQA datasets, and interpret its behaviors with case studies, with the code for experiments released 1 .

Introduction Many recently proposed question answering tasks require not only machine comprehension of the question and context, but also relational reasoning over entities (concepts) and their relationships by referencing external knowledge  (Talmor et al., 2019; Sap et al., 2019; . For example, the question in Fig.  1  requires a model to perform relational reasoning over mentioned entities, i.e., to infer latent relations among the concepts: {CHILD, SIT, DESK, SCHOOLROOM}. Background knowledge such as "a child is likely to appear in a schoolroom" may not be readily contained in the questions themselves, but are commonsensical to humans. Despite the success of large-scale pre-trained language models (PTLMs)  (Devlin et al., 2019;  ? The first two authors contributed equally. The major work was done when both authors interned at USC.   Liu et al., 2019b) , these models fall short of providing interpretable predictions, as the knowledge in their pre-training corpus is not explicitly stated, but rather is implicitly learned. It is thus difficult to recover the evidence used in the reasoning process. This has led many to leverage knowledge graphs (KGs)  (Mihaylov and Frank, 2018; Lin et al., 2019; Yang et al., 2019) . KGs represent relational knowledge between entities with multi-relational edges for models to acquire. Incorporating KGs brings the potential of interpretable and trustworthy predictions, as the knowledge is now explicitly stated. For example, in Fig.  1 , the relational path (CHILD AtLocation CLASSROOM Synonym SCHOOLROOM) naturally provides evidence for the answer SCHOOLROOM. A straightforward approach to leveraging a knowledge graph is to directly model these relational paths.  KagNet (Lin et al., 2019)  and MH-PGM  (Bauer et al., 2018)  extract multi-hop relational paths from KG and encode them with sequence models. Application of attention mechanisms upon these relational paths can further offer good interpretability. However, these models are hardly scalable because the number of possible paths in a graph is (1) polynomial w.r.t. the number of nodes (2) exponential w.r.t. the path length (see Fig.  2 ). Therefore, some  (Weissenborn et al., 2017; Mihaylov and Frank, 2018)  resort to only using one-hop paths, namely, triples, to balance scalability and reasoning capacities. Graph neural networks (GNNs), in contrast, enjoy better scalability via their message passing formulation, but usually lack transparency. The most commonly used GNN variant, Graph Convolutional Networks (GCNs)  (Kipf and Welling, 2017) , perform message passing by aggregating neighborhood information for each node, but ignore the relation types.  RGCNs (Schlichtkrull et al., 2018)  generalize GCNs by performing relationspecific aggregation, making it applicable to multirelational graphs. However, these models do not distinguish the importance of different neighbors or relation types and thus cannot provide explicit relational paths for model behavior interpretation. In this paper, we propose a novel graph encoding architecture, Multi-hop Graph Relation Network (MHGRN), which combines the strengths of path-based models and GNNs. Our model inherits scalability from GNNs by preserving the message passing formulation. It also enjoys interpretability of path-based models by incorporating structured relational attention mechanism. Towards multi-hop relational reasoning, our key motivation is to allow each node to directly attend to its multi-hop neighbors by performing multi-hop message passing within a single layer. We outline the desired merits of knowledge-aware QA models in Table  1  and compare MHGRN with them. We summarize the main contributions of this work as follows: 1) We propose MHGRN, a novel model architecture tailored to multi-hop relational reasoning, which explicitly models multi-hop relational paths at scale. 2) We propose a structured relational attention mechanism for efficient and interpretable modeling of multi-hop reasoning paths, along with its training and inference algorithms. 3) We conduct extensive experiments on two question answering datasets and show that our models bring significant improvements compared to knowledgeagnostic PTLMs, and outperform other graph encoding methods by a large margin. 

 Problem Formulation and Overview In this paper, we limit the scope to the task of multiple-choice question answering, although the formulation can be easily generalized to other knowledge-guided tasks (e.g., natural language inference). The overall paradigm of knowledgeaware QA is illustrated in Fig.  3 . Formally, given a question q and an external knowledge graph (KG) as the knowledge source, our goal is to identify the correct answer from a set C of given options. We turn this problem into measuring the plausibility score between q and each option a " C, after which we select the option with the highest plausibility score. To measure the score for q and a, we first concatenate q and a to form a statement s = [q; a] and encode the statement s into the statement representation s. Then we extract from the external KG a subgraph G (i.e., schema graph in  KagNet (Lin et al., 2019) ), with the guidance of s (detailed in ?5.1). This contextualized subgraph is defined as a multi-relational graph G = (V, E, ). Here V is a subset of entities in the external KG, containing only those relevant to s. E N V ? R ? V is the set of edges that connect nodes in V, where R = {1, ?, m} are the ids of all pre-defined relation types. The mapping function (i) ? V T = {E q , E a , E o } takes node i " V as input, and outputs E q if i is an entity mentioned in q, E a if it is mentioned in a, and E o otherwise 2 . Finally, we encode G into g, and concatenate s and g to calculate the plausibility score. 3 Background: Multi-Relational Graph Encoding Methods We leave encoding of s to pre-trained language models and focus on the challenge of encoding graph G to capture latent relations between entities. Current methods for encoding multi-relational graphs mainly fall into two categories: GNNs and path-based models. GNNs encode structured information by passing messages between nodes, directly operating on the graph structure, while pathbased methods first decompose the graph into paths and then pool their representations to form a graph representation. Graph Encoding with GNNs. For a graph with n nodes, a graph neural network (GNN) takes a set of node features {h 1 , h 2 , . . . , h n } as input, and computes their corresponding node embeddings {h ?1, h ?2, . . . , h ?n} via message passing  (Gilmer et al., 2017) . A compact graph representation for G can thus be obtained by pooling the node embeddings {h ?i}: GNN(G) = Pool({h ?1, h ?2, . . . , h ?n}). (1) As a notable variant of GNNs, graph convolutional networks (GCNs) (Kipf and Welling, 2017) additionally update node embeddings by aggregating messages from its direct neighbors. RGCNs  (Schlichtkrull et al., 2018)  extend GCNs to encode multi-relational graphs by defining relationspecific weight matrix W r for each edge type: h ?i = ? ? = r"R ?N r i ? 1 = r"R = j"N r i W r h j ? ? , (2) where N r i denotes neighbors of node i under relation r. 3 While GNNs have proved to have good scalability, their reasoning is done at the node level, and are therefore incompatible with path modeling. This property also hinders path-level interpretation of the model's decisions. Graph Encoding with Path-Based Models. In addition to directly modeling the graph with GNNs, a graph can also be viewed as a set of relational paths connecting pairs of entities. Relation Networks (RNs)  (Santoro et al., 2017 ) can be adapted to multi-relational graph encoding under QA settings. RNs use MLPs to encode all triples (one-hop paths) in G whose head entity is in Q = {j ? (j) = E q } and tail entity is in A = {i ? (i) = E a }. It then pools the triple embeddings to generate a vector for G as follows, RN(G) = Pool?{MLP(h j h e r h h i ) ? j " Q, i " A, (j, r, i) " E} . (3) Here h j and h i are features for nodes j and i, e r is the embedding of relation r " R, h denotes vector concatenation. To further equip RN with the ability to model nondegenerate paths,  KagNet (Lin et al., 2019)  adopts LSTMs to encode all paths connecting question entities and answer entities with lengths no more than K. It then aggregates all path embeddings via the attention mechanism:  KAGNET(G) = Pool?{LSTM(j, r 1 , j 1 , . . . , r k , i) ? (j, r 1 , j 1 ), ?, (j k 1 , r k , i) " E, 1 & k & K} . (4) 

 Proposed Method: Multi-Hop Graph Relation Network (MHGRN) This section presents Multi-hop Graph Relation Network (MHGRN), a novel GNN architecture that unifies both GNNs and path-based models. MHGRN inherits path-level reasoning and interpretabilty from path-based models, while preserving good scalability of GNNs. 

 MHGRN: Model Architecture We follow the GNN framework introduced in ?3, where node features can be initialized with pretrained weights (details in Appendix B). Here we focus on the computation of node embeddings. Type-Specific Transformation. To make our model aware of the node type , we first perform node type specific linear transformation on the input node features: x i = U (i) h i + b (i) , (5) where the learnable parameters U and b are specific to the type of node i. Multi-Hop Message Passing. As mentioned before, our motivation is to endow GNNs with the capability of directly modeling paths. To this end, we propose to pass messages directly over all the relational paths of lengths up to K. The set of valid k-hop relational paths is defined as: k = {(j, r 1 , . . . , r k , i) ? (j, r 1 , j 1 ), ?, (j k 1 , r k , i) " E} (1 & k & K). (6) We perform k-hop (1 & k & K) message passing over these paths, which is a generalization of the single-hop message passing in RGCNs (see Eq. 2): z k i = = (j,r 1 ,...,r k ,i)" k ?(j, r 1 , . . . , r k , i)/d k i W K 0 ?W k+1 0 W k r k ?W 1 r 1 x j (1 & k & K), (7) where the W t r (1 & t & K, 0 & r & m) matrices are learnable 4 , ?(j, r 1 , . . . , r k , i) is an attention score elaborated in ?4.2 and d k i = < (j?i)" k ?(j?i) is the normalization factor. The {W k r k ?W 1 r 1 ? 1 & r 1 , . . . , r k & m} matrices can be interpreted as the low rank approximation of a {m?m} k ?d?d tensor that assigns a separate transformation for each k-hop relation, where d is the dimension of x i . Incoming messages from paths of different lengths are aggregated via attention mechanism  (Vaswani et al., 2017) : z i = K = k=1 softmax bilinear s, z k i ? z k i . (8) Non-linear Activation. Finally, we apply shortcut connection and nonlinear activation to obtain the output node embeddings. h ?i = ?V h i + V ?zi , (9) where V and V ?are learnable model parameters, and is a non-linear activation function. 

 Structured Relational Attention Naive parameterization of the attention score ?(j, r 1 , . . . , r k , i) in Eq. 7 would require O(m k ) parameters for k-hop paths. Towards efficiency, we first regard it as the probability of a relation sequence ( (j), r 1 , . . . , r k , (i)) conditioned on s: which can naturally be modeled by a probabilistic graphical model, such as conditional random field  (Lafferty et al., 2001) : ?(j, r 1 , . . . , r k , i) = p ( (j), r 1 , . . . , r k , (i) ? s) , (10) p (? ? s) ? exp ?f ( (j), s) + k = t=1 (r t , s) + k 1 = t=1 ? (r t , r t+1 ) + g( (i), s)? = (r 1 , . . . , r k , s) ?" " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " -" " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " oe Relation Type Attention ( (j), (i), s) ?" " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " -" " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " " oe Node Type Attention , (11) where f ( ), ( ) and g( ) are parameterized by two-layer MLPs and ? ( ) by a transition matrix of shape m ? m. Intuitively, ( ) models the importance of a k-hop relation while ( ) models the importance of messages from node type (j) to (i) (e.g., the model can learn to pass messages only from question entities to answer entities). Our model scores a k-hop relation by decomposing it into both context-aware single-hop relations (modeled by ) and two-hop relations (modeled by ? ). We argue that ? is indispensable, without which the model may assign high importance to illogical multi-hop relations (e.g.,  [AtLocation, CapableOf] ) or noisy relations (e.g.,  [RelatedTo, RelatedTo] ). 

 Computation Complexity Analysis Although the message passing process in Eq. 7 and the attention module in Eq.11 handle potentially exponential number of paths, computation can be done in linear time with the help of dynamic programming (see Appendix C). As summarized in Table  2 , both the time complexity and space complexity of MHGRN on a sparse graph are linear w.r.t. the maximum path length K or the number of nodes n. 

 Expressive Power of MHGRN In addition to efficiency and scalability, we now discuss the modeling capacity of MHGRN. With the message passing formulation and relation-specific transformations, MHGRN is by nature the generalization of RGCN. It is also capable of directly modeling paths, making it interpretable as are pathbased models such as RN and KagNet. To show this, we first generalize RN (Eq. 3) to the multi-hop setting and introduce K-hop RN (formal definition in Appendix D), which models multi-hop relation as the composition of single-hop relations. We 

 Model Time Space G is a dense graph K-hop KagNet O ?m K n K+1 K O ?m K n K+1 K K-layer RGCN O ?mn 2 K O (mnK) MHGRN O ?m 2 n 2 K O (mnK) G is a sparse graph with maximum node degree 8 n can show that MHGRN is capable of representing K-hop RN (proof in Appendix E). K-hop KagNet O ?m K nK K O ?m K nK K K-layer RGCN O (mnK ) O (mnK) MHGRN O ?m 2 nK O (mnK) 

 Learning, Inference and Path Decoding We now discuss the learning and inference process of MHGRN instantiated for QA tasks. Following the problem formulation in ?2, we aim to determine the plausibility of an answer option a " C given the question q with the information from both text s and graph G. We first obtain the graph representation g by performing attentive pooling over the output node embeddings of answer entities {h ?i ? i " A}. Next we concatenate it with the text representation s and compute the plausibility score by ?(q, a) = MLP(s h g). During training, we maximize the plausibility score of the correct answer ? by minimizing the cross-entropy loss: L = E q,?,C log exp(?(q, a)) < a"C exp(?(q, a)) ? . ( 12 ) The whole model is trained end-to-end jointly with the text encoder (e.g., RoBERTa). During inference, we predict the most plausible answer by argmax a"C ?(q, a). Additionally, we can decode a reasoning path as evidence for model predictions, endowing our model with the interpretability enjoyed by path-based models. Specifically, we first determine the answer entity i ? with the highest score in the pooling layer and the path length k ? with the highest score in Eq. 8. Then the reasoning path is decoded by argmax ?(j, r 1 , . . . , r k ? , i ? ), which can be computed in linear time using dynamic programming.   

 Experimental Setup We introduce how we construct G ( ?5.1), the datasets ( ?5.2), as well as the baseline methods ( ?5.3). Appendix B shows more implementation and experimental details for reproducibility. G from KG, we recognize entity mentions in s and link them to entities in ConceptNet, with which we initialize our node set V. We then add to V all the entities that appear in any two-hop paths between pairs of mentioned entities. Unlike KagNet, we do not perform any pruning but instead reserve all the edges between nodes in V, forming our G. 

 Extracting 

 Datasets We evaluate models on two multiple-choice question answering datasets, CommonsenseQA and OpenbookQA. Both require world knowledge beyond textual understanding to perform well. CommonsenseQA  (Talmor et al., 2019)  necessitates various commonsense reasoning skills. The questions are created with entities from Concept-Net. It is noteworthy that although Common-senseQA is built upon ConceptNet, its questions are designed to probe multi-hop/compositional relations between entities that cannot be directly read from, or are even absent from ConceptNet. This de-5 Models based on ConceptNet are no longer shown on the leaderboard, and we got our results from the organizers.   ) . We also make use of the official split of CommonsenseQA, denoted as CommonsenseQA (OF). The numbers of instances in different dataset splits are listed in Table  5 . 

 Methods 

 Compared Methods We implement both knowledge-agnostic finetuning of pre-trained LMs and models that incorporate external KG as our baselines. Additionally, we directly compare our model with the results from corresponding leaderboard. These methods typically leverage textual knowledge or extra training data, as opposed to external KG. In all our implemented models, we use pre-trained LMs as text encoders for s for fair comparison. We do compare our models with those  (Ma et al., 2019; Lv et al., 2019; Khashabi et al., 2020)  augmented by other text-form external knowledge (e.g., Wikipedia), although we stick to our focus of encoding structured KG. Specifically, we fine-tune BERT-BASE, BERT-LARGE  (Devlin et al., 2019) , and ROBERTA  (Liu et al., 2019b)  for multiple-choice questions. We take RGCN (Eq. 2 in ?3), RN 8 (Eq. 3 in ?3), KagNet (Eq. 4 in ?3) and GconAttn  as baselines. GconAttn generalizes match-LSTM  (Wang and Jiang, 2016)  and achieves success in language inference tasks. 

 Results and Discussions In this section, we present the results of our models in comparison with baselines as well as methods on the leaderboards for both CommonsenseQA and OpenbookQA. We also provide analysis of models' components and characteristics. 

 Main Results For CommonsenseQA (Table  3 ), we first use the in-house data split (IH) (see ?5.2) to compare our models with implemented baselines. This is different from the official split used in the leaderboard methods. Almost all KG-augmented models achieve performance gain over vanilla pre-trained LMs, demonstrating the value of external knowledge on this dataset. Additionally, we evaluate For OpenbookQA (Table  6 ), we use official split and build models with ROBERTA-LARGE as text encoder. MHGRN surpasses all implemented baselines, with an absolute increase of ?2% on Test. Also, as our approach is naturally compatible with the methods that utilize textual knowledge or extra data, because in our paradigm the encoding of textual statement and graph are structurallydecoupled (Fig.  3 ). To empirically show MHGRN can bring gain over textual-knowledge empowered systems, we replace our text encoder with AristoRoBERTaV7 10 , and fine-tune our MHGRN upon OpenbookQA. Empirically, MHGRN continues to bring benefits to strong-performing textualknowledge empowered systems. One takeaway is that textual knowledge and structured knowledge are potentially complementary. 

 Performance Analysis Ablation Study on Model Components. As shown in Table  7 , disabling type-specific transformation results in ? 1.3% drop in performance, demonstrating the need for distinguishing node types for QA tasks. Our structured relational attention mechanism is also critical, with its two   Impact of Number of Hops (K). We investigate the impact of hyperparameter K for MHGRN with experiments on CommonsenseQA (Fig.  6 ). The increase of K continues to bring benefits until K = 4. However, performance begins to drop when K > 3. This might be attributed to exponential noise in longer relational paths in the knowledge graph.  is m times that of RGCN, the ratio of their empirical cost only approaches 2, demonstrating that our model can be better parallelized. It is noteworthy that the text encoder part actually dominates the overall cost. Therefore, the gap between our model and the RGCN are further narrowed if we consider the cost of the entire model. 

 Model Scalability 

 Model Interpretability We can analyze our model's reasoning process by decoding the reasoning path using the method described in ?4.5. Fig.  8  shows two examples from CommonsenseQA, where our model correctly answers the questions and provides reasonable path evidences. In the example on the left, the model links question entities and answer entity in a chain to support reasoning, while the example on the right provides a case where our model leverage unmentioned entities to bridge the reasoning gap between question entity and answer entities, in a way that is coherent with the latent relation between CHAPEL and the desired answer in the question. Where is known for a multitude of wedding chapels? A   (Yang and Mitchell, 2017; , triples  (Weissenborn et al., 2017; Mihaylov and Frank, 2018) , paths  (Bauer et al., 2018; Kundu et al., 2019; Lin et al., 2019) , or subgraphs  (Li and Clark, 2015) , and encode them to augment textual understanding. Recent success of pre-trained LMs motivates many  (Pan et al., 2019; Ye et al., 2019; Zhang et al., 2018; Banerjee et al., 2019)  to probe LMs' potential as latent knowledge bases. This line of work turn to textual knowledge (e.g. Wikipedia) to directly impart knowledge to pre-trained LMs. They generally fall into two paradigms: 1) Finetuning LMs on large-scale general-domain datasets (e.g. RACE  (Lai et al., 2017) ) or on knowledge-rich text. 2) Providing LMs with evidence via information retrieval techniques. However, these models cannot provide explicit reasoning and evidence, thus hardly trustworthy. They are also subject to the availability of in-domain datasets and maximum input token of pre-trained LMs. Neural Graph Encoding Graph Attention Networks (GAT)  (Velickovic et al., 2018)  incorporates attention mechanism in feature aggregation,  RGCN (Schlichtkrull et al., 2018)  proposes relational message passing which makes it applicable to multi-relational graphs. However they only perform single-hop message passing and cannot be interpreted at path level. Other work  (Abu-El-Haija et al., 2019; Nikolentzos et al., 2019)  aggregate for a node its K-hop neighbors based on node-wise distances, but they are designed for non-relational graphs. MHGRN addresses these issues by reasoning on multi-relational graphs and being interpretable via maintaining paths as reasoning chains. 

 Conclusion We present a principled, scalable method, MHGRN, that can leverage general knowledge via multi-hop reasoning over interpretable structures (e.g. Con-ceptNet). The proposed MHGRN generalizes and combines the advantages of GNNs and path-based reasoning models. It explicitly performs multi-hop relational reasoning and is empirically shown to outperform existing methods with superior scalablility and interpretability.  We merge relations that are close in semantics as well as in the general usage of triple instances in ConceptNet. Our models are implemented in PyTorch. We use cross-entropy loss and RAdam  (Liu et al., 2019a)  optimizer. We find it beneficial to use separate learning rates for the text encoder and the graph encoder. We tune learning rates for text encoders and    9 . We report the performance of these fine-tuned text encoders and also adopt their dataset-specific optimal learning rates in joint training with graph encoders. For models that involve KG, the learning rate of their graph encoders are chosen from {1 ? 10 4 , 3 ? 10 4 , 1 ? 10 3 , 3 ? 10 3 }, based on their best development set performance with ROBERTA-LARGE as the text encoder. We report the optimal learning rates for graph encoders in Table  10 . In training, we set the maximum input sequence length to text encoders to 64, batch size to 32, and perform early stopping. AristoRoBERTaV7+MHGRN is the only exception. 
