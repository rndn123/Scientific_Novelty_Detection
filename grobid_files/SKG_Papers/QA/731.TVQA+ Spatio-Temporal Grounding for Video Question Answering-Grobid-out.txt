title
TVQA+: Spatio-Temporal Grounding for Video Question Answering

abstract
We present the task of Spatio-Temporal Video Question Answering, which requires intelligent systems to simultaneously retrieve relevant moments and detect referenced visual concepts (people and objects) to answer natural language questions about videos. We first augment the TVQA dataset with 310.8K bounding boxes, linking depicted objects to visual concepts in questions and answers. We name this augmented version as TVQA+. We then propose Spatio-Temporal Answerer with Grounded Evidence (STAGE), a unified framework that grounds evidence in both spatial and temporal domains to answer questions about videos. Comprehensive experiments and analyses demonstrate the effectiveness of our framework and how the rich annotations in our TVQA+ dataset can contribute to the question answering task. Moreover, by performing this joint task, our model is able to produce insightful and interpretable spatio-temporal attention visualizations. 1

Introduction We have witnessed great progress in recent years on image-based visual question answering (QA) tasks  (Antol et al., 2015; Yu et al., 2015; Zhu et al., 2016b) . One key to this success has been spatial attention  (Anderson et al., 2018; Shih et al., 2016; Lu et al., 2016) , where neural models learn to attend to relevant regions for predicting the correct answer. Compared to image-based QA, there has been less progress on the performance of video-based QA tasks. One possible reason is that attention techniques are hard to generalize to the temporal nature of videos. Moreover, due to the high cost of annotation, most existing video QA datasets only contain QA pairs, without providing labels for the key clips or regions needed to answer the question. Inspired by previous work on grounded image and video captioning  (Lu et al., 2018; Zhou et al., 2019) , we propose methods that explicitly localize video clips as well as spatial regions for answering videobased questions. Such methods are useful in many scenarios, such as natural language guided spatiotemporal localization, and adding explainability to video question answering, which is potentially useful for decision making and model debugging. To enable this line of research, we also collect new joint spatio-temporal annotations for an existing video QA dataset. In the past few years, several video QA datasets have been proposed, e.g., MovieFIB  (Maharaj et al., 2017) , MovieQA  (Tapaswi et al., 2016) , TGIF-QA  (Jang et al., 2017) , PororoQA , MarioQA  (Mun et al., 2017) , and TVQA  (Lei et al., 2018) . TVQA is one of the largest video QA datasets, providing a large video QA dataset built on top of 6 famous TV series. Be-cause TVQA was collected on television shows, it is built on natural video content with rich dynamics and complex social interactions, where questionanswer pairs are written by people observing both videos and their accompanying dialogues, encouraging the questions to require both vision and language understanding to answer. Movie  (Tapaswi et al., 2016; Maharaj et al., 2017)  and television show  (Lei et al., 2018)  videos come with the limitation of being scripted and edited, but they are still more realistic than cartoon/animation  and game  (Mun et al., 2017)  videos, and they also come with richer, real-world-inspired inter-human interactions and span across diverse domains (e.g., medical, crime, sitcom, etc.), making them a useful testbed to study complex video understanding by machine learning models. One key property of TVQA is that it provides temporal annotations denoting which parts of a video clip are necessary for answering a proposed question. However, none of the existing video QA datasets (including TVQA) provide spatial annotation for the answers. Actually, grounding spatial regions correctly could be as important as grounding temporal moments for answering a given question. For example, in Fig.  1 , to answer the question of "What is Sheldon holding when he is talking to Howard about the sword?", we need to localize the moment when "he is talking to Howard about the sword?", as well as look at the region of "What is Sheldon holding". Hence, in this paper, we first augment a subset of the TVQA dataset with grounded bounding boxes, resulting in a spatio-temporally grounded video QA dataset, TVQA+. It consists of 29.4K multiplechoice questions grounded in both the temporal and the spatial domains. To collect spatial groundings, we start by identifying a set of visual concept words, i.e., objects and people, mentioned in the question or correct answer. Next, we associate the referenced concepts with object regions in individual frames, if there are any, by annotating bounding boxes for each referred concept (see examples in Fig.  1 ). Our TVQA+ dataset has a total of 310.8K bounding boxes linked with referred objects and people, spanning across 2.5K categories (more details in Sec. 3). With such richly annotated data, we then propose the task of spatio-temporal video question answering, which requires intelligent systems to localize relevant moments, detect referred objects and people, and answer questions. We further design several metrics to evaluate the performance of the proposed task, including QA accuracy, object grounding precision, temporal localization accuracy, and a joint temporal localization and QA accuracy. To address spatio-temporal video question answering, we propose a novel end-to-end trainable model, Spatio-Temporal Answerer with Grounded Evidence (STAGE), which effectively combines moment localization, object grounding, and question answering in a unified framework. We find that the QA performance benefits from both temporal moment and spatial region supervision. Additionally, we provide visualization of temporal and spatial localization, which is helpful for understanding what our model has learned. Comprehensive ablation studies demonstrate how each of our annotations and model components helps to improve the performance of the tasks. To summarize, our contributions are: ? We collect TVQA+, a large-scale spatiotemporal video question answering dataset, which augments the original TVQA dataset with frame-level bounding box annotations. To our knowledge, this is the first dataset that combines moment localization, object grounding, and question answering. ? We design a novel video question answering framework, Spatio-Temporal Answerer with Grounded Evidence (STAGE), to jointly localize moments, ground objects, and answer questions. By performing all three sub-tasks together, our model achieves significant performance gains over the baselines, as well as presents insightful, interpretable visualizations. 

 Related Work Question Answering In recent years, multiple question answering datasets and tasks have been proposed to facilitate research towards this goal, in both vision and language communities, in the form of visual question answering  (Antol et al., 2015; Yu et al., 2015; Jang et al., 2017)  and textual question answering  (Rajpurkar et al., 2016; Weston et al., 2016) , respectively. Video question answering  (Lei et al., 2018; Tapaswi et al., 2016;  with naturally occurring subtitles are particularly interesting, as it combines both visual and textual information for question answering. Different from   (Tapaswi et al., 2016)  Movie QA 6.8K/6.5K -TGIF-QA  (Jang et al., 2017)  Tumblr QA 71.7K/165.2K -PororoQA  Cartoon QA 16.1K/8.9K -DiDeMo  (Hendricks et al., 2017)  Flickr TL 10.5K/40.5K -Charades-STA  (Gao et al., 2017)  Home TL -/19.5K -TVQA  (Lei et al., 2018)  TV Show QA/TL 21.8K/152.5K -ANet-Entities  (Zhou et al., 2019)  Youtube  existing video QA tasks, where a system is only required to predict an answer, we propose a novel task that additionally grounds the answer in both spatial and temporal domains. Language-Guided Retrieval Grounding language in images/videos is an interesting problem that requires jointly understanding both text and visual modalities. Earlier works  (Kazemzadeh et al., 2014; Yu et al., 2017 Yu et al., , 2018b Rohrbach et al., 2016)  focused on identifying the referred object in an image. Recently, there has been a growing interest in moment retrieval tasks  (Hendricks et al., 2017 (Hendricks et al., , 2018 Gao et al., 2017) , where the goal is to localize a short clip from a long video via a natural language query. Our work integrates the goals of both tasks, requiring a system to ground the referred moments and objects simultaneously. Temporal and Spatial Attention Attention has shown great success on many vision and language tasks, such as image captioning  (Anderson et al., 2018; Xu et al., 2015) , visual question answering  (Anderson et al., 2018; Trott et al., 2018) , language grounding  (Yu et al., 2018b) , etc. However, sometimes the attention learned by the model itself may not agree with human expectations  (Liu et al., 2016; Das et al., 2016) . Recent works on grounded image captioning and video captioning  (Lu et al., 2018; Zhou et al., 2019)  show better performance can be achieved by explicitly supervising the attention.   

 Dataset In this section, we describe the TVQA+ Dataset, the first video question answering dataset with both spatial and temporal annotations. TVQA+ is built on the TVQA dataset introduced by Lei et al.. TVQA is a large-scale video QA dataset based on 6 popular TV shows, containing 152.5K multiple choice questions from 21.8K, 60-90 second long video clips. The questions in the TVQA dataset are compositional, where each question is comprised of two parts, a question part ("where was Sheldon sitting"), joined via a link word, ("before", "when", "after"), to a localization part that temporally locates when the question occurs ("he spilled the milk"). Models should answer questions using both visual information from the video, as well as language information from the naturally associated dialog (subtitles). Since the video clips on which the questions were collected are usually much longer than the context needed for answering the questions, the TVQA dataset also provides a temporal timestamp annotation indicating the minimum span (context) needed to answer each question. While the TVQA dataset provides a novel question format and temporal annotations, it lacks spatial grounding information, i.e., bounding boxes of the concepts (objects and people) mentioned in the QA pair. We hypothesize that object annotations could provide an additional useful training signal for models to learn a deeper understanding  

 Data Collection Identify Visual Concepts To annotate the visual concepts in video frames, the first step is to identify them in the QA pairs. We use the Stanford CoreNLP part-of-speech tagger  to extract all nouns in the questions and correct answers. This gives us a total of 152,722 words from a vocabulary of 9,690 words. We manually label the non-visual nouns (e.g., "plan", "time", etc.) in the top 600 nouns, removing 165 frequent non-visual nouns from the vocabulary. Bounding Box Annotation For the selected The Big Bang Theory videos from TVQA, we first ask Amazon Mechanical Turk workers to adjust the start and end timestamps to refine the temporal annotation, as we found the original temporal annotation were not ideally tight. We then sample one frame every two seconds from each span for spatial annotation. For each frame, we collect the bounding boxes for the visual concepts in each QA pair. We also experimented with semi-automated annotation for people with face detection  and recognition model  (Liu et al., 2017) , but they do not work well mainly due to many partial occlusion of faces (e.g., side faces) in the frames. During annotation, we provide the original videos (with subtitles) to help the workers understand the context for the given QA pair. More annotation details (including quality check) are presented in the appendix. 

 Dataset Analysis TVQA+ contains 29,383 QA pairs from 4,198 videos, with 148,468 images annotated with 310,826 bounding boxes. Statistics of TVQA+ are shown in Table  2 . Note that we follow the same data splits as the original TVQA dataset, supporting future research on both TVQA and TVQA+. Table  1  compares TVQA+ dataset with other videolanguage datasets. TVQA+ is unique as it supports three tasks: question answering, temporal localization, and spatial localization. It is also of reasonable size compared to the grounded video captioning dataset ANet-Entities  (Zhou et al., 2019) . On average, we obtain 2.09 boxes per image and 10.58 boxes per question. The annotated boxes cover 2,527 categories. We show the number of boxes (in log scale) for each of the top 60 categories in Fig.  2 . The distribution has a long tail, e.g., the number of boxes for the most frequent category "sheldon" is around 2 orders of magnitude larger than the 60th category "glasses". We also show the distribution of bounding box area over image area ratio in Fig.  3  (left). The majority of boxes are fairly small compared to the image, which makes object grounding challenging.  Fig. 3 (right)  shows the distribution of localized span length. While most spans are   T ? No ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 H 8 F 5 E d G 9 q Z N O 8 N 7 b 2 5 g a v R W L K I = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O N K K v Q G b Q i T y a Q d O s m E m R O h h I K v c N j W M l O U t a g U U n U D o p n g C W s B B 8 G 6 q W I k D g T r B K O b a b 3 z w J T m M m n C O G V e T A Y J j z g l Y C z f P m 7 i P v C Y a X z n y x 8 M f b v i V J 2 Z 8 D K 4 B V R Q o Y Z v f / Z D S b O Y J U A F 0 b r n O i l 4 O V H A q W C T c j / T L C V 0 R A a s Z z A h Z o 2 X z 8 6 f 4 D P j h D i S y r w E 8 M z 9 P Z G T W O t x H J j O m M B Q L 9 a m 5 n + 1 X g b R t Z f z J M 2 A J X S + K M o E B o m n W e C Q K 0 Z B j A 0 Q q r i 5 F d M h U Y S C S a x s Q n A X v 7 w M 7 V r V v a i 6 9 5 e V e q 2 I o 4 R O 0 C k 6 R y 6 6 Q n V 0 i x q o h S j K 0 R N 6 Q a / W o / V s v V n v 8 9 Y V q 5 g 5 Q n 9 k f X w D 5 m e U w A = = < / l a t e x i t > T ? Ls ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " o B J l / c W U f p H e s s m n S 3 Q r e S k a E t k = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + f o H Q 9 + u O F V n J r w M b g E V V K j h 2 5 / 9 M K F Z z C R Q Q b T u u U 4 K X k 4 U c C r Y p N z P N E s J H Z E B 6 x m U x K z x 8 t n 5 E 3 x m n B B H i T J P A p 6 5 v y d y E m s 9 j g P T G R M Y 6 s X a 1 P y v 1 s s g u v Z y L t M M m K T z R V E m M C R 4 m g U O u W I U x N g A o Y q b W z E d E k U o m M T K J g R 3 8 c v L 0 K 5 V 3 Y u q e 3 9 Z q d e K O E r o B J 2 i c + S i K 1 R H t 6 i B W o i i H D 2 h F / R q P V r P 1 p v 1 P m 9 d s Y q Z I / R H 1 s c 3 6 X 2 U w g = = < / l a t e x i t > Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " Y r 6 d m S x k 2 n d l g 8 w Y L d I 1 S 6 M y l V E = " > A A A B 8 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K k k V 9 F j w 4 s F D B f s B T S i b z a Z d u t m E 3 Y l Q S v + G F w + K e P X P e P P f u G 1 z 0 N Y H A 4 / 3 Z p i Z F 2 Z S G H T d b 2 d t f W N z a 7 u 0 U 9 7 d 2 z 8 4 r B w d t 0 2 a a 8 Z b L J W p 7 o b U c C k U b 6 F A y b u Z 5 j Q J J e + E o 9 u Z 3 3 n i 2 o h U P e I 4 4 0 F C B 0 r E g l G 0 k n / f H x I f R c I N i f q V q l t z 5 y C r x C t I F Q o 0 + 5 U v P 0 p Z n n C F T F J j e p 6 b Y T C h G g W T f F r 2 c 8 M z y k Z 0 w H u W K m r X B J P 5 z V N y b p W I x K m 2 p Z D M 1 d 8 T E 5 o Y M 0 5 C 2 5 l Q H J p l b y b + 5 / V y j G + C i V B Z j l y x x a I 4 l w R T M g u A R E J z h n J s C W V a 2 F s J G 1 J N G d q Y y j Y E b / n l V d K u 1 7 z L m v d w V W 3 U i z h K c A p n c A E e X E M D 7 q A J L W C Q w T O 8 w p u T O y / O u / O x a F 1 z i p k T + A P n 8 w c 6 X 5 E a < / l a t e x i t > T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t > T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = " > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t > T ? Lh ? 3d < l a t e x i t s h a 1 _ b a s e 6 4 = " T r H Q j l T s y L A Z y j K P t K 4 T W w k o S r s = " > A A A B / 3 i c b Z D L S s N A F I Z P v N Z 6 i w p u 3 A w W w V V J W k G X B T c u X F T o D d o Q J p N p O 3 Q y C T M T o c Q u f B U 3 L h R x 6 2 u 4 8 2 2 c t h G 0 9 Y e B j / + c w z n z B w l n S j v O l 7 W y u r a + s V n Y K m 7 v 7 O 7 t 2 w e H L R W n k t A m i X k s O w F W l D N B m 5 p p T j u J p D g K O G 0 H o + t p v X 1 P p W K x a O h x Q r 0 I D w T r M 4 K 1 s X z 7 u I F 6 m k V U o V t / + I P V 0 L d L T t m Z C S 2 D m 0 M J c t V 9 + 7 M X x i S N q N C E Y 6 W 6 r p N o L 8 N S M 8 L p p N h L F U 0 w G e E B 7 R o U 2 O z x s t n 9 E 3 R m n B D 1 Y 2 m e 0 G j m / p 7 I c K T U O A p M Z 4 T 1 U C 3 W p u Z / t W 6 q + 1 d e x k S S a i r I f F E / 5 U j H a B o G C p m k R P O x A U w k M 7 c i M s Q S E 2 0 i K 5 o Q 3 M U v L 0 O r U n a r Z f f u o l S r 5 H E U 4 A R O 4 R x c u I Q a 3 E A d m k D g A Z 7 g B V 6 t R + v Z e r P e 5 6 0 r V j 5 z B H 9 k f X w D U g q U 9 A = = < / l a t e x i t > T ? Lh ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 g r h i p J s J / q a y l D s j 4 w 3 r D e R M S 8 = "   less than 10 seconds, the largest spans are up to 20 seconds. The average span length is 7.2 seconds, which is short compared to the average length of the full video clips (61.49 seconds). > A A A B / n i c b Z D L S s N A F I Y n X m u 9 R c W V m 8 E i u C p J F X R Z c O P C R Y X e o A 1 h M p m 0 Q y e T M H M i l F D w V d y 4 U M S t z + H O t 3 H a R t D W H w Y + / n M O 5 8 w f p I J r c J w v a 2 V 1 b X 1 j s 7 R V 3 t 7 Z 3 d u 3 D w 7 b O s k U Z S 2 a i E R 1 A 6 K Z 4 J K 1 g I N g 3 V Q x E g e C d Y L R z b T e e W B K 8 0 Q 2 Y Z w y L y Y D y S N O C R j L t 4 + b u A 8 8 Z h r f + c M f D H 2 7 4 l S d m f A y u A V U U K G G b 3 / 2 w 4 R m M Z N A B d G 6 5 z o p e D l R w K l g k 3 I / 0 y w l d E Q G r G d Q E r P G y 2 f n T / C Z c U I c J c o 8 C X j m / p 7 I S a z 1 O A 5 M Z 0 x g q B d r U / O / W i + D 6 N r L u U w z Y J L O F 0 W Z w J D g a R Y 4 5 I p R E G M D h C p u b s V 0 S B S h Y B I r m x D c x S 8 v Q 7 t W d S + q 7 v 1 l p V 4 r 4 i i h E 3 S K z p G L r l A d 3 a I G a i G K c v S E X t C r 9 W g 9 W 2 / W + 7 x 1 x S p m j t A f W R / f 2 G 6 U t w = = < / l a t e x i t > T ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R 4 a 0 A I G C p 7 h F d 4 c 4 7 w 4 7 8 7 H s r X g 5 D O n 8 A f O 5 w / H K Z B H < / l a t e x i t > T ? d < l a t e x i t s h a 1 _ b a s e 6 4 = " 9 o U I a m f a O E F + W 8 i G q u o B 9 y I C g 9 8 = " > A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m q o M e C F 4 8 V + o V t K J v N p l 2 6 2 Y T d i V B K / 4 U X D 4 p 4 9 d 9 4 8 9 + 4 b X P Q 1 g c D j / d m m J k X p F I Y d N 1 v p 7 C x u b W 9 U 9 w t 7 e 0 f H B 6 V j 0 / a J s k 0 4 y 2 W y E R 3 A 2 q 4 F I q 3 U K D k 3 V R z G g e S d 4 L x 3 d z v P H F t R K K a O E m 5 H 9 O h E p F g F K 3 0 2 C R 9 F D E 3 J B y U K 2 7 V X Y C s E y 8 n F c j R G J S / + m H C s p g r Z J I a 0 / P c F P 0 p 1 S i Y 5 L N S P z M 8 p W x M h 7 x n q a J 2 j T 9 d X D w j F 1 Y J S Z R o W w r J Q v 0 9 M a W x M Z M 4 s J 0 x x Z F Z 9 e b i f 1 4 v w + j W n w q V Z s g V W y 6 K M k k w I f P 3 S S g 0 Z y g n l l C m h b 2 V s B H V l K E N q W R D 8 F Z f X i f t W t W 7 q n o P 1 5 V 6 L Y + j C G d w D p f g w Q 3 U 4 R U F r E 0 R 0 O E + i E r B e J Q H C G R u o W y q 6 E A E v U R X j A D P z R 2 Z Q 0 j k 4 d 6 i r R 6 + O J c U U I m v r 5 b q F o l + 1 J 0 U V w Z l A k s 6 p 1 C 1 + u H / M 0 h A i 5 Z F q 3 H T v B T s Y U C i 5 h l H d T D Q n j A 9 a D t s G I m T 2 d b H L X i B 4 b x a d B r M y L k E 7 U 3 x M Z C 7 U e h p 7 p D B n 2 9 b w 3 F v / z 2 i k G V 5 1 M R E m K E P H p o i C V F G M 6 D o n 6 Q g F H O T T A u B L m U Y g = " > A A A C B X i c b V D J S g N B E O 1 x j X E b 9 a i H w S B 4 C j M q m I s Q 8 O I x Q j b I x K G n U 0 m a 9 C x 0 1 w T D M B 6 8 + C t e P C j i 1 X / w 5 t / Y W Q 6 a + K D g 8 V 4 V V f X 8 W H C F t v 1 t L C 2 v r K 6 t 5 z b y m 1 v b O 7 v m 3 n 5 d R Y l k U G O R i G T T p w o E D 6 G G H A U 0 Y w k 0 8 A U 0 / M H 1 2 G 8 M Q S o e h V U c x d A O a C / k X c 4 o a s k z j 1 y E e 0 y H v A N R 9 u C m Q w / d z E v x y s n u q p 5 Z s I v 2 B N Y i c W a k Q G a o e O a X 2 4 l Y E k C I T F C l W o 4 d Y z u l E j k T k O X d R E F M 2 Y D 2 o K V p S A N Q 7 X T y R W a d a K V j d S O p K 0 R r o v 6 e S G m g 1 C j w d W d A s a / m v b H 4 n 9 d K s F t q p z y M E 4 S Q T R d 1 E 2 F h Z I 0 j s T p c A k M x 0 o Q y y f W t F u t T S R n q 4 P I 6 B G f + 5 U V S P y s 6 5 0 X n 9 q J Q L s 3 i y J F D c k x O i U M u S Z n c k A q p E U Y e y T N 5 J W / G k / F i v B s f 0 9 Y l Y z Z z Q P 7 A + P w B u r W Z T g = = < / l a t e x i t > subtitle {st} T t=1 < l a t e x i t s h a 1 _ b a s e 6 4 = " S R h G u S c V 9 o 2 S s j 1 x x R f L k l D b x b 8 = " > A A A C C H i c b V D L S s N A F J 3 U V 6 2 v q E s X B o v g q i Q q 2 I 1 Q c O O y Q l / Q x D C Z T t q h k w c z N 2 I J c e f G X 3 H j Q h G 3 f o I 7 / 8 Z p m o W 2 H h g 4 n H P P z N z j x Z x J M M 1 v r b S 0 v L K 6 V l 6 v b G x u b e / o u 3 s d G S W C 0 D a J e C R 6 H p a U s 5 C 2 g Q G n v V h Q H H i c d r 3 x 1 d T v 3 l E h W R S 2 Y B J T J 8 D D k P m M Y F C S q x / a Q O 8 h l Y m X h 7 M H O 5 U u 2 J m b w q W V 3 b Z c v W r W z B z G I r E K U k U F m q 7 + Z Q 8 i k g Q 0 B M K x l H 3 L j M F J s Q B G 1 P 0 V O 5 E 0 x m S M h 7 S v a I g D K p 0 0 X y Q z j p U y M P x I q B O C k a u / E y k O p J w E n p o M M I z k v D c V / / P 6 C f h 1 J 2 V h n A A N y e w h P + E G R M a 0 F W P A B C X A J 4 p g I p j 6 q 0 F G W G A C q r u K K s G a X 3 m R d E 5 r 1 l n N u j m v N u p F H W V 0 g I 7 Q C b L Q B W q g a 9 R E b U T Q I 3 p G r + h N e 9 J e t H f t Y z Z a 0 o r M P v o D 7 f M H U R y a v g = = < / l a t e x i t > hypothesis hk < l a t e x i t s h a 1 _ b a s e 6 4 = " U E b l 0 f 3 N i Y 3 G t 1 8 Y F f t f 3 1 X U J x I = " > A A A B / n i c b V B N S 8 N A E N 3 U r 1 q / o u L J S 7 A I n k q i g j 0 W v H i s Y D + g D W G z n T Z L N x / s T s Q Q K v 4 V L x 4 U 8 e r v 8 O a / c d v m o K 0 P B h 7 v z T A z z 0 8 E V 2 j b 3 0 Z p Z X V t f a O 8 W d n a 3 t n d M / c P 2 i p O J Y M W i 0 U s u z 5 V I H g E L e Q o o J t I o K E v o O O P r 6 d + 5 x 6 k 4 n F 0 h 1 k C b k h H E R 9 y R l F L n n n U R 3 j A P M i S G A N Q X E 0 e A 2 / s m V W 7 Z s 9 g L R O n I F V S o O m Z X / 1 B z N I Q I m S C K t V z 7 A T d n E r k T M C k 0 k 8 V J J S N 6 Q h 6 m k Y 0 B O X m s / M n 1 q l W B t Y w l r o i t G b q 7 4 m c h k p l o a 8 7 Q 4 q B W v S m 4 n 9 e L 8 V h 3 c 1 5 l K Q I E Z s v G q b C w t i a Z m E N u A S G I t O E M s n 1 r R Y L q K Q M d W I V H Y K z + P I y a Z / X n I u a c 3 t Z b d S L O M r k m J y Q M + K Q K 9 I g N 6 R J W o S R n D y T V / J m P B k v x r v x M W 8 t G c X M I f k D 4 / M H r + 2 W j w = = < / l a t e x i t > 

 Methods Our proposed method, Spatio-Temporal Answerer with Grounded Evidence (STAGE), is a unified framework for moment localization, object grounding and video QA. First, STAGE encodes the video and text (subtitle, QA) via frame-wise regional visual representations and neural language representations, respectively. The encoded video and text representations are then contextualized using a Convolutional Encoder. Second, STAGE computes attention scores from each QA word to object regions and subtitle words. Leveraging the attention scores, STAGE is able to generate QA-aware representations, as well as automatically detecting the referred objects/people. The attended QA-aware video and subtitle representation are then fused together to obtain a joint frame-wise representation. Third, taking the frame-wise representation as input, STAGE learns to predict QA relevant temporal spans, then combines the global and local (span localized) video information to answer the questions. In the following, we describe STAGE in detail. 

 Formulation In our tasks, the inputs are: (1) a question with 5 candidate answers; (2) a 60-second long video; (3) a set of subtitle sentences. Our goal is to predict the answer and ground it both spatially and temporally. Given the question, q, and the answers, {a k } 5 k=1 , we first formulate them as 5 hypotheses (QA-pair) h k = [q, a k ] and predict their correctness scores based on the video and subtitle context  (Onishi et al., 2016) . We denote the ground-truth (GT) answer index as y ans and thus the GT hypothesis as h y ans . We then extract video frames {v t } T t=1 at 0.5 FPS (T is the number of frames for each video). Subtitle sentences are then temporally aligned with the video frames. Specifically, for each frame v t , we pair it with two neighboring sentences based on the subtitle timestamps. We choose two neighbors since this keeps most of the sentences at our current frame rate, and also avoids severe misalignment between the frames and the sentences. The set of aligned subtitle sentences are denoted as {s t } T t=1 . We denote the number of words in each hypothesis and subtitle as L h , L s , respectively. We use N o to denote the number of object regions in a frame, and d = 128 as the hidden size. 

 STAGE Architecture Input Embedding Layer For each frame v t , we use Faster R-CNN  (Ren et al., 2015)  pre-trained on Visual Genome  (Krishna et al., 2017)  to detect objects and extract their regional representation as our visual features  (Anderson et al., 2018) . We keep the top-20 object proposals and use PCA to reduce the feature dimension from 2048 to 300, to save GPU memory and computation. We denote o t,r ? R 300 as the r-th object embedding in the t-th frame. To encode the text input, we use BERT  (Devlin et al., 2019) , a transformer-based language model  (Vaswani et al., 2017)  that achieves state-ofthe-art performance on various NLP tasks. Specifically, we first fine-tune the BERT-base model using the masked language model and next sentence pre-diction objectives on the subtitles and QA pairs from TVQA+ train set. Then, we fix its parameters and use it to extract 768D word-level embeddings from the second-to-last layer for the subtitles and each hypothesis. Both embeddings are projected into a 128D space using a linear layer with ReLU. Convolutional Encoder Inspired by the recent trend of replacing recurrent networks with CNNs  (Dauphin et al., 2016; Yu et al., 2018a)  and Transformers  (Vaswani et al., 2017; Devlin et al., 2019)  for sequence modeling, we use positional encoding (PE), CNNs, and layer normalization  (Ba et al., 2016)  to build our basic encoding block. As shown in the bottom-right corner of Fig.  4 , it is comprised of a PE layer and multiple convolutional layers, each with a residual connection  (He et al., 2016)  and layer normalization. We use Layernorm(ReLU(Conv(x)) + x) to denote a single Conv unit and stack N conv of such units as the convolutional encoder. x is the input after PE, Conv is a depthwise separable convolution  (Chollet, 2017) . We use two convolutional encoders at two different levels of STAGE, one with kernel size 7 to encode the raw inputs, and another with kernel size 5 to encode the fused video-text representation. For both encoders, we set N conv = 2. 

 QA-Guided Attention For each hypothesis h k = [q, a k ], we compute its attention scores w.r.t. the object embeddings in each frame and the words in each subtitle sentence, respectively. Given the encoded hypothesis H k ? R L h ?d for the hypothesis h k with L h words, and encoded visual feature V t ? R No?d for the frame v t with N o objects, we compute their matching scores M k,t ? R L h ?No = H k V T t . We then apply softmax at the second dimension of M k,t to get the normalized scores Mk,t . Finally, we compute the QA-aware visual representation V att k,t ? R L h ?d = Mk,t V t . Similarly, we compute QA-aware subtitle representation S att k,t . Video-Text Fusion The above two QA-aware representations are then fused together as: F k,t = [S att k,t ; V att k,t ; S att k,t V att k,t ]W F + b F , where denotes hadamard product, W F ? R 3d?d and b F ? R d are trainable weights and bias, F k,t ? R L h ?d is the fused video-text representation. After collecting F att k,t from all time steps, we get F att k ? R T ?L h ?d . We then apply another convolutional encoder with a max-pooling layer to obtain the output A k ? R T ?d . Span Predictor To predict temporal spans, we predict the probability of each position being the start or end of the span. Given the fused input A k ? R T ?d , we produce start probabilities p 1 k ? R T and end probabilities p 2 k ? R T using two linear layers with softmax, as shown in the top-right corner of Fig.  4 . Different from existing works  Yu et al., 2018a ) that used the span predictor for text only, we use it for a joint localization of both video and text, which requires properly-aligned joint embeddings. Span Proposal and Answer Prediction Given the max-pooled video-text representation A k , we use a linear layer to further encode it. We run maxpool across all the time steps to get a global hypothesis representation G g k ? R d . With the start and end probabilities from the span predictor, we generate span proposals using dynamic programming . At training time, we combine the set of proposals with IoU ? 0.5 with the GT spans, as well as the GT spans to form the final proposals {st p , ed p }  (Ren et al., 2015) . At inference time, we take the proposals with the highest confidence scores for each hypothesis. For each proposal, we generate a local representation G l k ? R d by maxpooling A k,stp:edp . The local and global representations are concatenated to obtain G k ? R 2d . We then forward {G k } 5 k=1 through softmax to get the answer scores p ans ? R 5 . Compared with existing works  (Jang et al., 2017; Zhao et al., 2017)  that use soft temporal attention, we use more interpretable hard attention, extracting local features (together with global features) for question answering. 

 Training and Inference In this section, we describe the objective functions used in the STAGE framework. Since our spatial and temporal annotations are collected based on the question and GT answer, we only apply the attention loss and span loss on the targets associated with the GT hypothesis (question + GT answer), i.e., M k=y ans ,t , p 1 k=y ans and p 2 k=y ans . For brevity, we omit the subscript k=y ans in the following. Spatial Supervision While the attention described in Sec. 4.2 can be learned in a weakly supervised end-to-end manner, we can also train it with supervision from GT boxes. We define a box as positive if it has an IoU ? 0.5 with the GT box. Consider the attention scores M t,j ? R No from a concept word w j in GT hypothesis h y ans to the set of proposal boxes' representations {o t,r } No r=1 at frame v t . We expect the attention on positive boxes to be higher than the negative ones, and therefore use LSE  loss for the supervision: L t,j = rp?p,rn?n log 1 + exp(M t,j,rn ? M t,j,rp ) , where M t,j,rp is the r p -th element of the vector M t,j . ? p and ? n denote the set of positive and negative box indices, respectively. LSE loss is a smoothed alternative to the widely used hinge loss, it is easier to optimize than the original hinge loss . During training, we randomly sample two negatives for each positive box. We use L att i to denote the attention loss for the i-th example, which is obtained by summing over all the annotated frames {v t } and concepts {w j } for L att t,j . We define the overall attention loss L att = 1 N N i=1 L att i . At inference time, we choose the boxes with scores higher than 0.2 as the predictions. Temporal Supervision Given softmax normalized start and end probabilities p 1 and p 2 , we apply cross-entropy loss: L span = ? 1 2N N i=1 log p 1 y 1 i + log p 2 y 2 i , where y 1 i and y 2 i are the GT start and end indices. Answer Prediction Similarly, given answer probabilities p ans , our answer prediction loss is: L ans = ? 1 N N i=1 log p ans y ans i , where y ans i is the index of the GT answer. Finally, the overall loss is a weighted combination of the three objectives above: L = L ans + w att L att + w span L span , where w att and w span are set as 0.1 and 0.5 based on validation set tuning. 

 Experiments As introduced, our task is spatio-temporal video question answering, requiring systems to temporally localize relevant moments, spatially detect referred objects and people, and answer questions. In this section, we first define the evaluation metrics, then compare STAGE against several baselines, and finally provide a comprehensive analysis of our model. Additionally, we also evaluate STAGE on the full TVQA dataset. 

 Model QA Grd. Temp. ASA Acc. mAP mIoU ST-VQA  (Jang et al., 2017)  48.28 --two-stream  (Lei et al., 2018)    

 Metrics To measure QA performance, we use classification accuracy (QA Acc.). We evaluate span prediction using temporal mean Intersection-over-Union (Temp. mIoU) following previous work  (Hendricks et al., 2017)  on language-guided video moment retrieval. Since the span depends on the hypothesis (QA pair), each QA pair provides a predicted span, but we only evaluate the span of the predicted answer. Additionally, we propose Answer-Span joint Accuracy (ASA), that jointly evaluates both answer prediction and span prediction. For this metric, we define a prediction to be correct if the predicted span has an IoU ? 0.5 with the GT span, provided that the answer prediction is correct. Finally, to evaluate object grounding performance, we follow the standard metric from the PASCAL VOC challenge  (Everingham et al., 2015)  and report the mean Average Precision (Grd. mAP) at IoU threshold 0.5. We only consider the annotated words and frames when calculating the mAP. 

 Comparison with Baseline Methods We consider the two-stream model  (Lei et al., 2018)  as our main baseline. In this model, two streams are used to predict answer scores from subtitles and videos respectively and final answer scores are produced by summing scores from both streams. We retrain the model using the official code 2 on TVQA+ data, with the same feature as STAGE. We also consider ST-VQA  (Jang et al., 2017)  model, which is primarily designed for question answering on short videos (GIFs). We also provide STAGE variants that use only video or subtitle to study the effect of using only one of the modalities. Table  3  shows the test results of STAGE and the baselines. STAGE outperforms the baseline model (two-stream) by a large margin in QA Acc., 3 with 9.83% relative gains. Additionally, STAGE also lo-  calizes the relevant moments with temporal mIoU of 32.49% and detects referred objects and people with mAP of 27.34%. However, a large gap is still observed between STAGE and human, showing space for further improvement. 

 Model Analysis Backbone Model Given the full STAGE model defined in Sec. 4, we define the backbone model as the ablated version of it, where we remove the span predictor along with the span proposal module, as well as the explicit attention supervision. We further replace the CNN encoders with RNN encoders, and remove the aligned fusion from the backbone model. This baseline model uses RNN to encode input sequences and interacts QA pairs with subtitles and videos separately. The final confidence score is the sum of the confidence scores from the two modalities. In the backbone model, we align subtitles with video frames from the start, fusing their representation conditioned on the input QA pair, as in Fig.  4 . We believe this aligned fusion is essential for improving QA performance, as the latter part of STAGE has a joint understanding of both video and subtitles. With both changes, our backbone model obtains 68.31% on QA Acc., significantly higher than the baseline's 65.79%. The results are shown in Table  4 . Model Temp. Sup. val test-public two-stream  (Lei et al., 2018)  65.85 66.46 PAMN  (Kim et al., 2019b)  66.38 66.77 multi-task  (Kim et al., 2019a)  66 Temporal and Spatial Supervision In Table  4 , we also show the results when using temporal and spatial supervision. After adding temporal supervision, the model is be able to ground on the temporal axis, which also improves the model's performance on other tasks. Adding spatial supervision gives additional improvements, particularly for Grd. mAP, with 121.92% relative gain. Span Proposal and Local Feature In the second-to-last row of Table  4 , we show our full STAGE model, which is augmented with local features G l for question answering. Local features are obtained by max-pooling the span proposal regions, which contain more relevant cues for answering the questions. With G l , we achieve the best performance across all metrics, indicating the benefit of using local features. Inference with GT Span The last row of Table  4  shows our model uses GT spans instead of predicted spans at inference time. We observe better QA Acc. with GT spans. Accuracy by Question Type In Table  5 , we show a breakdown of QA Acc. by question type. We observe a clear increasing trend on "what", "who", and "where" questions after using the backbone net and adding attention/span modules in each column. Interestingly, for "why" and "how" questions, our full model fails to present overwhelming performance, indicating some reasoning (textual) module to be incorporated as future work. Qualitative Examples We show two correct predictions in Fig.  5 , where Fig.  5   

 TVQA Results We also conduct experiments on the full TVQA dataset (Table  6 ), without relying on the bounding boxes and refined timestamps in TVQA+. Without temporal supervision, STAGE backbone is able to achieve 3.91% relative gain from the best published result (multi-task) on TVQA test-public set. Adding temporal supervision, performance is improved to 70.23%. For a fair comparison, we also provided STAGE variants using GloVe  (Pennington et al., 2014)  instead of BERT  (Devlin et al., 2019 ) as text feature. Using GloVe, STAGE models still achieve better results. 

 Conclusion We collected the TVQA+ dataset and proposed the spatio-temporal video QA task. This task requires systems to jointly localize relevant moments, detect referred objects/people, and answer questions. We further introduced STAGE, an end-to-end trainable framework to jointly perform all three tasks. Comprehensive experiments show that temporal and spatial predictions help improve QA performance, as well as providing explainable results. Though our STAGE achieves state-of-the-art performance, there is still a large gap compared with human performance, leaving space for further improvement. 
