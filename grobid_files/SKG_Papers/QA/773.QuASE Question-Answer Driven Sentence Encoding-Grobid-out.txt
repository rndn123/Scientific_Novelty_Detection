title
QUASE: Question-Answer Driven Sentence Encoding

abstract
Question-answering (QA) data often encodes essential information in many facets. This paper studies a natural question: Can we get supervision from QA data for other tasks (typically, non-QA ones)? For example, can we use QAMR  to improve named entity recognition? We suggest that simply further pre-training BERT is often not the best option, and propose the questionanswer driven sentence encoding (QUASE) framework. QUASE learns representations from QA data, using BERT or other state-ofthe-art contextual language models. In particular, we observe the need to distinguish between two types of sentence encodings, depending on whether the target task is a single-or multisentence input; in both cases, the resulting encoding is shown to be an easy-to-use plugin for many downstream tasks. This work may point out an alternative way to supervise NLP tasks. 1

Introduction It is labor-intensive to acquire human annotations for NLP tasks which require research expertise. For instance, one needs to know thousands of semantic frames in order to provide semantic role labelings (SRL)  (Palmer et al., 2010) . It is thus an important research direction to investigate how to get supervision signals from indirect data and improve one's target task. This paper studies the case of learning from question-answering (QA) data for other tasks (typically not QA). We choose QA because (1) a growing interest of QA has led to many large-scale QA datasets available to the community; (2) a QA task often requires comprehensive understanding of language and may encode rich information that is useful for other tasks; (3) it is much easier to answer questions relative to a sentence than to annotate linguistics phenomena in it, making this a plausible supervision signal  (Roth, 2017) . There has been work showing that QA data for task A can help another QA task T , conceptually by further pre-training the same model on A (an often larger) before training on T (a smaller)  (Talmor and Berant, 2019; Sun et al., 2019) . However, it remains unclear how to use these QA data when the target task does not share the same model as the QA task, which is often the case when the target task is not QA. For instance, QA-SRL  (He et al., 2015) , which uses QA pairs to represent those predicateargument structures in SRL, should be intuitively helpful for SRL parsing, but the significant difference in their surface forms prevents us from using the same model in both tasks. The success of modern language modeling techniques, e.g., ELMo  (Peters et al., 2018) , BERT  (Devlin et al., 2019) , and many others, has pointed out an alternative solution to this problem. That is, to further pre-train 2 a neural language model (LM) on these QA data in certain ways, obtain a sentence encoder, and use the sentence encoder for the target task, either by fine-tuning or as additional feature vectors. We call this general framework question-answer driven sentence encoding (QUASE). A straightforward implementation of QUASE is to first further pre-train BERT (or other LMs) on the QA data in the standard way, as if this QA task is the target, and then fine-tune it on the real target task. This implementation is technically similar to STILTS  (Phang et al., 2018) , except that STILTS is mainly further pre-trained on textual entailment (TE) data. However, similar to the observations made in STILTS and their follow-up works  (Wang et al., 2019) , we find that additional QA data does not necessarily help the target task using the implementation above. While it is unclear how to predict this behaviour , we do find that this happens a lot for tasks whose input is a single sentence, e.g., SRL and named entity recognition (NER), instead of a sentence pair, e.g., TE. This might be because QA is itself a paired-sentence task, and the implementation above (i.e., to further pre-train BERT on QA data) may learn certain attention patterns that can transfer to another paired-sentence task more easily than to a single-sentence task. Therefore, we argue that, for single-sentence target tasks, QUASE should restrict the interaction between the two sentence inputs when it further pre-trains on QA data. We propose a new neural structure for this and name the resulting implementation s-QUASE, where "s" stands for "single;" in contrast, we name the straightforward implementation mentioned above p-QUASE for "paired." Results show that s-QUASE outperforms p-QUASE significantly on 3 single-sentence tasks-SRL, NER, and semantic dependency parsing (SDP)-indicating the importance of this distinction. Let QUASE A be the QUASE further pre-trained on QA data A. We extensively compare 6 different choices of A: TriviaQA  (Joshi et al., 2017) , NewsQA  (Trischler et al., 2017) , SQuAD  (Rajpurkar et al., 2016) , relation extraction (RE) dataset in QA format (QA-RE for short)  (Levy et al., 2017) , Large QA-SRL  (FitzGerald et al., 2018) , and QAMR . Interestingly, we find that if we use s-QUASE for singlesentence tasks and p-QUASE for paired-sentence tasks, then QUASE QAMR improves all 7 tasks 3 in low resource settings, with an average error reduction rate of 7.1% compared to BERT.  4  While the set of tasks we experimented with here is nonexhaustive, we think that QUASE QAMR has the potential of improving on a wide range of tasks. This work has three important implications. First, it provides supporting evidence to an important alternative to supervising NLP tasks: using QA to annotate language, which has been discussed in works such as QA-SRL, QAMR, and QA-RE. If it is difficult to teach annotators the formalism of a certain task, perhaps we can instead collect QA data that query the target phenomena and thus get supervision from QA for the original task (and possibly more). Second, the distinction between s-QUASE and p-QUASE suggests that sentence encoders should consider some properties of the target task (e.g., this work distinguishes between single-and multi-sentence tasks). Third, the good performance of QUASE QAM R suggests that predicate-argument identification is an important capability that many tasks rely on; in contrast, many prior works observed that only language modeling would improve target tasks generally. 

 QA Driven Sentence Encoding This work aims to find an effective way to use readily available QA data to improve a target task that is typically not QA. A natural choice nowadaysgiven the success of language models-is to further pre-train sentence encoders, e.g. BERT, on QA data in certain ways, and then use the new encoder in a target task. This general framework is called QUASE in this work, and the assumption is that the sentence encoders learned from QA data have useful information for the target task. A straightforward implementation of QUASE is to further pre-train BERT on QA data in the standard way, i.e., fine-tune BERT as if this QA dataset is the target task, and then fine-tune BERT on the real target task. However, we find that this straightforward implementation is less effective or even negatively impacts target tasks with single-sentence input; similar observations were also made in STILTS  (Phang et al., 2018)  and its follow-ups  (Wang et al., 2019) : They further pretrain sentence encoders, e.g., ELMo, BERT, and GPT  (Radford et al., 2018) , on TE data and find that it is not effective for the syntax-oriented CoLA task and the SST sentiment task in GLUE, which are both single-sentence tasks  (Wang et al., 2018) . One plausible reason is that the step of further pre-training on QA data does not take into account some properties of the target task, for instance, the number of input sentences. QA is inherently a paired-sentence task; a typical setup is, given a context sentence and a question sentence, predict the answer span. Further pre-training BERT on QA data will inevitably learn how to attend to the context given the question. This is preferable when the target task is also taking a pair of sentences as input, while it may be irrelevant or harmful for single-sentence tasks. It points out that we may need two types of sentence encodings when further pre-training BERT on QA data, depending on the type of the target task. The following subsection discusses this issue in detail. 

 Two Types of Sentence Encodings Standard sentence encoding is the problem of converting a sentence S=[w 1 , w 2 , ? ? ?, w n ] to a se- quence of vectors h(S)=[h 1 , h 2 , ? ? ?, h n ] (e. g., skipthoughts  (Kiros et al., 2015) ). Ideally, h(S) should encode all the information in S, so that it is taskagnostic: given a target task, one can simply probe h(S) and retrieve relevant information. In practice, however, only the information relevant to the training task of h(S) is kept. For instance, when we have a task with multi-sentence input (e.g., QA and TE), the attention pattern A among these sentences will affect the final sentence encoding, which we call h A (S); in comparison, we denote the sentence encoding learned from single-sentence tasks by h(S), since there is no cross-sentence attention A. In a perfect world, the standard sentence encoding h(S) expresses also the conditional sentence encoding h A (S). However, we believe that there is a trade-off between the quality and the quantity of semantic information a model can encode. Our empirical results corroborate this conclusion and more details can be found in Appendix A.2. The distinction between the sentence encodings types may explain the negative impact of using QA data for some single-sentence tasks: Further pre-training BERT on QA data essentially produces a sentence encoding with cross-sentence attentions h A (S), while the single-sentence tasks expect h(S). These two sentence encodings may be very different: One view is from the theory of information bottleneck  (Tishby et al., 1999; Tishby and Zaslavsky, 2015) , which argues that training a neural network on a certain task is extracting an approximate minimal sufficient statistic of the input sentences with regard to the target task; information irrelevant to the target task is maximally compressed. In our case, this corresponds to the process where the conditional sentence encoding compresses the information irrelevant to the relation, which will enhance the quality but reduce the quantity of the sentence information. 

 Two Implementations of QUASE In order to fix this issue, we need to know how to learn h(S) from QA data. However, since QA is a paired-sentence task, the attention pattern between the context sentence and the question sentence is important for successful further pre-training on QA. Therefore, we propose that if the target task is single-sentence input, then fur-ther pre-training on QA data should also focus on single-sentence encodings in the initial layers; the context sentence should not interact with the question sentence until the very last few layers. This change is expected to hurt the capability to solve the auxiliary QA task, but it is later proved to transfer better to the target task. This new treatment is called s-QUASE with "s" representing "singlesentence," while the straightforward implementation mentioned above is called p-QUASE where "p" means "paired-sentence." The specific structures are shown in Fig.  1 . 

 s-QUASE The architecture of s-QUASE is shown in Fig.  1(a) . When further pre-training it on QA data, the context sentence and the question sentence are fed into two pipelines. We use the same Sentence2Question and Question2Sentence attention as used in BiDAF . Above that, "Sentence Modeling," "Question Modeling," and "Interaction Layer" are all bidirectional transformers  (Vaswani et al., 2017)  with 2 layers, 2 layers, and 1 layer, respectively. Finally, we use the same classification layer as BERT, which is needed for training on QA data. Overall, this implementation restricts interactions between the paired-sentence input, especially from the question to the context, because when serving the target task, this attention will not be available. Using s-QUASE in target tasks. Given a sentence S, s-QUASE can provide a sequence of hidden vectors h(S), i.e., the output of the "Sentence Modeling" layer in Fig.  1(a) . Although h(S) does not rely on the question sentence, h(S) is optimized so that upper layers can use it to handle those questions in the QA training data, so h(S) indeed captures information related to the phenomena queried by those QA pairs. For single-sentence tasks, we use h(S) from s-QUASE as additional features, and concatenate it to the word embeddings in the input layer of any specific neural model. 5 

 p-QUASE The architecture of p-QUASE is shown in Fig.  1(b) , which is the standard way of pre-training BERT. That is, when further pre-training it on QA data, the context sentence and the question sentence form a single sequence (separated by special tokens) and are fed into BERT. Using p-QUASE in target tasks. Given a sentence pair S (concatenated), p-QUASE produces h A (S), i.e., the output of the BERT module in Fig.  1(b) . One can of course continue fine-tuning p-QUASE on the target task, but we find that adding p-QUASE to an existing model for the target task is empirically better (although not very significant); specifically, we try to add h A (S) to the final layer before the classification layer, and we also allow p-QUASE to be updated when training on the target task, although it is conceivable that other usages may lead to even stronger results. For instance, when the target task is token classification, e.g., MRC, we can simply concatenate the vectors of h A (S) at each timestamp to any existing model; when the target task is sentence classification, e.g., TE, we apply max-pooling and average-pooling on h A (S), respectively, and concatenate the two resulting vectors to any existing model before the final classification layer. 

 Related Work on Sentence Encoding Modern LMs are essentially sentence encoders pretrained on unlabeled data and they outperform early sentence encoders such as skip-thoughts  (Kiros et al., 2015) . While an LM like BERT can handle lexical and syntactic variations quite well, it still needs to learn from some annotations to acquire the "definition" of many tasks, especially those requiring complex semantics  (Tenney et al., 2019) . Although we extensively use BERT here, we think that the specific choice of LM is orthogonal to our proposal of learning from QA data. Stronger LMs, e.g., RoBERTa  (Liu et al., 2019)  or XLNet  (Yang et al., 2019) , may only strengthen the proposal here. This is because a stronger LM represents unlabeled data better, while the proposed work is about how to represent labeled data better. CoVe  (McCann et al., 2017)  is another attempt to learn from indirect data, translation data specifically. However, it does not outperform ELMo or BERT in many NLP tasks  (Peters et al., 2018)  and probing analysis  (Tenney et al., 2019) . In contrast, our QUASE will show stronger experimental results than BERT on multiple tasks. In addition, we think QA data is generally cheaper to collect than translation data. The proposed work is highly relevant to  Phang et al. (2018)  and their follow-up works  (Wang et al., 2019)  to improve another target task. The key differences are as follows: First, we distinguish two types of sentence encodings, which provide explanation to their puzzle that sentence-pair tasks seem to benefit more from further pre-training than single-sentence tasks do. Second, they only focus on fine-tuning based methods which cannot be easily plugged in many single-sentence tasks such as SRL and Coref, while we analyze both fine-tuning based and feature-based approaches. Third, they mainly use TE signals for further pre-training, and evaluate their models on GLUE  (Wang et al., 2018)  which is a suite of tasks very similar to TE. Our work instead makes use of QA data to help tasks that are typically not QA. Fourth, from their suite of further pre-training tasks, they observe that only further pre-training on language modeling tasks has the power to improve a target task in general, while we find that QAMR may also have this potential, indicating the universality of predicate-argument structures in NLP tasks. Our work is also related to Sentence-BERT  (Reimers and Gurevych, 2019)  in terms of providing a better sentence representation. However, their focus was deriving semantically meaningful sentence embeddings that can be compared using cosine-similarity, which reduces the computational cost of finding the most similar pairs. In contrast, QUASE provides a better sentence encoder in the same format as BERT (a sequence of word embeddings) to better support tasks that require complex semantics. 

 Applications of QUASE In this section, we conduct thorough experiments to show that QUASE is a good framework to get supervision from QA data for other tasks. We first give an overview of the datasets and models used in these experiments before diving into the details of each experiment. Specifically, we use PropBank  (Kingsbury and Palmer, 2002)  (SRL), the dataset from the SemEval'15 shared task  (Oepen et al., 2015)  with DELPH-IN MRS-Derived Semantic Dependencies target representation (SDP), CoNLL'03 (Tjong Kim  Sang and De Meulder, 2003)  (NER), the dataset in SemEval'10 Task 8  (Hendrickx et al., 2009)   For single-sentence tasks, we use both simple baselines (e.g., BiLSTM and CNN; see Appendix B.1) and near-state-of-the-art models published in recent years. As in ELMo, we use the deep neural model in  for SRL, the model in  Peters et al. (2018)  for NER, and the end-to-end neural model in  for Coref. We also use the biaffine network in  Dozat and Manning (2018)  for SDP but we removed part-of-speech tags from its input, and the attention-based BiLSTM in  Zhou et al. (2016)  is the strong baseline for RE. In addition, we replace the original word embeddings in these models (e.g., GloVe  (Pennington et al., 2014) ) by BERT. Throughout this paper, we use the pre-trained case-insensitive BERT-base implementation. More details on our experimental setting can be found in Appendix B, including the details of simple models in B.1, some common experimental settings of QUASE in B.2, and s-QUASE combined with other SOTA embeddings (ELMo and Flair  (Akbik et al., 2018) ) in B.3. 

 Necessity of Two Representations We first consider a straightforward method to use QA data for other tasks-to further pre-train BERT on these QA data. We compare BERT further pre-trained on QAMR (denoted by BERT QAM R ) with BERT on two single-sentence tasks (SRL and RE) and two paired-sentence tasks (TE and MRC). We use a feature-based approach for singlesentence tasks and a fine-tuning approach for paired-sentence tasks. The reason is two-fold. On the one hand, current SOTAs of all singlesentence tasks considered in this paper are still   2 : Probing results of the sentence encoders from s-QUASE and p-QUASE. In all tasks, we fix the model QUASE and use the sentence encodings as input feature vectors for the model of each task. In order to keep the model structure as simple as possible, we use BiLSTM for SRL, NER, and TE, Biaffine for SDP, and BiDAF for MRC. We compare on 10% and 100% of the data in all tasks except TE, where we use 30% to save run-time. feature-based. How to efficiently use sentence encoders (e.g. BERT) in a fine-tuning approach for some complicated tasks (e.g. SRL and SDP) is unclear. On the other hand, the fine-tuning approach shows great advantage over feature-based on many paired-sentence tasks (e.g. TE and MRC). Similar to  Phang et al. (2018) , we find in Table  1  that the two single-sentence tasks benefit less than the two paired-sentence tasks from BERT QAM R , which indicates that simply "further pre-training BERT" is not enough. We then compare s-QUASE QAM R and p-QUASE QAM R on three single-sentence tasks (SRL, SDP and NER) and two paired-sentence tasks (TE and MRC) to show that it is important to distinguish two types of sentence representations. Rather than concatenating two embeddings as proposed in Sec. 2.2, here we replace BERT embeddings with QUASE embeddings for convenience. The results are shown in Table  2 . We find that s-QUASE has a great advantage over p-QUASE on single-sentence tasks and p-QUASE is better than s-QUASE on paired-sentence tasks. The proposal of two types of sentence encoders tackles the problem one may encounter when there is only further pre-training BERT on QAMR for single-sentence tasks. In summary, it is necessary to distinguish two types of sentence representations for singlesentence tasks and paired-sentence tasks.  

 Sample Complexity of QUASE 

 Data Choice for Further Pre-training We compare BERT with QUASE further pretrained with the same numbre of QA pairs on 6 different QA datasets (TriviaQA  (Joshi et al., 2017) , NewsQA  (Trischler et al., 2017) , SQuAD, QA-RE  (Levy et al., 2017) , Large QA-SRL  (FitzGerald et al., 2018) , and QAMR). s-QUASE further pretrained on different QA datasets are evaluated on four single-sentence tasks in a feature-based approach: SRL, SDP, NER and RE. p-QUASE further pre-trained on different QA datasets is evaluated on one task (TE) in a fine-tuning approach. In Table  3 , we find that the best options are quite different across different target tasks, which is expected because a task usually benefits more from a more similar QA dataset. However, we also find that QAMR is generally a good furtherpre-training choice for QUASE. This is consistent with our intuition: First, QAMR has a simpler concept class than other paragraph-level QA datasets, such as TriviaQA, NewsQA and SQuAD. It is easier for QUASE to learn a good representation with QAMR to help sentence-level tasks. Second, QAMR is more general than other sentence-level QA datasets, such as QA-RE and Large QA-SRL. 7 Therefore, we think that the capability to identify predicate-argument structures can generally help many sentence-level tasks, as we discuss next. 

 The Effectiveness of QUASE Here we compare QUASE QAM R with BERT on 5 single-sentence tasks and 2 paired-sentence tasks, where QUASE QAM R is further pre-trained on the training set (51K QA pairs) of the QAMR dataset. As shown in Table  4 , we find that QUASE QAM R 7 Although the average performance of QUASEQAMR on five tasks is slightly below QUASELarge QA?SRL, for which the benefit mostly comes from SRL. QUASE is mainly designed to improve a lot of tasks, so QAMR is a better choice in our setup, but in practice, we do not limit QUASE to any specific QA dataset and one can use the best one for corresponding target tasks. has a better performance than BERT on both singlesentence tasks and paired-sentence tasks, especially in the low-resource setting 8 , indicating that QUASE QAM R can provide extra features compared to BERT. Admittedly, the improvement in the "Full" setting is not significantly large, but we think that this is expected because large direct training data are available (such as SRL with 278K training examples in OntoNotes). However, it is still promising that 51K indirect QA pairs can improve downstream tasks in the low-resource setting (i.e. several thousands direct training examples). That is because they help the scalability of machine learning methods, especially for some specific domains or some low-resource languages where direct training data do not exist in large scale. 

 Discussion In this section we discuss a few issues pertaining to improving QUASE by using additional QA datasets and the comparison of QUASE with related symbolic representations.   

 Further 

 Comparison with Symbolic Meaning Representations Traditional (symbolic) shallow meaning representations such as SRL and AMR, suffer from having a fixed set of relations one has to commit to. Moreover, inducing these representations requires costly annotation by experts. Proposals such as QA-SRL, QAMR, semantic proto-roles  (Reisinger et al., 2015) , and universal dependencies  (White et al., 2016)  avoid some of these issues by using natural language annotations, but it is unclear how other tasks can take advantage of them. QUASE is proposed to facilitate inducing distributed representations instead of symbolic representations from QA signals; it benefits from cheaper annotation and flexibility, and can also be easily used in downstream tasks. The following probing analysis, based on the Xinhua subset in the AMR dataset, shows that s-QUASE QAM R embeddings encode more semantics related to AMR than BERT embeddings. Specifically, we use the same edge probing model as  Tenney et al. (2019) , and find that the probing accuracy (73.59) of s-QUASE QAM R embeddings is higher than that (71.58) of BERT. At the same time, we find that p-QUASE QAM R can achieve 76.91 F1 on the PTB set of QA-SRL, indicating that p-QUASE QAM R can capture enough information related to SRL to have a good zero-shot SRL performance. More details can be found in Appendix C.1. Another fact worth noting is that AMR can be used to improve downstream tasks, such as MRC  (Sachan and Xing, 2016) , TE  (Lien and Kouylekov, 2015) , RE  (Garg et al., 2019)  and SRL  (Song et al., 2018) . The benefits of QUASE QAM R on downstream tasks show that we can take advantage of AMR by learning from much cheaper QA signals dedicated to it. 

 Difficulties in Learning Symbolic Representations from QA Signals QUASE is designed to learn distributed representations from QA signals to help down-stream tasks. We further show the difficulties of learning two types of corresponding symbolic representations from QA signals, which indicates that the two other possible methods are not as tractable as ours. One option of symbolic representation is the QAMR graph.  show that question generation for QAMR representations can only achieve a precision of 28%, and a recall of 24%, even with fuzzy matching (multi-BLEU 10 > 0.8). Furthermore, it is still unclear how to use the complex QAMR graph in downstream tasks. These results indicate that learning a QAMR parser for down-stream tasks is mainly hindered by question generation, and how to use the full information of QAMR for downstream tasks is still unclear. Another choice of symbolic representation is AMR, since QAMR is proposed to replace AMR. We consider a simpler setting, learning an SRL parser from Large QA-SRL. We propose three models in different perspectives, but the best performance of them is only 54.10 F1, even with fuzzy matching (Intersection/Union ? 0.5). More details can be found in Appendix C.2. Although a lot of methods  (Khashabi et al., 2018; Marcheggiani and Titov, 2017; Strubell et al., 2018)  can be adopted to use SRL/AMR in downstream tasks, the difficulty of learning a good SRL/AMR parser from QA signals hinders this direction. The difficulties of learning the two types of symbolic representations from QA signals indicate that our proposal of learning distributed representations from QA signals is a better way of making use of the latent semantic information in QA pairs for down-stream tasks. 

 Conclusion In this paper, we investigate an important problem in NLP: Can we make use of low-cost signals, such as QA data, to help related tasks? We retrieve signals from sentence-level QA pairs to help NLP tasks via two types of sentence encoding approaches. For tasks with a single-sentence input, such as SRL and NER, we propose s-QUASE that provides latent sentence-level representations; for tasks with a sentence pair input, such as TE and MRC we propose p-QUASE, that generates latent 10 An average of BLEU1-BLEU4 scores. representations related to attentions. Experiments on a wide range of tasks show that the distinction of s-QUASE and p-QUASE is highly effective, and QUASE QAM R has the potential to improve on many tasks, especially in the low-resource setting. 
