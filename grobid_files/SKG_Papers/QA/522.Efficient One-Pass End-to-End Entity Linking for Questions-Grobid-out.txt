title
Efficient One-Pass End-to-End Entity Linking for Questions

abstract
We present ELQ, a fast end-to-end entity linking model for questions, which uses a biencoder to jointly perform mention detection and linking in one pass. Evaluated on WebQSP and GraphQuestions with extended annotations that cover multiple entities per question, ELQ outperforms the previous state of the art by a large margin of +12.7% and +19.6% F1, respectively. With a very fast inference time (1.57 examples/s on a single CPU), ELQ can be useful for downstream question answering systems. In a proof-of-concept experiment, we demonstrate that using ELQ significantly improves the downstream QA performance of GraphRetriever  (Min et al., 2019) . 1

Introduction Entity linking (EL), the task of identifying entities and mapping them to the correct entries in a database, is crucial for analyzing factoid questions and for building robust question answering (QA) systems. For instance, the question "when did shaq come to the nba?" can be answered by examining Shaquille O'Neal's Wikipedia article  (Min et al., 2019) , or its properties in a knowledge graph  (Yih et al., 2015; Yu et al., 2017) . However, real-world user questions are invariably noisy and ill-formed, lacking cues provided by casing and punctuation, which prove challenging to current end-to-end entity linking systems  (Yang and Chang, 2015; Sorokin and Gurevych, 2018) . While recent pre-trained models have proven highly effective for entity linking  (Logeswaran et al., 2019; , they are only designed for entity disambiguation and require mention boundaries to be given in the input. Additionally, such systems Figure  1 : Overview of our end-to-end entity linking system. We separately encode the question and entity. We use the question representations to jointly detect mentions and score candidate entities through innerproduct with the entity vector. have only been evaluated on long, well-formed documents like news articles  (Ji et al., 2010) , but not on short, noisy text. Also, most prior works have focused mainly on improving model prediction accuracy, largely overlooking efficiency. In this work, we propose ELQ, a fast and accurate entity linking system that specifically targets questions. Following the Wikification setup  (Ratinov et al., 2011) , ELQ aims to identify the mention boundaries of entities in a given question and their corresponding Wikipedia entity. We employ a biencoder based on BERT  as shown in Figure  1 . The entity encoder computes entity embeddings for all entities in Wikipedia, using their short descriptions. Then, the question encoder derives token-level embeddings for the input question. We detect mention boundaries using these embeddings, and disambiguate each entity mention based on an inner product between the mention embeddings (averaged embedding over mention tokens) and the entity embeddings. Our model ex-tends the work of  but with one major difference: our system does not require prespecified mention boundaries in the input, and is able to jointly perform mention detection and entity disambiguation in just one pass of BERT. Thus, at inference time, we are able to identify multiple entities in the input question efficiently. We extend entity disambiguation annotations from  Sorokin and Gurevych (2018)  to create an endto-end question entity linking benchmark. Evaluated on this benchmark, we are able to outperform previous methods in both accuracy and run-time. ELQ has much faster end-to-end inference time than any other neural baseline (by 2?), while being more accurate than all previous models we evaluate against, suggesting that it is practically useful for downstream QA systems. We verify the applicability of ELQ to practical QA models in a proofof-concept experiment, by augmenting GraphRetriever  (Min et al., 2019)  to use our model, improving its downstream QA performance on three open-domain QA datasets (by up to 6%). 

 Related Work Much prior work on entity linking has focused on long, grammatically coherent documents that contain many entities. This setting does not accurately reflect the difficulties of entity linking on questions. While there has been some previous work on entity linking for questions  (Sorokin and Gurevych, 2018; Blanco et al., 2015; Chen et al., 2018; Tan et al., 2017) , such works (mostly from the pre-BERT era) utilize complex models with many interworking modules. For example,  Sorokin and Gurevych (2018)  proposes a variable-context granularity (VCG) model to address the noise and lack of context in questions, which incorporates signals from various levels of granularity by using character-level, token-level, and knowledge-baselevel modules. They also rely on external systems as a part of the modeling pipeline. In this work, we take a much simpler approach that uses a biencoder. Biencoder models have been used in a wide range of tasks  (Seo et al., 2019; Karpukhin et al., 2020; . They enable fast inference time through maximum inner product search. Moreover, as we find, biencoders can be decomposed into reusable question and entity encoders, and we can greatly expedite training by training one component independently of the other. 

 Problem Definition & ELQ Model We formally define our entity linking task as follows. Given a question q and a set of entities E = {e i } from Wikipedia, each with titles t(e i ) and text descriptions d(e i ), our goal is to output a list of tuples, (e, [m s , m e ]), whereby e ? E is the entity corresponding to the mention span from the m s -th to m e -th token in q. In practice, we take the title and first 128 tokens of the entity's Wikipedia article as its title t(e i ) and description d(e i ). We propose an end-to-end entity linking system that performs both mention detection and entity disambiguation on questions in one pass of BERT. Given an input question q = q 1 ? q n of length n, we first obtain question token representations based on BERT : [q 1 ? q n ] = BERT([CLS] q 1 ? q n [SEP]) ? R n?h , where each q i is a h-dimensional vector. We then obtain entity representations x e for every e i ? E. x e = BERT [CLS] ([CLS]t(e i )[ENT]d(e i )[SEP]) ? R h , where  [CLS]  indicates that we select the representation of the [CLS] token. We consider candidate mentions as all spans [i, j] (i-th to j-th tokens of q) in the text up to length L. 

 Mention Detection To compute the likelihood score of a candidate span [i, j] being an entity mention, we first obtain scores for each token being the start or the end of a mention: s start (i) = w start q i , s end (j) = w end q j , where w start , w end ? R h are learnable vectors. We additionally compute scores for each token t being part of a mention: s mention (t) = w mention q t , where w mention ? R h is a learnable vector. We finally compute mention probabilities as: p([i, j]) = ?(s start (i) + s end (j) + j t=i s mention (t)). Entity Disambiguation We obtain a mention representation for each mention candidate [i, j] by averaging q i ? q j , and compute a similarity score s between the mention candidate and an entity candidate e ? E: y i,j = 1 (j ? i + 1) j t=i q t ? R h , s(e, [i, j]) = x e y i,j . We then compute a likelihood distribution over all entities, conditioned on the mention [i, j]: p(e|[i, j]) = exp(s(e, [i, j])) e ?E exp(s(e , [i, j])) . Training We jointly train the mention detection and entity disambiguation components by optimizing the sum of their losses. We use a binary crossentropy loss across all mention candidates: L MD = ? 1 N 1?i?j? min(i+L?1,n) y [i,j] log p([i, j]) +(1 ? y [i,j] ) log (1 ? p([i, j])) , whereby y [i,j] = 1 if [i, j] is a gold mention span, and 0 otherwise. N is the total number of candidates we consider.  2  The entity disambiguation loss is given by L ED = ? log p(e g |[i, j]), where e g is the gold entity corresponding to mention [i, j]. To expedite training, we use a simple transfer learning technique: we take the entity encoder trained on Wikipedia by  and freeze its weights, training only the question encoder on QA data. In addition, we mine hard negatives. As entity encodings are fixed, a fast search of hard negatives in real time is possible. Inference Figure  1  shows our inference process. Given an input question q, we use our mention detection model to obtain our mention set M = {[i, j] : 1 ? i ? j ? min(i+L?1, n), p([i, j]) > ?}, where ? is our threshold (a hyperparameter). We then compute p(e, [i, j]) = p(e|[i, j])p([i, j]) for each mention [i, j] ? M, and threshold according to ?. In contrast to a two-stage pipeline which first extracts mentions, then disambiguates entities  (F?vry et al., 2020)  us the flexibility to consider multiple possible candidate mentions for entity linking. This can be crucial in questions as it can be difficult to extract mentions from short, noisy text in a single step. More implementation details can be found in Appendix D. 

 Experiments 

 Data We evaluate our approach on two QA datasets, We-bQSP  (Yih et al., 2016)  and GraphQuestions  (Su et al., 2016) , with additional entity annotations provided by  Sorokin and Gurevych (2018) . The original datasets do not have all mention boundary labels annotated. Therefore, in order to evaluate both mention detection and entity disambiguation, we extend previous labels and create new end-to-end question entity-linking datasets, WebQSP EL and GraphQ EL .  3  In line with our task definition, all entities presented in each question are labeled with (e, [m s , m e ]), whereby e ? E is the entity corresponding to the mention span from the m s -th to m e -th token in q. We ask four in-house annotators to identify corresponding mention boundaries, given gold entities in the questions. We exclude examples that link to null or no entities, that are not in Wikipedia, or are incorrect or overly generic (e.g. linking a concept like marry). To check interannotation agreement amongst the 4 annotators, we set aside a shared set of documents (comprised of documents from both datasets) that all 4 annotators annotated. We found exact-match inter-annotator agreement to be 95% (39/41) on this shared set. Table  1  reports the statistics of the resulting datasets, WebQSP EL and GraphQ EL . Following  Sorokin and Gurevych (2018)  if the groundtruth entity is identified and the predicted mention boundaries overlap with the groundtruth boundaries. (This is sometimes known as "weak matching".) Specifically, let T be a set of gold entity-mention tuples and T be a set of predicted entity-mention tuples, we define precision (p), recall (r) and F1-score (F 1 ) as follows: C = e ? E|[m s , m e ] ? [ m s , m e ] = ?, (e, [m s , m e ]) ? T , (e, [ m s , m e ]) ? T , p = |C| | T | , r = |C| |T | , F 1 = 2pr p + r . Baselines We use the following baselines: (1) TAGME  (Ferragina and Scaiella, 2012) , a lightweight, on-the-fly entity linking system that is popular for many downstream QA tasks, being much faster than most neural models  (Joshi et al., 2017; Sun et al., 2018; Min et al., 2019) , (  2 ) VCG  (Sorokin and Gurevych, 2018) , the current state-of-the-art entity linking system on WebQSP, and (3) biencoder from BLINK . As BLINK requires pre-specified mention boundaries as input, we train a separate, BERT-based span extraction model on WebQSP in order to predict mention boundaries (details in Appendix B). 

 Results Table  2  show our main results. We find that BERTbased biencoder models far outperform the stateof-the-art (VCG) on both datasets, in performance and in runtime. Moreover, ELQ outperforms all other models trained in a comparable setting, and is much more efficient than every other neural baseline (VCG and BLINK). ELQ is also up to 2.3? better than TAGME -in the case of WebQSP EL . Performance ELQ outperforms BLINK, suggesting that it is possible to train representations   (Min et al., 2019) . from a single model to resolve both entity references as well as mention boundaries of all entities in text, without restricting the model to focusing on a single marked entity as in BLINK. Runtime We record the inference speed on CPUs in number of questions processed per second for all models (Table  2 ). For BLINK, we report the combined speed of our span extraction model and the BLINK entity linker, in order to compare the end-to-end speeds. ELQ, which performs both detection and disambiguation in one pass of BERT, is approximately 2? faster than BLINK, which performs multiple passes, while also outperforming BLINK in F1 score. Moreover, against TAGME  (Ferragina and Scaiella, 2012) , ELQ is only 1.5? slower on WebQSP EL and 2.0? slower on GraphQ EL , despite TAGME being a completely non-neural model (with much lower accuracy). 

 QA Experiments To demonstrate the impact of improved entity linking on the end QA accuracy, we experiment with the task of textual open-domain question answering, using GraphRetriever (GRetriever)  (Min et al., 2019) . GRetriever uses entity linking to construct a graph of passages in the retrieval step and deploys a reader model to answer the question. The original model uses TAGME for entity linking; we replace TAGME with ELQ and keep the other components the same, in order to isolate the impact of entity linking.  4  As an additional baseline, we also add the result of TF-IDF, implemented by Chen et al. (  2017 ), a widely used retrieval system. Results are shown in Table  3 . Following literature in open-domain QA, we evaluate our approach on three datasets, WebQuestions  (Berant et al., 2013 ), Natural Questions (Kwiatkowski et al., 2019  and TriviaQA  (Joshi et al., 2017) . In particular, WebQuestions (WQ) and Natural Questions (NQ) consist of short, noisy questions from Web queries, in line with the motivation of our work. We observe that simply replacing TAGME with ELQ significantly improves performance, including 5.9% and 3.9% absolute improvements on WQ and NQ, respectively. While ELQ trained on Wikipedia achieves good results overall, further fine-tuning on WebQSP EL gives extra gains on WQ. This indicates that, if entity linking annotations in the same domain are available, using them to finetune ELQ can bring further gains. 

 Analysis Mention Detector vs. Entity Linker We set up experiments to disentangle the capability of ELQ's entity linker and mention detector. First, to test just the mention detector (MD only), we measure just the mention boundary overlap between predicted and groundtruth mentions, ignoring the entity label. Next, to test just the entity linker (EL only), we give the entity linking component gold mention boundaries, and compute the resulting F1 score. We do this for both ELQ and BLINK. For comparability, we use the version of ELQ trained on Wikipedia. Results are reported in Table  4 . Surprisingly, we find that both components of ELQ Qualitative We manually examine all our model's errors on the WebQSP EL and GraphQ EL dev sets. We identify four broad error categories: (1) technically correct -where our model was technically correct but limitations in evaluation falsely penalized our model (i.e., we found a more or less precise version of the same entity), (  2 ) not enough entities -where the model did not fully identify all entities in the question, (3) wrong entities -where our model linked to the wrong entity, (4) insufficient context -where the model made reasonable mistakes due to the lack of context (that even reasonable humans would make). Error type breakdowns can be found in Table  5 . 

 Conclusion We proposed an end-to-end model for entity linking on questions that jointly performs mention detection and disambiguation with one pass through BERT. We showed that it is highly efficient, and that it outperforms previous state-of-the-art models on two benchmarks. Furthermore, when applied to a QA model, ELQ improves that model's end QA accuracy. Despite being originally designed with questions in mind, we believe ELQ could also generalize to longer, well-formed documents. 
