title
TORQUE: A Reading Comprehension Dataset of Temporal Ordering Questions

abstract
A critical part of reading is being able to understand the temporal relationships between events described in a passage of text, even when those relationships are not explicitly stated. However, current machine reading comprehension benchmarks have practically no questions that test temporal phenomena, so systems trained on these benchmarks have no capacity to answer questions such as "what happened before/after [some event]?" We introduce TORQUE, a new English reading comprehension benchmark built on 3.2k news snippets with 21k human-generated questions querying temporal relationships. Results show that RoBERTa-large achieves an exact-match score of 51% on the test set of TORQUE, about 30% behind human performance. 1

Introduction Time is important for understanding events and stories described in natural language text such as news articles, social media, financial reports, and electronic health records  (Verhagen et al., 2007 (Verhagen et al., , 2010 UzZaman et al., 2013; Minard et al., 2015; Bethard et al., 2016 Laparra et al., 2018) . For instance, "he won the championship yesterday" is different from "he will win the championship tomorrow": he may be celebrating if he has already won it, while if he has not, he is probably still preparing for the game tomorrow. The exact time of an event is often implicit in text. For instance, if we read that a woman is "expecting the birth of her first child", we know that the birth is in the future, while if she is "mourning the death of her mother", the death is in the past. These relationships between an event and a time point (e.g., "won the championship yesterday") or between two events (e.g., "expecting" is before 1 https://allennlp.org/torque.html Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the south-west of England. Rescuers searching for a woman trapped in a landslide at her home in Looe, Cornwall, said they had found a body. 

 Q1: What events have already finished? A: searching trapped landslide said found Q2: What events have begun but has not finished? A: snow causing disruption rainfall bringing flooding Q3: What will happen in the future? A: No answers. 

 Q4: What happened before a woman was trapped? A: landslide Q5: What had started before a woman was trapped? A: snow rainfall landslide Q6: What happened while a woman was trapped? A: searching Q7: What happened after a woman was trapped? A: searching said found "birth" and "mourning" is after "death") are called temporal relations  (Pustejovsky et al., 2003) . This work studies reading comprehension for temporal relations, i.e., given a piece of text, a computer needs to answer temporal relation questions (Fig.  1 ). Reading comprehension is a natural format for studying temporal phenomena, as the flexibility of natural language annotations allows for capturing relationships that were not possible in previous formalism-based works. However, temporal phenomena are studied very little in reading comprehension  (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018 Dua et al., 2019; Lin et al., 2019) , and existing systems are hence brittle when handling questions in TORQUE (Table  1 ). Reading comprehension for temporal relationships has the following challenges. First, reading  

 TIME We don't know if "snow" started before "rainfall" or if "disruption" started before "flooding." Disruption/flooding may last longer than the snow/rainfall. We know disruption/flooding started after snow/rainfall started, but we don't know if they started earlier than the landslide.   comprehension works rarely require event understanding. For the example in Fig.  1 , SQUAD  (Rajpurkar et al., 2016)  and most datasets largely only require an understanding of predicates and arguments, and would ask questions like "what was a woman trapped in?" But a temporal relation question would be "what started before a woman was trapped?" To answer it, the system needs to identify events (e.g., LANDSLIDE is an event and "body" is not), the time of these events (e.g., LANDSLIDE is a correct answer, while SAID is not because of the time when the two events happen), and look at the entire passage rather than the local predicate-argument structures within a sentence (e.g., SNOW and RAINFALL are correct answers to the question above). Second, there are many events in a typical passage of text, so temporal relation questions typically query more than one relationship at the same time. This means that a question can have multiple answers (e.g., "what happened after the landslide?"), or no answers, because the question may be beyond the time scope (e.g., "what happened before the snow started?"). Third, temporal relations queried by natural language questions are often sensitive to a few key words such as before, after, and start. Those questions can easily be changed to make contrasting questions with dramatically different answers. Models that are not sensitive to these small changes in question words will perform poorly on this task, as shown in Table  1 . In this paper, we present TORQUE, the first reading comprehension benchmark that targets these challenges. We trained crowd workers to label events in text, and to write and answer questions that query temporal relationships between these events. We also had workers write questions with contrasting changes to the temporal keywords, to give a comprehensive test of a machine's temporal reasoning ability and minimize the effect of any data collection artifacts  (Gardner et al., 2020) . We annotated 3.2k text snippets randomly selected from the TempEval3 dataset  (Uz-Zaman et al., 2013) . In total, TORQUE has 25k events and 21k user-generated and fully answered temporal relation questions. 20% of TORQUE was further validated by additional crowd workers to be used as test data. Results show that RoBERTalarge  achieves 51% in exactmatch on TORQUE after fine-tuning, about 30% behind human performance, indicating that more investigation is needed to better solve this problem. 

 Definitions 

 Events As temporal relations are relationships between events, we first define events. Generally speak- 

 Events in different modes The lion had a large meal and slept for 24 hours. [Negated] The lion didn't sleep after having a large meal. [Uncertain] The lion may have had a large meal before sleeping. [Hypothetical] If the lion has a large meal, it will sleep for 24 hours. [Repetitive] The lion used to sleep for 24 hours after having large meals. [Generic] After having a large meal, lions may sleep longer. ing, an event involves a predicate and its arguments  (ACE, 2005; Mitamura et al., 2015) . When studying time, events were defined as actions/states triggered by verbs, adjectives, and nominals  (Pustejovsky et al., 2003) . Later works on event and time have largely followed this definition, e.g., TempEval  (Verhagen et al., 2007) , TimeBank-Dense , RED  (O'Gorman et al., 2016) , and MATRES  (Ning et al., 2018b) . This work follows this line of event definition and uses event and event trigger interchangeably. We define an event to be either a verb or a noun (e.g., TRAPPED and LANDSLIDE in Fig.  1 ). Specifically, in copular constructions, we choose to label the verb as the event, instead of an adjective or preposition. This allows us to give a consistent treatment of "she was on the east coast yesterday" and "she was happy", which we can easily teach to crowd workers. Note that from the perspective of data collection, labeling the copula does not lose information as one can always do post-processing using dependency parsing or semantic role labeling to recover the connection between "was" and "happy." Note that events expressed in text are not always factual. They can be negated, uncertain, hypothetical, or have other associated modalities (see Fig.  3 ). Prior work dealing with events often tried to categorize and label these various aspects because they were crucial for determining temporal relations. Sometimes certain categories were even dropped due to annotation difficulties  (Pustejovsky et al., 2003; O'Gorman et al., 2016; Ning et al., 2018b) . In this work, we simply have people label all events, irrespective of their modality, and use natural language to describe relations between them, as discussed in Sec. 3. 

 Temporal Relations Temporal relations describe the relationship between two events with respect to time, or between one event and a fixed time point (e.g., yesterday).  2  We can use a triplet, (A, r, B), to represent this relationship, where A and B are events or time points, and r is a temporal relation. For example, the first sentence in Fig.  3  expresses a temporal relation (HAD, happened before, SLEPT). In previous works, every event is assumed to be associated with a time interval [t start , t end ]. When comparing two events, there are 13 possible relation labels (see Fig.  4 )  (Allen, 1984) . However, there are still many relations that cannot be expressed because the assumption that every event has a time interval is inaccurate: The time scope of an event may be fuzzy, an event can have a nonfactual modality, or events can be repetitive and invoke multiple intervals (see Fig.  5 ). To better handle these phenomena, we move away from the fixed set of relations used in prior work and instead use natural language to annotate the relationships between events, as described in the next section. 

 Natural Language Annotation of Temporal Relations Motivated by recent works  (He et al., 2015; Michael et al., 2017; Levy et al., 2017; Gardner et al., 2019b) , we propose using natural language question answering as an annotation for- 

 Confusing relations between the following events Fuzzy time scope: Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the southwest of England. "Follow" is negated: Colonel Collins didn't follow a normal progression anymore once she was picked as a NASA astronaut. 

 "Leaves" is a series of time intervals: The bus leaves at 10 am every day, so we will go to the bus stop at 9 am today. Figure  5 : It is confusing to label these relations using a fixed set of relations: they are not simply before or after, but they can be fuzzy, can have modalities as events, and/or need multiple time intervals to represent. mat for temporal relations. Recalling that we denote a temporal relation between two events as (A, r, B), we use (?, r, B) to denote a temporal relation question. We instantiate these temporal relation questions using natural language. For instance, (?, happened before, SLEPT) means "what happened before a lion slept?" We then expect as an answer the set of all events A in the passage such that (A, r, B) holds, assuming for any deictic expression A or B the time point when the passage was written, and assuming that the passage is true. 

 Fuzzy relations Heavy snow is causing disruption to transport across the UK, with heavy rainfall bringing flooding to the south-west of England. Q: What happens at about the same time as the disruption? A: flooding Q: What started after the snow started? A: disruption Figure  6 : Fuzzy relations that used to be difficult to represent using a predefined label set can be captured naturally in a reading comprehension task. 

 Advantages Studying temporal relations as a reading comprehension task gives us the flexibility to handle many of the aforementioned difficulties. First, fuzzy relations can be described by natural language questions (after all, the relations are expressed in natural language in the first place). In Fig.  6 , DIS-RUPTION and FLOODING happened at about the same time, but we do not know for sure which one is earlier, so we have to choose vague. Similarly for SNOW and DISRUPTION, we do not know which one ends earlier and have to choose vague again. In contrast, the question-answer (QA) pairs Questions that query events in different modes  [Negated]  What didn't the lion do after a large meal? [Uncertain] What might the lion do before sleeping? [Hypothetical] What will the lion do if it has a large meal? [Repetitive] What did the lion use to do after large meals? [Generic] What do lions do after a large meal? Figure  7 : Events in different modes can be distinguished using natural language questions. "Often before" vs "before" He used to take a walk after dinner. Q: What did he often do after dinner? A: walk Q: What did he do after dinner today? A: No answers.  in Fig.  6  can naturally capture these fuzzy relations. Second, natural language questions can conveniently incorporate different modes of events. Figure  7  shows how to accurately query the relation between "having a meal" and "sleeping" in different modes (original sentences can be found in Fig.  3 ). In contrast, if we could only choose one label, we must choose before for all these relations, although these relations are actually different. For instance, a repetitive event may be a series of intervals rather than a single one, and often before is very different from before (Fig.  8 ). Third, a major issue that prior works wanted to address was deciding when two events should have a relation  Mostafazadeh et al., 2016; O'Gorman et al., 2016; Ning et al., 2018b) . To avoid asking for relations that do not exist, prior works needed to explicitly annotate certain properties of events as a preprocessing step, but it still remains difficult to have a the-When should two events have a relation? Service industries showed solid job gains, an area expected to be hardest hit when the crisis hit the America economy. Some pairs have relations: (showed gains), (expected hit), (gains crisis), etc. Some don't: (showed hit), (gains hit) A passerby called the police to report the body, but the line was busy. Some pairs have relations: (called report), (called was) Some don't: (report was) ory explaining, for instance, why hit can compare to expected and crisis, but not to gains. Interestingly, when we annotate temporal relations in natural language, the annotator naturally avoids event pairs that do not have relations. For instance, for the sentences in Fig.  9 , one will not ask questions like "what happened after the service industries are hardest hit?" or "what happened after a passerby reported the body?" Instead, natural questions will be "what was expected to happen when the crisis hit America?" and "what was supposed to happen after a passerby called the police?" The format of natural language questions bypasses the need for explicit annotation of properties of events or other theories. While using QA as the format gives us many benefits in describing fuzzy relations and incorporating various temporal phenomena, we want to note that it may also lead to potential difficulties in transferring the knowledge to downstream tasks that are not in a QA format, and some special treatment in modeling may be needed (e.g.,  He et al. (2020) ). This paper focuses on constructing this QA dataset covering new phenomena, and the problem of successful transfer learning is beyond our scope here. 

 Penalize Shortcuts by Contrast Sets An important problem in building datasets is to avoid trivial solutions  (Gardner et al., 2019a) . As Fig.  10  shows, there are two events ATE and WENT in the text. Since ATE is already mentioned in the question, the answer of WENT seems a trivial option without the need to understand the underlying relationship. To address this issue, we create contrast questions which slightly modify the original questions, but dramatically change the an-  swers, so that shortcuts are penalized. Specifically, for an existing question (?, r, B) (e.g., "what happened after he ate his breakfast?"), one should keep using B and change r (e.g., "what happened before/shortly after/... he ate his breakfast?"), or modify it to ask about the start/end time (e.g., "what happened after he started eating his breakfast?" or "what would finish after he ate his breakfast?"). We also instructed workers to make sure that the answers to the new question are different from the original one to avoid trivial modifications (e.g., changing "what happened" to "what occurred"). 

 Data Collection We used Amazon Mechanical Turk to build TORQUE. Following prior work, we focus on passages that consist of two contiguous sentences, as this is sufficient to capture the vast majority of non-trivial temporal relations  (Ning et al., 2017) . We took all the articles used in the TempEval3 (TE3) workshop (2.8k articles)  (UzZaman et al., 2013)  and created a pool of 26k two-sentence passages. Given a random passage from this pool, the annotation process for crowd workers was: 1. Label all the events 2. Repeatedly do the following 3 (a) Ask a temporal relation question and point out all the answers from the list of events (b) Modify the temporal relation to create one or more new questions and answer them The annotation guidelines 4 and interface 5 are public. In the following sections, we further discuss issues of quality control and crowdsourcing cost. 

 Quality Control We used three quality control strategies: qualification, pilot, and validation. Qualification We designed a separate qualification task where crowd workers were trained and tested on 3 capabilities: labeling events, asking temporal relation questions, and questionanswering. They were tested on problems randomly selected from a pool we designed. Crowd workers were considered level-1 qualified if they could pass the test within 3 attempts. In practice, about 1 out of 3 workers passed our qualification. Pilot We then asked level-1 crowd workers to do a small amount of the real task. We manually checked the annotations and gave feedback to them. Those who passed this inspection were called level-2 workers, and only they could work on the large-scale real task. Roughly 1 out of 3 pilot submissions received a level-2 qualification. In the end, there were 63 level-2 annotators, and 60 of them actually worked on our large-scale task. Validation We randomly selected 20% of the articles from TORQUE for further validation. We first validated the events by 4 different level-2 annotators (with the original annotator, there were in total 5 different humans). We also intentionally added noise to the original event list so that the validators must carefully identify wrong events. The final event list was determined by aggregating all 5 humans using majority vote. Second, we validated the answers in the same portion of the data. Two level-2 workers were asked to verify the initial annotator's answers; we again added noise to the answer list as a quality control for the validators. Instead of using majority vote as we did for events, the final answers from all workers are considered correct. We did not do additional validation for the questions themselves, as a manual inspection found the quality to be very high already, with no bad questions in a random sample of 100. 

 Cost In each job of the main task, we presented 3 passages. The crowd worker could decide to use some or all of them. For each passage a worker decided to use, they needed to label the events, answer 3 hard-coded warm-up questions, and then ask and answer at least 12 questions (including contrast questions). The final reward is a base pay of $6 plus $0.5 for each extra question. Crowd workers thus had the incentive to (1) use fewer passages so that they can do event labeling and warm-up questions fewer times, (2) modify questions instead of asking from scratch, and (3) ask extra questions in each job. All these incentives were for more coverage of the temporal phenomena in each passage. In practice, crowd workers on average used 2 passages in each job. Validating the events in each passage and the answers to a specific question both cost $0.1. In total, TORQUE cost $15k for an average of $0.70/question. 

 TORQUE Statistics TORQUE has 3.2k passage annotations (?50 tokens/passage), 6 24.9k events (7.9 events/passage), and 21.2k user-provided questions (half of them were labeled by crowd workers as modifications of existing ones). Every passage comes with 3 hardcoded warm-up questions asking which events in the passage had already happened, were ongoing, or were still in the future. Table  3  shows some basic statistics of TORQUE. Note the 3 warm-up questions form a contrast set, and we treat the first as "original" and the others "modified." In a random sample of 200 questions in the test set of TORQUE, we found 94 questions querying about relations that cannot be directly represented by the previous single-interval-based labels. Table 2 gives example questions capturing these phenomena. More analysis of the event, answer, and workload distributions are in Appendix A-D. 

 Quality To validate the event annotations, we took the events provided by the initial annotator, added noise, and asked different workers to validate. We also trained an auxiliary event detection model using RoBERTa-large and added its predictions as event candidates. This tells us about the quality of events in TORQUE in two ways. First, the Worker Agreement with Aggregate (WAWA) F 1 here is 94.2%; that is, compare the majority-vote with all annotators, and perform micro-average on all instances. Second, if an event candidate is labeled by both the initial annotator and the model, then almost all of them (99.4%) are kept by the validators; if neither the initial annotator nor the model labeled a candidate, the candidate is almost surely removed (0.8%). As validators did not know which ones were noise or not before-  hand, this indicates that the validators could identify noise terms reliably. Similarly, the WAWA F 1 of the answer annotations is 84.7%, slightly lower than that for events, which is expected because temporal relation QA is intuitively harder. Results show that 12.3% of the randomly added answer candidates were labeled as correct answers by the validators. We manually inspected 100 questions and found 11.6% of the added noise terms were correct answers (very close to 12.3%), indicating that the validators were actually doing a good job in answer validation. More details of the metrics and the quality of annotations can be found in Appendix E. 

 Experiment We split TORQUE into train (80% of all the questions), dev (5%), and test (15%) and these three parts do not have the same articles. To solve TORQUE in an end-to-end fashion, the model here takes as input a passage and a question, then looks at every token in the passage and makes a binary classification of whether this token is an answer to the question or not. Specifically, the model has a one-layered perceptron on top of BERT  (Devlin et al., 2019)  or RoBERTa , and the input to the perceptron layer is the transformers' output corresponding to the token we're looking at. We fine-tuned BERT/RoBERTa (both "base" and "large") on the training set of TORQUE. We fixed batch size = 6 (each instance is a tuple of one passage, one question, and all its answers) with gradient accumulation step = 2 in all experiments. We selected the learning rate (from (1e ?5 , 2e ?5 )), the training epoch (within 10), and the random seed (from 3 arbitrary ones) based on performance on the dev set of TORQUE.  7  To compute an estimate of human performance, one author answered 100 questions from the test set and compared with crowd workers' annotations. Both the human performance and system performances are shown in Table  4 . We report the standard macro F 1 and exact-match (EM) metrics in question answering, and also EM consistency, the percentage of contrast question sets for which a model's predictions match exactly to all questions in a group  (Gardner et al., 2020) . We see warm-up questions are easier than user-provided ones because warm-up questions focus on easier phenomena of past/ongoing/future events. In addition, RoBERTa-large is expectedly the best system, but still far behind human performance, trailing by about 30% in EM. We further downsampled the training data to test the performance of RoBERTa. We find that with 10% of the original training data, RoBERTa fails to learn anything meaningful and simply pre- Table 4: Human/system performance on the test set of TORQUE. System performance is averaged from 3 runs; all std. dev. were ? 4% and those in [1%, 4%] are underlined. C (consistency) is the percentage of contrast groups for which a model's predictions have F 1 ? 80% for all questions in a group  (Gardner et al., 2020) . dicts "not an answer" for all tokens. With 50% of the training data, RoBERTa is slightly lower than but already comparable to that of using the entire training set. This means that the learning curve on TORQUE is already flat and the current size of TORQUE may not be the bottleneck for its low performance. Our data and code are public to facilitate more investigations into TORQUE. 8  

 Related Work The study of time is to understand when, how long, and how often things happen. While how long and how often usually require temporal common sense knowledge  (Vempala et al., 2018; Zhou et al., 2019 Zhou et al., , 2020 , the problem of when often boils down to extracting temporal relations. Modeling. Research on temporal relations often focuses on algorithmic improvement, such as structured inference  (Do et al., 2012; Ning et al., 2018a) , structured learning  (Leeuwenberg and Moens, 2017; Ning et al., 2017) , and neural networks  Tourille et al., 2017; Cheng and Miyao, 2017; Meng and Rumshisky, 2018; Leeuwenberg and Moens, 2018; . Formalisms. The approach that prior works took to handle the aforementioned temporal phenemona was to define formalisms such as the different modes of events (Fig.  3 ), a predefined label set (Fig.  4 ), different time axes for events  (Ning et al., 2018b) , and specific rules to follow when there is confusion. For example,  Bethard et al. (2007) ;  Ning et al. (2018b)  focused on a limited set of temporal phenomena and achieved high inter-annotator agreements (IAA), while  Styler IV et al. (2014) ;  O'Gorman et al. (2016)  aimed at covering more phenomena but suffered from low IAAs even between NLP researchers. QA as annotation. A natural choice is then to cast temporal relation understanding as a machine reading comprehension (MRC) problem. TORQUE is motivated by the philosophy in QA-SRL  (He et al., 2015)  and QAMR  (Michael et al., 2017) , where QA pairs were used as representations for predicate-argument structures. In zeroshot relation extraction (RE), they reduced relation slot filling to an MRC problem so as to build very large distant training data and improve zero-shot learning performance  (Levy et al., 2017) . However, our work differs from zero-shot RE since it centers around entities, while TORQUE is about events; the way to ask and answer questions, and the way to design a corresponding crowdsourcing pipeline, are thus significantly different between us. The QA-TempEval workshop  (Llorens et al., 2015) , desipte its name, is actually not studying temporal relations in an RC setting. The differences between TORQUE and QA-TempEval are as follows. First, QA TempEval is an evaluation approach for systems that generate TimeML annotations and actually is not a QA task. For instance, QA TempEval is to evaluate whether a system can answer questions like "IS <ENTITY 1> <RELATION> <ENTITY 2>?", where one clearly knows which event that <ENTITY> is referring to and where RELATION is selected from a predefined label set. Second, QA-TempEval's annotation relies on the existence of a TimeML corpus. From the perspective of data collection for studying a particular phenomenon, TORQUE has done more on defining the task and developing a scalable crowdsourcing pipeline. As a result, TORQUE is also much larger than QA-TempEval and the annotation pipeline of TORQUE can be easily adopted to collect even more data. 

 Conclusion Understanding temporal ordering of events is critical in reading comprehension, but existing works have studied very little about it. This paper presents TORQUE, a new English machine reading comprehension (MRC) dataset of temporal ordering questions. TORQUE has 3.2k news snippets, 9.5k hard-coded questions asking which events had happened, were ongoing, or were still in the future, and 21.2k human-generated questions querying more complex phenomena. We argue that an MRC setting allows for more convenient representation of these temporal phenomena than conventional formalisms. Results show that even a state-of-the-art language model, RoBERTa-large, falls behind human performance by a large margin, necessitating more investigation on improving MRC on temporal relationships in the future. Figure  13  further shows the 50 most common events in TORQUE. Unsurprisingly, the most comevents are reporting verbs (e.g., "say", "tell", "report", and "announce") and copular verbs. Other common events such as "meeting", "killed", "visit", and "war" are also expected given that the passages of TORQUE were taken from news articles. 
