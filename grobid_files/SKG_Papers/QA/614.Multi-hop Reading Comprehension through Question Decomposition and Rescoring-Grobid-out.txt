title
Multi-hop Reading Comprehension through Question Decomposition and Rescoring

abstract
Multi-hop Reading Comprehension (RC) requires reasoning and aggregation across several paragraphs. We propose a system for multi-hop RC that decomposes a compositional question into simpler sub-questions that can be answered by off-the-shelf single-hop RC models. Since annotations for such decomposition are expensive, we recast subquestion generation as a span prediction problem and show that our method, trained using only 400 labeled examples, generates sub-questions that are as effective as humanauthored sub-questions. We also introduce a new global rescoring approach that considers each decomposition (i.e. the sub-questions and their answers) to select the best final answer, greatly improving overall performance. Our experiments on HOTPOTQA show that this approach achieves the state-of-the-art results, while providing explainable evidence for its decision making in the form of sub-questions.

Introduction Multi-hop reading comprehension (RC) is challenging because it requires the aggregation of evidence across several paragraphs to answer a question. Table  1  shows an example of multi-hop RC, where the question "Which team does the player named 2015 Diamond Head Classics MVP play for?" requires first finding the player who won MVP from one paragraph, and then finding the team that player plays for from another paragraph. In this paper, we propose DECOMPRC, a system for multi-hop RC, that learns to break compositional multi-hop questions into simpler, singlehop sub-questions using spans from the original question. For example, for the question in Table 1, we can create the sub-questions "Which player named 2015 Diamond Head Classics MVP?" and "Which team does ANS play for?", Q Which team does the player named 2015 Diamond Head Classics MVP play for? P1 The 2015 Diamond Head Classic was ... Buddy Hield was named the tournament's MVP. P2 Chavano Rainier Buddy Hield is a Bahamian professional basketball player for the Sacramento Kings ... Q1 Which player named 2015 Diamond Head Classics MVP? Q2 Which team does ANS play for? Table  1 : An example of multi-hop question from HOT-POTQA. The first cell shows given question and two of given paragraphs (other eight paragraphs are not shown), where the red text is the groundtruth answer. Our system selects a span over the question and writes two sub-questions shown in the second cell. where the token ANS is replaced by the answer to the first sub-question. The final answer is then the answer to the second sub-question. Recent work on question decomposition relies on distant supervision data created on top of underlying relational logical forms  (Talmor and Berant, 2018) , making it difficult to generalize to diverse natural language questions such as those on HOTPOTQA . In contrast, our method presents a new approach which simplifies the process as a span prediction, thus requiring only 400 decomposition examples to train a competitive decomposition neural model. Furthermore, we propose a rescoring approach which obtains answers from different possible decompositions and rescores each decomposition with the answer to decide on the final answer, rather than deciding on the decomposition in the beginning. Our experiments show that DECOMPRC outperforms other published methods on HOT-POTQA , while providing explainable evidence in the form of sub-questions. In addition, we evaluate with alternative distrator paragraphs and questions and show that our decomposition-based approach is more ro-bust than an end-to-end BERT baseline  (Devlin et al., 2019) . Finally, our ablation studies show that our sub-questions, with 400 supervised examples of decompositions, are as effective as humanwritten sub-questions, and that our answer-aware rescoring method significantly improves the performance. Our code and interactive demo are publicly available at https://github.com/ shmsw25/DecompRC. 

 Related Work Reading Comprehension. In reading comprehension, a system reads a document and answers questions regarding the content of the document  (Richardson et al., 2013) . Recently, the availability of large-scale reading comprehensiondatasets  (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017)  has led to the development of advanced RC models  (Seo et al., 2017; Yu et al., 2018; Devlin et al., 2019) . Most of the questions on these datasets can be answered in a single sentence  (Min et al., 2018) , which is a key difference from multi-hop reading comprehension. Multi-hop Reading Comprehension. In multihop reading comprehension, the evidence for answering the question is scattered across multiple paragraphs. Some multi-hop datasets contain questions that are, or are based on relational queries  (Welbl et al., 2017; Talmor and Berant, 2018) . In contrast, HOTPOTQA , on which we evaluate our method, contains more natural, hand-written questions that are not based on relational queries. Prior methods on multi-hop reading comprehension focus on answering relational queries, and emphasize attention models that reason over coreference chains  (Dhingra et al., 2018; Zhong et al., 2019; Cao et al., 2019) . In contrast, our method focuses on answering natural language questions via question decomposition. By providing decomposed single-hop sub-questions, our method allows the model's decisions to be explainable. Our work is most related to  Talmor and Berant (2018) , which answers questions over web snippets via decomposition. There are three key differences between our method and theirs. First, they decompose questions that are correspond to relational queries, whereas we focus on natural language questions. Next, they rely on an underly-ing relational query (SPARQL) to build distant supervision data for training their model, while our method requires only 400 decomposition examples. Finally, they decide on a decomposition operation exclusively based on the question. In contrast, we decompose the question in multiple ways, obtain answers, and determine the best decomposition based on all given context, which we show is crucial to improving performance. Semantic Parsing. Semantic parsing is a larger area of work that involves producing logical forms from natural language utterances, which are then usually executed over structured knowledge graphs  (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011) . Our work is inspired by the idea of compositionality from semantic parsing, however, we focus on answering natural language questions over unstructured text documents. 

 Model 

 Overview In multi-hop reading comprehension, a system answers a question over a collection of paragraphs by combining evidence from multiple paragraphs. In contrast to single-hop reading comprehension, in which a system can obtain good performance using a single sentence  (Min et al., 2018) , multi-hop reading comprehension typically requires more complex reasoning over how two pieces of evidence relate to each other. We propose DECOMPRC for multi-hop reading comprehension via question decomposition. DE-COMPRC answers questions through a three step process: 1. First, DECOMPRC decomposes the original, multi-hop question into several single-hop sub-questions according to a few reasoning types in parallel, based on span predictions. Figure  1  illustrates an example in which a question is decomposed through four different reasoning types. Section 3.2 details our decomposition approach. 2. Then, for every reasoning types DECOMPRC leverages a single-hop reading comprehension model to answer each sub-question, and combines the answers according to the reasoning type. Figure  1  shows an example for which bridging produces 'City of New and produces the answer (Section 3.3). Lastly, the decomposition scorer decides which answer will be the final answer (Section 3.4). Here, "City of New York", obtained by bridging, is determined as a final answer.  York' as an answer while intersection produces 'Columbia University' as an answer. Section 3.3 details the single-hop reading comprehension procedure. 3. Finally, DECOMPRC leverages a decomposition scorer to judge which decomposition is the most suitable, and outputs the answer from that decomposition as the final answer. In Figure  1 , "City of New York", obtained via bridging, is decided as the final answer. Section 3.4 details our rescoring step. We identify several reasoning types in multi-hop reading comprehension, which we use to decompose the original question and rescore the decompositions. These reasoning types are bridging, intersection and comparison. Table  2  shows examples of each reasoning type. On a sample of 200 questions from the dev set of HOTPOTQA, we find that 92% of multi-hop questions belong to one of these types. Specifically, among 184 sam-ples out of 200 which require multi-hop reasoning, 47% are bridging questions, 23% are intersection questions, 22% are comparison questions, and 8% do not belong to one of three types. In addition, these multi-hop reasoning types correspond to the types of compositional questions identified by Berant et al. (  2013 ) and  Talmor and Berant (2018) . 

 Decomposition The goal of question decomposition is to convert a multi-hop question into simpler, single-hop subquestions. A key challenge of decomposition is that it is difficult to obtain annotations for how to decompose questions. Moreover, generating the question word-by-word is known to be a difficult task that requires substantial training data and is not straight-forward to evaluate  (Gatt and Krahmer, 2018; Novikova et al., 2017) . Instead, we propose a method to create subquestions using span prediction over the question. The key idea is that, in practice, each sub-question can be formed by copying and lightly editing a key span from the original question, with different span extraction and editing required for each reasoning type. For instance, the bridging question in Table  2  requires finding "the player named 2015 Diamond Head Classic MVP" which is easily extracted as a span. Similarly, the intersection question in Table  2  specifies the type of entity to find ("which actor and comedian"), with two conditions ("Stories USA starred" and "from "The Office""), all of which can be extracted. Comparison questions compare two entities using a discrete operation over some properties of the entities, e.g., "which is smaller". When two entities are extracted as spans, the question can be converted into two sub-questions and one discrete operation over the answers of the sub-questions. 

 Span Prediction for Sub-question Generation Our approach simplifies the sub-question generation problem into a span prediction problem that requires little supervision (400 annotations). The annotations are collected by mapping the question into several points that segment the question into spans (details in Section 4.2). We train a model Pointer c that learns to map a question into c points, which are subsequently used to compose sub-questions for each reasoning type through Algorithm 1. Pointer c is a function that points to c indices ind 1 , . . . , ind c in an input sequence. 1 Let S = [s 1 , . . . , s n ] denote a sequence of n words in the input sequence. The model encodes S using BERT  (Devlin et al., 2019) : U = BERT(S) ? R n?h , ( 1 ) where h is the output dimension of the encoder. Let W ? R h?c denote a trainable parameter matrix. We compute a pointer score matrix Y = softmax(U W ) ? R n?c , (2) where P(i = ind j ) = Y ij denotes the probability that the ith word is the jth index produced by the pointer. The model extracts c indices that yield the highest joint probability at inference: ind 1 , . . . , ind c = argmax i 1 ?ic c j=1 P(i j = ind j ) 1 c is a hyperparameter which differs in different reasoning types. 2 Details for find op, form subq in Appendix B. Algorithm 1 Sub-questions generation using Pointer c . 2 procedure GENERATESUBQ(Q : question, Pointerc) /* Find q b 1 and q b 2 for Bridging */ ind1, ind2, ind3 ? Pointer3(Q) q b 1 ? Q ind 1 :ind 3 q b 2 ? Q :ind 1 : ANS : Q ind 3 : article in Q ind 2 ?5:ind 2 ? 'which' /* Find q i 1 and q i 2 for Intersecion */ ind1, ind2 ? Pointer2(Q) s1, s2, s3 ? Q :ind 1 , Q ind 1 :ind 2 , Q ind 2 : if s2 starts with wh-word then q i 1 ? s1 : s2, q i 2 ? s2 : s3 else q i 1 ? s1 : s2, q i 2 ? s1 : s3 /* Find q c 1 , q c 2 and q c 3 for Comparison */ ind1, ind2, ind3, ind4 ?Pointer4(Q) ent1, ent2 ? Q ind 1 :ind 2 , Q ind3:ind 4 op ? find op(Q, ent1, ent2) q c 1 , q c 2 ? form subq(Q, ent1, ent2, op) q c 3 ? op (ent1, ANS) (ent2, ANS) 

 Single-hop Reading Comprehension Given a decomposition, we use a single-hop RC model to answer each sub-question. Specifically, the goal is to obtain the answer and the evidence, given the sub-question and N paragraphs. Here, the answer is a span from one of paragraphs, yes or no. The evidence is one of N paragraphs on which the answer is based. Any off-the-shelf RC model can be used. In this work, we use the BERT reading comprehension model  (Devlin et al., 2019)  combined with the paragraph selection approach from  Clark and Gardner (2018)  to handle multiple paragraphs. Given N paragraphs S 1 , . . . , S N , this approach independently computes answer i and y none i from each paragraph S i , where answer i and y none i denote the answer candidate from ith paragraph and the score indicating ith paragraph does not contain the answer. The final answer is selected from the paragraph with the lowest y none i . Although this approach takes a set of multiple paragraphs as an input, it is not capable of jointly reasoning across different paragraphs. For each paragraph S i , let U i ? R n?h be the BERT encoding of the sub-question concatenated with a paragraph S i , obtained by Equation 1. We compute four scores, y span i y yes i , y no i and y none i , indicating if the answer is a phrase in the paragraph, yes, no, or does not exist. [y span i ; y yes i ; y no i ; y none i ] = max(U i )W 1 ? R 4 , where max denotes a max-pooling operation across the input sequence, and W 1 ? R h?4 de-notes a parameter matrix. Additionally, the model computes span i , which is defined by its start and end points start i and end i . start i , end i = argmax j?k P i,start (j)P i,end (k), where P i,start (j) and P i,end (k) indicate the probability that the jth word is the start and the kth word is the end of the answer span, respectively. P i,start (j) and P i,end (k) are obtained by the jth element of p start i and the kth element of p end i from p start i = softmax(U i W start ) ? R n (3) p end i = softmax(U i W end ) ? R n (4) Here, W start , W end ? R h are the parameter matrices. Finally, answer i is determined as one of span i , yes or no based on which of y span i , y yes i and y no i is the highest. The model is trained using questions that only require single-hop reasoning, obtained from SQUAD  (Rajpurkar et al., 2016)  and easy examples of HOTPOTQA  (details in Section 4.2). Once trained, it is used as an offthe-shelf RC model and is never directly trained on multi-hop questions. 

 Decomposition Scorer Each decomposition consists of sub-questions, their answers, and evidence corresponding to a reasoning type. DECOMPRC scores decompositions and takes the answer of the top-scoring decomposition to be the final answer. The score indicates if a decomposition leads to a correct final answer to the multi-hop question. Let t be the reasoning type, and let answer t and evidence t be the answer and the evidence from the reasoning type t. Let x denote a sequence of n words formed by the concatenation of the question, the reasoning type t, the answer answer t , and the evidence evidence t . The decomposition scorer encodes this input x using BERT to obtain U t ? R n?h similar to Equation (1). The score p t is computed as p t = sigmoid(W T 2 max(U t )) ? R, where W 2 ? R h is a trainable matrix. During inference, the reasoning type is decided as argmax t p t . The answer corresponding to this reasoning type is chosen as the final answer. Pipeline Approach. An alternative to the decomposition scorer is a pipeline approach, in which the reasoning type is determined in the beginning, before decomposing the question and obtaining the answers to sub-questions. Section 4.6 compares our scoring step with this approach to show the effectiveness of the decomposition scorer. Here, we briefly describe the model used for the pipeline approach. First, we form a sequence S of n words from the question and obtain S ? R n?h from Equation  1 . Then, we compute 4-dimensional vector p t by: p t = softmax(W 3 max( S)) ? R 4 where W 3 ? R h?4 is a parameter matrix. Each element of 4-dimensional vector p t indicates the reasoning type is bridging, intersection, comparison or original. 

 Experiments 

 HOTPOTQA We experiment on HOTPOTQA , a recently introduced multi-hop RC dataset over Wikipedia articles. There are two types of questions-bridge and comparison. Note that their categorization is based on the data collection and is different from our categorization (bridging, intersection and comparison) which is based on the required reasoning type. We evaluate our model on dev and test sets in two different settings, following prior work.  on TF-IDF similarity between the question and the paragraph, using Document Retriever from DrQA  (Chen et al., 2017) . We train 3 instances with n = 0, 2, 4 for an ensemble, which we use as the single-hop model. To deal with ungrammatical questions generated through our decomposition procedure, we augment the training data with ungrammatical samples. Specifically, we add noise in the question by randomly dropping tokens with probability of 5%, and replace wh -word into 'the' with probability of 5%. Training Decomposition Scorer We create training data by making inferences for all reasoning types on HOTPOTQA medium and hard examples. We take the reasoning type that yields the correct answer as the gold reasoning type. Appendix C provides the full details. 

 Baseline Models We compare our system DECOMPRC with the state-of-the-art on the HOTPOTQA dataset as well as strong baselines. BiDAF is the state-of-the-art RC model on HOT-POTQA, originally from  Seo et al. (2017)  and implemented by . BERT is a large, language-model-pretrained model, achieving the state-of-the-art results across many different NLP tasks  (Devlin et al., 2019) . This model is the same as our single-hop model described in Section 3. is no access to the groundtruth answers of multihop questions, a decomposition scorer cannot be trained. Therefore, a final answer is obtained based on the confidence score from the single-hop model, without a rescoring procedure. 

 Results Table  3  compares the results of DECOMPRC with other baselines on the HOTPOTQA development set. We observe that DECOMPRC outperforms all baselines in both distractor and full wiki settings, outperforming the previous published result by a large margin. An interesting observation is that DECOMPRC not trained on multi-hop QA pairs (DECOMPRC-1hop train) shows reasonable performance across all data splits. We also observe that BERT trained on singlehop RC achieves a high F1 score, even though it does not draw inferences across different paragraphs. For further analysis, we split the HOT-POTQA development set into single-hop solvable (Single) and single-hop non-solvable (Multi).  4  We observe that DECOMPRC outperforms BERT by a large margin in single-hop non-solvable (Multi) examples. This supports our attempt toward more explainable methods for answering multihop questions. Finally, Table  4  shows the F1 score on the test set for distractor setting and full wiki setting on the leaderboard. 5 These include unpublished models that are concurrent to our work. DECOMPRC achieves the best result out of models that report both distractor and full wiki setting. 

 Evaluating Robustness In order to evaluate the robustness of different methods to changes in the data distribution, we set up two adversarial settings in which the trained model remains the same but the evaluation dataset is different. Modifying Distractor Paragraphs. We collect a new set of distractor paragraphs to evaluate if the models are robust to the change in distractors.  6  In particular, we follow the same strategy as the original approach  using TF-IDF similarity between the question and the paragraph, but with no overlapping distractor paragraph with the original distractor paragraphs. Table  5  compares the F1 score of DECOMPRC and BERT in the original distractor setting and in the modified distractor setting. As expected, the performance of both methods degrade, but DE-COMPRC is more robust to the change in distractors. Namely, DECOMPRC-1hop train degrades much less (only 3.41 F1) compared to other approaches because it is only trained on single-hop data and therefore does not exploit the data distribution. These results confirm our hypothesis that the end-to-end model is sensitive to the change of the data and our model is more robust. Adversarial Comparison Questions. We create an adversarial set of comparison questions by altering the original question so that the correct answer is inverted. For example, we change "Who was born earlier, Emma Bull or Virginia Woolf?" to "Who was born later, Emma Bull or Virginia Woolf?" We automatically invert 665 questions (details in Appendix D). We report the joint F1, taken as the minimum of the prediction F1 on the original and the inverted examples.  the joint F1 score of DECOMPRC and BERT. We find that DECOMPRC is robust to inverted questions, and outperforms BERT by 36.53 F1. 

 Ablations Span-based vs. Free-form sub-questions. We evaluate the quality of generated sub-questions using span-based question decomposition. We replace the question decomposition component using Pointer 3 with (i) sub-question decomposition through groundtruth spans, (ii) sub-question decomposition with free-form, hand-written subquestions (examples shown in Table  6 ). Table  7  (left) compares the question answering performance of DECOMPRC when replaced with alternative sub-questions on a sample of 50 bridging questions.  7  There is little difference in model performance between span-based and sub-questions written by human. This indicates that our span-based sub-questions are as effective as free-form sub-questions. In addition, Pointer 3 trained on 200 or 400 examples obtains close to human performance. We think that identifying spans often rely on syntactic information of the question, which BERT has likely learned from language modeling. We use the model trained on 200 examples for DECOMPRC to demonstrate sample-efficiency, and expect performance improvement with more annotations. Ablations in decomposition decision method. For comparison, we report the F1 score of the confidence-based method which chooses the decomposition with the maximum confidence score from the single-hop RC model, and the pipeline approach which independently selects the reasoning type as described in Section 3.4. In addition, we report an oracle which takes the maximum F1 score across different reasoning types to provide an upperbound. A pipeline method gets lower F1 score than the decomposition scorer. This suggests that using more context from decomposition (e.g., the answer and the evidence) helps avoid cascading errors from the pipeline. Moreover, a gap between DECOMPRC and oracle (6.2 F1) indicates that there is still room to improve. Upperbound of Span-based Sub-questions without a decomposition scorer. To measure an upperbound of span-based sub-questions without a decomposition scorer, where a human-level RC model is assumed, we conduct a human experiment on a sample of 50 bridging ques-Q What country is the Selun located in? P1 Selun lies between the valley of Toggenburg and Lake Walenstadt in the canton of St. Gallen. P2 The canton of St. Gallen is a canton of Switzerland. Q Which pizza chain has locations in more cities, Round    8  reports the breakdown of fifteen error cases. 53% of such cases are due to the incorrect groundtruth, partial match with the groundtruth or mistake from humans. 47% are genuine failures in the decomposition. For example, a multi-hop question "Which animal races annually for a national title as part of a post-season NCAA Division I Football Bowl Subdivision college football game?" corresponds to the last category in Table  8 . The question can be decomposed into "Which post-season NCAA Division I Football Bowl Subdivision college football game?" and "Which animal races annually for a national title as part of ANS?". However in the given set of paragraphs, there are multiple games that can be the answer to the first sub-question. Although only one of them is held with the animal racing, it is impossible to get the correct answer only given the first subquestion. We think that incorporating the original question along with the sub-questions can be one solution to address this problem, which is partially done by a decomposition scorer in DECOMPRC. Limitations. We show the overall limitations of DECOMPRC in Table  9 . First, some questions are not compositional but require implicit multihop reasoning, hence cannot be decomposed. Sec-8 A full set of samples is shown in Appendix E. ond, there are questions that can be decomposed but the answer for each sub-question does not exist explicitly in the text, and must instead by inferred with commonsense reasoning. Lastly, the required reasoning is sometimes beyond our reasoning types (e.g. counting or calculation). Addressing these remaining problems is a promising area for future work. 

 Conclusion We proposed DECOMPRC, a system for multihop RC that decomposes a multi-hop question into simpler, single-hop sub-questions. We recasted sub-question generation as a span prediction problem, allowing the model to be trained on 400 labeled examples to generate high quality sub-questions. Moreover, DECOMPRC achieved further gains from the decomposition scoring step. DECOMPRC achieved the state-of-the-art on HOTPOTQA distractor setting and full wiki setting, while providing explainable evidence for its decision making in the form of sub-questions and being more robust to adversarial settings than strong baselines. Table  10 : A set of discrete operations proposed for comparison questions, along with the example on each type. ANS is the answer of each query, and ENT is the entity corresponding to each query. The answer of each query is shown in the right side of ?. If the question and two entities for comparison are given, queries and a discrete operation can be obtained by heuristics. 
