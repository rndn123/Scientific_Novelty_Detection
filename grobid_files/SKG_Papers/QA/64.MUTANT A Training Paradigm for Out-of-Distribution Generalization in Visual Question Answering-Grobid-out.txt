title
MUTANT: A Training Paradigm for Out-of-Distribution Generalization in Visual Question Answering

abstract
While progress has been made on the visual question answering leaderboards, models often utilize spurious correlations and priors in datasets under the i.i.d. setting. As such, evaluation on out-of-distribution (OOD) test samples has emerged as a proxy for generalization. In this paper, we present MUTANT, a training paradigm that exposes the model to perceptually similar, yet semantically distinct mutations of the input, to improve OOD generalization, such as the VQA-CP challenge. Under this paradigm, models utilize a consistency-constrained training objective to understand the effect of semantic changes in input (question-image pair) on the output (answer). Unlike existing methods on VQA-CP, MUTANT does not rely on the knowledge about the nature of train and test answer distributions. MUTANT establishes a new state-ofthe-art accuracy on VQA-CP with a 10.57% improvement. Our work opens up avenues for the use of semantic input mutations for OOD generalization in question answering.

Introduction Availability of large-scale datasets has enabled the use of statistical machine learning in vision and language understanding, and has lead to significant advances. However, the commonly used evaluation criterion is the performance of models on test-samples drawn from the same distribution as the training dataset, which cannot be a measure of generalization. Training under this "independent and identically distributed" (i.i.d.) setting can drive decision making to be highly influenced by dataset biases and spurious correlations as shown in both natural language inference  (Kaushik and Lipton, 2018; Poliak et al., 2018; McCoy et al., 2019)  and visual question answering  (Goyal et   2017;  Agrawal et al., 2018a; Selvaraju et al., 2020) . As such, evaluation on out-of-distribution (OOD) samples has emerged as a metric for generalization. Visual question answering (VQA)  (Antol et al., 2015)  is a task at the crucial intersection of vision and language. The aim of VQA models is to provide an answer, given an input image and a question about it. Large datasets  (Antol et al., 2015)  have been extensively used for developing VQA models. However over-reliance on datasets can cause models to learn spurious correlations such as linguistic priors  (Agrawal et al., 2018a)  that are specific to certain datasets and do not generalize to "Out-of-Distribution" (OOD) samples, as shown in Figure  1 . While learning patterns in the data is important, learning dataset-specific spurious correlations is not a feature of robust VQA models. Developing robust models has thus become a key pursuit for recent work in visual question answer-ing through data augmentation  (Goyal et al., 2017) , reorganization  (Agrawal et al., 2018a) . Every dataset contains biases; indeed inductive bias is necessary for machine learning algorithms to work.  Mitchell (1980)  states that an unbiased learner's ability to classify is no better than a lookup from memory. However this bias has a component which is useful for generalization (positive bias), and a component due to spurious correlations (negative bias). We use the term "positive bias" to denote the correlations that are necessary to perform a task -for instance, the answer to a "What sport is . . . " question is correlated to a name of a sport. The term "negative bias" is used for spurious correlations tat may be learned from the data -for instance, always predicting "tennis" as the answer to "What sport. . . " questions. The goal of OOD generalization is to mitigate negative bias while learning to perform the task. However existing methods such as LMH  (Clark et al., 2019)  try to remove all biases between question-answer pairs, by penalizing examples that can be answered without looking at the image; we believe this to be counterproductive. The analogy of antibiotics which are designed to remove pathogen bacteria, but also end up removing useful gut microbiome  (Willing et al., 2011)  is useful to understand this phenomenon. We present a method that focuses on increasing positive bias and mitigating negative bias, to address the problem of OOD generalization in visual question answering. Our approach is to enable the mutation of inputs (questions and images) in order to expose the VQA model to perceptually similar yet semantically dissimilar samples. The intuition is to implicitly allow the model to understand the critical changes in the input which lead to a change in the answer. This concept of mutations is illustrated in Figure  1 . If the color of the frisbee is changed, or the child removed, i.e. when an image-mutation is performed, the answer to the question changes. Similarly, if a word is substituted by an adversarial word (bins?bottles), an antonym, or negation (healthy?not healthy), i.e. when a question-mutation is performed, the answer also changes. Notice that both mutations do not significantly change the input, most of the pixels in the image and words in the question are unchanged, and the type of reasoning required to answer the question is unchanged. However the mutation significantly changes the answer. In this work, we use this concept of mutations to enable models to focus on parts of the input that are critical to the answering process, by training our models to produce answers that are consistent with such mutations. We present a question-type exposure framework which teaches the model that although such linguistic priors may exist in training data (such as the dominant answer "tennis" to "What sport is ..." questions), other sports can also be answers to such questions, thus mitigating negative bias. This is in contrast to  Chen et al. (2020a)  who focus on using data augmentation as a means for mitigating language bias. Our method uses a pair-wise training protocol to ensure consistency between answer predictions for the original sample and the mutant sample. Our model includes a projection layer, which projects cross-modal features and true answers to a learned manifold and uses Noise-Contrastive Estimation Loss  (Gutmann and Hyv?rinen, 2010)  for minimizing the distance between these two vectors. Our results establish a new state-of-the-art accuracy of 69.52% on the VQA-CP-v2 benchmark outperforming the current best models by 10.57%. At the same time, our models achieves the best accuracy (70.24%) on VQA-VQA-v2 among models designed for the VQA-CP task. This work takes a step away from explicit debiasing as a method for OOD generalization and instead proposes amplification of positive bias and implicit attenuation of spurious correlations as the objective. Our contributions are as follow. ? We introduce the Mutant paradigm for training VQA models and the sample-generation mechanism which takes advantage of semantic transformations of the input image or question, for the goal of OOD generalization. ? In addition to the conventional classification task, we formulate a novel training objective using Noise Contrastive Estimation over the projections of cross-modal features and answer embeddings on a shared projection manifold, to predict the correct answer. ? Our pairwise consistency loss acts as a regularization that seeks to bring the distance between ground-truth answer vectors closer to the distance between predicted answer vectors for a pair of original and mutant inputs. ? Extensive experiments and analyses demonstrate advantages of our method on the VQA-CP dataset, and establish a new state-of-theart of 69.52%, an improvement of 10.57%. 

 MUTANT We consider the open-ended VQA problem as a multi-class classification problem. The VQA dataset D = {Q i , I i , a i } N i=1 consists of questions Q i ? Q and images I i ? I, and answers a i ? A. Many contemporary VQA models such as Up-Dn  (Anderson et al., 2018)  and LXMERT  (Tan and Bansal, 2019)  first extract cross-modal features from the image and question using attention layers, and then use these features as inputs to a neural network answering module which predicts the answer classes. In this section we define our Mutant paradigm under this formulation of the VQA task. 

 Concept of Mutations Let X = (Q, I) denote an input to the VQA system with true answer a. A mutant input X * is created by a small transformation in the image (Q, I * ) or in the question (Q * , I) such that this transformation leads to a new answer a * , as shown in Figure  1 . There are three categories of transformation T that create the mutant input X * = T (X), addition, removal, or substitution. For image mutations, these correspond to addition or removal of objects, and morphing the attributes of the objects, such as color, texture, and lighting conditions. For instance addition or removal of a person from the image in Figure  3  changes the answer to the question "How many persons are pictured". Question mutations can be performed by addition of a negative word ("no", "not", etc.) to the question, masking critical words in the question, and substituting an objectword with an antonym or adversarial word. Thus for each sample in the VQA dataset, we can obtain a mutant sample and use it for training. 

 Training with Mutants Our method of training with mutant samples relies on three key concepts that supplement the conventional VQA classification task. Answer Projection: The traditional learning strategy of VQA models optimizes for a standard classification task using softmax cross-entropy: L V QA = ?1 N N i=1 log(softmax f V QA (X i ), a i )). (1) QA as a classification task is popular since the answer vocabulary follows a long-tailed distribution over the dataset. However this formulation is problematic since it does not consider the meaning of the answer while making a decision, but instead learns a correlation between the one-hot vector of the answer-class and input features. Thus to answer the question "What is the color of the banana", models learn a strong correlation between the question features and the answer-class for "yellow", but do not encode the notion of yellowness or greenness of bananas. This key drawback negatively impacts the generalizability of these models to raw green or over-ripe black bananas at test-time. To mitigate this, in addition to the classification task, we propose a training objective that operates in the space of answer embeddings. The key idea is to map inputs (image-question pairs) and outputs (answers) to a shared manifold in order to establish a metric of similarity on that manifold. We train a projection layer that learns to project features and answers to the manifold as shown in Figure  2 . We then use Noise Contrastive Estimation  (Gutmann and Hyv?rinen, 2010)  as a loss function to minimize the distance between the projection of cross modal features z and projection of glove vector v for ground-truth answer a, given by: L N CE = ?log e cos(z f eat , za) a i ?A e cos(z f eat , z i a ) , (2) where z f eat = f proj (z) and z a = f proj (glove(a)), and A is the set of all possible answers in our training dataset. It is important to note that this similarity metric is not between the true answer and the predicted answer, but between the projection of input features and the projection of answers, to incorporate context in the answering task. Type Exposure: Linguistic priors in datasets have led models to learn spurious correlations between question and answers. For instance, in VQA, the most common answer for "What sport ..." questions is "tennis", and for "How many ..." questions is "two". Our aim is to remove this negative bias from the models. Instead of removing all bias from these models, we teach models to identify the question type, and learn which answers can be valid for a particular question type, irrespective of their frequency of occurrence in the dataset. For instance, the answer to "How many ..." can be all numbers, answers to "What color ..." can be all colors, and answers to questions such as "Is the / Are there ..." questions is either yes or no. We call this Type Exposure since it instructs the model that although a strong correlation may exist between a questionanswer pair, there are other answers which are also Pairwise-Consistency: The final component of Mutant is pairwise consistency. We jointly train our models with the original and mutant sample pair, with a loss function that ensures that the distance between two predicted answer vectors is close to the distance between two ground-truth answer vectors. The pairwise consistency loss is given below, where z a is the vector for answer a, m, GT denote mutant sample and ground-truth respectively. L P W = ||cos(z a GT , z m a GT ) ? cos(z a pred , z m a pred )|| 1 . This pairwise consistency is designed as a regularization that incorporates the notion of semantic shift in answer space as a consequence of a mutation. For instance, consider the image mutation in Figure  3  which changes the ground-truth answer from "two" to "one". This shift in answer-space should be reflected by the predictor. 

 Generating Input Mutations for VQA In order to train VQA models under the mutant paradigm, we need a mechanism to create mutant samples. Mutations are transformations that act on semantic entities in either the image or the question, in ways that can reliably lead to a new answer. For the question, semantic entities are words, while for images, semantic entities are objects. It is important to note that our mutation process is automated and does not use the knowledge about the test set distribution in order to create new samples. In this section, we delineate our automated generation process for both image and question-mutation. 

 Image Mutations For image mutation, we first identify critical objects from the image that results in a change in the answer, and either remove instances of these objects (removal) or morph their color (substitution). Removing Object Instances: Removing an instance of an object class can be either critical to the question (i.e. the answer to the question changes) or non-critical (i.e. answer is unchanged). If an object (or it's synonym or hypernym) is mentioned in the question, we deem it to be critical to the question, otherwise it is deemed non-critical. For each object with M instances in the image, we randomly remove m instances from the image s.t. m ? {0, . . . , M } using polygon annotations from the COCO  (Lin et al., 2014)  dataset. Thus for each image, we get multiple masked images, with pixels inside the instance bounding-box removed, as shown in Figure  3 . These masked images are fed to a GAN-based inpainting network  (Yu et al., 2018)  that makes the mutant image photo-realistic, and also prevents the model from getting cues from the shape of the mask. In the case of numeric questions, if m critical objects are removed, the answer to for the mutant image changes from n to n ? m. For yes-no questions, removal of all critical objects (m = n) will flip the answer from "yes" to "no", while removing m < n critical objects will not. Note that m = 0 corresponds to the original image and does not result in a change in the answer. Color Inversion: For mutations that involve a change in color, we use samples with questions about the color of objects in the image, and change the color of critical objects by pixel-level color inversion in RGB-space. The true answer is replaced with the new color of the critical objects. To get objects with new colors, we do not use the knowledge about colors of objects in the world. In some cases, the new colors of the object may not correspond to real-world scenes, thus forcing the model to actually identifying colors, and not answer from language priors, such as "bananas are yellow". 

 Question Mutations We use three types of question mutations as shown in the example in Table  1 . We first identify the critical object and then apply template-based question operators similar to  (Gokhale et al., 2020) . The first operator is negation for yes-no questions, which  is achieved by a template based procedure that negates the question by adding a "no" or "not" before a verb, preposition or noun phrase. The second is the use of antonyms or adversarial object-words to substitute critical words. The third mutation masks words in the question and thus introduces ambiguity in the question. Questions for which the new answer cannot be deterministically identified are annotated with a broad category label such as color, location, fruit instead of the exact answers such as red, library, apple which the model cannot be expected to answer since some words have been masked or replaced with adversarial words. Yet, we want the model to be able to identify this broad category of answers even under partially occluded inputs. The answer remains unchanged for mutations with non-critical objects or words. 

 Mutant Statistics: We use the training set of VQA-CP-v2  (Agrawal et al., 2018a)  to generate mutant samples. For each original sample, we generate 1.5 mutant samples on average, thus obtaining a total of 679k samples. Table  2  shows the distribution of our generated mutations with respect to the type of mutation. Addition of mutant samples does not change the distribution of samples per question-type.   

 Experiments 

 Setting Datasets: We train and evaluate our models on VQA-CP-v2. This is a natural choice for evaluating OOD generalization since VQA-CP is a noni.i.d. reorganization of the VQA dataset, and was created in order to evaluate VQA models in a setting where language priors cannot be relied upon for a correct prediction. This is because for every question type (65 types according to the question prefix), the prior distribution of answers is different in train and test splits of VQA-CP. We also train and evaluate our models on the VQA-v2  (Goyal et al., 2017)  validation set, and compare the gap between the imbalanced and non-i.i.d. setting of VQA-CP against the balanced i.i.d. setting of VQA. Hyperparameters: All of our models are trained on two NVIDIA Tesla V100 16GB GPUs for 10 epochs with batch size of 32 and learning rate 1e-5. 

 Each epoch takes approximately three hours for UpDn and four hours for LXMERT. 

 Baseline Models We compare our method with GVQA  (Agrawal et al., 2018b) , RUBI  (Cadene et al., 2019) , SCR  (Wu and Mooney, 2019) , LMH  (Clark et al., 2019) , CSS  (Chen et al., 2020a)  as our baselines. Since most of these methods are built with UpDn  (Anderson et al., 2018)  as the backbone, we investigate the efficacy of UpDn under the mutant paradigm. On the other hand, LXMERT  (Tan and Bansal, 2019)  has emerged as a powerful transformer-based cross-modal feature extractor, and is pre-trained on tasks such as masked language modeling and cross-modality matching, inspired by BERT  (Devlin et al., 2019) . LXMERT is a top performing single-model on multiple vision-andlanguage tasks such as VQA, GQA (Hudson and Manning, 2019), ViZWiz  (Bigham et al., 2010) , and NLVR2  (Suhr et al., 2019) . We therefore use is as a strong baseline for our experiments. LXMERT is representative of the recent trend towards using BERT-like pre-trained models  Su et al., 2019; Li et al., 2020;  and fine-tuning them on multiple downstream vision and language tasks. Note that we do not use ensemble models for our experiments and focus only on single-model baselines. 

 Results on VQA-CP-v2 and VQA-v2 Performance on two benchmarks VQA-CP-v2 and VQA-v2 is shown in Table  3 . We compare existing models against UpDn and LXMERT incorporated into our Mutant method. adding de-biasing techniques. We show our debiasing method improves on two SOTA models and outperforms all of the above baselines, unlike previous work which only modifies UpDn. This empirically shows Mutant to be model-agnostic. When trained and evaluated on the balanced i.i.d. VQA-v2 dataset, our method achieves the best performance amongst methods designed specifically for OOD generalization, with an accuracy of 70.24%. This is closest among baselines to the SOTA established by LXMERT, which is trained explicitly for the balanced, i.i.d. setting. To make this point clear, we report the gap between the overall scores for VQA-CP and VQA-v2, following the protocol from  Chen et al. (2020a)  in Table  3 . 

 Results on VQA-v2 without re-training: Additionally, we use our best model trained on VQA-CP and evaluate it on the VQA test standard set without re-training on VQA-v2 data. The objective here is to evaluate whether models trained on biased data (VQA-CP) and mutant data is able to generalize to VQA-v2 which uses an i.i.d. traintest split. This gives us an overall accuracy of 67.63% comprising with 88.56% on yes-no questions, 50.76% on number-based questions, and 54.56% on other questions. This is better than all existing VQA-CP models that are explicitly trained on VQA-v2 (reported in Table  3 ), and thus demonstrates the generalizability of our approach. 

 Analysis 

 Effect of Training with Mutant Samples: In this analysis we measure the effect of augmenting the training data with mutant samples on UpDn and LXMERT without any architectural changes. The results are reported in Table  4 . Both models improve when exposed to the mutant samples, UpDn by 10.42% and LXMERT by 13.46%. There is a markedly significant jump in performance for both models for the yes-no and number categories. UpDn especially benefits from Mutant samples in terms of the accuracy on numeric questions (a boost of 23.94%). We also compare our final model when trained only with image mutations and only with question mutations in Table  4 . While this is worse than training with both types of mutations, it can be seen that question mutations are better than image mutations in the case of yes-no and other questions, while image mutations are better on numeric questions. 

 Ablation Study: We conduct ablation studies to evaluate the efficacy of each component of our method, namely Answer Projection, Type Exposure and Pairwise Consistency, on both baselines, as shown in Table  5 . Introduction of Answer Projection significantly improves yes-no performance, while Type Exposure improves performance on other questions. We also observe that the pairwise consistency loss significantly boosts performance on numeric questions and yes-no questions. Note that there is a minor difference between the original and the mutant sample, and the model needs to understand this difference, which in turn can enable the model to reason about the question and predict the new answer. For instance the pairwise consistency loss allows the model to learn the correlation between one missing object and a change in answer from "two" to "one" in Figure  3 , resulting in an improvement in the counting ability of our VQA model. Similarly, the pairwise consistency allows the model to improve on yes-no questions for which the answer changes when a critical object is removed. 

 Effect of LMH Debiasing on Mutant: We compare the results of our model when trained with or without the explicit de-biasing method LMH  (Clark et al., 2019) . LMH is an ensemblebased method trained for avoiding dataset biases, and is the most effective among all de-biasing strategies developed for the VQA-CP challenge. LMH implements a learned mixing strategy, by using the main model in combination with a biasonly model trained only with the question, without the image. The learned mixing strategy uses the bias-only model to remove biases from the main model. It can be seen from Table  6  that LMH leads to a drop in performance when used in combination with Mutant. This is potentially because in the process of debiasing, LMH ends up attenuating positive bias introduced by Mutant that is useful for generalization.  Kervadec et al. (2020)  have concurrently shown that de-biasing methods such as LMH indeed result in a decrease in performance on out-of-distribution (OOD) test samples in the GQA (Hudson and Manning, 2019) dataset, mirroring our analysis on VQA-CP shown in Table  6 . 

 Related Work De-biasing of VQA datasets: The VQA-v1 dataset  (Antol et al., 2015)  contained imbalances and language priors between question-answer pairs. This was mitigated by VQA-v2  (Goyal et al., 2017)  which balanced the data by collecting complementary images such that each question was associated with two images leading to two differ-ent answers. Identifying that the distribution of answers in the VQA dataset led models to learn superficial correlations,  Agrawal et al. (2018a)  proposed the VQA-CP dataset by re-organizing the train and test splits such that the the distribution of answers per question-type was significantly different for each split. Robustness in VQA: Ongoing efforts seek to build robust VQA models for VQA for various aspects of robustness.  Shah et al. (2019)  propose a model that uses cycle-consistency to not only answer the question, but also generate a complimentary question with the same answer, in order to increase the linguistic diversity of questions. In constrast, our work generates questions with a different answer. Selvaraju et al. (  2020 ) provide a dataset which contains perception-related sub-questions for each VQA question. Antonym-consistency has been tackled in  Ray et al. (2019) . Inspired by invariant risk minimization  (Arjovsky et al., 2019)  which links out-of-distribution generalization to invariance and causality,  Teney et al. (2020b)  provide a method to identify invariant correlations in the training set and train models to ignore spurious correlations.  Asai and Hajishirzi (2020) ;  Gokhale et al. (2020)  explore robustness to logical transformation of questions using first-order logic connectives and (?), or (?), not (?). Removal of bias has been a focus of  Ramakrishnan et al. (2018) ;  Clark et al. (2019)  for the VQA-CP task. We distinguish our work from these by amplifying positive bias and attenuating negative bias. Data Augmentation: It is important to note that the above work on data de-biasing and robust models focuses on the language priors in VQA, but not much attention has been given to visual priors. Within the last year, there has been interest in augmenting VQA training data with counterfactual images  (Agarwal et al., 2020; Chen et al., 2020a) . Independently,  Teney et al. (2020a)  have also demonstrated that counterfactual images obtained via minimal editing such as masking or inpainting can lead to improved OOD generalization of VQA models, when trained with a pairwise gradient-based regularization. Self-supervised data augmentation has been explored in recent work  (Lewis et al., 2019; Fabbri et al., 2020;    

 Discussion and Conclusion In this paper, we present a method that uses input mutations to train VQA models with the goal of Out-of-Distribution generalization. Our novel answer projection module trained for minimizing distance between answer and input projections complements the canonical VQA classification task. Our Type Exposure model allows our network to consider all valid answers per question type as equally probable answer candidates, thus moving away from the negative question-answer linguistic priors. Coupled with pairwise consistency, these modules achieve a new state-of-the-art accuracy on the VQA-CP-v2 dataset and reduce the gap between model performance on VQA-v2 data. We differentiate our work from methods using random adversarial perturbations for robust learning  (Madry et al., 2018) . Instead we view input mutations as structured perturbations which lead to a semantic change in the input space and a deterministic change in the output space. We envision that the concept of input mutations can be extended to other vision and language tasks for robustness. Concurrent work in the domain of image classification shows that carefully designed perturbations or manipulations of the input can benefit generalization and lead to performance improve-ments  (Chen et al., 2020b; Hendrycks et al., 2019) . While perception is a cornerstone of understanding, the ability to imagine changes in the scene or language query, and predict outputs for that imagined input allows models to supplement "what" decision making (based on observed inputs) with "what if" decision making (based on imagined inputs). The Mutant paradigm is an effort towards "what if" decision making. Code is available here. 

 B Image Mutant Generation Process In this section we provide additional details about our process for generating mutant samples from original question-image-answer triplets (Q-I-A) in the VQA-CP dataset. For all linguistic operations we use a combination of SpaCy  (Honnibal and Montani, 2017)  and the LemmInflect library  (Jascob, v0.2.1 (February 22, 2020)  for lemmatization and inflection.  

 B.1 Selection of Objects For each VQA sample, a list of words W is created, which contains words from the ground-truth answers and the question. All nouns in W are converted to their singular form. For yes-no questions, numeric questions, and questions about colors of objects, a list of objects O is obtained from COCO. Background and crowd objects are filtered out from O. From O critical objects O C and and non-critical objects O N C are obtained. Critical objects are those objects in the image that when manipulated or removed, may change the answer to the question being asked. For this we follow a simple heuristic that states that if an object-word or it's synonym or hyponym is present in W , then it is a critical object. Then a critical object o ? O is chosen at random, and m instances of this object are chosen at random. The polygon annotations (a polygon border) for this object are obtained from the COCO dataset as shown in Figure  4 . Using these annotations, either a removal or color-inversion operation is applied to create the mutant image. 

 B.2 Object Removal and In-painting After the object instance is selected, it is removed from the image by replacing all pixel values by 1 (white). This masked image is then input to a GANbased image inpainting network  (Yu et al., 2018)  that fills up this pixels in the mask. This makes the image photorealistic. This network is one of the best available off-the-shelf blind image inpainting models, and is trained on the ImageNet  (Deng et al., 2009) . The masked image could also be used as the mutant image however we prefer to use photorealistic images for two main reasons. First, masked images do not lie in the same distribution as natural images, and secondly, the mask boundary may give clues to the network about the the shape or outline of the missing object. 

 B.3 Color Inversion Process For mutation that involves a change in the color of the object, we perform a simple pixel-wise color inversion operation on each pixel in the mask to get the mutant image as shown in Figure  5 . This is to ensure that we do not use any prior knowledge about valid colors of a specific object. For instance, bananas can typically be yellow, green, or black. However, if we only change the color or a banana to one of these three colors, we would be using domain knowledge and inadvertently introducing answers from the test set, defeating the purpose of OOD generalization. Although the simple inversion process can introduce unnatural colors like blue bananas, it forces the model to understand colors in the image to answer the question instead of simply answering from linguistic priors (such as the memorized knowledge that bananas can be green, yellow, or black). 

 B.4 Answer Generation The new answers are generated based on the type of question. For yes-no questions, if all instances of the object are removed then the answer changes from yes to no. If only some instances are removed or if the object is non-critical, the answer remains the same. For number questions, if m instances of a critical object are removed, the answer changes from n to n ? m, else the answer remains the same. For color-based questions we convert the answer color to their HEX value using Webcolors 2 , invert the value, and find the color in CSS-21 colors closest to this value to generate the new answer. 

 C Question Mutant Generation Process For generating question mutants, we use three operators: negation, substitution by antonyms or adversarial words, and masking critical words. 

 C.1 Negation For yes-no questions and color-based questions, we use a template-based negation technique that puts a negative word such as "not" or "no" before a preposition, noun phrase, or verb. For instance "Is this chair broken?" is negated to "Is this chair not broken?". We show examples of negation in Table  7 . Negation simply flips the answer from yes to no or no to yes. 

 C.2 Adversarial Words and Masking Another form of question mutation is substituting object-words with their adversarial words. To do so, we create a list of all object words and their synonyms and use BERT  (Devlin et al., 2019)  similarity to rank the most similar words. To replace a word, we chooser the most similar word which is not present in the image. The third type of mutation is masking, where a critical object word is removed from the question and replaced with the token "MASK". For both these types of mutations, determining the correct answer in some cases is not possible as can be seen from examples in Table  7 . Thus we use the broad category as the answer. For instance, when a question such as "How big is the book" is replaced with either "How big is the plane" or "How big is the [MASK]", it is clear that the question is about the size of an object. Thus we annotate this question with this broad category "size" as the answer. In other cases, where even a broad category cannot be ascertained, the answer is replaced with "can't say" or "don't know". Figure 1 : 1 Figure 1: Illustration of the mutant samples. The input mutation, either by manipulating the image or the question, results in a change in the answer. 

 Figure 2 : 2 Figure 2: Overall architecture of the Mutant Method includes a cross-modal feature extractor, answer projection layer, answering layer and type exposure model 

 Figure 3 : 3 Figure 3: Figure illustrating our dataset creation pipeline for image mutations. m object instances of "critical" object are identified from the question and image, and mutation performed either by removal or color inversion. A represents the answer to the question. 

 Figure 4 : 4 Figure 4: Illustration of COCO bounding box and polygon annotations for m instances of an object, and the inpainting results after removal 

 Figure 5 : 5 Figure 5: Illustration of color inversion procedure 


 Table 1 : 1 Examples of our question mutation. The image is shown on the left, and the original question is in the first row of the table. Examples of the two types of mutation are shown in the table. Mutation Type Question Answer Original Is the lady holding the baby? Yes Substitution (Negation) Is the lady not holding the baby? No Substitution (Adversarial) Is the cat holding the baby? No Original How many people are there? Three Deletion (Masking) How many [MASK] are there? "Number" Original What is the color of the man's shirt? Blue Substitution (Negation) What is not the color of the man's shirt? Magenta Deletion (Masking) Is the [MASK] holding the baby? Can't say Original What color is the umbrella ? Pink Deletion (Masking) What color is the [MASK]? "color" 

 Table 2 : 2 Distribution of generated mutant samples by category of mutation 

 Table 3 : 3 Accuracies on VQA-CP v2 test and VQA-v2 validation set, along with Percentage gap between overall accuracies on these two datasets. "Ours" represents the final model with Answer Projection, Type Exposure and Pairwise Consistency. Overall best scores are bold, our best are underlined. 

 Table 5 : 5 Ablation study to investigate the effect of each component of our method: Answer Projection (AP), Type Exposure (TE), Pairwise Consistency (PW), and independent effect of image and question mutations. Model VQA-CP v2 test ? (%) All Yes/No Num Other UpDn 50.16 61.45 35.87 50.14 UpDn + AP 54.51 88.35 41.01 32.89 UpDn + TE 56.32 80.56 46.14 46.41 UpDn + AP + TE 55.76 90.25 43.78 41.40 UpDn + AP + PW 57.54 91.59 49.17 41.93 UpDn + TE + PW 60.32 86.10 50.23 49.58 UpDn + AP + TE + PW 61.72 88.90 49.68 50.78 LXM 59.69 73.19 32.85 59.29 LXM + AP 60.45 88.46 43.24 50.49 LXM + TE 63.36 77.10 46.50 61.27 LXM + AP + TE 64.73 85.34 47.23 58.71 LXM + AP + PW 67.14 90.49 65.52 55.34 LXM + TE + PW 64.17 94.71 35.19 48.80 LXM + AP + TE + PW 69.52 93.15 67.17 57.78 

 Table 6 : 6 Effect of combining LMH de-biasing with the Mutant paradigm, measured as drop in accuracy (%) Model Method VQA-CP v2 test ? (%) All Yes/No Num Other UpDn + Ours Base 61.72 88.90 49.68 50.78 UpDn + Ours LMH 55.38 90.99 39.74 40.99 Drop in Accuracy 6.34 -2.09 9.95 9.80 LXMERT + Ours Base 69.52 93.16 67.17 57.78 LXMERT + Ours LMH 63.85 88.34 48.23 55.28 Drop in Accuracy 5.67 4.82 18.86 2.50 

 in the domain of text-based question answering. The mutant paradigm presented in this work is one of the first enable the generation of VQA samples that result in different answers, coupled with a novel architecture and a consistency loss between original and mutant samples as a training objective. Answer Embeddings: In one of the early works on VQA, Teney and Hengel (2016) use a combi- nation of image and question representations and answer embeddings to predict the final answer. Hu et al. (2018) learn two embedding functions that transform image-question pair and answers to a shared latent space. Our method is different from this since we use a combination of classification and NCE Loss on the projection of answer vec- tors, as opposed to a single training objective. This means that although the predicted answer is ob- tained as the most probable answer from a set of candidate answers, the NCE Loss in the answer- space embeds the notion of semantic similarity between the answer. Our Type Exposure model is in principal similar to Kafle and Kanan (2016) who use the predicted answer-type probabilities in a Bayesian framework, while we use it as an additional constraint, i.e. as a regularization for a maximum likelihood objective. 

			 More details about mutant samples are in Supp. material. 

			 captions along with bounding boxes and polygon annotations for each object instance in the image. 

			 https://pypi.org/project/webcolors/
