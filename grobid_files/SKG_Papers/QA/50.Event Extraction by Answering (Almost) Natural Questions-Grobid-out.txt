title
Event Extraction by Answering (Almost) Natural Questions

abstract
The problem of event extraction requires detecting the event trigger and extracting its corresponding arguments. Existing work in event argument extraction typically relies heavily on entity recognition as a preprocessing/concurrent step, causing the well-known problem of error propagation. To avoid this issue, we introduce a new paradigm for event extraction by formulating it as a question answering (QA) task that extracts the event arguments in an end-to-end manner. Empirical results demonstrate that our framework outperforms prior methods substantially; in addition, it is capable of extracting event arguments for roles not seen at training time (i.e., in a zeroshot learning setting). 1

Introduction Event extraction is a long-studied and challenging task in Information Extraction (IE)  (Sundheim, 1992; Grishman and Sundheim, 1996; Riloff, 1996) . Its goal is to extract structured information -"what is happening" and the persons/objects that are involved -from unstructured text. The task is illustrated via an example in Figure  1 , which depicts an ownership transfer event (the event type), triggered by the word "sale" (the event trigger) and accompanied by its extracted arguments -text spans denoting entities that fill a set of (semantic) roles associated with the event type (e.g., BUYER, SELLER and ARTIFACT for ownership transfer events). Recent successful approaches to event extraction have benefited from dense features extracted by neural models  (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018)  as well as contextualized lexical representations from pretrained language models  (Zhang et al., 2019b; Wadden et al., 2019) .  1  Our code and question templates for the work are open sourced at https://github.com/xinyadu/eeqa for reproduction purpose. 

 Input: As part of the 11-billion-dollar sale of USA Interactive's film and television operations to the French company and its parent company in December 2001, USA Interactive received 2.5 billion dollars in preferred shares in Vivendi Universal Entertainment. 

 Event type Transaction-Transfer-Ownership Trigger "sale" Args. Buyer "French company", "parent company" Seller "USA Interactive" Artifact "operations" Place - The approaches, however, exhibit two key weaknesses. First, they rely heavily on entity information for argument extraction. In particular, event argument extraction generally consists of two steps -first identifying entities and their general semantic class with trained models  (Wadden et al., 2019)  or a parser  (Sha et al., 2018) , then assigning argument roles (or no role) to each entity. Although joint models  (Yang and Mitchell, 2016; Nguyen and Nguyen, 2019; Zhang et al., 2019a; Lin et al., 2020)  have been proposed to mitigate this issue, error propagation  (Li et al., 2013)  still occurs during event argument extraction. 

 Extracted Event: A second weakness of neural approaches to event extraction is their inability to exploit the similarities of related argument roles across event types. For example, the ACE 2005  (Doddington et al., 2004)  CONFLICT.ATTACK events and JUS-TICE.EXECUTE events have TARGET and PERSON argument roles, respectively. Both roles, however, refer to a human being (who) is affected by an action. Ignoring the similarity can hurt performance, especially for argument roles with few/no examples at training time (e.g., similar to the zero-shot setting in  Levy et al. (2017)) . In this paper, we propose a new paradigm for the event extraction task -formulating it as a question answering (QA)/machine reading comprehension (MRC) task (Contribution 1). The general framework is illustrated in Figure  2 . Using BERT (Devlin  For each, we design one or more Question Templates that map the input sentence into the standard BERT input format. Thus, trigger detection becomes a request to identify "the action" or the "verb" in the input sentence and determine its event type; and argument extraction becomes a sequence of requests to identify the event's arguments, each of which is a text span in the input sentence. Details will be explained in Section 2. To the best of our knowledge, this is the first attempt to cast event extraction as a QA task. Treating event extraction as QA overcomes the weaknesses in existing methods identified above (Contribution 2): (1) Our approach requires no entity annotation (gold or predicted entity information) and no entity recognition pre-step; event argument extraction is performed as an end-to-end task; (2) The question answering paradigm naturally permits the transfer of argument extraction knowledge across semantically related argument roles. We propose rule-based question generation strategies (including incorporating descriptions in annotation guidelines) for templates creation, and conduct extensive experiments to evaluate our framework on the Automatic Content Extraction (ACE) event extraction task and show empirically that the performance on both trigger and argument extraction outperform prior methods (Section 3.2). Finally, we show that our framework extends to the zero-shot setting -it is able to extract event arguments for unseen roles (Contribution 3). 

 Methodology In this section, we first provide an overview for the framework (Figure  2 ), then go deeper into details of its components: question generation strategies for template creation, as well as training and inference of QA models. 

 Framework Overview Our QA framework for event extraction relies on two sets of Question Templates that map an input sentence to a suitable input sequence for two instances of a standard pre-trained bidirectional transformer (BERT ). The first of these, BERT_QA_Trigger (green box in Figure  2 ), extracts from the input sentence the event trigger which is a single token, and its type (one of a fixed set of pre-defined event types). The second QA model, BERT_QA_Arg (orange box in Figure  2 ), is applied to the input sequence, the extracted event trigger and its event type to iteratively identify candidate event arguments (spans of text) in the input sentence. Finally, a dynamic threshold is applied to the extracted candidate arguments, and only the arguments with probability above the threshold are retained. The input sequences for the two QA models share a standard BERT-style format: [CLS] <question> [SEP] <sentence> [SEP] where [CLS] is BERT's special classification token,  [SEP]  is the special token to denote separation, and <sentence> is the tokenized input sentence. We provide details on how to obtain the <question> in Section 2.2. Details on the QA models and the inference process will be explained in Section 2.3. What is the destination? Where the transporting is directed? Table  1 : Arguments (of event type MOVEMENT.TRANSPORT) and corresponding questions from three templates. "in <trigger>" is not added to the questions in this example. 

 Question Generation Strategies For our QA-based framework for event extraction to be easily moved from one domain to the other, we concentrated on developing question generation strategies that not only worked well for the task, but can be quickly and easily implemented. For event trigger detection, we experiment with a set of four fixed templates -"what is the trigger", "trigger", "action", "verb". Basically, we use the fixed literal phrase as the question. For example, if we choose the "action" template, the input sequence for the example sentence in Figures  1 and 2  is instantiated as: [CLS] action  [SEP]  As part of the 11billion-dollar sale .. 

 . [SEP] As for event argument extraction, we design three templates with argument role name, basic argument based question and annotation guideline based question, respectively: ? Template 1 (Role Name) For this template, <question> is simply instantiated with the argument role name (e.g., artifact, agent, place). ? Template 2 (Type + Role) Instead of directly using the argument role name (<role name>) as the question, we first determine the argument role's general semantic type -one of person, place, other; and construct the associated "WH" word question -who for person, where for place and what for all other cases, of the following form: <WH_word> is the <role name> ? Examples are shown in Table  1  for the arguments of event type MOVE-MENT.TRANSPORT. By adding the WH word, more semantic information is included as compared to Template 1. 

 ? Template 3 (Incorporating Annotation Guidelines) To incorporate even more semantic information and make the question more natural sounding, we utilize the descriptions of each argument role provided in the ACE annotation guidelines for events  (Linguistic Data Consortium, 2005)  for generating the questions. ? + "in <trigger>" Finally, for each template type, it is possible to encode the trigger information by adding "in <trigger>" at the end of the question (where <trigger> is instantiated with the real trigger token obtained from the trigger detection phase). For example, the Template 2 question incorporating trigger information would be: <WH_word> is the <argument> in <trigger>? To help better understand all the strategies above, Table  1  presents an example for argument roles of event type MOVEMENT.TRANSPORT. We see that the annotation guideline based questions are more natural and encode more semantics about a given argument role, than the simple Type + Role question "what is the artifact?". 

 Question Answering Models We use BERT  as the base model for getting contextualized representations for the input sequences for both BERT_QA_Trigger and BERT_QA_Arg. After the instantiation with question templates the sequences are of format [CLS] <question> [SEP] <sentence> [SEP]. Then we get the contextualized representations of each token for trigger detection and argument extraction with BERT T r and BERT Arg , respectively. For the input sequence (e 1 , e 2 , ..., e N ) prepared for trigger detection, we have: E = [e 1 , e 2 , ..., e N ] e 1 , e 2 , ..., e N = BERT T r (e 1 , e 2 , ..., e N ) For the input sequence (a 1 , a 2 , ..., a M ) prepared for argument span extraction, we have: A = [a 1 , a 2 , ..., a M ] a 1 , a 2 , ..., a M = BERT Arg (a 1 , a 2 , ..., a M ) The output layer of each QA model, however, differs: BERT_QA_Trigger predicts the event type for each token in sentence (or None if it is not an event trigger), while BERT_QA_Arg predicts the start and end offsets for the argument span with a different decoding strategy. More specifically, for trigger prediction, we introduce a new parameter matrix W tr ? R H?T , where H is the hidden size of the transformer and T is the number of event types plus one (for nontrigger tokens). softmax normalization is applied across the T types to produce P tr , the probability distribution across the event types: P tr = softmax(EW tr ) ? R T ? N At test time, for trigger detection, to obtain the type for each token e 1 , e 2 , ..., e N , we simply apply argmax to P tr . For argument span prediction, we introduce two new parameter matrices W s ? R H?1 and W e ? R H?1 ; softmax normalization is then applied across the input tokens a 1 , a 2 , ..., a M to produce the probability of each token being selected as the start/end of the argument span: P s (i) = softmax(a i W s ) P e (i) = softmax(a i W e ) To train the models (BERT_QA_Trigger and BERT_QA_Arg), we minimize the negative loglikelihood loss for both models, parameters are updated during the training process. In particular, the loss for the argument extraction model is the sum of two parts: the start token loss and end end token loss. For the training examples with no argument span (no answer case), we minimize the start and end probability of the first token of the sequence ([CLS]). L arg = L arg_start + L arg_end Inference with Dynamic Threshold for Argument Spans At test time, predicting the argument spans is more complex -for each argument role, there can be several or no spans to be extracted. After the output layer, we have the probability of each token a i ? (a 1 , a 2 , ..., a M ) being the start (P s (i)) and end (P e (i)) of the argument span. Firstly, we run an algorithm to harvest all valid argument spans candidates for each argument role (Algorithm 1). Basically, we: 1. Enumerate all the possible combinations of start offset (start) and end offset (end) of the argument spans (line 1-2); 2. Eliminate the spans not satisfying the constraints: start and end token must be within the sentence; the length of the span should be shorter than a maximum length constraint; Argument spans should have larger probability than the probability of "no argument" (which is stored at the [CLS] token) (line 3-5); 3. Calculate the relative no answer score (no_ans_score) for the candidate span and add the candidate to list (line 6-8). Then we run another algorithm to filter out candidate arguments that should not be included (Algorithm 2). More specifically, we obtain a probability threshold (best_thresh) that helps achieve best evaluation results on the dev set (line 1-9) and keep only those arguments with no_ans_score smaller than the threshold (line 10-13). With the dynamic threshold for determining the number of arguments to be extracted for each role, we avoid adding a (hard) hyperparameter for this purpose. Another easier way to get final argument predictions is to directly include all the candidates with no_ans_score < 0, which does not require tuning the dynamic threshold best_thresh. 

 Experiments 

 Dataset and Evaluation Metric We conduct experiments on the ACE 2005 corpus  (Doddington et al., 2004) , it contains documents crawled between year 2003 and 2005 from a variety of areas such as newswire (nw), weblogs (wl), broadcast conversations (bc) and broadcast news (bn). The part that we use for evaluation is fully annotated with 5,272 event triggers and 9,612 arguments. We use the same data split and preprocessing step as in the prior works  (Zhang et al., 2019b; Wadden et al., 2019) . As for evaluation, we adopt the same criteria defined in  Li et al. (2013) : An event trigger is correctly identified (ID) if its offsets match those of a gold-standard trigger; and it is correctly classified if its event type (33 in total) also match the type of the gold-standard trigger. An event argument is correctly identified (ID) if its offsets and event type match those of any of the reference argument mentions in the document; and it is correctly classified if its semantic role (22 in total) is also correct. Though our framework does not involve the trigger/argument identification step and tackles the identification + classification in an end-to-end way. We still report the trigger/argument identification's results to compare to prior work. It could be seen as a more lenient eval metric, as compared to the final trigger detection and argument extraction metric (ID + Classification), which requires both the offsets and the type to be correct. All the aforementioned elements are evaluated using precision (denoted as P), recall (denoted as R) and F1 scores (denoted as F1). 

 Results Evaluation on ACE Event Extraction We compare our framework's performance to a number of prior competitive models: dbRNN  (Sha et al., 2018)  is an LSTM-based framework that leverages the dependency graph information to extract event triggers and argument roles. Joint3EE (Nguyen and Nguyen, 2019) is a multi-task model that performs entity recognition, trigger detection and argument role assignment by shared BiGRU hidden representations. GAIL  (Zhang et al., 2019b ) is an ELMo-based model that utilizes generative adversarial network to help the model focus on harderto-detect events. DYGIE++  (Wadden et al., 2019)  is a BERT-based framework that models text spans and captures within-sentence and cross-sentence context. OneIE (Lin et al., 2020) is a joint neural model for extraction with global features.  2  In Table  2 , we present the comparison of models' performance on trigger detection. We also implement a BERT fine-tuning baseline and it reaches nearly same performance as its counterpart in the DYGIE++. We observe that our BERT_QA_Trigger model with best trigger questioning strategy reaches comparable (better) performance with the baseline models.  3  Table  3  shows the comparison between our model and baseline systems on argument extraction. Notice that the performance of argument extraction is directly affected by trigger detection. Because argument extraction correctness requires the trigger to which the argument refers to be correctly identified and classified. We observe, (1) This once again demonstrates the benefit of our new formulation for the task as question answering. To better understand how the dynamic threshold is affecting our framework's performance. We conduct an ablation study on this (Table  3 ) and find that the threshold increases the precision and the general F1 substantially. The last row in the table shows the test time ensemble performance of the predictions from BERT_QA_Arg trained with template 2 question, and another BERT_QA_Arg trained with template 3 question. The ensemble system outperforms the non-ensemble system in both precision and recall, demonstrating the benefit from both templates.   target event ontology. This framework's argument extraction's results are affected by the AMR results and their reported F1 is around 20-30% in their evaluation setting. 

 Evaluation on Unseen Argument Roles Using our QA-based framework, as we leverage more semantic information and naturalness into the question (from question template 1 to 2, to 3), both the precision and recall increase substantially. 

 Further Analysis 

 Influence of Question Templates To investigate how the question generation strategies affect the performance of event extraction, we perform experiments on trigger and argument extractions with different strategies, respectively. In Table  6 , we try different fixed questions for trigger detection. By "leaving empty", we mean instantiating the question with empty string.  4  There's no substantial gap between different alternatives. By using "verb" as the question, our BERT_QA_Trigger model achieves best performance (measured by F1 score). The QA model also encodes the semantic interactions between the fixed question ("verb") and the sentence, this explains why BERT_QA_Trigger is better than BERT FineTune in trigger detection. The comparison between different question generation strategies for argument extraction is even more interesting. In Table  5 , we present the results in two settings: event argument extraction with predicted triggers (the same setting as in Table  3 ), and with gold triggers. In summary, we finds that: ? Adding "in <trigger>" afterwards the question consistently improve the performance. the "in <trigger>", for each template (1, 2 & 3), the F1 of models' predictions drop around 3 percent when given predicted triggers, and more when given gold triggers. ? Our template 3 questioning strategy which is most natural achieves the best performance. As we mentioned earlier, template 3 questions are based on descriptions for argument roles in the annotation guideline, thus encoding more semantic information about the role name. And this corresponds to the accuracy of models' predictions -template 3 outperforms template 1&2 in both with "in <trigger>" and without "in <trigger>" setting. What's more, we observe that template 2 (adding a WH_word to form the questions) achieves better performance than the template 1 (directly using argument role name). 

 Error Analysis We further conduct error analysis and provide a number of representative examples.    3 , where the precision of our models are higher. In around 30% of the cases, our framework extracts same number of argument spans as in the gold data, half of them match exactly the gold arguments. After examining the example predictions, we find that reasons for errors can be mainly divided into the following categories: ? More complex sentence structures. In the following example, where the input sentence has multiple clauses, each with trigger and arguments (such as when triggers are partial or elided). Our model is capable of also extracting "Tom" as another ENTITY of the CONTACT.MEET event. [She] ENTITY visited the store and [Tom] ENTITY did too. But in the second example, when there is a higher-order event expressed spanning events in nested clauses: Canadian authorities arrested two Vancouver-area men on Friday and charged them in the deaths of [329 passengers and crew members of an Air-India Boeing 747 that blew up over the Irish Sea in 1985, en route from Canada to London] VICTIM . Our model did not extract the entire VICTIM correctly, which proves the difficulty of handling complex clauses structures. ? Lack of reasoning with document-level context. In sentence "MCI must now seize additional assets owned by Ebbers, to secure the loan." There is a TRANSFER-MONEY event triggered by loan, with MCI being the GIVER and Ebbers being the RECIPIENT. In the previous paragraph, it's men-tioned that "Ebbers failed to make repayment of certain amount of money on the loan from MCI." Without this context, it is hard to determine that Ebbers should be the recipient of the loan. ? Lack of knowledge for obtaining exact boundary for the argument span. For example, in "Negotiations between Washington and Pyongyang on their nuclear dispute have been set for April 23 in Beijing ...", for the ENTITY role, two argument spans should be extracted ("Washington" and "Pyongyang"). While our framework predicts the entire "Washington and Pyongyang" as the argument span. Although there's an overlap between the prediction and gold-data, the model gets no credit for it. These methods generally perform trigger detection ? entity recognition ? argument role assignment during decoding. Different from the works above, our framework completely bypasses the entity recognition stage (thus no annotation resources for NER needed), and directly tackles event argument extraction. Also related to our work includes  Wadden et al. (2019) , they model the entity/argument spans (with start and end offset) instead of labeling with BIO scheme. Different from our work, their learned span representations are later used to predict the entity/argument type. While our QA model directly extract the spans for certain argument role type. Contextualized representations produced by pre-trained language models  (Peters et al., 2018;  have been proved to be helpful for event extraction  (Zhang et al., 2019b; Wadden et al., 2019)  and question answering  (Rajpurkar et al., 2016) . The attention mechanism helps capture relationships between tokens in question and input sequence. We use BERT in our framework for capturing semantic relationship between question and input sentence. 

 Machine Reading Comprehension (MRC) Span-based MRC tasks involve extracting a span from a paragraph  (Rajpurkar et al., 2016)  or multiple paragraphs  (Joshi et al., 2017; Kwiatkowski et al., 2019) . Recently, there have been explorations on formulating NLP tasks as a question answering problem.  McCann et al. (2018)  propose natural language decathlon challenge (decaNLP), which consists of ten tasks (e.g., machine translation, summarization, question answering, etc.) They cast all tasks as question answering over a context and propose a general model for this. In the information extraction literature,  Levy et al. (2017)  propose the zero-shot relation extraction task and reduce the task to answering crowd-sourced reading comprehension questions.  Li et al. (2019)  casts entity-relation extraction as a multi-turn question answering task. Their questions lack diversity and naturalness. For example for the PART-WHOLE relation, the template questions is "find Y that belongs to X", where X is instantiated with the pre-given entity. The follow-up work from  Li et al. (2020)  propose better query strategies incorporating synonyms and examples for named entity recognition. Different from the works above, we focus on the more complex event extraction task, which involves both trigger detection and argument extraction. Our generated questions for extracting event arguments are more natural (incorporating descriptions from annotation guidelines) and leverage trigger information. Question Generation To generate question templates 2&3 (Type + Role question and annotation guideline based question) which are more natural, we draw insights from literature of automatic rule-based question generation  (Heilman and Smith, 2010) .  Heilman (2011)  propose to use linguistically motivated rules for WH word (question phrase) selection. In their more general case of question generation from sentences, answer phrases can be noun phrases, prepositional phrases, or subordinate clauses. Complicated rules are designed with help from superTagger  (Ciaramita and Altun, 2006) . In our case, event arguments are mostly noun phrases and the rules are simpler -"who" for person, "where" for place and "what" for all other types of entities. We sample around 10 examples from the development set to determine the entity type of each argument role. In the future, it is interesting to investigate how to utilize machine learning-based question generation method  (Du et al., 2017) , which would be more beneficial for schema/ontology containing a large number of event argument types. 

 Conclusion In this paper, we introduce a new paradigm for event extraction based on question answering. We investigate how the question generation strategies affect the performance of our framework on both trigger detection and argument span extraction, and find that more natural questions lead to better performance. Our framework outperforms prior works on the ACE 2005 benchmark, and is capable of extracting event arguments of roles not seen at training time. For future work, it would be interesting to try incorporating broader context (e.g., paragraph/document-level context  (Ji and Grishman, 2008; Huang and Riloff, 2011; Du and Cardie, 2020)  in our methods to improve the accuracy of the predictions. Figure 1 : 1 Figure 1: Event extraction example from the ACE 2005 corpus (Doddington et al., 2004). 
