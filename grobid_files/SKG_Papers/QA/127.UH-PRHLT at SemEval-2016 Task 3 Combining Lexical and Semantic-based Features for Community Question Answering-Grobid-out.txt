title
UH-PRHLT at SemEval-2016 Task 3: Combining Lexical and Semantic-based Features for Community Question Answering

abstract
In this work we describe the system built for the three English subtasks of the Se-mEval 2016 Task 3 by the Department of Computer Science of the University of Houston (UH) and the Pattern Recognition and Human Language Technology (PRHLT) research center -Universitat Polit?cnica de Val?ncia: UH-PRHLT. Our system represents instances by using both lexical and semantic-based similarity measures between text pairs. Our semantic features include the use of distributed representations of words, knowledge graphs generated with the BabelNet multilingual semantic network, and the FrameNet lexical database. Experimental results outperform the random and Google search engine baselines in the three English subtasks. Our approach obtained the highest results of subtask B compared to the other task participants.

Introduction The key role that the Internet plays today for our society benefited the dawn of thousands of new Web social activities. Among those, forums emerged with special relevance following the paradigm of the Community Question Answering (CQA). These type of social networks allow people to post a question to other users of that community. The usage is simple, without much restrictions, and infrequently moderated. The popularity of CQA is a strong indicator that users receive some good and valuable answers. However, there are several issues related to that type of community. First is the large amount of answers received that makes it difficult and time-consuming for users to search and distinguish the good ones. This is exacerbated with the amount of noise that these questions contain. It is not uncommon to have wrong or misguiding answers that produce more unrelated answers, discussions and sub-threads. Finally, there is a lot of redundancy, questions may be repeated or closely related to previously asked questions. Details of the SemEval 2016 Task 3 on CQA can be found in the overview paper  (Nakov et al., 2016) . In this work we evaluate the three English-related Task 3 subtasks on CQA. We first represent each instance to rank -question versus (vs.) comments, question vs. related questions, or question vs. comments of related questions -with a set of similarities computed at two different levels: lexical and semantic. This representation allows us to estimate the relatedness between text pairs in terms of what is explicitly stated and what it means. Our lexical similarities employ representations such as word and character n-grams, and bag-of-words (BOW). The semantic similarities include the use of distributed word bidirectional alignments, distributed representations of text, knowledge graphs, and frames from the FrameNet lexical database  (Baker et al., 1998) . This type of dual representations have been successfully employed for question answering by the highest performing system in the previous edition of this  Se-mEval task (Tran et al., 2015) . Other Natural Language Processing (NLP) tasks such as cross-language document retrieval and categorization also benefited from similar representations  (Franco-Salvador et al., 2014) . In this task, if the question or comment includes multiple text fields, e.g. body and subject, similarities are estimated using all possible combinations (see Section 3.2). Finally, the ranking of instances is performed using a state-of-the-art machine-learned ranking algorithm: SVM rank . 

 Related Work Automatic question answering has been a popular interest of research in NLP from the beginning of the Internet to more recently where voice interfaces have been incorporated  (Rosso et al., 2012) . The use of BOW representations allowed to correctly answer 60% of the questions of the first large-scale question answering evaluation at the TREC-8 Question Answering track  (Voorhees, 1999) . More complex systems used inference rules to connect expressions between questions and answers  (Lin and Pantel, 2001) . Similarly,  Ravichandran and Hovy (2002)  employed bootstrapping to generate surface text patterns in order to successfully answer questions. Other works such as  Buscaldi et al. (2010)  are based on the redundancy of n-grams in order to find one or more text fragments that include tokens of the original question and the answer.  Jeon et al. (2005)  studied the semantic relatedness between texts for question answering. They used translation obfuscation to paraphrase the text and to detect which terms are closer in context. Probabilistic topic models have been also useful for detecting the semantics in this task.  Celikyilmaz et al. (2010)  used Latent Dirichlet Allocation (LDA)  (Blei et al., 2003)  for representing questions by means of latent topics. The previous edition of the SemEval CQA task included two English subtasks  (Nakov et al., 2015) . The first one was focused on classifying answers as good, bad, or potentially relevant with respect to one question. The second subtask answered a question with yes, no, or unsure based on the list of all answers. In addition, the first subtask was also available in Arabic. Several teams experimented with complex solutions that included meta-learning, external resources, and linguistic features such as syntactic relations and distributed word representations. Similarly to our work, the highest performing approach employed a combination of lexical and semanticbased similarity measures  (Tran et al., 2015) . Its semantic features included the use of probabilistic topic models, translation obfuscation-based alignments, and pre-computed distributed representations of words both generated with the word2vec 1 and GloVe 2 toolkits. Their lexical features included BOW, word alignments, and noun matching. They employed a regression model for classification. Another interesting approach,  Hou et al. (2015) , included textual features -word lengths and punctuation -in addition to syntactical-based features -Part-of-Speech (PoS) tags. In this work we aim at differentiating from the other approaches by enhancing our ranking model with new similarity measures. These include the use of knowledge graphs obtained using the largest multilingual semantic network -BabelNet -frames from the FrameNet lexical database, and bidirectional distributed word alignments. 

 Lexical and Semantic-based Community Question Answering In this section we detailed the system that we designed for this CQA task. First in Section 3.1 we described our set of lexical features and semantic-based ones. Next, in Section 3.2 we detail the specific adaptation that we employed for each subtask and the ranking algorithm that we used. We note that all our features are similarity scores obtained with different text similarity measures. More details and examples can be found in their respective papers. 

 Feature Description Our system exploits both the verbatim and the contextual similarities between texts, i.e., questions and comments. In Section 3.1.1 we detailed our lexical and in Section 3.1.2 our semantic-based features. 

 Lexical Features The lexical features that we employed are the following: ? Cosine Similarity. We used cosine similarity to measure lexical similarity between two text snippets. We calculated cosine similarity based on word n-grams(n=1,2), character 3-grams and tf-idf  (Salton and McGill, 1986 ) scores of words. ? Word Overlap. We used the count of common words between two texts. This count was normalized by the length. ? Noun Overlap. We used NLTK 3 to partof-speech tag the text and computed the normalized count of overlapping nouns in two texts as a similarity measure. ? N-gram Overlap. We computed the normalized count of common ngrams(n=1,2,3) between two texts. 

 Semantic Features The semantic features that we employed are the following: ? Distributed representations of texts. We used the continuous Skip-gram model  (Mikolov et al., 2013)  of the word2vec toolkit to generate distributed representations of the words of the complete English Wikipedia.  4  Next, for each text, e.g. question or comment, we averaged its word vectors in order to have a single representation of its content as this setting has shown good results in other NLP tasks (e.g. for language variety identification (Franco-Salvador et al., 2015a) and discriminating similar languages  (Franco-Salvador et al., 2015b) ). Finally, the similarity between texts, e.g. question vs. comment, is estimated using the cosine similarity. ? Distributed word alignments. The use of word alignment strategies has been employed in the past for textual semantic relatedness  (Hassan and Mihalcea, 2011) .  Tran et al. (2015)  employed distributed representations to align the words of the question with the words of the comment. A more recent work introduced the Continuous Word Alignment-based Similarity Analysis (CWASA)  (Franco-Salvador et al., 2016a) . CWASA uses distributed representations to measure the similarity by doubledirection aligning words of texts. In this work we selected as feature the similarity provided by CWASA between questions and comments. ? Knowledge graphs. A knowledge graph is a labeled, weighted, and directed graph that expands and relates the concepts belonging to a text. Knowledge Graph Analysis (KGA)  (Franco-Salvador et al., 2016b ) measures semantic relatedness between texts by means of their knowledge graphs. In this work we used the Babel-Net  (Navigli and Ponzetto, 2012)  multilingual semantic network to generate knowledge graphs from questions and comments, and measured their similarity using KGA. ? Common frames. We used Framenet  (Baker et al., 1998)  to extract the frames associated with the lexical items in the text. For each frame present in the text, we calculated the common lexical items between sentences associated with this frame. The goal is to allow inference of similarity at the level of semantic roles. As additional feature, for Subtasks A and C we also used the ranking provided by the Google search engine for the questions related to the original questions. 

 Data Representation and Ranking Due to the representation of questions -composed by subject and body fields -and answers -a comment field -we adapted our system for the different English subtasks: ? Subtask A (question-comment similarity ranking): we used the aforementioned similarity-based features at three levels: question subject vs. comment, question body vs. comment, and full question vs. comment. ? Subtask B (question-related question similarity ranking): for this subtask we measured the similarities at body, subject, and full question level. ? Subtask C (question-external comment similarity ranking): we employed all the features of Subtasks A and B, plus the similarities of the original questionsubject, body, and full levels -with the related question comments. In order to rank the questions and comments, we selected a variant of Support Vector Machines (SVM)  (Hearst et al., 1998)  optimized for ranking problems: SVM rank  (Joachims, 2002) . In our evaluation of Section 4, we call our system as the combination of the acronyms of our member institutions: UH-PRHLT. Preproscessing steps included stopword removal, lemmatization, and stemming. However, for the distributed representation and knowledge graph-based features we did not employ stemming. These decisions were motivated for performance reasons during our prototyping. Note that each subtask allows to submit three runs per team: primary, contrastive 1 (contr. 1), and contrastive 2 (contr. 2). We used a linear kernel and optimized the cost factor parameter using Bayesian optimization 5 (Snoek, 2013). Our three runs differ only in the value for that parameter and correspond with the three best -and considerably distant -values. In addition to the ranking, the task requires also to provide with a label for each instance that reflects if the question or comment is relevant to the compared question. For each subtask we optimized a threshold to determine the relevance of each instance that is based on our predicted relevance ranking. In other words, we binarize our ranking. 

 Evaluation This section presents the evaluation of the Se-mEval 2016 Task 3 on CQA. Details about this task, the datasets, and the three subtasks can be found in the task overview  (Nakov et al., 2016) . Note that for our system we did not use data from SemEval 2015 CQA as we did not observe gains in performance. We compared the results of our approach with those provided by the random baseline and the Google search engine when ranking the questions and comments.  6  The official measure of the task is the Mean Average Precision (MAP), but we included also two alternative ranking measures: Average Recall (Av-gRec) and Mean Reciprocal Rank (MRR). In addition, we included four classification measures: Accuracy (acc.), Precision (P), Recall (R), and F1-measure (F1). 

 Results and Discussion The best results per partition and subtask are highlighted in bold. In addition, we always refer to the run with the highest performance. Finally, our percentage comparisons use always absolute values. We can see the results of Subtask A (question-comment similarity ranking) in Table  1 . In terms of ranking measures, our system outperformed both the random and the search engine baseline. Using the development set, we observed a MAP improvement of 9.4% compared with the results obtained by the search engine. We can see similar differences with respect to the other two ranking measures. Classification results are also superior. We obtain improvements in accuracy and F1 of 24.9% and 5.2% respectively. These results manifest the potential of the selected lexical and semantic-based features for this subtask. Similar to Subtask A, the performance of our approach has been also superior in Subtask B (question-related question similarity ranking). As we can see in Table  2 , using the development set, the improvement of MAP, AvgRec, and MRR has been of 4.6%, 5%, and 6.4% respectively compared to the search engine baseline. In this case, the similarity between questions was easier to estimate -also for the baselines -and the improvements in performance were slightly reduced. With respect to the classification measures, we outperformed the random baseline with 27.4% and 16.1 % of accuracy and F1-measure respectively. In Table  3  we can see the results of the Subtask C (question-external comment similarity ranking). In this case, we are ranking 100 comments (10 times more compared to the other subtasks). Therefore, this has been the most difficult subtask. However, we obtained improvements in line with those reported for the other subtasks. Compared to the search engine baseline, the MAP, AvgRec, and MRR improved 8.7%, 8.5%, and 7.5% respectively when using the development partition. The accuracy and F1-measure improved 61.5% and 12.2% respectively. The largest number of comments to rank, and the use of top 10 results when measuring results, benefited our approach with this especially high difference in accuracy. After the analysis of results in the three English subtasks, we highlight that the combination of lexical and semantic-based features that we employ in this work offers a competitive performance for the CQA task. This is true also when comparing results with other task participants. Our approach obtained the highest results -with considerable margin (1.04%) -for subtask B. It is worth mentioning that we designed our system for the subtask B and adapted it later for the other tasks. However, for the other two subtasks, we obtained a low ranking position. At this point we have not discovered any coding error that could explain this difference. In addition, we analysed the information gain ratio of the features for the three subtasks. That results showed an average decrease of ?66% for subtasks A and C. Therefore, we conclude that our approach is more adequate for tasks of similarity rather than question answering. That analysis also manifests that the most relevant features are the word n-gram ones followed by the CWASA, distributed representation-based, and knowledgegraph-based ones. The comparison of results of all the submitted systems and task participants can be found in the task overview  (Nakov et al., 2016) . 

 Conclusions In this work we evaluated the three English subtasks of the SemEval 2016 Task 3 on CQA. In order to measure similarities, our proposed approach combined lexical and semantic-based features. We included simple -and effective -representations based on BOW, character and word n-grams. We also employed semantic features which used distributed representations of words to represent documents or to directly measure similarity by means of distributed word bidirectional alignments. The use of knowledge graphs generated with the BabelNet multilingual semantic network has been exploited too. Experimental results showed that our system was able to outperform -with considerably differences -the random and Google search engine baselines in all the evaluated subtasks. In addition, our approach obtained the highest results in subtask B compared to the other task participants. This fact manifests the potential of our combination of lexical and semantic features for the CQA subtask. As future work we will continue studying how to approach CQA with knowledge graphs and distributed representations. In addition, we will further explore how to employ this type of lexical and semantic-based representations for other NLP tasks such as plagiarism detection. Table 1 : 1 Results of Subtask A: English Question-Comment Similarity. (a) Baselines; (b) proposed approach. Ranking measures Classification measures Model MAP AvgRec MRR Acc. P R F1 Development set results (a) Random baseline 0.456 0.654 0.535 0.433 0.344 0.764 0.475 Search engine 0.538 0.728 0.631 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.632 0.812 0.725 0.682 0.526 0.500 0.513 UH-PRHLT (contr. 1) 0.630 0.811 0.722 0.672 0.510 0.545 0.527 UH-PRHLT (contr. 2) 0.630 0.810 0.722 0.674 0.514 0.522 0.518 Test set results (a) Random baseline 0.528 0.665 0.587 0.525 0.452 0.405 0.428 Search engine 0.595 0.726 0.678 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.674 0.794 0.770 0.632 0.556 0.468 0.508 UH-PRHLT (contr. 1) 0.676 0.795 0.771 0.624 0.541 0.501 0.520 UH-PRHLT (contr. 2) 0.673 0.793 0.767 0.630 0.550 0.491 0.520 Ranking measures Classification measures Model MAP AvgRec MRR Acc. P R F1 Development set results (a) Random baseline 0.559 0.732 0.622 0.488 0.443 0.766 0.562 Search engine 0.713 0.861 0.766 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.759 0.911 0.830 0.762 0.721 0.724 0.723 UH-PRHLT (contr. 1) 0.757 0.911 0.830 0.758 0.712 0.729 0.721 UH-PRHLT (contr. 2) 0.755 0.910 0.817 0.758 0.714 0.724 0.719 Test set results (a) Random baseline 0.470 0.679 0.510 0.452 0.404 0.326 0.361 Search engine 0.747 0.883 0.838 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.767 0.903 0.830 0.766 0.635 0.695 0.664 UH-PRHLT (contr. 1) 0.766 0.902 0.830 0.763 0.627 0.708 0.665 UH-PRHLT (contr. 2) 0.773 0.908 0.840 0.767 0.636 0.704 0.668 

 Table 2 : 2 Results of Subtask B: English Question-Question Similarity. (a) Baselines; (b) proposed approach. Ranking measures Classification measures Model MAP AvgRec MRR Acc. P R F1 Development set results (a) Random baseline 0.138 0.096 0.160 0.284 0.070 0.759 0.128 Search engine 0.306 0.346 0.360 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.383 0.413 0.420 0.894 0.242 0.252 0.247 UH-PRHLT (contr. 1) 0.383 0.421 0.425 0.897 0.252 0.249 0.250 UH-PRHLT (contr. 2) 0.383 0.419 0.435 0.899 0.251 0.232 0.241 Test set results (a) Random baseline 0.150 0.114 0.152 0.167 0.296 0.094 0.143 Search engine 0.404 0.460 0.459 n/a n/a n/a n/a (b) UH-PRHLT (primary) 0.432 0.480 0.478 0.886 0.376 0.342 0.359 UH-PRHLT (contr. 1) 0.434 0.480 0.484 0.888 0.386 0.327 0.354 UH-PRHLT (contr. 2) 0.433 0.480 0.484 0.888 0.382 0.327 0.353 

 Table 3 : 3 Results of Subtask C: English Question-External Comment Similarity. (a) Baselines; (b) proposed approach. 

			 https://code.google.com/archive/p/word2vec/ 2 http://nlp.stanford.edu/projects/glove/ 

			 http://www.nltk.org/ 4 We used 200-dimensional vectors, context windows of size 10, and 20 negative words for each sample. 

			 We used the Spearmint toolkit: https://github. com/HIPS/Spearmint 

			 Some considerations about the evaluation: these subtasks employed binary classification. At testing time, Bad and PotentiallyUseful are both considered false. The same occurs with PerfectMatch and Relevant, which are both considered true. In addition, following the rules of the task, the employed measures used only the top 10 ranked instances.
