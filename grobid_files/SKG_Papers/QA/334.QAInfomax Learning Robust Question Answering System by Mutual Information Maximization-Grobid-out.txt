title
QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization

abstract
Standard accuracy metrics indicate that modern reading comprehension systems have achieved strong performance in many question answering datasets. However, the extent these systems truly understand language remains unknown, and existing systems are not good at distinguishing distractor sentences, which look related but do not actually answer the question. To address this problem, we propose QAInfomax as a regularizer in reading comprehension systems by maximizing mutual information among passages, a question, and its answer. QAInfomax helps regularize the model to not simply learn the superficial correlation for answering questions. The experiments show that our proposed QAInfomax achieves the state-of-the-art performance on the benchmark Adversarial-SQuAD dataset 1 .

Introduction Question answering tasks are widely used for training and testing machine comprehension and reasoning  (Rajpurkar et al., 2016; Joshi et al., 2017) . However, high performance in standard automatic metrics has been achieved with only superficial understanding, as models exploit simple correlations in the data that happen to be predictive on most test examples.  Jia and Liang (2017)  addressed this problem and proposed an adversarial version of the SQuAD dataset, which was created by adding a distractor sentence to each paragraph. The distractor sentences challenge the model robustness, and the created Adversarial-SQuAD data shows the inability of a model about distinguishing a sentence that actually answers the question from one that merely has words in common with it, where almost all state-of-the-art machine comprehension systems are significantly degraded on adversarial examples.  Lewis and Fan (2018)  argued that over-fitting to superficial biases is partially caused by discriminative loss functions, which saturate when simple correlations allow the question to be answered confidently, leaving no incentive for further learning on the example. Therefore, they designed generative QA models, which use a generative loss function in question answering instead, and showed the improvement on Adversarial-SQuAD. Instead of regularizing models by generative loss functions, we propose an alternative approach named "QAInfomax" by maximizing mutual information (MI) among passages, questions, and answers, aiming at helping models be not stuck with superficial biases in the data during learning. To efficiently estimate MI, QAInfomax incorporates the recently proposed deep infomax (DIM) in the model , which was proved effective in learning representations for image, audio  (Ravanelli and Bengio, 2018) , and graph domains  (Veli?kovi? et al., 2018) . In this work, the proposed QAInfomax further extends DIM to the text domain, and encourages the question answering model to generate answers carrying information that can explain not only questions but also itself, and thus be more sensitive to distractor sentences. Our contributions are summarized: ? This paper first attempts at applying DIMbased MI estimation as a regularizer for representation learning in the NLP domain. ? The proposed QAInfomax achieves the stateof-the-art performance on the Adversarial-SQuAD dataset without additional training data, demonstrating its better robustness. 

 Mutual Information (MI) Estimation In this section, we introduce how scalable estimation of mutual information is performed in terms of practical scenarios via mutual information neural estimation (MINE)  (Belghazi et al., 2018)  and the deep infomax (DIM)  described below. The mutual information between two random variable X and Y is defined as: MI(X, Y ) = D KL (p(X, Y ) k p(X)p(Y )), where D KL is the Kullback-Leibler (KL) divergence between the joint distribution p(X, Y ) and the product of marginals p(X)p(Y ). MINE estimates mutual information by training a classifier to distinguish between positive samples (x, y) from the joint distribution and negative samples (x, ?) from the product of marginals. Mutual information neural estimation (MINE) uses Donsker-Varadhan representation (DV)  (Donsker and Varadhan, 1983)  as a lower-bound to estimate MI. 

 MI(X, Y ) E P [g(x, y)] log(E N [e g(x,?) ]), where E P and E N denote the expectation over positive and negative samples respectively, and g is the discriminator function that outputs a real number modeled by a neural network. While the DV representation is the strong bound of mutual information shown in MINE, we are primarily interested in maximizing MI but not focusing on its precise value. Thus DIM proposes an alternative estimation using Jensen-Shannon divergence (JS), which can be efficiently implemented using the cross-entropy (BCE) loss: MI(X, Y ) E P [log(g(x, y))] (1) + E N [log(1 g(x, ?))]. While two representations should behave similarly, considering that both act like classifiers with objectives maximizing the expected log-ratio of the joint over the product of marginals, it is found that the BCE loss empirically works better than the DV-based objective  Ravanelli and Bengio, 2018; Veli?kovi? et al., 2018) . The reason may be that the BCE loss is bounded (i.e., its maximum is zero), making the convergence of the network more numerically stable. In our experiments, we primarily use the JS representation to estimate mutual information. Recently,  Tian et al. (2019)  showed strong empirical performance through the improved multiview CPC training  (Oord et al., 2018) , which shares many common ideas as mutual information maximization. Inspired by their work, we modify (1) by first switching the role of x and y and summing them up: MI(X, Y ) E P [log(g(x, y))] (2) + 1 2 E N [log(1 g(x, ?))] + 1 2 E N [log(1 g(x, y))], where (x, y) is also the negative sample sampled from the product of marginals. We empirically find that (2) gives the best performance, and more exploration about parameterization of MI is left as our future work. 

 Methodology In the extractive question answering dataset like SQuAD, the answer A = {a 1 , . . . , a M } to the question Q = {q 1 , . . . , q K } is guaranteed to be the span {p m , . . . , p m+M } in the paragraph P = {p 1 , . . . , p N }. Given Q and P , the encoded representations from the QA system M can be formulated as: {r q , r p } = {r q 1 , . . . , r q K , r p 1 , . . . , r p N } = M (Q, P ) , where r q and r p are representations of the question and the passage respectively after the reasoning process in the QA system M . Most models then feed the passage representation r p to a single-layer neural network, obtain the span start and end probabilities for each passage word, and compute the loss L span , which is the negative sum of log probabilities of the predicted distributions indexed by true start and end indices. Our QAInfomax aims at regularizing the QA system M to not simply exploit the superficial biases in the dataset for answering questions. Therefore, two constraints are introduced in order to guide the model learning. Intuitively, the model is expected to choose the answer span after fully considering the entire question and paragraph. However, traditional QA models suffered the overstability problem, and tended to be fooled by distractor answers, such as the one containing an unrelated human name. As  Lewis and Fan (2018)  argued, we also believe that the main reason is that QA models are only trained to predict start and end positions of answer spans. Correlation in the dataset allows QA models to find shortcuts and ignore what the answer span looks like. A learned behavior of traditional QA models can be viewed as a simple pattern matching, such as choosing the 5-length span after the word "river" if a question is about a river and the context talks about countries in European. Following the intuition, two constraints LC and GC are introduced to guide models to learn the desired behaviors. To prevent the model from only learning to match some specific word patterns to find the answer, LC forces the model to generate answer span representations while maximizing mutual information among words in the span and the context words surrounding the span. By maximizing the mutual information between an answer word and all of its context words, models need to incorporate the entire context into its decision process while choosing answers, and thus can be more robust to the adversarial sentences. Then we further require models to maximize mutual information among answer words, so models can no longer ignore any word in the chosen answer span. On the other hand, different from LC , which only focuses on the answer span and its context, GC pushes the model to prefer answer representations carrying information that is globally shared across the whole input conditions Q and P , because shortcuts do not necessarily appear near to the answer. If the model only learns to leverage the correlation specific to the partial input, the MI of any input word without such relationship would not increased. The overview about two proposed constraints is illustrated in Figure  1 . The detail of two constraints and our QAInfomax regularizer is described below. 

 Local Constraint As shown in Section 2, the maximization of MI needs positive samples and negative samples drawn from joint distribution and the product of marginal distribution respectively. In LC , because all answer word representations are expected to carry the information of each other and their contexts, we choose to maximize averaged MI between the sampled answer word representations and the whole answer sequence with its context words. Specifically, a positive sample is obtained by pairing the sampled answer word representation x 2 r a = {r p m , . . . , r p m+M } to all other answer and context words r c = {r p m C , . . . r p m+M +C } \ {x}, where C is the hyperparameter defining how many context words for consideration. Negative samples, on the other hand, are obtained by randomly sampling answer representation ra = {r p l , . . . , rp l+L } and the corresponding rc from other training examples. Following (2), the objective for sampled x, r c , x 2 ra and rc is formulated. LC(x, r c , x, rc ) = 1 |r c | X r c i 2r c log(g(x, r c i )) (3) + 1 2|r c | X rc j 2r c log(1 g(x, rc j )) + 1 2|r c | X r c i 2r c log(1 g(x, r c i )). 

 Global Constraint Different from LC described above, GC forces the learned answer representations r a to have information shared with all other question and passage representations. Here, we maximize the mutual information between the summarized answer vector s = S(r a ) and r l 2 r = {r q , r p } \ {r a } pairs. In 

 Model Original ADDSENT ADDONESENT BiDAF-S  (Seo et al., 2016)  75.5 34.3 45.7 ReasoNet-S  78.2 39.4 50.3 Reinforced Mnemonic Reader-S  (Hu et al., 2017)  78.5 46.6 56.0 QANet-S  (Yu et al., 2018)  83.8 45.2 55.7 GQA-S  (Lewis and Fan, 2018)  83 the experiments, we use S(r a ) = (  1 M P r a i ) as our summarization function, where is the logistic sigmoid nonlinearity. Specifically, a positive sample here is the pair of a answer summary vector s = S(r a ) and all other word representations in r. Negative samples are provided by sampling question, passage and answer representations {r q , rp , ra } from an alternative training example. Then we pair the summary s with r = {r q , rp } \ {r a }, and s = S(r a ) with r. Similar to (3), the objective for the sampled s, r, s and r is: GC(s, r, s, r) = 1 |r| X r i 2r log(g(s, r i )) (4) + 1 2|r| X rj 2r log((1 g(s, rj ))) + 1 2|r| X r i 2r log((1 g(s, r i ))). 

 QAInfomax In our proposed model, we combine two objectives and formulate the model as the complete QAInfomax regularizer. For each training batch consisting of training examples {{Q 1 , P 1 , A 1 }, . . . {Q B , P B , A B }}, we pass the batch into the model M and obtain representations {{r q 1 , r p 1 , r a 1 }, . . . , {r q B , r p B , r a B }}. Note that we abuse the subscripts to denote the example index in the batch for simplicity. Then we shuffle the whole batch to obtain negative examples {{r q 1 , rp 1 , ra 1 }, . . . , {r q B , rp B , ra B }}. The complete objective L inf o for QAInfomax becomes: 1 B B X i=1 (?LC(x i , r c i , xi , rc i ) + GC(s i , r i , ri )), where x i and xi are the representation sampled from r a i and ra i , r c i and rc i are r a i and ra i expanded with its context words respectively, s i and si are the summary vectors of r a i and ra i , and ? and are hyperparameters. Combined with QAInfomax as a regularizer, the final objective of the model becomes L = L span + L inf o , (5) where L span is the answer span prediction loss and is the regularize strength. The objective can be optimized through the simple gradient descent. 

 Experiments To evaluate the effectiveness of the proposed QAInfomax, we conduct the experiments on a challenging dataset, Adversarial-SQuAD. 

 Setup BERT-base  (Devlin et al., 2018)  is employed as our QA system M in the experiments, where we set the same hyperparameters as one released in SQuAD training 2 . We set C, ?, and to be 5, 1, 0.5, 0.3 respectively in all experiments, and add the proposed QAInfomax into the BERT model as described above. The discriminator function g is the bilinear function similar to the scoring used by  Oord et al. (2018) : g(x, y) = x T W y, ( 6 ) where W is a learnable scoring matrix. We train the BERT model with the proposed QAInfomax on the orignal SQuAD dataset, and straints are both important for achieving such results. We also show the training speed of the proposed method and its limitation, where the GC objective degrades the training speed by 28%. The reason is that GC measures the averaged MI over the whole question and passage representations, which may include a long sequence of vectors. Considering that the summarization function S plays an important role in GC, we explore its different variants in Table  3 : ? Mean: ( 1 M P r a i ) ? Max: (maxpool(r a )) ? Sample: randomly sample one r a i 2 r a According to the experimental results, Mean performs the best while Max and Sample has the competitive performance, showing the great robustness of the proposed methods to different architecture choices. 

 Conclusion This paper presents a novel regularizer based on MI maximization for question answering systems named QAInfomax, which helps models be not stuck with superficial correlation in the data and improves its robustness. The proposed QAInfomax is flexible to apply to different machine comprehension models. The experiments on Adversirial-SQuAD demonstrate the effectiveness of our model, and the augmented model achieves the state-of-the-art results. In the future, we will investigate more methods for reducing the limitations of QAInfomax and improving the capability of generalization in QA systems. 1.Figure 1 : 1 Figure 1: Illustration of the LC and GC. 
