title
Is it Dish Washer Safe? Automatically Answering "Yes/No" Questions using Customer Reviews

abstract
It has become commonplace for people to share their opinions about all kinds of products by posting reviews online. It has also become commonplace for potential customers to do research about the quality and limitations of these products by posting questions online. We test the extent to which reviews are useful in question-answering by combining two Amazon datasets, and focusing our attention on yes/no questions. A manual analysis of 400 cases reveals that the reviews directly contain the answer to the question just over a third of the time. Preliminary reading comprehension experiments with this dataset prove inconclusive, with accuracy in the range 50-66%.

Introduction Consumers often carry out online research about a product before purchasing. This can take the form of reading consumer reviews and/or asking specific questions on online fora. In this paper we ask whether a question-answering (QA) system can utilize the information in consumer reviews when answering yes/no questions about a product. We compile a dataset of questions about Amazon products together with consumer reviews of the same products, and manually analyse a sample of 100 questions from four domains. We find that the reviews contain the answer in only 45% of cases. In 36% of cases, the answer is directly expressed in at least one of the reviews, and 9% of the time, it is indirectly expressed. This suggests that reviews can sometimes be useful and so we go on to experiment with QA systems that use the reviews in addition to the question. We focus on yes/no questions. Being able to answer these is not only an indicator of whether reviews will be useful for other question types but is also a signal of how much comprehension is actually taking place. In our preliminary experiments with three domains from this new dataset, we compare systems which attempt to answer a yes/no question based on the question alone to those that also use related reviews. We experiment with two methods for selecting relevant sentences from the reviews, and with various representations for encoding the questions and reviews including bag-ofwords, word2vec  (Le and Mikolov, 2014) , ELMO  (Peters et al., 2018) , and BERT  (Devlin et al., 2018) . On the development set, our systems tend to outperform the chance baseline but not by a large margin -our development set results range from 50 to 66%. Over the three domains, we also find that the question-only systems tend to perform as well as and sometimes outperform those which also use the reviews, suggesting that separating the answers from the noise in these reviews is not straightforward. 

 Related Work A number of studies have explored the use of customer reviews in retrieval and question answering. Using Amazon data,  Yu et al. (2018)  develop a framework which returns a ranked list of sentences from reviews or existing question-answer pairs for a given question.  Xu et al. (2019)  create a new dataset comprising Amazon laptop reviews and questions and Yelp restaurant reviews and questions, where reviews are used to answer questions in multiple-turn dialogue form.  Bogdanova et al. (2017)  and  Bogdanova and Foster (2016) ) do not use review data but also focus on QA over usergenerated content, attempting to find similar questions or rank answers in user fora. We use the same Amazon data as  Yu et al. (2018)  but consider a wider set of domains (they consider only two), and attempt to directly answer yes/no questions. To the best of our knowledge, the novelty in our work lies in trying to directly answer customer questions using user-generated reviews. Unlike popular Reading Comprehension datasets such as MovieQA  (Tapaswi et al., 2016)  and SQuAD  (Rajpurkar et al., 2018 (Rajpurkar et al., , 2016 , which are created by crowdsourcing, we work with authentic user-generated data. This means that the data is collected from sources where users spontaneously created content for their own purposes. Since there is no guarantee that reviews contain text related to the question, there is no span data that can be reliably used to provide the answer. This, together with the considerable volume of review text, contributes to the difficulty of the task. 

 Data We work with two Amazon datasets: the first,  He and McAuley (2016) , 1 is a collection of product reviews from 24 domains. The second,  Wan and McAuley (2016) ;  McAuley and Yang (2016) , contains questions and answers about products from 21 domains. These two datasets have 17 domains in common and can be matched using the Amazon Standard Identification Number  (ASIN) . In order to obtain data with reviews, questions and answers, we first select all those products which contain reviews and questions, focusing on yes/no questions. We observe that the majority of questions can be answered "Yes" (65-75% depending on the domain), so we balance the data by selecting an equal amount of yes/no questions. This results in 80391 questions about 40806 products -see Table  1  for more details. All data is fully user-generated except the answer tags which are provided by  McAuley and Yang (2016) . An example of the combined data is shown below (we keep the original spelling). Reviews (R): ...I was a little surprised at how much time it took to assemble. There were alot of the smaller parts that I would have assumed pre-assembled that weren't... The authentic user-generated nature of this dataset makes it significantly different from other 1 More details here: http://jmcauley.ucsd.edu/ data/amazon/ -last verified (l.v.) 02/20119 2 To save space we provide only part of user review reading comprehension datasets. Table  2  shows a comparison with MovieQA and SQuAD2.0. The length of questions is almost the same (10-11.5 words) in all three datasets, although the number of instances (movie plots for MovieQA, topic for SQuAD and product for Amazon) is significantly bigger. Moreover, the average length of context in the Amazon data is unquestionably larger than in the MovieQA and SQuAD: 188 vs 35 and 5 sentences, and 3265 vs 728 and 117 words. To better understand the nature of questions we carried out some additional analysis looking into question formulation. 21% of questions are formulated with more than one sentence (16-31% depending on the domain), more than 25% of questions (  20686 ) start with the word Does, and more than 15% (>12000) with Can, Is or Will. 

 Do reviews contain the answers? According to  Kaushik and Lipton (2018)  reading comprehension datasets are not studied enough in terms of difficulty. We conduct a manual analysis to better understand the relationship between questions and reviews, to assess the feasibility of using user reviews to answer user questions and to estimate an upper bound on system performance. 100 questions from four domains are analysed. We define seven classes of questions: Easy: Questions are clearly answered in the reviews, e.g. (1) R: ... I used two of these, one for each side of the bed. Q: can this product be used if 2 bed rails are needed for one bed? (AT: Yes) Error: Questions where the answer tag contradicts the user-provided answer, e.g. ( Real-world: Questions where the review does not contain the answer but where an educated guess can be made using common sense or realworld knowledge, e.g. (4) Q: Can I use the cloth to clean the keys on my clarinet? (AT: Yes) (5) Q: Has anyone traveled with this stroller on an airplane? (AT: No) Opinion: Questions which can be answered differently based on different reviews (6) or when the answer and review contain contradictory information (  7 ). Often such questions ask for an opinion, so the answer depends on the user providing it, e.g. (6) R: ...all in all these pans are worthless ...so many folks have had a horrid experience!!! ...At $15.00, it's a good pan for my purposes ...This pan is awesome for the price 3 Q: is this item any good? (AT: No ) (7) R: ...but it seems to get a little hot and makes a plastic noise under the sheet...  3  The sentences are taken from different reviews. Q: does it make noise when baby moves around? A: No not with a sheet on it. (AT: No ) Unrelated: Questions which are asked not about the product but about service and delivery, e.g. (8) Q: Is there a warranty when you buy it from amazon? No answer: Questions which cannot be answered without additional information, i.e. reviews do not contain the required information. The indirect and real-world classes can be considered to be difficult questions. However, in general, we believe that the easy, indirect and realworld question classes can be answered without resorting to guessing. Detailed information is provided in Table  3 . Around 53.5% of questions can be answered (36.5% are easy and 17% are difficult). Although it is difficult to conclude too much from this sample of 400, we can roughly estimate that the best performance we could expect from an automatic QA system would be around 77%. This means the Table  3 : Selection of 100 questions from 4 domains for manual analysis. The last column contains the percentage of the analysed questions from each domain (eg. 100 is 4.6% of the Baby question data, 3.6% of Beauty, etc. ). system answers all answerable questions correctly (53.5%) and guesses half of those questions which cannot be answered (23.25%). 5 Preliminary Experiments 

 Approach In order to establish some baselines on this dataset and task, we carry out preliminary binary classification experiments with a sample of the domains. There are three aspects to the systems we evaluate: Text Representations To represent questions and reviews we experiment with simple Bag of words (BOW), word2vec (Le and Mikolov, 2014), Deep contextualized word representations (ELMO)  (Peters et al., 2018)  4 and Bidirectional Encoder Representations from Transformers (BERT)  (Devlin et al., 2018) . 5 Review Filtering The reviews in our dataset are long compared to the answer passages used in other reading comprehension tasks -see Tables  1  and 2 .  Pascanu et al. (2012)  report that long sequences are hard to process from both time and resource perspectives with sequence-to-sequence models. Therefore, rather than using the full text of the reviews, we use string similarity to select only those sentences that are likely to be relevant to the question. We base our selection method on our previous work which achieved state-of-theart performance on the MovieQA reading comprehension task  Tapaswi et al., 2016) . Review sentences are compared to the question using cosine similarity of tf-idf representations, bag of words overlap, character ngrams, and window slide. Two sets of sentences are extracted. The first one is based on sentence union, in other words, all sentences which have been marked as relevant by any of the metrics are selected. The second one is based on sentence intersection, i.e. only sentences which have been marked as relevant by more than one similarity metric are selected. Binary Classifier Following our previous work  we use logistic regression with the bag-of-word, ELMO and word2vec representations. In the BERT experiments we add a softmax classification layer on top of the final hidden state of the transformer. 

 Experimental Setup We compare systems which use the review and question text to systems which just use the question text. We select three of the four domains used for manual analysis.We exclude Clothing Shoes & Jewellery due to the small number of questions. Table  4  represents the number of questions in the training, development and test set and the ratio of "Yes" and "No" questions in each of them.  6  The evaluation metric is accuracy. For the bow and word2vec experiments, we normalize the text and remove stop words. We use a pre-trained Google News word2vec model. Every text is encoded as a sum of its word vectors and normalized. For the ELMO representation we average three layers of ELMO output and represent a sentence as concatenation of its words vectors. In the question-only BERT experiment, we perform single-sentence classification  (Devlin et al., 2018, Fig. 3b ). In the experiments where the reviews are used, we perform sentence 7 pair classification where the question is the first sentence and the review text the second  (Devlin et al., 2018, Fig. 3a ). We use the pretrained models B base and B large . Both of them are uncased.   

 Results The development set results are shown in Table  5 . Apparently, questions themselves provide some information and help find the correct answer: two out of three domains show the best performance using the question only. Only the Home and Kitchen domain shows better performance with the question+review systems. When selecting sentences from the reviews, there is no clear winner between the intersection and union methods. It varies according to method and domain. BERT, the base and large models, perform better then logistic regression on the development set so we apply the best question-only and ques-tion+review models to the test set (Table  6 ). Performance drops below chance for two domains. It remains to be seen why we are seeing this unstable performance. 

 Conclusions We introduce a fully user-generated reading comprehension dataset by composing two existing datasets into a new one designed to addresses yes/no questions about products using reviews. All data in this work is substantial and comes from real users. We provide a preliminary analysis of data and show that reviews can, to some extent, be used to answer yes/no questions . We build several baseline systems. Although performance does not reach our estimated upper bound of 77%, our results show that they are doing more than mere majority classification. The relatively good performance of the question-only systems leads us to believe that the systems are applying closed-world assumptions by associating terms in the training set questions with terms in the test set questions. Each of the components of our systems can be replaced or improved. Our immediate next step is to investigate more closely the part of the system which selects relevant sentences from the reviews. 2 Question (Q): Does it come assembled Answer (A): No, count on, at least an hour to assemble. (Answer Tag (AT): No) 

 Table 1 : 1 Balanced yes/no dataset statistics per domain: Number of products (P) which have yes/no questions, number of questions (# Question), count of sentences in questions (S), total number of words in questions (W), total number of reviews (# Reviews), all number of sentences in reviews, total number of words in reviews. Indirect: Questions which can be indirectly an- swered by the review, e.g. (3) R: ... it doesn't give an exact voltage and maxes out at 12.7 volts 2) Q: Can you mount this upside down i.e. The receiver on top of the bumper? A: I don't see why not, the is nothing preventing you. (AT: No) Q: Can this be used to charge a 48v battery? (AT: No) Q: Is this a good charger/jump starter for a 12v deep cell battery? (AT: Yes) 

 Table 2 : 2 Comparison of balanced yes/no dataset with MovieQA and SQuAD2. 0: Number of instances (I): articles, movie plots, or products; Number of text passages (T): context, reviews, or plots; Average number of words in the question (AVG W in Q), sentences in the text (AVG S in T), and words in the text (AVG W in T) 

 Table 4 : 4 Domain split of the training, development and test sets using the number of questions and the ratio of "Yes" and "No" questions in each of them. Domain All Training Yes % No % All Development Yes % No % All Test Yes % No % Home & Kitchen 8502 50.02 49.98 1688 50.00 50.00 1813 49.92 50.08 Beauty 1965 49.21 50.79 383 51.17 48.83 415 52.77 47.23 Baby 1542 50.26 49.74 300 48.00 52.00 321 50.78 49.22 Baby Beauty Home and Kitchen Method Q Only Q + Review Q Only Q + Review Q Only Q + Review Intersection Union Intersection Union Intersection Union LR bow 65.33 58.33 59.66 62.14 59.53 59.01 58.17 55.27 55.50 LR w2 v 59.33 60.67 59.66 57.44 55.09 56.40 59.03 55.69 57.17 LR elmo 53.33 56.99 56.99 60.83 60.57 55.35 52.37 49.17 48.93 B base 65.66 55.67 60.00 64.75 50.91 60.05 61.67 64.04 63,21 B large 52.00 63.00 48.00 48.82 62.92 64.23 50.00 62.20 63.68 

 Table 5 : 5 Results on development set of Logistic Regression (LR) applied to bag of word (bow), word2vec (w2v) and ELMO (elmo) representations, and BERT models (B base and B large) for 3 domains. The best question-only (Q only) and question+review (Q+Review) systems are in bold. Model Data Accuracy Baby B base Question Only 64.17 B large Inter Question+Review 49.22 Beauty B base Question Only 64.41 B large U nion Question+Review 47.22 Home and Kitchen B base Question Only 58.57 B base Inter Question+Review 60.23 

 Table 6 : 6 Results on test set with best-scoring question and review+question systems on development set. 

			 We use https://github.com/allenai/allennlp -l.v. 04/20195  We use https://github.com/huggingface/pytorchpretrained-BERT -l.v. 04/2019    

			 Although data is balanced, we divide the dataset by products so some fluctuation between the percentage of "Yes" and "No" questions is possible.7  We use the word "sentence" in the same way as Devlin et al. (2018)
