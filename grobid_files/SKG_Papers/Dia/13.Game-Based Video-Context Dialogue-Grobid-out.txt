title
Game-Based Video-Context Dialogue

abstract
Current dialogue systems focus more on textual and speech context knowledge and are usually based on two speakers. Some recent work has investigated static image-based dialogue. However, several real-world human interactions also involve dynamic visual context (similar to videos) as well as dialogue exchanges among multiple speakers. To move closer towards such multimodal conversational skills and visually-situated applications, we introduce a new video-context, many-speaker dialogue dataset based on livebroadcast soccer game videos and chats from Twitch.tv. This challenging testbed allows us to develop visually-grounded dialogue models that should generate relevant temporal and spatial event language from the live video, while also being relevant to the chat history. For strong baselines, we also present several discriminative and generative models, e.g., based on tridirectional attention flow (TriDAF). We evaluate these models via retrieval ranking-recall, automatic phrasematching metrics, as well as human evaluation studies. We also present dataset analyses, model ablations, and visualizations to understand the contribution of different modalities and model components.

Introduction Dialogue systems or conversational agents which are able to hold natural, relevant, and coherent interactions with humans have been a long-standing goal of artificial intelligence and machine learning. There has been a lot of important previous work in this field for decades  (Weizenbaum, 1966; Isbell et al., 2000; Rambow et al., 2001; Rieser et al., 2005; Georgila et al., 2006; Rieser and Lemon, 2008; Ritter et al., 2011), includ- We release all data, code, and models at:  ing recent work on introduction of large textualdialogue datasets (e.g.,  Lowe et al. (2015) ;  Serban et al. (2016) ) and end-to-end neural network based models  (Sordoni et al., 2015; Vinyals and Le, 2015; Su et al., 2016; Luan et al., 2016; Li et al., 2016; Serban et al., 2017a,b) . Current dialogue tasks are usually focused on the textual or verbal context (conversation history). In terms of multimodal dialogue, speechbased spoken dialogue systems have been widely explored  (Eckert et al., 1997; Young, 2000; Janin et al., 2003; Celikyilmaz et al., 2017; Wen et al., 2015; Su et al., 2016; Mrk?i? et al., 2016) , as well as work on gesture and haptics based dialogue  (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008) . In order to address the additional advantage of using visually-grounded context knowledge in dialogue, recent work introduced the visual dialogue task  (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017) . However, the visual context in these tasks is lim-ited to one static image. Moreover, the interactions are between two speakers with fixed roles (one asks questions and the other answers). Several situations of real-world dialogue among humans involve more 'dynamic' visual context, i.e., video-style information of the world moving around us (both spatially and temporally). Further, several human conversations involve more than two speakers, with changing roles. In order to develop such dynamically-visual multimodal dialogue models, we introduce a new 'manyspeaker, video-context chat' testbed, along with a new dataset and models for the same. Our dataset is based on live-broadcast soccer (FIFA-18) game videos from the 'Twitch.tv' live video streaming platform, along with the spontaneous, many-speaker live chats about the game. This challenging testbed allows us to develop dialogue models where the generated response is required to be relevant to the temporal and spatial events in the live video, as well as be relevant to the chat history (with potential impact towards videogrounded applications such as personal assistants, intelligent tutors, and human-robot collaboration). We also present several strong discriminative and generative baselines that learn to retrieve and generate bimodal-relevant responses. We first present a triple-encoder discriminative model to encode the video, chat history, and response, and then classify the relevance label of the response. We then improve over this model via tridirectional attention flow (TriDAF). For the generative models, we model bidirectional attention flow between the video and textual chat context encoders, which then decodes the response. We evaluate these models via retrieval ranking-recall, phrasematching metrics, as well as human evaluation studies. We also present dataset analysis as well as model ablations and attention visualizations to understand the contribution of the video vs. chat modalities and the model components. 

 Related Work Early dialogue systems had components of natural language (NL) understanding unit, dialogue manager, and NL generation unit  (Bates, 1995) . Statistical learning methods were used for automatic feature extraction  (Dowding et al., 1993; Mikolov et al., 2013) , dialogue managers incorporated reward-driven reinforcement learning  (Young et al., 2013; Shah et al., 2016) , and the generation units have been extended with seq2seq neural network models  (Vinyals and Le, 2015; Serban et al., 2016; Luan et al., 2016) . In addition to the focus on textual dialogue context, using multimodal context brings more potential for having real-world grounded conversations. For example, spoken dialogue systems have been widely explored  Gurevych and Strube, 2004; Georgila et al., 2006; Eckert et al., 1997; Young, 2000; Janin et al., 2003; De Mori, 2007; Wen et al., 2015; Su et al., 2016; Mrk?i? et al., 2016; Hori et al., 2016; Celikyilmaz et al., 2015 Celikyilmaz et al., , 2017 , as well as gesture and haptics based dialogue  (Johnston et al., 2002; Cassell, 1999; Foster et al., 2008) . Additionally, dialogue systems for digital personal assistants are also well explored  (Myers et al., 2007; Sarikaya et al., 2016; Damacharla et al., 2018) . In the visual modality direction, some important recent attempts have been made to use static image based context in dialogue systems  (Das et al., 2017; de Vries et al., 2017; Mostafazadeh et al., 2017) , who proposed the 'visual dialog' task, where the human can ask questions on a static image, and an agent interacts by answering these questions based on the previous chat context and the image's visual features. Also,  Celikyilmaz et al. (2014)  used visual display information for on-screen item resolution in utterances for improving personal digital assistants. In contrast, we propose to employ dynamic video-based information as visual context knowledge in dialogue models, so as to move towards video-grounded intelligent assistant applications. In the video+language direction, previous work has looked at video captioning  (Venugopalan et al., 2015)  as well as Q&A and fill-inthe-blank tasks on videos  (Tapaswi et al., 2016; Jang et al., 2017; Maharaj et al., 2017)  and interactive 3D environments  (Das et al., 2018; Yan et al., 2018; Gordon et al., 2017; Anderson et al., 2017) . There has also been early related work on generating sportscast commentaries from simulation (RoboCup) soccer videos represented as non-visual state information  (Chen and Mooney, 2008) . Also,  Liu et al. (2016a)  presented some initial ideas on robots learning grounded task representations by watching and interacting with humans performing the task (i.e., by converting human demonstration videos to Causal And-Or graphs). On the other hand, we propose a new video-chat dataset where the dialogue models need to generate the next response in the sequence of chats, conditioned both on the raw video features as well as the previous textual chat history. Moreover, our new dataset presents a many-speaker conversation setting, similar to previous work on meeting understanding and Computer Supported Cooperative Work (CSCW)  (Janin et al., 2003; Waibel et al., 2001; Schmidt and Bannon, 1992) . In the live video stream direction,  Fu et al. (2017)  and Ping and Chen (2017) used real-time comments to predict the frame highlights in a video, and  Barbieri et al. (2017)  presented emotes and troll prediction. 3 Twitch-FIFA Dataset 

 Dataset Collection and Processing For our new video-context dialogue task, we used the publicly accessible Twitch.tv live broadcast platform, and collected videos of soccer (FIFA-18) games along with the users' live chat conversations about the game. This dataset has videos involving various realistic human actions and events in a complex sports environment and hence serves as a good testbed and first step towards multimodal video-based dialogue data. An example is shown in Fig.  1  (and an original screenshot example in Fig.  2 ), where the users perform a complex 'manyspeaker', 'multimodal' dialogue. Overall, we collected 49 FIFA-18 game videos along with their users' chat, and divided them into 33 videos for training, 8 videos for validation, and 8 videos for testing. Each such video is several hours long, providing a good amount of data (Table  2 ). To extract triples (instances) of video context, chat context, and response from this data, we divide these videos based on the fixed time frames instead of fixed number of utterances in order to maintain conversation topic clusters (because of the sparse nature of chat utterances count over the time). First, we use 20-sec context windows to extract the video clips and users utterances in  this time frame, and use it as our video and chat contexts, resp. Next, the chat utterances in the immediately-following 10-sec window (response window) that do not overlap with the next instance's context window are considered as potential responses. 1 Hence, there are only two instances (triples) in a 60-sec long video, i.e., 20-sec video+chat context window and 10-sec response window, and there is no overlap between the instances. Now, out of these potential responses, to only allow the response that has at least some good coherence and relevance with the chat context's topic, we choose the first (earliest) response that has high similarity with some other utterance in this response window (using 0.5 BLEU-4 threshold, based on manual inspection). 2 Human Quality Evaluation of Data Filtering Process: To evaluate the quality of the responses that result from our filtering process described above, we performed an anonymous (randomly shuffled w/o identity) human comparison between the response selected by our filtering process vs. the first response from the response window without any filtering, based on relevance w.r.t. video and chat context. Table  1  presents the results on 100 sample size, showing that humans in a blindtest found 90% (34+56) of our filtered responses as valid responses, verifying that our response selection procedure is reasonable. Furthermore, out of these 90% valid responses, we found that 55% are chat-only relevant, 11% are video-only relevant, and 24% are both video+chat relevant. In order to make the above procedure safe and to make the dataset more challenging, we also discourage frequent responses (top-20 most-frequent generic utterances) unless no other response satisfies the similarity condition, hence suppressing the frequent responses. 3 If we couldn't find any utterance based on the multi-response matching procedure described above, then we just consider the first utterance in the 10-second window as the response.  4  We also make sure that the chat context window has at least 4 utterances, otherwise we exclude that context window and also the corresponding response window from the dataset. After all this processing, our final resulting dataset contains 10, 510 samples in training, 2, 153 samples in validation, and 2, 780 samples in test. 5 

 Dataset Analysis Dataset Statistics Table  2  presents the full statistics on train, validation, and test sets of our Twitch-FIFA dataset, after the filtering process described in Sec. 3.1. As shown, the average chat context length in the dataset is around 68 words, and the average response length is 6.3 words. Chat Context Size Fig.  3  presents the study of number of utterances in the chat context vs. the number of such training samples. As we limit the minimum number of utterances to 4, chat context with less than 4 utterances is not present in the dataset. From the Fig.  3 , it is clear that as the number of utterances in the chat context increases, the number of such training samples decrease. Frequent Words Fig.  4  presents the top-20 frequent words (excluding stop words) and their corresponding frequency in our Twitch-FIFA dataset. Most of these frequent words are related to soccer vocabulary. Also, some of these frequent words are twitch emotes (e.g. 'kappa', 'inceptionlove'). 3 Note that this filtering suppresses the performance of simple frequent-response baseline described in Sec. 4.1. 4 Other preprocessing steps include: omit the utterances in the response window which refer to a speaker name out of the current chat context; remove non-representative utterances, e.g., those with hyperlinks; replace (anonymize) all the user identities mentioned in the utterances with a common tag (i.e., anonymizing due to similar intuitions from the Q&A community  (Hermann et al., 2015) ). 5 Note that this is substantially larger than or comparable to most current video captioning datasets. We plan to further extend our dataset based on diverse games and video types.   4 Models Let v = {v 1 , v 2 , .., v m } be the video context frames, u = {u 1 , u 2 , . ., u n } be the textual chat (utterance) context tokens, and r = {r 1 , r 2 , .., r k } be response tokens generated (or retrieved).  

 Baselines 

 Discriminative Models 

 Triple Encoder For our simpler discriminative model, we use a 'triple encoder' to encode the video context, chat context, and response (see Fig.  5 ), as an extension of the dual encoder model in  Lowe et al. (2015) . The task here is to predict the given train- ing triple (v, u, r) as positive or negative. Let h v f , h u f , and h r f be the final state information of the video, chat, and response LSTM-RNN (bidirectional) encoders respectively; then the probability of a positive training triple is defined as follows: p(v, u, r; ?) = ?([h v f ; h u f ] T W h r f + b) (1) where W and b are trainable parameters. Here, W can be viewed as a similarity matrix which will bring the context [h v f ; h u f ] into the same space as the response h r f , and get a suitable similarity score. For optimizing our discriminative model, we use max-margin loss function similar to  Mao et al. (2016)  and  Yu et al. (2017) . Given a positive training triple (v, u, r), let the corresponding negative training triples be (v , u, r), (v, u , r), and (v, u, r ), i.e., one modality is wrong at a time in each of these three (see Sec. 5 for the negative example selection). The max-margin loss is: L(?) = [max(0, M + log p(v , u, r) ? log p(v, u, r)) + max(0, M + log p(v, u , r) ? log p(v, u, r)) + max(0, M + log p(v, u, r ) ? log p(v, u, r))] (2) where the summation is over all the training triples in the dataset. M is a tunable margin hyperparameter between positive and negative training triples. 

 Tridirectional Attention Flow (TriDAF) Our tridirectional attention flow model learns stronger joint spaces between the three modalities in a mutual-information way. We use bidirectional attention flow mechanisms  (Seo et al., 2017)  between the video and chat contexts, between the video context and the response, as well as between the chat context and the response, hence enabling attention flow across all three modalities, as shown in Fig.  6 . We name this model Tridirectional Attention Flow or TriDAF. We will next discuss the bidirectional attention flow mechanism between video and chat contexts, but the same formulation holds true for bidirectional attention between video context and response, and between chat context and response. Given the video context hidden state h v i and chat context hidden state h u j at time steps i and j respectively, the bidirectional attention mechanism is based on the similarity score: S (v,u) i,j = w T S (v,u) [h v i ; h u j ; h v i h u j ] (3) where S (v,u) i,j is a scalar, w S (v,u) is a trainable parameter, and denote element-wise multiplication. The attention distribution from chat context to video context is defined as ? i: = sof tmax(S i: ), hence the chat-to-video context vector c v?u i = j ? i,j h u j . Similarly, the attention distribution from video context to chat context is defined as ? j: = sof tmax(S :j ), hence the videoto-chat context vector c u?v j = i ? j,i h v i . We then compute similar bidirectional attention flow mechanisms between the video context and response, and between the chat context and response. Then, we concatenate each hidden state and its corresponding context vector from other two modalities, e.g., ?v i = [h v i ; c v?u i ; c v?r i ] for the i th timestep of the video context. Finally, we add self-attention mechanism  (Lin et al., 2017)  across the concatenated hidden states of each of the three modules.  6  If ?v i is the final concatenated vector of the video context at time step i, then the selfattention weights ? s for this video context are the softmax of e s : e s i = V v a tanh(W v a ?v i + b v a ) (4) where V v a , W v a , and b v a are trainable self-attention parameters. The final representation vector of the full video context after self-attention is ?v = i ? s i ?v i . Similarly, the final representation vectors of the chat context and the response are ?u and ?r , respectively. Finally, the probability that the given training triple (v, u, r) is positive is: p(v, u, r; ?) = ?([? v ; ?u ] T W ?r + b) (5) Again, here also we use max-margin loss (Eqn. 2). 

 Generative Models 

 Seq2seq with Attention Our simpler generative model is a sequence-tosequence model with bilinear attention mechanism (similar to  Luong et al. (2015) ). We have two encoders, one for encoding the video context and another for encoding the chat context, as shown in Fig.  7 . We combine the final state information from both encoders and give it as initial state to the response generation decoder. The two encoders and the decoder are all two-layer LSTM-RNNs. Let h v i and h u j be the hidden states of video and chat encoders at time step i and j respectively. At each time step t of the decoder with hidden state h r t , the decoder attends to parts of video and chat encoders and uses the combined information to generate the next token. Let ? t and ? t be the attention weight distributions for video and chat encoders respectively with video context vector c v t = i ? t,i h v i and chat context vector c u t = j ? t,j h u j . The attention distribution for video encoder is defined as (and the same holds for chat encoder): e t,i = h r t T W v a h v i ; ? t = softmax(e t ) (6) where W v a is a trainable parameter. Next, we concatenate the attention-based context information (c v t and c u t ) and decoder hidden state (h r t ), and do a non-linear transformation to get the final hidden state ?r t as follows: ?r t = tanh(W c [c v t ; c u t ; h r t ]) (7) where W c is again a trainable parameter. Finally, we project the final hidden state information to vocabulary size and give it as input to a softmax layer to get the vocabulary distribution p(r t |r 1:t?1 , v, u; ?). During training, we minimize the cross-entropy loss defined as follows: L XE (?) = ? t log p(r t |r 1:t?1 , v, u; ?) (8) where the final summation is over all the training triples in the dataset. the model to give higher generative decoder probability to the positive response as compared to all the negative ones), we use a max-margin loss (similar to Eqn. 2 in Sec. 4.2.1): LMM(?) = [max(0, M + log p(r|v , u) ? log p(r|v, u)) + max(0, M + log p(r|v, u ) ? log p(r|v, u)) + max(0, M + log p(r |v, u) ? log p(r|v, u))] (9) where the summation is over all the training triples in the dataset. Overall, the final joint loss function is a weighted combination of cross-entropy loss and max-margin loss: L(?) = L XE (?) + ?L MM (?) , where ? is a tunable hyperparameter. 

 Bidirectional Attention Flow (BiDAF) The stronger version of our generative model extends the two-encoder-attention-decoder model above to add bidirectional attention flow (BiDAF) mechanism  (Seo et al., 2017)  between video and chat encoders, as shown in Fig.  7 . Given the hidden states h v i and h u j of video and chat encoders at time step i and j, the final hidden states after the BiDAF are ?v i = [h v i ; c v?u i ] and ?u j = [h u i ; c u?v j ] (similar to as described in Sec. 4.2.2), respectively. Now, the decoder attends over these final hidden states, and the rest of the decoder process is similar to Sec 4.3.1 above, including the weighted joint cross-entropy and max-margin loss. 

 Experimental Setup Evaluation We first evaluate both our discriminative and generative models using retrieval-based recall@k scores, which is a concrete metric for such dialogue generation tasks  (Lowe et al., 2015) . For our discriminative models, we simply rerank the given responses (in a candidate list of size 10, based on 9 negative examples; more details below) Models r@1 r@2 r@5 BASELINES Most-Frequent-Response 10.0 16.0 20.9 in the order of the probability score each response gets from the model. If the positive response is within the top-k list, then the recall@k score is 1, otherwise 0, following previous Ubuntu-dialogue work  (Lowe et al., 2015) . For the generative models, we follow a similar approach, but the reranking score for a candidate response is based on the log probability score given by the generative models' decoder for that response, following the setup of previous visual-dialog work  (Das et al., 2017) . In our experiments, we use recall@1, recall@2, and recall@5 scores. For completeness, we also report the phrase-matching metric scores: METEOR  (Denkowski and Lavie, 2014)  and ROUGE  (Lin, 2004)  for our generative models. We also present human evaluation. Training Details For negative samples, during training, for every positive triple (video, chat, response) in the training set, we sample 3 random negative triples. For validation/test, we sample 9 random negative responses elsewhere from the validation/test set. Also, the negative samples don't come from the video corresponding to the positive response. More details of negative samples and other training details (e.g., dimension/vocab sizes, visual feature details, validationbased hyperparamater tuning and model selection), are discussed in the supplementary. 

 Results and Analysis 

 Human Evaluation of Dataset First, the overall human quality evaluation of our dataset (shown in Table  1 ) demonstrates that it contains 90% responses relevant to video and/or chat context. Next, we also do a blind human study on the recall-based setup (on a set of 100 samples from the validation set), where we anonymize the positive response by randomly mixing it with 9 tricky negative responses in the retrieval list, and ask the user to select the most relevant response for the given video and/or chat context. We found that human performance on this task is around 55% recall@1, demonstrating that this 10-way-discriminative recall-based task setup is reasonably challenging for humans, 7 but also that there is a lot of scope for future model improvements because the chance baseline is only 10% and the best-performing model so far (see Sec. 6.3) achieves only 22% recall@1 (on dev set), and hence there is a large 33% gap. 

 Baseline Results Table 3 displays all our primary results. We first discuss results of our simple non-trained and trained baselines (see Sec. 4.1). The 'Most-Frequent-Response' baseline, which just ranks the 10-sized response retrieval list based on their frequency in the training data, gets only around 10% recall@1. 8 Our other non-trained baselines: 'Chat-Response-Cosine' and 'Nearest Neighbor', which ranks the candidate responses based on (Twitch-trained RNN encoder's vector) cosine similarity with chat-context and K-best training contexts' response vectors, respectively, achieves slightly better scores. We also show that our simple trained baselines (logistic regression and nearest neighbor) also achieve relatively low scores, indicating that a simple, shallow model will not work on this challenging dataset. 

 Discriminative Model Results Next, we present the recall@k retrieval performance of our various discriminative models in Ta-7 This relatively low human recall@1 performance is because this is a challenging, 10-way-discriminative evaluation, i.e., the choice comes w.r.t. 9 tricky negative examples along with just 1 positive example (hence chance-baseline is only 10%). Note that these negative examples are an artifact of specifically recall-based evaluation only, and will not affect the more important real-world task of response generation (for which our dataset's response quality is 90%, as shown in Table  1 ). Moreover, our dataset filtering (see Sec. 3.1) also 'suppresses' simple baselines and makes the task even harder. 8 Note that the performance of this baseline is worse than the random choice baseline (recall@1:10%, recall@2:20%, recall@5:50%) because our dataset filtering process already suppresses frequent responses (see  Sec. 3.1) , in order to provide a challenging dataset for the community.   

 Generative Model Results Next, we evaluate the performance of our generative models with both retrieval-based recall@k scores and phrase matching-based metrics as discussed in Sec. 5 (as well as human evaluation). We first discuss the retrieval-based recall@k results in Table  3 . Starting with a simple sequenceto-sequence attention model with video only, chat only, and both video and chat encoders, the recall@k scores are better than all the simple baselines. Moreover, using both video+chat context is again better than using only one context modality. Finally, we show that the addition of the bidirectional attention flow mechanism improves the performance in all recall@k scores. 10 Note that generative model scores are lower than the discriminative models on retrieval recall@k metric, which is expected (see discussion in previous visual dialogue work  (Das et al., 2017) ), because discriminative models can tune to the biases in the response candidate options, but generative models are more useful for real-world tasks such as 9 Statistical significance of p < 0.01 for recall@1, based on the bootstrap test  (Noreen, 1989; Efron and Tibshirani, 1994)  with 100K samples. 10 Stat. signif. p < 0.05 for recall@1 w.r.t. Seq2seq+Atten (video+chat); p < 0.01 w.r.t. chat-and video-only models. Models recall@1 recall@2 recall@5 1 neg.  

 Human Evaluation of Models Finally, we also perform human evaluation to compare our top two generative models, i.e., the video+chat seq2seq with attention and its extension with BiDAF (Sec. 4.3), based on a 100-sized sample. We take the generated response from both these models, and randomly shuffle these pairs to anonymize model identity. We then ask two annotators (for 50 task instances each) to score the responses of these two models based on relevance. Note that the human evaluators were familiar with Twitch FIFA-18 video games and also the Twitch's unique set of chat mannerisms and emotes. As shown in Table  5 , our BiDAF based generative model performs better than the non-BiDAF one, which is already quite a strong video+chat encoder model with attention. 

 Ablations and Analysis 

 Negative Training Pairs We also compare the effect of different negative training triples that we discussed in Sec. 5. Table  6  shows the comparison between one negative  11 Liu et al. (2016b)  discussed that BLEU and most phrase matching metrics are not good for evaluating dialogue systems. Also, generative models have very low phrasematching metric scores because the generated response can be valid but still very different from the ground truth reference  (Lowe et al., 2015; Liu et al., 2016b; Li et al., 2016) . We present results for the relatively better metrics like paraphrase-enabled METEOR for completeness, but still focus on retrieval recall@k and human evaluation.   training triple (with just a negative response) vs. three negative training triples (one with negative video context, one with negative chat context, and another with negative response), showing that using the 3-negative examples setup is substantially better. 

 Discriminative Loss Functions Table  7  shows the performance comparison between the classification loss and max-margin loss on our TriDAF with self-attention discriminative model (Sec. 4.2.2). We observe that max-margin loss performs better than the classification loss, which is intuitive because max-margin loss tries to differentiate between positive and negative training example triples.  

 Generative Loss Functions For our best generative model (BiDAF), Table  8  shows that using a joint loss of cross-entropy and max-margin is better than just using only cross-entropy loss optimization (Sec. 4.3.1). Maxmargin loss provides knowledge about the negative samples for the generative model, hence improves the retrieval-based recall@k scores. 

 Attention Visualization and Examples Finally, we show some interesting output examples from both our discriminative and generative models as shown in Fig.  8 . Additionally, Fig.  9  Models recall@1 recall@2 recall@5 Cross-entropy (XE) visualizes that our models can learn some correct attention alignments from the generated output response word to the appropriate (goal-related) video frames as well as chat context words. 

 Conclusion We presented a new game-chat based videocontext, many-speaker dialogue task and dataset. We also presented several baselines and state-ofthe-art discriminative and generative models on this task. We hope that this testbed will be a good starting point to encourage future work on the challenging video-context dialogue paradigm. In future work, we plan to investigate the effects of multiple users, i.e., the multi-party aspect of this dataset. We also plan to explore advanced video features such as activity recognition, person identification, etc. https:// github.com/ramakanth-pasunuru/video-dialogue S1: what an offside trap OMEGALUL S2: Lol that finish bro S3: suprised you didn't do the extra pass S4: @S10 a drunk bet? S5: @S11 thanks mate S6: could have passed one more S7: Pass that S1: record now! S8: !record S9: done a nother pass there 
