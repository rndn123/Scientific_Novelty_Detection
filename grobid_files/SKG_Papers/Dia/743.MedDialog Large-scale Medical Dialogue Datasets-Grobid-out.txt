title
MedDialog: Large-scale Medical Dialogue Datasets

abstract
Medical dialogue systems are promising in assisting in telemedicine to increase access to healthcare services, improve the quality of patient care, and reduce medical costs. To facilitate the research and development of medical dialogue systems, we build large-scale medical dialogue datasets -MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, MedDialog is the largest medical dialogue dataset to date. We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, GPT, BERT-GPT, and compare their performance. It is shown that models trained on MedDialog are able to generate clinically correct and human-like medical dialogues. We also study the transferability of models trained on MedDialog to lowresource medical dialogue generation tasks. It is shown that via transfer learning which finetunes the models pretrained on MedDialog, the performance on medical dialogue generation tasks with small datasets can be greatly improved, as shown in human evaluation and automatic evaluation.

Introduction Telemedicine refers to the practice of delivering patient care remotely, where doctors provide medical consultations to patients using HIPAA compliant video-conferencing tools. As an important complement to traditional face-to-face medicine practiced physically in hospitals and clinics, telemedicine has a number of advantages. First, it increases access to care. For people living in medically under-served communities (e.g., rural areas) that are in shortage of clinicians, telemedicine enables them to receive faster and cheaper care compared with traveling over a long distance to visit a clinician. Second, it reduces healthcare costs. In a study 1 by Jefferson Health, it is shown that diverting patients from emergency departments with telemedicine can save more than $1,500 per visit. Third, telemedicine can improve the quality of care. The study in  (Pande and Morris, 2015)  shows that telemedicine patients score lower for depression, anxiety, and stress, and have 38% fewer hospital admissions. Other advantages include improving patient engagement and satisfaction, improving provider satisfaction, etc. Please refer to  (Wootton et al., 2017)  for a more comprehensive review. While telemedicine is promising, it has several limitations. First, it puts additional burden on physicians. In addition to practicing face-toface medicine which already makes physicians very busy, physicians need to provide remote telemedicine consultations, which further increases the risk of physician burnout. Second, different from in-hospital patients, the progression of whose medical conditions can be easily tracked by clinicians, remote patients are difficult to track and monitor. To address such problems, there has been increasing research interest in developing artificial intelligence (AI) methods to assist in telemedicine. In particular, medical dialogue systems are being developed to serve as "virtual doctors". These "virtual doctors" are aimed to interact with patients via natural dialogues, asking about the medical conditions and history of patients and providing clinical advice. They can also proactively reach out to patients to ask about the progression of patients' conditions and provide timely interventions. To build medical dialogue systems, a large collection of conversations between patients and doctors is needed as training data. Due to data privacy concerns, such data is difficult to obtain. The existing medical dialogue datasets  (Xu et al., 2019; Yang et al., 2020)  are limited in size or biased to certain diseases, which cannot adequately serve the purpose of training medical dialogue systems that can achieve doctor-level intelligence and cover many specialities in medicine. To address the limitations of existing datasets, we build large-scale medical dialogue datasets -MedDialog -that contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. Both datasets cover almost all specialities in medicine, ranging from internal medicine to family medicine and covers a wide spectrum of diseases, including cancer, pneumonia, etc. To our best knowledge, they are the largest Chinese and English medical dialogue datasets to date. The data is open to the public. Each consultation starts with a description of medical conditions and history, followed by the conversation between doctor and patient. In certain consultations, doctors make diagnosis conclusions and give suggestions on treatment. The conversations have multiple turns. On the Chinese MedDialog (MedDialog-CN) dataset, we train several dialogue generation models for the interested community to benchmark with. Generating a response given the conversation history can be formulated as a sequence-tosequence (seq2seq) learning problem, where we use the Transformer  (Vaswani et al., 2017)  architecture to perform this task. Transformer consists of an encoder which embeds the conversation history and a decoder which generates the response. Both the encoder and decoder use self-attention to capture long-range dependency between tokens. In addition to training the Transformer on MedDialog-CN from scratch, we can pretrain the encoder and decoder on a corpora much larger than MedDialog-CN, then finetune them on MedDialog-CN. BERT-GPT  (Wu et al., 2019; Lewis et al., 2019)  is a pretrained model where the encoder is pretrained using BERT  (Devlin et al., 2018)  and the decoder is pretrained using GPT  (Radford et al.) . Besides the seq2seq formulation, dialogue generation can be formulated as a language modeling problem which generates the next token in the response conditioned on the concatenation of the already generated tokens in the response and the conversation history. GPT  (Radford et al.; Zhang et al., 2019)  is a pretrained language model based on Transformer decoder. BERT-GPT and GPT are finetuned on MedDialog-CN. We perform evaluation of these models using automatic metrics including perplexity, BLEU  (Papineni et al., 2002a) , Dist  (Li et al., 2015) , etc. The generated responses are clinically informative, accurate, and human-like. We utilize the models trained on the large-scale MedDialog-CN dataset to improve performance in low-resource dialogue generation tasks where the dataset size is small. The study is performed on COVID-19 dialogue generation on the CovidDialog  (Yang et al., 2020)  dataset, which contains 1,088 dialogues and 9,494 utterances. The small size of this dataset incurs high risk of overfitting, if directly training the large-sized neural models on it. To alleviate this risk, we take the weights of dialogue generation models pretrained on MedDialog-CN and finetune the weights on CovidDialog. Human evaluation and automatic evaluation show that pretraining on MedDialog-CN can greatly improve the performance on CovidDialog and generate clinically meaningful consultations about COVID-19. The major contributions of this paper are: ? We build large-scale medical dialog datasets -MedDialog, which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, they are the largest of their kinds to date. ? We pretrain several dialogue generation models on the Chinese MedDialog dataset, including Transformer, BERT-GPT, and GPT, and compare their performance using automatic metrics. ? Through human evaluation and automatic evaluation, we show that the pretrained models on MedDialog-CN can significantly improve performance on medical dialogue generation tasks where the dataset size is small, via transfer learning. The rest of this paper is organized as follows. Section 2 and 3 present the datasets and dialogue generation models (DGMs). Section 4 gives experimental results of developing DGMs on Chinese MedDialog and studies the transferability of DGMs trained on MedDialog-CN to other low-resource medical dialogue generation tasks. Section 5 reviews related works and Section 6 concludes the paper. 

 Related Works There have been several works investigating medical dialogue generation.  built a task-oriented dialogue system for automatic diagnosis. The system detects the user intent and slots with values from utterances, tracks dialogue states, and generates responses.  Xu et al. (Xu et al., 2019)  developed a medical dialogue system for automatic medical diagnosis that converses with patients to collect additional symptoms beyond their self-reports and automatically makes a diagnosis. This system incorporates a medical knowledge graph into the topic transition in dialogue management. Xia et al.  (Xia et al.)  developed a reinforcement learning (RL) based dialogue system for automatic diagnosis. They proposed a policy gradient framework based on the generative adversarial network to optimize the RL model. 

 Datasets Our MedDialog consists of a Chinese dataset and an English dataset, collected from different sources. 

 The Chinese MedDialog dataset The Chinese MedDialog (MedDialog-CN) dataset contains 3.4 million Chinese dialogues (consultations) between patients and doctors. The total number of utterances is 11.3 million. Each consultation starts with the narration of patient' medical condition and history, including present disease, duration of the disease, allergies, medications, past diseases, etc. Then it follows with the multi-turn conversation between patient and doctor. In the conversation, there are cases that multiple consecutive utterances are from the same person (either doctor or patient) and these utterances were posted at different time points. For such cases, we combine the consecutive utterances from the same person into a single utterance. Optionally, at the end of the consultation, the doctor makes diagnosis and treatment suggestions to the patient. Table  1  shows statistics of the Chinese dataset. Figure  1  shows an exemplar consultation. The data is crawled from an online consultation website -haodf.com 2 , which provides consultation service to patients. The dialogues cover 29 broad categories of specialties including internal medicine, pediatrics, dentistry, etc. and 172 fine-grained specialties including cardiology, neurology, gastroenterology, urology, etc. The consultations are conducted from 2010 to 2020. 

 The English MedDialog dataset The English MedDialog (MedDialog-EN) dataset contains 0.26 million English consultations between patients and doctors. The total number of utterances is 0.51 million. Each consultation consists of two parts: (1) description of patient's medical conditions; (2) conversation between patient and doctor. The data is crawled from iclinic.com 3 and healthcaremagic.com 4 , which are two online platforms of healthcare services, including symptom self-checker, video consultation, online chat with doctors, etc. The consultations cover 51 categories of communities including diabetes, elderly problems, pain management, etc. and 96 specialties including andrology, cardiology, nephrology, pharmacology, etc. The consultations were conducted from 2008 to 2020.Table  2  shows statistics of the English dataset. 

 Advantages of our datasets To our best knowledge, MedDialog-CN and MedDialog-EN are the largest Chinese and English medical dialog dataset respectively. They have the following advantages. ? Large number of conversations and utterances. MedDialog-CN has about 3.4 mil- greatly minimizes population biases in these two datasets. Table  3  shows a comparison of our datasets with several other medical dialogue datasets. The number of dialogs and diseases in our datasets are both much larger than those in other datasets. 

 Methods We train several dialogue generation models on the Chinese MedDialog dataset for the interested research community to benchmark with. During training, given a dialogue containing a sequence of alternating utterances between patient and doctor, we process it into a set of pairs {(s i , t i )} where the target t i is a response from the doctor and the source s i is the concatenation of all utterances (from both patient and doctor) before t i . A dialogue generation model takes s as input and generates t. This problem can be formulated either as a sequence-to-sequence learning problem where the goal is to generate t conditioned on s via an encoder-decoder model, or as a language modeling problem which generates the i-th token t i in t conditioned on the concatenation of the conversation history s and the already generated sequence t 1 , ? ? ? , t i?1 in the response before t i via a language model. 

 Dialogue Generation as Sequence-to-Sequence Modeling The problem of response generation can be formulated as a sequence-to-sequence (seq2seq) learn-ing  (Sutskever et al., 2014)  problem: given the conversation history s, generate the response t. We use the Transformer  (Vaswani et al., 2017)  architecture for seq2seq modeling. Transformer consists of an encoder which embeds the input sequence into a latent space and a decoder which takes the embedding of the input sequence as input and generates the output sequence. Different from LSTM-based seq2seq models  (Sutskever et al., 2014)  which learn representations of a sequence of tokens in a recurrent manner and therefore suffer computational inefficiency due to their sequential nature, Transformer uses self-attention to capture the long-range dependency among tokens by calculating the similarity between each pair of tokens in the sequence. Self-attention avoids sequential computation and greatly facilitates parallel computation. A building block in Transformer contains the following modules: a self-attention sub-layer, a token-wise feed-forward sub-layer, residual connections  (He et al., 2016)  between sub-layers, and layer normalization  (Ba et al., 2016) . Both the encoder and decoder are composed of a stack of such building blocks. The encoder generates an encoding for each token in the input sequence. These encodings are fed into the decoder to generate the output sequence. To generate the token at position i, the decoder encodes the generated tokens from 1 to i ? 1 (like an encoder), calculates an attentional representation by performing attention between the encodings of input tokens and the encodings of output tokens 1, ? ? ? , i ? 1, then feeds the attentional representation into a softmax layer to generate token i. Transformer learns the weights in the encoder and decoder by maximizing the conditional likelihood of responses conditioned on conversation histories. 

 Dialogue Generation as Language Modeling Besides the sequence-to-sequence formulation, response generation can be formulated as a language modeling problem as well. Given the conversation history s, a language model defines the following probability on the sequence of tokens t = t 1 , ? ? ? , t n in the response: p(t|s) = p(t 1 |s) n i=2 p(t i |s, t 1 , ? ? ? , t i?1 ), (1) where s, t  pretrained language model which uses the Transformer decoder to model the conditional probability p(t i |s, t 1 , ? ? ? , t i?1 ) in Eq.(  1 ), which first encodes the tokens in s, t 1 , ? ? ? , t i?1 , then predicts t i based on the encodings. GPT learns the weights of the decoder by maximizing the likelihood (defined based on Eq.1) on the responses in the training data. 

 Pretraining Before training Transformer and GPT on the MedDialog-CN dataset, we can first pretrain them on general-domain text datasets which are much larger than MedDialog-CN, to get a good initialization of the weight parameters. BERT-GPT  (Wu et al., 2019; Lewis et al., 2019 ) is a pretraining approach of Transformer, which uses BERT  (Devlin et al., 2018)  to pretrain the Transformer encoder and uses GPT to pretrain the Transformer decoder. Given a sequence of tokens, BERT randomly marks out some of them. The masked sequence is fed into the transformer encoder, which aims to recover the masked tokens. The weights in the encoder are learned by maximizing the accuracy of recovery. In BERT-GPT, the BERT encoder generates representation of the input sequence, which is then fed into the GPT decoder to generate the response. 

 Experiments 

 Experiments on the Chinese MedDialog dataset 

 Experimental Settings We split the Chinese MedDialog dataset into a training set, a validation set, and a test set with a ratio of 0.8:0.1:0.1. The split was based on dialogues, not based on source-target pairs. The split statistics are summarized in Table  4 . The models were built at the Chinese character level. The validation set was used for hyperparameter tuning. The training procedure was stopped when the validation loss stopped to decrease. For Transformer, the implementation by HuggingFace 5 was used, where the hyperparameters followed the default settings in the original Transformer  (Vaswani et al., 2017) . In BERT-GPT, the BERT encoder and GPT decoder are Transformers with 12 layers. The hidden state size is 768. The optimization of weight parameters was performed using stochastic gradient descent, with a learning rate of 1e-4. The maximum length of input sequences was truncated to 400 and that of output sequences was truncated to 100. For GPT, the DialoGPT-small  (Zhang et al., 2019)  architecture was used, with 10 layers. We set the embedding size to 768 and the context size to 300. In layer normalization, the epsilon hyperparameter was set to 1e-5. In multi-head self-attention, we set the number of heads to 12. The weight parameters were learned with Adam (Kingma and  Ba, 2014) . The initial learning rate was set to 1.5e-4 and the batch size was set to 32. The learning rate scheduler was set to Noam, with 2000 warm-up steps. Top-k random sampling  (Fan et al., 2018)  with k = 50 was used for decoding in all methods. We evaluated the trained models using automatic metrics including perplexity, NIST-n  (Doddington, 2002)  (where n is the size of n-gram and is set to 4), BLEU-n  (Papineni et al., 2002b ) (where n is set to 2 and 4), METEOR  (Lavie and Agarwal, 2007) , Entropy-n  (Zhang et al., 2018 ) (where n is set to 4), and Dist-n  (Li et al., 2015)  (where n is set to 1 and 2). Perplexity measures the language quality of the generated responses. The lower, the better. NIST, BLEU, and METEOR measure the similarity between the generated responses and groundtruth via n-gram matching. The higher, the better. Entropy and Dist measure the lexical diversity of generated responses. The higher, the better. BERT-GPT is pretrained on Chinese corpus collected from the Large Scale Chinese Corpus for NLP 6 . The corpus includes Chinese Wikipedia containing 104 million documents, News containing 2.5 million news articles from 63,000 sources, Community QA containing 4.1 million documents belonging to 28 thousand topics, and Baike QA containing 1.5 million question-answering pairs from 493 domains. The total size of these datasets is 15.4 GB. GPT is pretrained on Chinese Chatbot Corpus 7 containing 14 million dialogues and 500k-Chinese-Dialog 8 containing 500K Chinese dialogues.  

 Results Table  5  shows the performance on the MedDialog-CN test set. From this table, we make the following observations. First, BERT-GPT achieves lower perplexity than Transformer. This is because BERT-GPT is pretrained on a large collection of corpora before being finetuned on MedDialog-CN. Pretraining enables the model to better capture the linguistic structure among words, which yields lower perplexity. Second, on machine translation metrics including NIST-4, BLEU-2, BLEU-4, and METEOR, BERT-GPT performs worse than Transformer. This indicates that Transformer is able to generate responses that have more overlap with the groundtruth. However, it is worth noting that the studies in  (Liu et al., 2016)  show that machine translation metrics are not reliable evaluation metrics for dialogue generation. Given the same conversation history, many responses are valid. A response should not be deemed as bad simply because it has little overlap with the response given by a doctor. Third, on diversity metrics, BERT-GPT and Transformer are on par, which indicates that they have similar capability in generating diverse responses. Fourth, compared with BERT-GPT, GPT has worse perplexity, better machine translation scores, and comparable diversity scores. Figure  2  shows an example of generated responses on the MedDialog-CN test set. The response generated by BERT-GPT is clinically informative and accurate. It prescribes Ebastine and gives detailed instructions of taking this medication. Ebastine is a medication for treating eczema. The patient mentioned that his/her baby has eczema. So this prescription is clinically meaningful. The language quality of the response is also good. It is syntactically and semantically correct and smooth. The response generated by GPT is also good, but less specific. It believes the baby has a skin allergy issue, but does not pinpoint the exact issue as BERT-GPT does. The response generated by Transformer is less clinically informative. It does not give medical suggestions. But it asks for further information, which is also a valid response. Figure  3  shows another example. The response generated by BERT-GPT is clinically accurate and concise. The language quality is great. The response generated by GPT is self-conflicting. It says "if there is no abnormality at the throat, you can take a laryngoscope test; if abnormal, you should take a laryngoscope test", which is semantically inconsistent. The response generated by Transformer prescribes two repetitive laryngoscope tests, which is clinically insensible. 

 Transfer to Other Datasets In this section, we study how to use the models pretrained on MedDialog-CN to improve the performance on low-resource dialogue generation tasks where the dataset size is small. The target task is generating medical dialogues related to COVID-19 on the small-sized CovidDialog-Chinese  (Yang et al., 2020)  dataset. We finetune the MedDialogpretrained models on CovidDialog-Chinese, and use the finetuned models to generate COVID-19related dialogues. 

 Data We use a Chinese dialogue dataset about COVID-19: CovidDialog-Chinese  (Yang et al., 2020)    6  shows the statistics of this dataset. 

 Experimental settings We split the CovidDialog-Chinese dataset into a training set, a validation set, and a test set with a ratio of 0.8:0.1:0.1. The split is based on dialogues. The split statistics are summarized in Table 7. Most hyperparameter settings follow those in Section 4.1, except the following: in optimization, the batch size was set to 8. We evaluate the trained models using automatic metrics including perplexity, NIST-4  (Doddington, 2002) , BLEU-2, 4 (Pa- strates the effectiveness of pretraining. We perform significance tests between different methods based on the double-sided Student's t-test. The results are shown in Table  10 . As can be seen, in most cases, the p-value is less than 0.015, demonstrating high statistical significance. For Transformer, GPT, and BERT-GPT, using pretraining (PT) on MedDialog-CN achieves significantly better performance than not using pretraining (No-PT). Figure  4  shows an example of generating a doctor's response given the utterance of a patient. As can be seen, models pretrained on MedDialog-CN perform better than their unpretrained counterparts. For example, the response generated by GPT without pretraining on MedDialog-CN is not understandable by human. With pretraining on MedDialog-CN, it generates a much better response which gives medical advice. Figure  5  shows another example. Similarly, without MedDialogpretraining, the response generated by GPT is not readable. With pretraining, the generated response is smooth and clinically informative. 

 Conclusions and Future Works To facilitate the research and development of medical dialogue systems that can potentially assist in telemedicine, we build large-scale medical dialogue datasets -MedDialog -which contain 1) a Chinese dataset with 3.4 million conversations between patients and doctors, 11.3 million utterances, 660.2 million tokens, covering 172 specialties of diseases, and 2) an English dataset with 0.26 million conversations, 0.51 million utterances, 44.53 million tokens, covering 96 specialties of diseases. To our best knowledge, they the largest of their kind. We pretrain Transformer, GPT, and BERT-GPT on MedDialog-CN. The results show that the generated dialogues by these pretrained models are clinically meaningful and human-like. We use transfer learning to apply these pretrained models for low-resource dialogue generation. On a COVID-19 dialogue generation task where the dataset is small, human evaluation and automatic evaluation show that models pretrained on MedDialog-CN can effectively improve the quality of generated responses. For future work, we will annotate medical entities in our datasets. Such annotations can facilitate the development of goal-oriented medical dialog systems. Figure 2 : 2 Figure 2: An example of generated responses on the MedDialog-CN test set. 

 Table 1 : 1 Statistics of the Chinese MedDialog dataset # dialogues 3,407,494 # utterances 11,260,564 # tokens 660,171,367 Avg. # of utterances in a dialogue 3.3 Max # of utterances in a dialogue 198 Min # of utterances in a dialogue 2 Avg. # of tokens in an utterance 55.6 Max # of tokens in an utterance 6,935 Min # tokens in an utterance 1 

 Description of medical conditions and history # dialogues 257,332 ?: ?, ?. (Disease: The baby's eyes are red and slightly ulcerated when becoming severe.) # utterances 514,664 ?: ?, ?, ?, ? # tokens 44,527,872 ?. ?. (Medical condition: The baby's eyes are red and itchy, scratched with Avg. # of utterances in a dialogue 2 hand, and slightly ulcerated when becoming severe. After using Burt's bee Res-Q ointment, it disappeared quickly but came out after two days.) Max # of utterances in a dialogue Min # of utterances in a dialogue 2 2 ?: ?. (Help needed: What's wrong with baby's red eyes?) ?: ?. (Hong long the condition has been: Less Avg. # of tokens in an utterance Max # of tokens in an utterance 86.5 3,672 than one month) ?: ? (Allergies: No) Min # tokens in an utterance 1 ?: ? (Past medical history: No) Dialogue ?: ?, ?. ? ?, ?: ?. ?. ?? (Doctor: Thank you for your trust. I have read the medical information in detail. Based on the existing information, the diagnosis is blepharitis. The picture is not very clear. Scratch it often, right?) ?: ?, ?, ? ?. (Patient: Drinks little amount of milk since birth, and the baby's lips are always dry, and not drooling like other babies.) Figure 1: An exemplar consultation, which includes (1) description of medical conditions and history of patient, (2) dialogue between doctor and patient, and (3) diag- nosis and treatment suggestions given by doctor. lion conversations and 11.3 million utterances. MedDialog-EN has about 0.3 million conver- sations and 0.5 million utterances. ? Broad coverage of medical specialities. Consultations in MedDialog-CN are about 29 broad categories of specialties and 172 fine-grained specialties. Consultations in MedDialog-EN are about 96 categories of spe- cialties. ? Diversity of the patients. The patients in MedDialog-EN are from all over the world, with different nationalities, ethics, age, gender, occupation, education, income, etc. The patients in MedDialog-CN are from 31 provincial-level administrative divisions in China, with different ethics, age, gender, occu- pation, education, income, etc. Such diversity ?: ? (Doctor: Eyes have local arthritis.) ?: ? (Patient: Yes) ?: ? (Doctor: Use Tobramycin and Dexamethasone eye ointment twice a day) ?: ? (Patient: What's going on?) ?: ? (Doctor: Consider blepharitis or blepharitis) ? Diagnosis and suggestions ?: ? (Summary of the condition and initial impressions: Blepharitis) ?: ?, ?, ? ?, ?. (Summary of recommendations: For local inflammation, use Tobramycin and Dexamethasone eye ointment eye ointment twice a day, monitor the recovery, and go to the hospital if necessary.) 

 Table 2 : 2 Statistics of the English dataset Dataset #dialogs #diseases Muzhi (Wei et al., 2018) 710 4 Dxy (Xu et al., 2019) 527 5 COVID-EN (Yang et al., 2020) 603 1 COVID-CN (Yang et al., 2020) 1,088 1 MedDialog-CN 3,407,494 172 MedDialog-EN 257,332 96 

 Table 3 : 3 Comparison with other datasets. 

 Table 4 : 4 The split statistics of the Chinese MedDialog dataset. 

 Table 5 : 5 Performance on the MedDialog-CN test set. 6 https://github.com/brightmart/nlp_ chinese_corpus 7 https://github.com/codemayq/chinese_ chatbot_corpus 8 https://drive.google.com/file/d/ 1nEuew_KNpTMbyy7BO4c8bXMXN351RCPp/view 

 Table 6 : 6 ?. (I need to see the skin to give detailed suggestions.) GPT: ?. (If the skin is allergic, can take some Ketoconazole cream.) BERT-GPT: ?. (Can use Ebastine, one tablet every day at bedtime.) Statistics of the CovidDialog-Chinese dataset.in total. Duplicated and incomplete dialogues were removed. The dialogues are multi-turn. The average number of utterances in a dialogue is 8.7. The utterances are reasonably long. The average number of tokens in an utterance is 42.8. Table , for Transformer: 

 Table 8 : 8 Automatic evaluation results on the CovidDialog-Chinese test set. Perplexity NIST-4 BLEU-2 BLEU-4 METEOR Entropy-4 Dist-1 Dist-2 Avg. Len Transformer, no PT 53.3 0.39 5.7% 4.0% 13.5% 7.9 5.5% 29.0% 19.3 Transformer, PT 13.7 0.50 7.8% 4.7% 16.0% 8.0 7.6% 36.3% 22.0 GPT, no PT 22.1 0.43 6.2% 4.0% 13.9% 9.0 5.9% 38.7% 35.0 GPT, PT 8.9 0.40 7.0% 4.0% 14.8 8.7 7.4% 39.7% 28.9 BERT-GPT, no PT 10.8 0.36 4.6% 2.8% 12.2% 8.5 7.9% 39.5% 21.6 BERT-GPT, PT 10.2 0.33 5.0% 2.7% 11.2% 8.4 8.6% 43.3% 21.4 Transformer No PT PT No PT PT No PT PT GPT BERT-GPT Groundtruth Relevance 2.25 2.68 1.82 2.74 2.65 2.93 3.42 Informativeness 2.06 2.40 1.72 2.53 2.37 2.77 3.26 Human-likeness 2.57 3.29 1.80 3.20 3.16 3.44 3.78 

 Table 9 : 9 Human evaluation results on the CovidDialog-Chinese test set. Transformer GPT BERT-GPT Groundtruth No-PT vs PT No-PT vs PT No-PT vs PT vs BERT-GPT Relevance 0.006 0.008 0.004 0.003 Informativeness 0.014 0.004 0.003 0.004 Human-likeness 0.009 0.001 0.031 0.036 

 Table 10 : 10 Significance tests on human evaluation results. 

			 https://www.healthleadersmedia.com/clinical-care/costsavings-telemedicine-estimated-19-120-patient-visit 

			 https://www.haodf.com/ 3 https://www.icliniq.com/ 4 https://www.healthcaremagic.com/ 

			 https://github.com/huggingface/transformers
