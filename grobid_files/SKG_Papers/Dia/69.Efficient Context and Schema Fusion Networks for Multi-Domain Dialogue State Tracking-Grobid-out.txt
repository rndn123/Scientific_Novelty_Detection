title
Efficient Context and Schema Fusion Networks for Multi-Domain Dialogue State Tracking

abstract
Dialogue state tracking (DST) aims at estimating the current dialogue state given all the preceding conversation. For multi-domain DST, the data sparsity problem is a major obstacle due to increased numbers of state candidates and dialogue lengths. To encode the dialogue context efficiently, we utilize the previous dialogue state (predicted) and the current dialogue utterance as the input for DST. To consider relations among different domain-slots, the schema graph involving prior knowledge is exploited. In this paper, a novel context and schema fusion network is proposed to encode the dialogue context and schema graph by using internal and external attention mechanisms. Experiment results show that our approach can outperform strong baselines, and the previous state-of-the-art method (SOM-DST) can also be improved by our proposed schema graph.

Introduction Dialogue state tracking (DST) is a key component in task-oriented dialogue systems which cover certain narrow domains (e.g., booking hotel and travel planning). As a kind of context-aware language understanding task, DST aims to extract user goals or intents hidden in human-machine conversation and represent them as a compact dialogue state, i.e., a set of slots and their corresponding values. For example, as illustrated in Fig.  1, (slot, value ) pairs like (name, huntingdon marriott hotel) are extracted from the dialogue. It is essential to build an accurate DST for dialogue management  (Young et al., 2013) , where dialogue state determines the next machine action and response. Recently, motivated by the tremendous growth of commercial dialogue systems like Apple Siri, Microsoft Cortana, Amazon Alexa, or Google Assistant, multi-domain DST becomes crucial to help Figure  1 : An example of multi-domain dialogues. Utterances at the left side are from the system agent, and utterances at the right side are from a user. The dialogue state of each domain is represented as a set of (slot, value) pairs. users across different domains  Eric et al., 2019) . As shown in Fig.  1 , the dialogue covers three domains (i.e., Hotel, Attraction and Taxi). The goal of multidomain DST is to predict the value (including NONE) for each domain-slot pair based on all the preceding dialogue utterances. However, due to increasing numbers of dialogue turns and domainslot pairs, the data sparsity problem becomes the main issue in this field. To tackle the above problem, we emphasize that DST models should support open-vocabulary based value decoding, encode context efficiently and incorporate domain-slot relations: 1. Open-vocabulary DST is essential for realworld applications  Ren et al., 2019) , since value sets for some slots can be very huge and variable (e.g., song names). 2. To encode the dialogue context efficiently, we attempt to get context representation from the previous (predicted) dialogue state and the current turn dialogue utterance, while not concatenating all the preceding dialogue utterances. 3. To consider relations among domains and slots, we introduce the schema graph which contains domain, slot, domain-slot nodes and their relationships. It is a kind of prior knowledge and may help alleviate the data imbalance problem. To this end, we propose a multi-domain dialogue state tracker with context and schema fusion networks (CSFN-DST). The fusion network is exploited to jointly encode the previous dialogue state, the current turn dialogue and the schema graph by internal and external attention mechanisms. After multiple layers of attention networks, the final representation of each domain-slot node is utilized to predict the corresponding value, involving context and schema information. For the value prediction, a slot gate classifier is applied to decide whether a domain-slot is mentioned in the conversation, and then an RNN-based value decoder is exploited to generate the corresponding value. Our proposed CSFN-DST is evaluated on Mul-tiWOZ 2.0 and MultiWOZ 2.1 benchmarks. Ablation study on each component further reveals that both context and schema are essential. Contributions in this work are summarized as: ? To alleviate the data sparsity problem and enhance the context encoding, we propose exploiting domain-slot relations within the schema graph for open-vocabulary DST. ? To fully encode the schema graph and dialogue context, fusion networks are introduced with graph-based, internal and external attention mechanisms. ? Experimental results show that our approach surpasses strong baselines, and the previous state-of-the-art method (SOM-DST) can also be improved by our proposed schema graph. 

 Related Work Traditional DST models rely on semantics extracted by natural language understanding to predict the current dialogue states  (Young et al., 2013; Henderson et al., 2014d; Sun et al., 2014b,a; Yu et al., 2015) , or jointly learn language understanding in an end-to-end way  (Henderson et al., 2014b,c) . These methods heavily rely on hand-crafted features and complex domain-specific lexicons for delexicalization, which are difficult to extend to new domains. Recently, most works about DST focus on encoding dialogue context with deep neural networks (such as CNN, RNN, LSTM-RNN, etc.) and predicting a value for each possible slot  (Mrk?i? et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018) . Multi-domain DST Most traditional state tracking approaches focus on a single domain, which extract value for each slot in the domain  Henderson et al., 2014a) . They can be directly adapted to multi/mixed-domain conversations by replacing slots in a single domain with domain-slot pairs (i.e. domain-specific slots)  Zhang et al., 2019; . Despite its simplicity, this approach for multi-domain DST extracts value for each domain-slot independently, which may fail to capture features from slot co-occurrences. For example, hotels with higher stars are usually more expensive (price range). Predefined ontology-based DST Most of the previous works assume that a predefined ontology is provided in advance, i.e., all slots and their values of each domain are known and fixed  (Williams, 2012; Henderson et al., 2014a) . Predefined ontology-based DST can be simplified into a value classification task for each slot  (Henderson et al., 2014c; Mrk?i? et al., 2017; Zhong et al., 2018; Ren et al., 2018; . It has the advantage of access to the known candidate set of each slot, but these approaches may not be applicable in the real scenario. Since a full ontology is hard to obtain in advance  (Xu and Hu, 2018) , and the number of possible slot values could be substantial and variable (e.g., song names), even if a full ontology exists . Open-vocabulary DST Without a predefined ontology, some works choose to directly generate or extract values for each slot from the dialogue context, by using the encoder-decoder architecture  or the pointer network  Ren et al., 2019; Le et al., 2020) . They can improve the scalability and robustness to unseen slot values, while most of them are not efficient in context encoding since they encode all the previous utterances at each dialogue turn. Notably, a multi-domain dialogue could involve quite a long history, e.g., MultiWOZ dataset  contains about 13 turns per dialogue on average. Graph Neural Network Graph Neural Network (GNN) approaches  (Scarselli et al., 2009; Veli?kovi? et al., 2018)  aggregate information from graph structure and encode node features, which can learn to reason and introduce structure information. Many GNN variants are proposed and also applied in various NLP tasks, such as text classification , machine translation  (Marcheggiani et al., 2018) , dialogue policy optimization  etc. We introduce graph-based multi-head attention and fusion networks for encoding the schema graph. 

 Problem Formulation In a multi-domain dialogue state tracking problem, we assume that there are M domains (e.g. taxi, hotel) involved, D = {d 1 , d 2 , ? ? ? , d M }. Slots included in each domain d ? D are denoted as a set S d = {s d 1 , s d 2 , ? ? ? , s d |S d | }. 1 Thus, there are J possible domain-slot pairs totally, O = {O 1 , O 2 , ? ? ? , O J }, where J = M m=1 |S dm |. Since different domains may contain a same slot, we denote all distinct N slots as S = {s 1 , s 2 , ? ? ? , s N }, where N ? J. A dialogue can be formally represented as {(A 1 , U 1 , B 1 ), (A 2 , U 2 , B 2 ), ? ? ? , (A T , U T , B T )}, where A t is what the agent says at the t-th turn, U t is the user utterance at t turn, and B t denotes the corresponding dialogue state. A t and U t are word sequences, while B t is a set of domain-slot-value triplets, e.g., (hotel, price range, expensive). Value v tj is a word sequence for j-th domain-slot pair at the t-th turn. The goal of DST is to correctly predict the value for each domain-slot pair, given the dialogue history. Most of the previous works choose to concatenate all words in the dialogue history,   [A 1 , U 1 , A 2 , U 2 , ? ? ? , A t , U t ], 

 Context and Schema Fusion Networks for Multi-domain DST In this section, we will introduce our approach for multi-domain DST, which jointly encodes the current dialogue turn (A t and U t ), the previous dialogue state B t?1 and the schema graph G by fusion networks. After that, we can obtain contextaware and schema-aware node embeddings for all J domain-slot pairs. Finally, a slot-gate classifier and RNN-based value decoder are exploited to extract the value for each domain-slot pair. The architecture of CSFN-DST is illustrated in Fig.  3 , which consists of input embeddings, context schema fusion network and state prediction modules.  

 Input Embeddings Besides token and position embeddings for encoding literal information, segment embeddings are also exploited to discriminate different types of input tokens. (1) Dialogue Utterance We denote the representation of the dialogue utterances at t-th turn as a joint sequence, X t = [CLS] ? A t ?; ?U t ? [SEP] , where [CLS] and [SEP] are auxiliary tokens for separation, ? is the operation of sequence concatenation. As [CLS] is designed to capture the sequence embedding, it has a different segment type with the other tokens. The input embeddings of X t are the sum of the token embeddings, the segmentation embeddings and the position embeddings  (Vaswani et al., 2017) , as shown in Fig.  3 . (2) Previous Dialogue State As mentioned before, a dialogue state is a set of domain-slot-value triplets with a mentioned value (not NONE). Therefore, we denote the previous dialogue state as B t?1 = [CLS] ? R 1 t?1 ? ? ? ? ? R K t?1 , where K is the number of triplets in B t?1 . Each triplet d-s-v is denoted as a sub-sequence, i.e., R = d ? -? s ? -? v. The domain and slot names are tokenized, e.g., price range is replaced with "price range". The value is also represented as a token sequence. For the special value DONTCARE which means users do not care the value, it would be replaced with "dont care". The input embeddings of B t?1 are the sum of the token, segmentation and position embeddings. Positions are re-enumerated for different triplets. (3) Schema Graph As mentioned before, the schema graph G is comprised of M domain nodes, N slot nodes and J domain-slot nodes. These nodes are arranged as G = d 1 ? ? ? ? ? d M ? s 1 ? ? ? ?s N ?o 1 ? ? ?o J . Each node embedding is initialized by averaging embeddings of tokens in the corresponding domain/slot/domain-slot. Positions embeddings are omitted in the graph. The edges of the graph are represented as an adjacency matrix A G whose items are either one or zero, which would be used in the fusion network. To emphasize edges between different types of nodes can be different in the computation, we exploit node types to get segment embeddings. 

 Context and Schema Fusion Network At this point, we have input representations H G 0 ? R |G|?dm , H Xt 0 ? R |Xt|?dm , H B t?1 0 ? R |B t?1 |?dm , where |.| gets the token or node number. The context and schema fusion network (CSFN) is utilized to compute hidden states for tokens or nodes in X t , B t?1 and G layer by layer. We then apply a stack of L context-and schema-aware self-attention layers to get final hidden states, H G L , H Xt L , H B t?1 L . The i-th layer (0 ? i < L) can be formulated as: H G i+1 , H Xt i+1 , H B t?1 i+1 = CSFNLayer i (H G i , H Xt i , H B t?1 i ) 4.2.1 Multi-head Attention Before describing the fusion network, we first introduce the multi-head attention  (Vaswani et al., 2017)   where z i ? R 1?d model and Z ? R |Z|?d model . For each vector y i , we can compute an attention vector c i over Z by using H heads as follows: e (h) ij = (y i W (h) Q )(zjW (h) K ) dmodel/H ; a (h) ij = exp(e (h) ij ) |Z| l=1 exp(e (h) il ) c (h) i = |Z| j=1 a (h) ij (zjW (h) V ); ci = Concat(c (1) i , ? ? ? , c (H) i )WO where 1 ? h ? H, W O ? R d model ?d model , and W (h) Q , W (h) K , W (h) V ? R d model ?(d model /H ) . We can compute c i for every y i and get a transformed matrix C ? R |Y |?d model . The entire process is denoted as a mapping MultiHead ? : C = MultiHead ? (Y, Z) (1) Graph-based Multi-head Attention To apply the multi-head attention on a graph, the graph adjacency matrix A ? R |Y |?|Z| is involved to mask nodes/tokens unrelated, where A ij ? {0, 1}. Thus, e (h) ij is changed as: e (h) ij = ? ? ? (y i W (h) Q )(z j W (h) K ) ? d model /H , if A ij = 1 ?, otherwise and Eqn. (  1 ) is modified as: C = GraphMultiHead ? (Y, Z, A) (2) Eqn. (1), can be treated as a special case of Eqn. (2) that the graph is fully connected, i.e., A = 1. 

 Context-and Schema-Aware Encoding Each layer of CSFN consists of internal and external attentions to incorporate different types of inputs. The hidden states of the schema graph G at the i-the layer are updated as follows: I GG = GraphMultiHead ? GG (H G i , H G i , A G ) E GX = MultiHead ? GX (H G i , H Xt i ) E GB = MultiHead ? GB (H G i , H B t?1 i ) C G = LayerNorm(H G i + I GG + E GX + E GB ) H G i+1 = LayerNorm(C G + FFN(C G )) where A G is the adjacency matrix of the schema graph and LayerNorm(.) is layer normalization function  (Ba et al., 2016) . FFN(x) is a feedforward network (FFN) function with two fullyconnected layer and an ReLU activation in between, i.e., FFN(x) = max (0, xW 1 + b 1 ) W 2 + b 2 . Similarly, more details about updating H Xt i , H B t?1 i are described in Appendix A. The context and schema-aware encoding can also be simply implemented as the original transformer  (Vaswani et al., 2017)  with graph-based multi-head attentions. 

 State Prediction The goal of state prediction is to produce the next dialogue state B t , which is formulated as two stages: 1) We first apply a slot-gate classifier for each domain-slot node. The classifier makes a decision among {NONE, DONTCARE, PTR}, where NONE denotes that a domain-slot pair is not mentioned at this turn, DONTCARE implies that the user can accept any values for this slot, and PTR represents that the slot should be processed with a value. 2) For domain-slot pairs tagged with PTR, we further introduced an RNN-based value decoder to generate token sequences of their values. 

 Slot-gate Classification We utilize the final hidden vector of j-th domainslot node in G for the slot-gate classification, and the probability for the j-th domain-slot pair at the t-th turn is calculated as: P gate tj = softmax(FFN(H G L,M +N +j )) The loss for slot gate classification is L gate = ? T t=1 J j=1 log(P gate tj ? (y gate tj ) ) where y gate tj is the one-hot gate label for the j-th domain-slot pair at turn t. 

 RNN-based Value Decoder After the slot-gate classification, there are J domain-slot pairs tagged with PTR class which indicates the domain-slot should take a real value. They are denoted as C t = {j|argmax(P gate tj ) = PTR}, and J = |C t |. We use Gated Recurrent Unit (GRU)  decoder like  and the soft copy mechanism  (See et al., 2017)  to get the final output distribution P value,k tj over all candidate tokens at the k-th step. More details are illustrated in Appendix B. The loss function for value decoder is L value = ? T t=1 j?Ct k log(P value,k tj ? (y value,k tj ) ) where y value,k tj is the one-hot token label for the j-th domain-slot pair at k-th step. During training process, the above modules can be jointly trained and optimized by the summations of different losses as: L total = L gate + L value 5 Experiment 

 Datasets We use MultiWOZ 2.0  and MultiWOZ 2.1  (Eric et al., 2019)  to evaluate our approach. MultiWOZ 2.0 is a task-oriented dataset of human-human written conversations spanning over seven domains, consists of 10348 multi-turn dialogues. MultiWOZ 2.1 is a revised version of MultiWOZ 2.0, which is re-annotated with a different set of inter-annotators and also canonicalized entity names. According to the work of  Eric et al. (2019) , about 32% of the state annotations is corrected so that the effect of noise is counteracted. Note that hospital and police are excluded since they appear in training set with a very low frequency, and they do not even appear in the test set. To this end, five domains (restaurant, train, hotel, taxi, attraction) are involved in the experiments with 17 distinct slots and 30 domain-slot pairs. We follow similar data pre-processing procedures as  on both MultiWOZ 2.0 and 2.1.  2  The resulting corpus includes 8,438 multi-turn dialogues in training set with an average of 13.5 turns per dialogue. Data statistics of MultiWOZ 2.1 is shown in Table  1 . The adjacency matrix A G of MultiWOZ 2.0 and 2.1 datasets is shown in Figure  4  of Appendix, while domain-slot pairs are omitted due to space limitations. 

 Experiment Settings We set the hidden size of CSFN, d model , as 400 with 4 heads. Following , the token embeddings with 400 dimensions are initialized by concatenating Glove embeddings  (Pennington et al., 2014)   In the inference, the predicted dialogue state of the last turn is applied, and we use a greedy search strategy in the decoding process of the value decoder. 

 Baseline Models We make a comparison with the following existing models, which are  

 Main Results Joint goal accuracy is the evaluation metric in our experiments, which is represented as the ratio of turns whose predicted dialogue states are entirely consistent with the ground truth in the test set. Table  2  illustrates that the joint goal accuracy of CSFN-DST and other baselines on the test set of MultiWOZ 2.0 and MultiWOZ 2.1 datasets. As shown in the table, our proposed CSFN-DST can outperform other models except for SOM-DST. By combining our schema graphs with SOM-DST, we can achieve state-of-the-art performances on both MultiWOZ 2.0 and 2.1 in the open-vocabulary setting. Additionally, our method using BERT (Bert-base-uncased) can obtain very competitive performance with the best systems in the predefined ontology-based setting. When a BERT is exploited, we initialize all parameters of CSFN with the BERT encoder's and initialize the token/position embeddings with the BERT's. 

 Analysis In this subsection, we will conduct some ablation studies to figure out the potential factors for the improvement of our method.  (   

 Effect of context information Context information consists of the previous dialogue state or the current dialogue utterance, which are definitely key for the encoder. It would be interesting to know whether the two kinds of context information are also essential for the RNN-based value decoder. As shown in Table  3 , we choose to omit the top hidden states of the previous dialogue state (H B t?1 L ) or the current utterance (H Xt L ) in the RNN-based value decoder. The results show both of them are crucial for generating real values. Do we need more context? Only the current dialogue utterance is utilized in our model, which would be more efficient than the previous methods involving all the preceding dialogue utterance. However, we want to ask whether the performance will be improved when more context is used. In Table  3 , it shows that incorporating the previous dialogue utterance X t?1 gives no improvement, which implies that jointly encoding the current utterance and the previous dialogue state is effective as well as efficient. 

 Effect of the schema graph In CSFN-DST, the schema graph with domain-slot relations is exploited. To check the effectiveness of the schema graph used, we remove knowledgeaware domain-slot relations by replacing the adjacency matrix A G as a fully connected one 1 or node-independent one I. Results in Table  4  show that joint goal accuracies of models without the schema graph are decreased similarly when BERT is either used or not. To reveal why the schema graph with domain- slot relations is essential for joint accuracy, we further make analysis on domain-specific and turnspecific results. As shown in Table  5 , the schema graph can benefit almost all domains except for Attaction (Attr.). As illustrated in Table  1 , the Attaction domain contains only three slots, which should be much simpler than the other domains. Therefore, we may say that the schema graph can help complicated domains. The turn-specific results are shown in Table  6 , where joint goal accuracies over different dialogue turns are calculated. From the table, we can see that data proportion of larger turn number becomes smaller while the larger turn number refers to more challenging conversation. From the results of the table, we can find the schema graph can make improvements over most dialogue turns. 

 Oracle experiments The predicted dialogue state at the last turn is utilized in the inference stage, which is mismatched with the training stage. An oracle experiment is conducted to show the impact of training-inference mismatching, where ground truth of the previous dialogue state is fed into CSFN-DST. The results in Table  4  show that joint accuracy can be nearly 80% with ground truth of the previous dialogue state. Other oracle experiments with ground truth slot-gate classification and ground truth value generation are also conducted, as shown in Table  4 . 

 Slot-gate classification We conduct experiments to evaluate our model performance on the slot-gate classification task. Table  7  shows F1 scores of the three slot gates, i.e., {NONE, DONTCARE, PTR}. It seems that the pretrained BERT model helps a lot in detecting slots of which the user doesn't care about values. The F1 score of DONTCARE is much lower than the others', which implies that detecting DONTCARE is a much challenging sub-task.   

 Discussion The main contributions of this work may focus on exploiting the schema graph with graph-based attention networks. Slot-relations are also utilized in DSTQA  (Zhou and Small, 2019) . However, DSTQA uses a dynamically-evolving knowledge graph for the dialogue context, and we use a static schema graph. We absorb the dialogue context by using the previous (predicted) dialogue state as another input. We believe that the two different usages of the slot relation graph can be complementary. Moreover, these two methods are different in value prediction that DSTQA exploits a hybrid of value classifier and span prediction layer, which relies on a predefined ontology. SOM-DST  is very similar to our proposed CSFN-DST with BERT. The main difference between SOM-DST and CSFN-DST is how to exploit the previous dialogue state. For the previous dialogue state, SOM-DST considers all domain-slot pairs and their values (if a domainslot pair contains an empty value, a special token NONE is used), while CSFN-DST only considers the domain-slot pairs with a non-empty value. Thus, SOM-DST knows which domain-slot pairs are empty and would like to be filled with a value. We think that it is the strength of SOM-DST. However, we choose to omit the domain-slot pairs with an empty value for a lower computation burden, which is proved in Table  8 . As shown in the last two rows of Table  2 , the schema graph can also improve SOM-DST, which achieves 52.23% and 53.19% joint accuracies on MultiWOZ 2.0 and 2.1, respectively. Appendix E shows how to exploit schema graph in SOM-DST. 

 Conclusion and Future Work We introduce a multi-domain dialogue state tracker with context and schema fusion networks, which involves slot relations and learns deep representations for each domain-slot pair dependently. Slots from different domains and their relations are organized as a schema graph. Our approach outperforms strong baselines on both MultiWOZ 2.0 and 2.1 benchmarks. Ablation studies also show that the effectiveness of the schema graph. It will be a future work to incorporate relations among dialogue states, utterances and domain schemata. To further mitigate the data sparsity problem of multi-domain DST, it would be also interesting to incorporate data augmentations  (Zhao et al., 2019)  and semi-supervised learnings  (Lan et al., 2018; Cao et al., 2019) . g k tj = GRU(g k?1 tj , e k tj ) GRU is initialized with g 0 tj = H Xt L,0 + H B t?1 L,0 and e 0 tj = H G L,M +N +j . The value generator transforms the hidden state to the probability distribution over the token vocabulary at the k-th step, which consists of two parts: 1) distribution over all input tokens, 2) distribution over the input vocabulary. The first part is computed as P ctx,k tj = softmax(ATT(g k tj , [H Xt L ; H B t?1 L ])) where  )  , and ATT(., .) is a function to get attention weights  with more details shown in Appendix B.1. The second part is calculated as P ctx,k tj ? R 1?(|Xt|+|B t?1 | c k tj = P ctx,k tj [H Xt L ; H B t?1 L ] P vocab,k tj = softmax([g k tj ; c k tj ]W proj E ) where P vocab,k tj ? R 1?d vocab , c k tj ? R 1?d model is a context vector, W proj ? R 2d model ?d model is a trainable parameter, and E ? R d vocab ?d model is the token embedding matrix shared across the encoder and the decoder. We use the soft copy mechanism  (See et al., 2017)  to get the final output distribution over all candidate tokens: where W gen ? R 3d model ?1 is a trainable parameter. The loss function for value decoder is L value = ? T t=1 j?Ct k log(P value,k tj ? (y value,k tj ) ) where y value,k tj is the one-hot token label for the j-th domain-slot pair at k-th step. 
