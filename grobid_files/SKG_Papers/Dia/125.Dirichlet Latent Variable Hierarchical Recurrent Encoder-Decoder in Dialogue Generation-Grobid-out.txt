title
Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder in Dialogue Generation

abstract
Variational encoder-decoders have achieved well-recognized performance in the dialogue generation task. Existing works simply assume the Gaussian priors of the latent variable, which are incapable of representing complex latent variables effectively. To address the issues, we propose to use the Dirichlet distribution with flexible structures to characterize the latent variables in place of the traditional Gaussian distribution, called Dirichlet Latent Variable Hierarchical Recurrent Encoder-Decoder model (Dir-VHRED). Based on which, we further find that there is redundancy among the dimensions of latent variable, and the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the latent variable. Therefore, controllable responses can be generated through specifying the value of each dimension of the latent variable. Experimental results on benchmarks show that our proposed Dir-VHRED yields substantial improvements on negative log-likelihood, word-embeddingbased and human evaluations.

Introduction Recurrent neural networks (RNNs)  (Bengio et al., 2003)  have achieved great success on many natural language processing tasks. For the dialogue generation task, a RNN-based Hierarchical Recurrent Encoder-Decoder (HRED) was first proposed in  (Serban et al., 2016) . It consists of three RNNs: encoder RNN, context RNN and decoder RNN. Firstly, the encoder RNN encodes each utterance into a fixed-size real-valued vector through word embedding. Then, the last hidden state of encoder RNN is feed into the context RNN to summarize the dialogue information. Finally, the decoder RNN takes the last state of context RNN as ? Corresponding author: Yisen Wang. input and produces the probability distribution of the word in the next utterance. Although it outperforms n-gram and other neural network language models, HRED only produces one word at a time, which is unable to fully grasp the holistic high-level syntactic properties (e.g., topics, tones or sentiment)  (Bowman et al., 2015) . When the sentence grows longer, it has the drawback of tending to generate short and generic responses  (Vinyals and Le, 2015) . Thus,  Serban et al. (2017)  proposed the Variational Hierarchical Recurrent Encoder-Decoder (VHRED) by combining HRED with Variational Autoencoders (VAEs)  (Kingma and Welling, 2013)  that introduced a latent variable to characterize the sentence-level representation for learning holistic properties. However, VHRED imposes a symmetric distribution (i.e., Gaussian distribution) to the latent variable, which, though facilitating analyzing, are incapable of representing complex latent variables effectively. Therefore, in order to generate more meaningful and expressive responses, a more flexible and tractable prior distribution of the latent variable is needed. In this paper, we propose to use the Dirichlet distribution to characterize the latent variable in VHRED, named Dir-VHRED. Dirichlet distribution is a popular conjugate prior for Multinomial distributions in Bayesian statistics. It can be concave or convex, monotonously rising or decreasing, symmetrical or asymmetrical, which makes it more flexible for better capturing the sentencelevel properties. Our main contributions are summarized as follows: ? We introduce Dirichlet distribution to VAEbased dialogue generation model and propose the Dir-VHRED model for better grasping the sentence-level properties and generating more meaningful and expressive responses. ? We find that the lengths and sentence patterns of the responses can be strongly correlated to each dimension of the latent variable. ? Experiments on three kinds of evaluation metrics demonstrate the superiority of our proposed Dir-VHRED model. 

 Preliminaries 

 Variational Autoencoders The key idea of Variational autoencoders (VAEs) is to reconstruct the input x through the latent variable z (Kingma and Welling, 2013). As the log-likelihood log p ? (x) is intractable, its lower bound Evidence Lower BOund (ELBO), denoted as L(?, ?; x), is involved to make the maximization tractable: log p ? (x) ? L(?, ?; x) = ?KL(q ? (z|x)||p(z)) + E q ? (z|x) [log p ? (x|z)], (1) where p(z) denotes the prior distribution of z, and q ? (z|x) is used for approximating the intractable true posterior distribution p ? (z|x). Note that the total loss is the negative of ELBO. 

 Dialogue Model Assuming dialogue D consisting of N utterances, the VAE-based dialogue model generates the responses by utilizing the latent variable z. The generation of the next utterance is defined by: p ? (z n |W <n ) = N (? prior(W<n),? prior (W<n) ), p ? (W n |z n , W <n ) = m p ? (w n,m |z n , w n,<m , W <n ), (2) where W n is the n-th utterance of the dialogue and w n,m indicates the m-th word of the n-th utterance. As the log-likelihood is intractable, instead of pursuing the exact maximum of the loglikelihood, we maximize its lower bound ELBO: log p ? (W 1 , W 2 , ? ? ? , W N ) ? N n=1 ?KL[q ? (z n |W 1 , W 2 , ? ? ? , W N )||p ? (z n |W <n )] + E q ? (zn|W 1 ,W 2 ,? ,W N ) [log(p ? (W N |z n , W <n ))], (3) where p ? (z n |W <n ) denotes the prior distribution of z n , and q ? (z n |W 1 , W 2 , ? ? ? , W N ) is used for approximating the intractable true posterior distri- bution p ? (z n |W 1 , W 2 , ? ? ? , W N ). 

 Dirichlet Distribution Dirichlet distribution, a family member of continuous multivariate probability distribution, is regarded as a multivariate generalization of the Beta distribution. In case of the Dirichlet distribution, it is a conjugate prior for the Multinomial distribution. The probability density function of Dirichlet distribution is given by: f (x 1 , ? ? ? , x K ; ? 1 , ? ? ? , ? K ) = 1 B(?) K i=1 x ? i ?1 i , (4) where parameter ? i > 0 is a K dimension vector and B(?) is a Beta function: B(?) = K i=1 ?(? i ) ?( K i=1 ? i ) , ? = (? 1 , ? ? ? , ? K ), (5) When ? 1 = ? 2 = ? ? ? = ? K , Dirichlet distribu- tion is a symmetric distribution. Particularly, when all the factors in ? equal 1, Dirichlet distribution becomes a uniform distribution. 

 The Proposed Dir-VHRED Model Mathematically, Dirichlet distribution owns a flexible structure. With different settings of parameter ?, Dirichlet distribution may have various forms, which can be concave or convex, monotonously rising or decreasing, symmetrical or asymmetrical. Therefore, equipped with the diverse structure, Dirichlet distribution is capable of modeling the complex latent variable. Furthermore, Dirichlet distribution has paved its way in many natural language processing tasks like topic model, text classification and so on. For example, it was introduced in the Latent Dirichlet Allocation (LDA)  (Blei et al., 2003)  to model the topic distribution and word distribution, and experimentally presents a well-recognized performance. Motivated by the above facts, we thus introduce Dirichlet distribution to model the latent variable z in order to better grasp the sentence-level properties which further encourages the better responses. With the distribution of z changing from Gaussian to Dirichlet, we may face a tough but important problem: the reparameterization of z. The variables with any distribution can be generated through the transformation of the samples from standard Uniform distribution if the corresponding inverse Cumulative Distribution Function (CDF) is known. For the Gaussian latent variable, it can make use of the inverse CDF to do the reparameterization  (Kingma and Welling, 2013) . While for the Dirichlet latent variable, unlike the known CDF of Gaussian distribution, the CDF of Dirichlet distribution is too complex to obtain. Inspired from  (Jankowiak and Obermeyer, 2018) , we use the reject sampling to solve the Dirichlet reparameterization problem. Furthermore, to mitigate KL-vanishing problem, in contrast to the static weight scheme in  (Bowman et al., 2015) , the imposed weight on KL-divergence in our model is dynamic. Since the KL-divergence and reconstruction loss antagonize each other, by setting the weight as the reciprocal of the reconstruction loss, the KL and reconstruction loss enable to keep a dynamic balance, which leads to a more stable KL-divergence. Following the techniques described above, ELBO then has the form: log p ? (x) ? L(?, ?; x) = ?KL(q ? (z|x)||p(z)) + E q ? (z|x) [log(p ? (x|z))], (6) where ? = ?1/E q ? (z|x) [log(p ? (x|z))], (7) and the KL-divergence term can be derived as * : KL(q ? (z|x)||p(z)) = log ?( K k=1 ? k ) ? K k=1 log ?(? k ) ? log ?( K k=1 ? k ) + K k=1 log ?(? k ) + K k=1 (? k ? ? k )(?(? k ) ? ?( K k=1 ? k )), (8) where ?, ? are parameters of Dirichlet distribution q ? (z|x) and p(z) respectively, K is the dimension of z, and ? is the Digamma function. 

 Experiments 

 Datasets All experiments are conducted on the following two dialogue datasets: * Detailed derivation can be found in Appendix A.1. The word drop rate is set to 0.25, the dimensionality of latent variable z is 3, and the word embedding size is 200 ? . For VHRED, the weight parameter ? of KLdivergence is initialized as 0 and gradually increased to 1 at the 20, 000-th/100, 000-th training steps for Movie/Ubuntu datasets. While, for Dir-VHRED, the parameter ? is adaptively determined by Eq. (  7 ). For the dataset, we truncate utterances longer than 30 words and split the train/validation/test sets by 0.8/0.1/0.1 respectively. All models are trained by Adam optimizer (Kingma and Ba, 2014) with batch size 40 and learning rate 1 ? 10 ?4 . At the training time, we stop the training when the loss on the validation set does not decrease within 5 epochs. At the evaluating time, beam search  (Wiseman and Rush, 2016)  with beam size 5 is used for generating output responses. Both the single response (1-turn) and the three consecutive responses (3-turn) are evaluated for each model. Our code is available at https://github.com/ cloversjtu/dir-vhred. 

 Performance Evaluation In order to comprehensively evaluate the model performance on dialogue generation, we adopt the following three evaluation metrics: 1) negative log-likelihood metric to measure the loss of the generating procedure; 2) word-embedding metric to measure the cosine distance between the generated responses and the ground truth responses; and 3) human evaluation. 

 Negative Log-likelihood Evaluation Table  1  reports the comparisons of the per-word negative log-likelihood (NLL) composition of different models. NLL consists of the reconstruction loss (reconstruction) and the KL-divergence (KL-div.). KL-divergence indicates the information encoded in z, and reconstruction loss represents the loss of reconstructing x through z. As can be seen from Table  1 , with almost the same KL-divergence, Dir-VHRED achieves the lowest NLL on both datasets, implying better performance compared with VHRED. Dir-VHRED also has the lower reconstruction loss, which indicates that Dirichlet prior is better than the Gaussian prior for reconstructing the responses.  

 Word-embedding-based Evaluation Word-embedding metric is designed to measure the similarity between words, and can be divided into three categories: average  (Foltz et al., 1998) , greedy  (Rus and Lintean, 2012)  and extrema  (Forgues et al., 2014) . The average metric calculates sentence-level embeddings, while the greedy and extrema ones compute the wordto-word cosine similarity. Their difference lies on that greedy takes the average word vector in the sentence as the sentence embedding while extrema adopts the extreme value of these word vectors. Tables  2 and 3  demonstrate the results of word-embedding metric on 1-turn and 3-turn responses from different models. Embedding metrics first map the generating responses to a vector space and then compute the cosine similarly with corresponding ground responses. Therefore,  it can be used to measure the sentence-level semantic similarity to a certain extent. Dir-VHRED achieves the best performance among the three word-embedding metrics on 1-turn and 3-turn responses of the two datasets. That is, the responses generated by Dir-VHRED own the highest topic similarity with the ground truth. 

 Human Evaluation Moreover, we invite 7 qualified volunteers to do the human evaluation on 700 randomly sampled dialogues from Ubuntu dataset. Volunteers are required to rate the generated responses from different models with scores from 1 to 3 (3 is the best), in term of the sentence coherence and contextual topic. The average human evaluation scores for HRED, VHRED and Dir-VHRED are 2.50, 2.65 and 2.70 respectively. We also conduct Friedman test  (Friedman, 1940)  on the human evaluation score to check its statistical significance, and the value of Chi-square test statistics is 5696.25 which indicates that the P-value is less than 0.05. Thus, Dir-VHRED is significantly better than others. Table  4  lists some examples of the generated sentences of different models. It can be seen that the responses generated by Dir-VHRED best match the context given the input sentence, compared with other models. Dir-VHRED experimentally demonstrates to be able to generate fluent and diverse responses which match the human daily speaking style smoothly. 

 Interpretability of Dir-VHRED In our above experiments, we find an interesting phenomenon for Dir-VHRED that there are many non-active dimensions of z and the number of active ones is almost always around 3. Therefore, we conduct additional ablation study experiments to analyze each dimension of z. To be specific, we circularly set one dimension of z to 1 and the others to 0, like "100", "010" and "001". We demonstrate some generated responses through ablation study on each dimension of z on Ubuntu and Movie datasets in Table We can observe that the dimension 1 (dim1) is inclined to generate the shortest responses which, to a certain extent, shows the attitude of the speaker. Specifically, it prefers to generate the responses like "thanks", "no" or "yes". The dimension 2 (dim2) usually generates longer and informative responses. For example on Movie dataset, the generated response is prone to end with "are you?" or "aren't you?", which shows a preference for rhetorical question. Other examples include "you're not going to be here, are you?" and "well, you're a good man, aren't you?". Intuitively, since the Movie dataset is a collection of conversations extracted from movie scripts, the generated responses are closer to daily dialogues. While for Ubuntu, the responses do not have strong individual emotion and rhetorical question rarely appear, as the conversations in this dataset are professional dialogue about Ubuntu system knowledge. The dimension 3 (dim3) focuses on generating the longest and most comprehensive responses that are usually declarative sentences and are high- # 1 Input sentence: does grub stay into hdd after installing another os? # 1 Generated response: dim1: yes. dim2: yes, it will be in the mbr of the drive. dim3: yes, it will overwrite grub, but it will overwrite the mbr, and then you can use the grub menu to boot the os. # 2 Input sentence: that leads me to the next question lol ... i do not know how to create new volumes exactly in cmdline, usually i use a gui. # 2 Generated response: dim1: gksudo. dim2: alt f2 gksudo nautilus and gksudo nautilus will work. dim3: <unk> is a gui app for managing users, you can use gksudo nautilus and you can use gksudo nautilus toget a root terminal and you can use. # 3 Input sentence: i see this use at your own risk. http askubuntu.com questions <unk> ubuntu 12 04 nvidia gtx 460 video card installation. # 3 Generated response: dim1: thanks. dim2: thanks, i 'll give that a try. thanks ! dim3: i have the nvidia drivers installed, but i can't see the nvidia logo on the screen. i have to install the drivers for the graphics card. # 4 Input sentence: she isn't my cousin. # 4 Generated response: dim1: yes. dim2: she 's a nurse, isn't she ? dim3: yes, she 's a good friend of mine, she 's a nurse. est close to the dialogue topic. Therefore, we can easily generate the responses with the preferred style and length through changing the value of each dimension of z. 

 Conclusion In this paper, we proposed to use Dirichlet distribution in place of traditional Gaussian distribution in VHRED for dialogue generation, called Dir-VHRED, to well capture the sentence-level properties. We also provided a new way on the setting of the weight of KL-divergence to alleviate KLvanishing problem. Moreover, we found that the lengths and sentence patterns of the generated responses are correlated to the value of each dimension of the latent variable, which can be used for generating the required responses. Experiments on benchmark datasets show the superior of our proposed Dir-VHRED model. Table 1 : 1 Results of NLL on Ubuntu and Movie datasets with different models. The ? symbol denotes the variational bound. Ubuntu dataset Model NLL reconstruction KL-div. HRED 3.844 - - VHRED ? 4.132 3.765 0.367 Dir-VHRED ? 3.999 3.614 0.385 Movie dataset Model NLL reconstruction KL-div. HRED 3.944 - - VHRED ? 4.233 3.904 0.330 Dir-VHRED ? 4.073 3.741 0.332 
