title
Dialogue-Act Prediction of Future Responses based on Conversation History

abstract
Sequence-to-sequence models are a common approach to develop a chatbot. They can train a conversational model in an end-to-end manner. One significant drawback of such a neural network based approach is that the response generation process is a black-box, and how a specific response is generated is unclear. To tackle this problem, an interpretable response generation mechanism is desired. As a step toward this direction, we focus on dialogue-acts (DAs) that may provide insight to understand the response generation process. In particular, we propose a method to predict a DA of the next response based on the history of previous utterances and their DAs. Experiments using a Switch Board Dialogue Act corpus show that compared to the baseline considering only a single utterance, our model achieves 10.8% higher F1-score and 3.0% higher accuracy on DA prediction.

Introduction Dialogue systems adopt neural networks (NNs)  (Vinyals and Le, 2015)  because they allow a model to be developed in an end-to-end manner without manually designed rules and patterns for response generation. However, in a NN-based approach, the response generation process is hidden in the model, which makes it difficult to understand why the model generates a specific response. This is a significant problem in commercially produced chatbots because the model outputs cannot be controlled. To tackle this problem,  Zhao et al. (2018)  argued that interpretable response generation models are important. As the first step toward this direction, we focus on dialogue-acts (DAs) as clues to understand the response generation process. We speculate that the predicted DAs indicates which types of response the model tries to generate. Utterance (DA) 1 Oh, I've only, I've only skied in Utah once. (Statement) 2 Oh, really? (Question) 3 I only skied once my whole life. (Statement) 4 Uh-huh. (Uninterpretable) 5 But, do you do a lot of skiing there? (Question) Table  1 : Example of utterances and their DAs (in parenthesis) sampled from the SwDA corpus. Specifically, we propose a method to predict the DA of the next response. This problem was proposed by  Reithinger et al. (1996) . A conversation consists of a sequence of utterances and responses, where the next response depends on the history of utterances and responses. Table  1  shows an example of a conversation with utterances and their DAs sampled from the Switch Board Dialogue Act (SwDA) corpus. The DA of the last response, "But, do you do a lot of skiing there? (Question)" is not predictable using the previous utterance of "Uh-huh." nor using its DA of "Uninterpretable". To correctly predict the DA, we need to refer to the entire sequence starting from first utterance of "Oh, I've only skied in Utah once." when the speaker is talking about skiing experience. Our model considers the conversation history for DA prediction. It independently encodes sequences of text surfaces and DAs of utterances using a recurrent neural network (RNN). Then it predicts the most likely DA of the next response based on the outputs of RNNs.  Cervone et al. (2018)  showed that a DA is useful to improve the coherency of response. The predicted DAs can be used to generate a future response, which adds controllability and interpretability into a neural di-alogue system. We used a SwDA corpus for the evaluation, in which telephone conversations are transcribed and annotated with DAs. The macro Precision, Recall, F1, and overall Accuracy measure the performance compared the baseline. The results show that our model, which considers the history of utterances and their DAs, outperforms the baseline, which only considers the input utterance by 10.8% F1 and 3.0% Accuracy. 

 Related Works Previous studies on DA prediction aimed to predict the current DA from the corresponding utterance text.  Kalchbrenner and Blunsom (2013)  proposed a method using Convolutional Neural Network (CNN) to obtain a representation capturing the local features of utterance and RNN to obtain the context representation of the utterance. Experiments using the SwDA corpus showed that their method outperformed previous methods for DA prediction using non-neural machine learning models.  Khanpour et al. (2016)  proposed a method based on multi-layer RNN that uses an utterance as an input. Their method achieved an 80.1% prediction accuracy of the SwDA corpus, and is the current state-of-the-art method. Unlike these previous studies, we focus on DA prediction of the next (i.e., unseen) response.  Reithinger et al. (1996)  proposed a statistical method using a Markov chain. Using their original corpus, their method achieved of the 76.1% top 3 accuracy. We tackled the problem of DA prediction of the next utterance considering the history of utterances and previous DAs using a NN. We anticipate that the predicted DA is useful for understanding the response generation process and improving the quality of the response generation. 

 Proposed Model Figure  1  illustrates the design of our model, which consists of three encoders with different purposes. The Utterance Encoder encodes the utterance text into a vector, which is then inputted into the Context Encoder that handles the history of utterance texts. The Dialogue-act (DA) Encoder encodes and handles the sequence of DAs. Finally, outputs of the Context and DA Encoders are concatenated and input to a classifier that predicts the DA of the next response. Note that our model does not peek into the text of next response to predict the DA. Consequently, the predicted DA is used to generate the response text in the future. 

 GRU ! ",$ ! ",% ! ",& ! "'$,& ! "'$,$ ! "'$,% () " () "'$ () "'$ () "'& GRU GRU GRU GRU 

 Utterance Encoder & Context Encoder The Utterance Encoder vectorizes an input utterance. It is an RNN that takes each word in the utterance in a forward direction by applying padding in order to realize a uniform input size. Then, the Context Encoder, which is another RNN, takes the final output of the Utterance Encoder to generate a context vector that handles the history of utterances. While our model takes a single sentence as an input to the Utterance Encoder, the speakers do not necessarily change at every single sentence in a natural conversation. Hence, our model allows cases where the same speaker continuously speaks. Specifically, a speaker change tag, which is inputted into the Context Encoder, is used to indicate when the speaker changes. 

 Dialogue-act (DA) Encoder The DA Encoder plays the role of handling the history of DAs. A DA is represented as a one-hot vector and encoded by RNN. During the training, we use teacher forcing to avoid error propagation. That is, the gold DA of the current utterance is inputted into the model instead of the predicted one. 

 Dialogue-act Prediction Finally, the classifier determines the DA of the next response. It is a single fully-connected layer culminating in the soft-max layer.  We evaluate the accuracy of our model to predict the DA of the next response using the SwDA corpus, which transcribes telephone conversation and annotates DAs of utterances. The SwDA corpus conforms to the damsl tag schema.  1  We assembled the tag sets referring to easy damsl (Isomura et al., 2009) into 9 tags (Table  2 ) in order to consolidate tags with a significantly low frequency. The SwDA corpus provides transcriptions of 1, 155 conversations with 219, 297 utterances. One conversation contains 189 utterances on average. Because the average length of utterance sequences is large, we use a sliding window with a size of 5 to cut a sequence into several conversations. The number of conversations increases to 212, 367 with 1, 061, 835 utterances. Table  2  shows the distribution of DAs in the processed corpus. We randomly divide the conversations in the corpus into 80%, 10%, and 10% for training, development, and testing, respectively. 

 Model Settings We apply a Gated Recurrent Unit (GRU)  (Cho et al., 2014)  to each RNN in our model. We set the dimensions of word embedding to 300 and those of the DA embedding to 100. The dimensions of the GRU hidden unit of the Utterance Encoder are 1 https://web.stanford.edu/ ?jurafsky/ ws97/manual.august1.html We use teacher forcing for training and similar setting for testing by inputting the gold DA of the previous time step into the DA Encoder. This means that the evaluation results here show the performance when the predictions of the previous time steps are all correct. As Table  2  shows, the numbers of DAs are highly diverse. To avoid frequent tags dominating the results, we measure the macro averages of precision, recall, and F1-score of each DA. We also measure the overall accuracy. 

 Baselines To investigate the effects of each encoder in our model, we compare our model to the baseline (Table 3). The second and third rows are simple methods. Max-Probability is another non-neural baseline that outputs the DA with highest conditional probability from the input DA. Figure  2  shows the conditional probability of DA transitions computed in our training set. "Greeting" has a no- ticeable pattern in which it is followed by "Greeting". This is natural considering human communication. On the other hand, other DAs are mostly followed by "Statement". This implies that only a previous DA is insufficient to predict the next DA. The rest of Table  3  shows NN-based baselines. The Utterance-only is the model that only has the Utterance Encoder (i.e., it predicts the DA of the next response based only on the input utterance). The Utterance-seq, which has the Utterance Encoder and Context Encoder, predicts the DA based on a sequence of utterances. On the other hand, the DAseq-only has only the DA Encoder and predicts the DA of the next response based on the sequence of previous DAs. The DAseq+Utterance has the Utterance Encoder and DA Encoder, which considers the sequence of DAs and the single utterance. The DA+Utterance-seq contains the Utterance Encoder and Context Encoder. It considers only the DA of the input utterance and not the sequence. 

 Results Table  3  shows the macro averages of the precision, recall, and F1-score, as well as overall accuracies for each model. For all evaluation model, our model exhibits the best performances; recall, F1, and accuracy 32.5%, 32.4%, and 69.7%, respectively. As discussed in Section 1,  Khanpour et al. (2016)  achieved 80.1% prediction accuracy for the same SwDA corpus. Their method predicts the DAs of the current utterance given in text. Although their accuracy is not directly comparable to ours due to differences in data splits, 80.1% can be regarded as the upper-bounds of our task. Our method achieves 87.0% of this upper-bound. Below we investigate which encoders contribute to prediction. Max-Probability performs quite poorly rather than other neural network based model. This may be because of the imbalanced transition patterns of DAs as shown in Figure  2 , which shows that the next DA prediction requires more features to achieve precise prediction. Utterance-seq achieves 1.8% higher accuracy than Utterance-only, demonstrating the effectiveness of considering the history of utterances rather than a single utterance. The DA+Utterance-seq outperforms Utteranceseq on F1 by 6.5%. This result implies that a previous DA is an effective hint for DA prediction of next responses. In addition, the sequence of DAs is also effective for the next DA prediction, which is shown by the superior performance of the DAseq-only to the Utteranceseq. Specifically, DAseq-only performs 4.1% higher macro-F1 than Utterance-seq, but has 1.4% lower accuracy than Utterance-seq. Similarly, DAseq+Utterance achieves 5.5% higher F1 than Utterance-seq. Overall, DAs of either single-turn or a sequence largely boost precision, recall, and F1. On the other hand, a sequence of utterances contributes to accuracy. These results imply that the sequence of DAs is effective to predict infrequent DAs and the sequence of utterances is effective to predict common DAs. This may be because the DA Encoder is more robust against the data sparseness issue due to its much smaller vocabulary size compared to that of the Utterance Encoder. These analyses show that our model achieves the best performance considering both sequence of utterances and DAs.  proposed model and Utterance-seq. The proposed model outperforms Utterance-seq on all the DAs. In particular, infrequent tags of "Agreement", "Greeting", "Question" and "Apology" show significant improvements between 6.1% and 34.6%. Furthermore, the proposed model correctly predicts "Directive" and "Other" even though Utterance-seq does not predict any of these correctly. 

 Examples of Predicted DAs Table  5  shows examples of the predicted DAs by the proposed model and Utterance-seq. The first example shows that the proposed model correctly predicts "Agreement", which only has 5.2% occurrence in the training set, whereas Utterance-seq most frequently predicts it as "Statement". The second and third examples demonstrate the difficulty of DA prediction of the next response. The input utterances of these examples have the same DA sequences, but the DAs of the final responses differ ("Understanding" and "Agreement"). While both the proposed model and Utterance-seq correctly predict the final DA of the second example, both fail in the third example. The third conversation is about an aerosol, and the response to the final utterance of "and it's kind of dangerous." depends on if one of the speakers understands the danger of the aerosol. To correctly predict DAs in such a case, a much longer conversation sequence and/or personalize the prediction model must be considered based on profiles or knowledge of speakers. This is the direction of our future work. 

 Conclusion We propose a method to predict a DA of the next response considering the sequences of utterances and DAs. The evaluation results using the SwDA corpus show that the proposed model achieves 69.7% accuracy and 32.4% macro-F1. Additionally, the results show that the sequence of DAs significantly helps the prediction of infrequent DAs. In the future, we plan to develop a response generation model using the predicted DAs. Figure 1 : 1 Figure 1: Design of our model consisting of three encoders that encode 1) text surfaces, 2) DAs, and 3) utterance history. (? concatenates vectors) 
