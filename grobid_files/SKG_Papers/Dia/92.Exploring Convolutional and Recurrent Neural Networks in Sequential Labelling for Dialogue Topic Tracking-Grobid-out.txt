title
Exploring Convolutional and Recurrent Neural Networks in Sequential Labelling for Dialogue Topic Tracking

abstract
Dialogue topic tracking is a sequential labelling problem of recognizing the topic state at each time step in given dialogue sequences. This paper presents various artificial neural network models for dialogue topic tracking, including convolutional neural networks to account for semantics at each individual utterance, and recurrent neural networks to account for conversational contexts along multiple turns in the dialogue history. The experimental results demonstrate that our proposed models can significantly improve the tracking performances in human-human conversations.

Introduction A human conversation often involves a series of multiple topics contextually related to each other. In this scenario, every participant in the conversation is required to understand the on-going topic discussed at each moment, detect any topic shift made by others, and make a decision to selfinitiate a new topic. These human capabilities for handling topics are also expected from dialogue systems to achieve natural and human-like conversations. Many studies have been conducted on multidomain or multi-task dialogue systems by means of sentence-level topic identification as a subtask of natural language understanding  (Lin et al., 1999; Nakata et al., 2002; Lagus and Kuusisto, 2002; Adams and Martell, 2008; Ikeda et al., 2008; Celikyilmaz et al., 2011) . In these approaches, a given user input at each turn is categorized into topic classes, each of which triggers the corresponding sub-system specializing in the particular topic. Despite many previous efforts, the sentence categorization methods still have the following limitations. Firstly, the effectiveness of the approaches is limited only in user-initiative conversations, because the categorization is performed mainly based on the user's input mentioned at a given turn. Secondly, no correlation between different topics is considered neither in the topic decision process nor in each topic-specific sub-system operated independently from the others. Lastly, the conversational coherence in a given dialogue history sequence has limited effects on determining the current topic. Another direction for multi-topic dialogue systems has been towards utilizing human knowledge represented in domain models  (Roy and Subramaniam, 2006)  and agendas  (Bohus and Rudnicky, 2003; Lee et al., 2008) . The knowledge-based approaches make the system capable of having more control of dialogue flows including topic sequences. This aspect contributes to better decisions of topics in system-initiative cases, but it can adversely affect the flexibility to deal with unexpected inputs against the system's suggestions. Moreover, the high cost of building the required resources is another problem that these methods face from a practical point of view. Recently, some researchers  (Morchid et al., 2014a; Morchid et al., 2014b; Esteve et al., 2015)  have worked on topic identification for analyzing human-human dialogues. Although they don't aim at building components in dialogue systems directly, the human behaviours learned from the conversations can suggest directions for further advancement of conversational agents. However, the problem defined in the studies is under the assumption that every dialogue session is assigned with just a single theme category, which means any topic shift occurred in a session is left out of consideration in the analyses. On the other hand, we previously addressed the problem of detecting multiple topic transitions in mixed-initiative human-human conversations, which is called dialogue topic tracking  (Kim et al., 2014a; Kim et al., 2014b) . In these studies, the tracking task is formulated as a classification problem for each utterance-level, similar to the sentence categorization approaches. But the target of the classification is not just an individual topic category to which each input sentence belongs, but the decision whether a topic transition occurs at a given turn as well as what the most probable topic category will follow after the transition. This paper presents our work also on dialogue topic tracking mainly focusing on the following issues. Firstly, in addition to transitions between dialogue segments from different topics, transitions between segments belonging to the same topic are also detected. This focuses the task more on detailed aspects of topic handling that are relevant to other subtasks such as natural language understanding and dialogue state tracking, rather than the conventional tracking of changes in topic categories only. Another contribution of this work is that we introduce a way to use convolutional neural networks in topic tracking to improve the classification performances with the learned convolutional features. In addition, we also propose the architectures based on recurrent neural networks to incorporate the temporal coherence that has not played an important role in previous approaches. The remainder of this paper is structured as follows. We present a problem definition of dialog topic tracking in Section 2. We describe our proposed approaches to this task using convolutional and recurrent neural networks in Section 3. We report the evaluation result of the methods in Section 4 and conclude this paper in Section 5. 

 Dialogue Topic Tracking Dialogue topic tracking is defined as a multi-class classification problem to categorize the topic state at each time step into the labels encoded in BIO tagging scheme  (Ramshaw and Marcus, 1995)  as follows: f (t) = ? ? ? ? ? ? ? ? ? ? ? B-{c ? C} if u t is at the beginning of a segment belongs to c, I-{c ? C} else if u t is inside a segment belongs to c, O otherwise, where u t is the t-th utterance in a given dialogue session and C is a closed set of topic categories. Figure  1  shows an example of topic tracking on a dialogue fragment between a tour guide and a tourist. Since each tag starting with 'B' should occur at the beginning of a new segment after a topic transition from its previous one, the label sequence indicates that this conversation is divided into six segments at t = {2, 4, 6, 11, 13}. The initiativity of each segment can be also found from who the speaker of the first utterance of the segment is. In this example, three of the cases are initiated by the tourist at t = {2, 4, 6}, but the others are leaded by the tour guide, which means it is a mixed-initiative type of conversation. Different from the former studies  (Kim et al., 2014a; Kim et al., 2014b ) that were only focused on detecting transitions between different topic categories, this work subdivides each dialogue sequence which belongs to a single topic category, but discusses more than one subject that can be more specifically differentiated from each other. The above example also has two cases of transitions with no change of topic categories at t = {4, 11}: the first one is due to the tourist's request for an alternative attraction from the recommendation in the previous segment, and the other transition is triggered by the tour guide to suggest another option of transportation which is also available for the route discussed previously.   

 Models The classifier f can be built with supervised machine learning techniques, when a set of example dialogues manually annotated with gold standard labels are available as a training set. The earlier studies  (Kim et al., 2014a; Kim et al., 2014b ) also proposed supervised classification approaches particularly focusing on kernel methods to incorporate domain knowledge obtained from external resources into the linear vector space models based on bag-of-words features extracted from the training dialogues. This work, on the other hand, aims at improving the classification capabilities only with the internal contents in given dialogues rather than making better uses of external knowledge. To overcome the limitations of the simple vector space models used in the previous work, we propose models based on convolutional and recurrent neural network architectures. These models are presented in the remainder of this section. 

 Convolutional Neural Networks A convolutional neural network (CNN) automatically learns the filters in its convolutional layers which are applied to extract local features from inputs. Then, these lower-level features are combined into higher-level representations following a given network architecture. These aspects of CNNs make themselves a good fit to solve the problems which are invariant to the location where each feature is extracted on its input space and also depend on the compositional relationships between local and global features, which is the reason why CNNs have succeed in computer vision  (LeCun et al., 1998) . As implied by the successes of bag-of-words or bag-of-ngrams considering the existence of each linguistic unit independently and the important roles of compositional structures in linguistics, CNN models have recently achieved significant improvements also in some natural language processing tasks  (Collobert et al., 2011; Shen et al., 2014; Yih et al., 2014; Kalchbrenner et al., 2014a; Kim, 2014) . The model for dialogue topic tracking (Figure  2 ) is basically based on the CNN architecture proposed by  Collobert et al. (2011)  and  Kim (2014)  for sentence classification tasks. In the architecture, a sentence of length n is represented as a matrix with the size of n ? k concatenated with n rows each of which is the kdimensional word vector x i ? R k representing the i-th word in the sentence. This embedding layer can be learned from scratch with random initialization or fine-tuned from pre-trained word vectors  (Mikolov et al., 2013)  with back propagation during training the network. Unlike other sentence classification tasks, dialogue topic tracking should consider not only a single sentence given at each time step, but also the other utterances previously mentioned. To incorporate the dependencies to the dialogue history into the topic tracking model, the input at the time step t is composed of three different channels each of which represents the current utterance u t , the previous utterance u t?1 , and the other utterances u t?h+1:t?2 within h time steps, respectively, where u t is the t-th utterance in a session, u i:j is the concatenation of the utterances occurred from the i-th to the j-th time steps in the history, and h is the size of history window. The height of the n ? k matrices of the first two channels for the current and previous utterances is fixed to the length of the longest utterance in the whole training dataset, and then all the remaining rows after the end of each utterance are zero-padded to make all inputs same size. Since the other channel is made up by concatenating the utterances from the (t ? h + 1)-th to the (t ? 2)-th time steps, it has a matrix with the dimension of ((h ? 2) ? n) ? k where all the gaps between contiguous utterances in the matrix are filled with zero. In the convolutional layer, each filter F ? R km which has the same width k as the input matrix and a given window size m as its height slides over from the first row to the (n ? m + 1)-th row of the input matrix. At the i-th position, the filter is applied to generate a feature c i = g (F ? x i:i+m?1 + b), where x i:j is the subregion from the i-th row to the j-th row in the input, b ? R is a bias term, and g is a non-linear activation function such as rectified linear units. This series of convolution operations produces a feature map c = [c 1 ? ? ? c n?m+1 ] ? R n?m+1 for the filter F. Then, the maximum value c = max( c) is selected from each feature map considered as the most important feature for the particular filter in the max-pooilng layer. Every filter is shared across all the three different channels, but both the convolution and maxpooling operations are performed individually for each channel. Thus, the total number of feature values generated in the pooling layer is three times the number of filters. Finally, these values are forwarded to the fully-connected layer with softmax which generates the probabilistic distribution over the topic labels for a given input. 

 Recurrent Neural Networks Dialogue topic tracking is conceptually performed on a sequence of interactions exchanged by the participants in a given session from its beginning to each turn. Thus, the contents discussed previously in the dialogue history are likely to have an important influence on tracking the current topic at a given turn, which is a fundamental difference from other text categorization problems that consider each input independently from all others. To make use of the sequential dependencies in dialogue topic tracking, we propose the models based on recurrent neural networks (RNN) which learn the temporal dynamics by recurrent computations applied to every time step in a given in- u t-h+1 ? u t-2 u t-1 u t 

 Inputs Utterance-level embedding layer RNNs have been successfully applied to several natural language processing tasks including language modeling  (Mikolov et al., 2010) , text generation  (Sutskever et al., 2011) , and machine translation  (Auli et al., 2013; Liu et al., 2014) , all of which focus on dealing with variable length word sequences. On the other hand, an input sequence to be handled in dialogue topic tracking is composed of utterance-level units instead of words. s f t-h+1 s f t-2 s f t-1 s f t Forward layer s b t-h+1 s b t-2 s b t- In our model (Figure  3 ), each utterance is represented by the k-dimensional vector u t ? R k assigned with pre-trained sentence-level embeddings  (Le and Mikolov, 2014) . And then, a sequence of the utterance vectors within h time steps are connected in the recurrent layers. The default sequence of applying the recurrent operation is the ascending order from the former to the recent utterances, which is performed in the forward layer. But the opposite direction can be also considered in the backward layer which is stacked on top of the forward layer to build a bidirectional RNN  (Schuster and Paliwal, 1997)  which outputs the concatenation of both forward and backward states as an outcome of the recurrent operations. Then, these hidden states from the recurrent layers are passed to the fully-connected softmax layer to generate the output distributions for every time step in the sequence. ? Inputs ? ut-1 ut ut-2 ut-h+1 

 Convolutional layer Forward layer s f t-1 s f t s f t-2 s f t-h+1 Backward layer s b t-1 s b t s b t-2 s b t-h+1 Output labels yt-1 yt yt-2 yt-h+1 Max pooling layer The output from the model at a given time step t is a label sequence [y t?h+1 , ? ? ? , y t ] for the recent h utterances. Since the labels for the earlier utterances should have been already decided at the corresponding turns, only y t is taken as the final outcome for the current time step. The hypothesis to be examined with this model is whether the other h ? 1 predictions that are not directly reflected to the results could help to improve the tracking performances by being considered together in the process of determining the current topic status. 

 Recurrent Convolutional Networks The last approach proposed in this work aims at combining the two models described in the previous sections. In this model (Figure  4 ), each feature vector generated through the embedding, convolutional, and max pooling layers in the CNN network (Section 3.1) is connected to the recurrent layers in the RNN model (Section 3.2). This combination is expected to play a significant role in overcoming the limitations of the sentence-level embedding considered as a feature representation in the RNN model. While the previous approach depends only on a pre-trained and non-tunable embedding model, all the parameters in the combined network can be fine-tuned with back propagation by considering the convolutional features extracted at each time step and also the temporal dependencies occurred through multiple time steps in given dialogue sequences. In computer vision, this kind of models connecting RNNs on top of CNNs is called recurrent convolutional neural networks (RCNN), which have been mostly used for exploring the dependencies between local convolutional features within a single image  (Pinheiro and Collobert, 2014; Liang and Hu, 2015) . Recently, they are also applied in video processing  (Donahue et al., 2015)  where visual features are extracted from the image at each frame using CNNs and the temporal aspects are learned with RNNs from the frame sequence of an input video. Our proposed model for dialogue topic tracking was originally motivated by this success of RCNNs particularly in video recognition considering that video and dialogue are analogous from the structural point of view. Each instance of a video and a dialogue consists of a temporal sequence of static units. 

 Evaluation 4.1 Data To demonstrate the effectiveness of our proposed models, we performed experiments on TourSG corpus released for the fourth dialogue state tracking challenge (DSTC4)  (Kim et al., 2016) . The dataset consists of 35 dialogue sessions collected from human-human conversations about tourism in Singapore between tour guides and tourists. All the dialogues have been manually transcribed and annotated with the labels for the challenge tasks. For the multi-topic dialogue state tracking which is the main task of the challenge, each dialogue session is divided into sub-dialogues and each segment is assigned with its topic category. Since the task particularly focuses on filling out the topicspecific frame structure with the detailed information representing the dialogue states of a given segment, it has been performed under the assumption that the manual annotations for both segmentations and topic categories are provided as parts of every input. But, in this work for dialogue topic tracking, these labels are considered as the targets to be generated automatically by the models. Every segment in the dataset belongs to one of eight topic categories. Following the nature of the tourism domain, the 'attraction' category accounts for the highest portion at 40.12% of the segments, which is followed by 'transportation', 'food  ', 'accommodation', 'shopping', 'closing' and 'opening'  in order according to decreasing frequencies. The other 10.53% considered as beyond the scope of the task are annotated with 'other'. Figure  5  shows the distributions of the segments by not only the topic categories, but also the transition types from two different points of views: the first one is which speaker initiates each segment, and the other is whether the segmentation causes  For our experiments, all these segment-level annotations were converted into utterance-level BIO tags each of which belongs to one of 15 classes: ({B-, I-} ? {c : c ? C; and c = 'other'}) ? {O}, where C consists of all the eight topic categories. The partition of the dataset (Table  1 ) have been kept the same as the one used for the state tracking task in DSTC4. 

 Models Based on the dataset, we built 16 different models classified into the following five model families. 

 Baseline 1: Support Vector Machines The first baseline uses support vector machine (SVM)  (Cortes and Vapnik, 1995)  models trained with the following features: ? BoN t : bag of uni/bi/tri-grams in u t weighted by tf-idf which is the product of term frequency in u t and inverse document frequency across all the training utterances. ? BoN t?1 : bag of n-grams computed in the same way as BoN t for the previous utterance. ? BoN history = h j=0 ? j ? BoN j : weighted sum of n-gram vectors in the recent h = 10 utterances with a decay factor ? = 0.9. ? SPK t , SPK t?1 : speakers of the current and the previous utterances. ? SPK {t?1,t} : bi-gram of SPK t and SPK t?1 . Another variation replaces the bag of n-grams with the utterance-level neural embeddings inferred by the pre-trained 300 dimensional doc2vec  (Le and Mikolov, 2014)  model on 2.9M sentences with 37M words in 553k Singapore-related posts collected from travel forums. Then, the third model takes the concatenation of both bag of n-grams and doc2vec features. All three baselines were implemented based on the one-against-all approach with the same number of binary classifiers as the total number of classes for multi-label classification. SVM light  (Joachims, 1999)  was used for building each binary classfier with the linear kernel. 

 Baseline 2: Conditional Random Fields To incorporate the temporal aspects also into the linear models, conditional random fields (CRFs)  (Lafferty et al., 2001)  which have been successfully applied for other sequential labelling problems were used for the second set of baselines. Similar to our proposed RNN architecture (Section 3.2), the recent utterances occurred within the window size of h = 10 composed the first-order linear-chain CRFs. Three CRF models were built using CRFsuite  (Okazaki, 2007)  with the same feature sets as in the SVM models. 

 CNN-based models For the CNN architecture (Section 3.1), we compared two different models: the first one learned the word embeddings from scratch with random parameters, while the other was initialized with word2vec  (Mikolov et al., 2013)  trained on the same dataset for the doc2vec model in Section 4.2.1. Both approaches generated a dense vector with a dimension of k = 300 for each word in utterances. Then, the embedded vectors were concatenated into three matrices representing the current, previous, and history utterances, respectively. While the first two channels for a single utterance, u t or u t?1 , had a size of 65 ? 300 according to the maximum number of words n = 65 in the training utterances, the number of rows in the other matrix was 520 which is eight times as large as the others to represent the history utterances from u t?9 to u t?2 where h = 10. In the convolutional layer, 100 feature maps were learned for each of three different filter sizes m = {3, 4, 5} by sliding them over the utterances, which produced 900 feature values in total after the max-pooling operations for all three channels. In addition to these learned features, SPK t and SPK t?1 values introduced in Section 4.2.1 were appended to each feature vector to take the speaker information into account as in the baselines. Before the fully-connected layer, dropout was performed with the rate of 0.25 for regularization. And then, training was done with stochastic gradient descent (SGD) by minimizing categorical cross entropy loss on the training set. All the neural network-based models in this work were implemented using Theano  (Bergstra et al., 2010)  with the parameters obtained from the grid search on the development set. 

 RNN and RCNN-based models Each proposed recurrent network (Section 3.2 and 3.3) was implemented with four variations categorized by whether the backward layer is included in each model or not and also which architecture is used in the recurrent layers between traditional RNNs and long short-term memories (LSTMs)  (Hochreiter and Schmidhuber, 1997) . The RCNN models based on LSTMs are particularly called long-term recurrent convolutional networks (LRCN)  (Donahue et al., 2015) . All the RCNN-based models were initialized with the pretrained word2vec model in the training phase. The dimension of the hidden layers of the recurrent cells was chosen to be |s| = 500 based on the development set. And the other settings including the parameters, the training algorithm, and the loss fuction were the same as in Section 4.2.3.  

 Results Table  2  compares the performances of the models trained on the combination of the training and development sets and evaluated on the test set. The parameters for each model were decided in the development phase which built the models under various different settings only on the training set and validated them with the development set. The evaluations were performed with precision, recall, and F-measure to the manual annotations under three different schedules at tourist turns, guide turns, and all the turns. Then, the statistical significance for every pair was computed using approximate randomization  (Yeh, 2000) . Comparing between two baseline families, the sequential extensions with the CRF models contributed to significant improvements (p < 0.05) from the SVM models in all the schedules. But in both SVM and CRF models, doc2vec features failed to achieve comparable performances to the simplest bag-of-ngrams features. Even the improvements by combining them to the word features were not statistically significant. While these sentence-level embeddings trained in the unsupervised manner exposed the limitations in dialogue topic tracking performances, our proposed CNN-based models outperformed all these baselines. Especially, the CNN initialized with the pre-trained word2vec model achieved higher performances by 8.38%, 6.41%, and 7.21% in F-measure under each schedule, respectively, than the best baseline results. Figure  6  presents the differences between two CNN models observed in the development phase. As the number of epochs increases, the performances of both models also increase up to certain points of saturation. But the model with random initialization required much longer time to be ready to gain scores in earlier iterations and its saturated performance was also lower than the other one learned on top of word2vec. In contrast to the success of the CNN models, the proposed RNN architectures were not able to produce quality results, which was also caused by the limitations of doc2vec representations as already shown in the baseline results. Although some RNN models showed little performance gains over the SVM baselines only with doc2vec features, they were even worse than the CRF model with the same features. On the other hand, the RCNN models connecting the results of CNNs to the RNNs contributed to performance improvements not only from the baselines, but also from the CNN models. While the uni-directional RNN was preferred in the RNN models only with doc2vec, the bi-directional LSTM showed better results in the RCNN architectures. As a result, the bidirectional LRCN model achieved the best performances against all the others, which were statistically significant (p < 0.01) compared to the sec-ond best results with bi-directional RCNN. Table  3  shows the segmentation performances evaluated by considering only the beginning of each segment predicted by the best model of each architecture family. The proposed CNN and LRCN models demonstrated better capabilities of detecting topic transitions in both intra-categorical and inter-categorical conditions than the baselines. While the CNN model tended to have a higher coverage in segmentation than the others, the LRCN model produced more precise decisions to recognize the boundaries on the strength of the consideration of conversational coherences in dialogue history sequences. However, the segmentation performances even with the best models were still very limited especially for inter-categorical transitions. And most of the models in the experiment had better performances in tourist turns than guide turns, as shown in Table  2 . Considering the general characteristics of the target domain conversations that guide-driven and inter-categorical transitions are more likely to be dependent on human background knowledge than tourist-driven and intracategorical cases, respectively, the current limitations are expected to be tackled by leveraging external resources into the models in future. Finally, the generated errors from the models were categorized into the following error types:  40.22 30.19 34.49 8.68 28.14 13.26 18.65 29.51 22.85 CRF (BoN+SPK+D2V) 36.42 25.92 30.28 11.57 24.40 15.70 21.58 25.41    ? Missing predictions: when the reference belongs to one of the labels other than 'O', but the model predicts it as 'O'. Intra-categorical Inter-categorical All Models P R F P R F P R F SVM (BoN+SPK+D2V) ? Extraneous labelling: when the reference belongs to 'O', but the model predicts it as another label. ? Wrong categorizations: when the reference belongs to a category other than 'O', but the model predicts it as another wrong category. ? Wrong boundary detections: when the model outputs the correct category, but with a wrong prediction from 'B' to 'I' or from 'I' to 'B'. The error distributions in Figure  7  indicate that the significantly decreased numbers of wrong categories were the decisive factor in performance improvements by our proposed approaches from the baselines. Besides, the enhanced capabilities of the models in distinguishing between 'O' and other labels were demonstrated by the reduced numbers of missing and extraneous predictions. The sequential architectures in CRF and LRCN models also showed its effectiveness especially in boundary detection, as expected. 

 Conclusions This paper presented various neural network architectures for dialogue topic tracking. Convolutional neural networks were proposed to capture the semantic aspects of utterances given at each moment, while recurrent neural networks were intended to incorporate temporal aspects in dialogue histories into tracking models. Experimental results showed that the proposed approaches helped to improve the topic tracking performance with respect to the linear baseline models. Furthering this work, there would be still much room for improvement in future. Firstly, the architectures based on a single convolutional layer and a single bi-directional recurrent layer in the proposed models can be extended by adding more layers as well as utilizing more advanced components including hierarchical CNNs  (Kalchbrenner et al., 2014b)  to deal with utterance compositionalities or attention mechanisms  (Denil et al., 2012)  to focus on more important segments in dialogue sequences. Secondly, the use of external knowledge could be a key to success in dialogue topic tracking, as proved in the previous studies. However, this work only takes internal dialogue information into account for making decisions. If we develop a good way of leveraging other useful resources into the neural network architectures, better performance can be expected especially for guide-driven and inter-categorical topic transitions that are considered to be more dependent on background knowledge of the speakers. The other direction of our future work is to investigate joint models for tracking dialogue topics and states simultaneously. Although the previous multi-topic state tracking task has assumed that the topics should be given as inputs to state trackers, we expect that a joint approach can contribute to both problems by dealing with the bi-directional relationships between them. Figure 2 : 2 Figure 2: Convolutional neural network architecture for dialogue topic tracking. 

 Figure 4 : 4 Figure 4: Recurrent convolutional network architecture for dialogue topic tracking. The backward layer is only for the bi-directional mode. 

 Figure 5 : 5 Figure 5: Distributions of the segments in TourSG corpus by topic categories and transition types. ATTR, TRSP, FOOD, ACCO and SHOP denotes the topic categories of attraction, transportation, food, accommodation, and shopping, respectively. 

 Figure 6 : 6 Figure 6: Comparisons of the topic tracking performances of the CNN models with different word embedding approaches according to the number of epochs for training in the development phase. 

 Figure 7 : 7 Figure 7: Distributions of errors generated from the best model of each architecture. 

 Table 1 : 1 Statistics of TourSG corpus. The whole dataset is divided into three subsets for training, development, and test purposes. Set # sessions # segments # utterances Train 14 2,104 12,759 Dev 6 700 4,812 Test 15 2,210 13,463 Total 35 5,014 31,034 a topic category shift or not. The most frequent type found in the dataset is guide-initiative and intra-categorical transitions. 63.86% and 61.31% of the total segments are initiated by guides and segmented keeping topic categories, respectively. 

 Table 2 : 2 Comparisons of the topic tracking performances with different models. D2V and W2V denote the vectors from doc2vec and word2vec, respectively. Schedule: Tourist Turns Schedule: Guide Turns Schedule: All Models P R F P R F P R F SVM (BoN+SPK) 61.60 62.18 61.89 58.65 58.42 58.53 59.85 59.94 59.90 SVM (D2V+SPK) 45.05 51.32 47.98 47.78 52.98 50.24 46.66 52.31 49.32 SVM (BoN+SPK+D2V) 61.60 62.18 61.89 58.74 58.53 58.63 59.91 60.01 59.96 CRF (BoN+SPK) 61.18 62.72 61.94 59.27 59.78 59.52 60.05 60.97 60.51 CRF (D2V+SPK) 61.53 49.42 54.81 61.94 49.68 55.13 61.77 49.57 55.00 CRF (BoN+SPK+D2V) 61.22 62.76 61.98 59.30 59.81 59.55 60.08 61.00 60.54 CNN (from scratch) 64.74 63.46 64.10 63.29 62.48 62.88 63.88 62.87 63.37 CNN (with W2V) 69.26 71.49 70.36 65.29 66.65 65.96 66.91 68.61 67.75 Uni-directional RNN 49.46 54.34 51.79 49.54 53.36 51.38 49.51 53.75 51.55 Bi-directional RNN 48.54 49.96 49.24 48.86 49.72 49.29 48.73 49.82 49.27 Uni-directional LSTM 49.52 50.81 50.15 49.41 49.85 49.63 49.45 50.23 49.84 Bi-directional LSTM 48.39 49.05 48.72 48.44 48.58 48.51 48.42 48.77 48.59 Uni-directional RCNN 69.49 71.59 70.52 65.43 66.68 66.05 67.08 68.67 67.86 Bi-directional RCNN 69.81 72.50 71.13 65.49 67.28 66.37 67.25 69.39 68.30 Uni-directional LRCN 69.37 71.45 70.40 66.22 67.41 66.81 67.50 69.04 68.26 Bi-directional LRCN 69.85 72.56 71.18 66.04 67.62 66.82 67.60 69.62 68.59 

 Table 3 : 3 23.34 CNN (with W2V) 41.25 41.50 41.37 17.02 40.87 24.03 28.06 41.29 33.41 Bi-directional LRCN 44.82 38.28 41.29 17.87 40.72 24.84 29.41 39.09 33.57 Comparisons of the segmentation performances with different models. 7500 missing extraneous 6500 7000 wrong category wrong boundary 6000 Number of errors 2500 3000 3500 4000 4500 5000 5500 2000 1500 1000 500 0 SVM CRF CNN LRCN
