title
Conversation Model Fine-Tuning for Classifying Client Utterances in Counseling Dialogues

abstract
The recent surge of text-based online counseling applications enables us to collect and analyze interactions between counselors and clients. A dataset of those interactions can be used to learn to automatically classify the client utterances into categories that help counselors in diagnosing client status and predicting counseling outcome. With proper anonymization, we collect counselor-client dialogues, define meaningful categories of client utterances with professional counselors, and develop a novel neural network model for classifying the client utterances. The central idea of our model, ConvMFiT, is a pre-trained conversation model which consists of a general language model built from an out-of-domain corpus and two role-specific language models built from unlabeled in-domain dialogues. The classification result shows that ConvMFiT outperforms state-of-the-art comparison models. Further, the attention weights in the learned model confirm that the model finds expected linguistic patterns for each category.

Introduction Some mental disorders are known to be treated effectively through psychotherapy. However, people in need of psychotherapy may find it challenging to visit traditional counseling services because of time, money, emotional barriers, and social stigma  (Bearse et al., 2013) . Recently, technologymediated psychotherapy services emerged to alleviate these barriers. Mobile-based psychotherapy programs  (Mantani et al., 2017) , fully automated chatbots  (Ly et al., 2017; Fitzpatrick et al., 2017) , and intervention through smart devices  (Torrado et al., 2017)  are examples. Among them, textbased online counseling services with professional counselors are becoming popular because clients can receive these services without traveling to an office and with reduced financial burden compared to traditional face-to-face counseling sessions  (Hull, 2015) . In text-based counseling, the communication environment changes from face-to-face counseling sessions. The counselor cannot read nonverbal cues from their clients, and the client uses text messages rather than spoken utterances to deliver their thoughts and feelings, resulting in changes of dynamics in the counseling relationship. Previous studies explored computational approaches to analyzing the dynamic patterns of relationship between the counselor and the client by focusing on the language of counselors  (Imel et al., 2015; Althoff et al., 2016) , clustering topics of client issues  (Dinakar et al., 2014) , and looking at therapy outcomes  (Howes et al., 2014; Hull, 2015) . Unlike previous studies, we take a computational approach to analyze client responses from the counselor's perspective. Client responses in counseling are crucial factors for judging the counseling outcome and for understanding the status of the client. So we build a novel categorization scheme of client utterances, and we base our categorization scheme on the cognitive behavioral theory (CBT), a widely used theory in psychotherapy. Also, in developing the categories, we consider whether they are adequate for the unique text-only communication environment, and appropriate for the annotation of the dialogues as training data. Then using the corpus of text-based counseling sessions annotated according to the categorization scheme, we build a novel conversation model to classify the client utterances. This paper presents the following contributions: ? First, we build a novel categorization method as a labeling scheme for client utterances in text-based counseling dialogues. ? Second, we propose a new model, Conversation Model Fine-Tuning (ConvMFiT) to classify the utterances. We explicitly integrate pre-trained language-specific word embeddings, language models and a conversation model to take advantage of the pre-trained knowledge in our model. ? Third, we empirically evaluate our model in comparison with other models including a state-of-the-art neural network text classification model. Also, we show typical phrases of counselors and clients for each category by investigating the attention layers. 

 Categorization of Client Utterances Client responses provide essential clues to understanding the client's internal status which can vary throughout counseling sessions. For example, client's responses describing their problems prevail at the early stage of counseling  (Hill, 1978; E. Hill et al., 1983) , but as counseling progresses, problem descriptions decrease while insights and discussions of plans continue to increase  (Seeman, 1949) . Client responses can also help in predicting counseling outcomes. For example, a higher proportion of insights and plans in client utterances indicates a high positive effect of counseling  (Hill, 1978; E. Hill et al., 1983) . Categorization Objective. Our final aim is to build a machine learning model to classify the client utterances. Thus, the categorization of the utterances should satisfy the following criteria: ? Suitable for the text-only environment: Categories should be detected only using the text response of clients. ? Available as a labeling scheme: The number of categories should be small enough for manual annotation by counseling experts. ? Meaningful to counselors: Categories should be meaningful for outcome prediction or counseling progress tracking. Previous studies in psychology proposed nine and fourteen categories for client and counselor verbal responses, respectively, by analyzing transcriptions from traditional face-to-face counseling sessions  (Hill, 1978; Hill et al., 1981) . But these categories were developed for face-to-face spoken interactions, and we found that for online text-only counseling dialogues, these categories are not directly applicable. Using text without non-verbal cues, a client's responses are inherently different from the transcriptions of verbally spoken responses which include categories such as 'silence' (no response for more than 5 seconds) and 'nonverbal referent' (physically pointing at a person). Another relevant study, derived from text-based counseling sessions with suicidal adolescents proposes 19 categories which we judged to be too many to be practical for manual annotation  (Kim, 2010) . The last criterion of "meaningful to counselors" is perhaps the most important. To meet that criterion, we base the categorization process on the cognitive behavioral theory (CBT) which is the underlying theory behind psychotherapy counseling. The details of using CBT for the categorization process is explained next. Categorization Process and Results. In developing the categories, we follow the Consensual Qualitative Research method  (Hill et al., 1997) . Two professional counselors with clinical experience participated in this qualitative research method to define the categorization. To begin, we randomly sample ten client cases considering demographic information including age, gender, education, job, and previous counseling experiences. We then start the categorization process with the fundamental components of the CBT which are events, thoughts, emotions, and behavior  (Hill et al., 1981) . The professional counselors annotate every client utterance to with those initial component categories with tags that add detail. For example, if an utterance is annotated as 'emotion', we add 'positive/negative' or concrete label such as 'hope'. If these tags are categorized to be a new category, we add that category to the list until it is saturated. When the number of categories becomes more than 40, we define higher level categories that cover the existing categories. In the second stage, annotators discuss and merge these categories into five high-level categories. Category 1 is informative responses to counselors, and category 2 is providing factual information and experiences. Categories 3 and 4 are related to the client factors, expressing appealing problems and psychological changes. The last category is about the logistics of the counseling sessions including scheduling the next session. The categories in detail are as follows: ? Factual information (Fact.) Informative responses to counselor's utterances, including age, gender, occupation, education, family, previous counseling experience, etc. ? Anecdotal Experience (Anec.) Responses describing past incidents and current situations related to the formation of appealing problems. Responses include traumatic experiences, interactions with other people, comments from other people, and other anecdotal experiences. ? Appealing Problems (Prob.) Utterances addressing the main appealing problem which is yet to be resolved, including client's internal factors or their behaviors related to the problems. Specifically, the utterances include cognition, emotion, physiological reaction, and diagnostic features of the problem and desire to be changed. ? Psychological Change (Chan.) Utterances describing insights, cognition of small and big changes in internal factors or behaviors. That is, an utterance at the point where the appealing problem is being resolved. ? Counseling Processes (Proc.) Utterances that include the objective of counseling, requests to the counselor, plans about the counseling sessions, and counseling relationship. This category also covers greetings and making an appointment for the next session. We summarize the category explanations and examples in Table  1 . 

 Dataset In this section, we explain how counseling dialogues differ from general dialogues, describe the dialogues we collected and annotated, and explain how we preprocessed the data. 

 Characteristics of Counseling Dialogues The counseling dialogues consist of multiple turns taken by a counselor and a client, and each turn can contain multiple utterances. Here we describe two unique characteristics of text-based online counseling conversations compared to general non-goal oriented conversations. Distinctive roles of speakers. Counseling conversations are goal-oriented with the aim to produce positive counseling outcomes, and the two speakers have distinctive roles. The client gives objective information about themselves and subjective experiences and feelings to the counselor to appeal the problems they are suffering from. Then the counselor establishes a therapeutic relationship with the client and elicits various strategies to induce psychological changes in the client. These distinct roles of the conversational participants distinguish counseling conversations from Multiple utterances in a turn. We define an utterance as a single text bubble. Counselors and clients can generate multiple utterances in a turn. Especially in a client turn, various information not to be missed by a counselor may occur across multiple utterances. Thus, we treat every utterance separately, as shown in Fig.  1  3.2 Collected Dataset Total dialogues. We collect counseling dialogues of clients with their corresponding professional counselors from Korean text-based online counseling platform Trost. 1 Overall, we use 1,448 counseling dialogues which are anonymized before researchers obtain access to the data, by removing personally identifiable information in the dialogues. All meta-data of the dialogues are not provided to the researchers, and named entities such as client's and counselor's names are replaced with random numeric identifiers. The research process including data anonymization and pre-processing is validated by the KAIST Institutional Review Board (IRB). Labeled Dialogues. We randomly choose and label 100 dialogues with the discovered five categories. Note that we only label client utterances. 1 https://www.trost.co.kr/ Based on these categories, five professional counselors annotated their own client's every utterance in the conversations, as shown in Fig.  1 . Note that each utterance can have multiple labels if it includes information across multiple categories. Table  .  2 shows the descriptive statistics of our labeled dataset. The first two rows present the average lengths of each utterance of counselors and clients in terms of words and characters, showing there is a small difference between the counselor utterance and the client utterance. On the other hand, the average number of utterances in a single counseling session differs; on average, clients write more utterances than counselors.  

 Preprocessing We intentionally leave in punctuations and emojis since they can help to infer the categories of the client utterances, treating them as separate tokens. Then we construct triples from the labeled dialogues consisting of 1) counselor's utterances (blue in Fig.  1 ), 2) client's context utterances (green), and 3) client's target utterances to be categorized. (yellow) We split the dataset into train, validation, and test sets. Table  .  3 shows the number of triples in each set, showing factual information (Fact.) and psychological change (Chan.) categories appear less frequently than the others. 

 Model We introduce ConvMFiT (Conversation Model Fine-Tuning), fine-tuning pre-trained seq2seq based conversation model to classify the client's utterances. Background. Our corpus of 100 labeled conversations is not large enough to fully capture the linguistic patterns of the categories without external knowledge. The small size of the dataset is difficult to overcome because the labeling by professional counselors is costly. We found a poten-  tially effective solution for a small labeled dataset by using pre-trained language models for various NLP tasks  (Ramachandran et al., 2017; Howard and Ruder, 2018) . Therefore, we focus on transferring knowledge from unlabeled in-domain dialogues as well as a general out-of-domain corpus. Overview. We illustrate the overall model architecture. We first stack pre-trained seq2seq layers which represent a conversation model (see Fig.  2 , lower colored part). Then we stack additional seq2seq layers and classification layers over it to capture task-specific features, and an attention layer is added between the two to enhance the model's interpretability (see Fig.  2 , upper white part). This approach leverages the advantages of us-ing a language model based conversation model for transfer learning. The model can be regarded as an extended version of ULMFiT  (Howard and Ruder, 2018)  with modifications to fit our task. In ConVMFiT, the model accepts a pre-trained seq2seq conversation model that requires two pretrained language models, like an encoder and a decoder, learning the dependencies between them on the seq2seq layers. This approach is shown to be effective for machine translation, applying a source language model for the encoder and target language model for the decoder (Ramachandran et al., 2017). 

 Model Components Word Vectors. Counseling dialogues consist of natural Korean text which is morphologically rich, so we train word vectors specifically developed for the Korean language  (Park et al., 2018a) . We train the vectors over a Korean corpus including out-ofdomain general documents, 1) Korean Wikipedia, 2) online news articles, and 3) Sejong Corpus, as well as in-domain unlabeled counseling dialogues. The corpus contains 0.13 billion tokens. To validate the trained vector quality, we check performance on word similarity task (WS353) for Korean  (Park et al., 2018a) . The Spearmans correlation is 0.682, which is comparable to the stateof-the-art performance. These vectors are used as inputs (see Fig.  2 , yellow). Pre-trained Language Models. We assume that counselors and clients have different language models because they have distinctive roles in the dialogue. Therefore, we train a counselor language model and a client language model separately. We collect counselor utterances in the total dialogue dataset, except dialogues in the test set, to train a counselor language model, and all others are used for training a client language model. Then, We fine-tune the two trained LMs with utterances in the labeled dialogues. For each model, we train word-level language models by using multilayer LSTMs. We apply various regularization techniques, weight tying  (Inan et al., 2017) , embedding dropout and variational dropout  (Gal and Ghahramani, 2016) , which are used to regularize LSTM language models  (Inan et al., 2017; Ramachandran et al., 2017; Merity et al., 2018) . We use a 3-layer LSTM model having 300 hidden units for every layer. We set embedding dropout and output dropout for each layer to 0.2, 0.1, respectively. The pre-trained language models generate inputs to seq2seq layer of a conversation model (Fig.  2 , blue and red). Pre-trained Conversation Model. Next, we train a seq2seq conversation model  (Vinyals and Le, 2015) . We use the pre-trained counselor language model as an encoder, and the client language model as a decoder. The dependency of the decoder on the encoder is trained by seq2seq layers, stacked over the pre-trained models. (Fig.  2 , green) We stack 2-layer LSTMs over the pre-trained counselor and client language models, respectively. The final states of the LSTMs on the counselor language model is used as an initial state of the LSTMs on the client language model. We set the hidden unit size of 300 for every LSTMs and set output dropout to 0.05. The outputs of pretrained conversation models is used for inputs to seq2seq layers of the task-specific layers. During training the conversation model, we regularize the parameters of the model by adding cross entropy losses of the pre-trained counselor and client language models to seq2seq cross entropy loss of the conversation model. Three losses are weighted equally. This prevents catastrophic forgetting of the pre-trained language models and is important to achieve high performance  (Ramachandran et al., 2017) . Also, there is room for improvement of the architecture of a conversation model, which integrates pre-trained language models, to capture dialogue patterns better and thus leads to higher classification performance. We will explore the architecture in future work. Task-specific layers. By leveraging pre-trained language model based conversation model, we finally add layers for classification. In order to capture task-specific features, we first stack seq2seq layers over the conversation model. Then we add attention mechanism for document classification  (Yang et al., 2016) . Lastly, we use a sigmoid function as an output layer to predict whether the information is included in utterances because multiple categories can appear in a single utterance. (Fig. 

 2, gray) We use a 2-layer LSTM model for the seq2seq layers. We set 300 hidden units for every LSTMs and set output dropout to 0.05. The size of attention layer is set to 500. 

 Model Training Thus the model is trained by three steps: 1) training word vectors and two language models, 2) training seq2seq conversation model with pretrained LMs, and 3) fine-tuning task-specific classification layers, after removing softmax of the conversation model. In the last step, we compute binary logistic loss between predicted probability for each category and label as a loss function. We use Adam with default parameters for the optimizer, in order to train language models, conversation model, and fine-tuning the classifier. Also, gradual unfreezing is applied while training the model, starting updating parameters from the task-specific layers and unfreezing the next lower frozen layer for each epoch. We unfreeze layers every other epoch until all layers are tuned, and we stop training when the validation loss is minimized. All hyperparameters are tuned over the development set. 

 Experiments 

 Comparison Models We compare our model with baseline models in Table  4 . Models 1-5 are classifiers which only use the target client utterance to classify it, and models 6-8 are conversation model-based classifiers considering counselor's utterances and client's context utterances. All models use the same pretrained word vectors for a fair comparison. (1) Random Forest, (2) SVM with RBF kernel. We represent an utterance by an average of all word vectors in it, then feed it to the classifier input. (3) CNN for text classification  (Kim, 2014) . In the convolution layer, we set the filter size to 1-10, and 30 filters each, then we apply max-over-time pooling. Then, a dense layer and sigmoid activation is applied over the layer. (4) RNN Bidirectional LSTM is used where the final states for each direction are concatenated to represent the utterances. Then, a dense layer and a sigmoid activation are stacked. (5) ULMFiT. A pre-trained client language model is used as a universal language model. Details are the same as described in Section 5.3. In addition, 2-LSTM layers and dense layer with sigmoid activations are stacked over the LM for classification. Gradual unfreezing is applied during training  (Howard and Ruder, 2018) . (6) Seq2Seq. Like encoder-decoder based conversation model  (Vinyals and Le, 2015) , three LSTMs are assigned for each Counselor's utterances, client's context/target utterances. The initial states of the client language models are set to the final states of preceding utterances. Then dense & sigmoid layers for classification are stacked over the final state of the client's target utterances. (7) HRED. Hierarchical encoder-decoder model (HRED) is used as a conversation model  (Serban et al., 2016) . For the encoder RNN, counselor's utterances and client's context utterances are given as inputs, and their information is stored in context RNN, which delivers it to the decoder accepting client's target utterances. Like (6) Seq2Seq, dense & sigmoid layers for classification are stacked over the final state of the client's target utterances. 

 Ablation Study We conduct an ablation study to investigate the effect of the pre-trained models in Table  5 . (1-4) Adding pre-trained models. Model 1-4 have the same architecture as ConvMFiT, which is Model (8) in Table  .  4. Model (1) in Table  .  5 initializes every parameter randomly. Model (2) starts training only with pre-trained word vectors. Model (3) leverages counselor and client language models as well, and Model (4) shows the performance of ConvMFiT. As Model (3) and (4) use more than two pre-trained components, gradual unfreezing in applied, unfreezing shallower layers first during training. (4-1) Task-specific Seq2Seq Layers. Model (4-1) removes task-specific Seq2Seq layers in the model, which leaves only attention and dense layer to capture task-specific features. It may result in an insufficient model capacity to capture relevant features for the task. (4-2) Effect of Gradual Unfreezing. Model (4-2) is trained without gradual unfreezing, allowing the parameters of every layer in the model change by the gradients from the first epoch. 

 Results 

 Classification Performance We show the performance of our model and comparison models in Table  .  4. (1) Random Forests and (2) Support Vector Machines underperform to classify utterances correctly which belong to rarely occurred classes. Compared to (1) and (  2 ), (3) CNN and (4) RNN show better performance since they look at the sequence of words  (.416, .431, respectively) . RNN shows slightly better performance than CNN. (  6 ) ULMFiT outperforms the others by using a pre-trained client language models (.455). The client target utterances have their context, and they also depend on the counselor's preceding utterances, so using the preceding counselor utterance as well as the client context utterances helps to improve the classification performance. When integrating the information using simple (6) seq2seq model, it shows better performance (.530) than (5) ULMFiT. (7) HRED adds a higher-level RNN to seq2seq models, but we find there is little performance gain (.001) which makes the model overfit easily. (8) ConvMFiT employs pre-trained conversation model based on pre-trained LMs and so outperforms all other baseline models (.642). This is because ConvMFiT integrates conversational contexts and the counselor language model, which helps to capture the patterns of client's language better. Also, the improvement is higher for the class (Fact.) and (Chan.) which have small numbers of examples since the ConvMFiT could leverage pre-trained knowledge to classify them. 

 Effect of Pre-trained Components With an ablation study applying pre-trained components step by step to our model, we show the Table  5 : Ablation study results. All models use the same architecture of ConvMFiT. Adding pre-trained word vectors (2), counselor and client language models (3), seq2seq layers of conversation model (  4 ) results in a performance improvement. Also, adding task-specific seq2seq layers to ensure the model's sufficient capacity for capturing relevant features (4-1), and applying gradual unfreezing lead to performance improvement (4-2). sources of performance improvement in Table  5 . Model (1) uses the same architecture but no pretrained models are applied, showing poor performance due to overfitting (.455). The f1 score is lower than that of the simple seq2seq model. Model (2) only uses pre-trained word vectors, and it helps to increase the performance slightly (.494). Also, Model (3) adds two pre-trained LMs with gradual unfreezing, resulting in a substantial performance increase (.626.), so we find pre-trained LMs are essential to the improvement. Lastly, Model (4) adds the dependency between two language models to fully leverage the pre-trained conversation model. This also helps classify utterances better (.642). In addition, we find only adding attention and dense layers are not sufficient to learn task-specific features, so providing the model more capacity helps improving the performance. Without taskspecific seq2seq layers, model (4-1) shows decreased f1 score (.563). Meanwhile, model (4-2) shows careful training scheme affects to the per-formance as well. 

 Qualitative Results To investigate how linguistic patterns of counselors and clients differ in the various categories, we report qualitative results based on the activation values of the attention layer. To protect the anonymity of the clients, we explore key phrases from utterances rather than publish parts of the conversation in any form. To this end, we compute the relative importance of n-grams. For any n of n-gram in an utterance of length N where n < N , the relative importance r is computed as follows: ( n i=1 a i ? ( 1/N ) n ) / ( 1/N ) n (1) where a i is corresponding attention value of a word, n i=1 a i is the product of the values for every words in the n-gram, normalized by the expected attention weights (1/N ) n . The normalization term considers the length of utterance since a word in short utterances tends to have high atten-tion value because of N i=1 a i = 1. We name this measure 'relative importance' r meaning that the degree of the n-gram is how much it is attended to compared to the expectation. Based on this, we select examples from 100 top-ranked key phrases for each category and the results are presented in  Appendix (Table. 6) . All of the presented key phrases are translated to English from Korean. Factual Information. Clients provide demographic information and previous experience of visits to counselors or psychiatrists. In some case, clients talk more about the motivation of counseling. Since this information is explored in an early stage of counseling sessions, we also find counselor greetings with client's names. Anecdotal Experience. Clients describe their experiences by using past tense verbs. Usually, utterances include phrases such as 'I thought that', 'I was totally wrong'. Counselors show simple responses 'well..', and reflections. Appealing Problem. Like anecdotal experience, clients describe their problems, but with using the present tense of verbs. They are appealing their thinking and emotions. Counselors also show simple responses or reflections. Since some clients immediately start pouring out their problems right after counseling sessions starts, so greetings from counselor appear in key phrases. Psychological Change. Clients obviously report their change of feelings, emotions or thoughts. It includes looking back on the past and then determining to change in the future. Counselors give supportive responses and empathetic understanding. Counseling Process. Clients and counselors exchange greetings with each other. Also, they discuss making an appointment for the next session. In some cases, counselors respond to client's questions about the logistics of the sessions. 

 Related Work Researchers have explored psycho-linguistic patterns of people with mental health problems  (Gkotsis et al., 2016) , depression  (Resnik et al., 2015) , Asperger's and autism  (Ji et al., 2014 ) and Alzheimer's disease  (Orimaye et al., 2014) . In addition, these linguistic patterns can be quantified, for example, overall mental health  (Loveys et al., 2017; Coppersmith et al., 2014) , and schizophrenia  (Mitchell et al., 2015) . To aid people with those mental issues, large portion of studies are dedicated to detecting those issues from natural language. Depression  (Morales et al., 2017; Jamil et al., 2017; Fraser et al., 2016) , anxiety  (Shen and Rudzicz, 2017) , distress  (Desmet et al., 2016) , and self-harm risk  (Yates et al., 2017)  can be effectively detected from narratives or social media postings. 

 Discussion and Conclusion In this paper, we developed five categories of client utterances and built a labeled corpus of counseling dialogue. Then we developed the Con-vMFiT for classifying the client utterances into the five categories, leveraging a pre-trained conversation model. Our model outperformed comparison models, and this is because of transferring knowledge from the pre-trained models. We also explored and showed typical linguistic patterns of counselors and clients for each category. Our ConvMFiT model will be useful in other classification tasks based on dialogues. ConvM-FiT is a seq2seq model for counselor-client conversation, however, another approach would be to model with existing non-goal oriented conversation models incorporating Variational Autoencoder (VAE)  (Serban et al., 2017; Park et al., 2018b; Du et al., 2018) . We plan to attempt these models in future work. We expect to apply our trained model to various text-based psychotherapy applications, such as extracting and summarizing counseling dialogues or using the information to build a model addressing the privacy issue of training data. We hope our categorization scheme and our ConvMFiT model become a stepping stone for future computational psychotherapy research. 

 Ethical Approval The study reported in this paper was approved by the KAIST Institutional Review Board (#IRB-17-95). Figure 1 : 1 Figure 1: Translated example conversation between a counselor and a client. Each turn can have multiple utterances, and we annotate the client turn at the utterance-level. The first two boxes indicate 1) Counselor's utterance and the next two are 2) Client's context utterance, and the last box is 3) Client's target utterance.The annotation for the last client utterance is "appealing problem". 

 Table 2 : 2 Descriptive statistics of labeled counseling dialogues. Counselor Client Mean Std. Mean Std. # of words 6.26 6.25 5.91 14.62 # of chars 25.71 23.44 24.31 61.10 # of utters 163.2 236.97 238.72 578.49 

 ConvMFiT (Conversation Model Fine-Tuning) model architecture. We use a pre-trained conversation model using seq2seq models. The model is based on pre-trained counselor's and client's language models. (lower colored part) Then, we stack additional task-specific seq2seq layers and classification layers over the conversation model to learn specific features. Between the two layers, attention layer is added to enhance its interpretability. Task-specific Classification Layers (Sigmoid) Attention & Dense Seq2Seq Layers Pre-trained Seq2Seq Conversation Layers Model Counselor's Client's LM LM Counselor's U Client's Context U Client's Target U Figure 2: Train Valid Test # of labels Fact. 817 140 172 1129 Anec. 5317 1180 1175 7726 Prob. 4728 1004 981 6713 Chan. 887 211 199 1297 Pros. 4570 964 961 6459 # of triples 14679 3166 3165 21100 

 Table 3 : 3 The number of triples (partner's utterance, client's context utterance, client's target utterance) and corresponding labels for each set. Factual information (Fact.) and psychological change (Chan.) categories have less number of instances compared to the others. 

 Table 4 : 4 Classification results. Among the models 1-6 which only use client's utterances to predict categories, (6) ULMFiT show better performance to the others. Models 7-9 are classifiers based on the conversation model, and ConvMFiT outperforms the others (.642). No. Dep. Model F1 (Fact.) F1 (Anec.) F1 (Prob.) F1 (Chan.) F1 (Proc.) Macro Prec Macro Rec Macro F1 1 X RF .000 .564 .420 .000 .723 .476 .269 .341 2 X SVM(rbf) .012 .683 .457 .000 .766 .602 .385 .384 3 X CNN .211 .528 .506 .128 .706 .450 .397 .416 4 X RNN .193 .574 .570 .046 .770 .607 .375 .431 5 X ULMFiT .205 .641 .591 .057 .784 .613 .413 .455 6 O Seq2Seq .263 .662 .678 .226 .823 .695 .472 .530 7 O HRED .261 .706 .675 .193 .820 .680 .475 .531 8 O ConvMFiT .441 .761 .726 .447 .835 .716 .602 .642 No. Emb. LM Conv. seq2seq Grad. Unf. Task. seq2seq F1 (Fact.) F1 (Anec.) F1 (Prob.) F1 (Chan.) F1 (Proc.) Macro Prec Macro Rec Macro F1 1 X X X - O .032 .706 .602 .222 .711 .418 .575 .455 2 O X X - O .043 .758 .661 .258 .748 .459 .662 .494 3 O O X O O .425 .782 .727 .365 .831 .587 .719 .626 4 O O O O O .441 .761 .726 .447 .835 .602 .716 .642 4-1 O O O O X .307 .738 .666 .304 .802 .513 .687 .563 4-2 O O O X O .417 .768 .721 .399 .824 .591 .695 .626 4 O O O O O .441 .761 .726 .447 .835 .602 .716 .642
