title
Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems

abstract
The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce Spot The Bot, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chatbots regarding their ability to mimic the conversational behavior of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chatbot can uphold human-like behavior the longest, i.e., Survival Analysis. This metric has the ability to correlate a bot's performance to certain of its characteristics (e.g., fluency or sensibleness), yielding interpretable results. The comparably low cost of our framework allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying Spot The Bot to three domains, evaluating several stateof-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool.

Introduction Evaluation is a long-standing issue in developing conversational dialogue systems (i.e., chatbots). The underlying difficulty in evaluation lies in the problem's open-ended nature, as chatbots do not solve a clearly-defined task whose success can be measured in relation to an a priori defined ground truth. Automatic metrics have so far failed to show high correlation with human evaluations  (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020) . Human evaluation approaches are mainly classified according to the following: single-turn vs. multi-turn evaluation, and direct user evaluation vs. expert judge evaluation. Single-turn analysis is usually performed by a human judge that rates a single response of the bot to a given context, whereas multi-turn analysis is often performed by a user that interacts with the bot and rates the interaction. Single-turn ratings disregard the multiturn nature of a dialogue  (See et al., 2019) . Although more and more multi-turn evaluations are performed, most of them are based on human-bot conversations, which are costly to obtain and tend to suffer from low quality  (Dinan et al., 2020a) . The instructions to be followed by annotators are often chosen ad-hoc and there are no unified definitions. Compounded with the use of often criticized Likert scales  (Amidei et al., 2019a) , these evaluations often yield a low agreement. The required cost and time efforts also inhibit the widespread use of such evaluations, which raises questions on the replicability, robustness, and thus significance of the results. In this work, we present the Spot The Bot framework, a cost-efficient evaluation methodology that can be used to rank several bots with regard to their ability to disguise as humans. It works as a multiturn-based evaluation with human judges. Spot The Bot is based on two observations: First, chatbots are trained on conversations between humans, and thus, they should be evaluated regarding their ability to mimic human behavior. Second, the longer a conversation is, the more likely it is that a bot exhibits non-human-like behavior. Spot The Bot works by generating conversations between bots, then mixing these bot-bot conversations with human-human conversations and letting human judges decide for each entity in the conversations if it is a human or a bot. The conversations are rated at different points in time, which introduces the time-dependent component. This setting allows for two different analyses: a ranking based on pairwise comparisons of bots, and the application of the Survival Analysis, which computes the survival rate for each bot at different conversation lengths. Furthermore, the human judges annotate the entities with respect to more fine-grained features, which can be chosen based on characteristics that the bots are expected to exhibit (e.g. fluency or informativeness). The Survival Analysis further allows to pin down the features that contribute to a dialogue system's survival, enabling interpretable results. We show that our framework produces reliable, repeatable results, while being quicker and more cost-effective to run than related approaches, as it does not rely on human-bot conversations and generally requires fewer annotations. Furthermore, we show that disagreement between human annotators can be interpreted as a feature of a system's performance, rather than a weakness in the evaluation approach. We apply the framework to three well-known domains and common baselines and state-of-the-art systems to produce a stable ranking among them. We release the framework as a ready-to-use tool for evaluating dialogue systems into which different systems can be plugged and compared 1 . 

 Related Work There exist various methods to evaluate dialogue systems, both automatic and human-based, but no single evaluation metric is widely agreed upon in the scientific community  (Deriu et al., 2020) . Automatic evaluation metrics for chatbots are known to correlate poorly with human ratings  (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020) , so we focus on human-based approaches, which can be classified in two dimensions: 1) single-turn vs. multi-turn approaches, and 2) approaches where the dialogue systems are judged by the user directly (interactive) or where judgments are made by objective experts, who do not participate in the dialogue (static). Single-turn Static Evaluations. Evaluations 1 https://github.com/jderiu/ spot-the-bot-code based on a static context and a single response from the dialogue systems are widely adopted. Usually, the rating is performed by expert raters that read the response of one or more dialogue systems to a static context and rate the responses  (Galley et al., 2018) . Alternatively, the responses of two bots can be compared directly to choose a preferred answer  (Li et al., 2016) . While being relatively time and cost-efficient, single-turn evaluation fails to capture the conversation's quality as a whole. A system that tends to produce repeated answers can obtain a high single-turn score, albeit a low multi-turn one  (See et al., 2019) . Some authors also report poor inter-annotator agreement  (Ghandeharioun et al., 2019) . Human-Bot Conversations. In order to perform interactive multi-turn evaluations, the standard method is to let humans converse with a chatbot and rate it afterward  (Ghandeharioun et al., 2019) , typically using Likert scales (van der  Lee et al., 2019) . The ConvAI2 challenge  (Dinan et al., 2020b)  and the Alexa Prize  (Venkatesh et al., 2018)  applied this procedure. Apart from the high cost of collecting human-bot conversations, this approach puts a high cognitive strain on humans, as they have to perform several tasks at once  (Schmitt and Ultes, 2015) . Besides, it is not always possible to get sensible conversations with bots, making it hard to get high-quality conversations. In fact, in the ConvAI2 challenge, half of the collected human-bot conversations were discarded due to their low quality  (Dinan et al., 2020b) . Finally, Likert scales are known to suffer from high annotation variance  (Ghandeharioun et al., 2019) , require normalization a posteriori, are prone to order effects and are less reliable than ranking-based ratings  (Amidei et al., 2019b) . Self-talk. Recently, using self-talk dialogues, i.e., dialogues where a bot talks to itself, gained traction as a cost-effective basis for evaluation. This idea is closely related to user simulations used to evaluate task-oriented systems  (Schatzmann et al., 2006) .  Ghandeharioun et al. (2019)  and  Deriu and Cieliebak (2019)  use self-talk to produce automatic evaluations. In ACUTE-EVAL , the authors propose to let humans evaluate self-talk dialogues. Since self-talk does not allow direct comparisons between bots, the authors let humans read two self-talk conversations  side-by-side and rate them with respect to various features. This increases the cognitive complexity of the annotation task. Furthermore, the resulting ranking of the bots is per criterion, whereas our method produces one ranking and can optionally incorporate annotations of features that yield interpretability of the results. Turing Test. Spot The Bot is reminiscent of the Turing Test  (Turing, 1950) , as the dialogue systems are evaluated based on their ability to mimic human behavior. The Turing test served as a useful mental model for understanding what machine intelligence might mean. However, it has also been criticized as a way to identify intelligence in NLP systems.  Bender and Koller (2020)  argues that a system may fool a user into believing it is human, and yet this does not prove that the system understands the meaning of the conversation they are having. In our approach, we claim that failing the test is a valid indicator to discriminate among bots. In fact, we presume that eventually all bots will fail the test, and we collect a time component to record the time it takes for a bot to be detected. 

 Spot The Bot In this section, we first provide an overview of the Spot The Bot framework and then describe the evaluation process's individual steps. 

 Overview Spot The Bot employs a tournament among chatbots to determine which performs the best at mimicking humans' conversational behavior. To measure the success of each bot, human crowd-workers are shown conversations between two competing bots at a time, mixed with conversations between two humans. The crowdworkers' task is to determine for each entity in a conversation whether it is a human or a bot (or whether the crowdworker is unsure). The bot that is most frequently annotated as being human wins the tournament. Figure  1  provides an overview of the process for one conversation. There are different use cases for Spot The Bot, e.g., when a novel dialogue strategy is to be compared against existing ones or if a set of chatbots is to be ranked in the context of a shared task. On top of returning a ranking, Spot The Bot employs the Survival Analysis, which introduces a time aspect into the evaluation and provides insights into how different features correlate to the bots' ability to pass as a human. Formally, assume a pool of b bots {B 1 , ..., B b }, which is to be ranked. For each pair of bots, a set of conversations is sampled by letting the bots talk to each other, where S ij denotes the set of conversations between bots B i and B j . Each conversation is defined as a sequence of exchanges e 0 , ..., e N , where each exchange consists of two turns: e i = {t e i 0 , t e i 1 }, one for each entity. Segmentation. The more exchanges there are in a conversation, the more likely it is that a bot gets recognized as such. Thus, we show different segments of the conversation to the crowdworkers. A segment is defined as the first k exchanges of the dialogue: S k ij = e 0 , ..., e k . Thus, an annotator only sees the first k exchanges of the conversation. 2 Each segment of the same conversation is rated by a different annotator to avoid that one annotator sees parts of the same conversation multiple times, which would bias the rating. We choose different segment lengths since we cannot know a priori which length is sufficient for the different bots to be recognized as such. Human Conversations. We add conversations among humans to the pool of conversations that are to be rated. The human conversations are sampled from the training set used to train the dialogue systems in the respective domain. The results of the annotations of the human dialogues establish an upper bound for the evaluation. Also, they are meant to prevent annotators from concluding that all entities are bots.  3  Annotation. The annotation procedure works in two steps: First, the annotators have to decide for each entity in a conversation segment if it is a bot or a human. Second, to correlate the outcome to various characteristics of a bot, the framework allows rating specific features (e.g., fluency or appropriateness). The framework then measures the influence of these features on the survival time of the bots, which serves as an explainability component (cf. Sections 3.3 and 4.2). Features. We chose three features: sensibleness, specificity  (Adiwardana et al., 2020) , and fluency. The first two are shown to capture the core conversational behavior of answering sensibly and not with illogical statements while being specific to the conversation's given context. The third feature states if the utterances are grammatically correct and fluent. The features are rated by preference ranking, that is, the annotator states which of the two entities performed better with respect to the features. 

 Ranking We define a win function for the annotations of the pairwise, direct conversations between two bots. The outputs of the win function are aggregated to  2  We experimented with letting crowdworkers decide where they were sure that an entity is a bot or a human. However, this approach required too much fine-tuning to constrain erratic annotator behavior, cf. Appendix B.  3  We investigated if annotators realize that conversations are either between bots or humans by looking at ratios of conversations where both entities are labeled identically, but found no evidence that this happens more often than by chance. determine the overall winner of the tournament. Win Function. Each annotation at each segment length S k ij = e 0 , ..., e k of a conversation constitutes the result of one annotation applied by one crowdworker, individually labeling each of the two entities as either bot, human, or unsure. The winner of segment S k ij under a crowdworker's annotation is determined by the following ordering of the labels: human > unsure > bot. That is, if bot B i is assigned the label human and bot B j has label bot or unsure, B i has won the segment.  4  Similar to  Bojar et al. (2013) , we define a win rate of B i against B j to aggregate the wins from all segments of all annotations stemming from conversations between bots B i and B j , as: WINS(B i , B j ) WINS(B i , B j ) + WINS(B j , B i ) (1) where WINS(B i , B j ) denotes the number of times that B i wins against B j . Ranking. To create the ranking, we follow the approach by  Du?ek et al. (2018) , where the ranking is generated by the TrueSkill  (Herbrich et al., 2006)  algorithm based on the win rate, and significant differences in performance are determined by bootstrap sampling. The result is a ranked set of clusters, where each cluster is composed of entities that do not have a significant difference in performance. 

 Survival Analysis While pair-wise win rates are well-suited to provide a relative ranking among a pool of bots, it does not serve as an absolute evaluation of a single bot's ability to disguise as a human. Also, the conversations' segmentation introduces a time component, which we leverage to investigate our intuition that bots are more likely to reveal themselves in longer conversations. In our evaluation, a bot that is able to disguise in long conversations can be said to be most successful. Thus, we complement our evaluation with Survival Analysis. Survival Analysis estimates probabilities for the occurrence of an event at different points in time. It has a long history in the medical domain, where it is used to estimate the effectiveness of different treatments  (Li and Ma, 2013) . In engineering disciplines, it is applied to estimate the time to failure of machine components  (Eyal et al., 2014) . In our case, we are interested in the time, corresponding to the number of exchanges, until a dialogue system is spotted as such. In addition, Survival Analysis allows us to correlate finer-grained characteristics to the survival probability, which allows us to inspect which of the annotated features impact a bot's survival. We interpret the annotation data as such: the spotted event occurred if the system was annotated as "bot" and it survived if it was annotated as "unsure" or "human". Let k be the number of exchanges in the annotated conversation segment, meaning that each dialog system produced k outputs. If the dialog system was not spotted, we know it survived for at least k exchanges. This is a so-called rightcensored data point. If the dialogue system was spotted as such, we cannot tell the exact number of exchanges it took for an annotator to spot it, meaning it could have taken less than k exchanges. We thus record that the spotting event happened in the interval (0, k], a so-called interval-censored event. From this data, we can get non-parametric estimates of the survival function of the different systems per domain  (Turnbull, 1974) . To check whether these differences are significant, we apply a generalized log-rank test  (Zhao and Sun, 2004) . We use the Cox Proportional Hazards Model  (Cox, 1972)  to study the influence of the features outlined in Section 3.1 on the time before the systems are spotted. 5 

 Experiments Domains. We apply Spot The Bot to three widely used domains for conversational dialogue systems: Dailydialog  (Li et al., 2017) , Empathetic Dialogues (Rashkin et al., 2019), and PersonaChat  (Zhang et al., 2018) . For each domain 6 , we prepared a pool of bots to be ranked and analyzed. For each pair of bots, we sampled |S ij | = 45 conversations. For this, we seed the conversations by using the first exchange of a conversation in the test set, which is sampled at random. Although there exists a probability that the bots resample parts of a conversation, we did not find evidence of this happening. In fact, only 2% of all sampled conversations contain an exchange, which can be found in the training material. For the annotation task, we recruited paid crowdworkers from Amazon Mechanical Turk (AMT). To avoid that, the results are biased towards the performance of a few crowdworkers, we designed a Human Intelligence Task as a batch of 20 conversations, and each worker was only allowed to work on three batches. We designed the batches so that two segments of the same conversations never appear in the same batch, and each batch contains different segments of different conversations. Segmentation. The segment lengths are based on the lengths of the dialogues in a domain. Since we add human conversations of the training set to be rated, the sampled dialogues should adhere to their lengths. PersonaChat and Dailydialog have longer conversations; thus, we used segments of 2, 3, and 5 exchanges. The Empathetic Dialogue domain has shorter dialogues; thus, we used segment lengths of 1, 2, and 3 exchanges. Dialogue Systems. For each domain, we prepared a pool of dialogue systems to be ranked. If applicable, we reused existing systems. In order to assess the performance of Spot The Bot regarding weak models, we trained a small sequence-tosequence model (DR) for only 3 epochs, which returns mostly general answers. For the Dailydialog domain, we trained all bots in the pool using Par-lAI as there were no pre-trained models available. To leverage the recently developed language models, we fine-tune a GPT-2 (GPT) model  (Radford et al., 2018) , and a BERT-Rank (BR) model. Additionally, we train a sequence-to-sequence model (S2) with attention to compare the language models to previous state-of-the-art approaches. Together with the DR model, the pool consists of b = 4 systems. For the Empathetic Dialogues, we prepared the same pool of models as in Dailydialog. Since the recently developed Blender model  (Roller et al., 2020)  is trained on the Empathetic Dialogue dataset as well, we add the pre-trained version to the pool (BL). For the PersonaChat domain, we mostly reuse the openly available systems of the ConvAI2 challenge  (Dinan et al., 2020a) , namely, Lost in Conversation 7 (LC) and Huggingface 8 (HF), which were the top-rated dialogue systems in the ConvAI2 challenge  (Dinan et al., 2020a) , as well as KVMemNN (KV), which served as the baseline. We also add the Blender model, which is also trained in this domain. In order to have more retrieval based systems, we train a BertRank (BR) model. Together with the DR model, the pool consists of b = 6 different dialogue systems. 

 Ranking Results Table  1  gives an overview of the win rates for each pair of bots and their ranking ranges. The Chisquare test computes the significance. For each domain, most pairwise win-rates are significant. As expected, DR performs worst in all three do- mains, which is due to its repetitive nature, which is exposed over the course of a dialogue. In the Dailydialog and the Empathetic Dialogues domains, the GPT2 and the BR models perform equally, i.e., they end up in the same cluster. In both domains, systems using pre-trained language models outperform the S2 model, which is learned from scratch, which aligns with the expectation of related findings. The BL model outperforms all other models in both the PersonaChat and Empathetic Dialogues domains, which is in line with the results presented by the authors of the Blender model  (Roller et al., 2020) . Furthermore, the LC model is ranked very highly. This corresponds to the findings of the Con-vAI2 challenge  (Dinan et al., 2020a) . However, in Spot The Bot, the KV is ranked much higher than the HF model, which is not in line with the ConvAI2 evaluation. Figure  2  shows the survival functions for the three domains. The survival rates produce the same rankings as those from pairwise win rates reported in Table  1 , except for the Empathetic Dialogues domain, where GPT and BR switch places. Importantly, the distinction between these two is not significant in any of the rankings. Further non-significant differences within the Survival Analysis are S2 and DR in the Empathetic Dialogues domain, BR and S2 in the Dailydialog domain, and LC and KV in the PersonaChat domain. All other pairwise comparisons of survival curves are significant with p < 0.05 after correction for multiple comparisons. 

 Survival Analysis Feature Influence. For each of the three featuresfluency, specificity, and sensibleness -annotators have to specify whether one entity performed better, the same, or worse than the other. We encode this information as 1, 0, and ?1 respectively and fit a Cox proportional hazards model  (Cox, 1972)  for every system independently with the features as covariates. The numerical entries in Table  2  refer to the perfeature win-rate of each bot, which is computed analogously to Equation 1 using the feature annotations directly. Bold entries in Table  2  show which features have a significant influence on the system being spotted. All significant effects go in the intuitive direction, meaning that a higher feature value leads to longer survival. For example, for the DR model, the fluency feature is significant across all three domains, and together with its low fluency win rate, we can deduce that it is often spotted due to its low fluency. Sensibleness seems to be an important feature across the board, meaning that in general, bots can be spotted due to inappropriate, nonsensical answers or hide if they respond in a suitable manner. Interestingly, specificity seems to be mostly unimportant, which could be due to either the bots not being noticeably unspecific, or it being an irrelevant feature for the chosen domains. 

 Discussion 

 On Inter-Annotator Agreement The robustness of the evaluation of chatbots is often hampered by inter-annotator agreement (IAA)  (Gandhe and Traum, 2016) . Measuring and reporting IAA is not yet a standard practice in evaluating chatbots  (Amidei et al., 2019a) , and producing annotations with high IAA on open-domain conversations is prone to be impeded by subjective interpretation of feature definitions and idiosyncratic annotator behavior  (Bishop and Herron, 2015) . In our setting, annotator disagreement on a bot's human-like behavior can be interpreted as a feature of a bot's performance: A bot that manages to fool one of two annotators into believing it is human can be said to have performed better than a bot that does not manage to fool any annotator. To analyze the annotator agreement in this light, we calculate per bot and label the percentage of cases where both annotators annotate the label if one of them does. Given three labels (human, bot, unsure), the chance for random agreement is 0.33. The results averaged over all investigated domains and segment lengths per bot, are shown in Table  3 .  9  The results confirm that the bots that rank high  based on win rates and in the survival analysis (BL, GPT, LC) obtain the highest agreement on the human label and lowest agreement on the bot label. Conversely, the DR system obtains the highest agreement when being identified as a bot, and lowest when it is perceived as a human. This analysis suggests that our experiments' results do not stem from a random agreement between the annotators, i.e., the annotations of the best and worst-performing systems show agreement distinctly higher than chance regarding the respective labels. 

 On Reliability One key requirement for an evaluation procedure is that repeated executions of the procedure result in the same outcome. We measure how many pairwise conversations between two bots are needed to guarantee a stable ranking.   A more in-depth analysis reveals that ranking stability depends on the significance of pairwise comparisons. For instance, in the PersonaChat domain, the KV and LC systems are not significantly different, which leads to two different rankings depending on the subsampling: in the first, KV and LC are in the same cluster, and in the second, LC and KV are in separate clusters, with LC being on top. Thus, removing either of them from the pool would yield a more stable ranking. To investigate this further, we applied a leave-one-out stability analysis. More precisely, we applied the analysis on B \ {sys i }, where sys i ? B. Figure  3b  shows the result of the leave-one-out stability analysis. When leaving one between LC or KV out, the stability is achieved with 25 pairwise dialogues. When removing one of the other systems, the stability is reached with at least 40 dialogues. Thus, the number of pairwise bot-bot chats needed for Spot the Bot evaluation depends on the pool of bots to be evaluated and should be determined empirically. 

 On Time Efficiency Evaluation methods, which are costly and take up a long time, slow down the development cycle of dialogue systems. Spot The Bot brings down the cost and time effort compared to other methods. In Table  4  the mean time per annotation is displayed. For the Dailydialog and PersonaChat domain, the average annotation time is at around 25 seconds. For the Empathetic Dialogues, it is at 18 seconds, which is due to the shorter dialogues. We compare this to the time to create conversations between humans and bots. We recruited three dialogue system experts from our lab to interact with the systems. Each expert created 5 conversations with each system. The average times do not take into account the time needed to instruct the experts. For the Dailydialog and Empathetic Dialogues domains, it takes over 2 Minutes per conversation. For PersonaChat, the time increased to almost 4 minutes. Similarly to our experts, the average time for a human-bot conversation in the wild evaluation of the ConvAI2 challenge 10 also lies at 4 minutes 11 . Considering the 100 dialogues per system used in ConvAI, the evaluation time would be 2,000 minutes per system. In Spot the Bot, 40 annotations times 24 seconds mean 16 minutes per pair of systems. Assuming a comparison between 5 systems, an approach based on human-bot annotations such as ConvAI would require 20 thousand minutes, while Spot the Bot would do with 0,16 thousand minutes 12 . Concerning other methods based on self-talk, ACUTE-EVAL did not report the time per annotation, but they reported the time required to achieve significant results in PersonaChat, which is close to 30 minutes. Our method requires only 16 minutes (with 40 annotations). Thus, Spot The Bot increases the annotation speed while reducing the human raters' mental strain. 

 Conclusion In this work, we introduced Spot The Bot, a robust and time-efficient approach for evaluating conversational dialogue systems. It is based on conversations between bots rated by humans with respect to the bots' ability to mimic human behavior. We show that Spot The Bot yields robust and significant results while reducing the evaluation time compared to other evaluation frameworks. A team of researchers who would like to benchmark their system against four competing chatbots could do that for the cost of fewer than 3 hours of crowdsourced annotations. Spot the Bot facilitates developers making real progress based on frequent manual evaluations data, avoiding the use of noisy automatic metrics or once-in-a-year costly manual evaluations. We make the framework as well as the data publicly available. 
