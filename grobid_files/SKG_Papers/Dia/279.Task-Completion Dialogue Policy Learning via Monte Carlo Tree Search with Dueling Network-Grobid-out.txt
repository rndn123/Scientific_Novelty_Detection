title
Task-Completion Dialogue Policy Learning via Monte Carlo Tree Search with Dueling Network

abstract
We introduce a framework of Monte Carlo Tree Search with Double-q Dueling network (MCTS-DDU) for task-completion dialogue policy learning. Different from the previous deep model-based reinforcement learning methods, which uses background planning and may suffer from low-quality simulated experiences, MCTS-DDU performs decision-time planning based on dialogue state search trees built by Monte Carlo simulations and is robust to the simulation errors. Such idea arises naturally in human behaviors, e.g. predicting others' responses and then deciding our own actions. In the simulated movie-ticket booking task, our method outperforms the background planning approaches significantly. We demonstrate the effectiveness of MCTS and the dueling network in detailed ablation studies, and also compare the performance upper bounds of these two planning methods.

Introduction Designing a task-completion dialogue system has become an important task due to its huge commercial values. The dialogue agent aims to help users to complete a single or multi-domain task, e.g. booking a flight and making a hotel reservation. The core of such a system is the dialogue policy module, which enables the agent to respond properly and provide users with the desired information. Early work has shown that dialogue policy learning can be designed as a Markov Decision Process (MDP)  (Singh et al., 2002; He et al., 2018; Zhao et al., 2019; Takanobu et al., 2019) . Reinforcement learning (RL) is a common framework to solve MDP but it requires huge amounts of interactions with real users, which is generally infeasible in the real world. One way to work around this problem is by designing a user simulator using the real human conversation data  (Schatzmann et al., 2007; Li et al., 2016) . Recently, following these ideas, many researchers have applied deep model-based RL methods to task-completion dialogue policy learning  (Peng et al., 2018; Wu et al., 2018) . In a model-based method, the agent not only updates its action value or policy function through real experiences but also learns how the environment produces the next states and rewards. The learned environment is called a model, which can be further used to generate simulated experiences. Using both real and simulated experiences is referred to as background planning and can substantially improve the learning efficiency  (Sutton and Barto, 2018) .  Peng et al. (2018)  extend Dyna-Q  (Sutton, 1990; Sutton et al., 2012)  to Deep Dyna-Q (DDQ) for dialogue policy learning and achieve appealing results. However, since model learning cannot be perfect, some simulated experiences with large errors may hinder policy learning .  propose to train a discriminator to filter low-quality simulated experiences.  Wu et al. (2018)  design a switcher-based mechanism to automatically balance the use of real and simulated experiences. Nevertheless, the overall improvement is still limited. In this paper, we first upgrade the common baseline model of the task-completion dialogue policy learning problem, Deep Q-network (DQN)  (Mnih et al., 2015)  by adopting its two variants: Deep Double Q-networks (DDQN)  (Van Hasselt et al., 2016)  and Dueling network  (Wang et al., 2015) . The purpose is to fully exploit the advanced valuebased methods that are orthogonal to planning. We show that the new baseline can achieve comparable performance with DDQ. To further boost the performance, we propose to use Monte Carlo Tree Search (MCTS)  (Chaslot et al., 2008)  as decision-time planning  (Sutton and Barto, 2018) . The differences between background and decision-time planning are illustrated in Figure  1  and 2. Decisiontime planning doesn't use the model to generate simulated experiences. In the testing stage or a real decision time, rather than directly picking actions based on action values, a rollout algorithm like MCTS uses the model to build a search tree by running simulations. Performing policy evaluation on the search tree generally yields more accurate estimations of action values assuming that the model is correct. Due to this property, MCTS has achieved huge success in the game of Go  (Silver et al., , 2017 . However, its applications in the non-gaming settings, such as dialogue systems, are still rare and little studied. One difficulty is that the model now has to learn the more complex dynamics of state transitions than the deterministic game rules. Consequently, MCTS may grow a erroneous search tree and the resulting estimated action values may be wrong. To alleviate simulation errors, we design a new MCTS method incorporating the DDQN object and the dueling architecture. The main idea is to focus on the more promising parts of the state-action space and reduce the rollout depths. The dueling network can be used as heuristic or scoring functions in this case. Given dialogue states, it outputs two streams of data: action advantages and state values. Action advantages can be viewed as the prior knowledge to differentiate actions. State values can be used as the approximated rollout results. We denote an agent under this design as MCTS-DDU. Experiments show that MCTS-DDU agent outperforms previous methods by a wide margin in both task success rate and learning efficiency. Briefly, the main contributions of our work are in the following aspects: ? For the direct reinforcement learning of dialogue policy, we show that an agent trained by the extensions of DQN can perform comparatively with the latest deep model-based methods, which can serve as an advanced baseline for future works. ? For the planning part, we propose to incorporate MCTS with DDQN and Dueling network which exceeds previous approaches significantly. To our best knowledge, we are the first to apply decision-time planning and adapt MCTS for this task.  2 Background 

 Reinforcement Learning for Task-Completion Dialogue Reinforcement learning is a framework to solve sequential decision problems. The problem can be formalized as a Markov Decision Process S, A, P, R, ? , where S is a finite state space, A is a finite action space, P is a state transition function, R is a reward function, and ? is a discount factor. A value-based agent aims to learn an action value function as its implicit policy, so that its expected long-term rewards are maximized. Next, we show how to formalize a task-completion dialogue session as a MDP. State s t is defined as the dialogue history of previous t turns, containing user intents, associated slots, and agent responses. Action a t is defined as dialog act, (intent, slot), representing the agent's intent on a specific slot. Take movie-ticket booking as an example, (request, #tickets) means that the agent asks the user how many tickets are needed. Transition P represents the dialogue state updates according to stochastic responses from the user to the agent. In the case of a user simulator, the handcrafted rules define the state transitions implicitly. Reward R is the immediate feedback signal after the agent takes an action to the user, which generally depends on the dialogue status, such as in-process, success, or failure. 

 Deep Q-networks and Variants Deep Q-networks (DQN) DQN combines qlearning (Watkins and Dayan, 1992) with a deep network, noted as Q(s, a; ?), to approximate stateaction values. Generally speaking, training a deep neural network as a value function approximator trends to be notoriously unstable and has no convergence guarantee. To mitigate this problem,  Mnih et al. (2015)  utilize the experience replay technique  (Lin, 1991)  to reduce data correlation and improve data efficiency. Another critical trick is to maintain a separate target network Q(s, a; ? ? ), whose outputs serve as target values, and the parameters ? ? get soft-updated towards ? periodically. To update Q(s, a; ?), the loss function is defined as: L(?) = E e?D [(y ? Q(s t , a t ; ?)) 2 ] (1) y = r t+1 + ? max a Q(s t+1 , a; ? ? ) where D is the replay buffer holding experiences e = (s t , a t , r t+1 , s t+1 ) and ? is the discount factor. Double Deep Q-networks (DDQN) Q-learning and the plain DQN have the maximization bias problem  (Sutton and Barto, 2018) . Since the action selection and evaluation are coupled via the maximization operator, Q(s, a; ?) trends to produce overoptimistic estimations  (Hasselt, 2010) . Extending the idea of double q-learning (Hasselt, 2010) to the deep reinforcement learning settings, DDQN proposes an alternate loss to use: L(?) = E e?D [(y ? Q(s t , a t ; ?)) 2 ] (2) ?t+1 = argmax a Q(s t+1 , a; ?) y = r t+1 + ?Q(s t+1 , ?t+1 ; ? ? ) Dueling networks Dueling network is proposed as a novel architecture design for DQN. The network architecture is shown in Figure  3 . Given a state, the shared layers generate a compact hidden representation. Then, instead of estimating action values directly, the computation is separated into two streams: state value and action advantages, according to Q(s, a) = V (s) + A(s, a)  (Baird III, 1993) . This decomposition brings several benefits, such as the improved training efficiency and the separate access to state values and action advantages. 

 Planning Besides directly improving the action value function, real experiences can also be used to learn a model M = (P, R), where P and R are defined in Section 2.1. Planning refers to utilizing M to further improve the policy  (Sutton and Barto, 2018) . Background planning uses M to generate simulated experiences. By doing this, more data is available for learning. Dyna-Q, DDQ  (Peng et al., 2018) , D3Q , and Switch-DDQ  (Wu et al., 2018)  all fall into this category. In contrast, decision-time planning focuses on how to pick an action for a specific state. Namely, it tries to solve a sub-MDP starting from the "current" state. Monte Carlo Tree Search (MCTS) is a kind of decision-time planning algorithm. It uses the model to grow search trees and continually simulate more promising trajectories. Appropriate methods, like Monte Carlo and temporal-difference learning, can be then applied on the search tree to select the best action. 

 Methodology For a dialogue process involving complex state dynamics, applying planning directly is problematic, since the model learning cannot be perfect. This may result in low-quality simulated experiences for Dyna-Q and an erroneous search tree for MCTS. In the former case, it is inevitable that parameter updates would be made in some wrong directions for the action value network. While for MCTS, it is possible to reduce the incorrect portions by trimming the depths based on value function that summarize the subtrees, and preferring the actions with higher advantages for branch exploration. Since a value-based agent picks actions based on the action advantages, the state transitions induced by the higher advantages are more likely to be generalized well. Thus, branch exploration with the higher action advantages contains fewer errors by focusing more on the neighborhood of those well learned state space. Our framework for task-completion dialogue policy learning is presented in Figure  3  and Algorithm 1. In the training stage, the action value network or Q-network Q = (A, V ) is optimized via direct reinforcement learning and the model M = (P, R) is optimized via model learning respectively. In the testing stage, the agent take actions in a more thoughtful way by performing MCTS with the action advantage head A and the state value head V . Direct reinforcement learning In this stage, the agent interacts with an user, receives real experiences of the next states and rewards, and optimizes Q(s, a; ?, ?) based on the DDQN objective (Eq.2). The reward function works as follows: in each step, the agent receives a penalty of -1. By the end of a dialogue session with the maximal dialogue turns L, the agent receives a reward of 2*L if the task is completed successfully or a reward of -L if the task fails. Note that, no planning is executed during this stage, actions are chosen using the -greedy strategy. Concretely, a * = argmax a Q(s, a) with probability 1 ? and a * = unif orm(A) with probability . 

 Model learning The model M(s, a; ?, ?) = (P(s, a; ?), R(s, a; ?)) is trained via supervised learning based on pairs {(s t , a t , s t+1 )}, {(s t , a t , r t+1 )} sampled from the replay buffer. We design M to be a sample model, whose P(s, a; ?) produces a sample of s t+1 not a distribution over all the next states. By such design, the modelings of user behaviors and state updating are combined in an end-to-end manner. For the transition loss, we use the l 2 -norm of the representational differ-ences between s t and s t+1 . For the reward loss, we use the regular regression loss, Mean-Square-Error(MSE). L P (?) = E e?D [ P(s t , a t ; ?) ? s t+1 2 2 ] (3) L R (?) = E e?D [(R(s t , a t ; ?) ? r t+1 ) 2 ] (4) 

 Monte Carlo Tree Search with Dueling network In MCTS, each node represents a state and each edge represents an action causing the state transition. Each edge also stores the statistics of a cumulative action value Q c (s, a) and a visit count N (s, a). There are four steps in one MCTS simulation process, including selection, expansion, simulation and backpropagation  (Chaslot et al., 2008) . To be more specific, we use the Upper Confidence Bounds for Tree(UCT)  among the MCTS family. As mentioned, using an approximated complex environment to simulate with large branch factors and depths may lead to both high bias and high variances problems. To address these issues, the dueling network can assist the plain MCTS by providing 1) normalized action advantages as breadthwise priorities for exploration and 2) state value estimation as depth-wise early stop, both of which essentially prune the enormous state-action space. Formally, we incorporate UCT with dueling architecture and propose a new upper confidence bound called UCT D : UCT D (s, a) = Q c (s, a) N (s, a) +c?A(s, a)? 2 ln N (s) N (s, a) where N (s) = a N (s, a) is the sum of visit counts of all available actions. The first term helps to track the action with the highest empirical action value. The second term encourages the search process to explore the actions with higher normalized advantages or lower visit counts. The constant c is the hyperparameter balancing exploitation and exploration.  use a policy network to produce a prior probability and formulate the second term as  P (s,a)  1+N (s,a) . The key difference from our method is that: policy network is trained by a policy gradient method  (Sutton et al., 2000) , which trends to concentrate on the right action given a state but neglects the differences among the rest actions. Next, we will describe the simulation process in detail. Selection Given a state as the root, the action with the highest UCT D score is chosen on each tree level. This selection process runs recursively until a not fully expanded node, whose children nodes haven't been expanded all. Expansion Once such node is reached and the action is again picked by UCT D , the model M would produce a leaf node s L that represents the next state. The reward is stored for that edge and would be used in the backpropagation step. Simulation In this step, unlike the conventional rollout strategies , we simply use the value head V (s; ?) of Q(s, a; ?, ?) to estimate the state value of s L as v(s L ). This approach has proved to be effective due to using a deep network as the value function and also efficient since no single rollout is played  (Silver et al., 2017) . Backpropagation When the simulation step is finished, v(s L ) is backpropagated upwards through all the ancestor edges and updates the corresponding statistics Q c (s, a) and N (s, a). The update rules are as follows: N (s, a) ? N (s, a) + 1 (5) ?Q c (s, a) ? r(s, a) + ?Q c (s , a ) (6) where (s , a ) is the child edge of (s, a). The update value for the last edge (s L?1 , a L?1 ) is defined as: ?Q c (s L?1 , a L?1 ) ? r(s L?1 , a L?1 ) + ?V (s L ). 

 Experiments In this section, we first introduce the experimental setup and baseline models. We also propose a new model-free baseline based on the recent extensions of DQN. The effectiveness of MCTS, advantage function, and DDQN objective are demonstrated via thorough ablation studies. We also explore the tradeoff between exploitation and exploration in MCTS. Lastly, we compare the performance upper bounds of background and decision-time planning with a perfect model. 

 Setup and Baselines We consider the movie-ticket booking task that has been studied in  Peng et al. (2018) ,  and  Wu et al. (2018) .  Li et al. (2016)  convert 280 real dialogues from Amazon Mechanical Turk to a user goal set G and a dialogue schema containing 11 intents and 16 slots, which defines the feasible actions for both users and the agent. Evaluation Algorithm 1 MCTS with Double-q and Dueling Network for Task-Completion Dialogue Policy Learning 1: Initialize q-network Q = (V (s; ?), A(s, a; ?)) 2: Initialize target network: ? ? = ?, ? ? = ? 3: Initialize model M = (P(s, a; ?), R(s, a; ?)) Update 36: ? ? = ? * ? ? + (1 ? ? ) * ?, 16: ? ? = ? * ? ? + (1 ? ? ) * Execute a t and observe s t+1 , r t+1 37: end for 38: end while metric is the task success rate. The task is considered as completed successfully only when the agent manages to provide all of the users' desired information, propose appropriate suggestions, and finally inform the booking. In the training stage, we use the user simulator implemented by  Li et al. (2016)  as the environment. For a dialogue session, the simulator first samples a goal from the goal set G and generate the first utterance. Once the conversation begins, the user simulator would make responses based on the predefined rules and agent's replies. Rewards are provided to the agent based on the dialogue status (as described in the part of Direct reinforcement learning). We also use the strategy called Replay Buffer Spiking proposed in  Lipton et al. (2018) , to prefill the experience replay buffer by allowing a rulebased agent to interact with the user simulator. The successful experiences executed by the rule-based agent could considerably speed up the following training stage. Otherwise, it may take thousands of episodes for the agent to get the first positive reward due to the large state-action space. We compare our method MCTS-DDU with recently proposed methods shown as follows. Moreover, we propose a stronger baseline called DDU. ? DQN: Agent is trained by DQN (Eq.1) using real experiences only. ? Deep Dyna-Q (DDQ(K)): Agent is trained by DQN (Eq.1) with background planning. The ratio between simulated experiences and real experiences is K ? 1  (Peng et al., 2018) . ? Switch-based Active Deep Dyna-Q (Switch-DDQ): Agent is trained by DQN (Eq.1) with background planning. The switcher automatically controls the ratio between simulated experiences and real experiences  (Wu et al., 2018) . ? DDU: Agent uses dueling architecture as Qnetwork and is trained by DDQN (Eq.2) using real experiences only. ? MCTS-DDU Agent is trained in the same way as DDU. While in the decision time, actions are picked based on MCTS with Qnetwork. For all agents, the main components of Qnetworks and models are implemented as two-layer neural networks with the hidden size being 80 and ReLU activation. 

 Evaluation with User Simulator In this part, agents are evaluated by interacting with the same user simulator used in the training stage. However, a reserved part of the goal set is used for testing here. We evaluate the performances of each agent on the test goal set by the end of every training episode. The evaluation process runs 50 trials and averages the success rates. The evaluation results of all agents are summarized in Table  1 . To align with the settings of  Wu et al. (2018) , we sample the success rates at episode 100, 200 and 300. MCTS-DDU has the highest performance at all times. Note that, MCTS-DDU continues to learn even after episode300. Detailed numerical comparisons are presented in the following parts. DDQ(K) is an important group of baselines as the first proposed deep planning method in dialogue tasks, but its performances are reported with     (2018) . We reproduce it on our own and present the results in Table  1 and Figure    We define the number of episodes taken for achieving 60% success rate as a metric for comparing training efficiency. With this setting, MCTS-DDU is relatively 45.31% faster than DDQ(20) and 78.26% faster than DQN. Moreover, MCTS-DDU can reach over 50% success rate within 10 episodes.  

 Ablation studies Effectiveness of MCTS We compare the performances between MCTS-DDU and DDU that exploits Q-network directly. The result is shown in the Table  1 and Figure 5 . Based on the metric defined above, MCTS-DDU exceeds DDU by absolute 8.9% and relative 9.8% with 73.3% faster efficiency. One interesting observation is that DDU could achieve slightly higher performance than DDQ(20) in spite of the lower training efficiency. It shows that the direct reinforcement learning part has not been investigated enough in the dialogue policy learning scenario. Solely using the advanced valuebased learning methods can actually bring considerable improvement. Thus, we consider DDU to be a stronger baseline model than DQN for future study, based on which more complex mechanisms like planning can be added on. 

 Effectiveness of advantages and double-q Next, we investigate the effectiveness of the advantages function A(s, a; ?) and DDQN objective incorporated in UCT D . The result is shown in the Figure  6 . Without advantages as prior knowledge for exploration, the performance of plain MCTS is much worse than that of MCTS-DDU. The success rate fluctuates drastically due to the fact that more simulations are needed to make the action value estimations converged. We observe a slow and unstable learning process, implying merely using V (s; ?) is insufficient. Therefore, the conclusion can be reached that the advantages function A(s, a; ?) indeed improves the efficiency of simulations and is also critical for the high performance guarantee. We also perform the experiment in which the DDQN objective(Eq. 2) is replaced with the original DQN objective(Eq. 1). Even though the learning processes are quite commensurate in the early stage, MCTS-DU runs into a performance bottleneck after 200 episodes. Exploitation v.s. Exploration We also explore how the coefficient c, balancing exploitation and exploration in UCT D , effects task performances. We set the testing range to be roughly from 2 ?1 to 2 4 and the results are shown in Figure  7  and 8. As c increases from 0.5 to 4, the final success rate gets improved, emphasizing the importance of exploration. However, the performance starts to degenerate when c continues to increase. But it is still higher than those of the cases where c is small. Empirically, we believe a well-guided exploration, such as guided by an advantage function, is more influential in this task. In short, this experiment result is a clear illustration of the tradeoff between exploitation and exploration when using MCTS. From a practical perspective, the coefficient c needs to be searched carefully for the optimal value. We present the highest success rates under different settings of c within 400 episodes in Figure  8 . 

 Performance upper bound comparisons Lastly, we compare the performances of DDQ(10) and MCTS-DDU under perfect modeling learning. By "perfect", we mean there is no error in learned transition and reward functions. Our goal is to investigate the performance upper bounds of background and decision-time planning. The result in Figure  9  shows that DDQ(10) still stucks in a local optimal whereas MCTS-DDU can solve the task perfectly. We argue that the bottleneck of DDQ(10) comes from the use of a value function approximator. Contrastly, decision-time planning is able to build a true sub-MDP with a perfect model and solves it exactly. In addition, we test the plain MCTS in this setting. It almost perfectly solves the task but is less stable than MCTS-DDU, which again demonstrates the effectiveness of the advantage function. 

 Conclusions Our work introduces a novel way to apply deep model-based RL to task-completion dialogue policy learning. We combine the advanced valuebased methods with MCTS as decision-time planning. In the movie-ticket booking task, MCTS-DDU agent exceeds recent background planning approaches by a wide margin with extraordinary data efficiency. In this paper, one main focus is to demonstrate the differences between background and decisiontime planning. However, it is reasonable and straightforward to combine them together. This might be an interesting topic for future work. 

 A Appendices 

 A.1 User goal set The user goal set consists of 280 goals from real human conversation in the movie domain. We split it into 70%, 15%, and 15% for training, validation(hyperparameter tuning), and testing respectively. A sample user goal is shown as follows, where constraint slots are the determined parts of a user goal and request slots are the slots that need to be recommended by the agent.   

 A.2 Hyperparameters The main hyperparameters used in our method are listed in Table  3  Hyperparameter tunings are based on task success rates and performed over three random seeds. We use one NVIDIA V100 GPU as computing infrastructure and average runtime is about 2 hours per trial. Figure 1 : 1 Figure 1: Training and testing stages of value-based background planning. 

 Figure 2 : 2 Figure 2: Training and testing stages of decision-time planning. 

 Figure 3 : 3 Figure 3: MCTS with dueling network as decision-time planning. 

 Figure 4 : 4 Figure 4: The learning curves of DDQ(K). 

 Figure 5 : 5 Figure 5: The learning curves of MCTS-DDU, DDU, DDQ(20) and DQN. 

 performance summary of MCTS-DDU and baselines in terms of success rate and discounted cumulative return. (K) stands for K planning steps. MCTS-DDU uses c = 4 in UCT D and runs 50 simulations per dialogue turn. The results are sampled at Episode 100, 200, and 300 and are averaged over three random seeds. All model parameters are initialized randomly without extra human conversational data pre-training. Superscripts indicate the data sources, 1 for (Peng et al., 2018), 2 for (Wu et al., 2018), and 3 for our own implementations based on open-sourced codes. 

 Figure 6 : 6 Figure 6: Ablation of advantages function. 

 4. We then study the effectness of planning step K and select the best K that results in the highest average success rate in long term as the representative of the group DDQ(K) for the latter studies. The learning curves of K = (2, 5, 10, 20) are shown in Figure4. We have the similar results to Peng et al. (2018) that larger values of K make the learning process faster and success rate higher.Then we compare DQN, DDQ(20), and MCTS-DDU. The result is shown in Figure5. Methods incorporated with planning outperform than DQN both in training efficiency and success rate signifi- 

 Figure 7 : 7 Figure 7: Effects of the balancing coefficient c. 

 Figure 8 : 8 Figure 8: The highest success rates under different c. 

 Figure 9 : 9 Figure 9: Performance comparison between background /decision-time planning with a perfect model. 

 ? 17: 18: Sample a user goal from G Testing 19: Initialize s 1 as the root 20: for t = 1, T do 21: for Simulation = 1, M do MCTS 22: s ? s t 23: while s is fully expanded do 24: 26: end while 27: 31: Update statistics Q c (s, a) and 32: N (s, a) based on Eq.(5-6) 33: end while 34: end for 35: a t ? argmax a Qc(st,a) N (st,a) a ? argmax a UCT D (s, a) 25: s ? P(s, a ; ?) a ? argmax a UCT D (s, a) 28:Expand leaf state s L ? P(s, a ; ?)29: Estimate v(s L ) ? V (s L ; ?) 30:while the root s t is not reached do 

 Table 1 : 1 The Agent Episodes100 Success Return Turns Success Return Turns Success Return Turns Episodes200 Episodes300 DQN 2 .2867 -17.35 25.51 .6733 32.48 18.64 .7667 46.87 12.27 DDQ(5) 1 .6056 20.35 26.65 .7128 36.76 19.55 .7372 39.97 18.99 DDQ(5) 2 .6200 25.42 19.96 .7733 45.45 16.69 .7467 43.22 14.76 DDQ(5) 3 .6456 28.48 21.83 .6394 29.96 17.28 .6344 28.34 18.92 DDQ(10) 1 .6624 28.18 24.62 .7664 42.46 21.01 .7840 45.11 19.94 DDQ(10) 2 .6800 34.42 16.36 .6000 24.20 17.60 .3733 -2.11 15.81 DDQ(10) 3 .6254 25.71 22.59 .6759 31.99 19.61 .7209 39.24 17.92 DDQ(20) 2 .3333 -13.88 29.76 .4467 5.39 18.41 .3800 -1.75 16.69 DDQ(20) 3 .7076 45.73 16.15 .8182 51.33 16.15 .7968 48.37 15.65 Switch-DDQ 2 .5200 15.48 15.84 .8533 56.63 13.53 .7800 48.49 12.21 DDU .4675 14.15 24.01 .7611 33.89 17.41 .8562 43.07 15.69 MCTS-DDU .7312 46.63 19.77 .9090 57.26 12.79 .9314 55.87 12.13 

 The feasible action sets of the user simulator and the agent are defined by the schema of intent and slot. { constraint slots : moviename: star wars #people: 2 state: illinois city: du quoin request slots : date: ? theater: ? starttime: ? } request, inform, deny, greeting, Intent confirm answer, confirm question, closing, not sure, multiple choice, thanks, welcome city, closing, distance, date greeting, moviename, #people, Slot price, starttime, state, taskcomplete, theater chain, theater, zip ticket, video format 

 Table 2 : 2 The schema of intent and slot. 

 Table 3 : 3 . Hyperparameters for MCTS-DDU. {?} indicates the exact value range. [?] indicates the lower and upper bound for searching. Hyperparameter Search range Optimal value Max dialogue turns (Horizon) None 32 Replay buffer capacity None 5000 Batch size {16, 32, 64, 128} 16 Optimizer {SGD, RMSprop, Adam, AdamW} AdamW Epochs [1, 15] (Step=5) 10 Learning rate [1e-5, 1e-3] (Step=5e-5) 5e-3 -greedy ( ) [0.05, 0.3] (Step=0.05) 0.2 Discount (?) [0.4, 0.9] (Step=0.1) 0.5 Exploitation v.s. Exploration (c) [1,20] (Step=1) 4
