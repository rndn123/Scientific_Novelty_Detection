title
Observing Dialogue in Therapy: Categorizing and Forecasting Behavioral Codes

abstract
Automatically analyzing dialogue can help understand and guide behavior in domains such as counseling, where interactions are largely mediated by conversation. In this paper, we study modeling behavioral codes used to asses a psychotherapy treatment style called Motivational Interviewing (MI), which is effective for addressing substance abuse and related problems. Specifically, we address the problem of providing real-time guidance to therapists with a dialogue observer that (1) categorizes therapist and client MI behavioral codes and, (2) forecasts codes for upcoming utterances to help guide the conversation and potentially alert the therapist. For both tasks, we define neural network models that build upon recent successes in dialogue modeling. Our experiments demonstrate that our models can outperform several baselines for both tasks. We also report the results of a careful analysis that reveals the impact of the various network design tradeoffs for modeling therapy dialogue. Code Count Description Examples Client Behavioral Codes FN 47715 Follow/ Neutral: unrelated to changing or sustaining behavior. "You know, I didn't smoke for a while." "I have smoked for forty years now." CT 5099 Utterances about changing unhealthy behavior. "I want to stop smoking." ST 4378 Utterances about sustaining unhealthy behavior. "I really don't think I smoke too much." Therapist Behavioral Codes FA 17468 Facilitate conversation "Mm Hmm.", "OK.","Tell me more." GI 15271 Give information or feedback. "I'm Steve.", "Yes, alcohol is a depressant." RES 6246 Simple reflection about the clients most recent utterance. C: "I didn't smoke last week" T: "Cool, you avoided smoking last week." REC 4651 Complex reflection based on a client's history or the broader conversation. C: "I didn't smoke last week." T: "You mean things begin to change". QUC 5218 Closed question "Did you smoke this week?" QUO 4509 Open question "Tell me more about your week." MIA 3869 Other MI adherent,e.g., affirmation, advising with permission, etc. "You've accomplished a difficult task." "Is it OK if I suggested something?" MIN 1019 MI non-adherent, e.g., confrontation, advising without permission, etc. "You hurt the baby's health for cigarettes?" "You ask them not to drink at your house.

Introduction Conversational agents have long been studied in the context of psychotherapy, going back to chatbots such as ELIZA  (Weizenbaum, 1966)  and PARRY  (Colby, 1975) . Research in modeling such dialogue has largely sought to simulate a participant in the conversation. In this paper, we argue for modeling dialogue observers instead of participants, and focus on psychotherapy. An observer could help an ongoing therapy session in several ways. First, by monitoring fidelity to therapy standards, a helper could guide both veteran and novice therapists towards better patient outcomes. Second, rather than generating therapist utterances, it could suggest the type of response that is appropriate. Third, it could alert a therapist about potentially important cues from a patient. Such assistance would be especially helpful in the increasingly prevalent online or text-based counseling services.  1  We ground our study in a style of therapy called Motivational Interviewing  (MI, Miller and Rollnick, 2003, 2012) , which is widely used for treating addiction-related problems. To help train therapists, and also to monitor therapy quality, utterances in sessions are annotated using a set of behavioral codes called Motivational Interviewing Skill Codes  (MISC, Miller et al., 2003) . Table  1  shows standard therapist and patient (i.e., client) codes with examples. Recent NLP work  (Tanana et al., 2016; Xiao et al., 2016; P?rez-Rosas et al., 2017; Huang et al., 2018, inter alia)  has studied the problem of using MISC to assess completed sessions. Despite its usefulness, automated post hoc MISC labeling does not address the desiderata for ongoing sessions identified above; such models use information from utterances yet to be said. To provide real-time feedback to therapists, we define two complementary dialogue observers: 1. Categorization: Monitoring an ongoing session by predicting MISC labels for therapist and client utterances as they are made. 2. Forecasting: Given a dialogue history, forecasting the MISC label for the next utterance, thereby both alerting or guiding therapists. Via these tasks, we envision a helper that offers assistance to a therapist in the form of MISC labels. We study modeling challenges associated with these tasks related to: (1) representing words and utterances in therapy dialogue, (2) ascertaining relevant aspects of utterances and the dialogue history, and (3) handling label imbalance (as evidenced in Table  1 ). We develop neural models that address these challenges in this domain. Experiments show that our proposed models outperform baselines by a large margin. For the categorization task, our models even outperform previous session-informed approaches that use information from future utterances. For the more difficult forecasting task, we show that even without having access to an utterance, the dialogue history provides information about its MISC label. We also report the results of an ablation study that shows the impact of the various design choices.  2  . In summary, in this paper, we (1) define the tasks of categorizing and forecasting Motivational Interviewing Skill Codes to provide real-time assistance to therapists, (2) propose neural models for both tasks that outperform several baselines, and (3) show the impact of various modeling choices via extensive analysis. 

 Background and Motivation Motivational Interviewing (MI) is a style of psychotherapy that seeks to resolve a client's ambivalence towards their problems, thereby motivating behavior change. Several meta-analyses and empirical studies have shown the high efficacy and success of MI in psychotherapy  (Burke et al., 2004; Martins and McNeil, 2009; Lundahl et al., 2010) . However, MI skills take practice to master and require ongoing coaching and feedback to sustain  (Schwalbe et al., 2014) . Given the emphasis on using specific types of linguistic behaviors in MI (e.g., open questions and reflections), finegrained behavioral coding plays an important role in MI theory and training. Motivational Interviewing Skill Codes (MISC, table  1 ) is a framework for coding MI sessions. It facilitates evaluating therapy sessions via utterance-level labels that are akin to dialogue acts  (Stolcke et al., 2000; Jurafsky and Martin, 2019) , and are designed to examine therapist and client behavior in a therapy session.  3  As Table  1  shows, client labels mark utterances as discussing changing or sustaining problematic behavior (CT and ST, respectively) or being neutral (FN). Therapist utterances are grouped into eight labels, some of which (RES, REC) correlate with improved outcomes, while MI non-adherent (MIN) utterances are to be avoided. MISC labeling was originally done by trained annotators performing multiple passes over a session recording or a transcript. Recent NLP work speeds up this process by automatically annotating a completed MI session (e.g.,  Tanana et al., 2016; Xiao et al., 2016; P?rez-Rosas et al., 2017) . Instead of providing feedback to a therapist after the completion of a session, can a dialogue observer provide online feedback? While past work has shown the helpfulness of post hoc eval-i si ui li 1 T: Have you used drugs recently? QUC 2 C: I stopped for a year, but relapsed. FN 3 T: You will suffer if you keep using. MIN 4 C: Sorry, I just want to quit. CT  ? ? ? ? ? ? ? ? ? 

 Task Definitions In this section, we will formally define the two NLP tasks corresponding to the vision in ?2 using the conversation in table 2 as a running example. Suppose we have an ongoing MI session with utterances u 1 , u 2 , ? ? ? , u n : together, the dialogue history H n . Each utterance u i is associated with its speaker s i , either C (client) or T (therapist). Each utterance is also associated with the MISC label l i , which is the object of study. We will refer to the last utterance u n as the anchor. We will define two classification tasks over a fixed dialogue history with n elements -categorization and forecasting. As the conversation progresses, the history will be updated with a sliding window. Since the therapist and client codes share no overlap, we will design separate models for the two speakers, giving us four settings in all. Task 1: Categorization. The goal of this task is to provide real-time feedback to a therapist during an ongoing MI session. In the running example, the therapist's confrontational response in the third utterance is not MI adherent (MIN); an observer should flag it as such to bring the therapist back on track. The client's response, however, shows an inclination to change their behavior (CT). Alerting a therapist (especially a novice) can help guide the conversation in a direction that encourages it. In essence, we have the following real-time classification task: Given the dialogue history H n which includes the speaker information, predict the MISC label l n for the last utterance u n . The key difference from previous work in pre-dicting MISC labels is that we are restricting the input to the real-time setting. As a result, models can only use the dialogue history to predict the label, and in particular, we can not use models such as a conditional random field or a bi-directional LSTM that need both past and future inputs. Task 2: Forecasting. A real-time therapy observer may be thought of as an expert therapist who guides a session with suggestions to the therapist. For example, after a client discloses their recent drug use relapse, a novice therapist may respond in a confrontational manner (which is not recommended, and hence coded MIN). On the other hand, a seasoned therapist may respond with a complex reflection (REC) such as "Sounds like you really wanted to give up and you're unhappy about the relapse." Such an expert may also anticipate important cues from the client. The forecasting task seeks to mimic the intent of such a seasoned therapist: Given a dialogue history H n and the next speaker's identity s n+1 , predict the MISC code l n+1 of the yet unknown next utterance u n+1 . The MISC forecasting task is a previously unstudied problem. We argue that forecasting the type of the next utterance, rather than selecting or generating its text as has been the focus of several recent lines of work (e.g.,  Schatzmann et al., 2005; Lowe et al., 2015; Yoshino et al., 2018) , allows the human in the loop (the therapist) the freedom to creatively participate in the conversation within the parameters defined by the seasoned observer, and perhaps even rejecting suggestions. Such an observer could be especially helpful for training therapists  (Imel et al., 2017) . The forecasting task is also related to recent work on detecting antisocial comments in online conversations  (Zhang et al., 2018)  whose goal is to provide an early warning for such events. 

 Models for MISC Prediction Modeling the two tasks defined in ?3 requires addressing four questions: (1) How do we encode a dialogue and its utterances? (2) Can we discover discriminative words in each utterance? (3) Can we discover which of the previous utterances are relevant? (4) How do we handle label imbalance in our data? Many recent advances in neural networks can be seen as plug-and-play components. To facilitate the comparative study of models, we will describe components that address the above questions. In the rest of the paper, we will use boldfaced terms to denote vectors and matrices and SMALL CAPS to denote component names. 

 Encoding Dialogue Since both our tasks are classification tasks over a dialogue history, our goal is to convert the sequence of utterences into a single vector that serves as input to the final classifier. We will use a hierarchical recurrent encoder  (Li et al., 2015; Sordoni et al., 2015; Serban et al., 2016, and others)  to encode dialogues, specifically a hierarchical gated recurrent unit (HGRU) with an utterance and a dialogue encoder. We use a bidirectional GRU over word embeddings to encode utterances. As is standard, we represent an utterance u i by concatenating the final forward and reverse hidden states. We will refer to this utterance vector as v i . Also, we will use the hidden states of each word as inputs to the attention components in ?4.2. We will refer to such contextual word encoding of the j th word as v ij . The dialogue encoder is a unidirectional GRU that operates on a concatenation of utterance vectors v i and a trainable vector representing the speaker s i .  4  The final state of the GRU aggregates the entire dialogue history into a vector H n . The HGRU skeleton can be optionally augmented with the word and dialogue attention described next. All the models we will study are twolayer MLPs over the vector H n that use a ReLU hidden layer and a softmax layer for the outputs. 

 Word-level Attention Certain words in the utterance history are important to categorize or forecast MISC labels. The identification of these words may depend on the utterances in the dialogue. For example, to identify that an utterance is a simple reflection (RES) we may need to discover that the therapist is mirroring a recent client utterance; the example in table 1 illustrates this. Word attention offers a natural mechanism for discovering such patterns. We can unify a broad collection of attention mechanisms in NLP under a single high level architecture  (Galassi et al., 2019) . We seek to define attention over the word encodings v ij in the history (called queries), guided by the word encodings in the anchor v nk (called keys). The output is Method fm fc BiDAF v nk v T ij [vij; aij; vij aij; vij a ] GMGRU w e tanh(W k v nk [vij; aij] + W q [vij; hj?1]) Table  3 : Summary of word attention mechanisms. We simplify BiDAF with multiplicative attention between word pairs for f m , while GMGRU uses additive attention influenced by the GRU hidden state. The vector w e ? R d , and matrices W k ? R d?d and W q ? R 2d?2d are parameters of the BiGRU. The vector h j?1 is the hidden state from the BiGRU in GM-GRU at previous position j ? 1. For combination function, BiDAF concatenates bidirectional attention information from both the key-aware query vector a ij and a similarly defined query-aware key vector a . GMGRU uses simple concatenation for f c . a sequence of attention-weighted vectors, one for each word in the i th utterance. The j th output vector a j is computed as a weighted sum of the keys: a ij = k ? k j v nk (1) The weighting factor ? k j is the attention weight between the j th query and the k th key, computed as ? k j = exp (f m (v nk , v ij )) j exp f m (v nk , v ij ) (2) Here, f m is a match scoring function between the corresponding words, and different choices give us different attention mechanisms. Finally, a combining function f c combines the original word encoding v ij and the above attention-weighted word vector a ij into a new vector representation z ij as the final representation of the query word encoding: z ij = f c (v ij , a ij ) (3) The attention module, identified by the choice of the functions f m and f c , converts word encodings in each utterance v ij into attended word encodings z ij . To use them in the HGRU skeleton, we will encode them a second time using a BiGRU to produce attention-enhanced utterance vectors. For brevity, we will refer to these vectors as v i for the utterance u i . If word attention is used, these attended vectors will be treated as word encodings. To complete this discussion, we need to instantiate the two functions. We use two commonly used attention mechanisms: BiDAF  (Seo et al., 2016)  and gated matchLSTM  (Wang et al., 2017) . For simplicity, we replace the sequence encoder in the latter with a BiGRU and refer to it as GMGRU. Table  3  shows the corresponding definitions of f c and f m . We refer the reader to the original papers for further details. In subsequent sections, we will refer to the two attended versions of the HGRU as BIDAF H and GMGRU H . 

 Utterance-level Attention While we assume that the history of utterances is available for both our tasks, not every utterance is relevant to decide a MISC label. For categorization, the relevance of an utterance to the anchor may be important. For example, a complex reflection (REC) may depend on the relationship of the current therapist utterance to one or more of the previous client utterances. For forecasting, since we do not have an utterance to label, several previous utterances may be relevant. For example, in the conversation in Table  2 , both u 2 and u 4 may be used to forecast a complex reflection. To model such utterance-level attention, we will employ the multi-head, multi-hop attention mechanism used in Transformer networks  (Vaswani et al., 2017) . As before, due to space constraints, we refer the reader to the original work for details. We will use the (Q, K, V ) notation from the original paper here. These matrices represent a query, key and value respectively. The multi-head attention is defined as: Multihead(Q, K, V ) = [head 1 ; ? ? ? ; head h ]W O (4) head i = softmax QW Q i KW K i T ? d k V W V i The W i 's refer to projection matrices for the three inputs, and the final W o projects the concatenated heads into a single vector. The choices of the query, key and value defines the attention mechanism. In our work, we compare two variants: anchor-based attention, and self-attention. The anchor-based attention is de- fined by Q = [v n ] and K = V = [v 1 ? ? ? v n ]. Self-attention is defined by setting all three matrices to [v 1 ? ? ? v n ]. For both settings, we use four heads and stacking them for two hops, and refer to them as SELF 42 and ANCHOR 42 . 

 Addressing Label Imbalance From Table  1 , we see that both client and therapist labels are imbalanced. Moreover, rarer la-bels are more important in both tasks. For example, it is important to identify CT and ST utterances. For therapists, it is crucial to flag MI nonadherent (MIN) utterances; seasoned therapists are trained to avoid them because they correlate negatively with patient improvements. If not explicitly addressed, the frequent but less useful labels can dominate predictions. To address this, we extend the focal loss (FL  Lin et al., 2017)  to the multiclass case. For a label l with probability produced by a model p t , the loss is defined as FL(p t ) = ? t (1 ? p t ) ? log(p t ) (5) In addition to using a label-specific balance weight ? t , the loss also includes a modulating factor (1 ? p t ) ? to dynamically downweight wellclassified examples with p t 0.5. Here, the ? t 's and the ? are hyperparameters. We use FL as the default loss function for all our models. 

 Experiments The original psychotherapy sessions were collected for both clinical trials and Motivational Interviewing dissemination studies including hospital settings (Roy-Byrne et al., 2014), outpatient clinics  (Baer et al., 2009) , college alcohol interventions  (Tollison et al., 2008; Neighbors et al., 2012; Lee et al., 2013 Lee et al., , 2014 . All sessions were annotated with the Motivational Interviewing Skills Codes (MISC) . We use the train/test split of  Can et al. (2015) ;  Tanana et al. (2016)  to give 243 training MI sessions and 110 testing sessions. We used 24 training sessions for development. As mentioned in ?2, all our experiments are based on the MISC codes grouped by  Xiao et al. (2016) . 

 Preprocessing and Model Setup An MI session contains about 500 utterances on average. We use a sliding window of size N = 8 utterances with padding for the initial ones. We assume that we always know the identity of the speaker for all utterances. Based on this, we split the sliding windows into a client and therapist windows to train separate models. We tokenized and lower-cased utterances using spaCy  (Honnibal and Montani, 2017) . To embed words, we concatenated 300-dimensional Glove embeddings  (Pennington et al., 2014)  with ELMo vectors  (Peters et al., 2018) . The appendix details the model setup and hyperparameter choices. 

 Results Best Models. Our goal is to discover the best client and therapist models for the two tasks. We identified the following best configurations using F 1 score on the development set: 1. Categorization: For client, the best model does not need any word or utterance attention. For the therapist, it uses GMGRU H for word attention and ANCHOR 42 for utterance attention. We refer to these models as C C and C T respectively 2. Forecasting: For both client and therapist, the best model uses no word attention, and uses SELF 42 utterance attention. We refer to these models as F C and F T respectively. Here, we show the performance of these models against various baselines. The appendix gives label-wise precision, recall and F 1 scores. Results on Categorization. Tables  4 and 5  show the performance of the C C and C T models and the baselines. For both therapist and client categorization, we compare the best models against the same set of baselines. The majority baseline illustrates the severity of the label imbalance lem.  Xiao et al. (2016)  The second set of baselines (below the line) are models that use dialogue context. Both  Can et al. (2015)  and  Tanana et al. (2016)  use wellstudied linguistic features and then tagging the current utterance with both past and future utterance with CRF and MEMM, respectively. To study the usefulness of the hierarchical encoder, we implemented a model that uses a bidirectional GRU over a long sequence of flattened utterance. We refer to this as CONCAT C . This model is representative of the work of  Huang et al. (2018) , but was reimplemented to take advantage of ELMo. For categorizing client codes, BiGRU ELMo is a simple but robust baseline model. It outperforms the previous best no-context model by more than 2 points on macro F 1 . Using the dialogue history, the more sophisticated model C C further gets 1 point improvement. Especially important is its improvement on the infrequent, yet crucial labels CT and ST. It shows a drop in the F 1 on the FN label, which is essentially considered to be an unimportant, background class from the point of view of assessing patient progress. For therapist codes, as the highlighted numbers in Table  5  show, only incorporating GMGRU-based word-level attention, GMGRU H has already outperformed many baselines, our proposed model F T which uses both GMGRU-based word-level attention and anchorbased multi-head multihop sentence-level attention can further achieve the best overall performance. Also, note that our models outperform approaches that take advantage of future utterances. For both client and therapist codes, concatenating dialogue history with CONCAT C always performs worse than the hierarchical method and even the simpler BiGRU ELMo . Results on Forecasting. Since the forecasting task is new, there are no published baselines to compare against. Our baseline systems essentially differ in their representation of dialogue history. as the model CONCAT C from the categorizing task. We also show comparisons to the simple HGRU model and the GMGRU H model that uses a gated matchGRU for word attention.  6  Tables 6 (a,b) show our forecasting results for client and therapist respectively. For client codes, we also report the CT and ST performance on the development set because of their importance. For the therapist codes, we also report the recall@3 to show the performance of a suggestion system that displayed three labels instead of one. The results show that even without an utterance, the dialogue history conveys signal about the next MISC label. Indeed, the performance for some labels is even better than some categorization baseline systems. Surprisingly, word attention (GMGRU H ) in Table  6  did not help in forecasting setting, and a model with the SELF 42 utterance attention is sufficient.  6  The forecasting task bears similarity to the next utterance selection task in dialogue state tracking work  (Yoshino et al., 2018) . In preliminary experiments, we found that the Dual-Encoder approach used for that task consistently underperformed the other baselines described here. For the therapist labels, if we always predicted the three most frequent labels (FA, GI, and RES), the recall@3 is only 67.7, suggesting that our models are informative if used in this suggestion-mode. 

 Analysis and Ablations This section reports error analysis and an ablation study of our models on the development set. The appendix shows a comparison of pretrained domain-specific ELMo/glove with generic ones and the impact of the focal loss compared to simple or weighted cross-entropy. 

 Label Confusion and Error Breakdown Figure  1  shows the confusion matrix for the client categorization task. The confusion between FN and CT/ST is largely caused by label imbalance. There are 414 CT examples that are predicted as ST and 391 examples vice versa. To further understand their confusion, we selected 100 of each for manual analysis. We found four broad categories of confusion, shown in Table  7 .   The first category requires more complex reasoning than just surface form matching. For example, the phrase seven out of ten indicates that the client is very confident about changing behavior; the phrase wind down after work indicates, in this context, that the client drinks or smokes after work. We also found that the another frequent source of error is incomplete information. In a face-to-face therapy session, people may use concise and effient verbal communication, with guestures and other body language conveying information without explaining details about, for example, coreference. With only textual context, it is difficult to infer the missing information. The third category of errors is introduced when speech is transcribed into text. The last category is about ambivalent speech. Discovering the real attitude towards behavior change behind such utterances could be difficult, even for an expert therapist. Figures  1 and 2  show the label confusion matrices for the best categorization models. We will examine confusions that are not caused purely by a label being frequent. We observe a common confusion between the two reflection labels, REC and RES. Compared to the confusion matrix from  Xiao et al. (2016) , we see that our models show much-decreased confusion here. There are two  reason for this confusion persisting. First, the reflections may require a much longer information horizon. We found that by increasing the window size to 16, the overall reflection results improved. Second, we need to capture richer meaning beyond surface word overlap for RES. We found that complex reflections usually add meaning or emphasis to previous client statements using devices such as analogies, metaphors, or similes rather than simply restating them. Closed questions (QUC) and simple reflections (RES) are known to be a confusing set of labels. For example, an utterance like Sounds like you're suffering? may be both. Giving information (GI) is easily confused with many labels because they relate to providing information to clients, but with different attitudes. The MI adherent (MIA) and non-adherent (MIN) labels may also provide information, but with supportive or critical attitude that may be difficult to disentangle, given the limited number of examples. 

 How Context and Attention Help? We evaluated various ablations of our best models to see how changing various design choices changes performance. We focused on the context window size and impact of different word level and sentence level attention mechanisms. Tables  8 and 9  summarize our results. History Size. Increasing the history window size generally helps. The biggest improvements are for categorizing therapist codes (Table  9 ), especially for the RES and REC. However, increasing the window size beyond 8 does not help to categorize client codes (Table  8 ) or forecasting (in appendix). Word-level Attention. Only the model C T uses word-level attention. As shown in Table  9 , when we remove the word-level attention from it, the overall performance drops by 3.4 points, while performances of RES and REC drop by 3.3 and 5 points respectively. Changing the attention to BiDAF decreases performance by about 2 points (still higher than the model without attention). Sentence-level Attention. Removing sentence attention from the best models that have it decreases performance for the models C T and F T (in appendix). It makes little impact on the F C , however. Table  8  shows that neither attention helps categorizing clients codes. 

 Can We Suggest Empathetic Responses? Our forecasting models are trained on regular MI sessions, according to the label distribution on Table 1, there are both MI adherent or non-adherent data. Hence, our models are trained to show how the therapist usually respond to a given statement. To show whether our model can mimic good MI policies, we selected 35 MI sessions from our test set which were rated 5 or higher on a 7-point scale empathy or spirit. On these sessions, we still achieve a recall@3 of 76.9, suggesting that we can learn good MI policies by training on all therapy sessions. These results suggest that our models can help train new therapists who may be uncertain about how to respond to a client. 

 Conclusion We addressed the question of providing real-time assistance to therapists and proposed the tasks of categorizing and forecasting MISC labels for an ongoing therapy session. By developing a modular family of neural networks for these tasks, we show that our models outperform several baselines by a large margin. Extensive analysis shows that our model can decrease the label confusion compared to previous work, especially for reflections and rare labels, but also highlights directions for future work.  We trained all models using Adam  (Kingma and Ba, 2015)  with learning rate chosen by cross validation between [1e ?4 , 5 * 1e ?4 ], gradient norms clipping from at [1.0, 5.0], and minibatch sizes of 32 or 64. We use the same hidden size for both utterance encoder, dialogue encoder and other attention memory hidden size; it has been selected from {64, 128, 256, 512}. We set a smaller dropout 0.2 for the final two fully connected layers. All the models are trained for 100 epochs with earlystoping based on macro F 1 over development results. Detailed Results of Our Main Models In the main text, we only show the F 1 score of each our proposed models. We summarize the performance of our best models for both categorzing and forecasting MISC codes in  
