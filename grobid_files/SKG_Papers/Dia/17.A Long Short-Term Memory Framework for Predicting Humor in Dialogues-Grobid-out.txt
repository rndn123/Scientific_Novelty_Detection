title
A Long Short-Term Memory Framework for Predicting Humor in Dialogues

abstract
We propose a first-ever attempt to employ a Long Short-Term memory based framework to predict humor in dialogues. We analyze data from a popular TV-sitcom, whose canned laughters give an indication of when the audience would react. We model the setuppunchline relation of conversational humor with a Long Short-Term Memory, with utterance encodings obtained from a Convolutional Neural Network. Out neural network framework is able to improve the F-score of 8% over a Conditional Random Field baseline. We show how the LSTM effectively models the setup-punchline relation reducing the number of false positives and increasing the recall. We aim to employ our humor prediction model to build effective empathetic machine able to understand jokes.

Introduction There has been many recent attempts to detect and understand humor, irony and sarcasm from sentences, usually taken from Twitter  (Reyes et al., 2013; Barbieri and Saggion, 2014; Riloff et al., 2013; Joshi et al., 2015) , customer reviews  (Reyes and Rosso, 2012)  or generic canned jokes  (Yang et al., 2015) .  Bamman and Smith (2015)  and  Karoui et al. (2015)  included the surrounding context. Our work has a different focus from the above. We analyze transcripts of funny dialogues, a genre somehow neglected but important for human-robot interaction. Laughter is the natural reaction of people to a verbal or textual humorous stimulus. We want to predict when the audience would laugh. Compared to a typical canned joke or a sarcastic Tweet, a dialog utterance is perceived as funny only in relation to the dialog context and the past history. In a spontaneous setting a funny dialog is usually built through a setup which prepares the audience to receive the humorous discourse stimuli, followed by a punchline which releases the tension and triggers the laughter reaction  (Attardo, 1997; Taylor and Mazlack, 2005) . Automatic understanding of a humorous dialog is a first step to build an effective empathetic machine fully able to react to the user's humor and to other discourse stimuli. We are ultimately interested in developing robots that can bond with humans better  (Devillers et al., 2015) . As a source of situational humor we study a popular TV sitcom: "The Big Bang Theory". The domain of sitcoms is of interest as it provides a full dialog setting, together with an indication of when the audience is expected to laugh, given by the background canned laughters. An example of dialog from this sitcom, as well as of the setup-punchline schema, is shown below (punchlines in bold): LAUGH He started America on a path to the metric system but then just gave up. LAUGH The utterances before the punchline are the setup. Without them, the punchlines may not be perceived as humorous (the last utterance, out of context, may be a political complaint), only with proper setup a laughter would be triggered. The humorous intent is also strengthen by the fact the dialog takes place in a bar (evident from the previous and following utterances), where a request of 40 ml of "Ethyl Alcohol" is unusual and weird. Our previous attempts on the same corpus  (Bertero and Fung, 2016b; Bertero and Fung, 2016a)  showed that using a bag-of-ngram representation over a sliding window or a simple RNN to capture the contextual information of the setup was not ideal. For this reason we propose a method based on a Long Short-Term Memory network  (Hochreiter and Schmidhuber, 1997) , where we encode each sentence through a Convolutional Neural Network  (Collobert et al., 2011) . LSTM is successfully used in context-dependent sequential classification tasks such as speech recognition  (Graves et al., 2013) , dependency parsing  (Dyer et al., 2015)  and conversation modelling  (Shang et al., 2015) . This is also to our knowledge the first-ever attempt that a LSTM is applied to humor response prediction or general humor detection tasks. 

 Methodology We employ a supervised classification method to detect when punchlines occur. The bulk of our classifier is made of a concatenation of a  Convolutional Neural Network (Collobert et al., 2011)  to encode each utterance, followed by a Long Short-Term Memory  (Hochreiter and Schmidhuber, 1997)  to model the sequential context of the dialog. Before the output softmax layer we add a vector of higher level syntactic, structural and sentiment features. A framework diagram is shown in Figure  1 . 

 Convolutional Neural Network for each utterance The first stage of our classifier is represented by a Convolutional Neural Network  (Collobert et al., 2011) . Low-level, high-dimensional input feature vectors are fed into a first embedding layer to obtain a low dimensional dense vector. A sliding window is lt are the high level feature vectors, and yt the outputs for each utterance. then moved on these vectors and another layer is applied to each group of token vectors, in order to capture the local context of each token. A max-pooling operation is then applied to extract the most salient features of all the tokens into a single vector for the whole utterance. An additional layer is used to generalize and distribute each feature to its full range before obtaining the final utterance vector. In our task we use three input features: 1. Word tokens: each utterance token is represented as a one-hot vector. This feature models how much each word is likely to trigger humor in the specific corpus. 2. Character trigrams: each token is represented as a bag-of-character-trigrams vector. The feature models the role of the word signifier and removes the influence of the word stems. 3. Word2Vec: we extract for each token a word vector from word2vec  (Mikolov et al., 2013) , trained on the text9 Wikipedia corpus 1 . This representation models the general semantic meanings, and matches words that do not appear to others similar in meaning. The convolution and max-pooling operation is applied individually to each feature, and the three vectors obtained are then concatenated together and fed to the final sentence encoding layer, which combines all the contributions. 

 Long/Short Term Memory for the utterance sequence The LSTM is an improvement over the Recurrent Neural Network aimed to improve its memory capabilities. In a standard RNN the hidden memory layer is updated through a function of the input and the hidden layer at the previous time instant: h t = tanh(W x x t + W h h t?1 + b) (1) where x is the network input and b the bias term. This kind of connection is not very effective to maintain the information stored for long time instants, as well as it does not allow to forget unneeded information between two time steps. The LSTM enhances the RNN with a series of three multiplicative gates. The structure is the following: i t = ?(W ix x t + W i h h t?1 + b i ) (2) f t = ?(W fx x t + W f h h t?1 + b f ) (3) o t = ?(W ox x t + W o h h t?1 + b o ) (4) s t = tanh(W sx x t + W s h h t?1 + b h ) (5) c t = f t c t?1 + i t s t (6) h t = tanh(c t ) o t (7) where is the element-wise product. Each gate factor is able to let through or suppress a specific update contribution, thus allowing a selective information retaining. The input gate i is applied to the cell input s, the forget gate f to the cell value at the previous time step c t?1 , and the output gate o to the cell output for the current time instant h t . In this way a cell value can be retained for multiple time steps when i = 0, ignored in the output when o = 0, and forgotten when f = 0. As dialog utterances are sequential, we feed all utterance vectors of a sitcom scene in sequence into a Long Short-Term Memory block to incorporate contextual information. The memory unit of the LSTM keeps track of the context in each scene, and mimics human memory to accumulate the setup that may trigger a punchline. Before the output we incorporate a set of high level features from our previous work (Bertero and Fung, 2016b) and past literature  (Reyes et al., 2013; Barbieri and Saggion, 2014) . They include: ? Structural features: average word length, sentence length, difference in sentence length with the five previous utterances. ? Part of speech proportion: noun, verbs, adjectives and adverbs. ? Antonyms: proportion of antonym words with the previous utterance (from  WordNet (Miller, 1995) ). ? Sentiment: positive, negative and average sentiment score among all words (from Senti-WordNet  (Esuli and Sebastiani, 2006) ). ? Speaker and turn: speaker character identity and utterance position in the turn (beginning, middle, end, isolated). ? Speaking rate: time duration of the utterance from the subtitle files, divided by the sentence length. All these features are concatenated to the LSTM output, and a softmax layer is applied to get the final output probabilities. 

 Experiments 

 Corpus We built a corpus from the popular TV-sitcom "The Big Bang Theory", seasons 1 to 6. We downloaded the subtitle files (annotated with the timestamps of each utterance) and the scripts 2 , used to segment all the episodes into scenes and get the speaker identity of each utterance. We extracted the audio track of each episode in order to retrieve the canned laughters timestamps, with a vocal removal tool followed by a silence/sound detector. We then annotated each utterance as a punchline in case it was followed by a laughter within 1s, assuming that utterances not followed by a laughter would be the setup for the punchline. We obtained a total of 135 episodes, 1589 overall scenes, 42.8% of punchlines, and an average interval between two punchlines of 2.2 utterances. We built a training set of 80% of the overall episodes, and a development and test set of 10% each. The episodes were drawn from all the seasons with the same proportion. The total number of utterances is 35865 for the training set, 3904 for the development set and 3903 for the test set. 

 Experimental setup and baseline In the neural network we set the size to 100 for all the hidden layers of the CNN and the LSTM, and 5 to the convolutional window. We applied a dropout regularization layer  (Srivastava et al., 2014)  after the output of the LSTM, and L2 regularization on the softmax output layer. The network was trained with standard backpropagation, using each scene as a training unit. The development set was used to tune the hyperparameters, and to determine the early stopping condition. When the error on the development set began to increase for the first time we kept training only the final softmax layer, this improved the overall results. The neural network was implemented with THEANO toolkit  (Bergstra et al., 2010) . We ran experiments with and without the extra high-level feature vector. As a baseline for comparison we used an implementation of the Conditional Random Field  (Lafferty et al., 2001)  from CRFSuite  (Okazaki, 2007) , with L2 regularization. We ran experiments using the same high level feature vector added at the end of the neural network, 1-2-3gram features of a window made by the utterance and the four previous, and the two feature sets combined. We also compared the overall system where we replace the CNN with an LSTM sentence encoder  (Li et al., 2015) , where we kept the same input features. 

 Results and discussion Results of our system and our baselines are shown in table 1. The LSTM with the aid of the high level feature vector generally outperformed all the CRF baselines with the highest accuracy of 70.0% and the highest F-score of 62.9%. The biggest improvement of the LSTM is the improvement of the recall without affecting too much the precision. Lexical features given by n-gram from a context window are very useful to recognize more punchlines in our baseline experiment, but they also yield many false positives, when the same n-gram is used in different contexts. A CNN-LSTM network seems to overcome this issue as the CNN stage is better in modeling the lexical and semantic content of the utterance, as the LSTM allows to put each utterance in relation with the past context, filtering out many false positives from wrong contexts. The choice of the CNN is further justified by the results obtained from the comparison between the CNN and the LSTM sentence encoding input, shown in table 2. The CNN is more effective, obtaining a recall of 10% higher and 6% more in Fscore. The CNN is a simpler model that might benefit more of a small-size corpus. It also required a much shorter training time compared to the LSTM. We may consider in the future to use more data, and try other sentence input encoders, including deeper or bi-directional LSTMs, to find the most effective one. Predicting humor response from the canned laughters is a particularly challenging task. In some cases canned laughters are inserted by the show producers with the purpose of solicit response to weak jokes, where otherwise people would not laugh. The audience must also be kept constantly amused, extra canned laughters may help in scenes where fewer jokes are used. 

 Conclusion and future work We proposed a Long Short-Term Memory based framework to predict punchlines in a humorous dialog. We showed that our neural network is particularly effective in increasing the F-score to 62.9% over a Conditional Random Field baseline of 58.1%. We furthermore showed that the LSTM is more effective in obtaining an higher recall with fewer false positives compared to simple n-gram shifting context window features. As future work we plan to use a virtual agent system to collect a set of human-robot humorous interactions, and adapt our model to predict humor from them. Figure 1 : 1 Figure 1: Framework diagram. at, bt and ct are the CNN three input features (words, word2vec and character trigrams). 
