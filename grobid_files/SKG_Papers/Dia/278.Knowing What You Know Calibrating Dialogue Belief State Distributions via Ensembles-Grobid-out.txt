title
Knowing What You Know: Calibrating Dialogue Belief State Distributions via Ensembles

abstract
The ability to accurately track what happens during a conversation is essential for the performance of a dialogue system. Current stateof-the-art multi-domain dialogue state trackers achieve just over 55% accuracy on the current go-to benchmark, which means that in almost every second dialogue turn they place full confidence in an incorrect dialogue state. Belief trackers, on the other hand, maintain a distribution over possible dialogue states. However, they lack in performance compared to dialogue state trackers, and do not produce well calibrated distributions. In this work we present state-of-the-art performance in calibration for multi-domain dialogue belief trackers using a calibrated ensemble of models. Our resulting dialogue belief tracker also outperforms previous dialogue belief tracking models in terms of accuracy.

Introduction Task-oriented dialogue systems aim to act as assistants to their users, solving tasks such as finding a restaurant, booking a train, or providing information about a tourist attraction. They have become very popular with the introduction of virtual assistants such as Siri and Alexa. Two tasks are fundamental to such a system. The first is the ability to track what happened in the conversation, referred to as tracking. Based on the result of tracking, the system needs to conduct the conversation towards the fulfilment of the user goal, referred to as planning. The tracking component summarises the dialogue history, or the past, while the planning component manages the dialogue and concerns the future. In this work we focus on the first component. Early approaches to statistical dialogue modelling view dialogue as a Markov decision process  (Levin et al., 1998)  and define a set of dialogue states that the conversation can be in at any given dialogue turn. The tracking component tracks the dialogue state. In recent years discriminative models achieve state-of-the-art dialogue state tracking (DST) results  Zhang et al., 2019; Heck et al., 2020) . Still, in a multidomain setting such as MultiWOZ  (Eric et al., 2019; , they achieve an accuracy of just over 55%. This means that in approximately 45% of cases they make a wrong prediction and, even worse, they have full confidence in that wrong prediction. In the wake of statistical dialogue modeling, the use of partially observable Markov decision processes has been proposed to address this issue. The idea is to model the probability over all possible dialogue states in every dialogue turn  (Williams and Young, 2007) . This probability distribution is referred to as the belief state. The advantages of belief tracking are probably best illustrated by an excerpt from a dialogue with a real user in  (Metallinou et al., 2013) : even though the dialogue state predicted with the highest probability is not the true one, the system is able to provide a valid response because the true dialogue state also has assigned a non-zero probability. A model is considered well calibrated if its confidence estimates are aligned with the empirical likelihood of its predictions  (Desai and Durrett, 2020) . The belief state can be modelled by deep learning-based approaches such as the neural belief tracker  (Mrk?i? et al., 2017) , the multi-domain belief tracker  (Ramadan et al., 2018) , the globally conditioned encoder belief tracker  (Nouri and Hosseini-Asl, 2018)  and the slot utterance matching belief tracker (SUMBT)  models. None of these models however address the issue of calibrating the probability distribution that they provide, resulting in them being more confident than they should be. In a dialogue setting, overconfidence can lead to bad decisions and unsuccessful dialogues. In this work, we present methods for learning well-calibrated belief distributions. Our contributions are the following: ? We present the state-of-the-art performance in calibration for dialogue belief trackers using a calibrated ensemble of models, called the calibrated ensemble belief state tracker (CE-BST). ? Our model achieves best overall joint goal accuracy among the state-of-the-art belief tracking models. Such a well-calibrated belief tracking model is essential for the planning component to successfully conduct dialogue. 

 Related Work Since no other belief tracking methods that we are aware of have achieved success in producing wellcalibrated confidence, we look towards methods used in other language tasks. Natural language inference is a related task that also benefits from wellcalibrated confidence in predictions.  Desai and Durrett (2020)  introduce the use of post-processing techniques such as temperature scaling to produce better-calibrated confidence estimates. Additionally, there have been recent advances in the construction of more adequate loss functions. These methods, including Bayesian matching and prior networks, aim to learn well-calibrated models without the burden of requiring many extra parameters. These methods achieve good calibration in computer vision tasks such as CIFAR  (Joo et al., 2020; Malinin and Gales, 2018; Szegedy et al., 2016) . When the limitations of a single model still inhibit us from producing more accurate and bettercalibrated models, a popular alternative is to use an ensemble of models. Recently  Malinin and Gales (2020)  showed the success of using an ensemble of models for machine translation, and in particular utilising accurate confidence predictions for analysing translation quality. 

 Calibration Techniques In this section we explain the details of three calibration techniques that we apply to dialogue belief tracking. 

 Loss Functions The loss function can have a great impact on the calibration and accuracy of models. The most commonly used loss function in belief tracking is the standard softmax cross entropy loss. However, it tends to cause overconfident predictions where most of the probability is placed on the top class. Label smoothing cross entropy  (Szegedy et al., 2016)  aims to resolve this problem by replacing the one-hot targets of cross entropy with a smoothed target distribution. That is, for label y i and smoothing parameter ? ? 0, 1 K , the target distribution will be: t(c|?, y i ) = 1 ? (K ? 1)? c = y i , ? otherwise, ( 1 ) where K is the number of possible values of c. The loss for a model with parameters ? and a set of N output logits ?1 , ?2 , ..., ?N with true labels y 1 , y 2 , ..., y N is defined as: L(?, ?) = 1 N N i=1 KL [Softmax(? i )||t(c i |?, y i )] , (2) where KL is the Kullback-Leibler divergence between two distributions  (Kullback and Leibler, 1951) . Alternatively, Bayesian matching loss  (Joo et al., 2020)  uses a Dirichlet distribution as the final activation function. The target is constructed using the Bayes rule, where we assume the observed label y i to be an observation from a categorical distribution y i |? i ? Cat(? i ) and ? i is the true underlying distribution of the label. To introduce uncertainty into the target distribution we assume that the prior of ? i is a Dirichlet distribution, Dir(1). In this way, we have a highly uncertain prior distribution. From this it can be shown that the posterior will be ? i |y i ? Dir(1 + I(y i )), where I(y i ) is the one-hot representation of y i . The loss function is then constructed using the negative log likelihood of the true label given the predicted distribution ?i ? Dir(? i ), penalised by the KL divergence from the the uncertain Dir(1) distribution: L(?, ?) = N i=1 {?KL [ ?i ||Dir(1)] ? E ?i [log(p(y i | ?i ))]}, (3) where ? > 0 is the penalisation parameter. 

 Ensemble Distribution Estimation From a Bayesian viewpoint, the probability of observing an outcome given the observed examples can be broken down into two components: the predictive distribution of the model and the posterior of the model given the observed examples. The posterior of the model given the data is an unknown distribution which can be estimated in various ways. One method is to use an ensemble of models, where the ensemble acts as an estimator for the posterior distribution of the parameters, p(?|D), where D represents the observed examples. Let q(?) represent the distribution over all possible members of an ensemble. This distribution could be seen as the ensemble estimate of the posterior, p(?|D),  (Malinin et al., 2019; Malinin and Gales, 2020) . Hence, p(y|x, D) = p(y|x, ?)q(?)d?. (4) Since this integral is still intractable we need to estimate it using Monte Carlo. To sample from the ensemble distribution q(?) we consider two approaches: using dropout during inference to collect an ensemble of N equally likely models  (Gal and Ghahramani, 2016) , or alternatively bootstrap sampling N equally likely subsets of the data to train N equally likely ensemble members. Let these N members be {?  (1)  , ? (2) , ..., ? (N ) }. The estimated predictive distribution can then be calculated as follows: p(y|x, D) = 1 N N i=1 p(y|x, ? (i) ) (5) 

 Temperature Scaling Temperature scaling is a post-processing technique which scales the logits of the model by a scaling factor ? > 1  (Guo et al., 2017) , resulting in bettercalibrated estimates. The temperature scaling parameter ? can be trained on a development set. 

 Experimental Setup We seek to build a well-calibrated dialogue belief tracker. For our baseline belief tracker, we use the SUMBT model architecture , which uses BERT  (Devlin et al., 2018)  as a turn encoder and multi-head attention for slot candidate matching. We perform all experiments on the Mul-tiWOZ 2.1 dataset  (Eric et al., 2019) , the current standard dataset for multi-domain dialogue. When training using Bayesian matching, we use a scaling coefficient of ? = 0.003, and for label smoothing, a smoothing coefficient of ? = 0.05. For the ensemble belief tracker, we train 10 identical independent models, each with a sub-sample of 7500 dialogues. All hyper-parameters are obtained using a parameter search based on validation set performance. For all training, we use the BERT-base-uncased model from PyTorch Transformers  (Wolf et al., 2019)  for turn embedding. We use a gated recurrent unit with a hidden dimension 300 for latent tracking and Euclidean distance for value candidate scoring. During training, we use a learning rate of 5e ? 5 in combination with a linear learning rate scheduler, the warm-up proportion is set to 0.1. A dropout rate of 0.3 is used, and training is performed for 100 epochs. 1 5 Evaluation Metrics 

 Joint Goal Accuracy The joint goal accuracy (JGA) is the percentage of turns for which the model predicts the complete user goal correctly. We further propose the introduction of an adjusted top 3 JGA, which considers a user goal prediction correct if the true label for each slot is among the top 3 predicted candidates for that slot in the belief state given there are at least 5 possible candidates. 

 L2 Norm Error The L2 norm error is the L2 norm of the difference between the true labels and the predicted distributions. To form the user goals and belief states we concatenate all the slot labels and slot distributions. This error measure does not only consider the accuracy of the predictions but also the uncertainty. 

 Joint Goal Calibration Error A well-calibrated model is one where the accuracy is aligned with the confidence predictions. The expected calibration error (ECE) evaluates the calibration by measuring the difference between the model's confidence and accuracy  (Guo et al., 2017) , meaning a lower ECE indicates better calibration. Hence: ECE = B k=1 b k N |acc(k) ? conf(k)|, ( 6 ) where B is the number of bins, b k are the bin sizes, N the number of observations, acc(k) and conf(k) the accuracy and confidence measures of bin k. We also propose an adapted ECE, called the expected joint goal calibration error (EJCE), which uses the joint goal accuracy for bin k as acc(k), and the following metric as confidence: conf(k) = 1 b k b k i=1 min s?slots max v?values pi (v|s), (7) where pi (v|s) is the predicted probability of value v for slot s given the i th observation in bin k.  All of the calibration techniques presented above can be combined. Here, we focus on the most important combinations and present the results in Table 1. We make the following observations. First, cross entropy on its own leads to a high EJCE, as expected. Second, label smoothing reduces EJCE while leading to a negligible drop in accuracy. Third, Bayesian matching underperformed in our experiments, suggesting a difficulty in choosing the right priors. Fourth, temperature scaling is not an effective way of calibrating uncertainty, as the same calibration is applied to each observation. Finally, the ensemble methods produce very promising results for both accuracy and calibration of the model. In particular, if we look at the Top 3 JGA, our method achieves an improvement of 14.11 percentage points over the baseline, in the Appendix we include a comprehensive set of Top n JGA results. In Figure  1  we plot JGA as a function of confidence. The best calibrated model is the one that is closest to the diagonal, i.e. the one whose confidence for each dialogue state is closest to the achieved accuracy. From this reliability diagram we see that both the dropout and model ensembles improve model calibration and do not produce over-confident output as the cross entropy baseline does. In Table  2  we compare our model to some of the best performing belief and state tracking models. Here we see that we outperform the best performing belief tracker but the state-of-the-art (SOTA) state trackers  (Heck et al., 2020; Chen et al., 2020; Hosseini-Asl et al., 2020)  have a significantly higher JGA. However, when analysing the L2 norm 2 we see that the uncertainty estimates of belief tracking models compensate for the lower joint goal accuracy. This corroborates our premise that it is important to have well calibrated confidence estimates and not just a high JGA. 

 Results 

 Model 

 Conclusion We applied a number of calibration techniques to a baseline dialogue belief tracker. We showed that a label smoothed trained ensemble provides stateof-the-art calibration of the belief state distributions and has the best accuracy among the available belief trackers. Although it does not compete with state trackers in terms of JGA, when considering top 3 predictions it achieves 84.08% accuracy (Top 3 JGA), almost 30 percentage points above state-of-the art state trackers. We also find that our model has the best L2 norm performance, which suggests that the quality of predicted uncertainty is as important as the average JGA. It is important to note that the proposed calibration methods can be applied to any neural dialogue belief tracking method. The uncertainty estimates predicted by this model could improve the success of dialogue systems because this model can provide the dialogue manager with a good measure of confidence. This could allow the system to ask questions in moments of confusion. In the Appendix we include example dialogues to illustrate this. In future, we aim to combine the state-of-the-art dialogue state tracking and belief tracking methods to create a method that can achieve both states-of-theart joint goal accuracy and well-calibrated belief states. 
