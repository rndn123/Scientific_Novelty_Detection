title
Knowledge-Grounded Dialogue Generation with Pre-trained Language Models

abstract
We study knowledge-grounded dialogue generation with pre-trained language models. To leverage the redundant external knowledge under capacity constraint, we propose equipping response generation defined by a pretrained language model with a knowledge selection module, and an unsupervised approach to jointly optimizing knowledge selection and response generation with unlabeled dialogues. Empirical results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods in both automatic evaluation and human judgment.

Introduction With advances in neural machine learning  (Sutskever et al., 2014; Gehring et al., 2017; Vaswani et al., 2017)  and availability of the huge amount of human conversations on social media  (Adiwardana et al., 2020) , building an open domain dialogue system with data-driven approaches has attracted increasing attention from the community of artificial intelligence and natural language processing. In this work, we are interested in generative approaches. Generative models for open domain dialogues are notorious for replying with generic and bland responses, resulting in meaningless and boring conversations  (Li et al., 2015) . Such deficiency is particularly severe when human participants attempt to dive into specific topics in conversation  (Dinan et al., 2019) . As a result, there is still a big gap between conversation with existing systems and conversation with humans. Very recently, there emerge two lines of research that seem promising to bridge the gap. One is to apply large-scale pre-trained language models, such as GPT-2  (Radford et al., 2019)  These adventures went on but were short lived and six feature films. DialoGPT I think it's worth it. Table  1 : An example from the test set (Test Seen) of Wizard of Wikipedia  (Dinan et al., 2019)  . such as DialoGPT  (Zhang et al., 2019c)  have exhibited compelling performance on generating responses that make sense under conversation contexts and at the same time carry specific content for keeping the conversation going. While the giant language models can memorize enough patterns in language during pre-training, they only capture "average" semantics of the data  (Zhang et al., 2019c) . As a result, responses could still be bland or inappropriate when specific knowledge is required, as illustrated by the example in Table 1. The other line is to ground dialogue generation by extra knowledge such as unstructured documents  (Zhao et al., 2020) . By the means, the documents (e.g., wiki articles) serve as content sources, and make a dialogue system knowledgeable regarding to a variety of concepts in discussion. However, collecting enough dialogues that are naturally grounded on documents for model training is not trivial. Although some benchmarks built upon crowd-sourcing have been released by recent papers  (Zhou et al., 2018b; Dinan et al., 2019; Gopalakrishnan et al., 2019) , the small training size makes the generation models generalize badly on unseen topics  (Dinan et al., 2019)  and the cost of building such data also prevents from transferring the techniques proved on the benchmarks to new domains and new languages. Encouraged by the results on pre-training for dialogue generation and knowledge-grounded dialogue generation, and motivated by the problems in both sides, we consider bringing the two together in this work. Specifically, we propose knowledgegrounded dialogue generation with pre-trained language models in order to endow a generative model with both rich knowledge and good generalization ability 1 . The challenge is that pre-trained language models often set constraints on the maximum number of tokens they can handle (e.g., the maximum number for GPT-2  (Radford et al., 2019)  is 1024), and thus hinders exploitation of the knowledge text which could be rather long and redundant (e.g., in Wizard of Wikipedia  (Dinan et al., 2019) , on average each conversation context is associated with 61.2 sentences retrieved from wiki articles, and the average number of tokens in the extra knowledge is 1625.6). Indeed, the conflict between model capacity and the ability required for processing long knowledge input represents an essential obstacle for applying pre-trained language models to knowledge-grounded dialogue generation, since on the one hand we always have to set up an upper bound to the capacity of pre-trained models in order to handle massive text corpus, and on the other hand we need to keep sufficient candidates with rich enough content in the procedure of response generation in order to guarantee the recall of relevant knowledge. To overcome the challenge, we consider equipping the pre-trained response generation model with a knowledge selection module whereby the redundant knowledge input is slimmed with relevant information (regarding to conversation contexts) kept to meet the capacity constraint. While some recent papers on knowledge-grounded dialogues have paid attention to the problem of knowledge selection  (Lian et al., 2019; Kim et al., 2020; Ren et al., 2019) , the knowledge selection module is either deeply coupled with the specially configured models  (Lian et al., 2019; Ren et al., 2019)  and thus is incompatible with the pre-trained language models, or it is learned with human annotations  (Dinan et al., 2019; Kim et al., 2018)  which are difficult to obtain in practice (e.g., the dataset in  (Zhou et al., 2018b)  does not contain annotations for knowledge selection). Therefore, we propose an unsupervised approach where learning of knowledge selection and fine-tuning of response generation are jointly conducted with unlabeled dialogues. Specifically, we build the knowledge selection module on the basis of BERT, and formalize knowledge selection as a sequence prediction process, by which the model can take advantage of the pre-training techniques and dynamically determine the relevant knowledge for a given context. The learning algorithm starts from training with pseudo ground-truth that is constructed by making full use of responses as an alternation of human annotations, and then alternatively updates the knowledge selection model and the response generation model through a reinforcement learning approach and a curriculum learning approach respectively. Thus, knowledge selection is further optimized with the feedback from response generation, and the knowledge used for fine-tuning the response generation model gradually moves from the pseudo ground-truth to the prediction of the knowledge selection module. We test the proposed method on two benchmarks of knowledge-grounded dialogue generation: Wizard of Wikipedia  (Dinan et al., 2019)  and CMU Document Grounded Conversations  (Zhou et al., 2018b) . Evaluation results indicate that our model can significantly outperform state-of-the-art methods as well as a few pre-trained models used in heuristic ways, and thus achieves new state-of-theart on the benchmarks. Moreover, as a byproduct, the knowledge selection module also outperforms the state-of-the-art model in terms of accuracy of knowledge selection on Wizard of Wikipedia, implying that other models could also benefit from the component. Our contributions in this paper are three-fold: (1) proposal of a knowledge selection module for applying pre-trained language models to the task of knowledge-grounded dialogue generation; (2) proposal of an unsupervised approach in which learning of knowledge selection and fine-tuning of the pre-trained model are conducted in a joint manner; and (3) empirical verification of the effectiveness of the proposed method on benchmarks of knowledge-grounded dialogue generation. 

 Related Work Early work on end-to-end open domain dialogue generation is inspired by the research of machine translation  (Ritter et al., 2011; Shang et al., 2015; Vinyals and Le, 2015) . Later, the vanilla encoderdecoder architecture is widely extended to improve diversity of responses  (Li et al., 2015; Xing et al., 2017a; Zhao et al., 2017; Tao et al., 2018) ; to model the structure of conversation contexts  (Serban et al., , 2017 Xing et al., 2017b; Zhang et al., 2019a) ; to control attributes of responses  Zhou et al., 2017; Zhang et al., 2018a; Wang et al., 2018; See et al., 2019) ; and to bias responses to some specific personas  (Li et al., 2016; Zhang et al., 2018b) . Recently, grounding dialogue generation by extra knowledge is emerging as an important step towards human-like conversational AI, where the knowledge could be obtained from knowledge graphs  (Zhou et al., 2018a; Moon et al., 2019; Tuan et al., 2019) , retrieved from unstructured documents  (Dinan et al., 2019; Lian et al., 2019; Zhao et al., 2020; Kim et al., 2020) , or extracted from visual background  (Mostafazadeh et al., 2017; Shuster et al., 2018; Huber et al., 2018) . In this work, we study document-grounded dialogue generation. Rather than learning from scratch like most existing work, we take advantage of the pre-trained language models and achieve new stateof-the-art on the benchmarks of the task. Big, deep neural language models pre-trained on huge unlabeled text corpus have led to strong improvements on numerous natural language understanding and natural language generation benchmarks  (Devlin et al., 2018; Radford et al., 2019; Song et al., 2019; Dong et al., 2019; , and therefore are revolutionizing almost the full spectrum of NLP applications  (Raffel et al., 2019; Sun et al., 2019b; Qiao et al., 2019; Zhang et al., 2019b; Lample and Conneau, 2019)  and some interdisciplinary applications in NLP and computer vision  Su et al., 2019; Sun et al., 2019a) . In the context of dialogue generation, by fine-tuning GPT-2  (Radford et al., 2019)  in different sizes on social media data, recent work has  (Zhang et al., 2019c; Wolf et al., 2019)  shown promising progress on conversation engagement and commonsense questionanswering. In this work, we further explore the application of pre-training to the task of open domain dialogue generation by equipping the pre-trained language models with external knowledge. Differ-ent from a very recent paper on pre-training for low-resource knowledge-grounded dialogue generation  (Zhao et al., 2020) , the work presents an in-depth investigation on how to release the power of the existing pre-trained language models on the task when input exceeds the capacity of the models. 

 Preliminary 

 Problem Formalization Suppose that we have a dataset D = {(U i , D i , r i )} N i=1 , where ?i ? {1, . . . , N }, U i is a dialogue context, D i is a document that contains relevant knowledge regarding to U i , and r i is a response to U i based on D i . The goal is to learn a generation model P (r|U, D; ?) (? denotes the parameters of the model) from D, and thus given a new dialogue context U associated with a document D, one can generate a response r following P (r|U, D; ?). 

 Pre-trained Language Models We define P (r|U, D; ?) on the basis of GPT-2 from OpenAI  (Radford et al., 2019) . GPT-2 are transformer language models with a stack of masked multi-head self-attention layers, and are learned from large scale web text. To apply GPT-2 to the task of knowledge-grounded dialogue generation, we formulate the generation problem as P (r|U, D; ?) = P (r|g(U, D); ?) = lr t=1 P (r t |g(U, D), r 1:t?1 ; ?), (1) where g(U, D) tailors U ? D to meet the length constraint of a GPT-2 model as the input of generation, and r t refers to the t-th token of r whose length is supposed to be l r . The problem then boils down to (1) how to define g(U, D); and (2) how to fine-tune ? (and probably learn g(U, D)) with D. In this work, we assume that labels that indicate the ground-truth knowledge are not available, which is practical but makes the problem even more challenging. Since D could be rather redundant with a lot of information irrelevant with the topic or the context of the conversation, simply truncating the concatenation of sentences of U and D as g(U, D) may cut the relevant knowledge and introduce noise into response generation, which hurts the performance of the GPT-2 model, as will be demonstrated in the experiments. Therefore, we consider learning a g(U, D) that can distill useful information from D for the GPT-2 model, as will be elaborated in the next section. 

 Approach Heading for learning a g(U, D) for applying GPT-2 to the task of knowledge-grounded dialogue generation, we need to deal with several challenges: (1) how to model the correlation between a context and the external knowledge; (2) how to learn g(U, D) when labels of ground-truth knowledge are absent; and (3) how to jointly optimize g(U, D) and the GPT-2 model with D, and thus the two can boost each other. Figure  1  illustrates the architecture of the model. On the basis of the transformer architecture, the knowledge selection module is made up of a context-aware knowledge encoder and a sequential knowledge selector. The former captures interaction patterns between a context U and each sentence in D through a stack of selfattention layers, and the patterns are then fed to the latter to decode useful knowledge one sentence per step. Since human annotations are not accessible, the learning method begins with pseudo groundtruth constructed by making full use of responses, and optimization of g(U, D) and optimization of the GPT-2 generation model are alternatively conducted with a reinforcement learning approach and a curriculum learning approach respectively. 

 Context-Aware Knowledge Encoder We choose BERT  (Devlin et al., 2018)  as the backbone of the encoder. Thus, the encoder can take advantage of pre-training, and the multi-layer bidirectional attention mechanism in BERT allows a dialogue context and the associated knowledge to sufficiently interact with each other, resulting in context-aware knowledge representations. Specifically, let U = (u 1 , . . . , u n ) and D = (d 1 , . . . , d m ) be the context and the knowledge respectively, then we concatenate {u i } n i=1 as (w u 1 , ? ? ? , w u lu ) with w u i the i-th word and l u the length of the sequence, and define the input of the encoder as S = (S 1 , . . . , S m ) with S i formulated as S i = [CLS]w u 1 . . .w u lu [SEP]w d i,1 . . .w d i,j . . .w d i,l d [SEP], (2) where w d i,j refers to the j-th word of d i ? D, and l d is the length of d i . Each S i ? S passes through the stacked self-attention layers, and is finally represented as e i = CLS(BERT(S i )) where BERT(S i ) refers to the sequence of vectors from the last layer of the encoder and CLS(?) is a function that returns the first vector of the sequence (i.e., the vector corresponding to the [CLS] token). The output of the encoder is given by E = (e 1 , . . . , e m ). 

 Sequential Knowledge Selector With E as input, the sequential knowledge selector determines a subset of D (denoted as D ) as the relevant knowledge and exploits D to construct g(U, D). Since there may exist one-to-many relations between a context and the relevant knowledge  (Kim et al., 2020) , the size of D could vary from context to context. Therefore, we regard the construction of D as a sequence prediction process in which D starts from an empty set and gradually expands by adding one sentence from D per step. By this means, the size of D can also be viewed as a parameter and is dynamically determined according to the given context. Formally, we maintain a sequence of hidden states {s t } T U,D t=0 with the initial state s 0 a trainable parameter, and weight {d i } m i=1 by an attention mechanism which can be formulated as P (d i |U, d j 1:t?1 ) = exp(? t,i )/ i exp(? t,i ) ? t,i = v tanh(W e e i + W s s t + b), (3) where W e , W s , b and v are trainable parameters. Then d jt will be added to D if j t = argmax i?{1,...,m} P (d i |U, d j 1:t?1 ). After that, s t+1 is calculated by s t+1 = LSTM(e jt , s t ) (4) To determine T U,D , we introduce a special embedding e spe into E, and terminate the prediction process if e spe is selected or an upper bound T max is reached. Finally, g(U, D) is defined as the concatenation of the sentences in U ? D . 

 Learning Method Learning a g(U, D) without human annotations is not trivial. For example, in a recent paper  (Kim et al., 2020) , when human labels are removed, the accuracy of knowledge selection drops from 27% to 0.3%. Moreover, since knowledge selection and response generation are entangled, ideally we hope g(U, D) and the GPT-2 model can enhance each other in learning. However, as the parameters of g(U, D) are far from optimal at the early stage, it is very possible that noise from g(U, D) will be fed to the GPT-2 model and then flows back to the learning procedure of g(U, D), resulting in inferior models on both sides. To cope with the challenges, we propose a joint optimization strategy with weak supervision as follows. The learning algorithm is summarized in Algorithm 1. Pseudo Ground-Truth Construction. To alleviate error accumulation in joint optimization, we consider constructing weak supervision and utilize the signals to warm up the learning of g(U, D) and the fine-tuning of GPT-2 beforehand. The intuition is that responses from humans carry clues to relevance of the knowledge candidates, and thus can be used to construct pseudo ground-truth. To be specific, we first sort D = {d t } m t=1 in a descending order as {d jt } m t=1 according to {Sim(d t , r)} m t=1 where Sim(?, ?) denotes a similarity function, and then build a subset of D by D = {d j 1 , . . . , d j m }, m = argmax t (Sim(d j 1:t , r)), (5) where d j 1:t refers to the concatenation of {d j i } t i=1 . With D, g(U, D) and the GPT-2 model are optimized via maximum likelihood estimation (MLE) on D K = {(U i , D i , Di )} N i=1 and D G = {(U i , Di , r i )} N i=1 respectively. Joint Optimization: the Reinforcement Step. We exploit the policy-gradient method  (Sutton et al., 2000)  to continue-train g(U, D) by which g(U, D) is further "supervised" by the GPT-2 model and is directly optimized for a target metric (e.g., F1 in the experiments). Specifically, we sample a D according to P (d i |U, d j 1:t?1 ) (in Eq.3.) under a termination criterion similar to D at each time step, and define the loss function as L K = ? 1 N N i=1 ? ? ? Ri | Di| t=1 log P (d i,jt |U i , d i,j 1:t?1 ) ? ? ? , Ri = R( Di ) ? b, (6) where R( Di ) = Sim(r i , r i ) with r i the response generated by the GPT-2 model given U i and Di , and b = N i=1 R( Di )/N is the baseline that is used to reduce the variance of gradient estimation  (Clark and Manning, 2016) . We can see that minimizing L K is equivalent to maximizing the conditional likelihood of Di if it obtains a higher reward than the baseline. Joint Optimization: the Curriculum Step. Though g(U, D) has been pre-trained with the pseudo ground-truth D, the relevant knowledge provided by the model (i.e., D ) may still be worse than D at the beginning of fine-tuning. Therefore, we mix D and D and exploit a curriculum learning strategy to fine-tune the GPT-2 model where D and D are regarded as hard materials and easy materials respectively and fine-tuning gradually moves from D to D . Formally, the loss function for fine-tuning the GPT-2 model is defined by L G = ? 1 N N i=1 z i lr t=1 log P (r i,t |U i , Di , r i,1:t?1 ) +(1 ? z i ) lr t=1 log P (r i,t |U i , D i , r i,1:t?1 ) , (7) where {z i } are sampled from a Bernoulli distribution parameterized by p. By gradually shrinking p, the generation model will be exposed to more hard materials with the learning procedure going on. Algorithm 1 Optimization Algorithm 1: Input: Training data D, pre-trained GPT-2, initial curriculum rate p0, exponential decay constant ?, maximum step M . 2: Construct DK and DG. 3: Optimize g(U, D) and GPT-2 using MLE on DK and DG respectively. 4: for m ? 1 to M do 5: Sample a mini-batch {(Ui, Di, ri)} from D. 

 6: Update the parameters of g(U, D) based on Eq.6. the Reinforcement Step. 7: Sample {zi} from a Bernoulli distribution parameterized by p, where p = p0e ?m . 8: Update the parameters of the GPT-2 model based on Eq.7. the Curriculum Step. 9: end for 10: return g(U, D) and GPT-2. 

 Experiments We conduct experiments on Wizard of Wikipedia (Wizard) and CMU Document Grounded Conversations (CMU DoG)  (Zhou et al., 2018b) . 

 Datasets and Evaluation Metrics Both datasets are built with crowd-sourcing on Amazon Mechanical Turk, employ Wikipedia as the knowledge base, and are split into training sets, validation sets, and test sets by the data owners. Topics in Wizard cover a wide range (1, 365 in total), and each conversation happens between a wizard who has access to the knowledge about a specific topic and an apprentice who is just eager to learn from the wizard about the topic. The test set is split into two subsets: Test Seen and Test Unseen. Test Seen contains new dialogues with topics appearing in the training set, while topics in Test Unseen never appear in the training set and the validation set. We follow  (Dinan et al., 2019)  and conduct the pre-processing with the code published on Par-lAI 2 . Different from Wizard, CMU DoG focuses on movie domain, and besides wizard-apprentice conversations, the data also contain conversations between two workers who know the document and try to discuss the content in depth. To better compare with the baselines, we adopt the version shared at https://github.com/lizekang/ITDD. In both data, only the turns where knowledge is accessible are considered in response generation. More details are described in supplementary material. We choose perplexity (PPL) of the ground-truth responses, BOW Embedding  (Liu et al., 2016) , and unigram F1  (Dinan et al., 2019)   Besides automatic evaluation, we randomly sample 300 examples from Test Seen, Test Unseen, and the test set of CMU DoG respectively, and recruit 3 well-educated native speakers as annotators for human evaluation. To each annotator, an example is presented with a context, the associated external knowledge 3 , and model responses (top 1 in greedy search) that are randomly shuffled to hide their sources. The annotators then judge the quality of the responses from three aspects, including fluency, context coherence and knowledge relevance, and assign a score in {0, 1, 2} (representing "bad", "fair", and "good") to each response for each aspect. Each response receives 3 scores per aspect, and the agreement among the annotators is measured via Fleiss' kappa  (Fleiss, 1971) . 

 Baselines The following models are selected as baselines: Transformer Memory Network (TMN): the model proposed in  (Dinan et al., 2019)  along with the release of the Wizard data. We implement it using the code shared at https: //github.com/facebookresearch/ParlAI/ blob/master/projects/wizard_of_wikipedia. Incremental Transformer with Deliberation Decoder (ITDD): a transformer-based model  (Li et al., 2019)  that incrementally encodes multi-turn dialogues and knowledge and decodes responses with a deliberation technique. We implement it using the code shared at https://github.com/ lizekang/ITDD. Sequential Knowledge Transformer (SKT): a sequential latent variable model with state-of-theart performance on knowledge selection published in a very recent paper  (Kim et al., 2020)  Models PPL F1 Average Extrema Greedy TMN  (Dinan et al., 2019)  75.2 9.9 0.789 0.399 0.615 ITDD  (Li et al., 2019)  26.0 10.4 0.748 0.390 0.587 DRD  (Zhao et al., 2020)  46  edge are crucial to the performance of the model, we only involve it as a baseline on the Wizard data. The model is implemented with the code shared at https://github.com/bckim92/ sequential-knowledge-transformer. Disentangled Response Decoder (DRD): a model that tackles the low-resource challenge with pre-training techniques  (Zhao et al., 2020) . We choose the one in which all parameters are finetuned with the full training data after pre-training as the baseline, since such a configuration results in state-of-the-art performance on Wizard, as reported in  (Zhao et al., 2020) . We name our model KnowledGPT. Besides the baselines described above, the following pretrained models are also included in comparison in order to have a thorough understanding towards the proposed method: (1) GPT-2 trunc . We concatenate a context and the associated knowledge as a long document, and then truncate the document to meet the length constraint of the GPT-2 model. This is to check if the simple heuristics work for the task. Note that in Wizard, we randomly mix the ground-truth knowledge with others and repeat the procedure 8 times. The means with standard deviation (i.e., numbers in "( )") are reported to remove randomness; and (2) SKT+GPT-2. We feed the candidate selected by SKT to GPT-2 for response generation. This is to examine if we can simply replace the proposed knowledge selection module as well as the learning approach with an off-the-shelf knowledge selection model. Similar to SKT, the comparison is only conducted on Wizard. 

 Implementation Details In both Wizard and CMU DoG, we set the hidden size and the number of layers of the sequential knowledge selector as 256 and 1 respectively. T max for D is set as 1 in Wizard, and 2 in CMU DoG. We choose BERT (110M) and GPT-2 (117M) as the pre-trained language models in KnowledGPT, and implement the models with the code in https://github.com/huggingface/ transformers. We employ greedy search in response decoding. All models are learned with Adam (Kingma and Ba, 2015) optimizer with ? 1 = 0.9 and ? 2 = 0.999. In warming up, we define Sim(?, ?) as unigram F1, and optimize g(U, D) and the GPT-2 model with the pseudo ground-truth for 1000 steps with a batch size of 64. In joint optimization, the batch size is set as 128, and the learning rates for g(U, D) and GPT-2 are set as 5e ? 6 and 5e ? 5 respectively. The learning rate will be halved if there is no improvement in terms of PPL on the validation sets. The parameter p of the Bernoulli distribution in the curriculum step is initially set as 1.0 and anneals with a rate of 1e ? 5. Early stopping on validation is adopted as a regularization strategy. 

 Evaluation Results Table  2  and Table  3  report evaluation results on Wizard and CMU DoG respectively. KnowledGPT achieves new state-of-the-art on most metrics in both datasets, which demonstrates the effectiveness of large-scale pre-trained language models on the task of knowledge-grounded dialogue generation. GPT-2 trunc is worse than KnowledGPT, due to (1) knowledge loss: we find that in 53% test examples (Test Seen+Test Unseen), the groundtruth knowledge is cut. In this case, GPT-2 trunc only relies on the context, the related knowledge in other candidates (thanks to the one-to-many relations between a context and knowledge), and the knowledge packed in the parameters of GPT-2 for responding, which explains the comparable per-   4  shows human evaluation results. While the three models are comparable on fluency, Knowl-edGPT is superior to the others on both context coherence and knowledge relevance, which is consistent with the results on automatic metrics. All kappa values are no less than 0.6, indicating substantial agreement among the annotators. We present a case study in supplementary material. 

 Discussions Ablation study. To understand the impact of the learning strategies on model performance, we compare the full KnowledGPT with the following variants: (1) -pseudo: the warming up stage is removed; (2) -joint: the joint optimization stage is removed; (3) -reinforcement: g(U, D) is fixed after it is optimized with MLE on D K ; and (4) -curriculum: GPT-2 is fixed after it is optimized with MLE on D G . Table  5  reports the evaluation results. We can conclude that (1) the pseudo ground-truth plays a crucial role in Wizard, as removing the step causes dramatic performance drop. This is because in Wizard, there is a strong correlation between the knowledge and human responses. The results indicate that though the pseudo ground-truth is constructed with heuristics, it still contains valuable information and thus allows the following joint optimization to start from a good point. On the other hand, in CMU DoG, the crowd-workers do not refer to the external knowledge as much as those workers do in Wizard when they form the responses; (2) the reinforcement step and curriculum step are useful because the reinforcement step allows the knowledge selection module to make better use of GPT-2's feedback, and through the curriculum step GPT-2 can take advantage of the output of knowledge selection module progressively; (3) joint optimization is meaningful, as removing this stage results in performance drop. Impact of T max (i.e., the upper bound in knowledge selection). Besides the learning strategies, we are also curious about how T max , as part of the termination criterion in knowledge selection described at the end of Section 4.2, influences the performance of KnowledGPT. To this end, we vary the value of T max in {1, 2, 3} and report the evaluation results in Table  6 . The larger T max is, the more chances KnowledGPT has to involve the groundtruth candidate into generation, and the lower PPL is. This also explains why the PPL of GPT-2 trunc is lower than that of KnowledGPT in Table  2 and  Table 3 . On the other hand, a larger T max also means more noise in generation. That is why when T max exceeds a value, F1 begins to drop. 

 Conclusions We apply large-scaled pre-trained language models to the task of knowledge-grounded dialogue generation. To this end, we devise a knowledge selection module, and propose an unsupervised approach to jointly optimizing knowledge selection and response generation. Evaluation results on two benchmarks indicate that our model can significantly outperform state-of-the-art methods. Table  8  shows the results, indicating that external knowledge is necessary even though one has exploited a powerful pre-trained language model for dialogue generation. In CMU DoG the gap between DialoGPT and KnowledGPT is narrowed because about 35% of the conversation has a weak correlation with the document (e.g. BLEU < 0.1).  

 A Details of Datasets 

 C Impact of Maximum Tokens of GPT-2 To further justify our claims on why GPT-2 trunc is worse than KnowledGPT, we keep the groundtruth knowledge in the input sequence of GPT-2 and gradually increase the constraint of the maximum number of tokens on Wizard. As the maximum token limit increases, more irrelevant knowledge is introduced. Note that in practice, one has no way to perfectly locate the ground-truth, and this experiment is only to provide more insights to GPT-2 trunc . Table  9  shows the performance of GPT-2 trunc with the increase of the maximum  number of tokens where Ground-truth Percentage indicates the percentage of ground-truth in the input knowledge. First, when the ground-truth is forced to be kept, GPT-2 trunc is always better than the one where the ground-truth is randomly mixed with other candidates and bears the risk to be cut. This echoes our claim that knowledge loss is one of the reasons for the poor performance of GPT-2 trunc used with the practical setting. Second, even if ground-truth is retained, once more noise is introduced, the performance of GPT-2 trunc will become worse. When the length is limited to 128 tokens, the PPL of the model is not good, mainly because under this limitation, the input sequence of some cases only contains the dialogue context and response. 

 D Impact of the Size of GPT-2 We further check if the performance of Knowl-edGPT can be further improved when the GPT-2 model is replaced with a larger one. Table  10  shows the results. Though GPT-2 (345M) can further reduce PPL, it does not bring significant improvement to F1 over GPT-2 (117M), probably because the larger model can not provide more accurate feedback to the knowledge selection module in learning. Therefore, to balance efficacy and cost, GPT-2 (117M) is still favored in practice.  

 E Case Study , to the task of open domain dialogue generation. Prototypes Context A I just discovered star trek and I really like watching star trek . B Gene Roddenberry created it based upon science fiction and it is American media. ... A If I remember Captain Kirk was not the original captain . B The Star Trek Canon of the series an ani- mated had 5 spin offs. A I watched a little of the next generation but could not get into it like i did with the original show . Response Human *Corresponding author: Rui Yan (ruiyan@pku.edu.cn). 

 as metrics, where Embedding-based metrics are computed with an NLG evaluation open source available at https://github.com/Maluuba/nlg-eval, and F1 is calculated with the code published at https: //github.com/facebookresearch/ParlAI/ blob/master/parlai/core/metrics.py. 2 https://github.com/facebookresearch/ ParlAI/blob/master/projects/wizard_of_ wikipedia 

 Table 2 : 2 . Since human labels that indicate ground-truth knowl-Evaluation results on Wizard. Models that leverage human labels are marked with *. Numbers in bold mean that the improvement to the best baseline is statistically significant (t-test with p-value < 0.01). Models PPL F1 Test Seen Average Extrema Greedy PPL F1 Test Unseen Average Extrema Greedy TMN (Dinan et al., 2019) 66.5 15.9 0.844 0.427 0.658 103.6 14.3 0.839 0.408 0.645 ITDD (Li et al., 2019) 17.8 16.2 0.841 0.425 0.654 44.8 11.4 0.826 0.364 0.624 SKT* (Kim et al., 2020) 52.0 19.3 0.846 0.440 0.665 81.4 16.1 0.839 0.418 0.652 DRD (Zhao et al., 2020) 19.4 19.3 0.852 0.452 0.674 23.0 17.9 0.849 0.439 0.664 SKT+GPT-2* 17.6 20.3 0.866 0.460 0.679 23.7 17.8 0.860 0.437 0.664 GPT-2trunc 14.6(2.2) 18.7(0.7) 0.864(0.002) 0.451(0.006) 0.674(0.004) 16.9(3.1) 18.3(0.6) 0.862(0.002) 0.444(0.005) 0.668(0.003) KnowledGPT 19.2 22.0 0.872 0.463 0.682 22.3 20.5 0.870 0.452 0.674 

 Table 3 : 3 Evaluation results on CMU DoG. Numbers in bold mean that the improvement to the best baseline is statistically significant (t-test with p-value < 0.01). 

 Table 4 : 4 Human evaluation results on Wizard and CMU DoG. Models Test Seen Wizard Test Unseen CMU DoG Fluency Context Coherence Knowledge Relevance Kappa Fluency Context Coherence Knowledge Relevance Kappa Fluency Context Coherence Knowledge Relevance Kappa DRD 1.71 1.50 1.26 0.67 1.64 1.44 1.18 0.69 1.58 1.48 1.07 0.60 GPT-2 trunc 1.86 1.54 1.22 0.71 1.84 1.47 1.20 0.59 1.83 1.58 1.06 0.64 KnowledGPT 1.89 1.67 1.71 0.70 1.88 1.60 1.68 0.73 1.83 1.65 1.50 0.77 Models Test Seen Wizard Test Unseen CMU DoG PPL F1 Average Extrema Greedy PPL F1 Average Extrema Greedy PPL F1 Average Extrema Greedy KnowledGPT 19.2 22.0 0.872 0.463 0.682 22.3 20.5 0.870 0.452 0.674 20.6 13.5 0.837 0.437 0.654 -pseudo 22.3 18.3 0.857 0.436 0.662 24.1 17.9 0.854 0.430 0.655 23.2 12.9 0.815 0.440 0.639 -joint 20.0 20.4 0.863 0.457 0.675 21.8 19.5 0.861 0.451 0.669 22.6 11.7 0.806 0.438 0.635 -curriculum 19.4 21.2 0.867 0.457 0.677 21.5 20.3 0.866 0.451 0.672 21.9 12.4 0.816 0.443 0.644 -reinforcement 19.4 21.3 0.866 0.459 0.677 21.9 20.2 0.863 0.449 0.670 20.3 12.6 0.817 0.437 0.643 Table 5: Ablation study on Wizard and CMU DoG formance with SKT and DRD; and (2) noisy in- put: even though the ground-truth knowledge is kept, the redundant and irrelevant information in the knowledge candidates are still harmful. Ev- idence is that GPT-2 trunc is worse than Knowl- edGPT on CMU DoG even though we do not cut anything on the knowledge (the maximum length of the knowledge input is 502, and thus is within the constraint of GPT-2). KnowledGPT also outper- forms SKT+GPT-2 on Wizard, because (1) Knowl- edGPT is more accurate than SKT on knowledge selection, even though it does not leverage any hu- man annotations in learning. In fact, the accuracy scores of knowledge selection for SKT are 26.8 and 18.3 on Test Seen and Test Unseen respectively, while the two numbers are 28.0 and 25.4 respec- tively for KnowledGPT; and (2) in KnowledGPT, knowledge selection and response generation are jointly optimized. Table 

 Table 6 : 6 Performance of KnowledGPT under different T max s. Models Wizard Test Seen Test Unseen CMU DoG PPL F1 PPL F1 PPL F1 T max =1 19.2 22.0 22.3 20.5 20.6 12.6 T max =2 18.2 21.3 21.0 20.3 20.6 13.5 T max =3 17.2 21.1 20.2 20.3 19.7 11.2 

 Table 7 : 7 Table 7 reports the statistics of the Wizard data and the CMU DoG data. Statistics of the two datasets. Wizard of Wikipedia CMU DoG Train Valid Test Seen Test Unseen Train Valid Test # Utterances 166,787 17,715 8,715 8,782 74,717 4,993 13,646 # Conversations 18,430 1,948 965 968 3,373 229 619 # Topics/Documents 1,247 599 533 58 30 30 30 Avg. # of Turns 9.0 9.1 9.0 9.1 22.2 21.8 22.0 B Comparison with DialoGPT We compare KnowledGPT and with DialoGPT in order to learn if a pre-trained generation model with state-of-the-art performance on open domain dialogues is already good enough when it is fine- tuned with knowledge-grounded dialogues. We discard the associated knowledge and fine-tune DialoGPT on the knowledge-grounded dialogues. We choose the model trained from OpenAI GPT-2 with 345M parameters, as it shows the best perfor- mance in the evaluation in the original paper. The model is implemented based on the code shared at https://github.com/microsoft/DialoGPT. 

 Table 8 : 8 Comparison with DialoGPT on Wizard and CMU DoG Models Wizard Test Seen Test Unseen CMU DoG PPL F1 PPL F1 PPL F1 DialoGPT 16.0 17.9 20.0 16.8 16.9 12.3 KnowledGPT 19.2 22.0 22.3 20.5 20.6 13.5 

 Table 9 : 9 Performance of GPT-2 trunc under different maximum tokens with ground-truth knowledge involved. Models Wizard of Wikipedia Test Seen Test Unseen CMUDoG PPL F1 PPL F1 PPL F1 KnowledGPT (117M) 19.2 22.0 22.3 20.5 20.6 13.5 KnowledGPT (345M) 16.1 22.0 17.9 20.6 18.1 13.4 

 Table 10 : 10 Performance of KnowledGPT under different sizes of GPT-2. 

 Table 11 and 11 Table 12 show the examples from Test Seen and Test Unseen of Wizard, each example contains the dialogue context and the background knowledge which is retrieved from Wikipedia given 

			 In this paper, we assume that knowledge is retrieved from documents. 

			 For ease of labeling, only the ground-truth knowledge is shown to the annotators in Wizard.
