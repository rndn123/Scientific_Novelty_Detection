title
Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation

abstract
Social biases present in data are often directly reflected in the predictions of models trained on that data. We analyze gender bias in dialogue data, and examine how this bias is not only replicated, but is also amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets before selecting the most biased one, the multi-player textbased fantasy adventure dataset LIGHT (Urbanek et al., 2019), as a testbed for bias mitigation techniques. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias by balancing the genderedness of generated dialogue utterances, and find that they are particularly effective in combination. We evaluate model performance with a variety of quantitative methods-including the quantity of gendered words, a dialogue safety classifier, and human assessments-all of which show that our models generate less gendered, but equally engaging chit-chat responses.

Introduction Machine learning algorithms learn to model patterns present in training datasets. In particular, they make predictions that directly reflect the harmful societal biases present in training datasets, such as racial bias in sports reports  (Merullo et al., 2019)  and political bias in news data . Such biases are rife in NLP, for example, in learned word embeddings  (Bolukbasi et al., 2016; Brunet et al., 2018; , visual semantic role labeling  (Zhao et al., 2017) , natural language inference  (He et al., 2019) , abusive language classification  (Park et al., 2018)  Table  1 : Counts of gendered words in several dialogue datasets. We report the percent of gendered words (% gend. words) as well as the percentage of male-gendered words out of all gendered words (% male bias). Datasets are arranged in descending order with respect to % male bias. LIGHT has the most % male bias; thus we chose it as our main testbed. coreference resolution  (Zhao et al., 2018a) . Although research into bias in NLP writ large is maturing, bias in dialogue utterances has received somewhat less attention  (Liu et al., 2019; Henderson et al., 2018) . As realworld use-cases for dialogue agents, such as interactive assistants, are rapidly developing, bias in dialogue models has the very real potential to invade downstream systems and exacerbate existing social biases. Thus, dialogue debiasing is becoming an increasingly important problem in NLP. In this work, we foreground dataset bias as a crucial cause of gender bias in dialogue models, and explore ways to address it. Gender bias has been found in many machine learning datasets, in both images and text  (Stock and Ciss?, 2017; Zhao et al., 2017) . Here, we analyze several existing dialogue datasets for gender bias (see Table  1 , and ?3 for more discussion) for the purpose of finding a good testbed for a deeper dive. Our analysis revealed that the dataset from the LIGHT text adventure world  (Urbanek et al., 2019)  was the most biased in our sample. LIGHT is also an interesting dataset for measur-Persona Example (Original LIGHT Dataset) daughter: I spend most of my time doing household chores. I want to find meaning in life. I am energetic and happy. chief wife: I am the king's chief wife. Of all the women that he has married, or who are his concubines, I am the principal one. I represent the kingdom of my father, who is the king's biggest ally. My sons are the ones who will most likely become the king after the death of my husband. women: I live with my husband and 4 children in the village. I spend my days washing clothing and cleaning our home. My husband works for the royal army defending out town. farmer Bob's wife: I am farmer Bob's wife. I like to take care of all our animals. I help Farmer Bob everyday on the farm. mother: I am a mother of eight children. I live with my family in a cottage in the countryside. I spend every day tending to the needs of all of my little ones which can be overwhelming, but I always manage to maintain a pleasing disposition and a happy smile. 

 wife: I am the wife of a farmer. While I may not be the most attractive woman ever, I am loyal and loving. My husband is a good man, but only seems to stay with me out of duty. This room is going to take a while to clean. You might want to come back later. merchant: It is fine I can set my booth up here. 

 wife: With all the foot traffic? merchant: Yes it should be ok. 

 wife: It doesn't appear that anyone ever comes up here! merchant: Well they will when they know I am here. wife: I have my doubts but I'll just go about my cleaning. merchant: Yea sounds like a good idea. 

 wife: What is that supposed to mean? merchant: I am saying we should both do our jobs. wife: Don't take that tone with me! ing and mitigating gender bias for three additional reasons: first, it has multiple potential sources of bias-not just dialogues, but also characters and personas-second, it was crowdsourced, and thus susceptible to reflecting the gender biases known to be present in crowdworkers' annotations  (Otterbacher et al., 2018; Barbosa and Chen, 2019) , and third, LIGHT's medieval, fantasy setting might encourage crowdworkers to impart text with their gender biases. After selecting LIGHT for particular scrutiny, we then explore three bias mitigation techniques, one of which is wholly novel, and another which is novel in its application to dialogue: (i) Counterfactual Data Augmentation (CDA)  (Hall Maudslay et al., 2019; Zmigrod et al., 2019) , (ii) a targeted data collection method, which we refer to as Positive-Bias Data collection, and (iii) Bias Controlled text generation. We show that these techniques are most effective in combination, resulting in dialogue models that produce engaging responses with measurably less gender bias and offensive content (see ?5). Models and code are released at https://parl.ai/projects/genderation_ bias/. 

 Related Work Recently, the NLP community has focused on exploring gender bias in NLP systems  (Sun et al., 2019) , uncovering many gender disparities and harmful biases in algorithms and text (Cao and  Chang and McKeown 2019; Costa-juss? 2019; Du et al. 2019; Emami et al. 2019; Garimella et al. 2019; Gaut et al. 2020; Habash et al. 2019; Hashempour 2019; Hoyle et al. 2019; Lee et al. 2019a; Lepp 2019; Qian 2019; Sharifirad and Matwin 2019; Stanovsky et al. 2019; O'Neil 2016; Blodgett et al. 2020; Nangia et al. 2020) . Particular attention has been paid to uncovering, analyzing, and removing gender biases in word embeddings  (Basta et al., 2019; Kaneko and Bollegala, 2019; Zhao et al., , 2018b Bolukbasi et al., 2016) . This word embedding work has even extended to multilingual work on gender-marking  Williams et al., 2019; Zhou et al., 2019; . Despite these efforts, many methods for debiasing embeddings have only succeeded in hiding word embedding biases as opposed to removing them -making gender debiasing still an open area of research. Despite the relatively ample literature on gender debiasing for word-level representations, very little work has focused on sentence representations  (Liang et al., 2020; Liu et al., 2019; Lee et al., 2019b) . Until this point, most debiasing work on sentences mainly focus on measuring bias  (Lee et al., 2019b; . Very few foreground the contribution of training data to gender bias in model outputs. For example, Kang et al. collect a corpus of text that is parallel across multiple stylistic categories, one of which is gender. Closer to our work, Liu et al. present a test dataset for dialogue and find that models can produce less diverse dialogues when prompted with sentences containing words describing individuals from underrepresented groups. Still, it differs from our work in that the data was created by combining templates and hand-created lists of wordpairs, rather than using real dialogue data. Liu et al. also proposes two methods for debiasing, one of which we also employ (i.e., CDA), and the other of which extends to sentences a word-embedding post-processing method  (Bolukbasi et al., 2016)  that has been shown to be ineffective at removing gender bias (Gonen and Goldberg 2019, but see  for a more recent, perhaps more effective attempt). Finally-and as a direct extension of this work-  Dinan et al. (2020)  decomposes gender bias along three semantic-pragmatic dimensions, and show that train more fine-grained classifiers allow for more accurate classification of dataset gender biases. The novelty of the present contribution lies in how we measure bias, and in the joint application of our three gender debiasing methods. 

 Measuring Bias Before one can mitigate bias, one must first measure it. As a first pass, we measured the counts of gendered words used (using a word list from  Zhao et al. 2018b) , and the percent of those which referred to male characters for six datasets (Table 1). We count the number of male and female gendered words in the training sets of several datasets  (LIGHT, ConvAI2, Reddit, Wizard of Wikipedia, Daily Dialog, Empathetic Dialogues, and ConvAI2) . We use this to calculate the percentage of gendered words out of all words, and the % male bias, that is the percentage of male gendered words among all gendered words in a dialogue. We find that LIGHT is the most gender imbalanced dataset among all datasets in this table, with a % male bias of 73%, although others, like Reddit, are close behind. Since LIGHT was found to be the most gender biased, we qualitatively examine it more closely, and find many biased utterances present in the training data. For example, the queen persona adheres to negatively stereotyped gender roles when uttering the line I spend my days doing embroidery and having a talk with the ladies. Another character admires a sultry wench with fire in her eyes. We conclude from examples like this that presenting crowdworkers with gender biased personas often leads them to create even more gender biased dialogues (see Table  3 ): for example, a wife persona contains the text I spend my days cooking and cleaning so my husband will have something to eat when he returns from his work..., and, in dialogue with a merchant, discusses only her cleaning duties. The merchant even derisively refers to cleaning as the wife's job. This could be an effect of gender stereotype priming  (Blair and Banaji, 1996; Steele and Ambady, 2006; Oswald, 2008; Derks et al., 2011; Verhaeghen et al., 2011) . Given this, we wonder how much biased character names and personas themselves lead to LIGHT dialogues being more biased than the others. Thus, we focus on persona-based dialogue text in particular for the remainder of the paper. Dialogue research has found that, while incorporating personas increases engagingness and improves consistency  (Zhang et al., 2018; Shuster et al., 2018; Mazar? et al., 2018; Olabiyi et al., 2018; Li et al., 2016b) , they can also crystallize gender bias  (Clark et al., 2019; Henderson et al., 2018) . Such bias propagates to subsequently generated conversations. Crowdworkers in particular might imbue their annotations with their particular gender biases at every stage of dataset creation. For example, LIGHT  (Urbanek et al., 2019)  was created by crowdworkers in stages: crowdworkers were first assigned a character (with previously crowdsourced names such as "farmer" or "witch"), as well as a previously crowdsourced persona, or short textual description of the char-  acter. Then, they were paired up, and tasked with generating a dialogue as those characters. To determine with more granularity precisely how bias manifests in persona-based dialogue datasets, we investigate the text for (i) characters such as fisherman (Table  1 ), and (ii) personas such as I love fishing (Table  2 ). We ask: (i) do crowdworkers generate male and female characters at an equal rate, (ii) do they imbue characters' personas with sexism or undesirable gender biases? Bias in Number of Characters. We first determine whether crowdworkers create an equal number of male and female characters. To quantify this, we asked annotators on Amazon Mechanical Turk to label the gender of each character name based on its persona description (choosing neutral if the gender was not explicit). This annotation is possible because many personas include text such as I am a young woman. 1 Since this measurement requires personas, we consider the two personabased dialogue datasets in our sample: LIGHT and ConvAI2  (Zhang et al., 2018) . LIGHT is highly gender imbalanced: there are over 1.6 times as many male characters as female ones 2 . LIGHT is also considerably less gender-balanced than Conv-AI2, which has a nearly equal number of male and female gendered personas (see Table  4 ). Bias in Personas. In addition to the stark underrepresentation of female characters, the medieval setting in LIGHT is likely to encourage crowdworkers to generate dialogues accentuating historical biases and inequalities of the time period  (Bowman, 2010; Garcia, 2017) . We investigate the number of references to men or women in the text of personas, as another source of bias. Take for example, a female persona that contains a gendered reference such as I want to follow in my father's footsteps rather than in my mother's. Although using gendered relational nouns  (Barker, 1992; Williams, 2018) , such as father, doesn't always signal sexism, if female characters are predominantly defined in reference to male characters, it becomes a problem. We count the appearance of gendered words in personas using the list compiled by  Zhao et al. (2018b) , and find that men are disproportionately referred to in the personas: there are nearly 3x as many mentions of men than women, which suggests that a large number of characters are defined by their relationships to men (see Table  2  for examples, and Table  4  for counts). Gender bias and sexism are clearly present in many dialogue datasets  (Henderson et al., 2018) , but finding a clear way to define these terms (and others that categorize unsafe text), let alone measure their effects at scale, is very challenging. For example, the persona for the character girl contains the line I regularly clean and cook dinner (see Table  2  for more examples), which strikes us as stereotypical and sexist, but it might not be noticed by others. In this paper, we rely on each annotator's own, subjective, definition(s) of the term but aggregate multiple opinions. Three na?ve annotators examined each persona for unsafe content. If annotators detected content was 'offensive' or 'maybe offensive', they were asked to select one of four categories-racist, sexist, classist, otherand to provide a reason for their response. Just over 2% of personas were flagged by at least one annotator, and these personas and their resulting dialogues were removed. 

 Mitigating Bias in Generative Dialogue In this section, we present a general framework for mitigating bias in generative dialogue. More specifically, we explore data augmentation and other algorithmic methods to mitigate bias in generative Transformer models. We (i) extend counterfactual data augmentation to dialogue We split test set across the four genderedness bins: F 0/+ M 0/+ . X 0 indicates there are no X-gendered words in the gold response, while X + indicates that there is at least one. We measure the percent of gendered words generated in the dialogue (% gend. words) and the percent of male bias (% male bias), i.e. the percent of malegendered words out of all generated gendered words. While each of these methods yield some improvement, combining them yields the best control over the genderedness of the utterances while improving the F1-score. The orange outline represents the best performing model. For % Gendered words, lower is better. For % Male Bias, closer to 50 is better. For F1 Score, higher is better.  (Hall Maudslay et al., 2019; Zmigrod et al., 2019)  following  (Liu et al., 2019) , (ii) perform positive data collection by augmenting the existing dataset via targeted data collection with crowdworkers, and lastly, (iii) apply controllable generation techniques to gender bias to control how many male and female gendered words models produce. 

 Counterfactual Data Augmentation A straightforward solution for gender bias in embeddings is Counterfactual Data Augmentation (CDA)  (Hall Maudslay et al., 2019; Zmigrod et al., 2019; Liu et al., 2019) . CDA swaps, say, all instances of grandmother with grandfather, she with he, etc. We apply this word-based augmentation to dialogue by first copying every dialogue, then swapping all gendered words with their counterpart from the paired list in  Zhao et al. (2018b) . The augmentation is limited to words on the list, and the swapping is performed automatically. The model is then retrained on the augmented data. While CDA is somewhat effective strategy for mitigating bias in word embeddings, this method has several pitfalls: it may result in ungrammatical sentences, and it relies on existing (and perhaps incomplete) lists to determine and swap gender. 

 Positive-Bias Data Collection To resolve the issues with CDA, we use humans to collect additional dialogue data via a two-pronged Positive-Bias Data Collection (Pos. Data) strategy. We first collect additional personas by having humans (i) manually swap the gender of the character name and all gendered references in the character's persona text (rather than relying on brittle word lists) and (ii) write additional, diversified personas. We then use these personas to seed the collection of additional, positively biased dialogue data, which we refer to as Pos. Data throughout. New Characters & Personas. When a dataset contains more male characters and references to male characters than it contains female characters and references to female characters (see Table  4 ), we balance existing characters and personas with gender swapping. For every gendered characterpersona pairing, annotators create a new oppositegendered character-persona pairing for which animate nouns or pronouns are changed, but the rest of the persona remains unchanged. For example, for every persona describing a male character like a king, annotators will create a new one describing a female character like a queen. Annotators are instructed to swap the gender(s) of other animate references in the text (e.g., if an original persona describes a woman in relation to her father, the new male persona will describe a man in relation to his mother). This method ensures that the created sentences will be grammatical, unlike heuristic data augmentation. However, simply balancing references to men and women is insufficient, as female characters might be specifically described in sexist ways (see ?3). As detecting sexism is challenging (also see ?3), we take our qualitative analysis to be sufficient motivation, and moved to further offset the bias by collecting a new set of interesting and independent female characters. We primed workers by showing examples of gender underspecified character names like adventurer with personas like I am a woman passionate about exploring a world I have not yet seen. I embark on ambitious adventures. We also provided crowdworkers with additional instruction to encourage them to create diverse characters: We're looking for strong and diverse descriptions. Avoid descriptions that could be considered hateful, offensive, or stereotypical. Even with explicit instruction, annotators created 3 times as many male characters as female characters, revealing the stubbornness of the inherent gender biases of the available crowdworker pool. We ultimately exclude all male-gendered personas created in this fashion from the new dataset, as including them would worsen the gender balance of the dataset. Our new dataset is approximately balanced then in the number of male or female characters and in the number of references to male or female characters (see Table  4 ). In total, we add 2,629 new characters and release the data for optional inclusion in the LIGHT dataset. New Dialogues. After gender-balancing the personas, we moved on to using the gender-balanced personas to crowdsource additional, hopefully gender-balanced, dialogues. We selected more female-gendered characters for new dialogue collection, and explicitly instructed annotators to be mindful of gender bias. In particular, we encouraged them to assume equality-social, economic, political, or otherwise-between genders (Note: this is uniquely possible with a dataset like LIGHT, which is situated in a fully fictional world). We collected a total of 507 new dialogues containing 6,658 utterances (approximately 6% of the original dataset size). We refer to this additional dialogue data as Pos. Data. 

 Bias Controlled Training Gender bias in dialogue can take the form of imbalanced use of gendered words. To create dialogue models that can generate an equal number of gendered words, we control model output with We apply conditional training techniques to control gender bias in generative dialogue by learning to associate control tokens with properties of gender bias. Any general function that takes as input a dialogue utterance and outputs a continuous or discrete value that provides information about gender bias could be used as a control variable. In our case, prior to training, each dialogue response is binned into one of four bins-F 0/+ M 0/+ -where X 0 indicates that there are zero X-gendered words in the response. X + indicates the presence of one or more X-gendered word. The percentage of test set examples that fall into each bin is in Table  5 . Nouns and adjectives are binned into gendered bins via an aggregation of existing gendered word lists  (Zhao et al., 2018b,a; Hoyle et al., 2019) . Note that other functions could be used as well, such as a bias classifier  (Dinan et al., 2020) . F 0 M 0 F 0 M + F + M 0 F + M + % of We append a special token to the input that indicates which bin the response falls into. During Bias Ctrl training, the model should learn to associate the special token with the genderedness of the dialogue response, such that at inference time, we could append different special tokens to control the genderedness of the model output. For example, a model trained with multiple gender control bins could be set to the gender neutral (in this case, F 0 M 0 ) setting at inference time, to produce a response containing few (or no) gendered words. 

 Implementation Details Following  Urbanek et al. (2019) , we fine-tune a large, pre-trained Transformer encoder-decoder on the dialogues in the LIGHT dataset for all generation experiments. Following , we pre-trained on Reddit conversations extracted and obtained by a third party, and made avail- able on pushshift.io. During pre-training, models learned to generate a comment conditioned on the preceding conversation thread. All comments that contained URLs or were shorter than 5 characters long were removed, along with child comments, resulting in approximately 2.2 billion training examples. Similarly during fine-tuning, models were conditioned on the full preceding dialogue history. All models are 8-layer encoders, 8-layer decoders, with 512 dimensional embeddings and 16 attention heads based on the ParlAI transformer implementation  (Miller et al., 2017) . We decode with a beam search size of 5. 

 Results We train five Transformer models: one baseline trained only on original LIGHT without any mitigation techniques, one Transformer for each of our three methods (see ?4.1 for CDA, ?4.2 for Positive-Bias Data Collection, and ?4.3 for Bias Control), and a final one combining all three methods (ALL) that achieves the best results. Bias is Amplified in Generation. Figure  1  compares the performance of the various techniques. We compare our methods to the gold labels from the test set and to the baseline. To do this, we divide the test set into four genderedness bins (as defined in ?4.3)-F 0 M 0 , F 0 M + , F + M 0 , and F + M + -and calculate: (i) the F1 word overlap with the gold response, (ii) the percentage of gendered words generated (% gend. words), and (iii) the percentage of male-gendered words generated (relative to the sum total of gendered words generated by the model). We find that Transformer models not only reflect dataset biases, but also they amplify them. When the model produces gendered words, it generates male-gendered words the vast majority of the time. Even when the gold label only contains female-gendered words (F + M 0 ), it still generates male-gendered words nearly 78% of the time. Comparing Debiasing Methods As shown in Figure  1 , each method improves on the metrics-% gendered words, % male bias, and F1-over the baseline Transformer, but we find that combining all methods (ALL) is most advantageous. While ALL has more data than CDA and Bias Ctrl, more data alone is not enough-the Positive-Bias Data Collection model does not achieve as strong results as ALL despite also having more data. Both the Bias Ctrl and ALL models benefit from knowing the data split (F 0 M 0 , for example), and both yield a gender ratio closest to ground truth. 

 Bias Controlled Training Controls Gendered Words. Our Bias Ctrl method can control the number of gendered words in generated dialogues (Figure  2 ). We examine the effect of Bias Ctrl by generating responses conditioning the ALL model on each bin. We observe that changing the bin radically changes the genderedness of generated text with only small differences in overall F1, which shows that the Bias Ctrl method is efficacious. Examples of generated text from both the baseline and the ALL model are shown in Table  6 . Further examples are provided in the Appendix in Table  12 . The baseline model generates malegendered words when the gold response contains no gendered words or only female-gendered  words, even generating unlikely sequences such as my name is abigail. i am the king of this kingdom. For various methods, we compute the top 20 words generated on the test set (after removing stop words), shown in Appendix Table  8 . We denote gendered nouns using an asterisk. Among the top 20 words generated by the baseline, there are only two gendered nouns-knight and king-both male-gendered. The ALL model generates similar words, but also features queen in its top 20, another indication that gender is more balanced. 

 Safety of Generated Text To further evaluate our techniques, we investigate whether the ALL model generates fewer offensive utterances than (i) the baseline, and (ii) the human-generated gold labels. Our bias mitigation techniques have the ancillary benefit of producing models that generate proportionately fewer offensive utterances; see Table  7  for results. We use a Transformer-based dialogue safety classifier to classify model-generated utterances as offensive or safe following  Liu et al. (2019) . The classifier was fine-tuned on an offensive language classification task , and achieves state-of-the-art results. We apply this classifier to each utterance generated by the ALL model and baseline models on the test set, in addition to the gold (human generated) labels from the test set. The dialogue safety classifier rates our proposed ALL model as less offensive than both the baseline model and the ground truth (gold) labels, which argues in favor of the efficacy of our debiasing methods. 

 Human Evaluation: Bias and Quality We compare the quality of our debiasing methods using human evaluation. One might hypothesize that some gender debiasing methods work by replacing contentful words (e.g., witch) with bleached or uninteresting ones (e.g., person, thing), effectively trading off gender bias with engagingness. Generative models in particular are well-known to produce generic text  (Li et al., 2016a; Fan et al., 2018b) , which is often less engaging. Overreliance on generic text might increase the chances of biases such as androcentrism, or the propensity of societies to consider men central but women peripheral  (Bem, 1993; Bailey et al., 2020) ; in language, male-gendered words often act as a gender-neutral standard  (Bailey et al., 2019) , as in Neil Armstrong's 1969 quote "one small step for a man, one giant leap for mankind". We use the dialogue evaluation system Acute-Eval  (Li et al., 2019)  to ask evaluators to compare pairs of conversations from models and decide which model generates (i) more biased dialogues and (ii) more engaging dialogues. We collect 100 model conversations with crowdworkers per method. Then, we compare conversations between a human and the baseline model to conversations between a human and the ALL model with all generations set to the F 0 M 0 gender-neutral control bin. We found that asking for predictions of speaker gender was more effective than asking about sexism directly. As shown in Figure  3 , predicting the gender accurately of ALL model generations is more challenging (significant at p < 0.01 with a t-test), but the responses are just as engaging according to human evaluators. We conclude our proposed methods are able to help mitigate gender bias without degrading dialogue quality.   

 Conclusion We analyze gender bias in dialogue data and resulting model generations for models trained on dialogue data. We propose general purpose techniques for reducing gender bias in generated text. The methods described in this paper combine data augmentation, positive-bias data collection, and bias controlled training. We note that our results show that data collection techniques help mitigate issues, so when it is possible, bias should be considered at the earliest stages of a project. Newly collected or constructed datasets should consider how to carefully craft the collection to mitigate bias issues from the very start. When this is not possible, however, such as in the case of using real-world data or a dataset that already exists, the techniques presented in this paper are shown to be effective at reducing gender bias. They are especially effective when combined, producing less gendered, more balanced, safer utterances that maintain the engagingness of the dialogue. 
