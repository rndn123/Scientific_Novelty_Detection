title
Filling Gender & Number Gaps in Neural Machine Translation with Black-box Context Injection

abstract
When translating from a language that does not morphologically mark information such as gender and number into a language that does, translation systems must "guess" this missing information, often leading to incorrect translations in the given context. We propose a black-box approach for injecting the missing information to a pre-trained neural machine translation system, allowing to control the morphological variations in the generated translations without changing the underlying model or training data. We evaluate our method on an English to Hebrew translation task, and show that it is effective in injecting the gender and number information and that supplying the correct information improves the translation accuracy in up to 2.3 BLEU on a female-speaker test set for a stateof-the-art online black-box system. Finally, we perform a fine-grained syntactic analysis of the generated translations that shows the effectiveness of our method.

Introduction A common way for marking information about gender, number, and case in language is morphology, or the structure of a given word in the language. However, different languages mark such information in different ways -for example, in some languages gender may be marked on the head word of a syntactic dependency relation, while in other languages it is marked on the dependent, on both, or on none of them  (Nichols, 1986) . This morphological diversity creates a challenge for machine translation, as there are ambiguous cases where more than one correct translation exists for the same source sentence. For example, while the English sentence "I love language" is ambiguous with respect to the gender of the speaker, Hebrew marks verbs for the gender of their subject and does not allow gender-neutral translation. This allows two possible Hebrew translations -one in a masculine and the other in a feminine form. As a consequence, a sentence-level translator (either human or machine) must commit to the gender of the speaker, adding information that is not present in the source. Without additional context, this choice must be done arbitrarily by relying on language conventions, world knowledge or statistical (stereotypical) knowledge. Indeed, the English sentence "I work as a doctor" is translated into Hebrew by Google Translate using the masculine verb form oved, indicating a male speaker, while "I work as a nurse" is translated with the feminine form ovedet, indicating a female speaker (verified on March 2019). While this is still an issue, there have been recent efforts to reduce it for specific language pairs. 1 We present a simple black-box method to influence the interpretation chosen by an NMT system in these ambiguous cases. More concretely, we construct pre-defined textual hints about the gender and number of the speaker and the audience (the interlocutors), which we concatenate to a given input sentence that we would like to translate accordingly. We then show that a black-box NMT system makes the desired morphological decisions according to the given hint, even when no other evidence is available on the source side. While adding those hints results in additional text on the target side, we show that it is simple to remove, leaving only the desired translation. Our method is appealing as it only requires simple pre-and-post processing of the inputs and outputs, without considering the system internals, or requiring specific annotated data and training procedure as in previous work  (Vanmassenhove et al., 2018) . We show that in spite of its simplicity, it is effective in resolving many of the ambiguities and improves the translation quality in up to 2.3 BLEU when given the correct hints, which may be inferred from text metadata or other sources. Finally, we perform a fine-grained syntactic analysis of the translations generated using our method which shows its effectiveness. 

 Morphological Ambiguity in Translation Different languages use different morphological features marking different properties on different elements. For example, English marks for number, case, aspect, tense, person, and degree of comparison. However, English does not mark gender on nouns and verbs. Even when a certain property is marked, languages differ in the form and location of the marking  (Nichols, 1986) . For example, marking can occur on the head of a syntactic dependency construction, on its argument, on both (requiring agreement), or on none of them. Translation systems must generate correct target-language morphology as part of the translation process. This requires knowledge of both the source-side and target-side morphology. Current state-of-the-art translation systems do capture many aspects of natural language, including morphology, when a relevant context is available  (Dalvi et al., 2017; Bawden et al., 2018) , but resort to "guessing" based on the training-data statistics when it is not. Complications arise when different languages convey different kinds of information in their morphological systems. In such cases, a translation system may be required to remove information available in the source sentence, or to add information not available in it, where the latter can be especially tricky. 

 Black-Box Knowledge Injection Our goal is to supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences, in order to produce the desired target-side morphology when the information is not available in the source sentence. The approach we take in the current work is that of black-box injection, in which we attempt to inject knowledge to the input in order to influence the output of a trained NMT system, without having access to its internals or its training procedure as proposed by  Vanmassenhove et al. (2018) . We are motivated by recent work by  Voita et al. (2018)  who showed that NMT systems learn to track coreference chains when presented with sufficient discourse context. We conjecture that there are enough sentence-internal pronominal coreference chains appearing in the training data of large-scale NMT systems, such that state-of-theart NMT systems can and do track sentenceinternal coreference. We devise a wrapper method to make use of this coreference tracking ability by introducing artificial antecedents that unambiguously convey the desired gender and number properties of the speaker and audience. More concretely, a sentence such as "I love you" is ambiguous with respect to the gender of the speaker and the gender and number of the audience. However, sentences such as "I love you, she told him" are unambiguous given the coreference groups {I, she} and {you, him} which determine I to be feminine singular and you to be masculine singular. We can thus inject the desired information by prefixing a sentence with short generic sentence fragment such as "She told him:" or "She told them that", relying on the NMT system's coreference tracking abilities to trigger the correctly marked translation, and then remove the redundant translated prefix from the generated target sentence. We observed that using a parataxis construction (i.e. "she said to him:") almost exclusively results in target-side parataxis as well (in 99.8% of our examples), making it easy to identify and strip the translated version from the target side. Moreover, because the parataxis construction is grammatically isolated from the rest of the sentence, it can be stripped without requiring additional changes or modification to the rest of the sentence, ensuring grammaticality. 

 Experiments & Results To demonstrate our method in a black-box setting, we focus our experiments on Google's machine translation system (GMT), accessed through its Cloud API. To test the method on real-world sentences, we consider a monologue from the standup comedy show "Sarah Silverman: A Speck of Dust". The monologue consists of 1,244 English sentences, all by a female speaker conveyed to a plural, gender-neutral audience. Our parallel corpora consists of the 1,244 English sentences from the transcript, and their corresponding He- We translate the monologue one sentence at a time through the Google Cloud API. Eyeballing the results suggest that most of the translations use the incorrect, but default, masculine and singular forms for the speaker and the audience, respectively. We expect that by adding the relevant condition of "female speaking to an audience" we will get better translations, affecting both the gender of the speaker and the number of the audience. To verify this, we experiment with translating the sentences with the following variations: No Prefix-The baseline translation as returned by the GMT system. "He said:"-Signaling a male speaker. We expect to further skew the system towards masculine forms. "She said:"-Signaling a female speaker and unknown audience. As this matches the actual speaker's gender, we expect an improvement in translation of first-person pronouns and verbs with first-person pronouns as subjects. "I said to them:"-Signaling an unknown speaker and plural audience. "He said to them:"-Masculine speaker and plural audience. "She said to them:"-Female speaker and plural audience-the complete, correct condition. We expect the best translation accuracy on this setup. "He/she said to him/her"-Here we set an (incorrect) singular gender-marked audience, to investigate our ability to control the audience morphology. 

 Quantitative Results We compare the different conditions by comparing BLEU  (Papineni et al., 2002)  with respect to the reference Hebrew translations. We use the multi-bleu.perl script from the Moses toolkit  (Koehn et al., 2007) . Table  1  shows BLEU scores for the different prefixes. The numbers match our expectations: Generally, providing an incorrect speaker and/or audience information decreases the BLEU scores, while providing the correct information substantially improves it -we see an increase of up to 2.3 BLEU over the baseline. We note the BLEU score improves in all cases, even when given the wrong gender of either the speaker or the audience. We hypothesise this improvement stems from the addition of the word "said" which hints the model to generate a more "spoken" language which matches the tested scenario. Providing correct information for both speaker and audience usually helps more than providing correct information to either one of them individually. The one outlier is providing "She" for the speaker and "her" for the audience. While this is not the correct scenario, we hypothesise it gives an improvement in BLEU as it further reinforces the female gender in the sentence. 

 Qualitative Results The BLEU score is an indication of how close the automated translation is to the reference translation, but does not tell us what exactly changed concerning the gender and number properties we attempt to control. We perform a finer-grained analysis focusing on the relation between the injected speaker and audience information, and the morphological realizations of the corresponding elements. We parse the translations and the references using a Hebrew dependency parser.  3  In addition to the parse structure, the parser also performs morphological analysis and tagging of the individual tokens. We then perform the following analysis. Speaker's Gender Effects: We search for firstperson singular pronouns with subject case (ani, unmarked for gender, corresponding to the English I), and consider the gender of its governing verb (or adjectives in copular constructions such as 'I am nice'). The possible genders are 'masculine', 'feminine' and 'both', where the latter indicates a case where the none-diacriticized written form admits both a masculine and a feminine reading. We expect the gender to match the ones   

 Interlocutors' Gender and Number Effects: We search for second-person pronouns and consider their gender and number. For pronouns in subject position, we also consider the gender and number of their governing verbs (or adjectives in copular constructions). For a singular audience, we expect the gender and number to match the requested ones. For a plural audience, we expect the masculine-plural forms. Results: Speaker. Figure  1  shows the result for controlling the morphological properties of the speaker ({he, she, I} said). It shows the proportion of gender-inflected verbs for the various conditions and the reference. We see that the baseline system severely under-predicts the feminine form of verbs as compared to the reference. The "He said" conditions further decreases the number of feminine verbs, while the "I said" conditions bring it back to the baseline level. Finally, the "She said" prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference (though still under-predicting some of the feminine cases). Results: Audience. The chart in Figure  2  shows the results for controlling the number of the audience (...to them vs nothing). It shows the proportion of singular vs. plural second-person pronouns on the various conditions. It shows a similar trend: the baseline system severely underpredicts the plural forms with respect to the reference translation, while adding the "to them" condition brings the proportion much closer to that of the reference.  Vanmassenhove et al. (2018)  Closely related to our work,  Vanmassenhove et al. (2018)  proposed a method and an English-French test set to evaluate gender-aware translation, based on the Europarl corpus  (Koehn, 2005) . We evaluate our method (using Google Translate and the given prefixes) on their test set to see whether it is applicable to another language pair and domain. 

 Comparison to Table  2  shows the results of our approach vs. their published results and the Google Translate baseline. As may be expected, Google Translate outperforms their system as it is trained on a different corpus and may use more complex machine translation models. Using our method improves the BLEU score even further.   Vanmassenhove et al. (2018)  on their English-French gender corpus. 

 Other Languages To test our method's outputs on multiple languages, we run our pre-and post-processing steps with Google Translate using examples we sourced from native speakers of different languages. For every example we have an English sentence and two translations in the corresponding language,   3  shows that for these specific examples our method worked on 6/10 of the languages we had examples for, while for 3/10 languages both translations are masculine, and for 1 language both are feminine. 5 Related Work  Rabinovich et al. (2017)  showed that given input with author traits like gender, it is possible to retain those traits in Statistical Machine Translation (SMT) models.  Gr?nroos et al. (2017)  showed that incorporating morphological analysis in the decoder improves NMT performance for morphologically rich languages.  Burlot and Yvon (2017)  presented a new protocol for evaluating the morphological competence of MT systems, indicating that current translation systems only manage to capture some morphological phenomena correctly. Regarding the application of constraints in NMT,  Sennrich et al. (2016)  presented a method for controlling the politeness level in the generated output.  Ficler and Goldberg (2017)  showed how to guide a neural text generation system towards style and content parameters like the level of professionalism, subjective/objective, sentiment and others.  Tiedemann and Scherrer (2017)  showed that incorporating more context when translating subtitles can improve the coherence of the generated translations. Most closely to our work,  Vanmassenhove et al. (2018)  also addressed the missing gender information by training proprietary models with a gender-indicating-prefix. We differ from this work by treating the problem in a black-box manner, and by addressing additional information like the number of the speaker and the gender and number of the audience. 

 Conclusions We highlight the problem of translating between languages with different morphological systems, in which the target translation must contain gender and number information that is not available in the source. We propose a method for injecting such information into a pre-trained NMT model in a black-box setting. We demonstrate the effectiveness of this method by showing an improvement of 2.3 BLEU in an English-to-Hebrew translation setting where the speaker and audience gender can be inferred. We also perform a fine-grained syn-tactic analysis that shows how our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them. In future work we would like to explore automatic generation of the injected context, or the use of cross-sentence context to infer the injected information. Figure 2 : 2 Figure 2: Number inflection statistics for secondperson pronouns. 

 Table 1 : 1 BLEU results on the Silverman dataset brew translations based on the Hebrew subtitles. 2 Speaker Audience BLEU Baseline 18.67 He - 19.2 He him 19.25 He her 19.3 He them 19.5 I - 19.84 I them 20.23 She - 20.8 She him 20.82 She her 20.98 She them 20.97 

 Table 2 : 2 Comparison of our approach (using Google Translate) to Male Female 

 Table 3 : 3 Examples of languages where the speaker's gender changes morphological markings in different languages, and translations using the prefix "He said:" or "She said:" accordingly one in masculine and one in feminine form. Not all examples are using the same source English sentence as different languages mark different information. Table English Text Masculine Feminine Hebrew I am nice ani nehmad ani nehmada Prefix ani nehmad ani nehmada Spanish I am delighted Estoy encantado Estoy encantada Prefix Estoy encantado Estoy encantada Portuguese I was called Eu fui chamado Eu fui chamada Prefix Eu fui chamado Eu fui chamado French I am patient je suis patient je suis patiente Prefix je suis patient je suis patiente Italian I am beautiful Sono bello Sono bella Prefix io sono bello io sono bella Russian I wrote a message ? ? ? ? ? ? Prefix ? ? ? ? ? ? Czech I gave her the flower j? jsem ji dal kv?tinu j? jsem ji dala kv?tinu Prefix Dala jsem j? kv?tinu Dala jsem j? kv?tinu Romanian I am patient Sunt r?bd?tor Sunt r?bd?toare Prefix Sunt r?bd?tor Sunt r?bd?toare Catalan I am rich s?c ric s?c rica Prefix s?c ric s?c ric Polish I am nice Jestem mi?y mi?a Prefix Jestem mi?y Jestem mi?a 

			 blog.google/products/translate/ reducing-gender-bias-google-translate/ 

			 The data is obtained from www.opensubtitles. org 

			 https://www.cs.bgu.ac.il/?yoavg/software/hebparsers/ hebdepparser/
