title


abstract
Neural machine translation (NMT) has achieved impressive performance recently by using large-scale parallel corpora. However, it struggles in the low-resource and morphologically-rich scenarios of agglutinative language translation task. Inspired by the finding that monolingual data can greatly improve the NMT performance, we propose a multi-task neural model that jointly learns to perform bi-directional translation and agglutinative language stemming. Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source-side sentence to specify the desired target outputs of the two different tasks. Experimental results on Turkish-English and Uyghur-Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data.

Introduction Neural machine translation (NMT) has achieved impressive performance on many high-resource machine translation tasks  (Bahdanau et al., 2015; Luong et al., 2015a; Vaswani et al., 2017) . The standard NMT model uses the encoder to map the source sentence to a continuous representation vector, and then it feeds the resulting vector to the decoder to produce the target sentence. However, the NMT model still suffers from the low-resource and morphologically-rich scenarios of agglutinative language translation tasks, such as Turkish-English and Uyghur-Chinese. Both Turkish and Uyghur are agglutinative languages with complex morphology. The morpheme structure of the word can be denoted as: prefix1 + ? + prefixN + stem + suffix1 + ? + suffixN  (Ablimit et al., 2010) . Since the suffixes have many inflected and morphological variants, the vocabulary size of an agglutinative language is considerable even in small-scale training data. Moreover, many words have different morphemes and meanings in different context, which leads to inaccurate translation results. Recently, researchers show their great interest in utilizing monolingual data to further improve the NMT model performance  (Cheng et al., 2016; Ramachandran et al., 2017; Currey et al., 2017) .  Sennrich et al. (2016)  pair the target-side monolingual data with automatic back-translation as additional training data to train the NMT model.  Zhang and Zong (2016)  use the source-side monolingual data and employ the multi-task learning framework for translation and source sentence reordering.  modify the decoder to enable multi-task learning for translation and language modeling. However, the above works mainly focus on boosting the translation fluency, and lack the consideration of morphological and linguistic knowledge. Stemming is a morphological analysis method, which is widely used for information retrieval tasks  (Kishida, 2005) . By removing the suffixes in the word, stemming allows the variants of the same word to share representations and reduces data sparseness. We consider that stemming can lead to better generalization on agglutinative languages, which helps NMT to capture the in-depth semantic information. Thus we use stemming as an auxiliary task for agglutinative language translation. In this paper, we investigate a method to exploit the monolingual data of the agglutinative language to enhance the representation ability of the encoder. This is achieved by training a multi-task neural model to jointly perform bi-directional translation and agglutinative language stemming, which utilizes the shared encoder and decoder. We treat stemming as a sequence generation task. 

 Multi-Task Neural Model for Agglutinative Language Translation Yirong Pan 1,2,3 , Xiao Li 1,2,3 , Yating Yang 1,2,3 , and Rui Dong Figure  1 : The architecture of the multi-task neural model that jointly learns to perform bi-directional translation between Turkish and English, and stemming for Turkish sentence. 

 Related Work Multi-task learning (MTL) aims to improve the generalization performance of a main task by using the other related tasks, which has been successfully applied to various research fields ranging from language  (Liu et al., 2015; Luong et al., 2015a) , vision  (Yim et al., 2015; Misra et al., 2016) , and speech  (Chen and Mak, 2015; Kim et al., 2016) . Many natural language processing (NLP) tasks have been chosen as auxiliary task to deal with the increasingly complex tasks.  Luong et al. (2015b)  employ a small amount of data of syntactic parsing and image caption for English-German translation.  Hashimoto et al. (2017)  present a joint MTL model to handle the tasks of part-of-speech (POS) tagging, dependency parsing, semantic relatedness, and textual entailment for English.  Kiperwasser and Ballesteros (2018)  utilize the POS tagging and dependency parsing for English-German machine translation. To the best of our knowledge, we are the first to incorporate stemming task into MTL framework to further improve the translation performance on agglutinative languages. Recently, several works have combined the MTL method with sequence-to-sequence NMT model for machine translation tasks.  Dong et al. (2015)  follow a one-to-many setting that utilizes a shared encoder for all the source languages with respective attention mechanisms and multiple decoders for the different target languages.  Luong et al. (2015b)  follow a many-to-many setting that uses multiple encoders and decoders with two separate unsupervised objective functions.  Zoph and Knight (2016)  follow a many-to-one setting that employs multiple encoders for all the source languages and one decoder for the desired target language.  Johnson et al. (2017)  propose a more simple method in one-to-one setting, which trains a single NMT model with the shared encoder and decoder in order to enable multilingual translation. The method requires no changes to the standard NMT architecture but instead requires adding a token at the beginning of each source sentence to specify the desired target sentence. Inspired by their work, we employ the standard NMT model with one encoder and one decoder for parameter sharing and model generalization. In addition, we build a joint vocabulary on the concatenation of the source-side and target-side words. Several works on morphologically-rich NMT have focused on using morphological analysis to pre-process the training data  (Luong et al., 2016; Huck et al., 2017; Tawfik et al., 2019) .  Gulcehre et al. (2015)  segment each Turkish sentence into a sequence of morpheme units and remove any nonsurface morphemes for Turkish-English translation.  Ataman et al. (2017)  propose a vocabulary reduction method that considers the morphological properties of the agglutinative language, which is based on the unsupervised morphology learning. This work takes inspiration from our previously proposed segmentation method  (Pan et al., 2020)  that segments the word into a sequence of subword units with morpheme structure, which can effectively reduce language complexity. 

 Multi-Task Neural Model 

 Overview We propose a multi-task neural model for machine translation from and into a low-resource and morphologically-rich agglutinative language. We train the model to jointly learn to perform both the bi-directional translation task and the stemming task on an agglutinative language by using the standard NMT framework. Moreover, we add an artificial token before each source sentence to specify the desired target outputs for different tasks. The architecture of the proposed model is shown in Figure  1 . We take the Turkish-English translation task as example. The "<MT>" token denotes the bilingual translation task and the "<ST>" token denotes the stemming task on Turkish sentence. 

 Neural Machine Translation (NMT) Our proposed multi-task neural model on using the source-side monolingual data for agglutinative language translation task can be applied in any NMT structures with encoder-decoder framework. In this work, we follow the NMT model proposed by  Vaswani et al. (2017) , which is implemented as Transformer. We will briefly summarize it here. Firstly, the Transformer model maps the source sequence ? = (? 1 , ? , ? ? ) and the target sentence ? = (? 1 , ? , ? ? ) into a word embedding matrix, respectively. Secondly, in order to make use of the word order in the sequence, the above word embedding matrices sum with their positional encoding matrices to generate the source-side and target-side positional embedding matrices. The encoder is composed of a stack of N identical layers. Each layer has two sub-layers consisting of the multi-head self-attention and the fully connected feed-forward network, which maps the source-side positional embedding matrix into a representation vector. The decoder is also composed of a stack of N identical layers. Each layer has three sub-layers: the multi-head self-attention, the multi-head attention, and the fully connected feed-forward network. The multi-head attention attends to the outputs of the encoder and decoder to generate a context vector. The feed-forward network followed by a linear layer maps the context vector into a vector with the original space dimension. Finally, the softmax function is applied on the vector to predict the target word sequence. 

 Experiment 

 Dataset The statistics of the training, validation, and test datasets on Turkish-English and Uyghur-Chinese machine translation tasks are shown in Table  1 . For the Turkish-English machine translation, following  (Sennrich et al., 2015a) , we use the WIT corpus  (Cettolo et al., 2012)  and the SETimes corpus (Tyers and Alperen, 2010) as the training dataset, merge the dev2010 and tst2010 as the validation dataset, and use tst2011, tst2012, tst2013, tst2014 from the IWSLT as the test datasets. We also use the talks data from the IWSLT evaluation campaign 1 in 2018 and the news data from News Crawl corpora 2 in 2017 as external monolingual data for the stemming task on Turkish sentences. For the Uyghur-Chinese machine translation, we use the news data from the China Workshop on Machine Translation in 2017 (CWMT2017) as the training dataset and validation dataset, use the news data from CWMT2015 as the test dataset. Each Uyghur sentence has four Chinese reference sentences. Moreover, we use the news data from the Tianshan website 3 as external monolingual data for the stemming task on Uyghur sentences. 

 Data Preprocessing We normalize and tokenize the experimental data. We utilize the jieba toolkit 4 to segment the Chinese sentences, we utilize the Zemberek toolkit 5 with morphological disambiguation  (Sak et al., 2007)  and the morphological analysis tool  (Tursun et al., 2016)  to annotate the morpheme structure of the words in Turkish and Uyghur, respectively. We use our previously proposed morphological segmentation method  (Pan et al., 2020) , which segments the word into smaller subword units with morpheme structure. Since Turkish and Uyghur only have a few prefixes, we combine the prefixes with stem into the stem unit. As shown in Figure  2 , the morpheme structure of the Turkish word "hecelerini" (syllables) is: hece + lerini. Then the byte pair encoding (BPE) technique  (Sennrich et al., 2015b)  is applied on the stem unit "hece" to segment it into "he@@" and "ce@@". Thus the Turkish word is segmented into a sequence of subword units: he@@ + ce@@ + lerini. 

 Task Training Sentence Samples En-Tr Translation <MT> We go through initiation rit@@ es. Ba?la@@ ma rit?el@@ lerini ya?@@ ?yoruz. Tr-En Translation <MT> Ba?la@@ ma rit?el@@ lerini ya?@@ ?yoruz. We go through initiation rit@@ es. Turkish Stemming <ST> Ba?la@@ ma rit?el@@ lerini ya?@@ ?yoruz. Ba?la@@ rit?el@@ ya?@@ Table 2: The training sentence samples for multi-task neural model on Turkish-English machine translation task. We add "<MT>" and "<ST>" before each source sentence to specify the desired target outputs for different tasks. In this paper, we utilize the above morphological segmentation method for our experiments by applying BPE on the stem units with 15K merge operations for the Turkish words and 10K merge operations for the Uyghur words. The standard NMT model trained on this experimental data is denoted as "baseline NMT model". Moreover, we employ BPE to segment the words in English and Chinese by learning separate vocabulary with 32K merge operations. Table  2  shows the training sentence samples for multi-task neural model on Turkish-English machine translation task. 

 Lang In addition, to certify the effectiveness of the morphological segmentation method, we employ the pure BPE to segment the words in Turkish and Uyghur by learning a separate vocabulary with 36K and 38K merge operations, respectively. The standard NMT model trained on this experimental data is denoted as "general NMT model". Table  3  shows the detailed statistics of using different word segmentation methods on Turkish, English, Uyghur, and Chinese. The "Vocab" token denotes the vocabulary size after data preprocessing. The "Avg.Len" token denotes the average sentence length. 

 Training and Evaluation Details We employ the Transformer model implemented in the Sockeye toolkit . The number of layer in both the encoder and decoder is set to N=6, the number of head is set to 8, and the number of hidden unit in the feed-forward network is set to 1024. We use an embedding size of both the source and target words of 512 dimension, and use a batch size of 128 sentences. The maximum sentence length is set to 100 tokens with 0.1 label smoothing. We apply layer normalization and add dropout to the embedding and transformer layers with 0.1 probability. Moreover, we use the Adam optimizer  (Kingma and Ba, 2015)  with an initial learning rate of 0.0002, and save the checkpoint every 1500 updates. Model training process stops after 8 checkpoints without improvements on the validation perplexity. Following  Niu et al. (2018a) , we select the 4 best checkpoint based on the validation perplexity values and combine them in a linear ensemble for decoding. Decoding is performed by using beam search with a beam size of 5. We evaluate the machine translation performance by using the case-sensitive BLEU score  (Papineni et al., 2002)  with standard tokenization. 

 Neural Translation Models In this paper, we select 4 neural translation models for comparison. More details about the models are shown below: General NMT Model: The standard NMT model trained on the experimental data segmented by BPE. 

 Baseline NMT Model: The standard NMT model trained on the experimental data segmented by morphological segmentation. The following models also use this word segmentation method. Bi-Directional NMT Model: Following  Niu et al. (2018b) , we train a single NMT model to perform bi-directional machine translation. We concatenate the bilingual parallel sentences in both directions. Since the source and target sentences come from the same language pairs, we share the source and target vocabulary, and tie their word embedding during model training. 

 Multi-Task Neural Model: We simply use the monolingual data of the agglutinative language from the bilingual parallel sentences. We use a joint vocabulary, tie the word embedding as well as the output layer's weight matrix.  

 Results and Discussion Table  4  shows the BLEU scores of the general NMT model and baseline NMT model on machine translation task. We can observe that the baseline NMT model is comparable to the general NMT model, and it achieves the highest BLEU scores on almost all the test datasets in both directions, which indicates that the NMT baseline based on our proposed segmentation method is competitive. 

 Using Original Monolingual Data Table  5  shows the BLEU scores of the baseline NMT model, bi-directional NMT model, and multi-task neural model on the machine translation task between Turkish and English. The table shows that the multi-task neural model outperforms both the baseline NMT model and bi-directional NMT model, and it achieves the highest BLEU scores on almost all the test datasets in both directions, which suggests that the multi-task neural model is capable of improving the bi-directional translation quality on agglutinative languages. The main reason is that compared with the bi-directional NMT model, our proposed multi-task neural model additionally employs the stemming task for the agglutinative language, which is effective for the NMT model to learn both the source-side semantic information and the target-side language modeling.  

 bidirectional The university was emulating its lives. multi-task The university was imitating life. The function of epochs and perplexity values on the validation dataset in different neural translation models are shown in Figure  3 . We can see that the perplexity values are consistently lower on the multi-task neural model, and it converges rapidly. Table  6  shows a translation example for the different models on Turkish-English. We can see that the translation result of the multi-task neural model is more accurate. The Turkish word "taklit" means "imitate" in English, both the baseline NMT and bi-directional NMT translate it into a synonym "emulate". However, they are not able to express the meaning of the sentence correctly. The main reason is that the auxiliary task of stemming forces the proposed model to focus more strongly on the core meaning of each word (or stem), therefore helping the model make the correct lexical choices and capture the in-depth semantic information. 

 Using External Monolingual Data Moreover, we evaluate the multi-task neural model on using external monolingual data for Turkish stemming task. We employ the parallel sentences and the monolingual data in a 1-1 ratio, and shuffle them randomly before each training epoch. More details about the data are shown below: Original Data: The monolingual data comes from the original bilingual parallel sentences. Talks Data: The monolingual data contains talks. News Data: The monolingual data contains news. Talks and News Mixed Data: The monolingual data contains talks and news in a 3:4 ratio as the same with the original bilingual parallel sentences. Table  7  shows the BLEU scores of the proposed multi-task neural model on using different external monolingual data. We can see that there is no obvious difference on Turkish-English translation performance by using different monolingual data, whether the data is in-domain or out-of-domain to the test dataset. However, for the English-Turkish machine translation task, which can be seen as agglutinative language generation task, using the mixed data of talks and news achieves further improvements of the BLEU scores on almost all the test datasets. The main reason is that the proposed multi-task neural model incorporates many morphological and linguistic information of Turkish rather than that of English, which mainly pays attention to the source-side representation ability on agglutinative languages rather than the target-side language modeling. We also evaluate the translation performance of the general NMT model, baseline NMT model, and multi-task neural model with external news data on the machine translation task between Uyghur and Chinese. The experimental results are shown in Table  8 . The results indicate that the multi-task neural model achieves the highest BLEU scores on the test dataset by utilizing external monolingual data for the stemming task on Uyghur sentences. 

 Conclusions In this paper, we propose a multi-task neural model for translation task from and into a low-resource and morphologically-rich agglutinative language. The model jointly learns to perform bi-directional translation and agglutinative language stemming by utilizing the shared encoder and decoder under standard NMT framework. Extensive experimental results show that the proposed model is beneficial for the agglutinative language machine translation, and only a small amount of the agglutinative data can improve the translation performance in both directions. Moreover, the proposed approach with external monolingual data is more useful for translating into the agglutinative language, which achieves an improvement of +1.42 BLEU points for translation from English into Turkish and +1.45 BLEU points from Chinese into Uyghur. In future work, we plan to utilize other word segmentation methods for model training. We also plan to combine the proposed multi-task neural model with back-translation method to enhance the ability of the NMT model on target-side language modeling. Figure 3 : 3 Figure 3: The function of epochs (x-axis) and perplexity (y-axis) values on the validation dataset in different neural translation models for the translation task. 

 -Decoder Framework Bilingual Data Monolingual Data Training data <MT> + English sentence <MT> + Turkish sentence <ST> + Turkish sentence Turkish sentence English sentence stem sequence 1,2,3 1 Xinjiang Technical Institute of Physics & Chemistry, Chinese Academy of Sciences, China 2 University of Chinese Academy of Sciences, China 3 Xinjiang Laboratory of Minority Speech and Language Information Processing, China panyirong15@mails.ucas.ac.cn {xiaoli, yangyt, dongrui}@ms.xjb.ac.cn Encoder 

 Table 1 : 1 The statistics of the training, validation, and test datasets on Turkish-English and Uyghur-Chinese machine translation tasks. The "# Src" denotes the number of the source tokens, and the "# Trg" denotes the numbers of the target tokens. Task Data # Sent # Src # Trg Tr-En train 355,251 6,356,767 8,021,161 valid 2,455 37,153 52,125 test 4,962 69,006 96,291 Uy- train 333,097 6,026,953 5,748,298 Ch valid 700 17,821 17,085 test 1,000 20,580 18,179 

 bir dilin son hecelerini kendisiyle birlikte mezara Morpheme Segmentation hece+ler+i+ni hece+lerini he@@+ce@@+lerini Apply BPE on Stem Stem+Combined Suffix Figure 2: The example of morphological segmentation method for the word in Turkish. 

 Method # Merge Vocab Avg.Len Tr Morph 15K 36,468 28 Tr BPE 36K 36,040 22 En BPE 32K 31,306 25 Uy Morph 10K 38,164 28 Uy BPE 38K 38,292 21 Ch BPE 32K 40,835 19 Table 3: The detailed statistics of using different word segmentation methods on Turkish, English, Uyghur, and Chinese. 

 85 15.93 15.45 15.93Table 4 : 4 The BLEU scores of the general NMT model and baseline NMT model on the machine translation task between Turkish and English. Task Model tst11 tst12 tst13 tst14 Tr- general 25.92 26.55 27.34 26.35 En baseline 26.48 27.02 27.91 26.33 En- general 13.73 14.68 13.84 14.65 Tr 14.Task Model baseline tst11 tst12 tst13 tst14 Tr- baseline 26.48 27.02 27.91 26.33 En bi- 26.21 27.17 28.68 26.90 directional multi-task 

 26.82 27.96 29.16 27.98 En- baseline 14.85 15.93 15.45 15.93 Tr bi- 15.08 16.20 16.25 16.56 directional multi-task 

 15.65 17.10 16.35 16.41Table 5 : 5 The BLEU scores of the baseline NMT model, bi-directional NMT model, and multi-task neural model on the machine translation task between Turkish and English. 

 Table 6 : 6 A translation example for the different NMT models on Turkish-English. 

 Table 7 : 7 The BLEU scores of the multi-task neural model on using external monolingual data of talks data, news data, and mixed data. Task Data tst11 tst12 tst13 tst14 Tr-En original 26.82 27.96 29.16 27.98 talks 26.55 27.94 29.13 28.02 news 26.47 28.18 28.89 27.40 mixed 26.60 27.93 29.58 27.32 En-Tr original 15.65 17.10 16.35 16.41 talks 15.57 16.97 16.22 16.91 news 15.67 17.19 16.26 16.69 mixed 15.96 17.35 16.55 16.89 Task Model BLEU Uy-Ch general NMT model 35.12 baseline NMT model 35.46 multi-task neural model with 36.47 external monolingual data Ch-Uy general NMT model 21.00 baseline NMT model 21.57 multi-task neural model with 23.02 external monolingual data 

 Table 8 : 8 The BLEU scores of the general NMT model, baseline NMT model, and the multi-task neural model with external monolingual data on Uyghur-Chinese and Chinese-Uyghur machine translation tasks. 

			 https://wit3.fbk.eu/archive/2018-01/additional_TED_xml/ 2 http://data.statmt.org/wmt18/translation-task/ 3 http://uy.ts.cn/ 

			 https://github.com/fxsjy/jieba 5 https://github.com/ahmetaa/zemberek-nlp
