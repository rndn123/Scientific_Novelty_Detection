title
Imitation Attacks and Defenses for Black-box Machine Translation Systems

abstract
Adversaries may look to steal or attack blackbox NLP systems, either for financial gain or to exploit model errors. One setting of particular interest is machine translation (MT), where models have high commercial value and errors can be costly. We investigate possible exploitations of black-box MT systems and explore a preliminary defense against such threats. We first show that MT systems can be stolen by querying them with monolingual sentences and training models to imitate their outputs. Using simulated experiments, we demonstrate that MT model stealing is possible even when imitation models have different input data or architectures than their target models. Applying these ideas, we train imitation models that reach within 0.6 BLEU of three production MT systems on both high-resource and low-resource language pairs. We then leverage the similarity of our imitation models to transfer adversarial examples to the production systems. We use gradient-based attacks that expose inputs which lead to semanticallyincorrect translations, dropped content, and vulgar model outputs. To mitigate these vulnerabilities, we propose a defense that modifies translation outputs in order to misdirect the optimization of imitation models. This defense degrades the adversary's BLEU score and attack success rate at some cost in the defender's BLEU and inference speed. Transfer Solve Eq. (2) Save me it's over 100?F Phase One: Model Imitation

Introduction NLP models deployed through APIs (e.g., Google Translate) can be lucrative assets for an organization. These models are typically the result of a considerable investment-up to millions of dollarsinto private data annotation and algorithmic improvements. Consequently, such models are kept hidden behind black-box APIs to protect system integrity and intellectual property. We consider an adversary looking to steal or attack a black-box NLP system. Stealing a produc-tion model allows an adversary to avoid long-term API costs or launch a competitor service. Moreover, attacking a system using adversarial examples  (Szegedy et al., 2014)  allows an adversary to cause targeted errors for a model, e.g., bypassing fake news filters or causing systems to output malicious content that may offend users and reflect poorly on system providers. In this work, we investigate these two exploits for black-box machine translation (MT) systems: we first steal (we use "steal" following  Tram?r et al. 2016)  production MT systems by training imitation models, and we then use these imitation models to generate adversarial examples for production MT systems. We create imitation models by borrowing ideas from knowledge distillation  (Hinton et al., 2014) : we query production MT systems with monolingual sentences and train imitation (i.e., student) models to mimic the system outputs (top of Figure  1 ). We first experiment with simulated studies which demonstrate that MT models are easy to imitate (Section 3). For example, imitation models closely replicate the target model outputs even when they are trained using different architectures or on outof-domain queries. Applying these ideas, we imitate production systems from Google, Bing, and Systran with high fidelity on English?German and Nepali?English. For example, Bing achieves 32.9 BLEU on WMT14 English?German and our imitation achieves 32.4 BLEU. We then demonstrate that our imitation models aid adversarial attacks against production MT systems (Section 4). In particular, the similarity of our imitation models to the production systems allows for direct transfer of adversarial examples obtained via gradient-based attacks. We find small perturbations that cause targeted mistranslations (e.g., bottom of Figure  1 ), nonsense inputs that produce malicious outputs, and universal phrases that cause mistranslations or dropped content. The reason we identify vulnerabilities in NLP Figure  1 : Imitating and attacking an English?German MT system. In phase one (model imitation), we first select sentences from English corpora (e.g., Wikipedia), label them using the black-box API, and then train an imitation model on the resulting data. In phase two (adversarial attacks), we generate adversarial examples against our imitation model and transfer them to the production systems. For example, we find an input perturbation that causes Google to produce a factually incorrect translation, see the link here (all attacks work as of April 2020). systems is to robustly patch them. To take steps towards this, we create a defense which finds alternate translations that cause the optimization of the imitation model to proceed in the wrong direction (Section 5). These alternate translations degrade the imitation model's BLEU score and the transfer rate of adversarial examples at some cost in the defender's BLEU and inference speed. 

 How We Imitate MT Models We have query access to the predictions (but no probabilities or logits) from a victim MT model. This victim is a black box: we are unaware of its internals, e.g., the model architecture, hyperparameters, or training data. Our goal is to train an imitation model  (Orekondy et al., 2019)  that achieves comparable accuracy to this victim on held-out data. Moreover, to enhance the transferability of adversarial examples, the imitation model should be functionally similar to the victim, i.e., similar inputs translate to similar outputs. 

 Past Work on Distillation and Stealing This problem setup is closely related to model distillation  (Hinton et al., 2014) : training a student model to imitate the predictions of a teacher. Distillation has widespread use in MT, including reducing architecture size  (Kim and Rush, 2016; Kim et al., 2019) , creating multilingual models  (Tan et al., 2019) , and improving non-autoregressive generation  (Ghazvininejad et al., 2019; Stern et al., 2019) . Model stealing differs from distillation because the victim's (i.e., teacher's) training data is unknown. This causes queries to typically be out-of-domain for the victim. Moreover, because the victim's output probabilities are unavailable for most APIs, imitation models cannot be trained using distribution matching losses such as KL divergence, as is common in distillation. Despite these challenges, prior work shows that model stealing is possible for simple classification  (Lowd and Meek, 2005; Tram?r et al., 2016) , vision  (Orekondy et al., 2019) , and language tasks  (Krishna et al., 2020; Pal et al., 2019) . In particular,  Pal et al. (2019)  steal text classifiers and  Krishna et al. (2020)  steal reading comprehension and textual entailment models; we extend these results to MT and investigate how model stealing works for production systems. Our Approach We assume access to a corpus of monolingual sentences. We select sentences from this corpus, query the victim on each sentence, and obtain the associated translations. We then train an imitation model on this "labeled" data. 

 Imitating Black-box MT Systems We first study imitation models through simulated experiments: we train victim models, query them as if they are black boxes, and then train imitation   1 : Imitation models are highly similar to their victims. We train imitation models that are different from their victims in input data and/or architecture. We test the models on IWSLT (Test) and out-of-domain news data from WMT (OOD). We also measure functionality similarity by reporting the BLEU score between the outputs of the imitation and the victim models (Inter). models to mimic their outputs. In Section 3.3, we turn to imitating production systems. 

 Research Questions and Experiments In practice, the adversary will not know the victim's model architecture or source data. We study the effect of this with the following experiments: ? We use the same architecture, hyperparameters, and source data as the victim (All Same). Datasets We consider German?English using the TED data from IWSLT 2014  (Cettolo et al., 2014) . We follow common practice for IWSLT and report case-insensitive BLEU  (Papineni et   2002). For Data Different, we use English sentences from Europarl v7. The predictions from the victim are generated using greedy decoding. 

 We Closely Imitate Local Models Test BLEU Score We first compare the imitation models to their victims using in-domain test BLEU. For all settings, imitation models closely match their victims (Test column in Table  1 ). We also evaluate the imitation models on OOD data to test how well they generalize compared to their victims. We use the WMT14 test set (newstest 2014). Imitation models perform similarly to their victims on OOD data, sometimes even outperforming them (OOD column in Table  1 ). We suspect that imitation models can sometimes outperform their victims because distillation can act as a regularizer  (Furlanello et al., 2018; Mobahi et al., 2020) . Data Efficiency When using OOD source data, model stealing is slowed but not prevented. Figure  2  shows the learning curves of the original victim model, the All Same imitation model, and the Data Different imitation model. Despite using OOD queries, the Data Different model can imitate the victim when given sufficient data. On the other hand, when the source data is the same, the imitation model can learn faster than the victim. In other words, stolen data is sometimes preferable to professionally-curated data. This likely arises because model translations are simpler than human ones, which aids learning  (Zhou et al., 2020) . Functional Similarity Finally, we measure the BLEU score between the outputs of the victim and the imitation models to measure their functional similarity (henceforth inter-system BLEU). As a reference for inter-system BLEU, two Transformer models trained with different random seeds achieve 62.1 inter-system BLEU. The inter-system BLEU for the imitation models and their victims is as high as 70.5 (Table  1 ), i.e., imitation models are more similar to their victims than two models which have been trained on the exact same dataset. 

 We Closely Imitate Production Models Given the effectiveness of our simulated experiments, we now imitate production systems from Google, Bing, and Systran. Language Pairs and Data We consider two language pairs, English?German (high-resource) and the Nepali?English (low-resource).  2  We collect training data for our imitation models by querying the production systems. For English?German, we query the source side of the WMT14 training set (? 4.5M sentences).  3  For Nepali?English, we query the Nepali Language Wikipedia (? 100,000 sentences) and approximately two million sentences from Nepali common crawl. We train Transformer Big  (Vaswani et al., 2017)  models on both datasets. Test BLEU Scores Our imitation models closely match the performance of the production systems. For English?German, we evaluate models on the WMT14 test set (newstest2014) and report standard tokenized case-sensitive BLEU scores. Our imitation models are always within 0.6 BLEU of the production models (Imitation in Table  2 ). For Nepali?English, we evaluate using FLoRes devtest  (Guzm?n et al., 2019) . We compute BLEU scores using SacreBLEU  (Post, 2018)   We query production systems with English news sentences and train imitation models to mimic their German outputs. The imitation models closely imitate the production systems for both in-domain (WMT newstest2014) and out-of-domain test data (IWSLT TED talks). best public system  (Guzm?n et al., 2019) . Our imitation model reaches a nearly identical 22.0 BLEU. OOD Evaluation and Functional Similarity Our imitation models have also not merely matched the production systems on in-domain data. We test the English?German imitation models on IWSLT: the imitation models are always within 0.9 BLEU of the production systems (IWSLT in Table  2 ). Finally, there is also a high inter-system BLEU between the imitation models and the production systems. In particular, on the English?German WMT14 test set the inter-system BLEU is 65.6, 67.7, and 69.0 for Google, Bing, and Systran, respectively. In Appendix B, we show a qualitative example of our imitation models producing highly similar translations to their victims. 

 Estimated Data Costs We estimate that the costs of obtaining the data needed to train our English?German models is as little as $10 (see Appendix C for full calculation). Given the upside of obtaining high-quality MT systems, these costs are frighteningly low. 

 Attacking Production Systems Thus far, we have shown that imitation models allow adversaries to steal black-box MT models. Here, we show that imitation models can also be used to create adversarial examples for black-box MT systems. Our attack code is available at https: //github.com/Eric-Wallace/adversarial-mt. 

 What are Adversarial Examples for MT? MT errors can have serious consequences, e.g., they can harm end users or damage an MT system's reputation. For example, a person was arrested when their Arabic Facebook post meaning "good morning" was mistranslated as "attack Attack System English Input (red = adversarial edit) Predicted Translation (blue = English meaning) 

 Targeted Flips Google I am going to die, its over 100 ? F, help! Ich werde sterben, es ist ?ber 100 ? F, Hilfe! Google I am going to die, its over 102 ? F, help! Ich werde sterben, es ist ?ber 22 ? C, Hilfe! 100 ? F ? 22 ? C (=72 ? F) Systran I am feeling grey that HK decided to join China Ich f?hle mich grau, dass HK beschlossen hat, China beizutreten Systran I am feeling gre y that HK decided to join China Ich f?hle mich froh, dass HK beschlossen hat, China beizutreten "grau" (gray) ? "froh" (happy) 

 Malicious Nonsense Google miei llll going ro tobobombier the Land Ich werde das Land bombardieren (I will bomb the country)  We show examples of adversarial attacks that transfer to production MT systems as of April 2020 (screenshots in Appendix G). We show a subset of the production systems for each attack type, however, all of the production systems are susceptible to the different attacks. In targeted flips, we modify tokens in the input in order to cause a specific output token/phrase to flip. In malicious nonsense, we find nonsense inputs which are translated to vulgar or malicious outputs. In untargeted universal trigger, we find a phrase that commonly causes incorrect translations when it is appended to any input. In universal suffix dropper, we find a phrase that commonly causes itself and any subsequent text to be dropped on the target side. 

 Untargeted them"  (Hern, 2018) . Additionally, Google was criticized when it mistranslated "sad" as "happy" when translating "I am sad to see Hong Kong become part of China"  (Klar, 2019) . Although the public occasionally stumbles upon these types of egregious MT errors, bad actors can use adversarial attacks  (Szegedy et al., 2014)   to the target model and can compute gradients with respect to its inputs  (Ebrahimi et al., 2018; Chaturvedi et al., 2019) . These gradients are used to generate attacks that flip output words  (Cheng et al., 2020) , decode nonsense into arbitrary sentences  (Chaturvedi et al., 2019) , or cause egregiously long translations . 

 Novelty of Our Attacks We consider attacks against production MT systems. Here, white-box attacks are inapplicable. We circumvent this by leveraging the transferability of adversarial examples  (Papernot et al., 2016; Liu et al., 2017) : we generate adversarial examples for our imitation models and then apply them to the production systems. We also design new universal (inputagnostic) attacks  (Moosavi-Dezfooli et al., 2017; Wallace et al., 2019)  for MT: we append phrases that commonly cause errors or dropped content for any input (described in Section 4.3). 

 How We Generate Adversarial Examples We first describe our general attack formulation. We use a white-box, gradient-based method for constructing attacks. Formally, we have white-box access to an imitation model f , a text input of tokens x, and an adversarial loss function L adv . We consider different adversarial example types; each type has its own L adv and initialization of x. Our attack iteratively replaces the tokens in the input based on the gradient of the adversarial loss L adv with respect to the model's input embeddings e. We replace an input token at position i with the token whose embedding minimizes the first-order Taylor approximation of L adv : arg min e i ?V e i ? e i ? e i L adv , ( 1 ) where V is the model's token vocabulary and ? e i L adv is the gradient of L adv with respect to the input embedding for the token at position i. Since the arg min does not depend on e i , we solve: arg min e i ?V e i ? e i L adv . (2) Computing the optimal e i can be computed using |V| d-dimensional dot products (where d is the embedding dimension) similar to  Michel et al. (2019) . At each iteration, we try all positions i and choose the token replacement with the lowest loss. Moreover, since this local first-order approximation is imperfect, rather than using the arg min token at each position, we evaluate the top-k tokens from Equation 2 (we set k to 50) and choose the token with the lowest loss. Using a large value of k, e.g., at least 10, is critical to achieving strong results. 

 Types of Adversarial Attacks Here, we describe the four types of adversarial examples we generate and their associated L adv . (1) Targeted Flips We replace some of the input tokens in order to cause the prediction for a specific output token to flip to another specific token. For example, we cause Google to predict "22 ? C" instead of "102 ? F" by modifying a single input token (first section of Table  3 ). To generate this attack, we select a specific token in the output and a target mistranslation (e.g., "100 ? F" ? "22 ? C"). We set L adv to be the cross entropy for that mistranslation token (e.g., "22 ? C") at the position where the model currently outputs the original token (e.g., "100 ? F"). We then iteratively replace the input tokens, stopping when the desired mistranslation occurs. (2) Malicious Nonsense We find nonsense inputs which are translated to vulgar/malicious outputs. For example, "I miii llllll wgoing rr tobobombier the Laaand" is translated as "I will bomb the country" (in German) by Google (second section of Table 3). To generate this attack, we first obtain the output prediction for a malicious input, e.g., "I will kill you". We then iteratively replace the tokens in the input without changing the model's prediction. We set L adv to be the cross-entropy loss of the original prediction and we stop replacing tokens just before the prediction changes. A possible failure mode for this attack is to find a paraphrase of the input-we find that this rarely occurs in practice. (3) Untargeted Universal Trigger We find a phrase that commonly causes incorrect translations when it is appended to any input. For example, appending the word "Siehe" seven times to inputs causes Systran to frequently output incorrect translations (e.g., third section of Table  3 ). (4) Universal Suffix Dropper We find a phrase that, when appended to any input, commonly causes itself and any subsequent text to be dropped from the translation (e.g., fourth section of Table  3 ). For attacks 3 and 4, we optimize the attack to work for any input. We accomplish this by averaging the gradient ? e i L adv over a batch of inputs. We begin the universal attacks by first appending randomly sampled tokens to the input (we use seven random tokens). For the untargeted universal trigger, we set L adv to be the negative cross entropy of the original prediction (before the random tokens were appended), i.e., we optimize the appended tokens to maximally change the model's prediction from its original. For the suffix dropper, we set L adv to be the cross entropy of the original prediction, i.e., we try to minimally change the model's prediction from its original. 

 Experimental Setup We attack the English?German production systems to demonstrate our attacks' efficacy on highquality MT models. We show adversarial examples for manually-selected sentences in Table  3 . Quantitative Metrics To evaluate, we report the following metrics. For targeted flips, we We report the percent of inputs which are successfully attacked for our imitation models, as well as the percent of tokens which are changed for those inputs. We then report the transfer rate: the percent of successful attacks which are also successful on the production MT systems. pick a random token in the output that has an antonym in German Open WordNet (https://github. com/hdaSprachtechnologie/odenet) and try to flip the model's prediction for that token to its antonym. We report the percent of inputs that are successfully attacked and the percent of the input tokens which are changed for those inputs (lower is better).  4  For malicious nonsense, we report the percent of inputs that can be modified without changing the prediction and the percent of the input tokens which are changed for those inputs (higher is better). The untargeted universal trigger looks to cause the model's prediction after appending the trigger to bear little similarity to its original prediction. We compute the BLEU score of the model's output after appending the phrase using the model's original output as the reference. We do not impose a brevity penalty, i.e., a model that outputs its original prediction plus additional content for the appended text will receive a score of 100. For the universal suffix dropper, we manually compute the percentage of cases where the appended trigger phrase and a subsequent suffix are either dropped or are replaced with all punctuation tokens. Since the universal attacks require manual analysis and additional computational costs, we attack one system per method. For the untargeted universal trigger, we attack Systran. For the universal suffix dropper, we attack Bing. Evaluation Data For the targeted flips, malicious nonsense, and untargeted universal trigger, we eval-uate on a common set of 200 examples from the WMT validation set (newstest 2013) that contain a token with an antonym in German Open Word-Net. For the universal suffix dropper, we create 100 sentences that contain different combinations of prefixes and suffixes (full list in Appendix D). 

 Results: Attacks on Production Systems The attacks break our imitation models and successfully transfer to production systems. We report the results for targeted flips and malicious nonsense in Table  4 . For our imitation models, we are able to perturb the input and cause the desired output in the majority (> 3/4) of cases. For the targeted flips attack, few perturbations are required (usually near 10% of the tokens). Both attacks transfer at a reasonable rate, e.g., the targeted flips attack transfers 23% of the time for Systran. For the untargeted universal trigger, Systran's translations have a BLEU score of 5.46 with its original predictions after appending "Siehe" seven times, i.e., the translations of the inputs are almost entirely unrelated to the model's original output after appending the trigger phrase. We also consider a baseline where we append seven random BPE tokens; Systran achieves 62.2 and 58.8 BLEU when appending two different choices for the random seven tokens. For the universal suffix dropper, the translations from Bing drop the appended phrase and the subsequent suffix for 76 of the 100 inputs. To evaluate whether our imitation models are needed to generate transferable attacks, we also attack a Transformer Big model that is trained on the WMT14 training set. The adversarial attacks generated against this model transfer to Google 8.8% of the time-about half as often as our imitation model. This shows that the imitation models, which are designed to be high-fidelity imitations of the production systems, considerably enhance the adversarial example transferability. 

 Defending Against Imitation Models Our goal is not to provide a recipe for adversaries to perform real-world attack. Instead, we follow the spirit of threat modeling-we identify vulnerabilities in NLP systems in order to robustly patch them. To take first steps towards this, we design a new defense that slightly degrades victim model BLEU while more noticeably degrading imitation model BLEU. To accomplish this, we repurpose Defending Against Imitation Models Figure  3 : A na?ve defense against model stealing equally degrades the BLEU score of the victim and imitation models (gray line). Better defenses are lower and to the right. Our defense (black line) has a parameter (BLEU match threshold) that can be changed to tradeoff between the victim and the adversary's BLEU. We outperform the na?ve defense in all settings, e.g., we degrade the victim's BLEU from 34.6 ? 33.8 while degrading the adversary's BLEU from 34.5 ? 32.7. prediction poisoning  (Orekondy et al., 2020)  for MT: rather than having the victim output its original translation y, we have it output a different (high-accuracy) translation ? that steers the training of the imitation in the wrong direction. Defense Objective Formally, assume the adversary will train their imitation model on the outputs of the victim model using a first-order optimizer with gradients g = ? ?t L(x, y), where ? t is the current imitation model parameters, x is an input, y is the victim output, and L is the cross-entropy loss. We want the victim to instead output a ? whose gradient g = ? ?t L(x, ?) maximizes the angular deviation with g, or equivalently minimizes the cosine similarity. Training on this ? effectively induces an incorrect gradient signal for ? t . Note that in practice the adversary's model parameters ? t is unknown to the victim. Thus, we instead look to find a g that has a high angular deviation across ten different Transformer MT model checkpoints that are saved from ten different epochs. To find ?,  Orekondy et al. (2020)  use information from the Jacobian. Unfortunately, computing the Jacobian for MT is intractable because the number of classes for just one output token is on the order of 5,000-50,000 BPE tokens. We instead design a search procedure to find ?. 

 Maximizing the Defense Objective We first gen-erate the original output y from the victim model (e.g., the top candidate from a beam search) and compute g using the ten Transformer model ensemble. We then generate a diverse set of 100 alternate translations from the victim model. To do so, we take the 20 best candidates from beam search, the 20 best candidates from diverse beam search  (Vijayakumar et al., 2018) , 20 random samples, 20 candidates generated using top-k truncated sampling (k = 10) from  Fan et al. (2018) , and 20 candidates generated using nucleus sampling with p = 0.9  (Holtzman et al., 2020) . Then, to largely preserve the model's original accuracy, we compute the BLEU score for all 100 candidates using the model's original output y as the reference, and we remove any candidate below a certain threshold (henceforth BLEU Match threshold). We finally compute the gradient g for all candidates using the model ensemble and output the candidate whose gradient maximizes the angular deviation with g.  5  In practice, generating the 100 candidates is done entirely in parallel, as is the computation of the gradient g. Table  5  shows examples of ? at different BLEU Match thresholds. For our quantitative results, we will sweep over different BLEU Match thresholds-lower thresholds will more severely degrade the victim's accuracy but will have more freedom to incorrectly steer the imitation model. 

 BM ? Text 

 Source andere orte im land hatten ?hnliche r?ume. Target other places around the country had similar rooms. y --other places in the country had similar rooms. ? 88.0 24.1 ? some other places in the country had similar rooms. ? 75.1 40.1 ? other sites in the country had similar rooms. ? 72.6 42.1 ? another place in the country had similar rooms. Table  5 : We show the victim model's original translation y. We then show three ? candidates, their BLEU Match (BM) with y and their angular deviation (?), i.e., the arccosine of the cosine similarity between g and g. Figure  4  in Appendix F shows a histogram of the angular deviations for the entire training set. Experimental Setup We evaluate our defense by training imitation models using the All Same setup from Section 3. We use BLEU Match thresholds of 70, 80, or 90 (lower thresholds than 70 resulted in large BLEU decreases for the victim). Results Figure  3  plots the validation BLEU scores of the victim model and the imitation model at the different BLEU match thresholds. Our defense degrades the imitation model's BLEU (e.g., 34.5 ? 32.7) more than the victim model's BLEU (e.g., 34.6 ? 33.8).  6  The inter-system BLEU also degrades from the original 69.7 to 63.9, 57.8, and 53.5 for the 90, 80, and 70 BLEU Match thresholds, respectively. Even though the imitation model's accuracy degradation is not catastrophic when using our defense, it does allow the victim model to have a (small) competitive advantage over the adversary. Adversarial Example Transfer Our defense also implicitly inhibits the transfer of adversarial examples. To evaluate this, we generate malicious nonsense attacks against the imitation models and transfer them to the victim model. We use 400 examples from the IWSLT validation set for evaluation. Without defending, the attacks transfer to the victim at a rate of 38%. Our defense can drop the transfer rates to 32.5%, 29.5%, and 27.0% when using the 90, 80, and 70 BLEU match thresholds, respectively. Also note that defenses may not be able to drive the transfer rate to 0%: there is a baseline transfer rate due to the similarity of the architectures, input distributions, and other factors. For example, we train two transformer models on distinct halves of IWSLT and observe an 11.5% attack transfer rate between them. Considering this as a very rough baseline, our defense can prevent about 20-40% of the additional errors that are gained by the adversary using an imitation model. Overall, our defense is a step towards preventing NLP model stealing (see Appendix E for a review of past defenses). Currently, our defense comes at the cost of extra compute (it requires generating and backpropagating 100 translation hypotheses) and lower BLEU. We hope to develop more effective and scalable defenses in future work. 

 Conclusion We demonstrate that model stealing and adversarial examples are practical concerns for production NLP systems. Model stealing is not merely hy-pothetical: companies have been caught stealing models in NLP settings, e.g., Bing copied Google's search outputs using browser toolbars  (Singhal, 2011) . Moving forward, we hope to improve and deploy defenses against adversarial attacks in NLP, and more broadly, we hope to make security and privacy a more prominent focus of NLP research. 
