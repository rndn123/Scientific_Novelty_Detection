title
Context-Dependent Translation Selection Using Convolutional Neural Network

abstract
We propose a novel method for translation selection in statistical machine translation, in which a convolutional neural network is employed to judge the similarity between a phrase pair in two languages. The specifically designed convolutional architecture encodes not only the semantic similarity of the translation pair, but also the context containing the phrase in the source language. Therefore, our approach is able to capture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points.

Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms  (Koehn et al., 2003) , which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space  (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014) . The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. The above methods, however, neglect the information of local contexts, which has been proven to be useful for disambiguating translation candidates during decoding  Marton and Resnik, 2008) . The matching scores of translation pairs are treated the same, even they are in different contexts. Accordingly, the methods fail to adapt to local contexts and lead to precision issues for specific sentences in different contexts. To capture useful context information, we propose a convolutional neural network architecture to measure context-dependent semantic similarities between phrase pairs in two languages. For each phrase pair, we use the sentence containing the phrase in source language as the context. With the convolutional neural network, we summarize the information of a phrase pair and its context, and further compute the pair's matching score with a multi-layer perceptron. We discriminately train the model using a curriculum learning strategy. We classify the training examples according to the difficulty level of distinguishing the positive candidate from the negative candidate. Then we train the model to learn the semantic information from easy (basic semantic similarities) to difficult (context-dependent semantic similarities). Experimental results on a large-scale translation task show that the context-dependent convolutional matching (CDCM) model improves the performance by up to 1.4 BLEU points over a strong phrase-based SMT system. Moreover, the CDCM model significantly outperforms its context-independent counterpart, proving that it is necessary to incorporate local contexts into SMT. Contributions. Our key contributions include: ? we introduce a novel CDCM model to capture context-dependent semantic similarities between phrase pairs (Section 2); ? we develop a novel learning algorithm to train the CDCM model using a curriculum learning strategy (Section 3).  

 Context-Dependent Convolutional Matching Model The model architecture, shown in Figure  1 , is a variant of the convolutional architecture of . It consists of two components: ? convolutional sentence model that summarizes the meaning of the source sentence and the target phrase; ? matching model that compares the two representations with a multi-layer perceptron  (Bengio, 2009) . Let ? be a target phrase and f be the source sentence that contains the source phrase aligning to ?. We first project f and ? into feature vectors x and y via the convolutional sentence model, and then compute the matching score s(x, y) by the matching model. Finally, the score is introduced into a conventional SMT system as an additional feature. Convolutional sentence model. As shown in Figure  1 , the model takes as input the embeddings of words (trained beforehand elsewhere) in f and ?. It then iteratively summarizes the meaning of the input through layers of convolution and pooling, until reaching a fixed length vectorial representation in the final layer. In Layer-1, the convolution layer takes sliding windows on f and ? respectively, and models all the possible compositions of neighbouring words. The convolution involves a filter to produce a new feature for each possible composition. Given a k-sized sliding window i on f or ?, for example, the jth convolution unit of the composition of the words is generated by: c i (1,j) = g( ?i (0) ) ? ?(w (1,j) ? ?i (0) + b (1,j) ) (1) where ? g(?) is the gate function that determines whether to activate ?(?); ? ?(?) is a non-linear activation function. In this work, we use ReLu  (Dahl et al., 2013)  as the activation function; ? w  (1,j)  is the parameters for the jth convolution unit on Layer-1, with matrix  ,1)  , . . . , w (1,J) ]; W (1) = [w (1 ? ?i (0) is a vector constructed by concatenating word vectors in the k-sized sliding widow i; ? b (1,j) is a bias term, with vector B (1) = [b (1,1) , . . . , b (1,J) ]. To distinguish the phrase pair from its context, we use one additional dimension in word embeddings: 1 for words in the phrase pair and 0 for the others. After transforming words to their tagged embeddings, the convolutional sentence model takes multiple choices of composition using sliding windows in the convolution layer. Note that sliding windows are allowed to cross the boundary of the source phrase to exploit both phrasal and contextual information. In Layer-2, we apply a local max-pooling in non-overlapping 1 ? 2 windows for every convolution unit c (2,j) i = max{c (1,j) 2i , c (1,j) 2i+1 } (2) In Layer-3, we perform convolution on output from Layer-2: c i (3,j) = g( ?i (2) ) ? ?(w (3,j) ? ?i (2) + b (3,j) ) (3) After more convolution and max-pooling operations, we obtain two feature vectors for the source sentence and the target phrase, respectively. Matching model. The matching score of a source sentence and a target phrase can be measured as the similarity between their feature vectors. Specifically, we use the multi-layer perceptron (MLP), a nonlinear function for similarity, to compute their matching score. First we use one layer to combine their feature vectors to get a hidden state h c : h c = ?(w c ? [x fi : y ?j ] + b c ) (4) Then we get the matching score from the MLP: s(x, y) = M LP (h c ) (5) 3 Training We employ a discriminative training strategy with a max-margin objective. Suppose we are given the following triples y + , y ? ) from the oracle, where x, y + , y ? are the feature vectors for f , ?+ , ? respectively. We have the ranking-based loss as objective: L ? (x, y + , y ? ) = max(0, 1+s(x, y ? )?s(x, y + )) (6 ) where s(x, y) is the matching score function defined in Eq. 5, ? consists of parameters for both the convolutional sentence model and MLP. The model is trained by minimizing the above objective, to encourage the model to assign higher matching scores to positive examples and to assign lower scores to negative examples. We use stochastic gradient descent (SGD) to optimize the model parameters ?. We train the CDCM model with a curriculum strategy to learn the contextdependent semantic similarity at the phrase level from easy (basic semantic similarities between the source and target phrase pair) to difficult (context-dependent semantic similarities for the same source phrase in varying contexts). 

 Curriculum Training Curriculum learning, first proposed by  Bengio et al. (2009)  in machine learning, refers to a sequence of training strategies that start small, learn easier aspects of the task, and then gradually increase the difficulty level. It has been shown that the curriculum learning can benefit the nonconvex training by giving rise to improved generalization and faster convergence. The key point is that the training examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. For each positive example (f , ?+ ), we have three types of negative examples according to the difficulty level of distinguishing the positive example from them: ? Easy: target phrases randomly chosen from the phrase table; ? Medium: target phrases extracted from the aligned target sentence for other non-overlap source phrases in the source sentence; ? Difficult: target phrases extracted from other candidates for the same source phrase. We want the CDCM model to learn the following semantic information from easy to difficult: ? the basic semantic similarity between the source sentence and target phrase from the easy negative examples; ? the general semantic equivalent between the source and target phrase pair from the medium negative examples; ? the context-dependent semantic similarities for the same source phrase in varying contexts from the difficult negative examples. Alg For each curriculum (lines 12-16), we compute the gradient of the loss objective L ? and learn ? using the SGD algorithm. Note that we meanwhile update the word embeddings to better capture the semantic equivalence across languages during training. If the loss function L ? reaches a local minima or the iterations reach the predefined number, we terminate this curriculum. 

 Related Work 

 Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, ,  and  Marton and Resnik (2008)  employed within-sentence contexts that consist of discrete words to guide rule matching.  Wu et al. (2014)  exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the phrase pairs and directly compute phrasal similarities with convolutional representations of the local contexts, integrating the strengths associated with the convolutional neural networks  (Collobert and Weston, 2008) . In recent years, there has also been growing interest in bilingual phrase representations that group phrases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space  (Gao et al., 2014; Zhang et al., 2014) . However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities.  Meng et al. (2015)  and  Zhang (2015)  have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of  Devlin et al. (2014)  to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 

 Experiments 

 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.  1  We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit  (Stolcke, 2002)  with modified Kneser-Ney Smoothing  (Kneser and Ney, 1995) . We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training  (Och, 2003)  to optimize the feature weights. For evaluation, case-insensitive NIST BLEU  (Papineni et al., 2002)  is used to measure translation performance. We perform a significance test using the sign-test approach  (Collins et al., 2005)  For training the neural networks, we use 4 convolution layers for source sentences and 3 convolution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding window k = 3, and the learning rate ? = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to  Yang et al. (2013) , and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 

 Evaluation of Translation Quality We have two baseline systems: ? Baseline: The baseline system is an opensource system of the phrase-based model -Moses  (Koehn et al., 2007)  with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. ? CICM (context-independent convolutional matching) model: Following the previous works  (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014) , we calculate the matching degree of a phrase pair without considering any contextual information. Each unique phrase pair serves as a positive example and a randomly selected target phrase from the phrase table is the corresponding negative example. The matching score is also introduced into Baseline as an additional feature. Table  1  summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the CDCM model significantly improves the translation quality on the overall test data (with gains of 1.0 BLEU points). The best improvement can be up to 1.4 BLEU points on MT04 with the fully trained CDCM. As expected, the translation performance is consistently increased with curriculum growing. This indicates that the CDCM model indeed captures the desirable semantic information by the curriculum learning from easy to difficult. Comparing with its context-independent counterpart (CICM, Row 2), the CDCM model shows significant improvement on all the test data consistently. We contribute this to the incorporation of useful discriminative information embedded in the local context. In addition, the performance of CICM is comparable with that of CDCM 1 . This is intuitive, because both of them try to capture the basic semantic similarity between the source and target phrase pair. One of the hypotheses we tested in the course of this research was disproved. We thought it likely that the difficult curriculum (CDCM 3 that distinguishs the correct translation from other candidates for a given context) would contribute most to the improvement, since this circumstance is more consistent with the real decoding procedure. This turned out to be false, as shown in Table  1 . One possible reason is that the "negative" examples (other candidates for the same source phrase) may share the same semantic meaning with the positive one, thus give a wrong guide in the supervised training. Constructing a reasonable set of negative examples that are more semantically different from the positive one is left for our future work. 

 Conclusion In this paper, we propose a context-dependent convolutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts. Experimental results show that our approach significantly improves the translation performance and obtains improvement of 1.0 BLEU scores on the overall test data. Integrating deep architecture into contextdependent translation selection is a promising way to improve machine translation. In the future, we will try to exploit contextual information at the target side (e.g., partial translations). Figure 1 : 1 Figure 1: Architecture of the CDCM model. The convolutional sentence model (bottom) summarizes the meaning of the tagged sentence and target phrase, and the matching model (top) compares the representations using a multi-layer perceptron. "/" indicates all-zero padding turned off by the gating function. 
