title
Minimum Imputed Risk: Unsupervised Discriminative Training for Machine Translation

abstract
Discriminative training for machine translation has been well studied in the recent past. A limitation of the work to date is that it relies on the availability of high-quality in-domain bilingual text for supervised training. We present an unsupervised discriminative training framework to incorporate the usually plentiful target-language monolingual data by using a rough "reverse" translation system. Intuitively, our method strives to ensure that probabilistic "round-trip" translation from a targetlanguage sentence to the source-language and back will have low expected loss. Theoretically, this may be justified as (discriminatively) minimizing an imputed empirical risk. Empirically, we demonstrate that augmenting supervised training with unsupervised data improves translation performance over the supervised case for both IWSLT and NIST tasks.

Introduction Missing data is a common problem in statistics when fitting the parameters ? of a model. A common strategy is to attempt to impute, or "fill in," the missing data  (Little and Rubin, 1987) , as typified by the EM algorithm. In this paper we develop imputation techniques when ? is to be trained discriminatively. We focus on machine translation (MT) as our example application. A Chinese-to-English machine translation system is given a Chinese sentence x and asked to predict its English translation y. This system employs statistical models p ? (y | x) whose parameters ? are discriminatively trained using bilingual sentence pairs (x, y). But bilingual data for such supervised training may be relatively scarce for a particular language pair (e.g., Urdu-English), especially for some topics (e.g., technical manuals) or genres (e.g., blogs). So systems seek to exploit additional monolingual data, i.e., a corpus of English sentences y with no corresponding source-language sentences x, to improve estimation of ?. This is our missing data scenario.  1  Discriminative training of the parameters ? of p ? (y | x) using monolingual English data is a curious idea, since there is no Chinese input x to translate. We propose an unsupervised training approach, called minimum imputed risk training, which is conceptually straightforward: First guess x (probabilistically) from the observed y using a reverse Englishto-Chinese translation model p ? (x | y). Then train the discriminative Chinese-to-English model p ? (y | x) to do a good job at translating this imputed x back to y, as measured by a given performance metric. Intuitively, our method strives to ensure that probabilistic "round-trip" translation from a targetlanguage sentence to the source-language and back again will have low expected loss. Our approach can be applied in an application scenario where we have (1) enough out-of-domain bilingual data to build two baseline translation systems, with parameters ? for the forward direction, and ? for the reverse direction; (2) a small amount of in-domain bilingual development data to discriminatively tune a small number of parameters in ?; and (3) a large amount of in-domain English monolingual data. The novelty here is to exploit (3) to discriminatively tune the parameters ? of all translation model components, 2 p ? (y|x) and p ? (y), not merely train a generative language model p ? (y), as is the norm. Following the theoretical development below, the empirical effectiveness of our approach is demonstrated by replacing a key supervised discriminative training step in the development of large MT systems -learning the log-linear combination of several component model scores (viewed as features) to optimize a performance metric (e.g. BLEU) on a set of (x, y) pairs -with our unsupervised discriminative training using only y. One may hence contrast our approach with the traditional supervised methods applied to the MT task such as minimum error rate training  (Och, 2003; , the averaged Perceptron  (Liang et al., 2006) , maximum conditional likelihood  (Blunsom et al., 2008) , minimum risk  (Smith and Eisner, 2006; Li and Eisner, 2009) , and MIRA  (Watanabe et al., 2007; Chiang et al., 2009) . We perform experiments using the open-source MT toolkit Joshua  (Li et al., 2009a) , and show that adding unsupervised data to the traditional supervised training setup improves performance. 

 Supervised Discriminative Training via Minimization of Empirical Risk Let us first review discriminative training in the supervised setting-as used in MERT  (Och, 2003)  and subsequent work. One wishes to tune the parameters ? of some complex translation system ? ? (x). The function ? ? , which translates Chinese x to English y = ? ? (x) need not be probabilistic. For example, ? may be the parameters of a scoring function used by ?, along with pruning and decoding heuristics, for extracting a high-scoring translation of x. The goal of discriminative training is to minimize the expected loss of ? ? (?), under a given taskspecific loss function L(y , y) that measures how L(? ? (x i ), y i ). (2) The search for ? * typically requires the use of numerical methods and some regularization. 5 3 Unsupervised Discriminative Training with Missing Inputs 

 Minimization of Imputed Risk We now turn to the unsupervised case, where we have training examples {y i } but not their corresponding inputs {x i }. We cannot compute the summand L(? ? (x i ), y i ) for such i in (2), since ? ? (x i ) requires to know x i . So we propose to replace 3 This goal is different from the minimum risk training of  Li and Eisner (2009)  in a subtle but important way. In both cases, ? * minimizes risk or expected loss, but the expectation is w.r.t. different distributions: the expectation in  Li and Eisner (2009)  is under the conditional distribution p(y | x), while the expectation in (1) is under the joint distribution p(x, y).  4  In the terminology of statistical decision theory, p(x, y) is a distribution over states of nature. We seek a decision rule ? ? (x) that will incur low expected loss on observations x that are generated from unseen states of nature.  5  To compensate for the shortcut of using the unsmoothed empirical distribution rather than a posterior estimate of p(x, y)  (Minka, 2000) , it is common to add a regularization term ||?|| 2 2 in the objective of (2). The regularization term can prevent overfitting to a training set that is not large enough to learn all parameters. L(? ? (x i ), y i ) with the expectation x p ? (x | y i ) L(? ? (x), y i ), (3) where p ? (? | ?) is a "reverse prediction model" that attempts to impute the missing x i data. We call the resulting variant of (2) the minimization of imputed empirical risk, and say that ? * = argmin ? 1 N N i=1 x p ? (x | y i ) L(? ? (x), y i ) (4) is the estimate with the minimum imputed risk 6 . The minimum imputed risk objective of (4) could be evaluated by brute force as follows. 1. For each unsupervised example y i , use the reverse prediction model p ? (? | y i ) to impute possible reverse translations X i = {x i1 , x i2 , . . .}, and add each (x ij , y i ) pair, weighted by p ? (x ij | y i ) ? 1, to an imputed training set . 2. Perform the supervised training of (2) on the imputed and weighted training data. The second step means that we must use ? ? to forward-translate each imputed x ij , evaluate the loss of the translations y ij against the corresponding true translation y i , and choose the ? that minimizes the weighted sum of these losses (i.e., the empirical risk when the empirical distribution p(x, y) is derived from the imputed training set). Specific to our MT task, this tries to ensure that probabilistic "roundtrip" translation, from the target-language sentence y i to the source-language and back again, will have a low expected loss.  7  The trouble with this method is that the reverse model p ? generates a weighted lattice or hypergraph X i encoding exponentially many translations of y i , and it is computationally infeasible to forwardtranslate each x ij ? X i . We therefore investigate several approximations to (4) in Section 3.4.  6  One may exploit both supervised data {(xi, yi)} and unsupervised data {yj} to perform semi-supervised training via an interpolation of (2) and (4). We will do so in our experiments. 7 Our approach may be applied to other tasks as well. For example, in a speech recognition task, ? ? is a speech recognizer that produces text, whereas p ? is a speech synthesizer that must produce a distribution over audio (or at least over acoustic features or phone sequences)  (Huang et al., 2010) . 

 The Reverse Prediction Model p ? A crucial ingredient in (4) is the reverse prediction model p ? (?|?) that attempts to impute the missing x i . We will train this model in advance, doing the best job we can from available data, including any outof-domain bilingual data as well as any in-domain monolingual data 8 x. In the MT setting, ? ? and p ? may have similar parameterization. One translates Chinese to English; the other translates English to Chinese. Yet the setup is not quite symmetric. Whereas ? ? is a translation system that aims to produce a single, low-loss translation, the reverse version p ? is rather a probabilistic model. It is supposed to give an accurate probability distribution over possible values x ij of the missing input sentence x i . All of these values are taken into account in (4), regardless of the loss that they would incur if they were evaluated for translation quality relative to the missing x i . Thus, ? does not need to be trained to minimize the risk itself (so there is no circularity). Ideally, it should be trained to match the underlying conditional distribution of x given y, by achieving a low conditional cross-entropy H(X | Y ) = ? x,y p(x, y) log p ? (x | y). (5) In practice, ? is trained by (empirically) minimiz- ing ? 1 M N j=1 log p ? (x j | y j ) + 1 2? 2 ? 2 2 on some bilingual data, with the regularization coefficient ? 2 tuned on held out data. It may be tolerable for p ? to impute mediocre translations x ij . All that is necessary is that the (forward) translations generated from the imputed x ij "simulate" the competing hypotheses that we would see when translating the correct Chinese input x i . 

 The Forward Translation System ? ? and The Loss Function L(? ? (x i ), y i ) The minimum empirical risk objective of (  2 ) is quite general and various popular supervised training methods  (Lafferty et al., 2001; Collins, 2002; Och, 2003; Crammer et al., 2006; Smith and Eisner, 2006)  can be formalized in this framework by choosing different functions for ? ? and L(? ? (x i ), y i ). The generality of (2) extends to our minimum imputed risk objective of (4). Below, we specify the ? ? and L(? ? (x i ), y i ) we considered in our investigation. 

 Deterministic Decoding A simple translation rule would define ? ? (x) = argmax y p ? (y | x) (6) If this ? ? (x) is used together with a loss function L(? ? (x i ), y i ) that is the negated BLEU score 9 , our minimum imputed risk objective of (  4 ) is equivalent to MERT  (Och, 2003)  on the imputed training data. However, this would not yield a differentiable objective function. Infinitesimal changes to ? could result in discrete changes to the winning output string ? ? (x) in (  6 ), and hence to the loss L(? ? (x), y i ).  Och (2003)  developed a specialized line search to perform the optimization, which is not scalable when the number of model parameters ? is large. 

 Randomized Decoding Instead of using the argmax of (  6 ), we assume during training that ? ? (x) is itself random, i.e. the MT system randomly outputs a translation y with probability p ? (y | x). As a result, we will modify our objective function of (4) to take yet another expectation over the unknown y. Specifically, we will replace L(? ? (x), y i ) in (4) with y p ? (y | x) L(y, y i ). (7) Now, the minimum imputed empirical risk objective of (4) becomes ? * = argmin ? 1 N N i=1 x,y p ? (x | y i ) p ? (y | x) L(y, y i ) (8) If the loss function L(y, y i ) is a negated BLEU, this is equivalent to performing minimum-risk training described by  (Smith and Eisner, 2006; Li and Eisner, 2009)  on the imputed data. 10 9 One can manipulate the loss function to support other methods that use deterministic decoding, such as Perceptron  (Collins, 2002)  and MIRA  (Crammer et al., 2006) .  10  Again, one may manipulate the loss function to support other probabilistic methods that use randomized decoding, such as CRFs  (Lafferty et al., 2001) . The objective function in (  8 ) is now differentiable, since each coefficient p ? (y | x) is a differentiable function of ?, and thus amenable to optimization by gradient-based methods; we use the L-BFGS algorithm  (Liu et al., 1989)  in our experiments. We perform experiments with the syntax-based MT system Joshua  (Li et al., 2009a) , which implements dynamic programming algorithms for second-order expectation semirings  (Li and Eisner, 2009)  to efficiently compute the gradients needed for optimizing (8). 

 Approximating p ? (x | y i ) As mentioned at the end of Section 3.1, it is computationally infeasible to forward-translate each of the imputed reverse translations x ij . We propose four approximations that are computationally feasible. Each may be regarded as a different approximation of p ? (x | y i ) in equations (  4 ) or (8). k-best. For each y i , add to the imputed training set only the k most probable translations {x i1 , . . . x ik } according to p ? (x | y i ). (These can be extracted from X i using standard algorithms  (Huang and Chiang, 2005) .) Rescale their probabilities to sum to 1. 

 Sampling. For each y i , add to the training set k independent samples {x i1 , . . . x ik } from the distribution p ? (x | y i ), each with weight 1/k. (These can be sampled from X i using standard algorithms  (Johnson et al., 2007) .) This method is known in the literature as multiple imputation  (Rubin, 1987) . Lattice. 11 Under certain special cases it is be possible to compute the expected loss in (3) exactly via dynamic programming. Although X i does contain exponentially many translations, it may use a "packed" representation in which these translations share structure. This representation may furthermore enable sharing work in forward-translation, so as to efficiently translate the entire set X i and obtain a distribution over translations y. Finally, the expected loss under that distribution, as required by equation (3), may also be efficiently computable. All this turns out to be possible if (a) the posterior distribution p ? (x | y i ) is represented by an un-ambiguous weighted finite-state automaton X i , (b) the forward translation system ? ? is structured in a certain way as a weighted synchronous context-free grammar, and (c) the loss function decomposes in a certain way. We omit the details of the construction as beyond the scope of this paper. In our experimental setting described below, (b) is true (using Joshua), and (c) is true (since we use a loss function presented by  Tromble et al. (2008)  that is an approximation to BLEU and is decomposable). While (a) is not true in our setting because X i is a hypergraph (which is ambiguous),  Li et al. (2009b)  show how to approximate a hypergraph representation of p ? (x | y i ) by an unambiguous WFSA. One could then apply the construction to this WFSA 12 , obtaining an approximation to (3). Rule-level Composition. Intuitively, the reason why the structure-sharing in the hypergraph X i (generated by the reverse system) cannot be exploited during forward translating is that when the forward Hiero system translates a string x i ? X i , it must parse it into recursive phrases. But the structure-sharing within the hypergraph of X i has already parsed x i into recursive phrases, in a way determined by the reverse Hiero system; each translation phrase (or rule) corresponding to a hyperedge. To exploit structure-sharing, we can use a forward translation system that decomposes according to that existing parse of x i . We can do that by considering only forward translations that respect the hypergraph structure of X i . The simplest way to do this is to require complete isomorphism of the SCFG trees used for the reverse and forward translations. In other words, this does round-trip imputation (i.e., from y to x, and then to y ) at the rule level. This is essentially the approach taken by . 

 The Log-Linear Model p ? We have not yet specified the form of p ? . Following much work in MT, we begin with a linear model score (x, y) = ? ? f (x, y) = k ? k f k (x, y) (9) where f (x, y) is a feature vector indexed by k. Our deterministic test-time translation system ? ? simply outputs the highest-scoring y for fixed x. At training time, our randomized decoder (Section 3.3.2) uses the Boltzmann distribution (here a log-linear model)  (x,y)  y e ?score(x,y ) (10) The scaling factor ? controls the sharpness of the training-time distribution, i.e., the degree to which the randomized decoder favors the highest-scoring y. For large ?, our training objective approaches the imputed risk of the deterministic test-time system while remaining differentiable. p ? (y | x) = e ?score(x,y) Z(x) = e ?score In a task like MT, in addition to the input x and output y, we often need to introduce a latent variable d to represent the hidden derivation that relates x to y. A derivation d represents a particular phrase segmentation in a phrase-based MT system  (Koehn et al., 2003)  and a derivation tree in a typical syntaxbased system  (Galley et al., 2006; Chiang, 2007) . We change our model to assign scores not to an (x, y) pair but to the detailed derivation d; in particular, now the function f that extracts a feature vector can look at all of d. We replace y by d in (  9 )-(  10 ), and finally define p ? (y|x) by marginalizing out d, p ? (y | x) = d?D(x,y) p ? (d | x) (11) where D(x, y) represents the set of derivations that yield x and y. 

 Minimum Imputed Risk vs. EM The notion of imputing missing data is familiar from other settings  (Little and Rubin, 1987) , particularly the expectation maximization (EM) algorithm, a widely used generative approach. So it is instructive to compare EM with minimum imputed risk. One can estimate ? by maximizing the loglikelihood of the data {(x i , y i ), i = 1, . . . , N } as argmax ? 1 N N i=1 log p ? (x i , y i ). ( 12 ) If the x i 's are missing, EM tries to iteratively maximize the marginal probability: argmax ? 1 N N i=1 log x p ? (x, y i ). ( 13 ) The E-step of each iteration comprises computing x p ?t (x | y i ) log p ? (x, y i ), the expected loglikelihood of the complete data, where p ?t (x | y i ) is the conditional part of p ?t (x, y i ) under the current iterate ? t , and the M-step comprises maximizing it: ? t+1 = argmax ? 1 N N i=1 x p ?t (x | y i ) log p ? (x, y i ). (14) Notice that if we replace p ?t (x|y i ) with p ? (x | y i ) in the equation above, and admit negated loglikelihood as a loss function, then the EM update (  14 ) becomes identical to (4). In other words, the minimum imputed risk approach of Section 3.1 differs from EM in (i) using an externally-provided and static p ? , instead of refining it at each iteration based on the current p ?t , and (ii) using a specific loss function, namely negated log-likelihood. So why not simply use the maximum-likelihood (EM) training procedure for MT? One reason is that it is not discriminative: the loss function (e.g. negated BLEU) is ignored during training. A second reason is that training good joint models p ? (x, y) is computationally expensive. Contemporary MT makes heavy use of log-linear probability models, which allow the system designer to inject phrase tables, linguistic intuitions, or prior knowledge through a careful choice of features. Computing the objective function of (  14 ) in closed form is difficult if p ? is an arbitrary log-linear model, because the joint probability p ? (x i , y i ) is then defined as a ratio whose denominator Z ? involves a sum over all possible sentence pairs (x, y) of any length. By contrast, our discriminative framework will only require us to work with conditional models. While conditional probabilities such as p ? (x | y) and p ? (y | x) are also ratios, computing their denominators only requires us to sum over a packed forest of possible translations of a given y or x.  13  In summary, EM would impute missing data using p ? (x | y) and predict outputs using p ? (y | x), both being conditional forms of the same joint model p ? (x, y). Our minimum imputed risk training method is similar, but it instead uses a pair of separately parameterized, separately trained models p ? (x | y) and p ? (y | x). By sticking to conditional models, we can efficiently use more sophisticated model features, and we can incorporate the loss function when we train ?, which should improve both efficiency and accuracy at test time. 

 Experimental Results We report results on Chinese-to-English translation tasks using Joshua  (Li et al., 2009a) , an open-source implementation of Hiero  (Chiang, 2007) . 

 Baseline Systems 

 IWSLT Task We train both reverse and forward baseline systems. The translation models are built using the corpus for the IWSLT 2005 Chinese to English translation task  (Eck and Hori, 2005) , which comprises 40,000 pairs of transcribed utterances in the travel domain. We use a 5-gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998), trained on the English (resp. Chinese) side of the bitext. We use a standard training pipeline and pruning settings recommended by  (Chiang, 2007) . 

 NIST Task For the NIST task, the TM is trained on about 1M parallel sentence pairs (about 28M words in each language), which are sub-sampled from corpora distributed by LDC for the NIST MT evaluation using a sampling method implemented in Joshua. We also used a 5-gram language model, trained on a data set consisting of a 130M words in English Gigaword (LDC2007T07) and the bitext's English side. 

 Feature Functions We use two classes of features f k for discriminative training of p ? as defined in (9). 

 Regular Hiero Features We include ten features that are standard in Hiero  (Chiang, 2007) . In particular, these include one baseline language model feature, three baseline translation models, one word penalty feature, three features to count how many rules with an arity of zero/one/two are used in a derivation, and two features to count how many times the unary and binary glue rules in Hiero are used in a derivation. 

 Target-rule Bigram Features In this paper, we do not attempt to discriminatively tune a separate parameter for each bilingual rule in the Hiero grammar. Instead, we train several hundred features that generalize across these rules. For each bilingual rule, we extract bigram features over the target-side symbols (including nonterminals and terminals). For example, if a bilingual rule's target-side is "on the X 1 issue of X 2 " where X 1 and X 2 are non-terminals (with a position index), we extract the bigram features on the, the X, X issue, issue of, and of X. (Note that the position index of a non-terminal is ignored in the feature.) Moreover, for the terminal symbols, we will use their dominant POS tags (instead of the symbol itself). For example, the feature the X becomes DT X. We use 541 such bigram features for IWSLT task (and 1023 such features for NIST task) that fire frequently. 

 Data Sets for Discriminative Training 

 IWSLT Task In addition to the 40,000 sentence pairs used to train the baseline generative models (which are used to compute the features f k ), we use three bilingual data sets listed in Table  1 , also from IWSLT, for discriminative training: one to train the reverse model p ? (which uses only the 10 standard Hiero features as described in Section 5.2.1), 14 one to train the forward model ? ? (which uses both classes of features described in Section 5.2, i.e., 551 features in total), and one for test. Note that the reverse model ? is always trained using the supervised data of Dev ?, while the forward model ? may be trained in a supervised or semisupervised manner, as we will show below. In all three data sets, each Chinese sentence x i has 16 English reference translations, so each y i is actually a set of 16 translations. When we impute data from y i (in the semi-supervised scenario), we actually impute 16 different values of x i , by using p ? to separately reverse translate each sentence in y i . This effectively adds 16 pairs of the form (x i , y i ) to the training set (see section 3.4), where each x i is a different input sentence (imputed) in each case, but y i is always the original set of 16 references. 

 NIST Task For the NIST task, we use MT03 set (having 919 sentences) to tune the component parameters in both the forward and reverse baseline systems. Additionally, we use the English side of MT04 (having 1788 sentences) to perform semi-supervised tuning of the forward model. The test sets are MT05 and MT06 (having 1082 and 1099 sentences, respectively). In all the data sets, each source sentence has four reference translations. 

 Main Results We compare two training scenarios: supervised and semi-supervised. The supervised system ("Sup") carries out discriminative training on a bilingual data set. The semi-supervised system ("+Unsup") additionally uses some monolingual English text for discriminative training (where we impute one Chinese translation per English sentence). Tables  2 and 3  report the results for the two tasks under two training scenarios. Clearly, adding unsupervised data improves over the supervised case, by at least 1.3 BLEU points in IWSLT and 0.5 BLEU in NIST. 

 Results for Analysis Purposes Below, we will present more results on the IWSLT data set to help us understand the behavior of the for each English sentence we impute the 1-best Chinese translation. "WLM" means a Chinese language model is used in the reverse system, while "NLM" means no Chinese language model is used. In addition to reporting the BLEU score on Eval ?, we also report "Imputed-CN BLEU", the BLEU score of the imputed Chinese sentences against their corresponding Chinese reference sentences. BLEU point in the forward translations. Still, even with the worse imputation (in the case of "NLM"), our forward translations improve as we add more monolingual data. 

 Imputation with Different k-best Sizes In all the experiments so far, we used the reverse translation system to impute only a single Chinese translation for each English monolingual sentence. This is the 1-best approximation of section 3.4. Table  5  shows (in the fully unsupervised case) that the performance does not change much as k increases.  16  This may be because that the 5-best sentences are likely to be quite similar to one another  (May and Knight, 2006) . Imputing a longer k-best list, a sample, or a lattice for x i (see section 3.4) might achieve more diversity in the training inputs, which might make the system more robust. 

 Conclusions In this paper, we present an unsupervised discriminative training method that works with missing inputs. The key idea in our method is to use a reverse model to impute the missing input from the observed output. The training will then forward translate the imputed input, and choose the parameters of the forward model such that the imputed risk (i.e., the expected loss of the forward translations with respect to the observed output) is minimized. This matches the intuition that the probabilistic "roundtrip" translation from the target-language sentence to the source-language and back should have low expected loss. We applied our method to two Chinese to English machine translation tasks (i.e. IWSLT and NIST). We showed that augmenting supervised data with unsupervised data improved performance over the supervised case (for both tasks). Our discriminative model used only a small amount of training data and relatively few features. In future work, we plan to test our method in settings where there are large amounts of monolingual training data (enabling many discriminative features). Also, our experiments here were performed on a language pair (i.e., Chinese to English) that has quite rich bilingual resources in the domain of the test data. In future work, we plan to consider lowresource test domains and language pairs like Urdu-English, where bilingual data for novel domains is sparse. Table 1 : 1 IWSLT Data sets used for discriminative training/test. Dev ? is used for discriminatively training of the reverse model ?, Dev ? is for the forward model, and Eval ? is for testing. The star * for Dev ? emphasizes that some of its Chinese side will not be used in the training (see Table2for details). Data set Purpose # of sentences Chinese English Dev ? training ? 503 503?16 Dev ? Eval ? training ? testing 503  *  506 503?16 506?16 

 Table 4 : 4 BLEU scores for unsupervised training with/without using a language model in the reverse system. A data size of 101 means that we use only the English sentences from a subset of Dev ? containing 101 Chinese sentences and 101?16 English translations; Data size Imputed-CN BLEU Test-EN BLEU WLM NLM WLM NLM 101 11.8 3.0 48.5 46.7 202 11.7 3.2 48.9 47.6 303 13.4 3.5 48.8 47.9 

 Table 5 : 5 BLEU scores for unsupervised training with different k-best sizes. We use 101?16 monolingual English sentences, and for each English sentence we impute the k-best Chinese translations using the reverse system. Training scenario Test BLEU Unsup, k=1 48.5 Unsup, k=2 48.4 Unsup, k=3 48.9 Unsup, k=4 48.5 Unsup, k=5 48.4 

			 Contrast this with traditional semi-supervised training that looks to exploit "unlabeled" inputs x, with missing outputs y. 

			 Note that the extra monolingual data is used only for tuning the model weights, but not for inducing new phrases or rules. 

			 In a translation task from x to y, one usually does not make use of in-domain monolingual data x. But we can exploit x to train a language model p ? (x) for the reverse translation system, which will make the imputed xij look like true Chinese inputs. 

			 The lattice approximation is presented here as a theoretical contribution, and we do not empirically evaluate it since its implementation requires extensive engineering effort that is beyond the main scope of this paper. 

			 Note that the forward translation of a WFSA is tractable by using a lattice-based decoder such as that by Dyer et al. (2008) . 

			 Analogously, discriminative CRFs have become more popular than generative HMMs because they permit efficient training even with a wide variety of log-linear features (Lafferty et al., 2001) . 

			 Ideally, we should train ? to minimize the conditional cross-entropy (5) as suggested in section 3.2. In the present results, we trained ? discriminatively to minimize risk, purely for ease of implementation using well versed steps. 

			 The BLEU scores are low even with the language model because only one Chinese reference is available for scoring. 

			 In the present experiments, however, we simply weighted all k imputed translations equally, rather than in proportion to their posterior probabilities as suggested in Section 3.4.
