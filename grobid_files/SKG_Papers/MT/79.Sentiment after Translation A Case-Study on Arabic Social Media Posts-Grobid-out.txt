title
Sentiment after Translation: A Case-Study on Arabic Social Media Posts

abstract
When text is translated from one language into another, sentiment is preserved to varying degrees. In this paper, we use Arabic social media posts as stand-in for source language text, and determine loss in sentiment predictability when they are translated into English, manually and automatically. As benchmarks, we use manually and automatically determined sentiment labels of the Arabic texts. We show that sentiment analysis of English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We discover that even though translation significantly reduces the human ability to recover sentiment, automatic sentiment systems are still able to capture sentiment information from the translations.

Introduction Automatic sentiment analysis of text, especially social media posts, has a number of applications in commerce, public health, and public policy development. However, a vast majority of prior research on automatic sentiment analysis has been on English texts. Furthermore, many sentiment resources essential to automatic sentiment analysis (e.g., sentiment lexicons) exist only in English. Thus there is a growing need for effective methods for analyzing text from other languages such as Arabic and Chinese, especially posts on social media. There has also been marked progress in automatic translation of texts, especially from other languages into English. Thus, instead of building source-language specific sentiment analysis systems, one can translate the texts into English and use an English sentiment analysis system. However, it is widely believed that aspects of sentiment may be lost in translation, especially in automatic translation. Though, the extent of this loss, in terms of drop in accuracy of automatic sentiment systems remains undetermined. This paper analyzes several methods available in annotating non-English texts for sentiment: ? Use a source-language sentiment analysis system. ? Run an English sentiment analysis system on manually created English translations of source language text. ? Run an English sentiment analysis system on automatically generated English translations of source language text. In our experiments, we use Arabic social media posts as a specific instance of the source language text. We use state-of-the-art Arabic and English sentiment analysis systems as well as a state-of-theart Arabic-to-English translation system. We outline the advantages and disadvantages of each of the methods listed above, and more importantly conduct experiments to determine accuracy of sentiment labels obtained using each of these methods. As benchmarks we use manually and automatically determined sentiment labels of the Arabic tweets. These results will help users determine methods best suited for their particular needs. Along the way, we answer several research questions such as: 1. What sentiment prediction accuracy is expected when Arabic blog posts and tweets are translated into English (using the current stateof-art techniques), and then run through a stateof-the-art English sentiment analysis system? 2. How does this performance compare with that of a current state-of-the-art Arabic sentiment system? 3. What is the loss in sentiment predictability when translating Arabic text into English automatically vs. manually? 4. How difficult is it for humans to determine sentiment of automatically translated text? 5. When dealing with translated text, which is more accurate at determining the sentiment of Arabic text: (1) automatic sentiment analysis of the translated text, or (2) human annotation of the translated text for sentiment? The inferences drawn from these experiments do not necessarily apply to language pairs other than Arabic-English. Languages can differ significantly in terms of characteristics that impact accuracy of an automatic sentiment analysis system. Our goal here specifically is to understand sentiment predictability of Arabic dialectal text on translation. However, a similar set of experiments can be used for other language pairs as well to determine the impact of translation on sentiment. Through our experiments on two different datasets, we show that sentiment analysis of English translations of Arabic texts produces competitive results, w.r.t. Arabic sentiment analysis. We also show that translation (both manual and automatic) introduces marked changes in sentiment carried by the text; positive and negative texts can often be translated into texts that are neutral. We also find that certain attributes of automatically translated text that mislead humans with regards to the true sentiment of the source text, do not seem to affect the automatic sentiment analysis system. In the process of developing these experiments to study how translation alters sentiment, we created a state-of-the-art Arabic sentiment analysis system by porting NRC-Canada's competition winning system  (Kiritchenko et al., 2014)  to Arabic. We also created a substantial amount of sentiment labeled data pertaining to Arabic social media texts and their English translations which is made freely available. 1 1 http://www.purl.com/net/ArabicSentiment This is the first such resource where text in one language and its translations into another language (both manually and automatically produced) are each manually labeled for sentiment. 

 Related Work 2.1 Sentiment Analysis of English Social Media Sentiment analysis systems have been applied to many different kinds of texts including customer reviews, newspaper headlines  (Bellegarda, 2010) , novels  (Boucouvalas, 2002; Mohammad and Yang, 2011) , emails  (Liu et al., 2003; Mohammad and Yang, 2011) , blogs  (Neviarouskaya et al., 2011), and tweets (Mohammad, 2012) . Often these systems have to cater to the specific needs of the text such as formality versus informality, length of utterances, etc. Sentiment analysis systems developed specifically for tweets include those by  Go et al. (2009) ,  Pak and Paroubek (2010) ,  Agarwal et al. (2011), and Thelwall et al. (2011) . A survey by  Mart?nez-C?mara et al. (2012)  provides an overview of the research on sentiment analysis of tweets. In the last two years, several shared tasks on sentiment analysis were organized by the Conference on Semantic Evaluation Exercises (SemEval), which allowed for comparison of different approaches on common datasets from different domains  (Wilson et al., 2013; Rosenthal et al., 2014; Pontiki et al., 2014) . The NRC-Canada system  (Kiritchenko et al., 2014)  ranked first in these competitions, and we use it in our experiments. Details of the system are described in Section 6. 

 Sentiment Analysis of Arabic Social Media Sentiment analysis of Arabic social media texts has several challenges. The text is often in a regional Arabic dialect rather than Modern Standard Arabic (MSA). Unlike MSA which is a standardized form of Arabic, dialectal Arabic is the spoken form of Arabic and lacks strict writing standards. The text often includes words from languages other than Arabic and multiple scripts may be used to express Arabic and foreign words. In addition, Arabic is a morphologically complex language, thus having a lexicon of word-sentiment associations that covers all different surface forms becomes a cumbersome task. Negation in MSA is expressed through negation par-ticles, but in some dialects (Egyptian) it is expressed using suffixes at the end of the word. We refer the reader to  Mourad and Darwish (2013)  for more details on these issues. There have been a few studies tackling sentiment analysis of Arabic texts  (Ahmad et al., 2006; Badaro et al., 2014) . The ones most closely related to our work are the studies of sentiment analysis of Arabic social media  (Al-Kabi et al., 2013; El-Beltagy and Ali, 2013; Mourad and Darwish, 2013; Abdul-Mageed et al., 2014) . Here we review existing Arabic sentiment analysis systems that were designed specifically for Arabic social media datasets.  Abdul-Mageed et al. (2014)  trained an SVM classifier on a manually labeled dataset and applied a two-stage classification that first separates subjective from objective sentences and then classifies the subjective into positive or negative instances. The authors have compiled several datasets from multiple social media resources that include chatroom messages, tweets, forum posts, and Wikipedia Talk pages. However, these resources have not been made publicly available yet.  Mourad and Darwish (2013)  trained SVM and Naive Bayes classifiers on Arabic tweets annotated by two native Arabic speakers. We compare our system's performance to theirs in Section 7. Refaee and Rieser (2014b) manually annotated tweets for sentiment by two native Arabic speakers. They used an SVM to classify tweets in a twostage approach, polar vs neutral, then positive vs. negative. The authors shared their data with us and we test our system on their dataset. However, the dataset they provided us is a larger superset than the one they had originally used  (Refaee and Rieser, 2014a) . Thus, the results of sentiment systems on the two sets are not directly comparable. 

 Multilingual Sentiment Analysis Work on multilingual sentiment analysis has mainly addressed mapping sentiment resources from English into morphologically complex languages.  Mihalcea et al. (2007)  used English resources to automatically generate a Romanian subjectivity lexicon using an English-Romanian dictionary. The generated lexicon is then used to classify Romanian text.  Wan (2008)  translated Chinese customer reviews to English using a machine trans-lation system. The translated reviews are then classified with a rule-based system that relies on English lexicons. A higher accuracy is achieved by using ensemble methods and combining knowledge from Chinese and English resources. Balahur and Turchi (2014) conducted a study to assess the performance of statistical sentiment analysis techniques on machine-translated texts. Opinionbearing phrases from the New York Times text corpus  (2002) (2003) (2004) (2005)  were automatically translated using publicly available machine-translation engines (Google, Bing, and Moses). Then, the accuracy of a sentiment analysis system trained on original English texts was compared to the accuracy of the system trained on automatic translations to German, Spanish, and French. The authors concluded that the quality of machine translation is sufficient for sentiment analysis to be performed on automatically translated texts without a substantial loss in accuracy. Contrary to that work, our study uses both manual and automatic translations as well as both manual and automatic sentiment assignments to systematically examine the effect of translation on sentiment. Additionally, we deal with noisy social media texts as opposed to more polished news media texts. There exists research on using sentiment analysis to improve machine translation  (Chen and Zhu, 2014) , but that is beyond the scope of this paper. 

 Method for Determining Sentiment Predictability on Translation In order to systematically study the impact of translation on sentiment analysis, we propose the following experimental setup: ? Identify or compile an Arabic social media dataset. We will refer to it as Ar. (Ar comes from the first two letter of Arabic.) ? Manually translate Ar into English. We will refer to these English translations as En(Manl.Trans.) [Manl. is for manual, and Trans. is for translations.] ? Automatically translate Ar into English. We will refer to these English translations as En(Auto.Trans.) [Auto. is for automatic.] ? Manually annotate Ar. for sentiment. We will refer to the sentiment-labeled dataset as Ar(Manl.Sent.) ? Manually annotate all English datasets [En(Manl.Trans.) and En(Auto.Trans.)] for sentiment, creating En(Manl.Trans., Manl.Sent.) and En(Auto.Trans., Manl.Sent.), respectively. ? Run a state-of-the-art Arabic sentiment analysis system on Ar, creating Ar(Auto.Sent.) ? Run a state-of-the-art English sentiment analysis system on all the English datasets [En(Manl.Trans.) and En(Auto.Trans.)], creating En(Manl.Trans., Auto.Sent.) and En(Auto.Trans., Auto.Sent.), respectively. Figure  1  depicts this setup. Once the various sentiment-labeled datasets are created, we can compare pairs of datasets to draw inferences. For example, comparing the labels for Ar(Manl.Sent.) and En(Manl.Trans., Manl.Sent.) will show how different the sentiment labels tend to be when text is translated from Arabic to English. The comparison will also show, for example, whether positive tweets tend to often be translated into neutral tweets, and to what extent. The results will also show how feasible it is to first translate Arabic text into English and then use automatic sentiment analysis (Ar(Manl.Sent.) vs. En(Auto.Trans., Auto.Sent.)). In Section 8, we provide an analysis of several such comparisons for two different Arabic social media datasets. DATA: Since manual translation of text from Arabic to English is a costly exercise, we chose, for our experiments, an existing Arabic social media dataset that has already been translated -the BBN Arabic-Dialect/English Parallel Text  (Zbib et al., 2012) . 2 It contains about 3.5 million tokens of Arabic dialect sentences and their English translations. We use a randomly chosen subset of 1200 Levantine dialectal sentences, which we will refer to as the BBN posts or BBN dataset, in our experiments. Additionally, we also conduct experiments on a dataset of 2000 tweets originating from Syria (a country where Levantine dialectal Arabic is commonly spoken). These tweets were collected in May 2014 by polling the Twitter API. We will refer to this dataset as the Syrian tweets or Syrian dataset. Note, however, that manual translations of the Syrian dataset are not available. The experimental setup described above involves several component tasks: generating translations manually and automatically (Section 4), manually annotating Arabic and English texts for sentiment (Section 5), automatic sentiment analysis of English texts (Section 6), and automatic sentiment analysis of Arabic texts (Section 7). 

 Generating English Translations The BBN dialectal Arabic dataset comes with manual translations into English. We generate automatic translations of the Arabic BBN posts and the Syrian tweets, by training a multi-stack phrase-based machine translation system to translate from Arabic to English. Our in-house system is quite similar to  Cherry and Foster (2012) . This statistical machine translation (SMT) system is trained on data from OpenMT 2012. We preprocess the training data by segmenting the Arabic source side of the training data with MADA 3.2  (Habash et al., 2009) , using Penn Arabic Treebank (PATB) segmentation scheme as recommended by El  Kholy and Habash (2012) . The Arabic script is further normalized by converting different forms of Alif and Ya to bare Alif and dotless Ya . The different forms are used interchangeably, and normalization decreases the sparcity of Arabic tokens and improves translation. The English side of the training data is lowercased and tokenized by stripping punctuation marks. We set the decoder's stack size to 10000 and distortion limit to 7. We replace the out-of-vocabulary words in the translated text with UNKNOWN token (which is shown to the annotators). The decoder's log-linear model is tuned with MIRA  (Chiang et al., 2008; Cherry and Foster, 2012) . A KN-smoothed 5gram language model is trained on the English Gigaword and the target side of the parallel data. 

 Creating sentiment labeled data in Arabic and English Manual sentiment annotations were performed on the crowdsourcing platform CrowdFlower 3 for three BBN datasets and two Syrian datasets: 1. Original Arabic posts (BBN and Syria datasets), annotated by Arabic speakers. 2. Manual English translations of Arabic posts, annotated by English speakers (only for BBN dataset). 3. Automatic English translations of Arabic posts (BBN and Syria datasets), annotated by English speakers. Each post was annotated by at least ten annotators and the majority sentiment label was chosen. Table 1 shows the class distribution of sentiment labels in various datasets. Observe from rows a and d that neutral tweets constitute only about 10% of the data in both BBN and Syria datasets. The Syrian tweets have a much higher percentage of negative posts, whereas in the BBN data, the percentages of positive and negative posts are comparable. (Arabic tweets in general tend to be much more skewed to the negative class than Arabic blog post sentences.) Rows b, c, and e show that translated texts tend to lose some of the sentiment information and there is a relatively higher percentage of neutral instances in the translated text than in the original text. For each post, we determine the count of the most frequent annotation divided by the total number of annotations. This score is averaged for all posts to determine the inter-annotator agreement shown in the last column of Table  1 . We use this agreement score as benchmark to compare performance of automatic sentiment systems (described below). 

 English Sentiment Analysis We use the English-language sentiment analysis system developed by NRC-Canada  (Kiritchenko et al., 2014)  in our experiments. This system obtained highest scores in two recent international competitions on sentiment analysis of tweets -SemEval-2013 Task 2 and SemEval-2014 Task 9  (Wilson et al., 2013; Rosenthal et al., 2014) . We briefly describe the system below; for more details, we refer the reader to  Kiritchenko et al. (2014) . A linear-kernel Support Vector Machine  (Chang and Lin, 2011)  classifier is trained on the available training data. The classifier leverages a variety of surface-form, semantic, and sentiment lexicon features described below. The sentiment lexicon features are derived from existing, generalpurpose, manual lexicons, namely NRC Emotion Lexicon  (Mohammad and Turney, 2010; Mohammad and Turney, 2013) , Bing Liu's Lexicon  (Hu and Liu, 2004) , and MPQA Subjectivity Lexicon  (Wilson et al., 2005) , as well as automatically generated, tweet-specific lexicons, Hashtag Sentiment Lexicon and Sentiment140 Lexicon  (Kiritchenko et al., 2014 ). 4 

 Generating English Sentiment Lexicon Ablation experiments in  showed that their sentiment system benefited most from the use of the Hashtag Sentiment Lexicon. The lexicon was created as follows. A list of 77 seed words, which are synonyms of positive and negative, was compiled from the Roget's Thesaurus. Then, the Twitter API was polled to collect tweets that had these words as hashtags. A tweet is considered positive if it has a positive hashtag and negative if it has a negative hashtag. For each term in the tweet set, a sentiment score is computed by measuring the PMI (pointwise mutual information) between the term and the positive and negative categories: SenScore (w) = P M I(w, pos) ? P M I(w, neg) (1) where w is a term in the lexicon. P M I(w, pos) is the PMI score between w and the positive class, and P M I(w, neg) is the PMI score between w and the negative class. A positive SenScore (w) suggests that the word is associated with positive sentiment and a negative score suggests that the word is associated with negative sentiment. The magnitude indicates the strength of the association. 

 Pre-processing and Feature Generation The following pre-processing steps are performed. URLs and user mentions are normalized to http://someurl and @someuser, respectively. Tweets are tokenized and part-of-speech tagged with the CMU Twitter NLP tool  (Gimpel et al., 2011) . Then, each tweet is represented as a feature vector. 

 The features: -Word and character ngrams; -POS: # occurrences of each part-of-speech tag; -Negation: # negated contexts. Negation also affects the ngram features: a word w becomes w NEG in a negated context; -Automatic sentiment lexicons: For each token w occurring in a tweet, its sentiment score score(w) is used to compute: # tokens with score(w) = 0; the total score = w?tweet score(w); the maximal score = max w?tweet score(w); the score of the last token in the tweet. -Manually created sentiment lexicons: For each of the three manual sentiment lexicons, the following features are computed: the sum of positive and the sum of negative scores for tweet tokens in affirmative contexts and in negated contexts, separately. 7 Arabic Sentiment Analysis 

 Building an Arabic Sentiment System We built an Arabic sentiment analysis system by reconstructing the NRC-Canada English system to deal with Arabic text. It extracts the same feature set as described in Section 6.2. We also generated a word-sentiment association lexicon as described in Section 6.1, but for Arabic words from Arabic tweets (more details in sub-section below). We preprocess Arabic text by tokenizing with CMU Twitter NLP tool to deal with specific tokens such as URLs, usernames, and emoticons. Then we use MADA to generate lemmas. Finally, we normalize different forms of Alif and Ya to bare Alif and dotless Ya to decrease token sparcity in Arabic datasets. 

 Generating Arabic Sentiment Lexicon We translated 77 positive and negative seed words used to generate the English NRC Hashtag Sentiment Lexicon into Arabic using Google Translate. Among the several translations provided by it, we chose words that were less ambiguous and tended to have strong sentiment in Arabic texts. To increase the coverage of our seed list, we manually added different inflections for these translations. We polled the Twitter API for the period of June to August 2014 and collected tweets with #(keyword). After filtering out duplicate tweets and retweets, we ended up with 163,944 positive unique tweets and 37,848 negative unique tweets. Then for each word w, SenScore (w) was calculated just as described in Section 6.1.  

 Evaluation We tested the Arabic sentiment system on two existing Arabic datasets  (Mourad and Darwish (2013)  (MD) and Refaee and Rieser (2014a) (RR)) and two newly sentiment-annotated Arabic datasets (BBN and Syria). Table  2  shows results of ten-fold crossvalidation experiments on each of the datasets. For MD and RR, the presented results are for the twoclass problem (positive vs. negative) to allow for comparison with prior published results. For BBN and Syria, the results are shown for the case where the system has to identify one of three classes: positive, negative, or neutral. Human agreement scores are shown where available. Note that the accuracy of our system is higher than previously published results on the MD dataset. The only previously published results on the RR dataset are on a small subset (about 1000 instances) for which  Refaee and Rieser (2014a)  obtained an accuracy of 87%. The results in Table  2  are for a larger dataset and so not directly comparable. 

 Sentiment After Translation Using the methods and systems described in Sections 4, 5, 6, and 7, we generated all the manually and automatically labeled datasets mentioned in Section 3's Experimental Setup. Table  3  shows the distribution of positive, negative, and neutral classes in datasets that have been automatically labeled with sentiment. These percentages can be compared with those in Table  1  (rows a and d) which show the true sentiment distribution in the BBN and Syria datasets. Observe that the automatic system has difficulty in assigning neutral class to posts. This is probably because of the small percentage (about 10%) of neutral tweets in the training data. Also notice that the system predominantly guesses negative, which is also a reflection of the distribution in the training data. The strong bias to negatives is lessened in the English translations. Main Result: Tables  4 and 5  show how similar the sentiment labels are across various pairs of datasets for the BBN posts and the Syrian posts, respectively. For example, row a. in Table  4  shows the comparison between Arabic tweets that were manually annotated for sentiment and those that were automatically labeled for sentiment by our Arabic sentiment analysis system. Column 2 shows the percentage of instances where the sentiment labels match across the two datasets being compared. For row a. the match percentage of 63.89% represents the accuracy of the automatic sentiment analysis system on the Arabic BBN posts. Row b. shows the difference in labels when text is manually translated from Arabic to English, even though sentiment labeling in both Arabic and English is done manually. Observe that the two labels match only 71.31% of the time. However, the agreement among human sentiment annotators on original Arabic texts was only 73.8%. So, the English translation does affect sentiment, but not dramatically. Row c. shows results for when the manually translated text is run through an English sentiment analysis system and the labels are compared against Ar(Manl.Sent.) Observe that the match for this pair is 68.65%, which is not too much lower than 71.31% obtained by manual sentiment labeling. This shows   6 : Examples where the automatic translation was annotated a sentiment different the sentiment of the original Arabic tweet, but whose original sentiment was correctly predicted by the English sentiment system. The manual translations are also listed for reference. One reason why the automatic sentiment analysis system correctly annotates several automatically translated instances (where manual annotations of the translation may fail), is that the system can learn an appropriate model even from mistranslated text -especially when automatic translation makes consistent errors. For example, (Oh God grant victory to) has been consistently translated to God forsake. All tweets having this phrase are correctly annotated as positive by our system, but were marked negative by the human annotators. 

 Caveats: The automatic systems employed in these experiments, i.e., Arabic sentiment analysis, English sentiment analysis, and SMT systems, exhibit state-of-the-art performance; nevertheless, further improvements are possible. The Arabic sentiment system will benefit from extended sentiment lexicons and features derived specifically for the Arabic language. The English sentiment analysis system can be further adapted to the peculiarities of machine-translated texts, which are notably different from regular English. The current translation system has been trained on non-tweet data that results in a high percentage of out-of-vocabulary words on our datasets. In our experiments, we assumed that all texts are written in Levantine dialect of the Arabic language. However, tweets can have a mixture of dialects or even a mixture of languages (e.g., Arabic and English). Addressing these factors will give even more insight on how sentiment is altered on translation, in specific contexts. 

 Conclusions We presented a set of experiments to systematically study the impact of English translation (manual and automatic) on sentiment analysis of Arabic social media posts. Our experiments show that automatic sentiment analysis of English translations (even of automatic translations) can lead to competitive results-results that are similar to that obtained by current state-of-the-art Arabic sentiment analysis systems. Our results also show that automatic sentiment analysis of automatic translations outperforms the manual sentiment annotations of the automatically translated text. This suggests that SMT errors impact human perception of sentiment markedly more than automatic sentiment systems. This is an interesting avenue for future exploration. We also show that translated texts tend to lose some of the sentiment information and there is a relatively higher percentage of neutral instances in the translated text than in the original dataset. The resources created as part of this project (Arabic sentiment lexicons, Arabic sentiment annotations of social media posts, and English sentiment annotations of their translations) are made freely available.  5   Figure 1 : 1 Figure 1: Experimental setup to determine the impact of translation on sentiment. We compare sentiment labels between Ar(Manl.Sent.) (shown in a shaded box) and other datasets shown on the right side of the figure. Ar(Manl.Sent.) is the original Arabic text manually annotated for sentiment. 

 Table 1 : 1 Class distribution (in percentage) of the sentiment annotated datasets. positive negative neutral agreement BBN data a. Ar(Manl.Sent) 41.50 47.92 10.58 73.80 b. En(Manl.Trans., Manl.Sent) 35.00 43.25 21.75 68.00 c. En(Auto.Trans., Manl.Sent) 36.17 36.50 27.34 65.70 Syria data d. Ar(Manl.Sent) 22.40 67.50 10.10 79.00 e. En(Auto.Trans., Manl.Sent) 14.25 66.15 19.60 76.10 

 Table 2 : 2 Accuracy (in percentage) of sentiment analysis (SA) systems on various Arabic social media datasets. Arabic Sentiment Labeled Dataset MD RR BBN Syria sentiment classes pos, neg pos,neg pos, neg, neu pos, neg, neu number of instances 1111 2681 1199 2000 Most frequent class baseline 66.06 68.92 47.95 67.50 Human agreement benchmark - - 73.82 79.05 Mourad and Darwish Arabic SA system 72.50 - - - Our Arabic SA system 74.62 85.23 63.89 78.65 pos neg neu BBN data a. Ar(Auto.Sent) 39.78 60.05 0.17 b. En(Manl.Trans., Auto.Sent) 43.12 55.63 1.25 c. En(Auto.Trans., Auto.Sent) 42.87 56.05 1.08 Syria data d. Ar(Auto.Sent) 20.60 75.30 4.10 e. En(Auto.Trans., Auto.Sent) 24.75 69.75 5.50 

 Table 3 : 3 Class distribution (in percentage) resulting from automatic sentiment analysis. 

 1. Bad auto. translation: mistranslation of ambiguous words Post negative Auto.Trans. the minimum taught me that more relatives clock neutral Manl.Trans. Life has taught me that most of the relatives are scorpions negative Trans. you're still good in front of the leakage of water existed from time positive Manl.Trans. Expect more good to come, water has been leaking since a long time negative Table 2. Bad auto. translation: mistranslation of ambiguous words Post positive Auto.Trans. i wish i live in a place not cut off by snow negative Manl.Trans. I wish I live in a place where snow never stops falling positive 3. Bad auto. translation: sarcasm is hard to translate Post negative Auto. 

			 https://catalog.ldc.upenn.edu/LDC2012T09 

			 http://www.crowdflower.com 

			 http://www.purl.com/net/lexicons 

			 http://www.purl.com/net/ArabicSentiment
