title
Decision Trees for Lexical Smoothing in Statistical Machine Translation

abstract
We present a method for incorporating arbitrary context-informed word attributes into statistical machine translation by clustering attribute-qualied source words, and smoothing their word translation probabilities using binary decision trees. We describe two ways in which the decision trees are used in machine translation: by using the attribute-qualied source word clusters directly, or by using attributedependent lexical translation probabilities that are obtained from the trees, as a lexical smoothing feature in the decoder model. We present experiments using Arabic-to-English newswire data, and using Arabic diacritics and part-ofspeech as source word attributes, and show that the proposed method improves on a state-of-the-art translation system.

Introduction Modern statistical machine translation (SMT) models, such as phrase-based SMT or hierarchical SMT, implicitly incorporate source language context. It has been shown, however, that such systems can still benet from the explicit addition of lexical, syntactic or other kinds of context-informed word features  (Vickrey et al., 2005; Gimpel and Smith, 2008; Brunning et al., 2009; Devlin, 2009) . But the benet obtained from the addition of attribute information is in general countered by the increase in the model complexity, which in turn results in a sparser translation model when estimated from the same corpus of data. The increase in model sparsity usually results in a deterioration of translation quality. In this paper, we present a method for using arbitrary types of source-side context-informed word attributes, using binary decision trees to deal with the sparsity side-eect. The decision trees cluster attribute-dependent source words by reducing the entropy of the lexical translation probabilities. We also present another method where, instead of clustering the attribute-dependent source words, the decision trees are used to interpolate attributedependent lexical translation probability models, and use those probabilities to compute a feature in the decoder log-linear model. The experiments we present in this paper were conducted on the translation of Arabicto-English newswire data using a hierarchical system based on  (Shen et al., 2008) , and using Arabic diacritics (see section 2.3) and part-ofspeech (POS) as source word attributes. Previous work that attempts to use Arabic diacritics in machine translation runs against the sparsity problem, and appears to lose most of the useful information contained in the diacritics when using partial diacritization  (Diab et al., 2007) . Using the methods proposed in this paper, we manage to obtain consistent improvements from diacritics against a strong baseline. The methods we propose, though, are not restrictive to Arabic-to-English translation. The same techniques can also be used with other language pairs and arbitrary word attribute types. The attributes we use in the described experiments are local; but long distance features can also be used. In the next section, we review relevant previous work in three areas: Lexical smoothing and lexical disambiguation techniques in machine translation; using decision trees in natural language processing, and especially machine translation; and Arabic diacritics. We present a brief exposition of Arabic orthogra-phy, and refer to previous work on automatic diacritization of Arabic text. Section 3 describes the procedure for constructing the decision trees, and the two methods for using them in machine translation. In section 4 we describe the experimental setup and present experimental results. Finally, section 5 concludes the paper and discusses future directions. 

 Previous Work 

 Lexical Disambiguation and 

 Lexical Smoothing Various ways have been proposed to improve the lexical translation choices of SMT systems. These approaches typically incorporate local context information, either directly or indirectly. The use of Word Sense Disambiguation (WSD) has been proposed to enhance machine translation by disambiguating the source words  (Cabezas and Resnick, 2005; Carpuat and Wu, 2007; Chan et al., 2007) . WSD usually requires that the training data be labeled with senses, which might not be available for many languages. Also, WSD is traditionally formulated as a classication problem, and therefore does not naturally lend itself to be integrated into the generative framework of machine translation.  Carpuat and Wu (2007)  formulate the SMT lexical disambiguation problem as a WSD task. Instead of learning from word sense corpora, they use the SMT training data, and use local context features to enhance the lexical disambiguation of phrasebased SMT.  Sarikaya et al. (2007)  incorporate context more directly by using POS tags on the target side to model word context. They augmented the target words with POS tags of the word itself and its surrounding words, and used the augmented words in decoding and for language model rescoring. They reported gains on Iraqi-Arabic-to-English translation. Finally, using word-to-word context-free lexical translation probabilities has been shown to improve the performance of machine translation systems, even those using much more sophisticated models. This feature, usually called lexical smoothing, has been used in phrase-based systems  (Koehn et al., 2003) .  Och et al. (2004)  also found that including IBM Model 1  (Brown et al., 1993)  word probabilities in their log-linear model works better than most other higher-level syntactic features at improving the baseline. The incorporation of context on the source or target side enhances the gain obtained from lexical smoothing.  Gimpel and Smith (2008)  proposed using source-side lexical features in phrase-based SMT by conditioning the phrase probabilities on those features. They used word context, syntactic features or positional features. The features were added as components into the log-linear decoder model, each with a tunable weight.  Devlin (2009)  used context lexical features in a hierarchical SMT system, interpolating lexical counts based on multiple contexts. It also used target-side lexical features. The work in the paper incorporates context information based on the reduction of the translation probability entropy. 

 Decision Trees Decision trees have been used extensively in various areas of machine learning, typically as a way to cluster patterns in order to improve classication  (Duda et al., 2000) . They have, for instance, been long used successfully in speech recognition to cluster contextdependent phoneme model states  (Young et al., 1994) . Decision trees have also been used in machine translation, although to a lesser extent. In this respect, our work is most similar to  (Brunning et al., 2009) , where the authors extended word alignment models for IBM Model 1 and Hidden Markov Model (HMM) alignments. They used decision trees to cluster the context-dependent source words. Contexts belonging to the same cluster were grouped together during Expectation Maximization (EM) training, thus providing a more robust probability estimate. While  Brunning et al. (2009)  used the source context clusters for word alignments, we use the attribute-dependent source words directly in decoding. The approach we propose can be readily used with any alignment model.  Stroppa et al. (2007)  presented a generalization of phrase-based SMT  (Koehn et al., 2003)  that also takes into account sourceside context information. They conditioned the target phrase probability on the source phrase as well as source phrase context, such as bordering words, or part-of-speech of bordering words. They built a decision tree for each source phrase extracted from the training data. The branching of the tree nodes was based on the dierent context features, branching on the most class-discriminative features rst. Each node is associated with the set of aligned target phrases and corresponding context-conditioned probabilities. The decision tree thus smoothes the phrase probabilities based on the dierent features, allowing the model to back o to less context, or no context at all depending on the presence of that context-dependent source phrase in the training data. The model, however, did not provide for a back-o mechanism if the phrase pair was not found in the extracted phrase table. The method presented in this paper diers in various aspects. We use context-dependent information at the source word level, rather than the phrase level, thus making it readily applicable to any translation model and not just phrase-based translation. By incorporating context at the word level, we can decode directly with attribute-augmented source data (see section 3.2). 

 Arabic Diacritics Since an important part of the experiments described in this paper use diacritized Arabic source, we present a brief description of Arabic orthography, and specically diacritics. The Arabic script, like that of most other Semitic languages, only represents consonants and long vowels using letters 1 . Short vowels can be written as small marks written above or below the preceding consonant, called diacritics. The diacritics are, however, omitted from written text, except in special cases, thus creating an additional level of lexical ambiguity. Readers can usually guess the correct pronunciation of words in non-diacritized text from the sentence and discourse context. Grammatical case on nouns and adjectives are also marked using diacritics at the end of words. Arabic MT systems use undiacritized text, since most available Arabic data is undiacritized. Automatic diacritization of Arabic has been done with high accuracy, using various generative and discriminative modeling techniques. For example,  Ananthakrishnan et al. (2005)  used a generative model that incorporates word level n-grams, sub-word level n-grams and part-of-speech information to perform diacritization.  Nelken and Shieber (2005)  modeled the generative process of dropping diacritics using weighted transducers, then used Viterbi decoding to nd the most likely generator.  Zitouni et al. (2006)  presented a method based on maximum entropy classiers, using features like character n-grams, word ngrams, POS and morphological segmentation.  Habash and Rambow (2007)  determined various morpho-syntactic features of the word using SVM classiers, then chose the corresponding diacritization. The experiments in this paper use the automatic diacritizer by Sakhr Software. The diacritizer determines word diacritics through rule-based morphological and syntactic analysis. It outputs a diacritization for both the internal stem and case ending markers of the word, with an accuracy of 97% for stem diacritization and 91% for full diacritization (i.e., including case endings). There has been work done on using diacritics in Automatic Speech Recognition, e.g.  (Vergyri and Kirchho, 2004) . However, the only previous work on using diacritization for MT is  (Diab et al., 2007) , which used the diacritization system described in  (Habash and Rambow, 2007) . It investigated the eect of using full diacritization as well as partial diacritization on MT results. The authors found that using full diacritics deteriorates MT performance. They used partial diacritization schemes, such as diacritizing only passive verbs, keeping the case endings diacritics, or only gemination diacritics. They also saw no gain in most congurations. The authors argued that the deterioration in performance is caused by the increase in the size of the vocabulary, which in turn makes the translation model sparser; as well as by errors during the automatic diacritization process. 3 Decision Trees for Source Word Attributes 

 Growing the Decision Tree In this section, we describe the procedure for growing the decision trees using contextinformed source word attributes. The attribute-qualied source-side of the parallel training data is rst aligned to the target-side data. If S is the set of attributedependent forms of source word s, and t j is a target word aligned to s i ? S, then we dene: p (t j |s i ) = count(s i ,t j ) count(s i ) (1) where count(s i , t j ) is the count of alignment links between s i and t j . A separate binary decision tree is grown for each source word. We start by including all the attribute-dependent forms of the source word at the root of the tree. We split the set of attributes at each node into two child nodes, by choosing the splitting that maximizes the reduction in weighted entropy of the probability distribution in (1). In other words, at node n, we choose the partition (S 1 , S 2 ) such that: (S 1 , S 2 ) = argmax (S 1 ,S 2 ) S 1 ?S 2 =S {h(S) ? (h(S 1 ) + h(S 2 ))} (2) where h(S) is the entropy of the probability distribution p(t j |s i ? S), weighted by the number of samples in the training data of the source words in S. We only split a node if the entropy is reduced by more than a threshold ? h . This step is repeated recursively until the tree cannot be grown anymore. Weighting the entropy by the source word counts gives more weight to the contextdependent source words with a higher number of samples in the training data, sine the lexical translation probability estimates for frequent words can be trusted better. The rationale behind the splitting criterion used is that the split that reduces the entropy of the lexical translation probability distribution the most is also the split that best separates the list of forms of the source word in terms of the target word translation. For a source word that has multiple meanings, depending on its context, the decision tree will tend to implicitly separate those meanings using the information in the lexical translation probabilities. Although we describe this method as growing one decision tree for each word, and using one attribute type at a time, a decision tree can clearly be constructed for multiple words, and more than one attribute type can be used in the same decision tree. 

 Trees for Source Word Clustering The source words could be augmented to explicitly incorporate the word attributes (diacritics or other attribute types). The augmented source will be less ambiguous if the attributes do in fact contain disambiguating information. This, in principle, helps machine translation performance. The ip side is that the resulting increase in vocabulary size increases the translation model sparsity, usually with a detrimental eect on translation. To mitigate the eect of the increase in vocabulary, decision trees can be use to cluster the attribute-augmented source words. More specically, a decision tree is grown for each source word as described in the previous section, using a predened entropy threshold ? h . When the tree cannot be expanded anymore, its leaf nodes will contain a multi-set partitioning of the list of attribute-dependent forms of that source word. Each of the clusters is treated as an equivalence class, and all forms in that class are mapped to a unique form (e.g. an arbitrarily chosen member of the cluster). The mappings are used to map the tokens in the parallel training data before alignment is run on the mapped data. The test data is also mapped consistently. This clustering procedure will only keep the attribute-dependent forms of the source words that decrease the uncertainty in the translation probabilities, and are thus useful for translation. The experiments we report on use diacritics as an attribute type. The various diacritized forms of a source word are thus used to train the decision trees. The resulting clusters are used to map the data into a subset of the vocabulary that is used in translation training and decoding (see section 4.2 for results). Diacritics are obviously specic to Arabic. But this method can be used with other attribute types, by rst appending the source words with their context (e.g. attach to each source word its part-of-speech tag or context), and then training decision trees and mapping the source side of the data. Figure  1  shows an example of a decision tree for the Arabic word sjn 2 using diacritics as a source attribute. The root contains the various diacritized forms (sijona `prison AC-CUSATIVE', sijoni `prison DATIVE', sajona `imprisonment ACCUSATIVE.', sajoni `imprisonment ACCUSATIVE.', sajana `he imprisoned '). The leaf nodes contain the attribute-dependent clusters. 

 Trees for Lexical Smoothing As mentioned in section 2.1, lexical smoothing, computed from word-to-word translation probabilities, is a useful feature, even in SMT systems that use sophisticated translation models. This is likely due to the robustness of context-free word-to-word translation probability estimates compared to the probabilities of more complicated models. In those models, the rules and probabilities are estimated from much larger sample spaces. In our system, the lexical smoothing feature is computed as follows: f (U)= t j ?T (U) 1? s i ?{S(U)?NULL} (1?p(t j |s i )) (3) where U is the modeling unit specic to the translation model used. For a phrase-based system, U is the phrase pair, and for a hierarchical system U is the translation rule. S (U) 2 Examples are written using Buckwalter transliteration. is the set of terminals on the source side of U, and T (U) is the set of terminals on its target. The NULL term in the equation above accounts for unaligned target words, which we found in our experiments to be benecial. One way of interpreting equation (  3 ) is that f (U) is the probability that for each target word t j in U, t j is a likely translation of at least one word s i on the source side. The feature value is then used as a component in the log-linear model, with a tunable weight. In this work, we generalize the lexical smoothing feature to incorporate the source word attributes. A tree is grown for each source word as described in section 3.1, but using an entropy threshold ? h = 0. In other words, the tree is grown all the way until each leaf node contains one attribute-dependent form of the source word. Each node in the tree contains a cluster of attribute-dependent forms of the source word, and a corresponding attribute-dependent lexical translation probability distribution. The lexical translation probability models at the root nodes are those of the regular attribute-independent lexical translation probabilities. The models at the leaf nodes are the most ne-grained, since they are conditioned on only one attribute value. Figure  2  shows a fully grown decision tree for the same source word as the example in Figure  1 . The lexical probability distribution at the leafs are from sparser data than the original distributions, and are therefore less robust. To address this, the attribute-dependent lexical smoothing feature is estimated by recursively interpolating the lexical translation probabilities up the tree. The probability distribution p n at each node n is interpolated with the probability of its parent node as follows: p n = p n if n is root, w n p n + (1 ? w n )p m otherwise where m is the parent of n (4) A fraction of the parent probability mass is thus given to the probability of the child node. If the probability estimate of an attributedependent form of a source word with a certain target word t is not reliable, or if the probability estimate is 0 (because the source word in this context is not aligned with t), then the model gracefully backs o by using the probability estimates from other attributedependent lexical translation probability models of the source word. The interpolation weight is a logistic regression function of the source word count at a node n: w n = 1 1 + e ? log(count(Sn)) (5) The weight varies depending on the count of the attribute-qualied source word in each node, thus reecting the condence in the estimates of each node's distribution. The two global parameters of the function, a bias ? and a scale ? are tuned to maximize the likelihood of a set of alignment counts from a heldout data set of 179K sentences. The tuning is done using Powell's method  (Brent, 1973) . During decoding, we use the probability distribution at the leaves to compute the feature value f (R) for each hierarchical rule R. We train and decode using the regular, attributeindependent source. The source word attributes are used in the decoder only to index the interpolated probability distribution needed to compute f (R). 

 Experiments 

 Experimental Setup As mentioned before, the experiments we report on use a string-to-dependency-tree hierarchical translation system based on the model described in  (Shen et al., 2008)  Other features such as rule probabilities and dependency tree language model  (Shen et al., 2008)  are also used. We use GIZA++  (Och and Ney, 2003)  for word alignments. The decoder model parameters are tuned using Minimum Error Rate training  (Och, 2003)  to maximize the IBM BLEU score  (Papineni et al., 2002) . For training the alignments, we use 27M words from the Sakhr Arabic-English Parallel Corpus (SSUSAC27). The language model uses 7B words from the English Gigaword and from data collected from the web. A 3-gram language model is used during decoding. The decoder produces an N-best list that is reranked using a 5-gram language model. We tune and test on two separate data sets consisting of documents from the following collections: the newswire portion of NIST MT04, MT05, MT06, and MT08 evaluation sets, the GALE Phase 1 (P1) and Phase 2 (P2) evaluation sets, and the GALE P2 and P3 development sets. The tuning set contains 1994 sentences and the test set contains 3149 sentences. The average length of sentences is 36 words. Most of the documents in the two data sets have 4 reference translations, but some have only one. The average number of reference translations per sentence is 3.94 for the tuning set and 3.67 for the test set. In the next section, we report on measurements of the likelihood of test data, and describe the translation experiments in detail. 

 Results In order to assess whether the decision trees are in fact helpful in decreasing the uncertainty in the lexical translation probabilities on unseen data, we compute the likelihood of the test data with respect to these probabilities with and without the decision tree splitting. We align the test set with its reference using GIZA++, and then obtain the link count l_count(s i , t j ) for each alignment link i = (s i, t i ) in the set of alignment links I. We calculate the normalized likelihood of the alignments: L = log ? ? i p(t i | s i ) l_count(s i ,t i ) 1 |I| ? ? = 1 |I| i?I l_count(s i , t i ) log p (t i | s i ) (6) where p (t i | s i ) is the probability for the word pair (t i , s i ) in equation (  4 ). If the same instance of source word s i is aligned to two target words t i and t j , then these two links are counted separately. If a source in the test set is out-of-vocabulary, or if a word pair (t i , s i ) is aligned in the test alignment but not in the training alignments (and thus has no probability estimate), then it is ignored in the calculation of the log-likelihood. Table  1  shows the likelihood for the baseline case, where one lexical translation probability distribution is used per source word. It also shows the likelihoods calculated using the lexical distributions in the leaf nodes of the decision trees, when either diacritics or part-ofspeech are used as an attribute type. The table shows an increase in the likelihood of 2.98% using diacritics, and 3.41% using part-of-speech. The translation result tables present MT scores in two dierent metrics: Translation Edit Rate  (Snover et al., 2006)   BLEU. The reader is reminded that a higher BLEU score and a lower TER are desired. The tables also show the dierence in scores between the baseline and each experiment. It is worth noting that the gains reported are relative to a strong baseline that uses a state-ofthe-art system with many features, and a fairly large training corpus. The decision tree clustering experiment as described in section 3.2 depends on a global parameter, namely the threshold in entropy reduction ? h . We tune this parameter manually on a tuning set. Figure  3  shows the BLEU scores as a function of the threshold value, with diacritics as an attribute type. The most gain is obtained for an entropy threshold of 50. The fully diacritized data has an average of 1.78 diacritized forms per source word. The average weighted by the number of occurrences is 6.28, which indicates that words with more diacritized forms tend to occur more frequently. After clustering using a value of ? h = 50, the average number of diacritized forms becomes 1.11, and the occurrence weighted average becomes 3.69. The clustering procedure thus seems to eliminate most diacritized forms, which likely do not contain helpful disambiguating information. Table  2  lists the detailed results of experiments using diacritics. In the rst experiment, we show that using full diacritization results in a small gain on the BLEU score and no gain on TER, which is somewhat consistent with the result obtained by  Diab et al. (2007) . The next experiment shows the results of clustering the diacritized source words using decision trees for the entropy threshold of 50. The TER loss of the full diacritics becomes a gain, and the BLEU gain increases. This conrms our speculation that the use of fully diacritized data in- creases the model sparsity, which undoes most of the benet obtained from the disambiguating information that the diacritics contain. Using the decision trees to cluster the diacritized source data prunes diacritized forms that do not decrease the entropy of the lexical translation probability distributions. It thus nds a sweet-spot between the negative eect of increasing the vocabulary size and the positive eect of disambiguation. In our experiments, using diacritics with case endings gave consistently better score than using diacritics with no case endings, despite the fact that they result in a higher vocabulary size. One possible explanation is that diacritics not only help in lexical disambiguation, but they might also be indirectly helping in phrase reordering, since the diacritics on the nal letter indicate the word's grammatical function. The results from using decision trees to interpolate attribute-dependent lexical smoothing features are summarized in table 3. In the rst experiment, we show the results of using diacritics to estimate the interpolated lexical translation probabilities. The results show a gain of +0.5 BLEU points and 0.39 TER points. The gain is statistically signicant with a 95% condence level. Using partof-speech as an attribute gives a smaller, but still statistically signicant gain. We also ran a control experiment, where we used diacriticdependent lexical translation probabilities obtained from the decision trees, but did not perform the probability interpolation of equation (  4 ). The gains mostly disappear, especially on BLEU, showing the importance of the interpolation step for the proper estimation of the lexical smoothing feature. 

 Conclusion and Future Directions We presented in this paper a new method for incorporating explicit context-informed word attributes into SMT using binary decision trees. We reported on experiments on Arabicto-English translation using diacritized Arabic and part-of-speech as word attributes, and showed that the use of these attributes increases the likelihood of source-target word pairs of unseen data. We proposed two specic ways in which the results of the decision tree training process are used in machine translation, and showed that they result in better translation results. For future work, we plan on using multiple source-side attributes at the same time. Dierent attributes could have dierent disambiguating information, which could provide more benet than using any of the attributes alone. We also plan on investigating the use of multi-word trees; trees for word clusters can for instance be grown instead of growing a separate tree for each source word. Although the experiments presented in this paper use local word attributes, nothing in principle prevents this method from being used with long-distance sentence context, or even with document-level or discourse-level features. Our future plans include the investigation of using such features as well. or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. A pproved for Public Release, Distribution Unlimited. Figure 1 : 1 Figure1: Decision tree for source word sjn using diacritics as an attribute. 
