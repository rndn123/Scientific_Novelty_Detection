title
PATQUEST: Papago Translation Quality Estimation

abstract
This paper describes the system submitted by Papago team for the quality estimation task at WMT 2020. It proposes two key strategies for quality estimation: (1) task-specific pretraining scheme, and (2) task-specific data augmentation. The former focuses on devising learning signals for pretraining that are closely related to the downstream task. We also present data augmentation techniques that simulate the varying levels of errors that the downstream dataset may contain. Thus, our PATQUEST models are exposed to erroneous translations in both stages of task-specific pretraining and finetuning, effectively enhancing their generalization capability. Our submitted models achieve significant improvement over the baselines for Task 1 (Sentence-Level Direct Assessment; EN-DE only), and Task 3 (Document-Level Score). * Equal contribution ? Work done during internship at Naver Corp.

Introduction With the widespread use of machine translation systems, there is a growing need to evaluate translated results at low-cost. The task of quality estimation (QE) addresses this issue, where the quality of a translation is predicted automatically given the source sentence and its translation. The estimated quality can inform users about the reliability of the translation, or whether it needs to be post-edited. Previous QE systems generally include pretraining and finetuning steps, where the former step involves masked language modeling (MLM) utilizing large parallel corpora, with the expectation that the models will learn cross-lingual relationships  (Kepler et al., 2019; Kim et al., 2019) . The models are, in turn, finetuned with task-specific data. However, while the pretraining step involves training data with near-perfect translations, lowquality translations are only introduced during the finetuning step. In this work, we suggest two key strategies that could alleviate this pretrain-finetune discrepancy in QE tasks by: (1) adopting a task-specific pretraining objective which is close to that of the downstream task, and (2) generating abundant taskspecific erroneous sentence pairs and their learning signals. Our approach, which is depicted in Figure  1 , is motivated from BLEURT  (Sellam et al., 2020) , where we extend their general approach to the bilingual QE setting. Our submitted systems achieve significant improvements in performance over the baseline systems on WMT20 Shared Tasks for QE  (Specia et al., 2020) : an absolute gain of +35.2% in Pearson score for (Task 1) Sentence-Level Direct Assessment (EN-DE), and +18.4% in Pearson score for (Task 3) Document-Level Score. 

 Sentence-Level QE: Direct Assessment The task of sentence-level QE for direct assessment (DA) involves predicting the perceived quality of the translation given the source and the translated sentences. Following the footsteps of the previous work on QE, our sentence-level system also utilizes the pretrained multilingual language models such as BERT  (Devlin et al., 2018)  and Cross-lingual Language Model (XLM)  (Conneau and Lample, 2019) . As the size of the training corpus for the QE task is very limited (7K sentence pairs), it is crucial to align these models closely to the task using more data in the form of task-specific pretraining. As opposed to pretraining the models on parallel corpora using the standard MLM approach, we pretrain the models in a multi-task setting using learning signals and data that are arguably more task-specific similar to  Sellam et al. (2020) . 

 Task-Specific Data Augmentation In order to better align the pretrained models to the QE task, synthetic sentence pairs that contain various types of translation errors are generated from clean parallel corpora 1 . For each target sentence, we generate two perturbed sentences by separately applying one of the four methods described below. 

 Omitted Word We randomly omit at most three words from the target-side, simulating inadequate translations. Word Order Based on the part-of-speech (POS) tag for each word in the target sentence, and predefined sequences of POS patterns, we randomly swap two target words if those words match one of the patterns. The POS patterns can be contiguous, e.g., adjective-space-noun, or long-ranged, e.g., noun-*-adjective. When none of the patterns are matched, we randomly swap two words. Lexical Selection For each target sentence, we mask out at most three words randomly, and apply mask-filling via a German BERT model from Hugging Face 2 . The purpose of this alteration is to generate fluent but somewhat inadequate target sentences. 

 Repeated Phrase In order to simulate the repetition problem in translations generated by neural machine translation models, we alter the target sentence by adding a repetition of a random phrase within the sentence. The length of the random phrase is at most three tokens. 1 Europarl v10 and News Commentary v15 2 bert-base-german-cased, https://huggingface.co/transformers/ pretrained_models.html 

 Task-Specific Learning Signals As the goal of the downstream task is to predict the DA scores which represent the "perceived quality" of the translation, we need to consider pretraining signals that can capture the somewhat subjective notion of "good" and "bad" translations. Consulting the related works, we prepared the three learning signals: ? SentenceBERT score  (Reimers and Gurevych, 2019)  ? BERTScore  (Zhang et al., 2019) , extended to multilingual setting ? Target (German) Language Model (GPT-2, Radford et al. ( 2019 )) score For each sentence pair in the original bilingual corpora as well as the augmented ones, the three types of learning signals are computed, and later used in the task-specific pretraining. 

 SentenceBERT Score For a given sentence, SentenceBERT produces a semantically meaningful sentence embedding that can be compared using a distance metric. We note that when comparing the distance between two sentence vectors, the Kendall rank correlation coefficient  (Kendall, 1938)  is computed instead of the cosine similarity measure as the former correlates better with the human judgement, possibly because it produces a more widespread range of scores than the latter especially when the dimension of the sentence vectors is high. In our experiments, we used the publicly available multilingual SentenceBERT model released from UKPLab 3 that supports 13 languages including English and German. 

 Multilingual BERTScore While SentenceBERT score looks at the sentence embedding as a whole, BERTScore computes a similarity score for each token in the pair of sentences. We include BERTScore as one of the learning signals because we feared that the meanpooling of the BERT-embedded tokens within the SentenceBERT model, while effective in extracting the overall meaning of the sentence, may overlook some of the small semantic details within the sentence. However, as the original BERTScore is designed to work in monolingual setting, i.e. evaluating a translation against a reference sentence, it needs to be extended in multilingual setting using a multilingual BERT (mBERT) model. Analogous to the original approach, the multilingual BERTScores can be computed in various ways depending on which side we are computing the maximum similarities from. In our experiments, we devise a metric where we merge both the source-and target-side maximum similarities between tokens with the corresponding inverse document frequency (IDF) weighting; thus, given a sequence of vectorized source and target tokens, s and t, we defined the mBERTScore of s and t to be: S s?t + S t?s s i ?s idf(s i ) + t j ?t idf(t j ) where S s?t = s i ?s idf(s i )max t j ?t s i ? t j S t?s = t j ?t idf(t j )max s i ?s t j ? s i 

 Target Language Model Score While SentenceBERT and multilingual BERTScore can be used as proxies for evaluating the "adequacy" of the translation, empirically, we noticed that they cannot seem to sufficiently represent the "fluency" of translated target sentence. In other words, both metrics may assign high scores to the translated sentence if key source tokens are translated and present in the translation, even when the overall sentence may not be articulate. To address this issue, the target language model (GPT-2) score is added to the set of learning signals. We simply use the arithmetic mean of the tokenlevel predictions to produce the score for a target sentence. We utilize the pretrained GPT-2 model for German released by Zamia Brain 4 . 

 Model Architecture We have two stages for task-specific training, i.e. first with the augmented data and the learning signals, and second with the provided QE dataset (ref. Section 2.4). As the output to predict for each stage is different, we utilize the following two types of model architectures. Figure  3 : The model architecture (left) for the taskspecific finetuning using the provided QE dataset. For each concatenated vector computed within each Score Block (c.f. Fig.  2 .), a Linear Block (right) is added on top of it. The results from the Linear Blocks are concatenated and used to produce the final DA score. 

 Model for Task-Specific Pretraining On top of the specific layer of the pretrained mBERT or XLM models, we attach a series of layers called "Score Block" for each type of learning signal as depicted in Figure  2 . We utilize the 9th and 5th layer of the BERT and XLM models, respectively, as these layers are reported to be more semantically relevant  (Jawahar et al., 2019; Zhang et al., 2019) . In addition to using the vector representation of the [CLS] token, utilizing the mean-pooled and max-pooled vectors from all tokens further improved the performance. 

 Model for Task-Specific Finetuning Once the task-specific pretraining is completed, we begin the finetuning by adding layers above the concatenation layer within each Score Block, as shown in Figure  3 . Thus, we have three concatenated vectors being fed to three "Linear Blocks" separately, whose purpose is to reduce the dimensions of the hidden representation, preparing it for the final regression layer. We note that applying dropout  (Srivastava et al., 2014)  to these linear layers helps with the performance. 

 Task-Specific Training We experiment with three different types of pretrained models: mBERT 5 , XLM trained with MLM (XLM-MLM) 6 , and XLM trained with causal language modeling (XLM-CLM)  7  . All of the pretrained models are available at Hugging Face. 

 Task-Specific Pretraining (TSP) As the size of the provided QE dataset is small, we make use of the existing parallel data as well as the error-induced synthetic data. For the EN-DE bilingual dataset, we select a subset from this year's training corpora for WMT News Translation Task, summing to just under 10M sentence pairs; for the synthetic dataset, the size is 3.4M. Given the concatenated source and target sentences as an input, the model for TSP is trained to predict the three types of learning signals in a multi-task setting by minimizing the sum of the mean squared error losses for each signal (ref.  

 Task-Specific Finetuning (TSF) Once the model is trained with the augmented data, its parameters are loaded to the model for TSF (ref. Figure  3 ), and finetuned using the QE dataset. This time, the model learns to predict the mean DA score. 3 Document-Level QE: MQM Scoring Given a source and its translated document, this task involves identifying translation errors and estimating the translation quality of the document based on the taxonomy of the Multidimensional Quality Metrics (MQM) 8 . With the pre-defined MQM taxonomy, human annotators assess whether the translation satisfies the specifications, and from these annotations, an MQM score is obtained. In this work, we focus on building a system that predicts the MQM score for a given pair of source and translated document. The major difficulty that we encountered in this task was the lack of training data. As the amount of provided data is limited (8,591 sentence pairs), a model that is solely finetuned on this small-scale data was not capable enough to differentiate sentences with varying level of errors. To address this issue, we propose simple yet effective methods for task-specific data augmentation, and task-specific training framework 9 . 

 Task-Specific Data Augmentation We generate erroneous sentence pairs and their pseudo-MQM scores from Europarl and QE training corpus in accordance with the MQM taxonomy. 

 Generating Erroneous Sentence Pairs Out of the 45 error categories specified in QE annotations, we select five frequent categories for which we can automatically perturb the target-side of the parallel corpus at little cost. More details on our data augmentation technique for each category are provided below. 

 Omitted Preposition We introduce an error into the target-side of a sentence pair by randomly omitting one of the French prepositions that exist in the sentence. Omitted Determiner The same process is done for French determiners as for prepositions. Wrong Preposition We replace a French preposition with another one. When more than one candidate exists, we choose one at random. 

 Word Order We exploit grammatical pattern that most descriptive adjectives go after the noun in French sentences (unlike English ones). Using an in-house French POS tagger, we identify post-nominal adjectives and place them in front of the corresponding nouns so that they are now pre-nominal. 

 Lexical Selection We mask-out target tokens at random positions, and substitute them with tokens predicted by the Camembert language model  (Martin et al., 2020) .  

 Task-Specific Learning Signal Once we introduce different types of errors into the target-side sentences, the next step is to obtain pseudo-MQM scores for the altered sentence pairs. Two key elements for computing MQM score are the length of a text, and its total error severity as follows: Pseudo-MQM = 100(1 ? 5.0 * n error + S N ) where N indicates the length of given target sentence and n error denotes the number of errors introduced in it. We assign 5.0, the most frequent severity, to each perturbation that we make. If an error severity score, S, is assigned to the sentence by human annotators, we add this score to compute the total error severity score. 

 Model Architecture We use pretrained mBERT or XLM 10 as initial parameters. The concatenation of a source sentence and its corresponding target sentence with special symbol tokens is taken as input: [CLS] source [SEP] target [SEP]. We experiment with two strategies for obtaining sentence embeddings. First, we feed a hidden state vector corresponding to [CLS] token (h [CLS] ) to a linear layer to compute a sentence-level MQM prediction of ?: ? = W h [CLS] + b where W and b are the weight matrix and bias vector of the linear layer, respectively. For the other 10 xlm-mlm-enfr-1024 method, we use the concatenation of a mean-pooled source representation (s ? R n ), mean-pooled target representation (t ? R n ) and their element-wise differences (|s ? t| ? R n ) in an attempt to enlarge the model capacity: ? = W ? ReLU(W r (s, t, |s ? t|) + b r ) + b where W r ? R 3n?n and b r are the weight matrix and bias vector of an intermediate dimensionreducing layer, respectively, and n denotes the dimension of hidden vectors. W and b are the weight matrix and bias vector of the final linear layer. 

 Task-Specific Training We suggest that the pretraining objective should be similar to that of the downstream task in order to mitigate the pretrain-finetune discrepancy  (Yang et al., 2019) , and fully leverage the erroneous sentence pairs that we generated. For this task, both phases minimize the mean-squared loss function: l = 1 K K k=1 y k ? ? 2 . 

 Task-Specific Pretraining (TSP) We utilize Europarl parallel corpus (English-French) to pretrain our submitted models 11 . To acquire high quality data, we carried out the following filtering processes: (1) language detection (filtering out non-English sentences in the sourceside, and non-French sentences in the target-side), (2) length ratio filtering (eliminating sentence pairs with length ratio greater than 1.8). We assume that the remaining sentence pairs do not contain any translation error. Therefore, we assign the total error severity score of zero to these pairs before the augmentation. About 15.2 million examples 12 are generated with the above-mentioned data augmentation techniques. The detailed examples are provided in Table 1. 

 Task-Specific Finetuning (TSF) The next step is to finetune our model using the augmented QE train data. Unlike Europarl corpus, we can fully leverage the MQM scores originally assigned to the QE training dataset. We found that performing the data augmentation with three categories (Omitted Determiner, Omitted Preposition, and Wrong Preposition) effectively improves the performance. The original QE training sentence pairs represent about 5% of 169,997 sentence pairs obtained from the data augmentation. We also provide the augmented examples for QE training data in Table  2 . Since the learning objective is identical to that of the pretraining phase, we can simply train the same model with the augmented downstream task data. 

 Document-Level MQM Score We specify that the models are trained at sentencelevel, learning to predict the non-truncated version of MQM scores which could take a range between negative infinity and 100; this is to avoid potential information loss that could arise from the truncation. Given a document, the document-level MQM score is computed from its sentence-level MQM predictions in a closed form. Afterwards, we truncate negative values to zero. 

 Experimental Results 

 Sentence-Level Task Table  3  shows the Pearson correlation coefficient between the predicted z-normalized DA scores and the reference scores on the development set. We note that the number of parameters for PATQUEST-mBERT (724M) is greater than that of PATQUEST-XLM (616M) models, resulting in the difference in the correlation scores. Nevertheless, computing the arithmetic mean of the scores produced  by these three models improves the performance (PATQUEST-ensemble). The final result on the QE test set is shown in Table  4 . We observe that finetuning the model with the additional error-induced synthetic data improves the performance as well as ensembling the models. Our final submitted system (PATQUESTensemble) finished 4th out of the 15 submitted systems 13 in the final ranking of the sentence-level QE task for English-German. In order to train a generally applicable QE system, we did not make use of the data such as internal information from the NMT models and in-domain Wikipedia texts that could be extracted from the provided Wikipedia titles. 

 Document-Level Task The validation results on development set are shown in Table  5 . Both PATQUEST-mBERT and PATQUEST-XLM models use representations from [CLS] token. We build another two models, PATQUEST-mBERT variant 1 and 2, using the concatenations of mean-pooled source representations, mean-pooled target representations, and their element-wise differences. Table  6  shows the test results of our submitted PATQUEST models. For PATQUEST-ensemble, we compute an average from the four models enumerated in Table  5 . In Table  7 , the effectiveness of our training scheme and data augmentation techniques is illustrated via an ablation study. Note that "Pretrained mBERT (A)" in the table refers to the mBERT model that is finetuned on the original QE data without any task-specific training. Both TSP and TSF enhance the generalization ability of model. Note that the mBERT model trained via TSP and TSF, "A + TSP + TSF", is the same model as PATQUEST-mBERT which itself achieves a significant improvement over the baselines as shown in Table  6 . Our final system (PATQUEST-ensemble) submitted for the document-level QE task, came 1st out of the three submitted systems 14 . Similar to our sentence-level system, our document-level system also did not utilize any internal information from the NMT models and in-domain Wikipedia data tailored to the benchmark. 

 Conclusion In this paper, we present a task-specific pretraining scheme for the QE task. Our pretraining objective is devised so that it is closely related (Task 1) or identical (Task 3) to the finetuning objective. In addition, the models are exposed to abundant amount of error-induced translations generated from large parallel corpora, effectively alleviating the issue of data scarcity. Our proposed models yield significant improvement over the baseline systems for the two tasks. Figure 1 : 1 Figure1: Overview of our approach for Task 1 and 3. 
