title
Generalizing Hierarchical Phrase-based Translation using Rules with Adjacent Nonterminals

abstract
Hierarchical phrase-based translation (Hiero,   (Chiang, 2005)) provides an attractive framework within which both short-and longdistance reorderings can be addressed consistently and ef ciently. However, Hiero is generally implemented with a constraint preventing the creation of rules with adjacent nonterminals, because such rules introduce computational and modeling challenges. We introduce methods to address these challenges, and demonstrate that rules with adjacent nonterminals can improve Hiero's generalization power and lead to signi cant performance gains in Chinese-English translation.

Introduction Hierarchical phrase-based translation  (Hiero, (Chiang, 2005) ) has proven to be a very useful compromise between syntactically informed and purely corpus-driven translation. By automatically learning synchronous grammar rules from parallel text, Hiero captures short-and long-distance reorderings consistently and ef ciently. However, implementations of Hiero generally forbid adjacent nonterminal symbols on the source side of hierarchical rules, a practice we will refer to as the non-adjacent nonterminals constraint. The main argument against such rules is that they cause the system to produce multiple derivations that all lead to the same translation a form of redundancy known as spurious ambiguity. Spurious ambiguity can lead to drastic reductions in decoding ef ciency, and the obvious solutions, such as reducing beam width, erode translation quality. In Section 2, we argue that the non-adjacent nonterminals constraints severely limits Hiero's generalization power, limiting its coverage of important reordering phenomena. In Section 3, we discuss the challenges that arise in relaxing this constraint. In Section 4 we introduce new methods to address those challenges, and Section 5 validates the approach empirically. Improving Hiero via variations on rule pruning and ltering is well explored, e.g.,  (Chiang, 2005; Chiang et al., 2008; Zollmann and Venugopal, 2006) , to name just a few.  Hiero can correctly translate the example if it learns any of the following rules from training data: X ? X 1 , rank 10th at X 1 (1) X ? X 1 , X 1 at Eastern div. (2) X ? X 1 X 2 , X 2 X 1 Eastern div. (3) However, in practice, data sparsity makes the chance of learning these rules rather slim. For instance, learning Rule 1 depends on training data containing instances of the shift with identical wording for the VP-A, which belongs to an open word class. If Hiero fails to learn any of the above rules, it will apply the glue rules S ? S X 1 , S X 1 and S ? X, X . But these glue rules clearly cannot model the VP-A's movement. In failing to learn Rules 1-3, Hiero has no choice but to translate VP-A in a monotone order. On the other hand, consider the following rules with adjacent nonterminals on the source side (or XX rules, for brevity): X? X 1 X 2 , X 2 at X 1 (4) X? X 1 X 2 , rank 10th X 1 X 2 (5) X? X 1 X 2 , X 2 X 1 (6) Note that although XX rules 4-6 can potentially increase the chance of modeling the pre-verbal to postverbal shift, not all of them are bene cial to learn. For instance, Rule 5 models the word order shift but introduces spurious ambiguity, since the nonterminals are translated in monotone order. Rule 6, which resembles the inverted rule of the Inversion Transduction Grammar  (Wu, 1997) , is highly ambiguous because its application has no lexical grounding. Rule 4 avoids both problems, and is also easier to learn, since it is lexically anchored by a preposition, (at), which we can expect to appear frequently in training. These observations will motivate us to focus on rules that model non-monotone reordering of phrases surrounding a lexical item on the target side. More formally, we de ne P ori t (ori t (Y, X)|Y ), where ori t (Y, X) ? {MA, RA, MG, RG} is the orientation of a target phrase X with a source function word Y as the reference point.  

 Experiments We evaluated the generalization of Hiero to include XX rules on a Chinese-to-English translation task. We treat the N = 128 most frequent words in the corpus as function words, an approximation that has worked well in the past and minimized dependence on language-speci c resources  (Setiawan et al., 2007) . We report BLEU r4n4 and assess signicance using the standard bootstrapping approach. We trained on the NIST MT06 Eval corpus excluding the UN data (approximately 900K sentence pairs), segmenting Chinese using the Harbin segmenter  (Zhao et al., 2001) .  To our knowledge, the work reported here is the rst to relax the non-adjacent nonterminals constraint in hierarchical phrase-based models. The results con rm that judiciously adding rules to a Hiero grammar, adjusting the modeling accordingly, can achieve signi cant gains. Although we found that XX-nonmono rules performed better than general XX rules, we believe the latter may nonetheless prove useful. Manually inspecting our system's output, we nd that the output is often shorter than the references, and the missing words often correspond to function words that are modeled by those rules. Using XX rules to model legitimate word insertions is a topic for future work. ( 2007 )Figure 1 : 20071 Figure 1: A Chinese-English verb phrase translation 

 3 Addressing XX Rule ChallengesThe rst challenge created by introducing XX rules is computational: relaxing the constraint signicantly increases the grammar size. Motivated by our earlier discussion, we address this by permitting only rules that model non-monotone reordering, i.e. those rules whose nonterminals are projected into the target language in a different word order, leaving monotone mappings to be handled by the glue rules as previously. This choice helps keep the search space more manageable, and also avoids spurious ambiguity. In addition, we disallow rules in which nonterminals are adjacent on both the source and target sides, by imposing the non adjacent nonterminal constraint on the target side whenever the constraint is relaxed on the source side. This forces any nonmonotone reorderings to always be grounded in lexical evidence. We refer to the permitted subset of XX rules as XX-nonmono rules.The second challenge involves modeling: introducing XX rules places them in competition with the existing glue rules. In particular, these two kinds of rules try to model the same phenomena, namely the translations of phrases that appear next to each other. However, they differ in terms of the features associated with the rules. XX rules will be associated with the same features as any other hierarchical rules, since they are all learned via an identical training method. In contrast, glue rules are introduced into the grammar in an ad hoc manner, and the only feature associated with them is a glue penalty. These distinct feature sets makes direct comparison of scores unreliable. As a result the decoder may simply prefer to always select glue rules because they are associated with fewer features resulting in adjacent phrases always being translated in a monotone order. To address this issue, we introduce a new model, which we call the target-side function words orientation-based model, or simply P orit , which evaluates the application of the two kinds of rules on the same context, i.e. for our ex-The P orit model is motivated by the function words reordering hypothesis (Setiawan et al., 2007) , which suggests that function words encode essential information about the (re)ordering of their neighboring phrases. In contrast to Setiawan et al. (2007) , who looked at neighboring contexts for function words on the source side, we focus here on modeling the in uence of function words on neighboring phrases on the target side. We argue that this focus better ts our purpose, since the phrases that we want to model are the function words' neighbors on the target side, as illustrated in Fig.1.To develop this idea, we rst de ne an ori t function that takes a source function word as a reference point, along with its neighboring phrase on the target side. The ori t function outputs one of the following orientation values (Nagata et al., 2006) : Monotone-Adjacent (MA); Reverse-Adjacent (RA); Monotone-Gap (MG); and Reverse-Gap (RG). The Monotone/Reverse distinction indicates whether the source order follows the target order. The Adjacent/Gap distinction indicates whether the two phrases are adjacent or separated by an intervening phrase on the source side. For example, in Fig. 1, the value of ori t for right neighbor Eastern division with respect to function word (at) is MA, since its corresponding source phrase is adjacent to (at) and their order is preserved on the English side. The value for left neighbor rank 10th with respect to (at) is RG, since is separated from (at) and their order is reversed on the English side. 

 Our 5-gram language model with modi ed Kneser-Ney smoothing was trained on the English side of our training data plus portions of the Gigaword v2 English corpus. We optimized the feature weights using minimum error rate training, using the NIST MT03 test set as the development set. We report the results on the NIST 2006 evaluation test (MT06) and the NIST 2008 evaluation test (MT08). 

 Table 1 1 orit model (+XX-nonmono+ori t ). The combination produces a signi cant, consistent gain across all test sets. This result suggests that the orientation model contributes more strongly in unseen cases when Hiero also considers non-monotone reordering. We interpret this result as a validation of our hypothesis that carefully relaxing the non- reports experiments in an incremental fashion, starting from the baseline model (the orig- inal Hiero), then adding different sets of rules, and nally adding the orientation-based model. In our rst experiments, we investigated the introduction of three different sets of XX rules. First (+itg), we simply add the ITG's inverted rule (Rule 6) to the baseline system in an ad-hoc manner, similar to the glue rules. This hurts performance consistently across MT06 and MT08 sets, which we suspect is a result of ITG rule applications often aggravating search error. Second (+XX), we permitted general XX rules. This results in a grammar size increase of 25-26%, ltering out rules irrelevant for the test set, 

			 In fact, separate models are developed for left and right neighbors, although for clarity we suppress this distinction throughout.
