title
Language Modeling Teaches You More Syntax than Translation Does: Lessons Learned Through Auxiliary Task Analysis

abstract


Introduction Recently, researchers have found that deep LSTMs  (Hochreiter and Schmidhuber, 1997)  trained on tasks like machine translation learn substantial syntactic and semantic information about their input sentences, including part-of-speech  (Belinkov et al., 2017a,b; Blevins et al., 2018) . These findings begin to shed light on why pretrained representations, like ELMo and CoVe, are so beneficial for neural language understanding models  (Peters et al., 2018; McCann et al., 2017) . We still, though, do not yet have a clear understanding of how the choice of pretraining objective affects the type of linguistic information that models learn. With this in mind, we compare four objectives-language modeling, translation, skip-thought, and autoencoding-on their ability to induce syntactic and part-of-speech information, holding constant the quantity and genre of the training data, as well as the LSTM architecture. 

 Methodology We control for the data domain by exclusively training on datasets from WMT 2016  (Bojar et al., 2016) . We train models on all tasks using the parallel En-De corpus, which allows us to make fair comparisons across all tasks. We also augment the parallel data with a large monolingual corpus from WMT to examine how the performance of the unsupervised tasks scales with more data. We analyze representations learned by language models and by the encoders of sequenceto-sequence models. 1 Following  Belinkov et al. (2017a) , after pretraining, we fix the LSTM model parameters and use the hidden states to train auxiliary classifiers on several probing tasks. We use two syntactic evaluation tasks: part-of-speech (POS) tagging on Penn Treebank WSJ  (Marcus et al., 1993)  and Combinatorial Categorical Grammar (CCG) supertagging on CCG Bank  (Hockenmaier and Steedman, 2007) . CCG supertagging allows us to measure the degree to which models learn syntactic structure above the word. We also measure how much LSTMs simply memorize input sequences with a word identity prediction task. 

 Results Comparing Pretraining Tasks For all pretraining dataset sizes, bidirectional language model (BiLM) and translation encoder representations outperform skip-thought and autoencoder representations on both POS and CCG tagging. Translation encoders, however, slightly underperform BiLMs, even when both models are trained on the same amount of data. Furthermore, BiLMs trained on the smallest amount of data (1 million sentences) outperform models trained on all other tasks using larger dataset sizes (5 million sentences for translation, and 63 million sentences for skip-thought and autoencoding). Especially since BiLMs do not require aligned data to train, the superior performance of BiLM representations on these tasks suggests that BiLMs (like ELMo) are better than translation encoders (like CoVe) for transfer learning of syntactic information. Untrained Baseline Surprisingly, we find that the untrained LSTM baseline-frozen after random initialization-performs quite well on syntactic tagging tasks (a few percentage points behind BiLMs) when using all auxiliary data; however, decreasing the amount of classifier training data leads to a significantly greater drop in the untrained encoder performance compared to trained encoders. We hypothesize that the classifiers can recover neighboring word identity information- even from untrained LSTMs representations-and thus perform well on tagging tasks by memorizing word configurations and their associated tags from the training data. We test this hypothesis directly with the word identity task. Word Identity For this task, we train classifiers to take LSTM hidden states and predict the identities of the words from different time steps. For example, for the sentence "I love NLP ." and a time step shift of -2, we would train the classifier to take the hidden state for "NLP" and predict the word "I". While trained encoders outperform untrained ones on both POS and CCG tagging, we find that all trained LSTMs underperform untrained ones on word identity prediction. This finding confirms that trained encoders genuinely capture substantial syntactic features, beyond mere word identity, that the auxiliary classifiers can use.  Belinkov et al. (2017a)  find that, for translation models, the first layer consistently outperforms the second on POS tagging. We find that this pattern holds for all our models, except BiLMs, where the first and second layers perform equivalently. This pattern occurs even in untrained models, which suggests that POS information is stored on the lower layer not necessarily because the training tasks encourage this, but due to properties of the deep LSTM architecture. For CCG supertagging though, the second layer performs better than the first in some cases (first layer performs best for untrained LSTMs). Which layer performs best appears to be independent of absolute performance on the supertagging task. 

 Effect of Depth On word identity prediction, we find that for both trained and untrained models, the first layer outperforms the second layer when predicting the identity of the immediate neighbors of a word. However, the second layer tends to outperform the first at predicting the identity of more distant neighboring words. As is the case for convolutional neural networks, our findings suggest that depth in recurrent neural networks has the effect of increasing the "receptive field" and allows the upper layers to have representations that capture a larger context. These results reflect the findings of  Blevins et al. (2018)  that for trained models, upper levels of LSTMs encode more abstract syntactic information, since more abstract information generally requires larger context information. 

 Conclusion By controlling for the genre and quantity of the training data, we make fair comparisons between several data-rich training tasks in their ability to induce syntactic information. Our results suggest that for transfer learning, bidirectional language models like ELMo  (Peters et al., 2018)  capture more useful features than translation encodersand that this holds even on genres for which data is not abundant. Our work also highlights the interesting behavior of untrained LSTMs, which show an ability to preserve the contents of their inputs better than trained models. Figure 1 : 1 Figure 1: POS accuracies when training on different amounts of encoder and classifier data. We show results for the best performing layer of each model. The most frequent class baseline is word-conditional. 
