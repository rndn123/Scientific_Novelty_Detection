title
Low-Resource Translation as Language Modeling

abstract
We present our submission to the very low resource supervised machine translation task at the Fifth Conference on Machine Translation. The goal of this task is to create a system which translates between German and the lowresource language Upper Sorbian. We use a decoder-only transformer architecture and formulate the translation task as language modeling. To address the low-resource aspect of the problem, we pretrain over a similar language parallel corpus. Then, we employ an intermediate back-translation step before fine-tuning. Finally, we present an analysis of the system's performance.

Introduction This work describes our system for translating in both directions between German (DE) and the lowresource language Upper Sorbian (HSB). German is a widespread language with tens of millions of speakers; Upper Sorbian is a West Slavic language spoken in Germany, and it is recognized as an endangered language by  UNESCO (Moseley, 2010) . This system constitutes our submission to the shared task on very-low-resource supervised machine translation at WMT20.  1  The ultimate goal of the task is to translate a blind test set from Upper Sorbian into German and vice versa. The task is constrained, meaning that all data sets used for training are selected from a set of corpora provided by the organizers. Our primary contribution is our application of a decoder-only language-modeling architecture to a low-resource translation task, which to our knowledge is not well-investigated. In Sections 2 and 3, we discuss related work and our system itself. Sections 4, 5, and 6 describe our architecture. Sections 7 and 8 contain our results and analysis. 1 http://www.statmt.org/wmt20 

 Related Work Current approaches to machine translation include neural networks based on encoder-decoder transformers  (Vaswani et al., 2017)  and sequence-tosequence models using recurrent networks  (Chen et al., 2018) . In both of these methods, the system learns how to produce an intermediate representation of a text sequence as a basis for the output translation. Language-neutral representations have been explored more deeply in the context of mBert  (Libovick? et al., 2019) . In the case of low-resource languages, where there is an absence of adequately sized parallel corpora, recent techniques focus on transfer learning  (Zoph et al., 2016) , relying on monolingual corpora  (Lample et al., 2018) , enriching the input to the system  (Irvine and Callison-Burch, 2013) , or expanding it through back-translation  (Sennrich et al., 2016) . Techniques related to back-translation include pseudo-labeling and self-labeling. Pseudo-labeling uses partially accurate data for training  (Ratner et al., 2017)  generated from knowledge bases, heuristic functions and crowdsourcing. Selflabeling is an area that lies between self-supervised learning and pseudo-labeling  (Caron et al., 2018; Asano et al., 2020) . The model is used to predict labels for an unlabeled dataset and then is trained on this dataset. 

 Overview Our system uses a transformer architecture, though instead of the traditional encoder-decoder layout, we use a single decoder-only transformer as do  Radford et al. (2018) , formulating the translation task as a language modeling task. This architecture was suggested by  Radford et al. (2019)  and explored concretely by  Guo et al. (2019)  for widely-used languages. Unlike previous approaches, in this method there is no intermediate representation of the input; instead, the translation is predicted directly through the attention mechanism. Furthermore, in our submission, we rely on a similar-language pretraining task with a shared vocabulary, using Czech (CS) / English (EN) sentence pairs, similarly to  Kocmi and Bojar (2018)  and  Nguyen and Chiang (2017) . Finally, we supplement these techniques with traditional back-translation, using monolingual corpora in the target languages. 

 Data Preprocessing 

 Datasets In this work, we only use datasets that were made available by WMT20: ? HSB/DE parallel corpus (60K pairs) ? Monolingual HSB data (600K sentences) ? Monolingual DE news data (600K sentence subset) ? CS/EN parallel news corpus (60M pairs) For the initial pretraining, we use CS/EN parallel data. The unlabeled and labeled HSB/DE parallel data are used for the back-translation and finetuning steps. In Section 8, we also show a comparison to a reference pretraining dataset: a CS/DE parallel corpus (1.6M pairs). 

 Preprocessing Method Figure  1  shows the preprocessing of the training corpus. This method of preprocessing the corpus allows us to use a single decoder-only transformer and train it on a classical language modeling task.  Since we use a CS/EN corpus on the initial pretraining step and HSB/DE corpora on the remaining steps, we create a joint byte-pair encoding which is generated by combining all of the corpora. 

 Training Method Figure  2  shows the training method. In total, our method consists of five individual steps. These include: a pretraining step, an intermediate step made up of three sub-steps (a pre-fine-tuning step, back-translation, back-translated training), and a final fine-tuning step. The following subsections describe these steps in detail. All of these steps (except the back-translation step, which is performed in inference mode) are performed as translation tasks using parallel corpora. The parallel corpora are either real or synthetic (in the case of the corpora resulting from back-translation).   

 Initial Pretraining and Back-translation We start by pretraining the model on a language translation task using a large (60M pair) parallel corpus consisting of Czech and English. As described by  Kocmi and Bojar (2018) , large (10M pair or above) parallel pretraining corpora provide significant performance gains. This is reinforced by our findings in Section 8.3. Also, Czech and English are related to the the target languages, which can provide an additional performance benefit, according to Nguyen and Chiang (2017). Figure  2  shows the pretraining on a CS/EN translation dataset and the first round of fine-tuning on the labeled data for the HSB/DE translation tasks. In our method, we use the notion of Model states. After the pretraining step, the model reaches the Model 1 state as depicted in Figure  2 . At this step, the model is fine-tuned and reaches the state Model 2. This state is used to back-translate the monolingual HSB/DE data into parallel corpora. 

 Back-Translated Training and Final Fine-Tuning Output of the bulk translation is then saved and the Model 2 state is discarded. The bulk translation is used as a parallel corpus for the back-translated training step beginning at the Model 1 state: it consists of 600K pairs (per target language), whereby one sentence in the pair is from the monolingual corpus, and the other, parallel sentence is from the bulk translation output. After the back-translated training, the model enters the Model 3 state: this is the final state before the last fine-tuning. The last step is training the model in a supervised fashion on the labeled dataset. One should note that, at this point, the model has not seen the labeled dataset yet. The state Model 2 was trained on the labeled dataset but it is only used for the back-translation and discarded later. The final step trains the model using the highestquality dataset: human-generated translations from the source language to the target language. 

 Implementation We follow the GPT2 paper  (Radford et al., 2019)  for the model architecture, excepting hyperparameters. An overview of the system hyperparameters is shown in Table  1 . We use layer normalization, a standard dropout rate of 10%, and a learning rate of 5e-5 for all tasks. For the fine-tuning task, we employ L2 regularization. We train separate models for each translation direction. The total size of the model is 40M parameters. Our choice of hyperparameters is based on a modified grid search over the attention heads, learning rate, layer count, and embedding size. We used a single Nvidia GTX 1080TI GPU during training, and training times are shown in Figure  2 . We argue that our method is time and resource efficient, easy to reproduce, and powerful. 

 Evaluation In this section, we provide details about our implementation and the final results of the submitted system on the shared task: translation between German and Upper Sorbian. 

 Inference Versus Training During the training tasks, we combine the source text, the target text, and control tokens. To use the resulting model to perform a translation of unfamiliar text, we use a slightly modified preprocessing step: we concatenate only the source text with a translation token. Since the model is trained to perform a classical language modeling task, it begins predicting the next token probabilities of the target text. We then apply beam search (with a beam width of five) to these tokens to arrive at the final translation. 

 Results Table  2  shows the official BLEU score of our method on the blind test submission to WMT20. Submissions to the shared task ranged from 38.5 to 61.1 BLEU for DE to HSB translations and from 40.5 to 60.0 for HSB to DE translations. In this section, all BLEU scores other than the blind test are calculated on the HSB/DE public test set and reference translations provided by WMT20. Table  3  shows a sample translation. The model's word choice is a slight generalization of the German reference, with correct grammar, spelling, and capitalization. 

 Analysis 

 Performance Breakdown In order to understand the contribution of each training step to the final result, we performed experiments on different training sequences using the public HSB/DE test set provided by WMT20. The results of these experiments are reflected in Table  4 . In the table, each step is cumulative and includes the steps above it: e.g., the "back-translation" step includes both fine-tuning and back-translation, but not pretraining.  We conclude from these experiments that the most significant gains came from back-translation (around 10 BLEU), followed by the pretraining step (4-6 BLEU). For reference, we reiterate the blind test results in Table  2 . The blind test results are different from the pretraining step due to differences in the data set. 

 Pretraining Task Selection We considered using unsupervised learning as a pretraining task; however, a comparison of unsupervised pretraining in the target language with translation-task pretraining using related languages showed that the translation task had a greater impact on the final model's performance. In this experiment, we compared the effect of different pretraining tasks on the model's translation performance. Recall that our architecture formulates the translation task as a language modeling task. Since the architecture acts as a language model, it is also possible to pretrain the model, without modification, on unsupervised text in the target languages. To compare the unsupervised language modeling pretraining task with a translation pretraining task, we pretrained one model with the full HSB/DE unsupervised data set (600K sentences each, 10 epochs), a second model with a CS/DE parallel corpus (1.6M pairs, 3 epochs), and a third model with a subset of the CS/EN parallel corpus (60M pairs, 0.17 epochs), and then fine-tuned each of them using the supervised data set. We compare these models to a baseline (finetuned only) model in Table  5 . From these results, we conclude that the similar language translation tasks are more effective pretraining tasks than unsupervised language modeling in this context. The two related-language pretraining tasks were comparable in performance, though we only used a fraction of the CS/EN corpus due to its much larger size. Finally, we examined the effect of the number of pretraining epochs on the final BLEU score. As shown in Table  6 , roughly doubling the corpus size led to an increase of nearly 1.0 BLEU in the final model performance. This represents close to 20% of the performance increase we attribute to our pretraining task, which suggests that an even larger corpus, or additional pretraining epochs, would contribute further to model performance.  

 Conclusion Since our model produces high-quality translations, we have shown that a small decoder-only transformer, configured to perform classical language modeling, is an effective translation system for lowresource language pairs. Furthermore, we have shown that a similar language translation pretraining task can contribute substantially to the quality of such translation systems. Finally, we have provided an analysis of the model's components and their relative contribution to its ultimate performance. Further investigation would be needed to understand our model's relationship to other architectures under the same data sets and pretraining tasks. Figure 1 : 1 Figure 1: Preprocessing of the training corpus. The source and translation texts are concatenated with translation direction and beginning-and end-of-sequence tokens. 
