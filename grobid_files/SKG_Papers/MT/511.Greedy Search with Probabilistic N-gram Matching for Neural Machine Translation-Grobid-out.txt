title
Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation

abstract
Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias. Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation. On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework. In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias. Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.

Introduction Neural machine translation (NMT)  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014;  has now achieved impressive performance  Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Lample et al., 2018)  and draws more attention. NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word. Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser, *Corresponding Author 1989), which forces the model to generate translation strictly matching the ground-truth at the word level. However, in practice it is impossible to generate translation totally the same as ground truth. Once different target words are generated, the word-level loss cannot evaluate the translation properly, usually under-estimating the translation. In addition, the teacher forcing algorithm suffers from the exposure bias  (Ranzato et al., 2015)  as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.  Kim and Rush (2016)  proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words. Scheduled sampling(SS)  (Bengio et al., 2015; Venkatraman et al., 2015)  attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training. However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss. In contrast, sequence-level objectives, such as BLEU  (Papineni et al., 2002) , GLEU , TER  (Snover et al., 2006), and NIST (Doddington, 2002) , evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss. However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training  (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2017; Yang et al., 2017)  mainly rely on reinforcement learning algorithms  (Williams, 1992; Sutton et al., 2000)  to find an unbiased gradient estimator for the gradient update. Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable training and limited improvements.  Lamb et al. (2016) ;  Gu et al. (2017) ;  Ma et al. (2018)  respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update. However, these methods do not allow for direct optimization with respect to evaluation metrics. In this paper, we propose a method to combine the strengths of the word-level and sequencelevel training, that is the direct gradient update without gradient estimation from word-level training and the greater flexibility from sequence-level training. Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable. During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words. Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework. The experiments also indicate that greedy search strategy indeed has superiority over teacher forcing. 

 Background NMT is based on an end-to-end framework which directly models the translation probability from the source sentence x to the target sentence ?: P (?|x) = T j=1 p(? j |? <j , x, ?), (1) where T is the target length and ? is the model parameters. Given the training set D = {X M , Y M } with M sentences pairs, the training objective is to maximize the log-likelihood of the training data as ? = arg max ? {L(?)} L(?) = M m=1 l m j=1 log(p(? m j |? m <j , x m , ?)), (2) where the superior m indicates the m-th sentence in the dataset and l m is the length of m-th target sentence. In the above model, the probability of each target word p(? m j |? m <j , x m , ?) is conditioned on the previous target words. The scenario is that in the training time, the teacher forcing algorithm is employed and the ground truth words from the target sentence are fed as context, while during inference, the ground truth words are not available and the previous predicted words are instead fed as context. This discrepancy is called exposure bias. 

 Model 

 Sequence-Level Objectives Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching. Assuming that y and ? are the output sentence and the ground truth sentence with length T and T respectively, the count of an n-gram g = (g 1 , . . . , g n ) in sentence y is calculated as C y (g) = T ?n t=0 n i=1 1{g i = y t+i }, (3) where 1{?} is the indicator function. The matching count of the n-gram g between ? and y is given by C ? y (g) = min (C y (g), C ?(g)). (4) Then the precision p n and the recall r n of the predicted n-grams are calculated as follows p n = g?y C ? y (g) g?y C y (g) , (5) r n = g?y C ? y (g) g? C ?(g) . (6) BLEU, the most widely used metric for machine translation evaluation, is defined based on the n-gram precision as follows BLEU = BP ? exp( N n=1 w n log p n ), (7) where BP stands for the brevity penalty and w n is the weight for the n-gram. In contrast, GLEU is the minimum of recall and precision of 1-4 grams where 1-4 grams are counted together: GLEU = min(p 1-4 , r 1-4 ). (8) 

 probabilistic Sequence-Level Objectives In the output sentence y, the prediction probability varies among words. Some words are translated by the model with high confidence while some words are translated with high uncertainty. Figure  1 : The overview of our model with greedy search. At each decoding step, the predicted word which has the highest probability in the probability vector is selected as context and fed into the RNN, and meanwhile this word and its probability are also used to calculate the probabilistic n-gram count. However, when calculating the count of n-grams in Eq.(  3 ), all the words in the output sentence are treated equally, regardless of their respective prediction probabilities. To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(y j |y <j , x, ?) as the count of word y j , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore. Then the probabilistic count of g = (g 1 , . . . , g n ) is calculated by summing over the output sentence y as C y (g) = T ?n t=0 n i=1 1{g i = y t+i } ? p(y t+i |y <t+i , x, ?). (9) Now the probabilistic sequence-level objective can be got by replacing C y (g) with C y (g) (the tilde over the head indicates the probabilistic version) and keeping the rest unchanged. Here, we take BLEU as an example and show how the probabilistic BLEU (denoted as P-BLEU) is defined. From this purpose, the matching count of n-gram g in Eq.(  4 ) is modified as follows C ? y (g) = min( C y (g), C ?(g)). (10) and the predict precision of n-grams changes into pn = g?y C ? y (g) g?y C y (g) . ( 11 ) Finally, the probabilistic BLEU (P-BLEU) is defined as P-BLEU = BP ? exp( N n=1 w n log pn ), (12) Probabilistic GLEU (P-GLEU) can be defined in a similar way. Specifically, we denote the probabilistic precision of n-grams as P-Pn. The probabilistic precision is more reasonable than recall since the denominator in Eq.(  11 ) plays a normalization role, so we modify the definition in Eq.(  8 ) and define P-GLEU as simply the probabilistic precision of 1-4 grams. The general probabilistic loss function is: L(?) = ? M m=1 P(y m , ?m ), (13) where P represents the probabilistic sequencelevel objectives, and y m and ?m are the predicted translation and the ground truth for the m-th sentence respectively. The calculation of the probabilistic objective is illustrated in Figure  1 . This probabilistic loss can work with decoding strategies such as greedy search and teacher forcing. In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.   (Papineni et al., 2002)  for the translation task. We apply our method to an attention-based NMT system  implemented by Pytorch. Both source and target vocabularies are limited to 30K. All word embedding sizes are set to 512, and the sizes of hidden units in both encoder and decoder RNNs are also set to 512. All parameters are initialized by uniform distribution over [?0.1, 0.1]. The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model with batch size of 40. In addition, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ? = 0.95 and = 1e-6. Dropout is applied on the output layer with dropout rate of 0.5. The beam size is set to 10. 

 System 

 Performance Systems We first pretrain the baseline model by maximum likelihood estimation (MLE) and then refine the model using probabilistic sequencelevel objectives, including P-BLEU, P-GLEU and P-P2 (probabilistic 2-gram precision). In addition, we reproduce previous works which train the NMT model through minimum risk training (MRT)  (Shen et al., 2016)  and REINFORCE algo-rithm (RF)  (Ranzato et al., 2015) . When reproducing their works, we set BLEU, GLEU and 2-gram precision as training objectives respectively and find out that GLEU yields the best performance. In the following, we only report the results with training objective GLEU. Performance Table  1  shows the translation performance on test sets measured in BLEU score. Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms. We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2. Notice that our method only changes the loss function, without any modification on model structure and training data. 

 Why Pretraining We use the probabilistic loss to finetune the baseline model rather than training from scratch. This is in line with our motivation: to alleviate the exposure bias and make the model exposed to its own output during training. In the very beginning of the training, the model's translation capability is nearly zero and the generated sentences are often meaningless and do not contain useful information for the training, so it is unreasonable to directly apply the greedy search strategy. Therefore, we first apply the teacher forcing algorithm to pretrain the model, and then we let the model generate the sentences itself and learn from its own outputs. Another reason favoring pretraining is that pretraining can lower the training cost. The training cost of the introduced probabilistic loss is about three times higher than the cost of cross entropy. Without pretraining, the training time will be much higher than usual. Otherwise, the training cost is acceptable if the probabilistic loss is only for finetuning.  

 Effect of Decoding Strategy The probabilistic loss, defined in Eq.(  13 ), is computed from the model output y and reference ?. In this section, we apply two different decoding strategies to generate y: 1. teacher forcing, which uses the ground truth as decoder input. 2. greedy search, which feeds the word with maximum probability. By conducting this experiment, we attempt to figure out where the improvements come from: the modification of loss or the mitigation of exposure bias? Figure  2  shows the learning curves of the two decoding strategies with training objective P-P2. Teacher forcing raises about 0.5 BLEU improvements and greedy search outperform the teacher forcing algorithm by nearly 1 BLEU point. We conclude that the probabilistic loss has its own advantage even when trained by the teacher forcing algorithm, and greedy search is effective in alleviating the exposure bias. Notice that the greedy search strategy highly relys on the probabilistic loss and can not be conducted independently. Greedy search together with the word-level loss is very similar with the scheduled sampling(SS). However, SS is inconsistent with the word-level loss since the word-level requires strict alignment between hypothesis and reference, which can only be accomplished by the teacher forcing algorithm. 

 Correlation with Evaluation Metrics In this section, we explore how the probabilistic objective correlates with the real evaluation metric. We randomly sample 100 pairs of sentences from the training set and compute their P-GLEU and GLEU scores  indicates that GLEU have better performance in the sentencelevel evaluation than BLEU). Directly computing the correlation between GLEU and P-GLEU gives the correlation coefficient 0.86, which indicates strong correlation. In addition, we draw the scatter diagram of the 100 pairs of sentences in Figure  3  with GLEU as x-axis and P-GLEU as y-axix. Figure  3  shows that P-GLEU correlates well with GLEU, suggesting that it is reasonable to directly train the NMT model with P-GLEU. 

 Conclusion Word-level loss cannot evaluate the translation properly and suffers from the exposure bias, and sequence-level objectives are usually indifferentiable and require gradient estimation. We propose probabilistic sequence-level objectives based on ngram matching, which relieve the dependence on gradient estimation and can directly train the NMT model. Experiment results show that our method significantly outperforms previous sequence-level training works and successfully alleviates the exposure bias through performing greedy search. Figure 2 : 2 Figure 2: learning curves of different decoding strategies with training objective P-P2. 
