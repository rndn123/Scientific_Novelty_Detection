title
Addressing Word-order Divergence in Multilingual Neural Machine Translation for Extremely Low Resource Languages

abstract
Transfer learning approaches for Neural Machine Translation (NMT) trains a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to major improvements in the translation quality in extremely low-resource scenarios.

Introduction Transfer learning for multilingual Neural Machine Translation (NMT)  (Zoph et al., 2016; Dabre et al., 2017; Nguyen and Chiang, 2017)  attempts to improve the NMT performance on the source to target language pair (child task) using an assisting source language (assisting to target language translation is the parent task). Here, the parent model is trained on the assisting and target language parallel corpus and the trained weights are used to initialize the child model. If source-target language pair parallel corpus is available, the child model can further be fine-tuned. The weight initialization reduces the requirement on the training data for the source-target language pair by transferring knowledge from the parent task, thereby improving the performance on the child task. However, the divergence between the source and the assisting language can adversely impact the benefits obtained from transfer learning. Multiple studies have shown that transfer learning works best when the languages are related  (Zoph et al., 2016; Nguyen and Chiang, 2017; Dabre et al., 2017) .  Zoph et al. (2016)  studied the influence of language divergence between languages chosen for training the parent and the child model, and showed that choosing similar languages for training the parent and the child model leads to better improvements from transfer learning. Several studies have tried to address the lexical divergence between the source and the target languages either by using Byte Pair Encoding (BPE) as basic input representation units  (Nguyen and Chiang, 2017)  or character-level NMT system  (Lee et al., 2017)  or bilingual embeddings  (Gu et al., 2018) . However, the effect of word order divergence and its mitigation has not been explored. In a practical setting, it is not uncommon to have source and assisting languages with different word order. For instance, it is possible to find parallel corpora between English (SVO word order) and some Indian (SOV word order) languages, but very little parallel corpora between Indian languages. Hence, it is natural to use English as an assisting language for inter-Indian language translation. To address the word order divergence, we propose to pre-order the assisting language sentences (SVO) to match the word order of the source language (SOV). We consider an extremely resourceconstrained scenario, where there is no parallel corpus for the child task. From our experiments, we show that there is a significant increase in the translation accuracy for the unseen source-target language pair. 

 Related Work To the best of our knowledge, no work has addressed word order divergence in transfer learning for multilingual NMT. However, some work exists for other NLP tasks in a multilingual setting. For Named Entity Recognition (NER),  Xie et al. (2018)  use a self-attention layer after the Bi-LSTM layer to address word-order divergence for Named Entity Recognition (NER) task. The approach does not show any significant improvements, possibly because the divergence has to be addressed before/during construction of the contextual embeddings in the Bi-LSTM layer.  Joty et al. (2017)  use adversarial training for cross-lingual questionquestion similarity ranking. The adversarial training tries to force the sentence representation generated by the encoder of similar sentences from different input languages to have similar representations. Pre-ordering the source language sentences to match the target language word order has been found useful in addressing word-order divergence for Phrase-Based SMT  (Collins et al., 2005; Ramanathan et al., 2008; Navratil et al., 2012; . For  NMT, Ponti et al. (2018)  and  Kawara et al. (2018)  have explored preordering.  Ponti et al. (2018)  demonstrated that by reducing the syntactic divergence between the source and the target languages, consistent improvements in NMT performance can be obtained. On the contrary,  Kawara et al. (2018)  reported drop in NMT performance due to pre-ordering. Note that these works address source-target divergence, not divergence between source languages in multilingual NMT scenario. 

 Proposed Solution Consider the task of translating for an extremely low-resource language pair. The parallel corpus between the two languages, if available may be too small to train an NMT model. Similar to  Zoph et al. (2016) , we use transfer learning to overcome data sparsity between the source and the target languages. We choose English as the assisting language in all our experiments. In our resource-scarce scenario, we have no parallel corpus for training the child model. Hence, at test time, the source language sentence is translated using the parent model after performing a word-byword translation from source to the assisting language using a bilingual dictionary. Since the source language and the assisting language (English) have different word order, we hypothesize that it leads to inconsistencies in the con- 

 Before Reordering After Reordering  textual representations generated by the encoder for the two languages. Specifically, given an English sentence (SVO word order) and its translation in the source language (SOV word order), the encoder representations for words in the two sentences will be different due to different contexts of synonymous words. This could lead to the attention and the decoder layers generating different translations from the same (parallel) sentence in the source or assisting language. This is undesirable as we want the knowledge to be transferred from the parent model (assisting source? target) to the child model (source?target). 

 S In this paper, we propose to pre-order English sentences (assisting language sentences) to match the source language word-order and train the parent model on the pre-ordered corpus. Table  1  shows one of the pre-ordering rules  (Ramanathan et al., 2008 ) used along with an example sentence illustrating the effect of pre-ordering. This will ensure that context of words in the parallel source and assisting language sentences are similar, leading to consistent contextual representations across the source languages. Pre-ordering may also be beneficial for other word order divergence scenarios (e.g., SOV to SVO), but we leave verification of these additional scenarios for future work. 

 Experimental Setup In this section, we describe the languages experimented with, datasets used, the network hyperparameters used in our experiments. Languages: We experimented with English ? Hindi translation as the parent task. English is the assisting source language. Bengali, Gujarati, Marathi, Malayalam and Tamil are the source languages, and translation from these to Hindi constitute the child tasks. Hindi, Bengali, Gujarati and Marathi are Indo-Aryan languages, while Malayalam and Tamil are Dravidian languages. All these languages have a canonical SOV word order. Datasets: For training English-Hindi NMT systems, we use the IITB English-Hindi parallel corpus  (Kunchukuttan et al., 2018)  (1.46M sentences from the training set) and the ILCI English-Hindi parallel corpus (44.7K sentences). The ILCI (Indian Language Corpora Initiative) multilingual parallel corpus (Jha, 2010) 1 spans multiple Indian languages from the health and tourism domains. We use the 520-sentence dev-set of the IITB parallel corpus for validation. For each child task, we use 2K sentences from ILCI corpus as test set. Network: We use OpenNMT-Torch  (Klein et al., 2018)  to train the NMT system. We use the standard encoder-attention-decoder architecture (Bahdanau et al., 2015) with input-feeding approach  (Luong et al., 2015) . The encoder has two layers of bidirectional LSTMs with 500 neurons each and the decoder contains two LSTM layers with 500 neurons each. We use a mini-batch of size 50 and a dropout layer. We begin with an initial learning rate of 1.0 and continue training with exponential decay till the learning rate falls below 0.001. The English input is initialized with pre-trained fast-Text embeddings  (Grave et al., 2018)  2 . English and Hindi vocabularies consists of 0.27M and 50K tokens appearing at least 2 and 5 times in the English and Hindi training corpus respectively. For representing English and other source languages into a common space, we translate each word in the source language into English using a bilingual dictionary (we used Google Translate to get single word translations). In an end-to-end solution, it would be ideal to use bilingual embeddings or obtain word-by-word translations via bilingual embeddings  (Xie et al., 2018) . However, publicly available bilingual embeddings for English-Indian languages are not good enough for obtaining good-quality, bilingual representations  (Smith et al., 2017; Jawanpuria et al., 2019)  and publicly available bilingual dictionaries have limited coverage. The focus of our study is the in-  fluence of word-order divergence on Multilingual NMT. We do not want bilingual embeddings quality or bilingual dictionary coverage to influence the experiments, rendering our conclusions unreliable. Hence, we use the above mentioned largecoverage bilingual dictionary. Pre-ordering: We use CFILT-preorder 3 for prereordering English sentences. It contains two preordering configurations: (1) generic rules (G) that apply to all Indian languages  (Ramanathan et al., 2008) , and (2) hindi-tuned rules (HT) which improves generic rules by incorporating improvements found through error analysis of English-Hindi reordering  (Patel et al., 2013) . The Hindituned rules improve translation for other English to Indian language pairs too . 

 Results We experiment with two scenarios: (a) an extremely resource scarce scenario with no parallel corpus for child tasks, (b) varying amounts of parallel corpora available for child task. 

 No Parallel Corpus for Child Task The results from our experiments are presented in the scores. We observe that both the pre-ordering models significantly improve the translation quality over the no-preordering models for all the language pairs. The results support our hypothesis that word-order divergence can limit the benefits of multilingual translation. Thus, reducing the word order divergence improves translation in extremely low-resource scenarios. An analysis of the outputs revealed that preordering significantly reduced the number of UNK tokens (placeholder for unknown words) in the test output (Table  3 ). We hypothesize that due to word order divergence between English and Indian languages, the encoder representation generated is not consistent leading to decoder generating unknown words. However, the pre-ordered models generate better encoder representations leading to lesser number of UNK tokens and better translation, which is also reflected in the BLEU scores and Table  4 . 

 Parallel Corpus for Child Task We study the impact of child task parallel corpus on pre-ordering. To this end, we finetune the parent task model with the child task parallel corpus. Table  5  shows the results for Bengali-Hindi,  Gujarati-Hindi, Marathi-Hindi, Malayalam-Hindi, and Tamil-Hindi translation.  We observe that pre-ordering is beneficial when almost no child task corpus is available. As the child task corpus increases, the model learns the on edit distance, hence it can handle morphological variations and cognates  (Virpioja and Gr?nroos, 2015) . word order of the source language; hence, the non pre-ordering models perform almost as good as or sometimes better than the pre-ordered ones. The non pre-ordering model is able to forget the wordorder of English and learn the word order of Indian languages. We attribute this behavior of the non pre-ordered model to the phenomenon of catastrophic forgetting  (McCloskey and Cohen, 1989; French, 1999)  which enables the model to learn the word-order of the source language when sufficient child task parallel corpus is available. We also compare the performance of the finetuned model with the model trained only on the available source-target parallel corpus with randomly initialized weights  (No Transfer Learning) . Transfer learning, with and without pre-ordering, is better compared to training only on the small source-target parallel corpus. 

 Conclusion In this paper, we show that handling word-order divergence between the source and assisting languages is crucial for the success of multilingual NMT in an extremely low-resource setting. We show that pre-ordering the assisting language to match the word order of the source language significantly improves translation quality in an extremely low-resource setting. If pre-ordering is not possible, fine-tuning on a small source-target parallel corpus is sufficient to overcome word order divergence. While the current work focused on Indian languages, we would like to validate the hypothesis on a more diverse set of languages. We would also like to explore alternative methods to address word-order divergence which do not re-quire expensive parsing of the assisting language corpus. Further, use of pre-ordering to address word-order divergence for multilingual training of other NLP tasks can be explored. Table 1 : 1 Example showing transitive verb before and after reordering (Adapted from Chatterjee et al. (2014)) 
