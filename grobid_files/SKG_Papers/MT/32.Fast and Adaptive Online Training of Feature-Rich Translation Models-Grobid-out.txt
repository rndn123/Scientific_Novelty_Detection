title
Fast and Adaptive Online Training of Feature-Rich Translation Models

abstract
We present a fast and scalable online method for tuning statistical machine translation models with large feature sets. The standard tuning algorithm-MERT-only scales to tens of features. Recent discriminative algorithms that accommodate sparse features have produced smaller than expected translation quality gains in large systems. Our method, which is based on stochastic gradient descent with an adaptive learning rate, scales to millions of features and tuning sets with tens of thousands of sentences, while still converging after only a few epochs. Large-scale experiments on Arabic-English and Chinese-English show that our method produces significant translation quality gains by exploiting sparse features. Equally important is our analysis, which suggests techniques for mitigating overfitting and domain mismatch, and applies to other recent discriminative methods for machine translation.

Introduction Sparse, overlapping features such as words and ngram contexts improve many NLP systems such as parsers and taggers. Adaptation of discriminative learning methods for these types of features to statistical machine translation (MT) systems, which have historically used idiosyncratic learning techniques for a few dense features, has been an active research area for the past half-decade. However, despite some research successes, feature-rich models are rarely used in annual MT evaluations. For example, among all submissions to the WMT and IWSLT 2012 shared tasks, just one participant tuned more than 30 features  (Hasler et al., 2012a) . Slow uptake of these methods may be due to implementation complexities, or to practical difficulties of configuring them for specific translation tasks  (Gimpel and Smith, 2012; Simianer et al., 2012, inter alia) . We introduce a new method for training featurerich MT systems that is effective yet comparatively easy to implement. The algorithm scales to millions of features and large tuning sets. It optimizes a logistic objective identical to that of PRO  (Hopkins and May, 2011)  with stochastic gradient descent, although other objectives are possible. The learning rate is set adaptively using AdaGrad  (Duchi et al., 2011) , which is particularly effective for the mixture of dense and sparse features present in MT models. Finally, feature selection is implemented as efficient L 1 regularization in the forward-backward splitting (FOBOS) framework  (Duchi and Singer, 2009) . Experiments show that our algorithm converges faster than batch alternatives. To learn good weights for the sparse features, most algorithms-including ours-benefit from more tuning data, and the natural source is the training bitext. However, the bitext presents two problems. First, it has a single reference, sometimes of lower quality than the multiple references in tuning sets from MT competitions. Second, large bitexts often comprise many text genres  (Haddow and Koehn, 2012) , a virtue for classical dense MT models but a curse for high dimensional models: bitext tuning can lead to a significant domain adaptation problem when evaluating on standard test sets. Our analysis separates and quantifies these two issues. We conduct large-scale translation quality experiments on Arabic-English and Chinese-English. As baselines we use MERT  (Och, 2003) , PRO, and the Moses  implementation of k-best MIRA, which  Cherry and Foster (2012)  recently showed to work as well as online MIRA  (Chiang, 2012)  for feature-rich models. The first experiment uses standard tuning and test sets from the NIST OpenMT competitions. The second experiment uses tuning and test sets sampled from the large bitexts. The new method yields significant improvements in both experiments. Our code is included in the Phrasal  (Cer et al., 2010)  toolkit, which is freely available. 

 Adaptive Online Algorithms Machine translation is an unusual machine learning setting because multiple correct translations exist and decoding is comparatively expensive. When we have a large feature set and therefore want to tune on a large data set, batch methods are infeasible. Online methods can converge faster, and in practice they often find better solutions  (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia) . Recall that stochastic gradient descent (SGD), a fundamental online method, updates weights w according to w t = w t?1 ? ? t (w t?1 ) (1) with loss function 1 t (w) of the t th example, (sub)gradient of the loss with respect to the parameters ? t (w t?1 ), and learning rate ?. SGD is sensitive to the learning rate ?, which is difficult to set in an MT system that mixes frequent "dense" features (like the language model) with sparse features (e.g., for translation rules). Furthermore, ? applies to each coordinate in the gradient, an undesirable property in MT where good sparse features may fire very infrequently. We would instead like to take larger steps for sparse features and smaller steps for dense features. 

 AdaGrad AdaGrad is a method for setting an adaptive learning rate that comes with good theoretical guarantees. The theoretical improvement over SGD is most significant for high-dimensional, sparse features. AdaGrad makes the following update: w t = w t?1 ? ? 1/2 t ? t (w t?1 ) (2) ? ?1 t = ? ?1 t?1 + ? t (w t?1 )? t (w t?1 ) = t i=1 ? i (w i?1 )? i (w i?1 ) (3) A diagonal approximation to ? can be used for a high-dimensional vector w t . In this case, AdaGrad is simple to implement and computationally cheap. Consider a single dimension j, and let scalars v t = w t,j , g t = ? j t (w t?1 ), G t = t i=1 g 2 i , then the update rule is v t = v t?1 ? ? G ?1/2 t g t (4) G t = G t?1 + g 2 t (5) Compared to SGD, we just need to store G t = ? ?1 t,jj for each dimension j. 1 We specify the loss function for MT in section 3.1. 

 Prior Online Algorithms in MT AdaGrad is related to two previous online learning methods for MT. MIRA  Chiang et al. (2008)  described an adaption of MIRA  (Crammer et al., 2006)  to MT. MIRA makes the following update: w t = arg min w 1 2? w ? w t?1 2 2 + t (w) (6) The first term expresses conservativity: the weight should change as little as possible based on a single example, ensuring that it is never beneficial to overshoot the minimum. The relationship to SGD can be seen by linearizing the loss function t (w) ? t (w t?1 ) + (w ? w t?1 ) ? t (w t?1 ) and taking the derivative of (  6 ). The result is exactly (1). AROW  Chiang (2012)  adapted AROW  (Crammer et al., 2009)  to MT. AROW models the current weight as a Gaussian centered at w t?1 with covariance ? t?1 , and does the following update upon seeing training example x t : w t , ? t = arg min w,? 1 ? D KL (N (w, ?)||N (w t?1 , ? t?1 )) + t (w) + 1 2? x t ?x t (7) The KL-divergence term expresses a more general, directionally sensitive conservativity. Ignoring the third term, the ? that minimizes the KL is actually ? t?1 . As a result, the first two terms of (7) generalize MIRA so that we may be more conservative in some directions specified by ?. To see this, we can write out the KL-divergence between two Gaussians in closed form, and observe that the terms involving w do not interact with the terms involving ?: w t = arg min w 1 2? (w ? w t?1 ) ? ?1 t?1 (w ? w t?1 ) + t (w) (8) ? t = arg min ? 1 2? log |? t?1 | |?| + 1 2? Tr ? ?1 t?1 ? + 1 2? x t ?x t (9) The third term in (7), called the confidence term, gives us adaptivity, the notion that we should have smaller variance in the direction v as more data x t is seen in direction v. For example, if ? is diagonal and x t are indicator features, the confidence term then says that the weight for a rarer feature should have more variance and vice-versa. Recall that for generalized linear models ? t (w) ? x t ; if we substitute x t = ? t ? t (w) into (9), differentiate and solve, we get: ? ?1 t = ? ?1 t?1 + x t x t = ? ?1 0 + t i=1 ? 2 i ? i (w i?1 )? i (w i?1 ) (10) The precision ? ?1 t generally grows as more data is seen. Frequently updated features receive an especially high precision, whereas the model maintains large variance for rarely seen features. If we substitute (10) into (8), linearize the loss t (w) as before, and solve, then we have the linearized AROW update w t = w t?1 ? ? t ? t (w t?1 ) (11) which is also an adaptive update with per-coordinate learning rates specified by ? t (as opposed to ? 1/2 t in AdaGrad). 

 Comparing AdaGrad, MIRA, AROW Compare (3) to (10) and observe that if we set ? ?1 0 = 0 and ? t = 1, then the only difference between the AROW update (11) and the AdaGrad update (2) is a square root. Under a constant gradient, AROW decays the step size more aggressively (1/t) compared to AdaGrad (1/ ? t), and it is sensitive to the specification of ? ?1 0 . Informally, SGD can be improved in the conservativity direction using MIRA so the updates do not overshoot. Second, SGD can be improved in the adaptivity direction using AdaGrad where the decaying stepsize is more robust and the adaptive stepsize allows better weight updates to features differing in sparsity and scale. Finally, AROW combines both adaptivity and conservativity. For MT, adaptivity allows us to deal with mixed dense/sparse features effectively without specific normalization. Why do we choose AdaGrad over AROW? MIRA/AROW requires selecting the loss function (w) so that w t can be solved in closed-form, by a quadratic program (QP), or in some other way that is better than linearizing. This usually means choosing a hinge loss. On the other hand, Ada-Grad/linearized AROW only requires that the gradient of the loss function can be computed efficiently. Algorithm 1 Adaptive online tuning for MT. Require: Tuning set {fi, e 1:k i }i=1:M 1: Set w0 = 0 2: Set t = 1 3: repeat 4: for i in 1 . . . M in random order do 5: Decode n-best list Ni for fi 6: Sample pairs {dj,+, dj,?}j=1:s from Ni 7: Compute Dt = {?(dj,+) ? ?(dj,?)}j=1:s 8: Set gt = ? (Dt; wt?1)} 9: Set ? ?1 t = ? ?1 t?1 + gtg t Eq. (3) 10: Update wt = wt?1 ? ? 1/2 t gt Eq. (  2 ) 11: Regularize wt Eq. (  15 ) 12: Set t = t + 1 13: end for 14: until convergence Linearized AROW, however, is less robust than Ada-Grad empirically 2 and lacks known theoretical guarantees. Finally, by using AdaGrad, we separate adaptivity from conservativity. Our experiments suggest that adaptivity is actually more important. 

 Adaptive Online MT Algorithm 1 shows the full algorithm introduced in this paper. AdaGrad (lines 9-10) is a crucial piece, but the loss function, regularization technique, and parallelization strategy described in this section are equally important in the MT setting. 

 Pairwise Logistic Loss Function Algorithm 1 lines 5-8 describe the gradient computation. We cast MT tuning as pairwise ranking  (Herbrich et al., 1999, inter alia) , which Hopkins and May (2011) applied to MT. The pairwise approach results in simple, convex loss functions suitable for online learning. The idea is that for any two derivations, the ranking predicted by the model should be consistent with the ranking predicted by a gold sentence-level metric G like BLEU+1  (Lin and Och, 2004) . Consider a single source sentence f with associated references e 1:k . Let d be a derivation in an n-best list of f that has the target e = e(d) and the feature map ?(d). Let M (d) = w ? ?(d) be the model score. For any derivation d + that is better than d ? under G, we desire pairwise agreement such that G e(d + ), e 1:k > G e(d ? ), e 1:k ? M (d + ) > M (d ? ) 2 According to experiments not reported in this paper. Ensuring pairwise agreement is the same as ensuring w ? [?(d + ) ? ?(d ? )] > 0. For learning, we need to select derivation pairs (d + , d ? ) to compute difference vectors x + = ?(d + ) ? ?(d ? ). Then we have a 1-class separation problem trying to ensure w ? x + > 0. The derivation pairs are sampled with the algorithm of  Hopkins and May (2011) . We compute difference vectors D t = {x 1:s + } (Algorithm 1 line 7) from s pairs (d + , d ? ) for source sentence f t . We use the familiar logistic loss: t (w) = (D t , w) = ? x + ?Dt log 1 1 + e ?w?x + (12) Choosing the hinge loss instead of the logistic loss results in the 1-class SVM problem. The 1class separation problem is equivalent to the binary classification problem with x + = ?(d + ) ? ?(d ? ) as positive data and x ? = ?x + as negative data, which may be plugged into an existing logistic regression solver. We find that Algorithm 1 works best with minibatches instead of single examples. In line 4 we simply partition the tuning set so that i becomes a mini-batch of examples. 

 Updating and Regularization Algorithm 1 lines 9-11 compute the adaptive learning rate, update the weights, and apply regularization. Section 2.1 explained the AdaGrad learning rate computation. To update and regularize the weights we apply the Forward-Backward Splitting (FOBOS)  (Duchi and Singer, 2009)  framework, which separates the two operations. The two-step FOBOS update is w t? 1 2 = w t?1 ? ? t?1 ? t?1 (w t?1 ) (13) w t = arg min w 1 2 w ? w t? 1 2 2 2 + ? t?1 r(w) (14) where (  13 ) is just an unregularized gradient descent step and (  14 ) balances the regularization term r(w) with staying close to the gradient step. Equation (  14 ) permits efficient L 1 regularization, which is well-suited for selecting good features from exponentially many irrelevant features  (Ng, 2004) . It is well-known that feature selection is very important for feature-rich MT. For example, simple indicator features like lexicalized re-ordering classes are potentially useful yet bloat the the feature set and, in the worst case, can negatively impact Algorithm 2 "Stale gradient" parallelization method for Algorithm 1. Require: Tuning set {fi, e 1:k i }i=1:M 1: Initialize threadpool p1, . . . , pj 2: Set t = 1 3: repeat 4: for i in 1 . . . M in random order do 5: Wait until any thread p is idle 6: Send (fi, e 1:k i , t) to p Alg. 1 lines 5-8 7: while ? p done with gradient g t do t ? t 8: Update wt = wt?1 ? ?g t Alg. 1 lines 9-11 9: Set t = t + 1 10: end while 11: end for 12: until convergence search. Some of the features generalize, but many do not. This was well understood in previous work, so heuristic filtering was usually applied  (Chiang et al., 2009, inter alia) . In contrast, we need only select an appropriate regularization strength ?. Specifically, when r(w) = ? w 1 , the closedform solution to (  14 ) is w t = sign(w t? 1 2 ) |w t? 1 2 | ? ? t?1 ? + (15) where [x] + = max(x, 0) is the clipping function that in this case sets a weight to 0 when it falls below the threshold ? t?1 ?. It is straightforward to adapt this to AdaGrad with diagonal ? by setting each dimension of ? t?1,j = ? 1 2 t,jj and by taking element-wise products. We find that ? t?1 (w t?1 ) only involves several hundred active features for the current example (or mini-batch). However, naively following the FOBOS framework requires updating millions of weights. But a practical benefit of FOBOS is that we can do lazy updates on just the active dimensions without any approximations. 

 Parallelization Algorithm 1 is inherently sequential like standard online learning. This is undesirable in MT where decoding is costly. We therefore parallelize the algorithm with the "stale gradient" method of  Langford et al. (2009)  (Algorithm 2). A fixed threadpool of workers computes gradients in parallel and sends them to a master thread, which updates a central weight vector. Crucially, the weight updates need not be applied in order, so synchronization is unnecessary; the workers only idle at the end of an epoch. The consequence is that the update in line 8 of Algorithm 2 is with respect to gradient g t with t ? t.  Langford et al. (2009)  gave convergence results for stale updating, but the bounds do not apply to our setting since we use L 1 regularization. Nevertheless,  Gimpel et al. (2010)  applied this framework to other non-convex objectives and obtained good empirical results. Our asynchronous, stochastic method has practical appeal for MT. During a tuning run, the online method decodes the tuning set under many more weight vectors than a MERT-style batch method. This characteristic may result in broader exploration of the search space, and make the learner more robust to local optima local optima  (Liang and Klein, 2009; Bottou and Bousquet, 2011, inter alia) . The adaptive algorithm identifies appropriate learning rates for the mixture of dense and sparse features. Finally, large data structures such as the language model (LM) and phrase table exist in shared memory, obviating the need for remote queries. 

 Experiments We built Arabic-English and Chinese-English MT systems with Phrasal  (Cer et al., 2010) , a phrasebased system based on alignment templates  (Och and Ney, 2004) . The corpora 3 in our experiments (Table  1 ) derive from several LDC sources from 2012 and earlier. We de-duplicated each bitext according to exact string match, and ensured that no overlap existed with the test sets. We produced alignments with the Berkeley aligner  (Liang et al., 2006b)  with standard settings and symmetrized via the grow-diag heuristic. For each language we used SRILM  (Stolcke, 2002)  to estimate 5-gram LMs with modified Kneser-Ney smoothing. We included the monolingual English data and the respective target bitexts. 

 Feature Templates The baseline "dense" model contains 19 features: the nine Moses baseline features, the hierarchical lexicalized re-ordering model of , the (log) count of each rule, and an indicator for unique rules. To the dense features we add three high dimensional "sparse" feature sets. Discrimina-  3  We tokenized the English with packages from the Stanford Parser  (Klein and Manning, 2003)  according to the Penn Treebank standard  (Marcus et al., 1993) , the Arabic with the Stanford Arabic segmenter  (Green and DeNero, 2012)  according to the Penn Arabic Treebank standard  (Maamouri et al., 2008) , and the Chinese with the Stanford Chinese segmenter  (Chang et al., 2008)  according to the Penn Chinese Treebank standard  (Xue et al., 2005) . 

 Bilingual Monolingual 

 Sentences Tokens Tokens Ar-En 6.6M 375M 990M Zh-En 9.3M 538M  

 Tuning Algorithms The primary baseline is the dense feature set tuned with MERT  (Och, 2003) . The Phrasal implementation uses the line search algorithm of  Cer et al. (2008) , uniform initialization, and 20 random starting points.  4  We tuned according to  BLEU-4 (Papineni et al., 2002) . We built high dimensional baselines with two different algorithms. First, we tuned with batch PRO using the default settings in Phrasal (L 2 regularization with ?=0.1). Second, we ran the k-best batch MIRA (kb-MIRA)  (Cherry and Foster, 2012)  implementation in Moses. We did implement an online version of MIRA, and in small-scale experiments found that the batch variant worked just as well.  Cherry and Foster (2012)  reported the same result, and their implementation is available in Moses. We ran their code with standard settings. Moses 5 also contains the discriminative phrase table implementation of  (Hasler et al., 2012b) , which is identical to our implementation using Phrasal. Moses and Phrasal accept the same phrase table and LM formats, so we kept those data structures in common. The two decoders also use the same multi-stack beam search  (Och and Ney, 2004) . For our method, we used uniform initialization, 16 threads, and a mini-batch size of 20. We found that ?=0.02 and ?=0.1 worked well on development sets for both languages.     2 . we sampled 15 derivation pairs for each tuning example and scored them with BLEU+1. 

 NIST OpenMT Experiment The first experiment evaluates our algorithm when tuning and testing on standard test sets, each with four references. When we add features, our algorithm tends to overfit to a standard-sized tuning set like MT06. We thus concatenated MT05, MT06, and MT08 to create a larger tuning set. Table  2  shows the Ar-En results. Our algorithm is competitive with MERT in the low dimensional "dense" setting, and compares favorably to PRO with the PT feature set. PRO does not benefit from additional features, whereas our algorithm improves with both additional features and data. The underperformance of kb-MIRA may result from a difference between Moses and Phrasal: Moses MERT achieves only 45.62 on MT09. Moses PRO with the PT feature set is slightly worse, e.g., 44.52 on MT09. Nevertheless, kb-MIRA does not improve significantly over MERT, and also selects an unnecessarily large model. 48.56 BLEU on MT09. For Ar-En, our algorithm thus has the desirable property of benefiting from more and better features, and more data. Table  3  shows Zh-En results. Somewhat surprisingly our algorithm improves over MERT in the dense setting. When we add the discriminative phrase table, our algorithm improves over kb-MIRA, and over batch PRO on two evaluation sets. With all features and the MT05/6/8 tuning set, we improve significantly over all other models. PRO learns a smaller model with the PT+AL+LO feature set which is surprising given that it applies L 2 regularization (AdaGrad uses L 1 ). We speculate that this may be an consequence of stochastic learning. Our algorithm decodes each example with a new weight vector, thus exploring more of the search space for the same tuning set. 

 Bitext Tuning Experiment Tables  2 and 3  show that adding tuning examples improves translation quality. Nevertheless, even the larger tuning set is small relative to the bitext from which rules were extracted.  He and Deng (2012)  and  Simianer et al. (2012)  showed significant translation quality gains by tuning on the bitext. However, their bitexts matched the genre of their test sets. Our bitexts, like those of most large-scale systems, do not. Domain mismatch matters for the dense feature set  (Haddow and Koehn, 2012) . We show that it also matters for feature-rich MT. Before aligning each bitext, we randomly sampled and sequestered 5k and 15k sentence tuning sets, and a 5k test set. We prevented overlap be- tween the tuning sets and the test set. We then tuned a dense model with MERT on MT06, and feature-rich models on both MT05/6/8 and the bitext tuning set. Table  4  shows the Ar-En results. When tuned on bitext5k the translation quality gains are significant for bitext5k-test relative to tuning on MT05/6/8, which has multiple references. However, the bitext5k models do not generalize as well to the NIST evaluation sets as represented by the MT04 result. Table  5  shows similar trends for Zh-En. D A D B |A| |B| |A ? B| MT04 MT06 70k 72k 

 Analysis 

 Feature Overlap Analysis How many sparse features appear in both the tuning and test sets? In It is also important to balance the number of features with how well weights can be learned for those features, as tuning on bitext15k produced higher coverage for MT04 but worse generalization than tuning on MT06. 

 Domain Adaptation Analysis To understand the domain adaptation issue we compared the non-zero weights in the discriminative phrase table (PT) for Ar-En models tuned on bi-text5k and MT05/6/8. Table  7  illustrates a statistical idiosyncrasy in the data for the American and British spellings of program/programme. The mass is concentrated along the diagonal, probably because MT05/6/8 was prepared by NIST, an American agency, while the bitext was collected from many sources including Agence France Presse. Of course, this discrepancy is consequential for both dense and feature-rich models. However, we observe that the feature-rich models fit the tuning data more closely. For example, the were and ? manned space flight programmes. We observed similar trends for 'defense/defence', 'analyze/analyse', etc. This particular genre problem could be addressed with language-specific pre-processing, but our system solves it in a data-driven manner. 

 Re-ordering Analysis We also analyzed re-ordering differences. Arabic matrix clauses tend to be verb-initial, meaning that the subject and verb must be swapped when translating to English. To assess re-ordering differencesif any-between the dense and feature-rich models, we selected all MT09 segments that began with one dhkr 'commented', aDaaf 'added', a c ln 'announced'. We compared the output of the MERT Dense model to our method with the full feature set, both tuned on MT06. Of the 208 source segments, 32 of the translation pairs contained different word order in the matrix clause. Our featurerich model was correct 18 times (56.3%), Dense was correct 4 times (12.5%), and neither method was correct 10 times (31.3%). (1) ref: lebanese prime minister , fuad siniora , announced a. and lebanese prime minister fuad siniora that b. the lebanese prime minister fouad siniora announced (2) ref: the newspaper and television reported a. she said the newspaper and television b. television and newspaper said In (1) the dense model (1a) drops the verb while the feature-rich model correctly re-orders and inserts it after the subject (1b). The coordinated subject in (2) becomes an embedded subject in the dense output (2a). The feature-rich model (2b) performs the correct re-ordering. The core of our method is an inner product between the adaptive learning rate vector and the gradient. This is easy to implement and is very fast even for large feature sets. Since we applied lazy regularization, this inner product usually involves hundred-dimensional vectors. Finally, our method does not need to accumulate n-best lists, a practice that slows down the other algorithms. 

 Runtime Comparison 

 Related Work Our work relates most closely to that of  Hasler et al. (2012b) , who tuned models containing both sparse and dense features with Moses. A discriminative phrase table helped them improve slightly over a dense, online MIRA baseline, but their best results required initialization with MERT-tuned weights and re-tuning a single, shared weight for the discriminative phrase table with MERT. In contrast, our algorithm learned good high dimensional models from a uniform starting point.  Chiang (2012)  adapted AROW to MT and extended previous work on online MIRA  (Chiang et al., 2008; Watanabe et al., 2007) . It was not clear if his improvements came from the novel Hope/Fear search, the conservativity gain from MIRA/AROW by solving the QP exactly, adaptivity, or sophisticated parallelization. In contrast, we show that AdaGrad, which ignores conservativity and only capturing adaptivity, is sufficient.  Simianer et al. (2012)  investigated SGD with a pairwise perceptron objective. Their best algorithm used iterative parameter mixing  (McDonald et al., 2010) , which we found to be slower than the stale gradient method in section 3.3. They regularized once at the end of each epoch, whereas we regularized each weight update. An empirical comparison of these two strategies would be an interesting future contribution. Watanabe (2012) investigated SGD and even randomly selected pairwise samples as we did. He considered both softmax and hinge losses, observing better results with the latter, which solves a QP. Their parallelization strategy required a line search at the end of each epoch. Many other discriminative techniques have been proposed based on: ramp loss  (Gimpel, 2012) ; hinge loss  (Cherry and Foster, 2012; Haddow et al., 2011; Arun and Koehn, 2007) ; maximum entropy  (Xiang and Ittycheriah, 2011; Ittycheriah and Roukos, 2007; Och and Ney, 2002) ; perceptron  (Liang et al., 2006a) ; and structured SVM  (Tillmann and Zhang, 2006) . These works use radically different experimental setups, and to our knowledge only  (Cherry and Foster, 2012)  and this work compare to at least two high dimensional baselines. Broader comparisons, though time-intensive, could help differentiate these methods. 

 Conclusion and Outlook We introduced a new online method for tuning feature-rich translation models. The method is faster per epoch than MERT, scales to millions of features, and converges quickly. We used efficient L 1 regularization for feature selection, obviating the need for the feature scaling and heuristic filtering common in prior work. Those comfortable with implementing vanilla SGD should find our method easy to implement. Even basic discriminative features were effective, so we believe that our work enables fresh approaches to more sophisticated MT feature engineering. Table 1 1 : Bilingual and monolingual corpora used in these experiments. The monolingual English data comes from the AFP and Xinhua sections of English Gigaword 4 (LDC2009T13). tive phrase table (PT): indicators for each rule in the phrase table. Alignments (AL): indica-tors for phrase-internal alignments and deleted (unaligned) source words. Discriminative re-ordering (LO): indicators for eight lexicalized re-ordering classes, including the six standard mono-tone/swap/discontinuous classes plus the two sim-pler Moses monotone/non-monotone classes. 
