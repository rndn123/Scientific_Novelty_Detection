title
On Exposure Bias, Hallucination and Domain Shift in Neural Machine Translation

abstract
The standard training algorithm in neural machine translation (NMT) suffers from exposure bias, and alternative algorithms have been proposed to mitigate this. However, the practical impact of exposure bias is under debate. In this paper, we link exposure bias to another well-known problem in NMT, namely the tendency to generate hallucinations under domain shift. In experiments on three datasets with multiple test domains, we show that exposure bias is partially to blame for hallucinations, and that training with Minimum Risk Training, which avoids exposure bias, can mitigate this. Our analysis explains why exposure bias is more problematic under domain shift, and also links exposure bias to the beam search problem, i.e. performance deterioration with increasing beam size. Our results provide a new justification for methods that reduce exposure bias: even if they do not increase performance on in-domain test sets, they can increase model robustness to domain shift.

Introduction Neural Machine Translation (NMT) has advanced the state of the art in MT  (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) , but is susceptible to domain shift.  Koehn and Knowles (2017)  consider out-of-domain translation one of the key challenges in NMT. Such translations may be fluent, but completely unrelated to the input (hallucinations), and their misleading nature makes them particularly problematic. We hypothesise that exposure bias  (Ranzato et al., 2016) , a discrepancy between training and inference, makes this problem worse. Specifically, training with teacher forcing only exposes the model to gold history, while previous predictions during inference may be erroneous. Thus, the model trained with teacher forcing may over-rely on previously predicted words, which would exacerbate error propagation. Previous work has sought to reduce exposure bias in training  Ranzato et al., 2016; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019) . However, the relevance of error propagation is under debate:  Wu et al. (2018)  argue that its role is overstated in literature, and that linguistic features explain some of the accuracy drop at higher time steps. Previous work has established a link between domain shift and hallucination in NMT  (Koehn and Knowles, 2017; M?ller et al., 2019) . In this paper, we will aim to also establish an empirical link between hallucination and exposure bias. Such a link will deepen our understanding of the hallucination problem, but also has practical relevance, e.g. to help predicting in which settings the use of sequence-level objectives is likely to be helpful. We further empirically confirm the link between exposure bias and the 'beam search problem', i.e. the fact that translation quality does not increase consistently with beam size  (Koehn and Knowles, 2017; Stahlberg and Byrne, 2019) . We base our experiments on German?English IWSLT'14, and two datasets used to investigate domain robustness by  M?ller et al. (2019) : a selection of corpora from OPUS  (Lison and Tiedemann, 2016)  for German?English, and a low-resource German?Romansh scenario. We experiment with Minimum Risk Training (MRT)  (Och, 2003; Shen et al., 2016) , a training objective which inherently avoids exposure bias. Our experiments show that MRT indeed improves quality more in out-of-domain settings, and reduces the amount of hallucination. Our analysis of translation uncertainty also shows how the MLE baseline over-estimates the probability of random translations at all but the initial time steps, and how MRT mitigates this problem. Finally, we show that the beam search problem is reduced by MRT. 

 Minimum Risk Training The de-facto standard training objective in NMT is to minimize the negative log-likelihood L(?) of the training data D 1 : L(?) = (x,y)?D |y| t=1 ? log P (y t |x, y <t ; ?) (1) where x and y are the source and target sequence, respectively, y t is the t th token in y, and y <t denotes all previous tokens. MLE is typically performed with teacher forcing, where y <t are groundtruth labels in training, which creates a mismatch to inference, where y <t are model predictions. Minimum Risk Training (MRT) is a sequencelevel objective that avoids this problem. Specifically, the objective function of MRT is the expected loss (risk) with respect to the posterior distribution: R(?) = (x,y)?D ?Y(x) P (?|x; ?) ? (?, y) (2) in which the loss ? (?, y) indicates the discrepancy between the gold translation y and the model prediction ?. Due to the intractable search space, the posterior distribution Y(x) is approximated by a subspace S(x) by sampling a certain number of candidate translations, and normalizing: P (?|x; ?, ?) = P (?|x; ?) ? y ?S(x) P (y |x; ?) ? (3) where ? is a hyperparameter to control the sharpness of the subspace. Based on preliminary results, we use random sampling to generate candidate translations, and following  Edunov et al. (2018) , do not add the reference translation to the subspace. 

 Experiments 

 Data To verify the effectiveness of our MRT implementation on top of a strong Transformer baseline  (Vaswani et al., 2017) , we first conduct experiments on IWSLT'14 German?English (DE?EN)  (Cettolo et al., 2014) , which consists of 180 000 sentence pairs. We follow previous work for data splits  (Ranzato et al., 2016; Edunov et al., 2018) . For experiments with domain shift, we use data sets and preprocessing as  M?ller et al. (2019)  2 . For DE?EN, data comes from OPUS  (Lison and Tiedemann, 2016) , and is comprised of five domains: medical, IT, law, koran and subtitles. We use medical for training and development, and report results on an in-domain test set and the four other domains (out-of-domain; OOD). German?Romansh (DE?RM) is a low-resource language pair where robustness to domain shift is of practical relevance. The training data is from the Allegra corpus  (Scherrer and Cartoni, 2012 ) (law domain) with 100 000 sentence pairs. The test domain are blogs, using data from Convivenza 3 . We have access to 2000 sentences for development and testing, respectively, in each domain. We tokenise and truecase data sets with Moses  (Koehn et al., 2007) , and use shared BPE with 32 000 units  (Sennrich et al., 2016) . 

 Model We implement 4 MRT in the Nematus toolkit  (Sennrich et al., 2017) . All our experiments use the Transformer architecture  (Vaswani et al., 2017) . Following  Edunov et al. (2018) , we use 1-BLEU smooth  (Lin and Och, 2004)  as the MRT loss. Models are pre-trained with the token-level objective MLE and then fine-tuned with MRT. Hyperparameters mostly follow previous work  (Edunov et al., 2018; M?ller et al., 2019) ; for MRT, we conduct limited hyperparameter search on the IWSLT'14 development set, including learning rate, batch size, and the sharpness parameter ?. We set the number of candidate translations for MRT to 4 to balance effectiveness and efficiency. Detailed hyperparameters are reported in the Appendix. 

 Evaluation For comparison to previous work, we report lowercased, tokenised BLEU  (Papineni et al., 2002)  with multi-bleu.perl for IWSLT'14, and cased, detokenised BLEU with SacreBLEU (Post, 2018) 5 otherwise. For settings with domain shift, we report average and standard deviation of 3 independent training runs to account for optimizer instability. The manual evaluation was performed by two native speakers of German who completed bilin-   (Edunov et al., 2018)  32.8 (+0.6) Transformer (MLE)  (Wu et al., 2019)  34.4 DynamicConv (MLE)  (Wu et al., 2019)    gual (German/English) high school or University programs. We collected ?3600 annotations in total, spread over 12 configurations. We ask annotators to evaluate translations according to fluency and adequacy. For fluency, the annotator classifies a translation as fluent, partially fluent or not fluent; for adequacy, as adequate, partially adequate or inadequate. We report kappa coefficient (K)  (Carletta, 1996)  for inter-annotator and intra-annotator agreement in Table  1 , and assess statistical significance with Fisher's exact test (two-tailed). 

 Results Table  2  shows results for IWSLT'14. We compare to results by  Edunov et al. (2018) , who use a convolutional architecture  (Gehring et al., 2017) , and  Wu et al. (2019) , who report results with Transfomerbase and dynamic convolution. With 34.7 BLEU, our baseline is competitive. We observe an improvement of 0.5 BLEU from MRT, comparable to  Edunov et al. (2018) , although we start from a stronger baseline (+2.5 BLEU). Table  3  shows results for data sets with domain shift. To explore the effect of label smoothing  (Szegedy et al., 2016) , we train baselines with and without label smoothing. MLE with label smoothing performs better by itself, and we also found MRT to be more effective on top of the initial model with label smoothing. For DE?EN, MRT increases average OOD BLEU by 0.8 compared to the MLE baseline with label smoothing; for DE?RM the improvement is 0.7 BLEU. We note that MRT does not consistently improve in-domain performance, which is a first indicator that exposure bias may be more problematic under domain shift. Our OOD results lag slightly behind those of  M?ller et al. (2019) , but note that the techniques employed by them, namely reconstruction  (Tu et al., 2017; Niu et al., 2019) , subword regularization  (Kudo, 2018) , and noisy channel modelling  (Li and Jurafsky, 2016)  are orthogonal to MRT. We leave the combination of these approaches to future work. 

 Analysis BLEU results indicate that MRT can improve domain robustness. In this section, we report on additional experiments to establish more direct links between exposure bias and domain robustness, hallucination, and the beam search problem. Experiments are performed on DE?EN OPUS data. 

 Hallucination We manually evaluate the proportion of hallucinated translations on out-of-domain and in-domain test sets. We follow the definition and evaluation by  M?ller et al. (2019) , considering a translation a hallucination if it is (partially) fluent, but unrelated in content to the source text (inadequate). We report the proportion of such hallucinations for each system. Results in Table  4  confirm that hallucinations are much more pronounced in out-of-domain test sets (33-35%) than in in-domain test sets (1-2%). MRT reduces the proportion of hallucinations on out-of-domain test sets (N=500 for each system; reductions statistically significant at p < 0.05) and improves BLEU. Note that the two metrics do not correlate perfectly: MLE with label smoothing has higher BLEU (+1) than MRT based on MLE without label smoothing, but a similar proportion of hallucinations. This indicates that label smoothing increases translation quality in other aspects, while MRT has a clear effect on the number of hallucinations, reducing it by up to 21% (relative). A closer inspection of segments where the MLE system was found to hallucinate shows that some segments were scored higher in adequacy with MRT, others lower in fluency. One example for each case is shown in Table  5 . Even the example where MRT was considered disfluent and inadequate actually shows an attempt to cover the source sentence: the source word 'Ableugner' (denier) is   (M?ller et al., 2019)  61.5 11.7 52.5 18.9 NMT+RC+SR+NC  (M?ller et al., 2019)     mistranslated into 'dleugner'. We consider this preferable to producing a complete hallucination. 

 Uncertainty Analysis Inspired by , we analyse the model's uncertainty by computing the average probability at each time step across a set of sentences. Besides the reference translations, we also consider a set of 'distractor' translations, which are random sentences from the in-domain test set which match the corresponding reference translation in length. In Figure  1 , we show out-of-domain results for an MLE model and multiple checkpoints of MRT fine-tuning. The left two graphs show probabilities for references and distractors, respectively. The right-most graph shows a direct comparison of probabilities for references and distractors for the MLE baseline and the final MRT model. The MLE baseline assigns similar probabilities to tokens in the references and the distractors. Only for the first time steps is there a clear preference for the references over the (mostly random!) distractors. This shows that error propagation is a big risk: should the model make a wrong prediction initially, this is unlikely to be penalised in later time steps. MRT tends to increase the model's certainty at later time steps 6 , but importantly, the increase is sharper for the reference translations than for the distractors. The direct comparison shows a widening gap in certainty between the reference and distractor sentences.  7  In other words, producing a hallucination will incur a small penalty at each time step (compared to producing the reference), presumably due to a higher reliance on the source signal, lessening the risk of error propagation and hallucinations. Our analysis shows similar trends on in-domain references. However, much higher probabilities are assigned to the first few tokens of the references than to the distractors. Hence, it is much less likely that a hallucination is kept in the beam, or will overtake a good translation in overall probability, reducing the practical impact of the model's overreliance on its history. 8 

 Beam Size Analysis Figure  1  shows that with MLE, distractor sentences are assigned lower probabilities than the references at the first few time steps, but are assigned similar, potentially even higher probabilities at later time steps. This establishes a connection between exposure bias and the beam search problem, i.e. the problem that increasing the search space can lead to worse model performance. 9 With larger beam size, it is more likely that hallucinations survive pruning at the first few time steps, and with high probabilities assigned to them at later time steps, there is a chance that they become the top-scoring translation. We investigate whether the beam search problem is mitigated by MRT. In Table  6 , we report OOD BLEU and the proportion of hallucinations with beam sizes of 1, 4 and 50. While MRT does not eliminate the beam search problem, performance drops less steeply as beam size increases. With beam size 4, our MRT models outperform the MLE baseline by 0.5-0.8 BLEU; with beam size 50, this difference grows to 0.6-1.5 BLEU. Our manual evaluation (N=200 for each system for beam size 1 and 50) shows that the proportion of hallucinations increases with beam size, and that MRT consistently reduces the proportion by 11-21% (relative). For the system with label smoothing, the relative increase in hallucinations with increasing beam size is also smaller with MRT (+33%) than with MLE (+44%).    9  The beam search problem has previously been linked to length bias  (Yang et al., 2018; Murray and Chiang, 2018)  and the copy mode . We consider hallucinations another result of using large search spaces with MLE models. 

 Conclusions Our results and analysis show a connection between the exposure bias due to MLE training with teacher forcing and several well-known problems in neural machine translation, namely poor performance under domain shift, hallucinated translations, and deteriorating performance with increasing beam size. We find that Minimum Risk Training, which does not suffer from exposure bias, can be useful even when it does not increase performance on an in-domain test set: it increases performance under domain shift, reduces the number of hallucinations substantially, and makes beam search with large beams more stable. Our findings are pertinent to the academic debate how big of a problem exposure bias is in practice -we find that this can vary substantially depending on the dataset -, and they provide a new justification for sequence-level training objectives that reduce or eliminate exposure bias. Furthermore, we believe that a better understanding of the links between exposure bias and well-known translation problems will help practitioners decide when sequence-level training objectives are especially promising, for example in settings where the test domain is unknown, or where hallucinations are a common problem.   7 : Configurations of NMT systems used to pre-train and fine-tune over three datasets. Note in general hyperparameters, the items in brackets denote the options that will be used in MRT fine-tuning.  

 A Appendix Figure 1 : 1 Figure 1: Per-token probability of out-of-domain reference translations and in-domain distractors (first two graphs share legend). Rightmost plot shows direct comparison for MLE baseline and final MRT model. DE?EN OPUS . 

 Figure 2 :Figure 3 : 23 Figure 2: Per-token probability of out-of-domain reference translations and in-domain distractors for different checkpoints in MRT training, showing a widening gap between references and distractors. DE?EN OPUS. 

 Table 2 : 2 Results for IWSLT'14 DE?EN with MLE and MRT (in brackets, improvement over MLE). 

 Table 3 : 3 Average BLEU and standard deviation on in-domain and out-of-domain test sets for models trained on OPUS (DE?EN) and Allegra (DE?RM). RC: reconstruction; SR: subword regularization, NC: noisy channel. % hallucinations (BLEU) system out-of-domain in-domain MLE w/o LS 35% (9.7) 2% (58.3) +MRT 29% (10.2) - MLE w/ LS 33% (11.2) 1% (58.9) +MRT 26% (12.0) - 

 Table 4 : 4 Proportion of hallucinations and BLEU on outof-domain and in-domain test sets. DE?EN OPUS. source Wir haben ihn gefunden. reference We found him. MLE Do not pass it. MRT We have found it. sourceSo h?re nicht auf die Ableugner. reference So hearken not to those who deny (the Truth).MLEDo not drive or use machines.MRTDo not apply to dleugner. 

 Table 5 : 5 Out-of-domain translation examples. MLE hallucinates in both examples; MRT was rated more adequate in top example, less fluent in bottom one. 

 Table 6 : 6 Average OOD BLEU and proportion of hallucinations with different beam sizes k. DE?EN OPUS. 

			 This is equivalent to maximizing the likelihood of the data, hence Maximum Likelihood Estimation (MLE).2 https://github.com/ZurichNLP/ domain-robustness 

			 The uncertainty of the baseline is due to label smoothing.7  For intermediate checkpoints, see Appendix, Figure2.8  Figures are shown in the Appendix (Figure3).
