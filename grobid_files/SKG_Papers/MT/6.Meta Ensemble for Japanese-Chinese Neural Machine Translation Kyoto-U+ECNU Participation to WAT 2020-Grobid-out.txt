title
Meta Ensemble for Japanese-Chinese Neural Machine Translation: Kyoto-U+ECNU Participation to WAT 2020

abstract
This paper describes the Japanese-Chinese Neural Machine Translation (NMT) system submitted by the joint team of Kyoto University and East China Normal University (Kyoto-U+ECNU) to WAT 2020 (Nakazawa et al., 2020 . We participate in APSEC Japanese-Chinese translation task. We revisit several techniques for NMT including various architectures, different data selection and augmentation methods, denoising pre-training, and also some specific tricks for Japanese-Chinese translation. We eventually perform a meta ensemble to combine all of the models into a single model. BLEU results of this meta ensembled model rank the first both on 2 directions of ASPEC Japanese-Chinese translation.

Introduction Neural Machine Translation (NMT)  (Sutskever et al., 2014; Bahdanau et al., 2015)  has led to large improvements in machine translation quality when large parallel corpora are available for training. In this work, we revisit several existing NMT based techniques on ASPEC Japanese-Chinese translation task. Furthermore, we conduct a meta ensemble to fuse various NMT based systems. The aspects this work feature can be summarized as: ? We revisit various NMT based systems including LSTM  (Sutskever et al., 2014) , Transformer  (Vaswani et al., 2017) , ConvS2S  (Gehring et al., 2017)  and Lightconv  with different data augmentation and filtering techniques. ? We implement a meta ensemble on various NMT systems and obtain the state-of-the-art results on ASPEC Japanese-Chinese translation task. ? We empirically compare fully (semi-) supervised training with the recent popular language model pre-training based methods  (Conneau and Lample, 2019; Song et al., 2019; . ? We revisit and explore the trick of character mapping  Chen et al., 2020)  between Chinese and Japanese on AS-PEC translation task. Although only our team participated in the ASPEC Japanese-Chinese translation task this year, BLEU results we report on the WAT official leader-board rank 1st both on ja?zh and zh?ja compared with all the previous submitted systems. 

 ASPEC Japanese-Chinese Translation Task Number ASPEC (Asian Scientific Paper Excerpt Corpus)  was constructed in the Japanese-Chinese machine translation project from 2006 to 2010 by Japan Science and Technology Agency. ASPEC Japanese-Chinese (ASPEC-JC, shown in Table  1 ) and ASPEC Japanese-English (ASPEC-JE) respectively consists approximately 0.68M and 3M parallel sentences for training. In this work, we focus on NMT system training for ASPEC-JC while parts of ASPEC-JE is leveraged for data augmentation. 

 Our System 

 Sequence-to-Sequence Framework Sequence-to-sequence framework (S2S) is the basic technique being used to learn a mapping from a source sentence to a target sentence in an end-toend manner. In this section, we revisit four different architectures for sequence-to-sequence learning including LSTM, ConvS2S, Transformer, and Lightconv. In our system, most experimental settings are based on Transformer while we also implement other 3 architectures to compare the performance on ASPEC task. LSTM  (Sutskever et al., 2014) . Long-Short-Term Memory (LSTM) is a special recurrent neural network (RNN), which solves the problem of gradient vanishing/exploding on the long sequence by integrating three gates (one input gate, one forget gate, and one output gate) and memory cells. Therefore, LSTM is capable of storing and forgetting information for longer periods of time on the sequence. ConvS2S  (Gehring et al., 2017) . Compared with RNNs that maintain a hidden state of the entire past, convolution operations can be fully parallelized during training. ConvS2S integrates the convolution operations into the sequence-to-sequence framework, which not only improves computation efficiency but also captures the long-range dependencies over the input sequence through multi-layer hierarchical structure. Transformer  (Vaswani et al., 2017) . Transformer eschew recurrence and convolutions entirely, relying on the attention mechanism to capture global dependencies between input and output sequence. Due to its high parallelism and the high quality in the translation task, it has become the most popular model among researchers. Lightconv . Lightconv is a lightweight convolution model that utilizes lightweight convolutions and dynamic convolutions instead of the self-attention mechanism. The attention weights of self-attention are computed by comparing the current time-step to all elements on the sequence, which brings a great challenge for longer sequences due to the quadratic complexity. Lightconv builds dynamic convolutions to predict a different kernel at each time-step rather than the entire sequence, which drastically reduces the number of parameters.  

 Data Augmentation by Filtering Out-of-domain Parallel Data NMT quality depends highly on the size of the training data. Thus, high quality augmented training dataset help ameliorate NMT. There exist just around 0.68M parallel sentences in ASPEC-JC, so we expect that extra parallel data can significantly improve the translation quality. We use Japanese-Chinese parallel datasets on other domains collected by IWSLT 2020  (Ansari et al., 2020) . 1 All the out-of-domain datasets used are shown in Table  2  including Ubuntu corpora from OPUS  (Tiedemann, 2012) , Global Voices, and News Commentary; OpenSubtitles  (Lison and Tiedemann, 2016) ; TED talks ; Wikipedia  (Chu et al., 2014 (Chu et al., , 2016 ; Wikitionary.org; 2 Tatoeba.org under CC-BY License; 34 and WikiMatrix . In total, over 1.9M out-of-domain parallel data are publicly available which can be leveraged to enhance the performance on the ASPEC task. However, we observe some sentence alignments are of low accuracy. Thus, we also conduct a filtering by using LASER  (Artetxe and Schwenk, 2019) . Specifically, we remove sentence pairs with the cossimilarity score less than a fixed threshold where similarity scores are calculated by LASER embeddings of two sentences. We observe that alignment quality tends to be high if the similarity score is over 0.6, so we use this value as the filtering thresh-old for most experiments. This results in 1.5M filtered out-of-domain parallel sentences which we leverage to train the NMT system jointly with ASPEC-JC dataset. We also revisit the domain adaption method  (Chu et al., 2017)  by adding tags of 2indomain and 2outof domain during the training phase. 

 Data Augmentation by Back Translation Besides using out-of-domain parallel data, we also implement back translation  (Sennrich et al., 2016a; Edunov et al., 2018) , another effective data augmentation technique for NMT. For monolingual corpora, we use the Japanese sentences in ASPEC-JE as in-domain Japanese monolingual data, where 3M Japanese sentences are used to augment the ASPEC-JC corpus. We do not perform the back translation by using Chinese monolingual data because no in-domain Chinese sentences are available. We neither do not consider using other outof-domain monolingual data. For the translation direction of ja?zh, we name it forward translation because of the absence of the target-side monolingual data, which means we use the pre-trained system of ja?zh to forward translate monolingual Japanese sentences into Chinese to augment the original dataset. 

 Character Mapping Character mapping is another essential trick frequently being used in Japanese-Chinese translation tasks  Chen et al., 2020)  because there are a large number of shared Chinese characters in Chinese and Japanese. Usually they not only share the character but also share the semantic function within a sentence, so pre-mapping Chinese characters to the target-side can help amplify the cross-lingual supervision. More precisely, for Kana characters in Japanese, we remain them with the same tokenization granularity whereas for Chinese characters, we first tokenize them into by characters, then use the character mapping table  (Chu et al., 2012)  to pre-map them to target-side. 

 mBART: Multilingual Denoising Pre-training After the apperance of BERT  (Devlin et al., 2019) , several pre-training methods are proposed for enhancing NMT  (Conneau and Lample, 2019; Song et al., 2019; Ren et al., 2019; . Recently, mBART  is a multilingual sequence-to-sequence language model pre-trained by denoising tasks on 25 languages including Japanese and Chinese. Specifically, we fine-tune mBART25 5 by Japanese-Chinese parallel sentences and compare this kind of multilingual pre-training with other fully (semi-) supervised baselines. 

 Meta Ensemble Systems   (Kasai et al., 2020) . Finally, we conduct an self-ensemble to combine all the above introduced systems. We take the checkpoint average for each system, thus the final ensemble is an ensemble of self-ensemble. Consequently, we call the final ensemble phase by meta ensemble. 

 Preprocessing and Training Details We conduct tokenization by using Juman 6  (Kurohashi et al., 1994; Morita et al., 2015)  for Japanese and stanford parser pku 7 for Chinese. Sentences over 175 tokens are removed for training. We build a joint vocabulary with 30k merge operations by Byte-Pair Encoding  (Sennrich et al., 2016b) . This results in a joint vocabulary with approximately 40k tokens. Note that we build a single vocabulary for all the settings except fine-tuning mBART25. Table  3 : BLEU results on test sets of ASPEC ja-zh and zh-ja translation). Bold denotes top-5 BLEU scores of each column. "Best" and "Avg.10" respectively means the best checkpoint and the average of last 10 checkpoints. "out-of-domain" means training with additional out-of-domain parallel data. "BT / FT (i)" denotes i-th turns of the back translation or forward translation. "big" means transformer-big setting and "big(12/1)" means transformerbig with 12 encoder layers and 1 decoder layer. "ls", "dp" and "ffhd" represents label smoothing, dropout and feed-forward hidden dimension, respectively. Model settings without declarations of "ls", "dp" and "ffhd" are set to "ls(0.1)", "dp(0.3)" and "ffhd(4,096)" for transformer-big and default for other S2S frameworks. "Threshold" means the filtering threshold value for LASER embedding. for fine-tuning mBART25. For parts of the outof-domain Chinese sentences that are in traditional Chinese, we transfer Traditional Chinese characters to Simplified Chinese ones.  8  We conduct all the experiments by using Fairseq 9  (Ott et al., 2019) , an open source sequenceto-sequence framework implementation. Most systems are set to the Transformer-big setting except those built up by other architectures or different model capacities. In particular, our model has a 6layer encoder and decoder, a hidden size of 1,024, a feed-forward hidden layer size of 4,096, batch-size of 2,048, dropout rate of 0.3 and 16 attention heads. For LSTM, we also use a 6-layer encoder and decoder architecture. For ConvS2S and Lightconv, we use the default settings in Fairseq. All the systems are early stopped if BLEU does not improve for continuous 50,000 steps. Experiments are run on 8 TITAN X (Pascal) GPUs except that single V100 GPU is used for mBART25 fine-tuning. We use BLEU  (Papineni et al., 2002)  for automatic evaluation. 

 Results 

 Single Model Results Results of 16 single models with different settings are shown in Table  3 . First, we report the results of vanilla transformerbig model (#1). By averaging last 10 checkpoints, BLEU results of 35.82 and 49.03 on ja-zh and zh-ja are obtained, which is slightly higher than results on the best checkpoints. Second, we train NMT systems with out-ofdomain parallel corpora (#2 & #3). We observe almost no BLEU improvements from additional training data, which can be attributed to extremely high alignment quality of the ASPEC-JC training data. BLEUs of averaged models increase by adding the domain tag (#3) that has once been demonstrated effective in LSTM architecture on the same task by  Chu et al. (2017) . Third, by conducting back translation and forward translation (#4 ? #7) with Japanese indomain monolingual data from ASPEC-JE, we obtain significant improved BLEUs. However, iterative back (or forward) translation have marginal contributions. Furthermore, we also mix out-ofdomain parallel data with back (or forward) translated synthetic parallel data to train the NMT system with the threshold of 0.9. Although no BLEU improvements observed, different generations will contribute to model ensembling. Fourth, we report the results of systems trained by different architectures. For the implementation of LSTM, ConvS2S, and Lightconv, we utilize the same training set as that of back (or forward) translation with 0.6 filtering threshold. As shown by #8, 9, 10 in Table  3 , Lightconv yields better results compared with transformer architecture (#5). LSTM and ConvS2S underperform the other 2 architectures but are capable to yield generations with discrepancy (see 5.2). Fifth, we report results of deep encoder shallow decoder system (#11), larger transformer (#12) and deeper transformer (#13). These 3 systems yield comparable results as the standard transformer-big system (#5). Whether they give contributions to model ensembling will be demonstrated in the next section. Sixth, we revisit the trick of character mapping on Japanese-Chinese translation task. The results of #14 show that only averaged result on jazh marginally outperforms the vanilla transformer (#1). The failure of this trick on APPEC task can be ascribed to the high quality of ASPEC-JC, which indicates that this trick harms the Chinese character embedding learning on a well-constructed parallel corpus. Last, we explore recent popular pre-training methods on this task. We observe improvements by fine-tuning mBART25 on zh-ja whereas BLEU decreases on ja-zh (#15). Moreover, by fine-tuning mBART25 with the back (or forward) translated training set, no significant BLEU improvements are observed, which indicates the conflict nature between denoising pre-training and back translation in the semi-supervised scenario. 

 Ensembled Model Results In this section, we report ensembled model results which are shown in Table  4 . We have trained 16 NMT systems and 12 of them (#1, #2, #4 ? #13) can be directly ensembled because they share an identical vocabulary and the same format of the source sentence (without adding tags or premapping). However, it is difficult to discover the best ensemble combinations. Thus, we conduct model ensembling in a greedy manner by the model order (start from #1). In Table  4 , we only list out positive ensemble combinations. We observe that BLEU improves significantly for the first 2 ensemble (#17 ? #19) while BLEU improvements slow down after #19. We also observe that most ensemble combinations give BLEU improvements (9 models give positive contribution on zh-ja while 8 on zh-ja). This demonstrates that ensembled systems can yield better results by changing the training set, utilizing different architectures, and modifying model capacities even though their single models do not provide performance improvements. As shown in Table  5 , we observe generated results are of extremely high quality. Last, systems we submitted for official evaluation are shown in Table 6. It is worth mentioning that the result of #25 on zh-ja and the result of #26 on ja-zh rank first on ASPEC-JC leaderboard. Ensembled results without using other resources (#28 & #29) respectively rank second on zh-ja and third on ja-zh. 

 Conclusion In this work, we participated in ASPEC-JC translation task. We revisited several strong NMT base-lines, tricks for handling training set, and also pre-training + fine-tuning methods for Japanese-Chinese NMT. Furthermore, we conducted a deliberate search of better ensembled models and obtained state-of-the-art translation results on this task. Because of the high BLEU results on this task, in the future, adapting systems trained on ASPEC to other domains should be explored and unsupervised machine translation on this task can be focused on. Table 1 : 1 ASPEC-JC overview. We merge "dev" and "devtest" as development set in our experiments. of parallel sentences train 672,315 dev 2,090 devtest 2,148 test 2,107 
