title
Jointly Masked Sequence-to-Sequence Model for Non-Autoregressive Neural Machine Translation

abstract
The masked language model has received remarkable attention due to its effectiveness on various natural language processing tasks. However, few works have adopted this technique in the sequence-to-sequence models. In this work, we introduce a jointly masked sequence-to-sequence model and explore its application on non-autoregressive neural machine translation (NAT). Specifically, we first empirically study the functionalities of the encoder and the decoder in NAT models, and find that the encoder takes a more important role than the decoder regarding the translation quality. Therefore, we propose to train the encoder more rigorously by masking the encoder input while training. As for the decoder, we propose to train it based on the consecutive masking of the decoder input with an ngram loss function to alleviate the problem of translating duplicate words. The two types of masks are applied to the model jointly at the training stage. We conduct experiments on five benchmark machine translation tasks, and our model can achieve 27.69/32.24 BLEU scores on WMT14 English-German/German-English tasks with 5+ times speed up compared with an autoregressive model.

Introduction The encoder-decoder based sequence-to-sequence framework  (Sutskever et al., 2014; Bahdanau et al., 2014)  has achieved great success on the task of Neural Machine Translation (NMT)  (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Sheng et al., 2020) . In this framework, the encoder takes the source sentence as input and extracts its hidden representation, based on which the decoder generates the target sentence word by word and from left to right, i.e., * Corresponding author. in an autoregressive manner, which is a natural bottleneck for the inference speed due to the sequential conditional dependence. As the performance of NMT models have been substantially promoted, the translation efficiency is becoming a new research hotspot. Nonautoregressive neural machine translation (NAT) models are proposed to reduce the translation latency while inference, by removing the conditional dependence between target tokens and predicting all tokens in parallel  (Gu et al., 2017) . As the context dependency cannot be utilized while decoding, the inference speedup of NAT models comes at the cost of the degradation in performance. As studied by previous works  (Guo et al., 2019; , the inferior accuracy of NAT models mainly occurs from two aspects: 1) the source-side information is not adequately encoded which results in incomplete translation; 2) the decoder cannot handle the task well which leads to repeated translations and poor performance on long sentences. To tackle these problems and promote the performance of NAT models, in this paper, we empirically conduct a thorough study on the functionalities of the encoder and decoder in NAT models, and conclude that the encoder has a more direct influence on the final translation performance, and is harder to train than the decoder. Therefore, we propose a jointly masked sequence-to-sequence model which is inspired by the idea of masked language modeling  (Devlin et al., 2018) . Specifically, for the encoder, we follow the masking strategy of BERT  (Devlin et al., 2018)  and randomly mask a number of tokens of the source sentence. This strategy trains the encoder more rigorously by forcing it to encode the complete information with residual input. For the decoder, we mask the consecutive fragment of the target sentence to make the decoder concentrate more on predicting adjacent tokens, and propose an n-gram based loss function to learn the consecutive tokens as a whole objective. In this way, we can alleviate the problem of repeated translations of NAT models. During inference, we adopt a mask-and-predict  (Ghazvininejad et al., 2019)  strategy to iteratively generate the translation result, which masks and predicts a subset of the current translation candidates in each iteration. We verify the effectiveness of our model on five benchmark translation tasks including WMT14 English ? German, WMT16 English ? Romanian and IWSLT14 German ? English. Our model outperforms all the NAT models in comparison, and can achieve comparative performance with its autoregressive counterpart while enhanced with 5+ times speedup on inference (27.69/32.24 BLEU scores and 5.73 times speedup on the WMT14 En-De/De-En tasks with an autoregressive teacher of 28.04/32.69 BLEU scores). Our main contributions can be summarized as follows: ? While previous works only concentrate on manipulating the decoder, we illustrate and emphasize the importance of the encoder in NAT models and propose the encoder masking strategy to improve its training. ? We propose the consecutive masking strategy of the decoder input and the n-gram loss function to alleviate the problem of repetitive translations of NAT models. ? We integrate the two parts above in the jointly masked sequence-to-sequence model which shows strong performance on benchmark machine translation datasets. 2 Related Work 

 Non-Autoregressive Machine Translation Neural machine translation (NMT) models have achieved great success in recent years. Traditional NMT models are based on the sequenceto-sequence framework  (Bahdanau et al., 2014; Sutskever et al., 2014) , taking the source sentence as input and generating the target sentence in an autoregressive manner. Specifically, given the source sentence x = (x 1 , x 2 , ..., x Tx ), the target sentence y = (y 1 , y 2 , ..., y Ty ) is generated as: P (y|x) = Ty t=1 P (y t |y <t , x; ? enc , ? dec ), (1) where y <t indicates the generated target tokens before timestep t, and ? enc and ? dec denote the pa-rameters of the encoder and decoder respectively. For a target sentence with length n, autoregressive models have to take O(n) iterations to generate it during inference. To break the sequential conditional dependency and make the generation process parallelizable, non-autoregressive machine translation (NAT) models are proposed to generate all target tokens independently  (Gu et al., 2017)  and reduce the time complexity from O(n) to O(k) where k is a constant number: P (y|x) = P (T y |x) ? Ty t=1 P (y t |x; ? enc , ? dec ), (2) where P (T y |x) is the explicit length prediction process for NAT models. Although the inference speed of NAT is significantly boosted, the translation accuracy is sacrificed due to the lack of context information at the target side. Therefore, lots of works have been conducted to promote the performance of NAT models. Specifically,  Gu et al. (2017)  takes a copy of the encoder input x as the decoder input and trains a fertility predictor to guide the copy procedure.  and  Ghazvininejad et al. (2019)  generate the target sentence by iteratively refining the current translation. Other works enhance the performance of NAT models by utilizing auxiliary information, such as extra loss functions  Li et al., 2019; Shao et al., 2019) , SMT components  (Guo et al., 2019)  and fine-tuning from an AT model . Recently, some works  (Stern et al., 2019; Welleck et al., 2019; Gu et al., 2019)  propose to change the generation order from the traditional left-to-right manner to a tree-based manner, resulting in a time complexity of O(log n). In this paper, we focus on the NAT model with O(k) generation complexity. 

 Masked Language Model The masked language model proposed by BERT  (Devlin et al., 2018)  has become the essential component of the state-of-the-art pre-training methods  (Song et al., 2019; Dong et al., 2019; Lample and Conneau, 2019)  in natural language understanding tasks. The standard paradigm of masked language modeling is to substitute a subset of tokens in the input sentence by a special symbol [MASK], and predict the missing tokens by the residual ones. We denote the residual tokens as x r and the masked target tokens as x m . As BERT is designed for language understanding tasks which can be handled with a single Transformer encoder, it is non-trivial to extend the paradigm into NMT tasks, where a sequence-tosequence framework is utilized. To address that, XLM  (Lample and Conneau, 2019)  concatenates the source sentence and the target sentence as the encoder input to let the model learn the crosslingual information, but still using a single Transformer encoder. MASS  (Song et al., 2019)  presents a sequence-to-sequence pre-training framework, which takes x r as the encoder input and takes x m as the decoder input as well as the target, still yielding a monolingual pre-training framework. In this paper, we propose a jointly masked language modeling method to handle the cross-lingual challenge in a unified sequence-to-sequence framework, based on which the translation accuracy of AT models and the inference speedup of NAT models can both be preserved. 

 Preliminary Study To explore the functionalities of the encoder and decoder in NAT models, we conduct a thorough empirical study. We mainly follow the settings in . We train a basic NAT model proposed by  Gu et al. (2017) , except that we remove the fertility predictor and keep the decoder input as a hard copy of the source sentence in a similar way with  (Guo et al., 2019; . We conduct the following experiments on the IWSLT14 German to English dataset and train the model with the same number of training steps for each setting. We study the importance of the encoder and decoder from three aspects. Firstly, we vary the number of encoder and decoder layers respectively to see which will bring more performance gain. Specifically, on a basic model with a 5-layer encoder and a 5-layer decoder, we increase the number of layers to the encoder and decoder separately. Results are illustrated in Table  1 , from which we can conclude that adding the layers of the encoder can bring more performance gain than the decoder. Secondly, we compare the convergence speed of the encoder and decoder by initializing the NAT model with a pretrained decoder/encoder and fix it during training, while randomly initialize a trainable encoder/decoder. The convergence speed is illustrated by the BLEU score along with the training steps, as shown in Figure  1(a) . From the results, we can observe that the decoder converges faster than the encoder. In conclusion, we find that the encoder is dealing with a more sophisticated task than the decoder, and the encoder is not adequately trained in the initial NAT model. Thirdly, we further conduct an investigation on the encoder input, encoder output and decoder input to evaluate their importance in the inference stage. During inference, we add random noise to the three types of inputs respectively, by randomly replacing the embeddings of some tokens with random noise. This experiment is conducted on a basic 5-layer encoder and decoder NAT model, and the results are illustrated in Figure  1(b) . Obviously, the encoder input and encoder output both largely influence the translation quality, which implies that the encoder plays an important role in the inference of NAT models, while the decoder input is the least important due to its conditional independence in nature. In a word, the performance of NAT models rely more on the encoder rather than the decoder. 

 Methodology While most existing NAT works only focus on refining the decoder to obtain better performance, we have explored and shown the significance of the encoder in the previous section. Therefore, we propose to improve the translation performance by further manipulating the encoder, and we will introduce the proposed framework to tackle the problems discussed above in this section. We start with the problem definition. Problem Definition Given a pair of source and target sentence (x, y) ? (X , Y) from the parallel training dataset X and Y, the negative loglikelihood objective function of an NMT model can be written as: L nll (x, y; ? enc , ? dec ) = ? log P (y|x; ? enc , ? dec ), (3) where the conditional probability can be either Equation (1) or Equation (2) for AT or NAT models, and ? enc , ? dec represent the parameters of the encoder and decoder respectively. 

 Encoder Masking As studied in Section 3, the encoder needs to handle a harder task than the decoder but is not adequately trained in previous works. To maximize the functionality of the encoder, we propose to train it with masked language modeling. The general masking strategy is as follows. Given a source sentence x = (x 1 , x 2 , ..., x Tx ), we randomly sample a subset from x, denoted as x m with T m x tokens, and substitute them with other tokens in position. Specifically, we follow the similar substitution strategy as BERT  (Devlin et al., 2018) : we randomly select 10% of the tokens in x, of which 80% are substituted with a special symbol [MASK], 10% are substituted with a random token in the vocabulary, and 10% are kept unchanged. And we denote the substituted result of the source sentence as x r . Then the loss function on the encoder of predicting the missing source tokens can be written as: L enc (x m |x r ) = ? T m x t=1 log P (x m t |x r ). (4) 

 Decoder Masking For the decoder, as it is shown that the repetitive translations mainly result from the nonautoregressive nature of NAT, we alleviate this problem by applying a consecutive masking strategy and proposing a tailored n-gram based loss function. During training, given a target sentence y = (y 1 , y 2 , ..., y Ty ), we randomly select multiple sets of consecutive tokens and mask them in a similar strategy as masking the encoder. Each set contains n consecutive tokens, and we denote the masked target set as y m and the substituted result as y r , and their corresponding lengths as T m y and T y . Note that in the decoder, the total number of masking tokens is uniformly sampled from 1 to T y instead of being computed with a fixed ratio. We provide an illustration of our framework in We propose an n-gram based loss function, which has been applied to NMT models recently  (Ma et al., 2018; Shao et al., 2018 Shao et al., , 2019 , to enhance the sentence-level information and alleviate the problem of repetitive translations of NAT models. The loss function is tied with the consecutive masking where n equals to the number of the consecutive masked tokens in each set. Specifically, given an n-gram g = (g 1 , ..., g n ), its occurrence count in the target sentence y can be written as: C y (g) = Ty?n t=0 n i=1 1{g i = y t+i }. As for the count in the masked sequence y m , we introduce the probabilistic variant of the n-gram count to make the objective differentiable  (Shao et al., 2018)  by representing each token with the prediction probability: Cy m (g) = T m y ?n t=0 n i=1 1{g i = y m t+i } ? p(y m t+i |x). (6) Considering all possible n-grams in y, the proposed n-gram based loss function can be written as: L gram (y, y m |y r ,x r ) = (7) K ? g min(C y (g), Cy m (g)), where min(C y (g), Cy m (g)) represents the matching count between y and y m w.r.t the n-gram g, and K is the upper bound of the total matching count which equals to the number of sets of consecutive masked tokens. The n-gram loss function will encourage the model to treat the consecutive masked tokens as a whole objective to match the sequential fragments in the target sentence, thus reducing the occurrence of repetitive translations. 

 Jointly Masked Model Based on the proposed framework, the objective function of our model contains three parts: the traditional negative log-likelihood loss function to predict the missing target tokens L nll (?), the prediction loss function on the encoder side L enc (?), and the n-gram loss function L gram (?). By integrating the three loss functions, given a training pair (x, y), the complete objective function of our model is: min ? L(x, y) = L nll (y m |x r , y r ; ? enc , ? dec ) + ? 1 L enc (x m |x r ; ? enc ) (8) + ? 2 L gram (y, y m |y r , x r ; ? enc , ? dec ), where ? = (? enc , ? dec ), ? 1 and ? 2 are the hyperparameters that control the weights of different loss functions. In the proposed training framework, the importance of the encoder has been emphasized by masking the encoder input and introducing L enc (?). The encoder is encouraged to produce better representations of other tokens in order to predict the missing tokens. On the decoder side, the consecutive masking strategy augmented with the n-gram based loss function can help the model better capture the sentence-level information and alleviate the problem of repetitive translations. 

 Decoding Algorithm For inference, we propose to iteratively refine the translation result in a mask-and-predict manner mainly following the strategy proposed in  (Ghazvininejad et al., 2019) , and details are introduced below. During inference, the first step for NAT models is to determine the length of the target translation. We follow  (Ghazvininejad et al., 2019)  and introduce an additional prediction process to estimate the length by the source sentence. Specifically, we add a special token to the encoder and predict the target length with the output hidden vector of this token. The negative log-likelihood loss function of this token is then added to the word prediction loss in Equation (8) as the final loss. In experiments, we also consider selecting the translation with highest probability over multiple translation candidates with different target lengths to obtain better results. Thereafter, based on the mask-and-predict paradigm, we design our decoding algorithm as follows. Given the target length T y , we initiate the target sentence with [MASK] at all positions, and take it as the decoder input followed by conducting translation. Next, for each iteration, we apply consecutive masking to the translation candidates as we have done in the training stage. Specifically, we select several tokens with the lowest probabilities from the current translation candidates, and mask these tokens as well as their adjacent ones. The number of tokens to mask at each iteration follows a linear decay function utilized in  (Ghazvininejad et al., 2019) . As for the stop condition, the final translation is taken either when a pre-defined number of iterations is reached, or the translation candidates do not change between two iterations. 

 Experiments 

 Experimental Setup 

 Datasets We evaluate our method on five widely used benchmark tasks: IWSLT14 German?English translation (IWSLT14 De-En) 1 , WMT16 English?Romanian translation (WMT16 En-Ro/Ro-En) 2 , and WMT14 English?German translation (WMT14 En-De/De-En) 3 . We strictly follow the dataset configurations of previous works. For the IWSLT14 De-En task, we train the model on its training set with 157k training samples, and evaluate on its test set. For the WMT14 En-De/De-En task, we train the model on the training set with 4.5M training samples, where newstest2013 and newstest2014 are used as the validation and test set respectively. As for the WMT16 En-Ro task which has 610k training pairs, we utilize newsdev2016 and newstest2016 as the validation and test set. For each dataset, we tokenize the sentences by Moses  (Koehn et al., 2007)  and segment each word into subwords using Byte-Pair Encoding (BPE)  (Sennrich et al., 2015) , resulting in a 32k vocabulary shared by source and target languages. 

 Model Settings We strictly follow the previous works to set the configurations of models. Our model is based on the Transformer  (Vaswani et al., 2017)  architecture, with multi-head positional attention proposed in  (Gu et al., 2017) . We utilize the small Transformer (d model = d hidden = 256, n head = 4) with 5-layer encoder and decoder for the IWSLT14 De-En task, and the base Transformer (d model = d hidden = 512, n layer = 6, n head = 8) for the WMT14 and WMT16 tasks. We set n = 2 for all tasks, i.e., we consider two-gram matchings when calculating L gram . The hyper-parameters ? 1 and ? 2 are both set to 0.01 for all tasks. 

 Baselines We consider seven recent works as our baselines, including five NAT works: NAT with fertility (NAT-FT)  (Gu et al., 2017) , NAT with Imitation Learning (Imitate-NAT) , NAT with Regularizations (NAT-Reg) , NAT with Curriculum Learning (FCL-NAT) , NAT with Dynamic Conditional Random Field (NAT-DCRF) ; and two iterative decoding based works: NAT with Iterative Refinement (NAT-IR)  and Conditional Masked NAT (CM-NAT)  (Ghazvininejad et al., 2019) . The first five models are purely non-autoregressive, whose time complexities during inference are all O(1). The other two models are based on iteratively refining the translation results by k iterations, where k is a constant number, yielding O(k) complexity. In the experiments, we also compare with them in terms of the inference latency on clock. 

 Sequence-Level Knowledge Distillation We adopt sequence-level knowledge distillation  (Kim and Rush, 2016)  on the training set of each task, which has been proved by previous NAT models that it can produce less noisy and more deterministic training data  (Gu et al., 2017) . As stated by , the performance of the AT teacher will affect the final performance of the NAT student model. While AT teachers used in previous works have various performance, we utilize the teacher model which has similar performance with the one used in our main baseline CM-NAT  (Ghazvininejad et al., 2019)  to construct a fair comparison. In addition, we also provide the performance of our model trained by a weakened AT teacher (denoted as WT in Table  2 ) which has similar performance with the one used in  to compare with them. 

 Training and Inference We train the model with 8/1 Nvidia 1080Ti GPUs on the WMT datasets and IWSLT14 dataset respectively, and we utilize the Adam optimizer while following the same settings used in the original Transformer. During inference, we generate multiple translation candidates by taking the top B length predictions into consideration, and select the translation with the highest probability as the final result. We set B = 3 on WMT tasks and B = 4 on IWSLT14 tasks. We also report the clock time of inference latency on a single Nvidia 1080Ti GPU in our experiments, where we set the batch size to 1 and calculate the average per sentence translation time on newstest2014 for the WMT14 En-De task to keep consistence with previous works. As for evaluation, we use BLEU scores  (Papineni et al., 2002)  as the evaluation metric, and The BLEU scores of our proposed JM-NAT and the baseline methods on the WMT14 En-De/De-En, WMT16 En-Ro/Ro-En and IWSLT14 De-En tasks. We report the best results for the baseline methods and also list the inference latency on clock as well as the speedup w.r.t autoregressive models. " ?" indicates that the result is provided by , " * " indicates the results obtained by our implementation, "/" indicates the corresponding result is not reported in the original paper, and "-" indicates the same numbers as above. "Weak Teacher (WT)" indicates the NAT is trained with a weakened AT teacher through knowledge distillation. NPD stands for Noisy Parallel Decoding utilized in previous works. "k" represents the number of iterations while inference. report the tokenized case-sensitive scores for the WMT datasets, as well as the tokenized caseinsensitive scores for the IWSLT14 dataset. Our implementation is based on fairseq  and is avaliable at https://github.com/ lemmonation/jm-nat. 

 Results The main results are listed in Table  2 . We denote our model as Jointly Masked NAT (JM-NAT), and show the results when the upper bound of iterations k is set to 4 and 10. As can be observed from Table  2 , our model achieves comparable performance with its AT teacher on all datasets (only 0.5 BLEU score behind in average), while achieving 5+ times speedup on the inference latency. Compared with the pure NAT models with O(1) time complexity, with similar inference latency by setting k = 4, our model outperforms all baselines with a consistent margin on different tasks. Compared with the models based on iterative refinement, JM-NAT also shows consistent superiority with the same time complexity. Our model outperforms CM-NAT  (Ghazvininejad et al., 2019)  with margins from 0.41 to 1.71 on different tasks, illustrating the boosted performance brought by the jointly masked model as well as the proposed loss functions. It is worth noting that CM-NAT utilizes a much stronger AT teacher on the WMT14 En-De task (using the large configuration of Transformer and achieving 28.65 BLEU score). Our model, even with less iterations or a weaker AT teacher, still outperforms CM-NAT in most cases, and it is straightforward to further improve our performance with a stronger teacher. 

 Analysis 

 Encoder Performance As there does not exist a clear metric (such as the perplexity in language generation tasks) to evaluate the quality of the encoder in a sequence-tosequence model, we adopt a naive version of the adversarial attack on text  (Belinkov and Bisk, 2017)  to the encoder input to test the robustness of the encoder. Specifically, during inference, we follow the same strategy used in Section 3 to add noise to the source sentence x. Given the noise ratio ? ? (0, 1), we randomly select ? ? T x (where ? stands for the rounding function) source tokens and either drop or replace them with other tokens in the vocabulary. We increase ? from 0 to 10% and test the performance of each model on the validation set of the IWSLT14 De-En task, and show the results in Figure  3 . We compare our model with baselines  including NAT-FT and CM-NAT. According to the results, compared with CM-NAT, which is also an iterative decoding based method, our model shows more robust performance with regard to the noise on the encoder input, showing the efficacy of the proposed masking strategy and the better quality of our encoder. 

 Repetitive Words As studied by , the tendency of producing repetitive words in translation is a major drawback of NAT models. We propose to alleviate this problem by training the decoder with the consecutive masking strategy as well as the n-gram loss function. We compute the average number of consecutive repetitive tokens per sentence in the translation results on the validation set of the IWSLT14 De-En task. Results are shown in Table  3 . Without introducing explicit regularizations , our method is still able to alleviate the problem of repetitive words. Compared with CM-NAT who also utilizes an iterative decoding method, the superiority of our method demonstrates the proposed consecutive masking strategy better solves the problem than random masking. 

 Ablation Study We conduct the ablation study on the validation set of the IWSLT14 De-En task to illustrate the contribution of different components in our model. Results are shown in Table  4 . For the encoder, both encoder masking and the objective function L enc contribute to the final performance, and encoder masking provides the most prominent performance promotion. On the decoder side, both of the consecutive masking strategy and the n-gram loss function are indispensable to produce solid performance as they are tied together through the hyper-parameter n. In addition, all the proposed components are effective in alleviating the repetitive translations, and the n-gram loss function contributes the most.     3 . 

 Case Study We further conduct case studies to intuitively demonstrate the performance of different models and the generation process of our model. Results are listed in Table  5 . As we discussed in Section 1, repetitive translations and missing translations are two stubborn problems of NAT models. In Table  5 , both NAT-FT and CM-NAT tend to generate repetitive words (such as "eliminate diabetes diabetes" and "reduce cancer risk risk") as well as incomplete translations (both of them miss the word "eliminate" in the second clause), while our model achieves better results. 

 Conclusion In this paper, we propose a jointly masked sequence-to-sequence model for nonautoregressive neural machine translation. We first empirically investigate the functionalities of the non-autoregressive translation model, and Table  5 : A case study on the translation results of different models on the IWSLT14 De-En task. We set k = 10 for our model. The bold italics represent the repetitive words in the translation results. improve the training of the encoder by masking its input and introducing a prediction based loss function. For the decoder, we propose to utilize consecutive masking and introduce an n-gram based loss function to alleviate the problem of repetitive translations. Our model outperforms all compared NAT baselines and achieves comparable performance with autoregressive models on five benchmark tasks with 5+ times speed up on the inference latency. In the future, we will extend the investigation on the functionalities of the encoder and decoder to other sequence-to-sequence tasks such as text summarization and text style transfer to explore more applications of our model. Figure 1 : 1 Figure 1: (a) The convergence speed of the encoder and the decoder. (b) The performance when adding noise to the encoder input, encoder output and decoder input in the inference stage of a basic NAT model. 
