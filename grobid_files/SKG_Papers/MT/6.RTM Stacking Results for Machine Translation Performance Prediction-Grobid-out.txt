title
RTM Stacking Results for Machine Translation Performance Prediction Ergun Bic Â¸ici

abstract
We obtain new results using referential translation machines with increased number of learning models in the set of results that are stacked to obtain a better mixture of experts prediction. We combine features extracted from the word-level predictions with the sentence-or document-level features, which significantly improve the results on the training sets but decrease the test set results.

Referential Translation Machines for Machine Translation Performance Predicion Quality estimation task in WMT19 (Specia et al., 2019) (QET19) address machine translation performance prediction (MTPP), where translation quality is predicted without using reference translations, at the sentence-and word-(Task 1), and document-levels (Task 2). The tasks contain subtasks involving English-German, English-Russian, and English-French machine translation (MT). The target to predict in Task 1 is HTER (human-targeted translation edit rate) scores  (Snover et al., 2006)  and binary classification of word-level translation errors and the target in Task 2 is multi-dimensional quality metrics (MQM)  (Lommel, 2015) . Table  1  lists the number of sentences in the training and test sets for each task and the number of instances used as interpretants in the RTM models (M for million). We use referential translation machine (RTM)  (Bic ?ici, 2018; Bic ?ici and Way, 2015)  models for building our prediction models. RTMs predict data translation between the instances in the training set and the test set using interpretants, data close to the task instances. Interpretants provide context for the prediction task and are used during the derivation of the features measuring the closeness of the test sentences to the training data, the difficulty of translating them, and to identify translation acts between any two data sets for building prediction models. With the enlarging parallel and monolingual corpora made available by WMT, the capability of the interpretant datasets selected by RTM models to provide context for the training and test sets improve as can be seen in the data statistics of parfda instance selection  (Bic ?ici, 2019) . Figure  1  depicts RTMs and explains the model building process. RTMs use parfda for instance selection and machine translation performance prediction system (MTPPS) for obtaining the features, which includes additional features from word alignment and also from GLMd for word-level prediction. We use ridge regression, kernel ridge regression, k-nearest neighors, support vector regression, AdaBoost (Freund and Schapire, 1997), gradient tree boosting, gaussian process regressor, extremely randomized trees  (Geurts et al., 2006) , and multi-layer perceptron  (Bishop, 2006)  as learning models in combination with feature selection (FS)  (Guyon et al., 2002)  and partial least squares (PLS)  (Wold et al., 1984)  where most of these models can be found in scikit-learn.  1  We experiment with: ? including the statistics of the binary tags obtained as features extracted from word-level tag predictions for sentence-level prediction, ? using KNN to estimate the noise level for SVR, which obtains accuracy with 5% error compared with estimates obtained with known noise level (Cherkassky and Ma,  2004 ) and set = ?/2.  2017 ) used a hybrid stacking model to combine the word-level predictions from 15 predictors using neural networks with different initializations together with the previous features from a linear model. The neural network architecture they used is also hybrid with different types of layers: input word embedding use 64 dimensional vectors, the three layers are two feedforward layers with 400 nodes and a bidirectional gated recurrent units layer with 200 units, followed by similar three layers with half nodes, followed by a feedforward layer with 50 nodes and a softmax layer. 

 Martins et al. ( We use Global Linear Models (GLM)  (Collins, 2002)  with dynamic learning (GLMd)  (Bic ?ici, 2018)  for word-and phrase-level translation performance prediction. GLMd uses weights in a range  [a, b]  to update the learning rate dynamically according to the error rate. Evaluation metrics listed are Pearson's correlation (r), mean absolute error (MAE), and root mean squared error (RMSE). 

 Mixture of Experts Models We use prediction averaging  (Bic ?ici, 2018)  to obtain a combined prediction from various prediction outputs better than the components, where the performance on the training set is used to obtain weighted average of the top k predictions, ? with evaluation metrics indexed by j ? J and weights with w: w j,i = w j,i 1?w j,i ? ? ? k = 1 k k i=1 ? ? ?i MEAN ? ? ?j,w j k = 1 k i=1 w j,i k i=1 w j,i ? ? ?i ? ? ?k = 1 |J| j?J ? ? ?j,w j k MIX (1) We assume independent predictions and use p i /(1 ? p i ) for weights where p i represents the accuracy of the independent classifier i in a weighted majority ensemble  (Kuncheva and Rodr?guez, 2014) . We only use the MIX prediction if we obtain better results on the training set. We select the best model using r and mix the results using r, RAE, MRAER, and MAER. We filter out those results with higher than 1 relative evaluation metric scores. We also use stacking to build higher level models using predictions from base prediction models where they can also use the probability associated with the predictions (Ting and Witten, 1999). The stacking models use the predictions from predictors as features and build second level predictors. For the document-level RTM model, instead of running separate MTPPS instances for each training or test document to obtain specific features for each document, we concatenate the sentences from each document to obtain a single sentence representing each and then run an RTM model. This conversion decreases the number of features and obtains close results  (Bic ?ici, 2018) . Before model combination, we further filter prediction results from different machine learn- ing models based on the results on the training set to decrease the number of models combined and improve the results. A criteria that we use is to include results that are better than the best RR model's results. In general, the combined model is better than the best model in the set and stacking achieves better results than MIX. 

 Results We tokenize and truecase all of the corpora using Moses'  (Koehn et al., 2007)  processing tools. 2 LMs are built using kenlm  (Heafield et al., 2013) . The comparison of results on the training set are in Table  2  and the results on the test set we obtained after the competition are in Tables  3 and 5 . Official competition results of RTMs are similar. We convert MQM annotation to word-level tags to train GLMd models and obtain word-level predictions. Addition of the tagging features from the word-level prediction improves the training results significantly but does not improve the test results at the same rate, which indicates overfitting. The reason for the overfitting with the word-level features is due to their high correlation with the target. Table  4  lists some of the top individual feature correlations for en-ru in Task1. Top 26 highly correlated features belong to word-level features. We also obtained new results on QET18 datasets and experimented adding features from word-level predictions on the QET18 sentencelevel results. QET18 results in Table  3  are improved overall. 

 Conclusion Referential translation machines pioneer a language independent approach and remove the need to access any task or domain specific information or resource and can achieve top performance in automatic, accurate, and language independent prediction of translation scores. We present RTM results with stacking.    Figure 1 : 1 Figure 1: RTM depiction: parfda selects interpretants close to the training and test data using parallel corpus in bilingual settings and monolingual corpus in the target language or just the monolingual target corpus in monolingual settings; an MTPPS use interpretants and training data to generate training features and another use interpretants and test data to generate test features in the same feature space; learning and prediction takes place taking these features as input. 
