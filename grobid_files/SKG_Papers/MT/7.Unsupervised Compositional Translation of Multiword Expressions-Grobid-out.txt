title
Unsupervised Compositional Translation of Multiword Expressions

abstract
This article describes a dependency-based strategy that uses compositional distributional semantics and cross-lingual word embeddings to translate multiword expressions (MWEs). Our unsupervised approach performs translation as a process of word contextualization by taking into account lexico-syntactic contexts and selectional preferences. This strategy is suited to translate phraseological combinations and phrases whose constituent words are lexically restricted by each other. Several experiments in adjective-noun and verb-object compounds show that mutual contextualization (co-compositionality) clearly outperforms other compositional methods. The paper also contributes with a new freely available dataset of English-Spanish MWEs used to validate the proposed compositional strategy.

Introduction In the field of compositional distributional semantics there have been some interesting research, though not too much, making use of a syntaxsensitive vector space to compose the meaning of phrases and sentences  (Erk and Pad?, 2008; Thater et al., 2010; Erk et al., 2010; Weir et al., 2016) . In those approaches, dependency-based combination of vectors enables words to be disambiguated as a process of contextualization. More precisely, given two words, a and b, related by a syntactic dependency (r), the meaning of the corresponding composite expression is actually two contextualized senses: a , which is the contextualized sense of a resulting from combing this word with the selectional restrictions imposed by b in relation r; and b , which stands for the contextualized sense of b as a result of combining this word with the restrictions imposed by a in r. Moving towards a multilingual scenario, the objective of this paper is to apply this unsupervised method to a bilingual vector space so as to model translation as a process of compositional contextualization. In this regard, we first create contextualized vectors using selectional preferences, and then we generate possible translations by taking advantage of crosslingual word embeddings learned from monolingual corpora. The results of several experiments in English-Spanish adjective-noun and verb-object compounds show that mutual contextualization (or co-compositionality) clearly outperforms other compositional methods. Additionally, this paper also contributes with a new freely available dataset of 273 English-Spanish compound equivalents. This new resource contains multiword expressions (MWEs) with different degrees of semantic compositionality (free combinations such as use a computer, collocations -for instance, hard drug-, light-verb construction -e.g., take a cab-, or idioms such as lone wolf ), which are useful to evaluate translation strategies using compositional approaches. It is worth noting that MWEs can fall into a wide spectrum of compositionality, from compositional compounds to idiomatic expressions  (Cordeiro et al., 2019) . To restrict the object of study, in this article, we focus on a specific subset of MWEs: adjective-noun and verb-noun compounds. The rest of this article is organized as follows. Section 2 describes the compositional translation method. In Section 3 we describe the English-Spanish dataset and use it to evaluate the proposed strategy. Then, some related work is presented in Section 4. Finally, Section 5 addresses conclusions, drawbacks of the strategy and future work. 

 Compositional Translation with Cross-Lingual Embeddings The proposed method consists of two main tasks: i) the construction of contextualized word mean-ing by means of a syntax-sensitive compositional distributional strategy (see 2.1); ii) word contextualization in a bilingual vector space allowing the translation of compounds (See 2.2). We will focus on the translation two-word compounds encoded through a single syntactic dependency. 

 Compositional Distributional Meaning We abandon the traditional choice of representing the meaning of a phrase or sentence as a single vector. In our approach, the meaning of a composite expression is represented by a contextualized vector for each constituent word rather than by a single vector standing for the entire expression  (Erk and Pad?, 2008; Weir et al., 2016; Gamallo, 2017) . This is in accordance with the main postulates of Dependency Grammar which only defines linguistic categories for words and relations, but not for composite units such as phrases or sentences. Let us take the dependency (r, h, d), where r is a binary relation between the head word, h, and the dependent one, d. This dependency can be used to yield two lexico-syntactic contexts: (?r, h) (1) (?r, d) (2) where ?r and ?r are the head and dependent roles of relation r, respectively. The tuple in 1 represents a lexico-syntactic context of word d while tuple 2 is a context of h. Given these two contexts, the meaning of a binary dependency is represented by two contextualized vectors: h (?r,d) and d (?r,h) , which are defined as follows: h (?r,d) = h + d ?r (3) d (?r,h) = d + h ?r (4) where h ?r and d ?r are vectors representing selectional preferences, more precisely, h ?r stands for the selectional preferences imposed by the head, h, to the dependent word, d, and d ?r represents those imposed by the dependent one to the head. So, the contextualized sense of a word is the result of adding (by component-wise vector sum) its direct vector with another one representing the selectional preferences imposed by the word linked to it in the syntactic dependency. Head and depen-dent selectional preferences are defined as follows: h ?r = 1 N d:(?r,d)?Sal ?r (h) d (5) d ?r = 1 N h:(?r,h)?Sal ?r (d) h (6) where Sal ?r (h) and Sal ?r (d) are two sets of salient contexts: the most salient contexts of the head, h, with the role ?r and the salient contexts of the dependent d with the role ?r, N being the cardinality of each set. The set of salient contexts of a word consists of its top-N contexts ranked using a lexical association measure (e.g., PPMI, loglikelihood, etc). The top-N contexts are considered to be the most salient and informative for the given word. The summation runs through the lemmas that make up the salient contexts in equations 5 and 6. Equation 5 defines the head preferences and Equation 6 the dependent preferences. Let us take an example. The dependency (amod, drug, hard), from the compound "hard drugs", gives rise to two contextualized senses: drug (?amod,hard) = drug + hard ?amod (7) hard (?amod,drug) = hard + drug ?amod (8) The resulting vector in Equation 7 is the contextualized sense of drug as being modified by the adjective hard, while the vector in 8 represents the contextualized sense of hard when it modifies the noun drug. The selectional preferences imposed by the noun (head preferences), noted drug ?amod , are actually the result of adding the vectors of the most representative (salient) adjectives modifying that noun, divided by the number of representative adjectives. Intuitively, it represents the main properties of drugs, for instance, psychoactive, hallucinogenic and illicit are the three more salient adjectives modifying the noun drug in our experiments. On the other hand, the selectional preferences imposed by the adjective (dependent preferences), and noted hard (?amod,drug) , are the result of adding the vectors of the most representative nouns modified by the adjective, divided by the number of representative nouns. So, it represents the set of most salient hard things; for example, bop, disc and rock are the three most salient nouns modified by the adjective hard in our corpus. English dependency Spanish candidates (amod, drug, hard) (amod, medicamento, duro) , (amod, medicamento, dif?cil) (amod, medicamento, f?cil) , (amod, medicamento, imposible) (amod, medicamento, arduo) , (amod, droga, duro) (amod, droga, dif?cil) , (amod, droga, f?cil) (amod, droga, imposible) , (amod, droga, arduo) (amod, estupefaciente, duro) , (amod, estupefaciente, dif?cil) (amod, estupefaciente, f?cil) , (amod, estupefaciente, imposible) (amod, estupefaciente, arduo) , (amod, coca?na, duro) (amod, coca?na, dif?cil) , (amod, coca?na, f?cil) (amod, coca?na, imposible) , (amod, coca?na, arduo) (amod, f?rmaco, duro) , (amod, f?rmaco, dif?cil) (amod, f?rmaco, f?cil) , (amod, f?rmaco, imposible) (amod, f?rmaco, arduo) Table  1 : 25 Spanish candidate translations of the English collocation "hard drug". Only the one in bold is an acceptable translation. The English drug was translated into Spanish by: medicamento (medicine), droga (narcotic), estupefaciente (narcotic), coca?na (cocaine), and f?rmaco (medicine). And the adjective hard was translated by: duro (hard), dif?cil (difficult), f?cil (easy), imposible (impossible), and arduo (arduous). We added the most common English translation of each Spanish word so that readers who do not know Spanish will understand the ambiguity issue. 

 Compositional Translation of Dependencies The compositional translation of an expression syntactically codified in a binary dependency consists of three steps: i) generation of translation candidates in the target language, ii) construction of the compositional meaning of the source dependency and the candidates in the target language, and iii) selection of the most similar candidate to the source dependency. The input of the system is a dependency in the source language which is expanded into a set of candidate translations in the target language by making use of a translation lexicon automatically built with cross-lingual embeddings and Cosine similarity. For instance, let us take an English-Spanish translation lexicon and select the five most similar nouns to drug and the five most similar adjectives to hard. Taking into account these translations, the English dependency (amod, drug, hard) is expanded in the 5x5 Spanish candidates shown in Table  1 . Once the candidates have been generated, the next step is to build the compositional vectors (contextualized senses) of both the input dependency and translation candidates, by making use of the algorithm used in the previous sub-section (2.1) and the cross-lingual embeddings of the previous step. Finally, the compositional vectors of the candidates are compared pairwise with the source compositional vectors by means of cosine similarity and the most similar is selected. For the binary dependency in the source language, a translation candidate is selected by computing the contextualized translation measure, CT , which selects the most similar dependency in the target language by comparing the degree of similarity between heads and dependents in both languages. More precisely, given a dependency (r, h, d) in the source language, its translation into the target language is computed as follows: CT (r, h, d) = (9) arg max (r ,h ,d )? S(h (?r,d) , h (?r ,d ) ) + S(d (?r,h) , d (?r ,h ) ) 2 where (r , h , d ) is any target dependency belonging to the set of translation candidates, ?. The first S computes the similarity between the two compositional vectors derived from the contextualized heads in the two languages. The second one computes the similarity between the vectors derived from the contextualized dependent words. So, CT is nothing more than the overall similarity between two composite expressions, which is the addition mean of the similarity scores obtained by comparing their head-based and dependent-based compositional vectors. The resulting translation is, thus, the composite expression belonging to ? with the highest overall similarity score. 

 Experiments To have an idea about the quality of compositional vectors, most of the research done so far has made use of monolingual datasets prepared to measure the correlation between individual human similarity scores and the system's predictions  (Mitchell and Lapata, 2008; . Nonetheless, we consider that translation of composite expressions and MWEs is a more reliable way of evaluating the quality of compositional strategies. For instance, it is not clear whether blue car is semantically closer to red car than to yellow car, however, no one doubts that the Spanish translation of red car is coche rojo. In order to allow an evaluation based on compositional translation, we have created two bilingual datasets with MWEs syntactically coded by means of two dependencies: adjective-noun (amod) and verb-noun (vobj). 

 Test Datasets To evaluate our compositional translation algorithm, it is required a bilingual resource containing a set of phrases with a simple syntactic structure in the source language with their possible translations into the target language. As there is no such resource, we decided to generate it by taking advantage of a free list of multilingual MWEs which was obtained using parallel corpora (Garcia, 2018). The method presented in the referred paper extracts candidates of syntactic collocations using PPMI and frequency thresholds, and then identifies multilingual equivalents using bilingual word embeddings. From this resource, we selected 200 English-Spanish examples: 100 bilingual equivalents of adj-noun (amod) collocations (e.g., facial hair), and 100 verb-object (vobj) examples (e.g., take [a] cab). These lists were manually reviewed and enlarged with more possible translations, obtaining a final resource of 273 English-Spanish pairs (92 amod expressions with 143 translations, and 83 vobj English examples with 130 Spanish equivalents). It is worth mentioning that as these lists were built using statistical association measures they contain not only phraseological combinations, but also other expressions with different degrees of se-mantic compositionality: free combinations (use [a] computer), true collocations (e.g., deep condolence, and also light-verb constructions such as take [a] cab), terms (sulfuric acid), quasiidioms (buy [the] silence), or idioms (lone wolf )  (Mel'?uk, 1998) . 1 Thus, this variety of expressions converts the lists into a valuable resource for evaluating the translation of adj-noun and verbobject instances. 2 

 Corpora and Distributional Models In order to build bilingual compositional vectors, we made use of English and Spanish wikipedias (dumps files of December 2018), with 21 and 5 billion words, respectively. The two wikipedias were PoS tagged and syntactically analyzed with LinguaKit  (Gamallo et al., 2018) . The syntactically analyzed corpus was the basis for the elaboration of the salient lexico-syntactic contexts with which we constructed selectional preferences and contextualized vectors. Preliminary experiments were performed to find the best configuration, which was set to 50 salient contexts per lemma/PoS tag pair. Bilingual embeddings were created with VecMap  (Artetxe et al., 2018a ) by using the supervised configuration and an open available English-Spanish dictionary, Apertium, containing 6,249 nouns, verbs, and adjectives. 3 . To make the evaluation fairer, we have removed from the dictionary all English words belonging to the test datasets. The original embeddings mapped by VecMap were created with Word2Vec, configured with CBOW algorithm, window 5, and 300 dimensions  (Mikolov et al., 2013b) . Word2Vec was applied on PoS tagged wikipedias and each token was coded as a lemma/tag pair. The bilingual mapped models with lemma/tag embeddings are made freely available. 4 

 Translation Candidates Using the bilingual vectors built from Wikipedia, each English word appearing in the test datasets was associated with the 10 most similar Spanish words and, so, each English binary dependency of the dataset was expanded with 10x10 candidate 1 Note, however, that in ambiguous cases, the compositional translation was preferred (e.g., cut [a] cable). 2 Both datasets have been added as suplementary material to the submission 3 https://github.com/apertium/ apertium-trunk 4 https://ufile.io/lrze1 (anonymous account) Spanish dependencies. It means that each English expression was compared with 100 Spanish translation candidates. It is worth pointing out that the correct translation is not always present in the 100 candidates. Yet, previous experiments allowed us to verify that increasing the number of translation candidates did not improve the final results. 

 Evaluation To evaluate our compositional strategy, CT(head+dep), which combines both head and dependent contextualized words (see equation 9), we compared its performance to five other approaches: CT(head), which only considers the contextualized head; CT(dep), which only takes into account the contextualized dependent word; mult, which combines the vectors of the two related words by pairwise multiplication; add, which combines vectors by pairwise addition; and corpus, which implements the corpus-based strategy described in  (Grefenstette, 1999)  by just selecting the most frequent translation candidates in the Spanish corpus. All strategies but corpus use the same bilingual word embeddings and the same similarity measure (cosine) between compositional vectors. Additionally, we also included UNdreaMT in the evaluation. UNdreaMT is a recent neural machine translation system which uses monolingual corpora and cross-lingual word embeddings to learn translation models in an unsupervised way  (Artetxe et al., 2018b) . In the learning process, UNdreaMT applies backtranslation and uses a single shared encoder for both languages. To compare our compositional strategy with UNdreaMT, this system was trained with exactly the same monolingual corpora and word embeddings used by the other models. As UNdreaMT works with surface structures (and not dependency pairs), we adapted the input to not harm the system (e.g., package,bring ? bring the package). Also, we manually modified the output to adapt it to the gold-standard format (e.g., b?sico instinto ? instinto,b?sico). Table  2  shows the results of all these methods on the two datasets (amod and vobj) described above. The table shows the accuracy, which is the number of correct translations divided by the number of different English expressions (source language) in each dataset. It is worth noting the significant difference between the proposed strategy, CT(head+dep), and the rest of methods. The two methods based on just one contextualized word, CT(head) and CT(dep), obtain similar scores to the well-known baselines, mult and add, as well as to the unsupervised MT strategy implemented with UNdreaMT. However, all these systems reached values far below those obtained by CT(head+dep) combining the two contextualizations within the dependency. Going into more detail, vector addition (add) outperforms vector multiplication (mult) in the two datasets, and also the contextualized dependent word performs better than the contextualized head in the two datasets. Finally, corpus gets the lowest values of all the compared methods.  

 System 

 Error Analysis We carried out an error analysis of the CT(head+dep) model to know in detail in what types of expressions our strategy fails. So every wrong translation of the system was analyzed and classified into the following five error types (see Table  3  for quantitative results): DistSimil: the most frequent errors arose from the distributional strategy (they are common in other vector-based approaches), since words belonging to different semantic relations (e.g., antonyms) may have very similar vectors. In our experiments, CT(head+dep) translated male victim by v?ctima femenina (female victim), or take a cab as tomar un furg?n (take a van). Conventions: another frequent source of errors was the generation of expressions which do not collocate, e.g., they do not follow the conventions of the target language, even if the meaning is transparent. In this regard, fill a report was translated by llenar un informe instead of rellenar un informe (both verbs in Spanish mean to fill, but llenar is most used for physical objects, e.g., llenar el vaso, fill the glass). Similarly, the system generated evidencia verdadera (instead of evidencia real) from real evidence. Translation: 11% of the errors were approximate translations which do not appear in the dataset. This includes some combinations which may have slightly different meaning (depending on the context), such as pr?xima d?cada and siguiente d?cada (from next decade), and cases of polysemy: share a cell, where cell may refer to a biological cell (c?lula in Spanish), and a room in a prison or a part of a spreadsheet (both translated as celda). Idiomacity: some non-compositional expressions were not correctly translated, such as lone wolf (which usually refers to a person and not to an animal), which was translated as lobo indefenso (vulnerable or defenseless wolf ). Data processing: finally, few errors emerged from problems in the data (or in its preprocessing: tokenization, lemmatization, etc.). As an example, the noun in industrial area was translated by area (which does not exist in Spanish) instead of ?rea. 

 Discussion on Co-Compositionality The high accuracy reached by the strategy based on the two contextualizations seems to verify the co-compositionality hypothesis  (Pustejovsky, 1995) , which states that the head word imposes selectional restrictions on the dependent one, while this one also imposes its restrictions on the former. It follows that a syntactic dependency between two words carries two complementary selective functions, each one imposing its own selectional pref-erences. These two functions allow the two related words to mutually disambiguate or discriminate the sense of each other by co-composition However, co-compositionality has not been considered by many formal semantic approaches. In most approaches to formal semantics, inspired by Categorial Grammar, the interpretation of composite expressions such as "hard drug" relies on a rigid function-argument structure. In an adjectivenoun construction, the adjective denotes an unary function applied to the noun denotation. Any syntactic dependency between two lexical words is generally represented in the semantic space as the assignment of an argument to a lexical function which impose its selectional preferences. There is just one direction in the process of contextualization: the word representing the lexical function contextualizes (imposes its preferences to) the word representing the passive argument. This one-way compositional procedure is also present in some work on distributional compositional semantics  (Baroni et al., 2014; . Unfortunately, a comparison with these one-way strategies has not been possible because they have not yet been applied to compositional translation. 

 Related Work The proposed compositional method integrates three different tasks: to build compositional vectors representing the contextualized sense of composite expressions; to build cross-lingual word embeddings from monolingual corpora; to propose contextualized translations with compositional and cross-lingual vectors. The basic approach to distributional composition is to combine vectors of two syntactically related words with arithmetic operations: addition and component-wise multiplication  (Mitchell and Lapata, 2008 , 2009 . This approach is not strictly compositional since it does not take into account the syntactic structure underlying the expression. It does not consider the functionargument relationship underlying compositionality in Categorial Grammar approaches  (Montague, 1970) . Other approaches propose compositional models inspired by Categorial Grammar. Some induce the compositional meaning of functional words from examples adopting regression techniques commonly used in machine learning  (Ba-roni and Zamparelli, 2010; Krishnamurthy and Mitchell, 2013; Baroni, 2013; Baroni et al., 2014) , and others use tensor products for composition  (Coecke et al., 2010; . Although compositional, none of them is based on co-compositional strategy, like ours. There are also studies making use of neuralbased approaches, namely bidirectional long short-term memory networks, to deal with word contextualization  (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018) . However, word contextualization is not defined by means of syntax-based compositional functions, as they do not consider the syntactic functions of the constituent words. As has been said, our compositional approach is inspired by the work described in  Erk and Pad? (2008)  and  Erk et al. (2010) , in which second order vectors represent selectional preferences and each word combination gives rise to two contextualized word senses. More recently,  Weir et al. (2016)  describe a similar approach where the meaning of a sentence is represented by the contextualized senses of its constituent words. Each word occurrence is modeled by what they call anchored packed dependency tree, which is a dependency-based graph that captures the full sentential context of the word. The main drawback of this context-based approach is its critical tendency to build very sparse word representations. Our approach is an attempt to join the main ideas of these syntax-sensitive models (namely, the use of selectional preferences and two returning word senses per combination) in order to apply them to contextualized translation. The method proposed in this paper also relies on count-based techniques to build bilingual vectors from monolingual corpora  (Fung and McKeown, 1997; Rapp, 1999; Saralegi et al., 2008; Ansari et al., 2014) . Neural-based strategies also have been used to learn translation equivalents from word embeddings  (Mikolov et al., 2013a; Artetxe et al., 2016 Artetxe et al., , 2018a . They learn a linear mapping between embeddings in two languages that minimizes the distances between equivalences listed in a bilingual dictionary. Finally, many approaches to compositional translation of phrases and composite terms consist in decomposing the source term into atomic components, translating these components into the target language and recomposing the translated com-ponents into target terms  (Delpech et al., 2012; Tanaka and Baldwin, 2003; Grefenstette, 1999) . Selection of the best translation candidate is performed by means of corpus-based searching. However, this strategy has not yielded good results in the experiments described in the previous section. Our translation approach also follows the decomposing strategy but, unlike the works cited above, we use compositional/contextualized vectors to select the best candidate instead of basic corpus-based frequencies. 

 Conclusions In this article, we tried to show that it is possible to apply compositional distributional semantics on a bilingual vector space to propose contextualized translations. However, the proposed contextualization method has several drawbacks that need to be addressed in future work. First, it will be necessary to deal with fertile translations, i.e. translations in which the target term has a different number of words (and so a different syntactic structure) than the source one. For this purpose, we will expand the set of translation candidates by making use of a great variety of extraction strategies as, for instance, a Mel'?uk-based strategy consisting of identifying similar words to the base of a collocation  (Mel'?uk, 1998) . Second, our method does not distinguish between compositional and non-compositional expressions. It will probably be necessary to first identify the degree of compositionality of the source MWE before choosing the compositional translation strategy that best suits that expression  (Cordeiro et al., 2019) . And third, increasingly complex expressions consisting of more than one dependency will have to be dealt with. For this purpose, the method will have to be generalized to any input sentence with any syntactic structure, giving rise to an unsupervised machine translation approach. ators (BBVA Foundation), Juan de la Ciervaincorporaci?n grant  (IJCI-2016-29598) . Finally, we gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan Xp GPUs used for this research. Table 3 : 3 Error classification (type and percentage) of the CT(head+dep) system. Total values are the microaverage. Type amod vobj Total DistSimil 42.86 66.67 53.85 Convention 21.43 25 23.08 Translation 14.29 8.33 11.54 Idiomacity 14.29 0 7.69 DataProcess 7.14 0 3.85
