title
The University of Edinburgh's English-Tamil and English-Inuktitut Submissions to the WMT20 News Translation Task

abstract
We describe the University of Edinburgh's submissions to the WMT20 news translation shared task for the low resource language pair English-Tamil and the mid-resource language pair English-Inuktitut. We use the neural machine translation transformer architecture for all submissions and explore a variety of techniques to improve translation quality to compensate for the lack of parallel training data. For the very low-resource English-Tamil, this involves exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, we explore the use of multilingual systems, which, despite not being part of the primary submission, would have achieved the best results on the test set.

Introduction The University of Edinburgh participated in the WMT20 news translation shared task for English-Tamil and English-Inuktitut in both translation directions.  1, 2  Neither language pair benefits from large quantities of parallel data, so we approach training using different techniques to compensate for this lack of data: pretraining and iterative backtranslation for English-Tamil and multilingual systems for English-Inuktitut. We use neural machine translation (MT) and specifically the transformer architecture  (Vaswani et al., 2017) : the base variant for the lower-resourced English-Tamil and the big variant for the mid-resource English-Inuktit. In both cases, significant improvements are seen when compared to the in-house baselines tested, particularly notable for pretraining for English-Tamil. Awaiting the results of the official human evaluation, we report the automatic evaluation scores using BLEU  (Papineni et al., 2002)  as implemented in sacreBLEU  (Post, 2018) . A summary of these results on the dev and test sets can be found in Table 1 for all UEDIN submissions. The details of our submissions can be found in Section 2 for English-Tamil and in Section 3 for English-Inuktitut.  

 English?Tamil As for our English?Gujarati systems last year at WMT19  (Bawden et al., 2019) , we use pretraining and data augmentation to tackle the low-resource language pair English-Tamil. Our experiments show that pre-training, training on backtranslated data and then fine-tuning is useful in both directions, although we introduce slight variations in the training and fine-tuning approaches used for each language direction. 

 Data and pre-processing Our models are trained in the constrained scenario, using publicly available WMT20 data. We choose to exclude the terminology-like Wikititles as well as WikiMatrix 3 from our training data, using only  the corpora shown in Table  2 . We use both parallel data and monolingual data for English-Tamil and also exploit parallel data available for English-German as a form of pre-training. All data was first cleaned, keeping sentences of 3-100 (untokenised) units, for which the length ratio between the parallel sentences is maximum 2.2, and do not contain more than 50% non-alphabetic characters or more than 50% of words without an alphabetic character.  4  We deduplicate the data and normalise punctuation using Moses  (Koehn et al., 2007) . We then apply subword segmentation using SentencePiece  (Kudo and Richardson, 2018)  and the BPE strategy  (Sennrich et al., 2016) . 5 

 Approach used We adopt a three-step approach to training our models, consisting of: (i) pre-training model parameters using either an mBART language model or a translation model for the highly resourced De-En language pair, (ii) iterative backtranslation to produce synthetic parallel data of increasing quality, and (iii) final model creation consisting of finetuning pretrained models using both genuine parallel and backtranslated data. We provide the details of these three steps below. 

 Pre-training We experimented with several pretraining objectives: language modelling using XLM  (Lample and Conneau, 2019a)  or mBART  (Liu et al., 2020) , and MT pre-training using a higher-resourced language pair (namely English-German). Using a higher-resourced language pair for pretraining, even if this pair is unrelated to the language pair on which the model is fine-tuned, has  4  An alphabetic character is one belonging to the language in question: the Latin alphabet for English and the Tamil script, which is an abugida script. 5 All models are learnt jointly over the languages used for training (English, Tamil and in one case German too). The vocabulary size is dependent on the model trained and is specified in the experimental details below. shown to be an effective and simple way of boosting performance  (Kocmi and Bojar, 2018; Aji et al., 2020) . For the De-En models, we had to choose between (i) initialising only model parameters and (ii) preserving all model and training parameters from the parent model (similar to ). We chose the first option as it produced better results in our experiments. For mBART pretraining, we use all Tamil and English monolingual data without shuffling or deduplication. We tag the input segments with a language tag and a domain tag: either in-domain (news) or out-of-domain as in  (Caswell et al., 2019) . For XLM pretraining we use the deduplicated and shuffled corpus (since cross-sentence context is not needed) and we subsample the English because of computing cost. We also use domain tags, with language information provided in the form of language embeddings as per the standard implementation. For De-En pre-training, we use all De-En parallel data described in Table  2 , with a joint English-Tamil-German vocabulary. We experiment with pretraining models in the two directions (De?En and En?De) and find that the De?En model produces better results when fine-tuned on TA-EN data.  Table  3  shows the results of each of the pretraining methods once they have been fine-tuned on Ta-En parallel data: the results are very similar and all methods perform substantially better than the baseline, which is trained on parallel data only but optimised in terms of training parameters and subword segmentation. We choose to use De-En pretraining for our final models and a mixture of De-En and mBART pretraining for intermediate MT models used for data augmentation (see the next paragraph). Iterative backtranslation Data augmentation by backtranslating monolingual data has long been used in MT to provide greater amounts of in-domain parallel data in low resource settings  (Bertoldi and Federico, 2009; Bojar and Tamchyna, 2011) . We use backtranslation to translate the monolingual in-domain English and Tamil texts into the other language using an intermediate MT model and use the resulting synthetic parallel data to train new MT models. We apply this iteratively , as shown in Figure  1 , to produce successively better MT models, initialising the models at each stage using either mBART or De-En pretraining. The intermediate MT models used to produce backtranslations are in white and the final models, which are then fine-tuned (as specified in the section entitled Final model creation) are in grey. 1. We first train a Ta?En model initialised with mBART pretraining and fine-tuned on parallel data only. We then use this model to backtranslate all monolingual Tamil data into English. 2. We use the resulting backtranslated data together with the genuine parallel data to train an mBART-pretrained En?Ta model. After early stopping, we continue training using the genuine parallel data only. We then use this model to backtranslate 5M sentences of indomain English data into Tamil. 6 3. We use this new backtranslated data together with the genuine parallel data oversampled 7 times to train a second mBART-pretrained Ta?En model. After early stopping, we continue training using genuine parallel data only. We then use this model to backtranslate all the monolingual Tamil data. 4. We repeat step 2 with this latest backtranslated data, generating the final backtranslations to be used for the Ta?En direction. 5. We use 5M of these final backtranslations along with the Ta-En genuine parallel data oversampled 15 times to fine-tune a De-En pretrained model and use this to generate the final backtranslations to be used for the En?Ta direction. The results of the iterative backtranslation steps on the dev set can be found in In addition to the described strategy, we also experimented with training different pretrained models using backtranslations produced by different ing steps are filtered using the same processing as described in Section 2.1, filtered using dual conditional cross-entropy filtering  (Junczys-Dowmunt, 2018)  and the top sentences are selected to train the next step. models (e.g. training an mBart pretrained model on XLM-produced backtranslations). We report a small selection of these experiments here for one of the backtranslation steps, comparing the use of alternatives to system 2 (from Figure  1 ). These results are shown in Table  5 : (i) a pretrained mBART model trained on backtranslations from each of the pretrained models, and (ii) a pretrained De-En model trained on XLM backtranslations. For this particular step of the iterative process, training a pretrained mBART model on backtranslations produced by the pretrained mBART model produced the best scores, explaining why this was chosen. Final model creation Our final models are pretrained De-En models (in grey in Figure  1 ). After pretraining, the finalisation of these models follows a two-step training strategy to incorporate the synthetic and genuine parallel data: 1. We first train our models on the synthetic data described previously (20M sentences for Ta?En and just over 2.1M sentences for En?Ta). 2. We then fine-tune the models on a mixture of parallel and synthetic data. This approach of pre-training on synthetic data and fine-tuning on genuine and synthetic data has been found to work well for other tasks  (Junczys-Dowmunt and Grundkiewicz, 2016; . For the second step, we adopt different strategies for each language direction, depending on which worked best. For Ta?En, we finetune on genuine parallel data and 500k of the top scored backtranslations.  7  For En?Ta we fine-tune on a mixture of genuine parallel data, synthetic data produced using multi-agent dual learning (MADL;  Wang et al., 2019; Kim et al., 2019)  and the top 1M backtranslations. This MADL data comprises a mixture of forward translations and backtranslations of the parallel data created using intermediate models in both directions. We also carried out preliminary experiments with multilingual training using other Indian languages and experiments with phrase-based MT using Moses  (Koehn et al., 2007)  but they did not achieve good results. 

 Experimental settings We use the Marian toolkit  for all models except for those using XLM pretraining, for which we use the Facebook XLM toolkit  (Lample and Conneau, 2019b) . All models trained (including those used to produce backtranslations) use the Transformer-base architecture  (Vaswani et al., 2017)  with default hyperparameters according to the Marian or XLM implementation (6 encoder and 6 decoder layers, embedding dimension of 512, 16 heads, feedforward dimension of 2,048, standard learning rate warm-up). Parallel-only baseline Our parallel-only baseline is trained with a joint vocabulary of size 5,414 for Ta-En and 418 for En-Ta. The En-Ta model was trained with a small batch size of 1000 tokens. mBart and XLM models are trained with a joint vocabulary size of 20,000 SentencePiece BPE subwords (including special tokens for language and domain tags, masking and sentence separators). mBART training English and Tamil sentences are mixed in equal amounts in each batch. We use our re-implementation of mBART using Marian.  8  We deviate from the original implementation by always using two sentences per input segment, whereas the original paper used as many sentences as they could fit into the 512-token limit. The noise hyperparameters are the same as the original paper (35% of tokens are masked in contiguous spans of an average of 3.5 tokens. Masked spans do not cross sentence boundaries). Unlike XLM, we do not use online backtranslation during pre-training. We train until early stopping based on an held-out non-parallel dataset generated using the same noise function as the training data. During monolingual pretraining we early stop after the validation score (measured every 5,000 updates) does not improve for 10 consecutive times. When training on backtranslations or finetuning on parallel data we early stop on the parallel development corpus, measuring the valdation score every 500 updates. De-En pretraining For models with De-En pretraining, we trained a SentencePiece model with a vocabulary size of 32k on roughly equal amounts of Tamil, English and German data (subsampling Ger-man and English). The final MT vocabulary size is 49,213 as it is based on using all German-English data for training. The models are trained using tied target embeddings, a learning rate of 0.0002, the Adam optimiser  (Kingma and Ba, 2015)  and optimiser delay of 2 on 4 GPUs. We train all models until convergence based on the BLEU score on the held-out dev set provided for the task. 

 Results Table  6  shows the final automatic evaluation score of our submissions for both directions on the dev set and the test set, including an ablation of the various components: pretraining using the De-En MT data (and fine-tuned on parallel data), addition of synthetic data to this setup and finally fine-tuning of the resulting model as specified previously. Table  6 : EN?TA results (BLEU scores) for the successive steps in the creatino of our final models. The Last row represents the primary submission systems. The best results (8.40 for En-Ta and 16.60 for Ta-En) are achieved with all three approaches to training. We found that ensembling did not improve our results and therefore our submitted systems are single models. We note that our final approach sees a big difference in the BLEU score between the dev and test sets. While BLEU scores are not directly comparable across datasets, the drop is quite significant and could indicate a domain shift between the two sets. Our models rely heavily on the use of backtranslated data and therefore could be adapting to translationese, which is rewarded in the dev set but not in the test set. 

 English?Inuktitut Compared to English-Tamil, the English-Inuktitut language pair is relatively well-resourced at approximately 1.3M sentence pairs. As such we were able to train conventional bilingual Transformer systems, which formed the basis of our submission. We also trained multilingual systems, but opted not to use these in our submission as results on the dev set did not appear to be promising (although evaluation proved challenging for this pair due to overlap between the training and dev data). Postsubmission evaluation showed that our multilingual systems actually outperformed our submitted systems on the test sets. 

 Data and Preprocessing We used all of the Nunavut Hansard data provided by the task organisers. For Inuktitut?English, this was supplemented with a similar volume of synthetic data, back-translated from the English side of the Europarl and News Crawl corpora. The only additional monolingual Inuktitut data was 163k sentences of common-crawl data, which we backtranslated for the English?Inuktitut system. We developed two multilingual systems: English? {Inuktitut,German,Russian} and {Inuktitut,German,Russian} ? English. The Russian and German languages were selected due to the availability of suitable volumes of data in the domains of interest (news and parliamentary proceedings). Both multilingual systems used the same dataset, which contains genuine iu-en, synthetic iu-en, genuine de-en, and genuine ru-en in a ratio of approximately 1:1:2:2 (both systems used all of the synthetic data, regardless of back-translation direction).  For the bilingual systems, our preprocessing pipeline consisted of corpus cleaning and segmentation. For corpus cleaning, we used the clean-corpus-n.perl script from the Moses toolkit  (Koehn et al., 2007) . This applies a maximum length threshold of 80 as well as removing empty sentences and sentence pairs with length ratios greater than 9:1. For segmentation, we trained language-specific SentencePiece models  (Kudo and Richardson, 2018)  with a vocabulary size of 32,000 BPE subwords and a vocabulary threshold of 50. Preprocessing was identical for the multilingual systems except that for the English? {de,iu,ru} we added a token to each source sentence to specify the target language (as in  Johnson et al. (2017) ). After the release of the test set, the task organisers reported that some test sentences had been enclosed in extraneous quotes. For our submission, and for post-submission evaluation, we removed outer quotes prior to translation for any test sentence that began and ended with a double quote character. 

 Experimental Settings We used the Nematus toolkit  (Sennrich et al., 2017)  for all models. For preliminary systems, our hyperparameter settings matched the 'base' configuration of  Vaswani et al. (2017) . We used these systems for back-translation. For the multilingual systems and final bilingual systems, our settings matched  Vaswani et al. (2017) 's 'big' configuration. We used a batch size of 16,384 tokens for all models. Since the bilingual 'big' systems looked the most promising during development, we trained a second model for each direction and used ensembling in our submission systems. 

 Results Table  8  shows the automatic evaluation scores for our submitted ensemble systems as well as individual bilingual systems and multilingual systems. Post-submission evaluation on the test set shows that the multilingual systems outperformed the bilingual systems, which is in contrast to the results obtained on the dev sets during system development. We suspect that the large differences in BLEU between dev/test and bilingual/multilingual to overlap between the Nunavut training and dev data. We found that a large proportion of dev sentences were present in the training data, although many were short, frequently used phrases, such as 'Thank you, Mr. Speaker.' and 'The motion is carried.' During development we tried filtering the dev set to reduce overlap at the sentence level. This lowered the scores, but still produced the same overall order: bilingual big > bilingual base > multilingual and so we used this result to guide our decision on which systems to submit. With hindsight, we suspect that the prevalence of formulaic, but not necessarily identical, constructions in the text may be a complicating factor and that more aggressive filtering of the dev set may have produced more robust results. Compared to the bilingual base or multilingual models, the bilingual big models have more capacity available for memorisation of the training data and it seems that our filtering was not enough to counter this effect. 

 Conclusion In this submission we focused on a low-resource language pair (English-Tamil) and a mediumresource language pair (English-Inuktitut). All our translation systems are based on the Transformer architecture. We found it beneficial to use monolingual data in the form of backtranslations. In the case of En-Ta, we saw notable gains by using pretraining using both the denoising autoencoding (mBART) objective and multilinguality in the form of German-English pretraining. However, we were not able to gain any quality from multilingual training on data for other Indian languages. For English-Inuktit, multilinguality did not appear to help on the dev set, but was found, post-submission, to help on the test set. In general, we found that English-Tamil is a much more challenging task, where pretraining is absolutely necessary to reach acceptable quality, while for English-Inuktit reasonable translation quality can be achieved using only parallel data.  Figure 1 : 1 Figure 1: Iterative backtranslation process 
