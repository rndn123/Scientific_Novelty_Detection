title
Improving NMT via Filtered Back Translation

abstract
Document-Level Machine Translation (MT) has become an active research area among the NLP community in recent years. Unlike sentence-level MT, which translates the sentences independently, document-level MT aims to utilize contextual information while translating a given source sentence. This paper demonstrates our submission (Team ID -DEEPNLP) to the Document-Level Translation task organized by WAT 2020 1 . This task focuses on translating texts from a business dialog corpus while optionally utilizing the context present in the dialog. In our proposed approach, we utilize publicly available parallel corpus from different domains to train an open domain base NMT model. We then use monolingual target data to create filtered pseudo parallel data and employ Back-Translation to finetune the base model. This is further followed by fine-tuning on the domain-specific corpus. We also ensemble various models to improvise the translation performance. Our best models achieve a BLEU score of 26.59 and 22.83 in an unconstrained setting and 15.10 and 10.91 in the constrained settings for En ? Ja & Ja ? En direction, respectively.

Introduction Neural Machine Translation  (Bahdanau et al., 2015; Vaswani et al., 2017a)  has performed impressively in recent years, especially for high resource language pairs. However, one of the shortcomings while translating texts in the form of a paragraph or a document is that the inter-relations among sentences are ignored and the sentences are translated independently. Document Level MT  (Maruf et al., 2020; Zhang and Zong, 2020; Kim et al., 2019b)  aims to utilize these inter-sentential context information to deal with context-dependent phenomena such as coreference, lexical cohesion, and consistency, lexical disambiguation, etc.  (Voita et al., 2019; Lopes et al., 2020)  The meaning of a translated sentence can deviate from its originality when treated independently. WAT 2020's  (Nakazawa et al., 2020 ) Document-level Business Scene Dialogue (BSD) Translation sub-task aims to foster research in the area of document-level MT. To tackle this task, we perform the following steps. Firstly, we gather several publicly available English Japanese corpus and combine them to train an open domain base model. Then, we utilize the monolingual corpus in the target language to create the pseudo parallel corpus. Since the generated pseudo parallel corpus might consist of noisy translated sentences, we use a sentence-level similarity-based filtration technique to filter out such pairs. We then fine-tune the base model on the filtered data followed by fine-tuning on in-domain parallel BSD 2 data. We also utilize checkpoint ensembles to further improve the translation performance. 

 Problem Description This task aims to translate all the sentences in the BSD test file from Ja ? En and vice-versa. Participants could participate either in the constrained setting in which only the official BSD corpus needs to be used or in an unconstrained setting where other resources such as parallel corpora, monolingual corpora, and parallel dictionaries in addition to the official corpora could be utilized. We participate in both settings. BLEU  (Papineni et al., 2002) , RIBES 3 and AMFM  (Banchs et al., 2015)  are used as the official automatic evaluation metrics and are calculated on the tokenized version of translated and reference sentences using different tokenizers such as Juman, KyTea, MeCab, & Moses. 

 Related Work There are two major directions in MT, which has attracted a lot of attention from the research community in recent years -Document Level MT & MT on low resource language pairs. Numerous works have been proposed to tackle document-level MT  (Maruf et al., 2019; Miculicich Werlen et al., 2018) . This area's work involves utilizing contexts on source, target, or both side and designing architectures using either the single  (Ma et al., 2020)  or additional encoder  (Zhang et al., 2018;  to handle contextual information. Some work in this area also tries to analyze the contextual errors  (Kim et al., 2019a) . The work related to low resource language pairs involves making use of monolingual data to create pseudo parallel corpus using back translation  (Sennrich et al., 2016) , iterative back translation  (Hoang et al., 2018)  & filtered back translation techniques  (Junczys-Dowmunt, 2018; Dou et al., 2020) , etc. For filtering noisy pairs,  Imankulova et al. (2017)  uses the Round Trip BLEU score between true and synthetic sentences.  Wang et al. (2019)  propose dynamic domain-data selection along with dynamic clean-data selection. 

 System Description We describe our proposed approach in this section. For the unconstrained setting, we first create BASE models by training N M T s?t and N M T t?s on the open domain dataset. Then, we use these trained N M T s?t & N M T t?s models to translate the monolingual data M s & M t to M t & M s respectively. We then utilize the pseudo parallel data M t and M s along with equal amount of true parallel data to fine-tune N M T t?s model. Similarly, we use the pseudo parallel data M s and M t along with equal amount of true parallel data to fine-tune N M T s?t model. This results in the creation of the back-translated (BT) models. In other settings, instead of utilizing the entire pseudo parallel data, we apply the filtering technique described below on these data to filter out noisy pairs. Then we use these filtered pairs along with an equal amount of true parallel data to fine-tune the N M T t?s & N M T s?t models. This results in the creation of the filtered back-translated (FBT) models. We further fine-tune BT as well as FBT models on the BSD corpus. For the constrained setting, we train N M T s?t and N M T t?s models directly on the BSD corpus. We also experiment with fine-tuning    (Yang et al., 2019)  to obtain the sentence embeddings of S and T. Then cosine similarity is calculated on the obtained embeddings of S and T, and if the cosine score is below a certain threshold, we treat this pair as noisy. The threshold value is decided based on the cosine score on the entire monolingual data and its corresponding generated translations. We also utilize this filtering strategy to sample sentence pairs from the true parallel data. For this, we sort the entire true parallel data in decreasing order of similarity scores. Then we remove pairs that contain the text in the same language in the source and target side using Langid  (Lui and Baldwin, 2012)  library. We also remove pairs where the same text is present on the source and target side. Finally, we return the top n sentence pairs from the above data where n is the number of samples required from the true parallel data. 5 Experiments and Results 

 Data Preparation & Preprocessing We   (Neubig, 2011) , JESC (Japanese-English Subtitle Corpus)  (Pryzant et al., 2017) , Japanese-English Legal Parallel Corpus  (Neubig, 2011) , WikiMatrix  (Schwenk et al., 2019) , News Commentary 4 , Wiki Titles v2 4 , TED Talks  (Hochreiter and Schmidhuber, 1997)  and MTNT (Machine Translation of Noisy Text) parallel corpus  (Michel and Neubig, 2018) . While combining the datasets, we follow the train, validation, and test set as provided in the respective corpus and use it in a similar fashion in our combined dataset. We sample 3 million monolingual data from News Crawl 4 dataset to create pseudo parallel corpus. Along with these pseudo parallel data, we randomly sample the same amount of true parallel data from the open domain dataset. We combine and shuffle both the pseudo parallel and true parallel data. Finally, we utilize the BSD corpus provided by WAT 2020. This corpus is manually created and consists of Japanese-English business conversations. We use the provided training, development, and evaluation splits, which are described in Table  2 . We use different preprocessing rules for each translation direction based on our initial experimentation results as well as the findings from the literature. For Ja-En, we train & apply sentencepiece  (Kudo and Richardson, 2018)  model to tokenize the raw text into subwords with the vocabulary size of 32,000 for each language. For En-Ja, we first tokenize the raw text by KyTea and the Moses tok-enizer for Japanese and English, respectively. We also use Moses toolkits to truecase English words. We then further train & apply sentencepiece model to tokenize these words into subwords with the vocabulary size of 32,000 for each language. 

 Implementation Details Here, we describe a detailed setup of our experiments in both the constrained and unconstrained settings. For the unconstrained setting, we utilize Transformer-base  (Vaswani et al., 2017b) model  for training the open domain BASE models. The encoder and decoder consist of 6 layers, 8 attention heads, and the hidden size is kept to 512. We use Adam optimizer with an initial learning rate of 0.001 and dropout regularization, whose value is fixed at 0.3. We use Fairseq  (Ott et al., 2019)  to implement all our experiments. All the models are trained until the convergence with patience of five. Once the BASE models are trained, we use monolingual data to create pseudo parallel data and train the BT models. For filtering based on the sentence similarity, we use the MUSE model from the TensorFlow Hub library to obtain the sentence embeddings. For the constrained setting, we experiment with a Transformer-base model with two as well as three encoder and decoder layers for training on BSD corpus. We also experiment with fine-tuning the mBART model on the BSD corpus. 

 Results and Analysis This section discusses the results of our different experiments on both constrained and unconstrained settings. For the unconstrained setting, we first summarize the results of the BASE model, BT model, and the FBT model in both directions on four different publicly available test sets, including the BSD corpus in For the constrained setting, Table  4  presents the overall results. For the En ? Ja translation, the BLEU score using different Japanese tokenizers such as juman, kytea, and mecab are reported. For the Ja ? En direction, moses tokenizer is used for the evaluation. Although the ensemble model gave us better performance compared to the single model alone, but it is the mBART model whose fine-tuning on BSD corpus surpasses all other models by a large margin in both directions. Table  5  presents the unconstrained setting results obtained by fine-tuning the BASE, BT and FBT models on the BSD corpus. It also reports the results of ensembles formed by using different models. We can observe that the ensemble model comprising of fine-tuning BASE and FBT models gives us the best performance for the En ? Ja direction, whereas in the case of Ja ? En, ensemble model comprising of fine-tuning BASE and BT models achieves the highest BLEU score. Table  6  reports the human evaluation results in both the settings. 

 Conclusion We experimented with a variety of techniques in both constrained & unconstrained settings. For the constrained setting, fine-tuning mBART on the BSD corpus gave the best translation performance in both directions. Thus, mBART can be fine-tuned for MT tasks, especially for low resource language pairs. For the unconstrained scenario, the models trained & fine-tuned using the pseudo-parallel corpus showed the best overall translation performance. We also showed that by using a simple ensemble technique of averaging different model checkpoints, the translation performance could be further improvised. Table 2 : 2 BSD Corpus Statistics mBART (Liu et al., 2020)  model on the BSD corpus. We finally build several ensembles by averaging checkpoints of different trained models. Filtering Technique: We apply a naive filtering model based on sentence similarity to filter out noisy pseudo parallel data. Given a monolingual source sentence S, we obtain the corresponding translated sentence T using the trained N M T s?t model. We then apply MUSE (Multilingual Universal Sentence Encoder) 
