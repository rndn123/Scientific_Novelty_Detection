title
BERT Enhanced Neural Machine Translation and Sequence Tagging Model for Chinese Grammatical Error Diagnosis

abstract
This paper presents the UNIPUS-Flaubert team's hybrid system for the NLPTEA 2020 shared task of Chinese Grammatical Error Diagnosis (CGED). As a challenging NLP task, CGED has attracted increasing attention recently and has not yet fully benefited from the powerful pre-trained BERT-based models. We explore this by experimenting with three types of models. The position-tagging models and correction-tagging models are sequence tagging models fine-tuned on pre-trained BERTbased models, where the former focuses on detecting, positioning and classifying errors, and the latter aims at correcting errors. We also utilize rich representations from BERT-based models by transferring the BERT-fused models to the correction task, and further improve the performance by pre-training on a vast size of unsupervised synthetic data. To the best of our knowledge, we are the first to introduce and transfer the BERT-fused NMT model and sequence tagging model into the Chinese Grammatical Error Correction field. Our work achieved the second-highest F1 score at the detecting errors, the best F1 score at correction top1 subtask and the second-highest F1 score at correction top3 subtask.

Introduction Recently, the pre-trained language models such as BERT  (Devlin et al., 2019)  obtain state-of-the-art results on a wide range of natural language processing (NLP) tasks, such as text classification, reading comprehension, machine translation  (Zhu et al., 2020) , etc. The English Grammatical Error Correction (GEC) task also benefits from the pretrained language models. For example, in the work of  Kaneko et al. (2020) , they not only follow  Zhu et al. (2020)  to incorporate BERT into an Encoder-Decoder model for GEC, but also maximize the benefit by additionally training BERT on GEC corpora (BERT-fuse mask) or fine-tuning BERT as a GED model (BERT-fuse GED). Another route to improve the performance of GEC is using BERT as an encoder and incorporating it into a sequence tagging model  (Malmi et al., 2019; Awasthi et al., 2019; Omelianchuk et al., 2020) . In the Chinese NLP community, a variety of pre-trained Chinese language models have been proposed and publicly available  (Sun et al., 2019; Cui et al., 2019 Cui et al., , 2020 . Those models are proved to have a significant improvement in a variety of down-stream tasks, including reading comprehension, natural language inference, sentiment classification, etc. In this paper, we apply the state-of-the-art English GEC models to the CGED task. Our CGED system consists of three types of models. We propose the position-tagging model, which is a sequence tagging model with a BERT encoder, to concentrate on the error localization task. The output label consists of 8 types of tags and indicates the start and end of each error for the input sentence, but it will not tell us how to correct it in the case of S (word selection) and M (missing word) errors. The correction-tagging model  (Malmi et al., 2019; Awasthi et al., 2019; Omelianchuk et al., 2020)  concentrates on the error correction task, and the output label contains 8772 types of tags. The tags reveal the editing operations for each Chinese character, e.g. KEEP, DELETE, APPEND, and REPLACE. The APPEND tags (3788 in total) and REPLACE tags (4982 in total) cover most Chinese characters. The BERT-fused model  (Zhu et al., 2020)  is proposed for Neural Machine Translation (NMT) task and adaptively controls the interaction between representations from BERT and each layer of the Transformer  (Vaswani et al., 2017)  by using the attention mechanism.  (Kaneko et al., 2020)  transfers the BERT-fused model to the English GEC task and further advances it. Due to time limitations, we only follow the training settings in  (Zhu et al., 2020) . Besides, we perform unsupervised data augmentation by introducing synthetic errors on a large amount of error-free corpora, then pair synthetic and original sentences to pre-train Transformers  (Grundkiewicz et al., 2019) . This paper is organized as follows: Section 2 summarizes the recent developments in the field of CGED. Section 3 introduces the dataset we used to train the models, including human-annotated data and synthetic data. Section 4 is the overview of each component of our system, including BERTfused NMT, position-tagging model, correctiontagging model, and error annotation tool. Section 5 describes our training and ensemble process. Section 6 discusses the result of our models and Section 7 concludes the paper. 2 Related Work  Zhao et al. (2015)  used a statistical machine translation method to the CGED task and examined corpus-augmentation and explored alternative translation models including syntax-based and hierarchical phrase-based models.  Zheng et al. (2016) ,  and  Liao et al. (2017)  treat the CGED task as a sequence tagging problem to detect the grammatical errors.  Li and Qi (2018)  applied a policy gradient LSTM model to the CGED task.  Fu et al. (2018b)  built a CGED system based on a BiLSTM-CRF model and combined with rule-based templates to bring in grammatical knowledge.  Hu et al. (2018)  employed a sequence-to-sequence model and used pseudo data to pre-training the model.  designed a system for CGED which is composed of a BiLSTM-CRF model, an NMT model, and a statistical machine translation model to detect and correct the grammatical errors. A similar system  achieved a competitive result in NLPCC 2018 shared task.  Fu et al. (2018a)  also treated the CGED task as a translation problem and used character-based and sub-word based NMTs to correct the grammatical errors.  and Ren et al.  (2018)  introduced the convolutional sequence-to-sequence model into the CGED task. 

 Datasets Training data The datasets of the NLPTEA 2014?2018 & 2020 shared task of CGED are corpora composed of parallel sentences written by Chinese as a Foreign Language (CFL) learners and their corrections. The source sentences are selected from the essay section of the computer-based TOCFL (Test of Chinese as a Foreign Language) and written-based HSK (Pinyin of Hanyu Shuiping Kaoshi, Test of Chinese Level). Before 2016, there are only TOCFL data written in traditional Chinese. In the dataset of 2016, we have both TOCFL and HSK data. We use the opencc 1 package to convert the traditional Chinese to simplified Chinese for the TOCFL corpus. Since 2017, only HSK data are provided that are all written in simplified Chinese. The grammatical errors were manually annotated by native Chinese speakers. There are four kinds of errors: R (redundant word), M (missing word), S (word selection error), and W (word ordering error). Each error type has a different proportion in the corpus and each sentence may contain several errors. For example, in the CGED 2020 training set, W/S/R/M accounted for 7%, 42%, 23%, 28% of the total errors respectively. There are 2909 manually annotated errors in 1641 sentences, and only 2 sentences are error-free. We also collect several external datasets from NLPCC 2018 GEC 2 and other resources 3 . The NLPCC 2018 GEC data contains more than 700,000 sentences and each sentence may be correct or have one or more candidate corrections. 

 Synthetic data We train BERT-fused NMT models in pre-training mode and no pre-training mode. For pre-training mode, the model is pre-trained on a large amount of synthetic data  (Grundkiewicz et al., 2019) . The other models did not use the synthetic data. We first split each error-free sentence into words by a Chinese word segmentation tool 4 , and then randomly select several words for each sentence. The number of selected word is the product of a probability which is sampled from the normal distribution and the number of words in the sentence. For each selected word, one of the four operations including substitution, deletion, insertion, and transposition is performed with a probability of 0.5, 0.2, 0.2, 0.1, which simulates the proportions of S, M, R, W errors in the CGED data. For substitution, the selected word is replaced by a word that has a similar meaning, pronunciation,   (7, 8, S) X 7  (11, 11, M) X 5  (4, 4, R) X 3  (7, 7, M) X 1  (8, 8,   or shape. To simulate the confusion from similar meaning, we randomly choose a replacement from the following sources: (1) synonyms of the selected word 5 with a word similarity greater than 0.75; (2) a Chinese dictionary that we can search the word contain at least one character identical to the selected word; (3) a confusion dictionary consists of Japanese and Chinese word pairs that might be misused by Japanese learners. To mimic the confusion from similar pronunciation, we replace the selected word with a word that has the same pinyin. When introducing confusion from similar shapes, we define the similarity between two characters by their four-corner code 6 . 

 Voting of All Edits For deletion, we simply remove the selected word. For insertion, we add on a word randomly taken from a set after the selected word. The set consists of stop words 7 and redundant words from R errors in the past CGED dataset. For transpo-5 https://github.com/chatopera/Synonyms 6 http://code.web.idv.hk/misc/four.php? i=3 7 https://github.com/goto456/stopwords sition, we swap the selected word with the next word or with a random word in the sentence. We skip the named entities for substitution and deletion operations. After introducing the word-level error to each error-free sentence, we introduce character-level errors by similar methods. The corpora we used to generate synthetic data are the wiki2019zh (9.64 million sentences), the news2016zh (51.4 million sentences), the web-text2019zh (1.06 million sentences) 8 and the So-gouCA (0.94 million sentences) 9 . 

 System Overview Our system consists of a sequence labeling model concentrated on the error detection subtask, and two types of error correction models aimed at generating candidate corrections.   1 : Summary of the three training sets we constructed to train the BERT-NMT models at different stages. The number after the multiplication sign stands for how many times the data was oversampled. 

 Position-tagging Model The position-tagging model is a sequence tagging model aimed to locate grammatical errors. We use RoBERTa  10 as the model's encoder then fine-tune it during training. The output tags are generated by applying a softmax layer over the encoder's logits. Given a sequence of Chinese characters as input, the model predicts the label of each character. The output label consists of 8 types of tag, including O (correct), B-S (begin of S), I-S (middle of S), B-W (begin of W), I-W (middle of W), B-M (begin of M), B-R (begin of R), and I-R (middle of R). We extract the location and type of each error directly from the output labels. For S and M errors, the model can not give any candidate corrections. 

 BERT-fused NMT The BERT-fused NMT model proposed in  (Zhu et al., 2020)  aims at the NMT task, we transfer the original work to the correction subtask. The BERTfused NMT model is made up of two modules: the NMT module and the BERT module. Both modules take erroneous sentences as input. We start with training a Transformer from scratch until it converges. Then, we use the encoder and decoder of this Transformer to initialize the encoder and decoder of the NMT module. The BERT module is identical to a ready-made pre-trained BERT model. The way to fuse the NMT module and the BERT module is to feed the representations from the BERT module (i.e. the output of the last layer of the BERT module) to each layer of the NMT module. Taking the NMT encoder as an example, the BERT-encoder attention is introduced into each NMT encoder layer and processes the representations from the BERT module. The original selfattention of each NMT encoder layer still processes the representations from the previous NMT encoder layer. The output of the BERT-encoder attention and the original self-attention are further processed by the encoder layer's original feedforward network. The NMT decoder works similarly by introducing BERT-decoder attention to each NMT decoder layer. The parameters of the BERT-encoder attention and BERT-decoder attention are randomly initialized. During the training of the BERT-fused NMT model, the parameters of the BERT module are fixed. 

 Correction-tagging Model The correction-tagging model is a sequence tagging model 11 specific to the GEC task. The output labels consist of 8772 tags, which form a large edit space. We obtain corrections by iteratively feeding a sentence to the model, getting the edit operations of each character, then editing the sentence. To prepare the training data, we first convert the target sentence into a sequence of tags where each tag represents an edit operation on each source token. Take the following sentence pair as an example: Source: ? ? ? ? ? ? ? ? Target: ? ? ? ? ? ? ? ? We use the minimum edit distance algorithm to align the source tokens with the target tokens. For each mapping in alignment, we collect the edit steps from the source token to the target subsequence: ? KEEP ? KEEP & APPEND ? & APPEND ? ? KEEP ? DELETE ? KEEP ? DELETE ? KEEP ? KEEP Lastly, we leave only one edit for each source token, because in the training stage, each token can only have one label. In the case of the above example, ? KEEP ? APPEND ? ? KEEP ? DELETE ? KEEP ? DELETE ? KEEP ? KEEP The correction-tagging model is a pre-trained BERT-like Transformer encoder stacked with two linear layers and softmax layers on the top. In the inference stage, we tag and edit the sentence iteratively to obtain a fully corrected sentence. In each iteration, we apply the edits according to the output labels on the input sentence and send the edited sentence to the next iteration. 

 Error Classification For the BERT-fused NMT and correction-tagging model, the final output is a corrected sentence. To match with the official submission format, we align the target sentence with the source sentence to locate the start and end of the error and classify error types. In the field of GED, there is a widely used error annotation tool -errant  (Bryant et al., 2017) , which automatically annotates error type information of parallel English sentences. However, there is no such tool in the CGED task. We developed a simple rule-based annotation tool to locate the error and classify the error type. Our tool first segment the source and target sentence into words using Jieba 12 , then align the source and target words based on the minimum edit distance algorithm. In each mapping, if the blocks of source and target words are not the same, our tool judges this mapping as a grammatical error and determines the position and type of this error. However, even if we have the golden corrected sentence, there exists some ambiguity when localizing and classifying the error. For example, in the CGED 2020 training set, given the following sentence pairs: Source: ? ? ? Target: ? ? ? The official result is an S error starts from the 24th character and ends at the 27th character ("? ?") with a correction "?". But there may be many possible solutions that depend on the word segmentation. For example, if we split "? 12 https://github.com/fxsjy/jieba ?" into "?" and "?"?the result becomes an R error starts from the 24th character and ends at the 25th character and an S error starts from the 26th character and end the 27th character ("?") with a substitution "?". So, it is hard to locate and classify errors unambiguously due to different word segmentation rules. We tested our annotation tool on the CGED 2020 training data set, which are shown in Table  2 . Our error annotation tool loses some precision and recall at the detection, identification, and position subtasks when annotating the error information from parallel sentences. 

 Experiments 

 Position-tagging Model We trained the position-tagging models with two different combinations of CGED data and used the CGED 2016 test set as the development set. For each data combination, we tried serval models with different parameter initialization and training settings. When using CGED 2016 (HSK)?2018 & 2020 training set and 2017 test set as the training set, we get the best performance of the F1 score on detection and identification subtask on the CGED 2018 test set. When adding the TOCFL data from 2014 to 2016 to the training set, we get the best performance of the F1 score on the position subtask(see Table  3 ). Four position-tagging models (two models from each data combination) are used in ensemble modeling. 

 BERT-fused NMT We prepared several datasets to train the BERTfused NMT models. The first dataset is named Pre-Training data (PT data) consisting of synthetic sentences from the wiki2019zh corpus and the news2016zh corpus. The second dataset is the Manually Annotated data (MA data) which is composed of the CGED 2016?2018 training set, HSK, and NLPCC 2018 GEC data. We filtered out the errorfree sentences in HSK and NLPCC 2018 GEC dataset and oversampled the CGED data. The last dataset is the Augmented Manually-Annotated data (AMA data) consists of oversampled MA data and synthetic sentences from the text2019zh corpus and the SogouCA corpus. See details at Table  1    We use the fairseq  to train Transformers and the bert-nmt to train BERTfused models 13 . We use Transformer Base architecture to train all the Transformer models and reset the learning rate scheduler and optimizer parameters when training the fine-tuned Transformer and BERT-fused model. The parameters of the fine-tuned Transformer are used to initialize the encoder and decoder of the BERT-fused model. BERT-encoder attention and BERT-decoder attention are randomly initialized. We adopt the label smoothed cross-entropy as a loss function. The overall performance of each NMT model are listed in Table  4 . 

 Correction-tagging Model The training of the correction-tagging model is decomposed into two stages, which are inspired by  Omelianchuk et al. (2020) . The first stage uses all training sets from CGED 2014?2018 and NLPCC 2018 as the training set and the CGED 2020 training set as the development set. For NLPCC 2018 training set, we discard the sentence that is correct or has more than one correction. The second stage fine-tunes on 80% CGED 2020 training set and takes the other 20% as the development set. The difference between our training process and Omelianchuk et al. (  2020 ) is that we do not use synthetic data to pre-train the model. It will be investigated in future work that if a pre-training step on a large synthetic data set can improve the performance of the current model. We fine-tune four models using the BERT  (Devlin et al., 2019) , RoBERTa 14 , ELECTRA  (Clark et al., 2020)    15  , and XLNet    16  encoders. The learning rate for each model on the first stage is 2e-5, 2e-5, 4e-5, and 4e-5 respectively, and all 1e-5 on the second stage. In the first stage, we freeze the encoder's weights for the first epoch and set the learning rate to 1e-3. We adjust several hyperparameters after finetuning the models. The first is a threshold of the KEEP tag probability. If the KEEP tag probability is greater than the threshold, we will not change the source token. The other hyperparameters are the threshold of sentence-level minimum error probability and the number of iterations. These hyperparameters are tuned on the CGED 2018 test set to trade-off precision and recall. A simple ensemble of RoBERTa and BERT got an additional boost of the F1 score. We use BERT, RoBERTa, and their ensemble during the ensemble modeling. Both the BERT-fused NMT models and correction-tagging models are character-based instead of word-based for two reasons. First, the Chinese word segmentation tools are usually trained on grammatical sentences and will generate unexpected word segmentation results when applied to erroneous sentences. Second, word-based models use a larger vocabulary dictionary and more data is needed to obtain well-trained models, which conflicts with the fact that CGED is obviously a low-resource task. 

 Ensemble Modeling We adopt a weighted voting strategy inspired by . The output of position-tagging models provides the position and type of each error but lack corrections for S and M errors. The output of BERT-fused NMT models and correction-tagging models are corrected sentences and are converted into the official submission format using our annotation tool in Section 4.4. First, we omit the corrections for S and M errors temporarily and vote to determine the result of the position and type of all the errors. We accept an error proposal only if it gets the votes more than a threshold. A sentence is treated as correct if all its error proposals are not accepted. Then, we fill  the corrections. For each accepted S and M error, we rank the candidate corrections from the BERTfused NMT models and correction-tagging models according to votes. We take the first three candidates as the final corrections. A demonstration of our ensemble strategy is showed in Figure  1 . Each group of models has different weights during voting. All the thresholds and weights are tuned on the CGED 2018 test set using grid search, aiming at obtaining the best F1 score in the correction top1 subtask. The official evaluation of our three submissions are described in Table  5 . Run 1 got 1st place in the correction top1 subtask and 2nd place in the correction top3 subtask. The difference between Run 1 and Run 2 is that the hyperparameter of n-best in BERT-fused NMT models is set to 1 and 8 respectively. For Run 2 (n-best is 8), each BERT-fused NMT model generates 8 candidate sentences and all take part in the voting. Run 3 tried a different ensemble modeling which mainly focused on improving recall and got the 2nd place at the detection subtask. 

 Discussion For the BERT-fused NMT models, the BERT-fused stage improves the F1 scores for both non-pretraining and pre-training mode (See Table  4 ). In the non-pre-training mode, fine-tuning on AMA data further improves the performance on the CGED 2020 test set. By comparing the Baseline Transformer at the non-pre-training mode with the Finetuned Transformer at the pre-training mode, we find a substantial improvement of the performance on both the CGED 2018 and 2020 test sets. This proves that the CGED task can benefit from pretraining on synthetic data. However, the best results of the non-pre-training mode surpass the pretraining mode unexpectedly after the BERT-fused stage. We will investigate the reason in the future work.  (Kaneko et al., 2020)    demonstrated that the GED task can help improve the performance of the GEC task. Due to time limitations, we did not try to combine the detection and correction processes in our system, which can be further improved in the future work. In the ensemble modeling, we found that FPR (False Positive Rate) decreased as the threshold in the voting stage increased. Our submissions did not rank high in the FPR subtask, since we focused on the detection and correction rather than the FPR subtask. Compared to the methods proposed in the NLPTEA 2018 shared task of CGED, our system greatly improves the F1 score on correction top1 and correction top3 subtask on the CGED 2018 test set. This advance mainly comes from: (1) we not only fully exploit the Transformer model for the correction subtask, but also comprehensively incorporate the power of pre-trained BERT-based models into every subtask of the CGED task; (2) the low-resource problem in the GEC task restricts the performance of NMT models  (Junczys-Dowmunt et al., 2018) , and we address this by utilizing the power of pre-trained BERT models and synthesizing extensive artificial data. 

 Conclusion In this work, we present our solutions to the NLPTEA 2020 shared task of CGED. Three kinds of models are used in our system: position-tagging models, BERT-fused NMT models and correctiontagging models. Our hybrid system achieved the second-highest F1 score in the detection subtask, the highest F1 score in the correction top1 subtask and the second-highest F1 score in the correction top3 subtask, which shows that the CGED task can benefit from the recent advances of pre-trained language models. Figure 1 : 1 Figure1: A demonstration of our hybrid system using a real sentence from the CGED 2020 test set. Each edit format (start, end, type, correction) stands for an error and its start position, end position, type and correction. Here, all groups of models have an equal weight 1 and the threshold is set to 7. 
