title
Neural Machine Translation Between Myanmar (Burmese) and Rakhine (Arakanese)

abstract
This work explores neural machine translation between Myanmar (Burmese) and Rakhine (Arakanese). Rakhine is a language closely related to Myanmar, often considered a dialect. We implemented three prominent neural machine translation (NMT) systems: recurrent neural networks (RNN), transformer, and convolutional neural networks (CNN). The systems were evaluated on a Myanmar-Rakhine parallel text corpus developed by us. In addition, two types of word segmentation schemes for word embeddings were studied: Word-BPE and Syllable-BPE segmentation. Our experimental results clearly show that the highest quality NMT and statistical machine translation (SMT) performances are obtained with Syllable-BPE segmentation for both types of translations. If we focus on NMT, we find that the transformer with Word-BPE segmentation outperforms CNN and RNN for both Myanmar-Rakhine and Rakhine-Myanmar translation. However, CNN with Syllable-BPE segmentation obtains a higher score than the RNN and transformer.

Introduction The Myanmar language includes a number of mutually intelligible Myanmar dialects, with a largely uniform standard dialect used by most Myanmar standard speakers. Speakers of the standard Myanmar may find the dialects hard to follow. The alternative phonology, morphology, and regional vocabulary cause some problems in communication. Machine translation (MT) has so far neglected the importance of properly handling the spelling, lexical, and grammar divergences among language varieties. In the Republic of the Union of Myanmar, there are many ethnical groups, and dialectal varieties exist within the standard Myanmar language. To address this problem, we are developing a Myanmar and Rakhine dialectal corpus with monolingual and parallel text. We conducted statistical machine translation (SMT) experiments and obtained results similar to previous research  (Oo et al., 2018) . Deep learning revolution brings rapid and dramatic change to the field of machine translation. The main reason for moving from SMT to neural machine translation (NMT) is that it achieved the fluency of translation that was a huge step forward compared with the previous models. In a trend that carries over from SMT, the strongest NMT systems benefit from subtle architecture modifications and hyperparameter tuning. NMT models have advanced the state of the art by building a single neural network that can learn representations better  (Sutskever et al., 2014a) . Other authors  (Rikters et al., 2018)  conducted experiments with different NMTs for less-resourced and morphologically rich languages, such as Estonian and Russian. They compared the multi-way model performance to one-way model performance, by using different NMT architectures that allow achieving state-of-the-art translation. For the multiway model trained using the transformer network architecture, the reported improvement over the baseline methods was +3.27 bilingual evaluation understudy (BLEU) points.  (Honnet et al., 2017)  proposed solutions for the machine translation of a family of dialects, Swiss German, for which parallel corpora are scarcee. The authors presented three strategies for normalizing Swiss German input to address the regional and spelling diversity. The results show that character-based neural machine translation was the most promising strategy for text normalization and that in combination with phrase-based statistical machine translation it achieved 36% BLEU score. In their study, NMT outperformed SMT. In our study, we performed the first comparative NMT analysis of Myanmar dialectal language with three prominent architectures: recurrent neural network (RNN), convolutional neural network (CNN), and transformer. We investigated the translation quality of the corresponding hyper-parameters (batch size, learning rate, cell type, and activation function) in machine translation between the standard Myanmar and national varieties of the same group of languages. In addition, we used two types of segmentation schemes: word byte pair encoding (Word-BPE) segmentation and syllable byte pair encoding (Syllable-BPE) segmentation. We compared the performance of this method to SMT and NMT experiments with the RNN, transformer, and CNN. We found that the transformer with Word-BPE segmentation outperformed both CNN and RNN for both Myanmar-Rakhine and Myanmar-Rakhine translations. We also found that CNN with Syllable-BPE segmentation obtained a higher score compared with RNN and the transformer. 

 Rakhine Language Rakhine (Arakanese) is one of the eight national ethnic groups in the Republic of the Union of Myanmar. The Arakan was officially altered to "Rakhine" in 1989 and is located on a narrow coastal strip on the west of Myanmar, 300 miles long and 50 to 20 miles wide. The total population in all countries is nearly 3 million. The Rakhine language has been studied by researchers. L.F-Taylor's "The Dialects of Burmese" described comparative pronunciation, sentence construction, and grammar usage in Rakhine, Dawei, In-tha, Taung-yoe, Danu, and Yae. Professor Denise Bernot, in "The vowel system of Arakanese and Tavoyan," mainly emphasized the vowels of standard Myanmar and Tavoyan (Dawei) in 1965. In "Three Burmese Dialects" (1969), the linguist John Okell studied the spoken language of Myanmar, Dawei, and In-tha: specifically, usage of grammar and vowel differences  (OKELL, 1995) . Although the Rakhine language used the script as Arakanese or Rakkhawanna Akkhara before at least the 8th century A.D., the current Rakhine script is nearly the same as the Myanmar script. Generally, the Arakanese language is mutually intelligible with the Myanmar language and has the same word order  (namely, subject-object-verb (SOV)   

 Difference between the Rakhine and standard Myanmar language The Rakhine language is a largely monosyllabic and analytic language, with a SOV word order, and it uses the Myanmar script. It is considered by some to be a dialect of the Myanmar language, though it differs significantly from the standard Myanmar language in its vocabulary and includes loan words from Bengali, Hindi, and English. Compared with the Myanmar language, the speech of the Rakhine language is likely to be closer to the written form. The Rakhine language notably retains an /r/ sound that has become /j/ in the Myanmar language. Rakhine speakers pronounce the medial " ?" as "Yapint" (i.e., /j/ sound) and the medial "?" as "Rayit" (i.e., /r/ sound). Moreover, Myanmar vowel "?" (/e/ sound) is pronounced as "? " (/i/ sound) in Rakhine language. Thus, for example, the word "dog" in the Myanmar language is written as "? ?" (Khwe), and in the Rakhine language it is written as "? ? ?" (khwii). Similarly, Rakhine pronounce "?" (/e:/) for Myanmar pronunciation of " ? " (/ai/) syllable. Thus, Myanmar word "? ? ?" (peh-hinn) (pea curry in English) is pronounced "? ?" (pay-hinn) in the Rakhine language. Some Pali words are also used in the Rakhine language. For example, the word "guest" of Myanmar monks "? ?" (Agantu) is used in normal speech of Rakhine and it is similar to the normal Myanmar word "guest," "? ? " (Ai thay). In summary, the most significant differences between the Rakhine and Myanmar languages are in their pronunciation and vocabulary; there are no grammatical differences. 

 Segmentation 

 Word Segmentation In both Myanmar and Rakhine texts, spaces are used to separate the phrases for easier reading. The spaces are not strictly necessary and are rarely used in short sentences. There are no clear rules for using spaces. Thus, spaces may (or may not) be inserted between words, phrases, and even between root words and their affixes. Although Myanmar sentences of ASEAN-MT corpus  (Boonkwan and Supnithi, 2013)  are already segmented, we have to consider some rules for manual word segmentation of Rakhine sentences. We defined Rakhine "word" to be a meaningful unit. Affix, root word, and suffix (s) are separated such as "? ? ", "? ? ? ", "? ? ? ? ". Here, "?" ("eat" in English) is a root word and the others are suffixes for past and future tenses. As Myanmar language, Rakhine plural nouns are identified by the following particle. We added a space between the noun and the following particle: for example a Rakhine word "? ? ? ? ? " (ladies) is segmented as two words "? ? ? ?" and the particle "? ". In Rakhine grammar, particles describe the type of noun and are used after a number or text number. For example, a Rakhine word "? ? ? ? ? ? ? " ("two coins" in English) is segmented as "? ? ? ? ? ? ? ". In our manual word segmentation rules, compound nouns are considered as one word. Thus, a Rakhine compound word "? ?" + "? ? " ("money" + "bag" in English) is written as one word "? ? ? " ("wallet" in English). Rakhine adverb words such as "? " ("really" in English), "? " ("quickly" in English) are also considered as one word. The following is an example of word segmentation for a Rakhine sentence in our corpus, and the meaning is "Among the four air conditioners in our room, two are out of order." Unsegmented sentence: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? Segmented sentence: ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 

 Syllable Segmentation Generally, Myanmar words are composed of multiple syllables, and most of the syllables are composed of more than one character. Syllables are also the basic units for pronunciation of Myanmar words. If we only focus on consonant-based syllables, the structure of the syllable can be described with Backus normal form (BNF) as follows: Syllable := CM W [CK][D] Here, C stands for consonants, M for medials, V for vowels, K for vowel killer character, and D for diacritic characters. Myanmar syllable segmentation can be done with a rule-based approach  (Maung and Makami, 2008; Thu et al., 2013) , finite state automaton (FSA) (Hlaing, 2012), or regular expressions (RE) (https://github.com/ye-kyawthu/sylbreak). In our experiments, we used RE-based Myanmar syllable segmentation tool named "sylbreak."  (Sennrich et al., 2016)  proposed a method to enable open-vocabulary translation of rare and unknown words as a sequence of subword units representing BPE algorithm  (Gage, 1994) . The input is a monolingual corpus for a language (one side of the parallel training data, in our case) and starts with an initial vocabulary, the characters in the text corpus. The vocabulary is updated using an iterative greedy algorithm. In every iteration, the most frequent bigram (based on the current vocabulary) in the corpus is added to the vocabulary (the merge operation). The corpus is again encoded using the updated vocabulary, and this process is repeated for a predetermined number of merge operations. The number of merge operations is the only hyperparameter of the system that needs to be tuned. A new word can be segmented by looking up the learnt vocabulary. For instance, a new word "rocket," ? ? ? ? may be segmented as ?@@? ? ? ? ? after looking up the learnt vocabulary, assuming ? and ? ? ? ? ? as BPE units learnt during the training. 

 Byte-Pair-Encoding 

 Encoder-Decoder Models for NMT The core idea is to encode a variable-length input sequence of tokens into a sequence of vector representations, and then decode these representations into a sequence of output tokens. Formally, with a given sentence  p(y t |Y 1:t?1 , X; ?) = sof tmax(W o st +b o ), (1) where W o scales to the dimension of the target vocabulary V trg . 

 Stacked RNN with attention The encoder consists of a bidirectional RNN followed by a stack of unidirectional RNNs. An RNN at layer l produces a sequence of hidden states h l 1 . . . h l n : h l i = f enc (h l?1 i , h l i?1 ), (2) where f rnn is some non-linear function, such as a gated recurrent unit (GRU) or long shortterm memory (LSTM) cell, and h l?1 i is the output from the lower layer at step i. The bidirectional RNN on the lowest layer uses embeddings E S x i as input and concatenates the hidden states of a forward and a reverse RNN: h 0 i = [ ? ? h 0 i ; ? ? h 0 i ]. With deeper networks, learning becomes increasingly difficult  (Hochreiter et al., 2001; Pascanu et al., 2012) , and residual connections of the form  (He et al., 2016) . h l i = h l?1 i + f enc (h l?1 i , h l i?1 ) become essential The decoder consists of an RNN to predict one target word at a time through a state vector s: s t = f dec ([E T y t?1 ; st?1 ], s t?1 ), (3) where f dec is a multilayer RNN, s t?1 the previous state vector, and st?1 the sourcedependent attentional vector. Providing the attentional vector as an input to the first decoder layer is also called input feeding  (Luong et al., 2015) . The initial decoder hidden state is a non-linear transformation of the last encoder hidden state: s 0 = tanh(W init h n + b init ). The attentional vector st combines the decoder state with a context vector c t : st = tanh(W s[s t ; c t ]), (4) where c t is a weighted sum of encoder hidden states: c t = ? n i=1 ? ti h i . The attention vector ? t is computed by an attention network  (Bahdanau et al., 2014; Luong et al., 2015) : ? ti = softmax(score(s t , h i )) score(s, h) = v ? a tanh(W u s + W v h). (5) 

 Self-Attentional Transformer The transformer model  (Vaswani et al., 2017)  uses attention to replace recurrent dependencies, making the representation at time step i independent from the other time steps. This requires the explicit encoding of positional information in the sequence by adding fixed or learned positional embeddings to the embedding vectors. The encoder uses several identical blocks consisting of two core sublayers: self-attention and a feedforward network. The self-attention mechanism is a variation of the dot-product attention  (Luong et al., 2015)  generalized to three inputs: query matrix Q ? R n?d , key matrix K ? R n?d , and value V ? R n?d , where d denotes the number of hidden units.  (Vaswani et al., 2017)  further extend attention to multiple heads, allowing for focusing on different parts of the input. A single head u produces a context matrix C u = softmax ( QW Q u ( KW K u ) T ? d u ) VW V u , (6) where matrices W Q u , W K u , and W V u are in R d?du . The final context matrix is given by concatenating the heads, followed by a linear transformation: C = [C 1 ; . . . ; C h ]W O . The form in Equation 6 suggests parallel computation across all time steps in a single large matrix multiplication. Given a sequence of hidden states h i (or input embeddings), concatenated to H? R n?d , the encoder computes selfattention using Q = K = V = H. The second subnetwork of an encoder block is a feedforward network with ReLU activation defined as FFN(x) = max(0, xW 1 + b 1 )W 2 + b 2 , (7) which is also easily parallelizable across time steps. Each sublayer, self-attention and feedforward network, is followed by a postprocessing stack of dropout, layer normalization, and residual connection. The decoder uses the same self-attention and feedforward networks subnetworks. To maintain auto-regressiveness of the model, self-attention is masked out on future time steps according to  (Vaswani et al., 2017) . In addition to self-attention, a source attention layer, which uses the encoder hidden states as key and value inputs, is added. Given decoder hidden states S ? R m?s and the encoder hidden states of the final encoder layer H l , source attention is computed as in Equation  5 with Q = S, K = H l , V = H l . As in the encoder, each sublayer is followed by a postprocessing stack of dropout, layer normalization  (Ba et al., 2016) , and residual connection. 

 Fully Convolutional Models The convolutional model  (Gehring et al., 2017)  uses convolutional operations and also dispenses with recurrence. Hence, input embeddings are again augmented with explicit positional encodings. The convolutional encoder applies a set of (stacked) convolutions that are defined as h l i = v(W l [h l?1 i?k/2? ; . . . ; h l?1 i+?k/2? ] + b l ) + h l?1 i , ( 8 ) where v is a non-linearity such as a gated linear unit  (Gehring et al., 2017; Dauphin et al., 2016) , and W l ? R dcnn?kd are the convolutional filters. To increase the context window captured by the encoder architecture, multiple layers of convolutions are stacked. To maintain sequence length across multiple stacked convolutions, inputs are padded with zero vectors. The decoder is similar to the encoder but adds an attention mechanism to every layer. The output of the target side convolution s l * t = v(W l [s l?1 t?k+1 ; . . . ; sl?1 t ] + b l ) (9 ) is combined to form S * and then fed as an input to the attention mechanism of Equation 6 with a single attention head and Q = S * , K = H l , V = H l , resulting in a set of context vectors c t . The full decoder hidden state is a residual combination with the context such that sl t = s l * t + c t + sl?1 t ( 10 ) To avoid convolving over future time steps at time t, the input is padded to the left. 

 Experiments 

 Corpus Preparation and Statistics We used 18,373 Myanmar sentences (with no name entity tags) of the ASEAN-MT Parallel Corpus  (Boonkwan and Supnithi, 2013) , which is a parallel corpus in the travel domain. It contains six main categories: people (greeting, introduction, and communication), survival (transportation, accommodation, and finance), food (food, beverages, and restaurants), fun (recreation, traveling, shopping, and nightlife), resource (number, time, and accuracy), special needs (emergency and health). Manual translation into the Rakhine language was done by native Rakhine students from two Myanmar universities, and the translated corpus was checked by the editor of a Rakhine newspaper. Word segmentation for Rakhine was done manually, and there are exactly 123,018 words in total. We used 14,076 sentences for training, 2,485 sentences for development, and 1,812 sentences for evaluation. 

 Moses SMT system We used the Moses toolkit  (Koehn et al., 2007)  for training the operation sequence model (OSM) statistical machine translation systems. We did not consider phrase-based statistical machine translation (PBSMT) and hierarchical phrase-based statistical machine translation (HPBSMT), because the OSM approach achieved the highest BLEU  (Papineni et al., 2002)  and RIBES  (Isozaki et al., 2010)  scores among three approaches  (Oo et al., 2018)  for both Myanmar-Rakhine to Rakhine-Myanmar statistical machine translations. The word-segmented (i.e., Syllable-BPE and Word-BPE) source language was aligned with the word-segmented target language using GIZA++. The alignment was symmetrized by grow-diag-final and heuristic. The lexicalized reordering model was trained with the msd-bidirectional-fe option. We used KenLM  (Heafield, 2011)  for training the 5gram language model with modified Kneser-Ney discounting. Minimum error rate training (MERT)  (Och, 2003)  was used to tune the decoder parameters, and the decoding was done using the Moses decoder (version 2.1.1)  (Koehn et al., 2007) . We used the default settings of Moses for all experiments.  

 Batch 

 Framework for NMT An open-source sequence-to-sequence toolkit for NMT written in Python  (Hieber et al., 2017)  and built on Apache MXNET  (Chen et al., 2015) , the toolkit offers scalable training and inference for the three most prominent encoder-decoder architectures: attentional recurrent neural network  (Schwenk, 2012; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014b) , self-attentional transformers  (Vaswani et al., 2017) , and fully convolutional networks  (Gehring et al., 2017) . 

 Training Details We used the Sockeye toolkit, which is based on MXNet, to train NMT models. The initial learning rate is set to 0.0001. If the per-formance on the validation set has improved for 8 checkpoints, the learning rate is multiplied by 32 checkpoints. All the neural networks have eight layers. For RNN Seq2Seq, the encoder has one bi-directional LSTM and six stacked unidirectional LSTMs, and the encoder is a stack of eight unidirectional LSTMs. The size of hidden states is 512. We apply layer-normalization and label smoothing (0.1) in all models. We tie the source and target embeddings. The dropout rate of the embeddings and transformer blocks is set to (0.1). The dropout rate of RNNs is (0.2). The attention mechanism in the transformer has eight heads. We applied three different batch sizes (128, 256, and 512) for RNN, Transformer, and CNN network architectures. The learning rate varies from 0.0001 to 0.0005. Two memory cell types GRU and LSTM were used for the RNN and transformer. Moreover, two activation functions were applied to the CNN architecture. The comparison between Syllable-BPE and Word-BPE segmentation schemes was done for both SMT (i.e., OSM) and NMT  (RNN, Transformer, and CNN)  techniques. All experiments are run on NVIDIA Tesla K80 24GB GDDR5. We trained all models for the maximum number of epochs using the AdaGrad and adaptive moment estimation (Adam) optimizer. The BPE segmentation models were trained with a vocabulary of 8,000. 

 Evaluation We used automatic criteria to evaluate the machine translation output. The metric BLEU  (Papineni et al., 2002)  measures the adequacy of the translation between language pairs, such as Myanmar and English. The Higher BLEU scores are better. Before computing BLEU, the translations were decomposed into their constituent syllables to ensure that the results are cross-comparable. 

 Results and Discussion The BLEU score results for three NMT approaches (RNN, Transformer, and CNN) with three batch sizes  (128, 256, and 512)  for Syllable-BPE segmentation scheme are shown in Table  1 . Bold numbers indicate the highest BLEU score among different batch sizes. CNN achieved the highest BLEU scores for both Myanmar-Rakhine and Rakhine-Myanmar translations. However, the transformer architecture achieved the top BLEU scores for Word-BPE segmentation schemes for both Myanmar-Rakhine and Rakhine-Myanmar neural machine translations (see Table  2 ). From the experimental results of Table  1  and Table 2 , we noticed that RNN and Transformer NMT with Syllable-BPE have a decreased translation performance for batch size 512. Thus, we used batch size 256 for further experiments with the RNN and transformer architectures. The NMT performance of the RNN and transformer with Syllable-BPE segmentation schemes together with different learning rates (from 0.0001 to 0.0005) and two different memory cell types (GRU and LSTM) can be seen in Table  3 . From these BLEU scores of the RNN and transformer approaches, LSTM gave the highest NMT performance for both Myanmar-Rakhine dialect translation and vice versa. To observe the maximum translation performance of CNN architecture, we extended experiments by using two activation functions (ReLu and Soft-ReLu), two batch sizes (128 and 256), and five learning rates (from 0.0001 to 0.0005) (see Table  4 ). Here, bold numbers indicate the highest BLEU scores of each batch size. From these results, we can clearly see that Soft-ReLu achieved the highest BLEU scores for both Myanmar to Rakhine and Rakhine to Myanmar translations. We found that the training processes with learning rate 0.0004 and 0.0005 were stopped for the batch size 128 for both ReLu and Soft-ReLu activation functions. We also made a comparison between SMT and NMT, and the results can be seen in Table 5. In this study, we run only OSM approach for the SMT experiments based on the our previous SMT work  (Oo et al., 2018) . The Table  5  presents that although CNN achieved the top BLEU score (83.75) for Myanmar to Rakhine translation, OSM gave the best BLEU (84.36) score for Rakhine to Myanmar translation. Furthermore, we also found that Syllable-BPE segmentation scheme is working well for both SMT and NMT for Myanmar-Rakhine dialect language pair. As shown in the experimental results of Table 1 to Table  5 , our dialect NMT experiments give significantly higher BLEU scores than other SMT on different language pairs such as Myanmar-Chinese, Myanmar-German, Myanmar-Japanese, Myanmar-Malaysian, Myanmar-Korean, Myanmar-Spanish, Myanmar-Thai, Myanmar-Vietnamese  (Thu et al., 2016) , and also for NMT on Myanmar-English (Sin and Soe, 2018). As we discussed in Section 3, Rakhine and Myanmar have the same word order of SOV and also share a lot of vocabulary. For these reasons, we assume that both SMT and NMT systems reach a very high machine translation performance. 

 Conclusion This paper presents the first study of the neural machine translation between Standard Myanmar and Rakhine (a spoken Myanmar dialect). We implemented three prominent NMT systems: RNN, transformer, and CNN. The systems were evaluated on a Myanmar-Rakhine parallel text corpus that we are developing. We also investigated two types of segmentation schemes (Word-BPE segmentation and Syllable-BPE segmentation). Our results clearly show that the highest performance of SMT and NMT was obtained with Syllable-BPE segmentation for both Myanmar-Rakhine and Rakhine-Myanmar translation. If we only focus on NMT, we find that the transformer with Word-BPE segmentation outperforms CNN and RNN for both Myanmar-Rakhine and Rakhine-Myanmar. We also find that CNN with syllable-BPE segmentation obtains a higher BLEU score compared with the RNN and transformer. In the near future, we plan to conduct a further study with a focus on NMT models with one more subword segmentation scheme SentencePiece for Myanmar-Rakhine NMT. Moreover, we intend to investigate SMT and NMT approaches for other Myanmar dialect languages, such as Myeik and Dawei. |X) as a target language sequence model, conditioning the probability of target word y t on target history Y 1:t?1 and source sentence X. Both x i and y t are integer IDs given by the source and target vocabulary mapping, V src and V trg , built from the training data tokens and represented as one-hot vectors x i ? {0, 1} |Vsrc| and y t ? {0, 1} |Vtrg| . These are embedded into e-dimensional vector representations (Vaswani et al., 2017)  E S x i and E T y t , using embedding matrices R e?|Vsrc| and E T ? R e?|Vtrg| . The target sequence is factorized as p(Y |X; ?) = ? m t=1 p(y t |Y 1:t?1 , X; ?). The model, parameterized by ?, consists of an encoder and decoder part, which vary depending on the model architecture. p(y t |Y 1:t?1 , X; ?) is parameterized via a softmax output layer over some decoder representations St : X = x 1 , ?, x n and target sentence Y = y 1 , ?, y m , an NMT system models p(Y 
