title
Measuring Immediate Adaptation Performance for Neural Machine Translation

abstract
Incremental domain adaptation, in which a system learns from the correct output for each input immediately after making its prediction for that input, can dramatically improve system performance for interactive machine translation. Users of interactive systems are sensitive to the speed of adaptation and how often a system repeats mistakes, despite being corrected. Adaptation is most commonly assessed using corpus-level BLEU-or TERderived metrics that do not explicitly take adaptation speed into account. We find that these metrics often do not capture immediate adaptation effects, such as zero-shot and oneshot learning of domain-specific lexical items. To this end, we propose new metrics that directly evaluate immediate adaptation performance for machine translation. We use these metrics to choose the most suitable adaptation method from a range of different adaptation techniques for neural machine translation systems.

Introduction Incremental domain adaptation, or online adaptation, has been shown to improve statistical machine translation and especially neural machine translation (NMT) systems significantly  (Turchi et al., 2017; Karimova et al., 2018)  (inter-alia). The natural use case is a computeraided translation (CAT) scenario, where a user and a machine translation system collaborate to translate a document. Each user translation is immediately used as a new training example to adapt the machine translation system to the specific document. Adaptation techniques for MT are typically evaluated by their corpus translation quality, but such evaluations may not capture prominent aspects of the user experience in a collaborative translation scenario. This paper focuses on directly measuring the speed of lexical acquisition for in-domain vocabulary. To that end, we propose three related metrics that are designed to reflect the responsiveness of adaptation. An ideal system would immediately acquire indomain lexical items upon observing their translations. Moreover, one might expect a neural system to generalize from one corrected translation to related terms. Once a user translates "bank" to German "Bank" (institution) instead of "Ufer" (shore) in a document, the system should also correctly translate "banks" to "Banken" instead of "Ufer" (the plural is identical to the singular in German) in future sentences. We measure both one-shot vocabulary acquisition for terms that have appeared once in a previous target sentence, as well as zeroshot vocabulary acquisition for terms that have not previously appeared. Our experimental evaluation shows some surprising results. Methods that appear to have comparable performance using corpus quality metrics such as BLEU can differ substantially in zero-shot and one-shot vocabulary acquisition. In addition, we find that fine-tuning a neural model tends to improve one-shot vocabulary recall while degrading zero-shot vocabulary recall. We evaluate several adaptation techniques on a range of online adaptation datasets. Fine tuning applied to all parameters in the NMT model maximizes one-shot acquisition, but shows a worrisome degradation in zero-shot recall. By contrast, fine tuning with group lasso regularization, a technique recently proposed to improve the space efficiency of adapted models  (Wuebker et al., 2018) , achieves an appealing balance of zero-shot and one-shot vocabulary acquisition as well as high corpus-level translation quality. 

 Measuring Immediate Adaptation 

 Motivation For interactive, adaptive machine translation systems, perceived adaptation performance is a crucial property: An error in the machine translation output which needs to be corrected multiple times can cause frustration, and thus may compromise acceptance of the MT system by human users. A class of errors that are particularly salient are lexical choice errors for domain-specific lexical items. In the extreme, NMT systems using subword modeling  (Sennrich et al., 2015)  can generate "hallucinated" words-words that do not exist in the target language-which are especially irritating for users  (Lee et al., 2018; Koehn and Knowles, 2017) . Users of adaptive MT have a reasonable expectation that in-domain vocabulary will be translated correctly after the translation of a term or some related term has been corrected manually. Arguably, more subtle errors, referring to syntax, word order or more general semantics are less of a focus for immediate adaptation, as these types of errors are also harder to pinpoint and thus to evaluate 1  (Bentivogli et al., 2016) . Traditional metrics for evaluating machine translation outputs, e.g. BLEU and TER, in essence try to measure the similarity of a hypothesized translation to one or more reference translations, taking the full string into account. Due to significant improvements in MT quality with neural models  (Bentivogli et al., 2016)  (interalia), more specialized metrics, evaluating certain desired behaviors of systems become more useful for specific tasks. For example,  Wuebker et al. (2016)  show, that NMT models, while being better in most respects, still fall short in the handling of content words in comparison with phrase-based MT. This observation is also supported by  Bentivogli et al. (2016) , who show smaller gains for NMT for translation of nouns, an important category of content words. Another reason to isolate vocabulary acquisition as an evaluation criterion is that interactive translation often employs local adaptation via prefix-decoding  (Knowles and Koehn, 2016; Wuebker et al., 2016) , which can allow the system to recover syntactic structure or resolve local am-biguities when given a prefix, but may still suffer from poor handling of unknown or domainspecific vocabulary. In this work, we therefore focus on translation performance with respect to content words, setting word order and other aspects aside. 

 Metrics We propose three metrics: one to directly measure one-shot vocabulary acquisition, one to measure zero-shot vocabulary acquisition, and one to measure both. In all three, we measure the recall of target-language content words so that the metrics can be computed automatically by comparing translation hypotheses to reference translations without the use of models or word alignments 2 . We define content words as those words that are not included in a fixed stopword list, as used for example in query simplification for information retrieval. Such lists are typically compiled manually and are available for many languages.  3  For western languages, content words are mostly nouns, main verbs, adjectives or adverbs. For the i-th pair of source sentence and reference translation, i = 1, . . . , |G|, of an ordered test corpus G, we define two sets R 0,i and R 1,i that are a subset of the whole set of unique 4 content words (i.e. types) of the reference translation for i. R 0,i includes a word if its first occurrence in the test set is in the i-th reference of G, and R 1,i if its second occurrence in the test set is in the i-th reference of G. The union R 0,i ? R 1,i includes content words occurring for either the first or second time. To measure zero-shot adaptation in a given hypothesis H i , also represented as a set of its content words, we propose to evaluate the number of word types that were immediately translated correctly: R0 = |H i ? R 0,i | |R 0,i | . To measure one-shot adaptation, where the system correctly produces a content word after ob-2 In each of the data sets considered in this work, the average number of occurrences of content words ranges between 1.01 and 1.11 per sentence. We find this sufficiently close to 1 to evaluate in a bag-of-words fashion and not consider alignments. 3 For German we used the list available here: https://github.com/stopwords-iso. 4 All proposed metrics operate on the set-level, without clipping  (Papineni et al., 2002)  or alignment  (Banerjee and Lavie, 2005; Kothur et al., 2018) , as we have found this simplification effective. Figure  1 : Example for calculating R0, R1, and R0+1 on a corpus of two sentences. Content words are written in brackets, the corpus-level score is given below the per-segment scores. In the example, the denominator for R1 is 2 due to the two repeated words dog and bites in the reference. serving it exactly once, we propose: R1 = |H i ? R 1,i | |R 1,i | . This principle can be extended to define metrics Rk, k > 1 to allow more "slack" in the adaptation, but we leave that investigation to future work. Finally, we define a metric that measures both zero-and one-shot adaptation: R0+1 = |H i ? [R 0,i ? R 1,i ] | |R 0,i ? R 1,i | . All metrics can either be calculated for single sentences as described above, or for a full test corpus by summing over all sentences, e.g. for R0: |G| i=1 |H i ? R 0,i | |G| i=1 |R 0,i | . Figure  1  gives an example calculation of all three metrics across a two-sentence corpus. 

 Related Work An important line of related work is concerned with estimating the potential adaptability of a system given a source text only, the so-called repetition rate  (Cettolo et al., 2014) . The metric is inspired by BLEU, and uses a sliding window over the source text to count singleton N -grams. The modus operandi for our metrics is most similar to HTER  (Snover et al., 2006) , since we are also assuming a single, targeted reference translation 5 for evaluation. The introduction of NMT brought more aspects of translation quality evaluation into focus, such as discourse-level evaluation  (Bawden et al., 2017) , or very fine-grained evaluation of specific aspects of the translations  (Bentivogli et al., 2016) , highlighting the differences between phrase-based and NMT systems. Online adaptation for (neural) machine translation has been thoroughly explored using BLEU  (Turchi et al., 2017) , simulated keystroke and mouse action ratio  (Barrachina et al., 2009)  for effort estimation  (Peris and Casacuberta, 2018) , word prediction accuracy  (Wuebker et al., 2016) , and user studies  (Denkowski et al., 2014; Karimova et al., 2018 ) (all inter-alia). In  (Simianer et al., 2016)  immediate adaptation for hierarchical phrase-based MT is specifically investigated, but they also evaluate their systems using humantargeted BLEU and TER. Regularization for segment-wise continued training in NMT has been explored by  by means of knowledge distillation, and with the group lasso by  Wuebker et al. (2018) , as used in this paper. Most relevant to our work, in the context of document-level adaptation,  Kothur et al. (2018)  calculate accuracy for novel words based on an automatic word alignment. However, they do not focus on zero-and one-shot matches, but instead aggregate counts over the full corpus. 

 Online Adaptation NMT systems can be readily adapted by finetuning (also called continued training) with the same cross-entropy loss (L) as used for training the parameters of the baseline system, which also serves as the starting point for adaptation  (Luong and Manning, 2015) . Following  Turchi et al. (2017) , we perform learning from each example i using (stochastic) gradient descent, using the current source x i and reference translation y i as a batch of size 1: ? i ? ? i?1 ? ?L(? i?1 , x i , y i ). (1) Evaluation is carried out using simulated postediting  (Hardt and Elming, 2010) , first translating the source using the model with parameters ? i?1 , before performing the update described above with the now revealed reference translation. The machine translation system effectively only trains for a single iteration for any given data set. The na?ve approach, updating all parameters ? of the NMT model, while being effective, can be infeasible in certain settings 6 , since tens of millions of parameters are updated depending on the respective model. While some areas of a typical NMT model can be stored in a sparse fashion without loss (source-and target embeddings), large parts of the model cannot. We denote this type of adaptation as full. A light-weight alternative to adaptation of the full parameter set is to introduce a second bias term in the final output layer of the NMT model, which is trained in isolation, freezing the rest of the model  (Michel and Neubig, 2018) . This merely introduces a vector in the size of the output vocabulary. This method is referred to as bias. Another alternative is freezing parts of the model , for example determining a subset of parameters by performance on a held-out set  (Wuebker et al., 2018) . In our experiments we use two systems using this method, fixed and top, the former being a pre-determined fixed selection of parameters, and the latter being the topmost encoder and decoder layers in the Transformer NMT model  (Vaswani et al., 2017) . Finally, a data-driven alternative to the fixed freezing method was introduced to NMT by  Wuebker et al. (2018) , implementing tensor-wise 1 / 2 group lasso regularization, allowing the learning procedure to select a fixed number of parameters after each update. This setup is referred to as lasso. 

 Experiments 

 Neural Machine Translation Systems We adapt an English?German NMT system based on the Transformer architecture trained with an in-house NMT framework on about 100M bilingual sentence pairs. The model has six layers in the encoder, three layers in the decoder, each with eight attention heads with dimensionality 256, distinct input and output embeddings, and vocabulary sizes of around 40,000. The vocabularies are generated with byte-pair encoding  (Sennrich et al., 2015) . For adaptation we use a learning rate ? of 10 ?2 (for the bias adaptation a learn- ing rate of 1.0 is used), no dropout, and no labelsmoothing. We use a tensor-wise 2 normalization to 1.0 for all gradients (gradient clipping). Updates for a sentence pair are repeated until the perplexity on that sentence pair is ? 2.0, for a maximum of three repetitions. The fixed adaptation scheme, which involves selecting a subset of parameters on held-out data following  Wuebker et al. (2018) , uses about two million parameters excluding all embedding matrices, in addition to potentially the full source embeddings, but in practice this is limited to about 1M parameters. The top scheme only adapts the top layers for both encoder and decoder. For the lasso adaptation, we allow 1M parameters excluding the embeddings, for which we allow 1M parameters in total selected from all embedding matrices. This scheme also always includes the previously described second bias term in the final output layer. Since the proposed metrics operate on words, the machine translation outputs are first converted to full-form words using sentencepiece  (Kudo and Richardson, 2018) , then tokenized and truecased with the tokenizer and truecaser distributed with the Moses toolkit  (Koehn et al., 2007) . 

 Results Tables  1 and 2  show the performance of different adaptation techniques on the Autodesk dataset  (Zhechev, 2012) , a public post-editing software domain dataset for which incremental adaptation is known to provide large gains for corpus-level metrics. BLEU, sentence BLEU, and TER scores (Table  1 ) are similar for full adaptation, sparse adaptation with group lasso, and adaptation of a fixed subset of parameters. However (in lasso substantially outperforms the other methods in zero-shot (R0), and combined zero-and oneshot recall of content words (R0+1). Zero-shot recall is considerably degraded relative to the non-adapted baseline for both full and adaptation of a fixed subset of tensors (fixed and top). That is, terms never observed before during online training are translated correctly less often than they would be with an unadapted system, despite the data set's consistent domain. These approaches trade off long-term gains in BLEU and high one-shot recall for low zero-shot recall, which could be frustrating for users who may perceive the degradation in quality for terms appearing for the first time in a document. The lasso technique is the only one that shows an improvement in R0 over the baseline. However, lasso has considerably lower one-shot recall compared to the other adaptation methods, implying that it often must observe a translated term more than once to acquire it. Appendix A shows similar experiments for several other datasets. 

 Analysis For a better understanding of the results described in the previous section, we conduct an analysis varying the units of the proposed metrics, while focusing on full and lasso adaptation. For the first variant, only truly novel words are taken into account, i.e. words in the test set that do not appear in the training data. Results for these experiments are depicted in Table  3 . It is apparent that the findings of Table  2  are confirmed, and that relative differences are amplified. This can be explained by the reduced number of total occurrences considered, which is only 310 words in this data set. It is also important to note that all of these Method R0 R1 R0+1 baseline 27.1 40.7 29.9 full 26.1 63.0 33.8 lasso 31.9 53.1 36.3 words are made up of known subwords 7 , since our NMT system does not include a copying mechanism and is thus constrained to the target vocabulary. Further results using the raw subword output 8 of the MT systems are depicted in Table  4 : R0 for the lasso method is degraded only slightly below the baseline (-1%, compared to +2% for the regular metric), the findings for R1 and R0+1 remain the same as observed before. Compared to the results for novel words this indicates that the improvement in terms of R0 for lasso mostly come from learning new combinations of subwords. A discussion of the adaptation behavior over time, with exemplified differences between the metrics, can be found in Appendix B. 

 Conclusions To summarize: In some cases, the strong gains in corpus-level translation quality achieved by fine tuning an NMT model come at the expense of zero-shot recall of content words. This concerning impact of adaptation could affect practical user experience. Existing regularization methods mitigate this effect to some degree, but there may be more effective techniques for immediate adaptation that have yet to be developed. The proposed metrics R0, R1, and R0+1 are useful for measuring immediate adaptation performance, which is crucial in adaptive CAT systems.   Figure 2 : 2 Figure 2: Differences in cumulative scores for R0 (top left), R1 (top right), R0+1 (bottom left), and BLEU (bottom right) to the baseline system on the Autodesk test set for full and lasso adaptation. The peculiarities discussed in the running text are marked by solid vertical lines (at x = 751 and x = 774). 

 Table 1 : 1 Results on the Autodesk test set for traditional MT quality metrics. SBLEU refers to an average of sentence-wise BLEU scores as described by Nakov et al. (2012) . The best result in each column is denoted with bold font. Method BLEU SBLEU TER baseline 40.3 49.3 45.2 bias 40.4 49.5 45.0 full 47.0 55.9 44.0 lasso 46.3 54.3 42.6 fixed 47.1 55.5 41.0 top 43.2 54.0 49.5 

 Table 2 2 ), 

 Table 2 : 2 Results on the Autodesk test set for the proposed metrics R0, R1, and R0+1. 

 Table 3 : 3 Results on Autodesk data calculating the metrics only for truly novel content words, i.e. ones that do not occur in the training data. Method R0 R1 R0+1 baseline 44.1 48.1 45.5 full 40.4 54.6 45.4 lasso 43.7 51.7 46.5 

 Table 4 : 4 Results on Autodesk data calculating the metrics with subwords. 

 Table 5 : 5 BLEU, sentence-wise BLEU, TER, R0+1, R0, and R1 metrics for a number of data sets, comparing different adaptation methods as described in Section 4. Baseline results are given as absolute scores, results for adaptation are given as relative differences. Best viewed in color. User 1 BLEU SBLEU TER R0+1 R0 R1 baseline 35.7 55.2 52.4 44.3 42.8 50.3 bias 8 6 -4 -5 -5 -4 full 36 18 -22 -4 -7 6 lasso 38 18 -23 1 -1 8 fixed 34 18 -22 -6 -9 4 top 29 16 -17 -5 -8 4 User 2 BLEU SBLEU TER R0+1 R0 R1 baseline 35.5 56.2 51.0 43.6 41.0 51.2 bias 0 0 0 0 0 -1 full 0 5 5 -3 -5 4 lasso 6 6 -6 2 0 7 fixed -5 4 13 -4 -7 1 top -3 3 4 -5 -7 -2 Autodesk BLEU SBLEU TER R0+1 R0 R1 baseline 40.3 49.3 45.2 41.0 39.3 44.9 bias 0 0 0 0 0 1 full 17 13 -3 1 -9 22 lasso 15 10 -6 4 3 8 fixed 17 13 -9 0 -9 16 top 7 10 10 -2 -9 12 TED BLEU SBLEU TER R0+1 R0 R1 baseline 25.9 56.0 54.2 42.6 39.5 53.2 bias 1 0 0 0 0 0 full 0 1 1 -3 -6 3 lasso 4 2 -2 -1 -3 4 fixed -3 0 4 -4 -7 2 top -6 0 9 -2 -5 5 Patent BLEU SBLEU TER R0+1 R0 R1 baseline 53.5 62.1 31.7 51.8 49.7 57.3 bias 2 1 -2 0 0 0 full 3 2 -2 -2 -5 7 lasso 4 2 -4 0 -2 5 fixed 2 1 1 -4 -7 4 top 2 1 -1 -3 -5 2 

			 Some practitioners observed that these subtle errors become harder to spot due the improved fluency of NMT systems (Burchardt, 2017) . 

			 A reference translation which was produced from postediting output of the to-be-evaluated MT system. 

			 For example in setups where a large number of these adapted models need to be stored and transferred. 

			 The test set does not contain any unknown characters.8  Note that this includes all tokens, not just parts of content words. 

			 . The dip in the BLEU score at segment 752, observable for both adapted systems, depicting a relative degradation of about 10%, is due to a pathological repetition of a single character in the output of the adapted MT systems for this segment, which has a large impact on the score.The dip observed with R0 is also noticeable in BLEU, but much less pronounced at 4% relative for full and 2% relative for lasso. The dip in BLEU on the other hand is not noticeable with R0, R1, or R0+1.9  For each sentence i in the data set, the metrics for all systems are calculated up to the ith sentence. The difference for the adapted systems is then calculated by subtracting the baseline score.
