title
JUST System for WMT20 Chat Translation Task

abstract
Machine Translation (MT) is a sub-field of Artificial Intelligence and Natural Language Processing that investigates and studies the ways of automatically translating a text from one language to another. In this paper, we present the details of our submission to the WMT20 Chat Translation Task, which consists of two language directions, English?German and German?English. The major feature of our system is applying a pre-trained BERT embedding with a bidirectional recurrent neural network. Our system ensembles three models, each with different hyperparameters. Despite being trained on a very small corpus, our model produces surprisingly good results.

Introduction The language of chat texts is considered a common language where people are rarely paying attention to correct spelling. Therefore, using the traditional methods of Machine Translation (MT), like dictionaries, is insufficient  (Hern?ndez, 2009) . As deep learning (DL) models are becoming more evolved and complex, this motivates the natural language processing (NLP) community researchers to employ them for challenging tasks such as MT of informal language, such as what is used in chat. Techniques like contextual word embeddings and pre-trained DL models are becoming very common in natural language generation (NLG) tasks such as MT  (Kusner et al., 2015; Zou et al., 2013; Abdullah and Shaikh, 2018; Al-Bdour et al., 2019) . The Chat Translation Task is a new task in the Fifth Conference on Machine Translation (WMT20).  1  Translating chat text, specifically the chats of customer support, is a main and exciting task in the field of MT. This kind of tasks has not been widely considered in previous MT studies, 1 http://www.statmt.org/wmt20/chat-task.html mostly because of the absence of openly existing datasets. The target of this new Chat Translation Task is to translate the customer support chat text from English to German and vice versa. The essential goal of this task is to develop models that can translate conversational text and study the use of multilingual models. We take part in the WMT20 shared chat translation task in two language directions: English?German and German?English. In this paper, we discuss our submission for this task, which is based on the bidirectional recurrent neural networks (bi-RNN)  (Schuster and Paliwal, 1997)  and using the pre-trained BERT embedding, known as bert-base-multilingual-cased  (Devlin et al., 2018) . This paper is constructed as follows. In Section 2, the task and data descriptions are provided. Section 3 discusses our proposed model. Section 4 shows the experiments we conduct and their results. Finally, the Conclusion is in Section 5. 

 Task and Data Description The Chat Translation shared task of WMT20 offers participants the opportunity to address a challenging problem faced by many companies today as they expand their customer support units to multiple different languages. The shared task provides a dataset consisting of a set of conversations between agents and customers. The organizers supplied a corpus for the English-German language pair. Specifically, the task involves translating the chat text of an agent speaking English and a customer speaking German. We are asked to translate the agent's chat text from English to German, and the customer's from German to English. The dataset used for this shared task depends on the corpus of Taskmaster-1  (Byrne et al., 2019) , which has the English language, and it consists of dialogues in six fields. A small part of this dataset was chosen and translated to German. The shared task has been provided with train, development, and test sets in JSON format. Each chat in the data file has a specific structure. Table  1   Each conversation contains a speaker (who is either an agent or a customer), a source chat text, and a target chat text. For the test set file, we are asked to translate the source chat text to target depending on the speaker. If it is an agent, the translation is from English to German. Otherwise, the translation is from German to English. For evaluating the participating models, the task organizers employ both automatic metrics (BLEU  (Papineni et al., 2002)  and TER  (Snover et al., 2006) ) as well as human evaluation. 

 JUST System Our System follows the sequence of steps shown in Figure  1 . In the following subsections, we discuss each step in details. 

 Preprocessing Data For the dataset preprocessing, we first converted the files from JSON file, as given in the shared task, to text files, so we can work with them easily. The training, dev, and test sets are divided into two groups: one that contains the agent as the speaker (English?German) and one that contains the customer as the speaker (German?English).  

 Extracting Features After preparing the dataset and preprocessing it, we use the pre-trained BERT model to get the word em-beddings of the dataset. Specifically, we use Bertbase-multilingual-cased 2 to extract feature vectors of the dataset to be used in the training of our models. For each word in the sentence of the encoder side, we get a file containing the word's embedding. The same is done for the decoder side. 

 The System Architecture Our system is an adaptation of OpenNMT 3 , an open-source toolkit for neural machine translation (NMT)  (Klein et al., 2017) . It is created on the PyTorch framework  (Paszke et al., 2017) . After ensuring that the dataset is ready to be trained in our system, we feed our dataset to the bi-RNN with long short-term memory (LSTM) cells  (Hochreiter and Schmidhuber, 1997 ) and an attention mechanism  (Luong et al., 2015)  along with the word embeddings we extract from the dataset and trained everything jointly. For each different set of hyperparameters, we train the model separately. We save the best three models. Table  3  shows the different hyperparameters used for the three models as well some of the experiments that have been done using GloVe embedding  (Pennington et al., 2014)  + byte pair encoding (BPE)  (Sennrich et al., 2015)  with a vocabulary of 10K sub-word units (Experiment-1), GloVe + without BPE (Experiment-2), and the default model. The rest of the hyperparameters are left at their default value. We also experiment with the celebrated Transformer mode  (Vaswani et al., 2017) . However, this model results in very low BLEU scores when evaluated on the dev set. Moreover, it takes about four days to finish training in one experiment. So, we decide to exclude it from further consideration. 

 Model Ensembling Before the test set is released, we train different models using the training set and evaluate them using the dev set. After training our system, we choose the best three models and ensemble them to get the final output. 

 Results The results based on the dev set are show in For evaluation on the test set, we combine the train and dev dataset of each group into one file.  We train each group separately and then we ensemble the three models into one. This model is used to get the target of each sentence in the test set of each group. It is worth mentioning that we only use the small dataset provided with the shared task. Table  6  shows the results for the human evaluation between the human, best score and our model for the English?German scores. Table  7  shows the results we get in the shared task compared to the baseline and the best results. We can see that the agent BLEU score of our model is higher than the baseline, which is translating from English to German. On the other hand, the customer BLEU score for the baseline beat our model, which is translating from German to English. 

 Conclusion This work describes JUST's submission to the WMT20 chat translation task. For all two translation directions, English?German and German?English, we used the pre-trained BERT embedding with the bi-RNN. We trained one model with different hyperparameters and then ensembled to one final system to translate the test set provided by the shared task. At the end of this work, we find out that a simple NMT model with BERT embedding can achieve surprisingly good results even if it is trained on a very small corpus. Figure 1 : 1 Figure 1: Flowchart of our system. 
