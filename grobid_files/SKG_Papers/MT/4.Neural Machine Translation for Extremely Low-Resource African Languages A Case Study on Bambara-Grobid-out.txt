title
Neural Machine Translation for Extremely Low-Resource African Languages: A Case Study on Bambara

abstract
Low-resource languages present unique challenges to (neural) machine translation. We discuss the case of Bambara, a Mande language for which training data is scarce and requires significant amounts of pre-processing. More than the linguistic situation of Bambara itself, the socio-cultural context within which Bambara speakers live poses challenges for automated processing of this language. In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource machine translation (MT).

Introduction Underresourced languages, from a natural language processing (NLP) perspective, are those lacking the resources (large volumes of parallel bitexts) needed to support state-of-the-art performance on NLP problems like machine translation, automated speech recognition, or named entity recognition. Yet the vast majority of the world's languages-representing billions of native speakers worldwide-are underresourced. And the lack of available training data in such languages usually reflects a broader paucity of electronic information resources accessible to their speakers. For instance, there are over six million Wikipedia articles in English but fewer than sixty thousand in Swahili and fewer than seven hundred in Bambara, the vehicular and most widely-spoken native language of Mali that is the subject of this paper.  1  Consequently, only 53% of the worlds population have access to "encyclopedic knowledge" in their primary language, according to a 2014 study by Facebook.  2  MT technologies could help bridge this gap, and there is enormous interest in such applications, ironically enough, from speakers of the languages on which MT has thus far had the least success. There is also great potential for humanitarian response applications  (?ktem et al., 2020) . Fueled by data, advances in hardware technology, and deep neural models, machine translation (NMT) has advanced rapidly over the last ten years. Researchers are beginning to investigate the effectiveness of (NMT) low-resource languages, as in recent WMT 2019 and WMT 2020 tasks  (Barrault et al., 2019) , and in underresourced African languages. Most prominently, the Masakhane (? et al., 2020) community 3 , a grassroots initiative, has developed open-source NMT models for over 30 African languages on the base of the JW300 corpus  (Agi? and Vuli?, 2019) , a parallel corpus of religious texts. Since African languages cover a wide spectrum of linguistic phenomena and language families  (Heine and Nurse, 2000) , individual development of translations and resources for selected languages or language families are vital to drive the overall progress. Just within the last year, a number of dedicated studies have significantly improved the state of African NMT:  van Biljon et al. (2020)  analyzed the depth of Transformers specifically for low-resource translation of South-African languages, based on prior studies by  Martinus and Abbott (2019)  on the Autshumato corpus (Groenewald and du  Plooy, 2010) . Dossou and Emezue (2020) developed an MT model and compiled resources for translations between Fon and French,  Akinfaderin (2020)  modeled translations between English and Hausa,  Orife (2020)  for four languages of the Edoid language family, and Ahia and Ogueji (2020) investigated supervised vs. unsupervised NMT for Nigerian Pidgin. In this paper, we present the first parallel data set for machine translation of Bambara into and from English and French and the first benchmark results on machine translation to and from Bambara. We discuss challenges in working with low-resource languages and propose strategies to cope with data scarcity in low-resource MT. We discuss the sociocultural context of Bambara translation and its implications for model and data development. Finally, we analyze our best-performing neural models with a small-scale human evaluation study and give recommendations for future development. We find that the translation quality on our in-domain data set is acceptable, which gives hope for other languages that have previously fallen under the radar of MT development. We released our models and data upon publication  4  . Our evaluation setup may serve as benchmark for an extremely challenging translation task. 

 The Bambara Language Bambara is the first language of five million people and the second language of approximately ten million more. Most of its speakers are members of Bambara ethnic groups, who live throughout the African continent. Approximately 30-40 million people speak some language in the Mande family of languages, to which Bambara belongs  (Lewis et al., 2014) . Bambara is a tonal language with a rich morphology. Over the years, several competing writing systems have developed, however, as an historically predominately oral language, a majority of Bambara speakers have never been taught to read or write the standard form of the language. Many are incapable of reading or writing the language at all. The standardization of words and the coinage of new ones are still works in progress; this poses challenges to automated text processing. During Muslim expansion and French colonization, Arabic and French mixed with local languages, resulting in a lingua franca, e.g., Urban Bambara. Most of the existing Bambara resources are cultural (folk stories or news/topical) or come from social media or text messages, and these are a written in a melange of French, Bambara and Arabic. Consequently, corpora based on common Bambara usage must account for the code switching found in these mixtures. Most of these characteristics are shared with related languages, e.g., a subset of the Mande family of languages, where many languages are mutually intelligible. Thus, our hope is that our approach will be transferable to the other twelve official local languages of Mali, or to other African languages with a comparable socio-cultural and linguistic embedding, for example Wolof (non-Mande), which is comparable in terms of number of speakers, borrowings from Arabic and French influence, and oral traditions. The next section will provide more details on digital resources and describe the process of exploring and collecting data and choosing parallel corpora for the training of the NMT model. 

 Data Collection 

 Bambara Corpora We discovered that there has been no prior development of automatic translation of Bambara, despite a relatively large volume of research on the language  (Culy, 1985; Aplonova and Tyers, 2017; Aplonova, 2018) . As a pilot study for assessing the potential for automatic translation of Bambara,  crowdsourced a small set of written or oral translations from French to Bambara. Additional work was carried out exploring novel crowdsourcing strategies for data collection in Mali . The Corpus Bambara de R?f?rence  (Vydrin et al., 2011)  is the largest collection of electronic texts in Bambara. It includes scanned and textbased electronic formats. A number of parallel texts based on this data exist. For example, Vydrin (2018) analyzed Bambara's separable adjectives using this data. To survey the known available sources of parallel texts with Bambara, we consulted with a number of authorities on Bambara, including the Academie Malienne des Langues (AMALAN) in Mali and the Institut National des Langues et Civilisations Orientales (INALCO) in France, as well as a number of individual linguists and machine translation experts throughout the world. These two organisations play key roles in the definition and the promotion of a standard form of written Bambara through the collection and annotation of corpora, the publishing of dictionaries, and, in formulating recommendations for language policy in Mali. Our efforts uncovered several sources of parallel texts between Bambara and French and/or English that are listed in Table  5  in Appendix A. The table provides a rating of each of the identified resources, and the rationale why they were in-or excluded from our translation study. Ultimately, most of these resources proved either of very little or no practical use as sources of training data. Many did not actually contain aligned texts and some not even suitable monolingual text. A systematic problem was lack of adherence to the standardized Bambara orthography, due to it being a predominately oral language. This is also one of the reasons why our search for parallel data on the web generally did not yield many findscommonly being used in written form, Bambara is used even less on the web. For example, the Bambara Wikipedia contains currently 667 articles (compared to 6M for English), of which a large percentage are only stubs. Of the small number of full articles, most do not consistently employ the standard orthography of Bambara. A selection of those, however, was prepared to be used as monolingual data for MT data augmentation. Most African NMT studies have been based on the JW300 corpus  (Agi? and Vuli?, 2019) , e.g. most of the Masakhane benchmarks  (? et al., 2020) . JW300 only contains less than 200 sentences of Dyula, a closely related language to Bambara that it is mutually intelligible with. It might be useful for future cross-lingual studies  Goyal et al., 2020) , but in order to avoid interference between languages, we focus on Bambara data exclusively in this first study. The most promising for our NMT approach was a dictionary data set from SIL Mali 5 with examples of sentences used to demonstrate word usage in Spanish, French, English, and Bambara; and a tri-lingual health guide titled "Where there is no doctor.  6  " Detailed corpus statistics are listed in Table  1 . 

 Sentence Alignment The part of the dictionary that we are focusing on in this study, are the dictionary entries that consist of examples of Bambara expressions followed by their translations in French and in English. Most of these are single sentences, so there is sentence-tosentence alignment in the majority of cases. However, there remains a sufficient number of exceptions to render automated pairing impossible. Part of the problem lies in the unique linguistic and cultural elements of the bambaraphone environment; it is often not possible to meaningfully translate an expression in Bambara without giving an explanation of the context. The medical health guide is aligned by chapters, each of which is roughly aligned by paragraphs. But at the paragraph level there are too many exceptions for automated pairing to be feasible. Many of the bambaraphone-specific problems found in the dictionary dataset are present at the sentence level as well, particularly in explanations of concepts that can be succinctly expressed in English or French but for which Bambara lacks terminology and the bambaraphone environment lacks an equivalent physical or cultural context. Both datasets therefore required manual alignment by individuals fluent in written Bambara and either French or English. The annotators need to be able to exercise expert-level judgment on linguistic and, occasionally, medical questions. Access to such human resources was a major factor limiting the quantity of data we were able to align. Because of this, and since the dictionary data was more closely aligned at the sentence level and did not require as much domain knowledge as the medical dataset, we have thus far only used the dictionary dataset in our machine learning experiments. In order to facilitate this alignment, we imple-Figure  1 : The custom aligner we developed to manually align the dictionary data set. The controls are as follows: for each language, ">" goes to the next item and "<" goes to the previous item; for all languages, ">>>" goes to the next items and "<<<" goes to the previous items; "Aligned B-F-E" saves to memory the alignment of all 3 languages; "Aligned B-F" saves to memory the alignment of Bambara and French items; "Aligned B-E" saves to memory the alignment of Bambara and English items; "Save" saves to a new file; "Continue Saving" continues saving the file created. mented an alignment interface, as shown in Figure  1 . It allows annotators to manually align sentences and to save those sentence pairs that another annotator considered properly aligned. In separate tasks, four annotators with a secondary school level understanding of Bambara performed alignment on French-Bambara and English-Bambara sentence pairs using the tool. 

 Preprocessing Before we could align these sentences, we needed to clean the retrieved dictionary entries. Below we give examples of cases we had to handle manually, going through the entire corpus line by line. 1. Only one language is represented: Discarded. 2. Ambiguous pronouns in Bambara: bam: "Bolok?ni kelen t? se ka b?l? ta." 7 One can imagine that translating these into French or English is difficult since there is no indicator of the correct choice.  (Johnson, 2018)  After: fr: "Un doigt ne peut pas prendre un caillou." bam: "Bolok?ni kelen t? se ka b?l? ta. " 

 Proverbs: Before: fr: "Proverbe: Une longue absence vaut mieux qu'un communiqu? (d'un d?c?s)." bam: "Fama ka sa ni k?munike ye." After: fr: "Une longue absence vaut mieux qu'un communiqu?." bam: "Fama ka sa ni k?munike ye." Data preparation, including alignment, proved to be about 60% of the overall time spent in person-hours on the experiment and required onthe-ground organisation and recruitment of skilled volunteers in Mali. 

 Parallel Data The final data set contains 2,146 parallel sentences of Bambara-French and 2,158 parallel sentences of Bambara-English-a very small data set for NMT compared to the massive state-of-the-art models that are trained on billions of sentences  (Arivazhagan et al., 2019) . We split the data randomly into training, validation, and test sets of 75%, 12.5% and 12.5% respectively. The training set is composed of 1611 sentences, the validation set of 268 sentences, the test set of 267 sentences for Bambara-French. The training set is composed of 1620 sentences, the validation set of 270 sentences, the test set of 268 sentences for Bambara-French. 

 Monolingual Data In addition to the translations, we obtained a dataset of 488 monolingual Bambara sentences, sampled from all articles in the Bambara Wikipedia and covering a range of topics, but with preponderance of articles related to Mali. We used this monolingual dataset for experiments in data augmentation through back-translation, described in Section 5.1. 

 NMT Development 

 Hyperparameters Our NMT is a transformer  (Vaswani et al., 2017)  of appropriate size for a relatively smaller training dataset  (van Biljon et al., 2020) . It has six layers with four attention heads for encoder and decoder, the transformer layer has a size of 1024, and the hidden layer size 256, the embeddings have 256 units. Embeddings and vocabularies are not shared across languages, but the softmax layer weights are tied to the output embedding weights. The model is implemented with the Joey NMT framework  (Kreutzer et al., 2019)  based on PyTorch  (Paszke et al., 2019) . Training runs for 120 epochs in batches of 1024 tokens each. The ADAM optimizer (Kingma and Ba, 2014) is used with a constant learning rate of 0.0004 to update model weights. This setting was found to be best to tune for highest BLEU, compared to decaying or warmup-cooldown learning rate scheduling. For regularization, we experimented with dropout and label smoothing  (Szegedy et al., 2016) . The best values were 0.1 for dropout and 0.2 for label smoothing across the board. For inference, beam search with width of 5 is used. The remaining hyperparameters are documented in the Joey NMT configuration files that we will provide with the code. 

 Segmentation There is no standard tokenizer for Bambara. Therefore, we simply apply whitespace tokenization for word-based NMT models and compute BLEU with "international" tokenization.  8  Of the 542 distinct word types in the Bambara dev set, 166 are not contained in the vocabulary (seen during training), 174 of 590 (29.5%) for the test split. For the French portion it is 243 of 713 (34.1%) for the dev split and 274 of 756 (36.2%) for the test split. Because of this large proportion of unknown words, we segment the data for both language pairs into subword units (byte pair encodings, BPE) (500 or 1000, separately) using subword-nmt 9  (Sennrich et al., 2016) , and apply BPE dropout to the training sets of both languages  (Provilkov et al., 2019) . We also experiment with character-level translation for French. 

 Results 

 Automatic Evaluation Segmentation. We evaluate the models' translations against reference translations on our heldout sets with corpus BLEU  (Papineni et al., 2002)  and ChrF  (Popovi?, 2015)  computed with Sacre-BLEU  (Post, 2018) .  10  Tables  2 and 3  show the results for French and English translations respectively. We find that word-and character-level modeling performs sub par compared to subword-level segmentation, which is in line with previous work on low-resource MT. The word-based model cannot resolve out-of-vocabulary words, and the characterlevel model struggled with word composition. With BPE, smaller subwords seem to perform slightly better than larger ones. BPE dropout  (Provilkov et al., 2019) , which was previously reported to be helpful for low-resource MT  (Richburg et al., 2020) , did not increase the quality of the results. We observe a trend towards higher scores for translations into Bambara than in the reverse direction, but this cross-lingual comparison has to be taken with a grain of salt, since it is influenced by source and target complexity  (Bugliarello et al., 2020) . Ambiguities on the Bambara side, such as the gender of pronouns illustrated in the example in Section 3.2, might make translation into English and French particularly difficult. Back-translation. In addition, we experimented with back-translated Wikipedia data: fine-tuning the original model on a combination of the backtranslated and original data, or training it from scratch on a combination of both, as e.g. in (Przystupa and Abdul-Mageed, 2019). However, this did not yield improvements over the original BPE model.  11  We speculate that the mismatch between domains hindered improvement. Indeed, we discovered that when we selected only short sentences from the Wikipedia data, we observed slightly better results, but they still did not outperform the baseline. This highlights the importance of general domain evaluation sets for future work, so that the effectiveness of leveraging additional out-ofdomain data can be measured. Multilingual modeling. Another promising approach for the improvement of extremely lowresourced languages is multilingual modeling  (Johnson et al., 2017) . In our case, we combined the tasks of translating from English and French into Bambara to strengthen the Bambara decoding abilities of the translation model, by concatenating the training data and learning joint BPE models. The training data is filtered so that it does not contain sentences from the evaluation sets of the respective other language. However, we do not find improvements over the bilingual model.  12  We would have expected improvements on translating into Bambara because of larger variation on the source side. However, one reason for not seeing this improvement might be that the sentences are relatively short, and fluency is not as much of an issue as in larger scale studies from previous works. 

 Human evaluation Two native Bambara speakers from Mali, coauthors of this paper, with both college-level French and English reading skills, evaluated a random sample of 21 of the test set translations from Bambara into French and 21 different test set translations from Bambara into English produced by the highest scoring models. Both native speakers received their basic education in Bambara and can read and write the language with fluency. Evaluation Schema. For translations that had only a limited correspondence to the source text, the evaluators were given a number of questions specific to the quality of the translations. For translations that rose to the level of being qualified as conveying most of the sense of the source text we asked for two numerical ratings: First, whether "most people" would be able to understand what the meaning of the source sentence from its translation. This was intended to cover translations where the technical accuracy might be low, as a BLEU score might measure, but that substantially conveyed the meaning of the source text. Second, whether the translation was a "good translation," meaning that exact word choices and structure hewed closely to the style and meaning of the source text. We chose to use relatively inexact terminology to describe the ranking criteria as we felt that, as non-professional translators, our evaluators would have difficulty using more technical guidance. Quantitative results. The evaluation schema and the results obtained are presented in Table  4 . The proportion of adequate words appears similar for English and French (rows 1 and 3). However, more English translations are judged as being adequate (rows 2 and 4). The overall percentage of translations that might be said to be useful (row 7), in that they convey at least the gist of the source sentence, is low at 36%, similar to the results obtained by automated methods. Examples. The following translation excited our admiration because the sentence is relatively complex and the translation is flawless: "Faranna tilebinyanfan tun tilalen b? Angil?w ni Tubabuw c?.", which gets translated to "L'Afrique de l'Ouest a ?t? divis?e entre les Anglais et les Fran?ais." ("West Africa was divided between the English and the French".) We also observe that the MT system often translated verb tense correctly, perhaps helped by the fact that verb tenses are extremely simple in Bambara. Some of the sentences that did not qualify as adequate translations nonetheless were instructive and demonstrated specific pattern recognition capabilities. For another example with the Bambara source "I b? gojogojo wa?", the model translates "Have you ever eaten you is your wife?". The word "gojogojo" is slang Bambara mainly used by youth playfully employing reduplication, onomatopoeia, and inspiration from a foreign language. While the term produced a nonsense sentence, the translation seems to carry some of the playfulness in the source sentence. We also notice that it does pick up the subject and uses interrogative word order. In Bambara the word order is the same word order used for a declarative sentence, "You" -"are" -"athletic", the sentence is made interrogative by the interrogative marker "wa". Looking at the following example, with Bambara source "Araba, ?kut?burukalo tile 8 san 2003" and translation "Apr?s la mort de 25 ans.", the failure of translating time expressions is surprising because we would have expected the system to have been trained on this pattern -it says, Wednesday, the month of October, the 8th day, the year 2003. The translation ("After the death of 25 years") is not close. Still, there are markers of time in both the original and translated sentences. Finally, for Bambara source "A nalolen don i n'a f? suruku." the translation says "Il met les mains dans ses poches." ("He put the hands in his pockets."), even though the correct translation would say "He is as crude as a hyena" (the word for crude does not translate very exactly into English). While the translation seems to have nothing to do with the source, it has the right subject and somehow seems a reasonable guess if you did not know the key words and gives a bit of the spirit of the source sentence. 

 Conclusion Our study constitutes the first attempt of modeling automatic translation for the extremely lowresource language of Bambara. We identified challenges for future work, such as the development of alignment tools for small-scale datasets, and the need for a general domain evaluation set. The current limitation of processing written text as input might furthermore benefit from the integration of spoken resources through speech recognition or speech translation, since Bambara is primarily spoken and the lack of standardization in writing complicates the creation of clean reference sets and consistent evaluation. 
