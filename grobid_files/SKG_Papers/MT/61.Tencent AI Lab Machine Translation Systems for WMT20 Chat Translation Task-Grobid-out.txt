title
Tencent AI Lab Machine Translation Systems for WMT20 Chat Translation Task

abstract
This paper describes the Tencent AI Lab's submission of the WMT 2020 shared task on chat translation in English?German. Our neural machine translation (NMT) systems are built on sentence-level, document-level, nonautoregressive (NAT) and pretrained models. We integrate a number of advanced techniques into our systems, including data selection, back/forward translation, larger batch learning, model ensemble, finetuning as well as system combination. Specifically, we proposed a hybrid data selection method to select highquality and in-domain sentences from out-ofdomain data. To better capture the source contexts, we exploit to augment NAT models with evolved cross-attention. Furthermore, we explore to transfer general knowledge from four different pre-training language models to the downstream translation task. In general, we present extensive experimental results for this new translation task. Among all the participants, our German?English primary system is ranked the second in terms of BLEU scores.

Introduction Although neural machine translation (NMT,  Bahdanau et al., 2015; Vaswani et al., 2017; Gehring et al., 2017)  has achieved great progress in recent years, translating conversational text is still a challenging task due to its inherent characteristics such as discourse awareness  (Maruf et al., 2018; , informality  Yang et al., 2019)  and personality  (Mirkin et al., 2015; Wang et al., 2016) . This is a task-oriented chat translation task  (Wang et al., 2017a; Farajian et al., 2020) , which aims to translating conversations between customers and agents. As a customer and an agent can respectively natively speak in German and English, the systems should translate the customer's utterances in German?English (De?En) while the agent's in German?English  (De?En) . In this paper, we present our submission to the novel task in De?En. We explore a breadth of established techniques for building Chat NMT systems. Specifically, our systems are based on the self-attention networks including both sentenceand document-level Transformer  (Vaswani et al., 2017; Wang et al., 2017b) . Besides, we investigated non-autoregressive translation (NAT) models augmented with our recently proposed evolved cross-attention  (Ding et al., 2020) . Technically, we used the most recent effective strategies including back/forward translation, data selection, domain adaptation, batch learning, finetuning, model ensemble and system combination. Particularly, we proposed a multi-feature data selection on large general-domain data. We not only use three language models (i.e. n-gram, Transformer and BERT based LMs) to filter low-quality sentences, but also employ feature decay algorithms  (FDA, Bic ?ici and Yuret, 2011)  to select domain-relevant data. In addition, we explore large batching  (Ott et al., 2018)  for this task and found that it can significantly outperform models with regular batching settings. To alleviate the low-resource problem, we employ large scale pre-training language models including monolingual BERT  (Devlin et al., 2019a) , bilingual XLM  (Conneau and Lample, 2019)  and multilingual mBART , of which knowledge can be transferred to chat translation models. 1 For better finetuning, we investigate homogenous and heterogeneous strategies (e.g. from sentence-level to document-level architectures). Simultaneously, we conduct fully-adapted data processing, model ensemble, back/forward translation and system combination. According to the official evaluation results, our systems in De?En and De?En are respectively ranked 2nd and 4th. 2 Furthermore, a number of advanced technologies reported in this paper are also adapted to our systems for biomedical translation  and news translation  tasks, which respectively achieve up to 1st and 2nd ranks in terms of BLEU scores. Though our empirical experiments, we gain some interesting findings on the chat translation task: 1. The presented data selection method improves the baseline model by up to +18.5 BLEU points. It helps a lot for small-scale data. 2. The large batch learning works well, which makes sentence-level NMT models perform the best among different NMT models. 3. Our proposed method can improve the NAT model by +0.6 BLEU point, which is still hard to beat its autoregressive teachers. 4. Document-level contexts are not useful on the chat translation task due to the limitation of contextual data. 5. It is difficult to transfer general knowledge from pretrained LMs to the downstream translation task. The rest of this paper is organized as follows. Section 2 introduces data statistics and our processing methods. In Section 3, we present our system with four different models: sentence-level NMT, document-level NMT, non-autoregressive NMT and NMT with pre-training LMs. Section 4 describes advanced technique integrated into our systems such as data selection and system combination. In Section 5, we reports ablation study and experimental results, which is followed by our conclusion in Section 6. 

 Data and Processing 

 Data The parallel data we use to train NMT systems consist of two parts: in-domain and out-of-domain corpora. The monolingual data used for back/forward translation are all out-of-domain. In-domain Parallel Data The small-scale indomain corpus is constructed by the task organizer.  3  The training, validation and test sets contain utterances in task-based dialogues with contextual information. We use both w/ and w/o context formats for training corresponding models. Although there exists duplicated/noisy sentences, we do not further filter such limited data. Out-of-domain Parallel Data The participants are allowed to use all the training data in the News shared task. 4 Thus, we combine six corpora including Euporal, ParaCrawl, CommonCrawl, TildeRapid, NewsCommentary and WikiMatrix. We first filter noisy sentence pairs (as detailed in Section 2.2) and simultaneously select parts of them as pseudo-in-domain data (as detailed in Section 4.1). Out-of-domain Monolingual Data Due to the high degree of sentence similarity within the TaskMaster monolingual corpus, 5 participants are not allowed to use the in-domain monolingual data to train their systems. Thus, we collect part of monolingual data in news domain, which consists of CommonCrawl and NewsCommentary. We conduct data selection (in Section 4.1) to select similar amount of sentences for back/forward translation. We do not use larger monolingual corpora (e.g. CommonCrawl) and leave this for future work. 

 Processing Pre-processing To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, Unicode conversation, punctuation normalization, tokenization and true-casing. After filtering steps, we generate subwords via Joint BPE  (Sennrich et al., 2016b)  with 32K merge operations. Filtering To improve the quality of data, we filter noisy sentence pairs according to their characteristics in terms of language identification, duplication, length, invalid string and edit distance. According to our observations, the filtering method can significantly reduce noise issues including misalignment, translation error, illegal characters, over-translation and under-translation. Post-processing After decoding, we process detokenizer and de-truecaser on system outputs. We found that the toolkit can not precisely deal with all cases. Thus, we automatically fix these bugs according to bilingual agreement. 

 Models We adopt four different model architectures namely: SENT, DOC, NAT and PRETRAIN. 

 Sentence-level NMT (SENT) We use standard TRANSFORMER models  (Vaswani et al., 2017)  with two customized settings. Due to data limitation, we use the small settings (SENT-S) 6 with regular batch size (4096 tokens ? 8 GPUs). Based on the base settings (SENT-B), 7 we also empirically adopt big batch learning  (Ott et al., 2018)  (16348 tokens ? 4 GPUs) with larger dropout (0.3). 

 Document-level NMT (DOC) To improve discourse properties for chat translation, we re-implement our document-level model  (Wang et al., 2017b)  on top of TRANS-FORMER. Its addition encoder reads N = 3 previous source sentences as history context and the representations are integrated into the standard NMT 6 https://github.com/pytorch/fairseq/ blob/master/fairseq/models/transformer. py#L947. 7 https://github.com/pytorch/fairseq/ blob/master/fairseq/models/transformer. py#L902. S>,?@-1 $>,?@-2 $>,?@-3 N-best N-best N-best Token or sentence level System Combination Source Sentence 

 Hypothesis Figure 1: The simplified system combination process, into which we feed each system/model with the source sentence, in turn obtain corresponding n-best result. After pooling all system results, we can perform the tokenlevel or sentence-level system combination decoding and obtain the final hypothesis. for translating the current sentence. The other configures are same as SENT with small settings. 

 Non-autoregressive NMT (NAT) Different from autoregressive NMT models that generate each target word conditioned on previously generated ones, NAT models break the autoregressive factorization and produce target words in parallel  (Gu et al., 2018) . Although NAT is proposed to speed up the inference, we exploit it to alleviate sequential error accumulation and improve the diversity in conversational translation. To adequately capture the source contexts, we proposed evolved cross-attention for NAT decoder by modeling the local and global attention simultaneously  (Ding et al., 2020) . Accordingly, we implement our method based on the advanced MaskPredict model  (Ghazvininejad et al., 2019)  8 , which uses the conditional mask LM  (Devlin et al., 2019a)  to iteratively generate the target sequence from the masked input. 

 Pretraining NMT (PRETRAIN) To transfer the general knowledge to chat translation models, we explore to initialize (part of) model parameters with different pretrained language/generation models.  Li et al. (2019)  showed  that large scale generative pretraining could be used to initialize the the document-level translation model by concatenating the current sentence and its context. We follow their work to build the BERT?DOC model. Furthermore,  Conneau and Lample (2019)  proposed to directly train a novel cross-lingual pretraining language model (XLM) to facilitate translation task. Accordingly, we adopt XLM pretrained model 9 to sentence-level NMT (XLM?SENT). More recently,  proposed a sequence-to-sequence denoising autoencoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. As they showed promising results on document translation, we additionally conducted the experiment on Chat data after submitting our systems. 10 

 Approaches We integrated advanced techniques into our systems, including data selection, model ensemble, back/forward translation, larger batch learning, finetuning, and system combination. 

 Data Selection Inspired by  Ding and Tao (2019) , multi-feature language modelling can select high-quality data from a large monolingual or bilingual corpus. We present a four-feature selection criterion, which scoring each sentence by BERT LM  (Devlin et al., 2019b) , Transformer LM  (Bei et al., 2018) , N-gram LM  (Stolcke, 2002)   are ranked by a sum of the above feature scores, and we selected top-M instances as pseudo-in-domain data. According to our observations, the selected data can maintain both high-quality and in-domain properties. For BERT LMs, we exploit two models built by Google 11 and our Tencent AI Lab, which are trained on massive multilingual data. The Transformer LM is trained on all in-domain and out-ofdomain data via Marian. 12 Besides, we used FDA toolkit  13  to score domain relevance between indomain and out-of-domain data. 

 Checkpoint Average and Model Ensemble For each model, we stored the top-L checkpoints according to their BLEU scores (instead of PPL or training time) on validation set and generated a final checkpoint with averaged weights to avoid stochasticity. To combine different models (maybe different architectures), we further ensembled the averaged checkpoints in each model. In our preliminary experiments, we find that this hybrid combination method outperforms solely combining checkpoints or models in terms of robustness and effectiveness. 

 Finetuning We employ various finetuning strategies at different phases of training. For Sent-Out?Sent-In finetune (same architecture but different data), we first train a sentence-level model on large pseudo-indomain data and then continuously train it on small in-domain data. We apply similar strategy for Doc-Out?Doc-In finetuning, and the only difference is to use document-level data. and we use " /s " symbols as their pseudo contexts  (Kim et al., 2019; . Besides, we conduct Sent-Out?Doc-In finetuning (different architectures and data). Specifically, we first train a sentence-level model on pseudo-in-domain data and then use parts of corresponding parameters to warm-up a document-level model, which will be continuously trained on in-domain data. 

 Back/Forward Translation Following Section 2, we obtain processed monolingual data. For back translation (BT), we use the best backward translation model to translate from target to source side and produce the synthetic corpus, which is used to enhance the autoregressive NMT models  (Sennrich et al., 2016a) . About forward translation (FT), we employ forward translation model to perform sequence distillation for NAT models  (Kim and Rush, 2016)  . 

 System Combination As shown in Figure  1 , in order to take full advantages of different systems (Model 1 , Model 2 and Model 3 ), we explore both token-and sentencelevel combination strategies. 

 Token-level We perform token-level combination with confusion network. Concretely, our method follows Consensus Network Minimum Bayes Risk (ConMBR)  (Sim et al., 2007) , which can be modeled as E ConM BR = argmin E L(E , E con ), where E con was obtained as backbone through performing consensus network decoding.  

 Sentence-level 

 Experimental Results Unless otherwise specified, reported BLEU scores are calculated based on combined and tokenized validation set by muti-bleu.perl, which is different from the official evaluation method. 

 Ablation Study Table  2  investigates effects of different settings on translation quality. We then apply the best hyperparameters to the models in Section 4 if applicable. Effects of Model Average and Ensemble Following Section 4.2, we averaged top-L checkpoints in SENT-B model and found that it performs best when L = 5. We followed the same operation for SENT-S model and then combined two best averaged models (one from SENT-B and the other one from SENT-S) via ensemble method. As shown in respectively. As seen, it obtains the best performance when using larger beam size (e.g. 8 or 16). The length penalty prefers around 1.0 because En and De belong to similar language family. 

 Main Results This section mainly reports translation qualities across different models and approaches (in Section 3 and 4). Finally we combine all of them via techniques integration and system combination. Data Selection Table  3  demonstrates the translation performances of SENT-BASE on different FDA variants. "+Bi-FDA" means using bilingual in-domain data as seed to select N sentences from out-of-domain data while "+Bi-FDA-XL" indicates using larger seed (iteratively add selected pseudoin-domain data to seed). "Mono" means that we only use source-side data for data selection. As seen, selected data from News domain can help to significantly improve translation quality. However, monolingual selection ("+Mono-FDA-XL") performs better than bilingual one ("+Bi-FDA-XL") and obtain the best BLEU score when N = 800K. context ("IN") by +5.47 BLEU points. We think there are two main reasons: 1) it lacks of large-scale training data with contextual information; 2) it is still unclear how the context help document translation, which is similar to the conclusion in previous work  (Kim et al., 2019; . About NAT models, our proposed approach can improve the vanilla NAT by +0.6 BLEU point, which are lower than those of autoregressive NMT models. About pre-training, we first explore SENT?DOC, which train a sentence-level model and then use part of their parameters to warm-up a document-level model. However, it is still lower than sentence-level models. The performance of BERT?DOC is much better than pure document-level models (56.01 vs. 51.93), which confirms our hypothesis that contextual data is limited in this task. Furthermore, the XLM?SENT can obtain 59.61 BLEU points which are closed to that of SENT-B. The MBART?SENT with CC25 pretrained model can achieve 57.48 BLEU points. We find that performances of most pretraining models can not beat that of the best sentence-level model. There are two possible reasons: 1) needing a number of tricks on finetuning; 2) it is not easy to transfer general knowledge to downstream specific tasks. may be that we only use a small part of large-scale monolingual data in news domain. In future work, we will exploit to select in-domain data from the larger monolingual corpus. 

 Models and Pretraining 

 Back/Forward Translation 

 Sub-domain Adaptation Modeling of all the speakers and language directions involved in the conversation, where each can be regarded as a different sub-domain. We conduct domain adaptation for different models to avoid performance corruption caused by domain shifting in Table  6 . Specifically, we finetune the well-trained models w/ and w/o domain adaptation, denoted as "-Dom." and "+Dom.", and evaluated them on domain combined and split valid sets. As seen, domain adaptation helps De?En more on valid set ("AVE." 61.27 vs. 61.48), while has no much benefits on En?De tasks. While evaluating on combined valid sets has a bias towards models without domain adaptation. We attribute this interesting phenomenon to personality and will explore it in the future. 

 System Combination In order to make full use of the optimal models obtained by the above strategies, we perform token-and sentence-level system combination simultaneously. For each strategy, we generate the n-best candidates to perform the combination. As shown in Table  7 , although tokenlevel combination preserves more lexical diversity and avoids the stochasticity, its translation performance is significantly weaker (averagely -2.19 BLEU points) than sentence-level combination. Encouragingly, the sentence-level combination outperforms token-level one on valid set, which is thus used in our final system (in Table  8 ). 

 Official Results The official automatic evaluation results of our submissions for WMT 2020 are presented in Table  8 . For the primary submission, the SYS-1 combines SENT (ensembled SENT-B and SENT-S), DOC and NAT models. As contrastive submissions, the SYS-2 combines SENT and XLM models while the SYS-3 combines SENT, DOC, NAT and XLM ones. Among participated teams, our primary systems achieve the second and the forth BLEU scores on De?En and En?De, respectively. 

 Conclusion The paper is a system description for the Tencent AI Lab's entry into the WMT2020 Chat Translation Task. We explore a breadth of established techniques for building chat translation systems. The paper includes numerous models making use of sentence-level, document-level, non-autoregressive NMT. It also investigates a number of advanced techniques including data selection, model ensemble, finetuing, back/forward translation and initialization using a pretrained LMs. We present extensive experimental results and hope that this work could help both MT researchers and industries to boost the performance of discourse-aware MT systems  (Hardmeier, 2014; Wang, 2019) . Table 1 shows the statistics of data in En-De. Data # Sents # Ave. Len. Parallel In-domain 13,845 10.3/10.1 Valid 1,902 10.3/10.2 Test 2,100 10.1/10.0 Out-of-domain 46,074,573 23.4/22.4 +filter 33,293,382 24.3/23.6 +select 1,000,000 21.4/20.9 Monolingual Out-of-domain De 58,044,806 28.0 +filter 56,508,715 27.1 +select 1,000,000 24.2 Out-of-domain En 34,209,709 17.2 +filter 32,823,301 16.6 +select 1,000,000 14.5 Table 1: Data statistics after pre-processing. Note that in-domain/valid/test set is speaker-ignored combined and their average lengths are counted based on En/De. 
