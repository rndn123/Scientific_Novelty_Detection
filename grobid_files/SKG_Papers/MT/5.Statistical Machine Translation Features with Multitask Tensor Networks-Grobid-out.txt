title
Statistical Machine Translation Features with Multitask Tensor Networks

abstract
We present a three-pronged approach to improving Statistical Machine Translation (SMT), building on recent success in the application of neural networks to SMT. First, we propose new features based on neural networks to model various nonlocal translation phenomena. Second, we augment the architecture of the neural network with tensor layers that capture important higher-order interaction among the network units. Third, we apply multitask learning to estimate the neural network parameters jointly. Each of our proposed methods results in significant improvements that are complementary. The overall improvement is +2.7 and +1.8 BLEU points for Arabic-English and Chinese-English translation over a state-of-the-art system that already includes neural network features.

Introduction Recent advances in applying Neural Networks to Statistical Machine Translation (SMT) have generally taken one of two approaches. They either develop neural network-based features that are used to score hypotheses generated from traditional translation grammars  (Sundermeyer et al., 2014; Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012) , or they implement the whole translation process as a single neural network  (Bahdanau et al., 2014; Sutskever et al., 2014) . The latter approach, sometimes referred to as Neural Machine Translation, attempts to overhaul SMT, while the former capitalizes on the strength of the current SMT paradigm and leverages the modeling power of neural networks to improve the scoring of hypotheses generated by phrase-based or hierarchical translation rules. This paper adopts the former approach, as n-best scores from state-of-the-art SMT systems often suggest that these systems can still be significantly improved with better features. We build on  (Devlin et al., 2014)  who proposed a simple yet powerful feedforward neural network model that estimates the translation probability conditioned on the target history and a large window of source word context. We take advantage of neural networks' ability to handle sparsity, and to infer useful abstract representations automatically. At the same time, we address the challenge of learning the large set of neural network parameters. In particular, ? We develop new Neural Network Features to model non-local translation phenomena related to word reordering. Large fullylexicalized contexts are used to model these phenomena effectively, making the use of neural networks essential. All of the features are useful individually, and their combination results in significant improvements (Section 2). ? We use a Tensor Neural Network Architecture  (Yu et al., 2012)  to automatically learn complex pairwise interactions between the network nodes. The introduction of the tensor hidden layer results in more powerful features with lower model perplexity and significantly improved MT performance for all of neural network features (Section 3). ? We apply Multitask Learning (MTL)  (Caruana, 1997)  to jointly train related neural network features by sharing parameters. This allows parameters learned for one feature to benefit the learning of the other features. This results in better trained models and achieves additional MT improvements (Section 4). We apply the resulting Multitask Tensor Networks to the new features and to existing ones, obtaining strong experimental results over the strongest previous results of  (Devlin et al., 2014) . We obtain improvements of +2.5 BLEU points for Arabic-English and +1.8 BLEU points for Chinese-English on the DARPA BOLT Web Forum condition. We also obtain improvements of +2.7 BLEU point for Arabic-English and +1.9 BLEU points for Chinese-English on the NIST Open12 test sets over the best previously published results in  (Devlin et al., 2014) . Both the tensor architecture and multitask learning are general techniques that are likely to benefit other neural network features. 

 New Non-Local SMT Features Existing SMT features typically focus on local information in the source sentence, in the target hypothesis, or both. For example, the n-gram language model (LM) predicts the next target word by using previously generated target words as context (local on target), while the lexical translation model (LTM) predicts the translation of a source word by taking into account surrounding source words as context (local on source). In this work, we focus on non-local translation phenomena that result from non-monotone reordering, where local context becomes non-local on the other side. We propose a new set of powerful MT features that are motivated by this simple idea. To facilitate the discussion, we categorize the features into hypothesis-enumerating features that estimates a probability for each generated target word (e.g., n-gram language model), and sourceenumerating features that estimates a probability for each source word (e.g., lexical translation). More concretely, we introduce a) Joint Model with Offset Source Context (JMO), a hypothesis enumerating feature that predicts the next target word the source context affiliated to the previous target words; and b) Translation Context Model (TCM), a source-enumerating feature that predicts the context of the translation of a source word rather than the translation itself. These two models extend pre-existing features: the Joint (language and translation) Model (JM) of  (Devlin et al., 2014)  and the LTM respectively respectively. We use a large lexicalized context for there features, making the choice of implementing them as neural networks essential. We also present neuralnetwork implementations of pre-existing sourceenumerating features: lexical translation, orien-tation and fertility models. We obtain additional gains from using tensor networks and multitask learning in the modeling and training of all the features. 

 Hypothesis-Enumerating Features As mentioned, hypothesis-enumerating features score each word in the hypothesis, typically by conditioning it on a context of n-1 previous target words as in the n-gram language model. One recent such model, the joint model of  Devlin et al. (2014)  achieves large improvements to the stateof-the-art SMT by using a large context window of 11 source words and 3 target words. The Joint Model with Offset Source Context (JMO) is an extension of the JM that uses the source words affiliated with the n-gram target history as context. The source contexts of JM and JMO overlap highly when the translation is monotone, but are complementary when the translation requires word reordering. 

 Joint Model with Offset Source Context Formally, JMO estimates the probability of the target hypothesis E conditioned on the source sentence F and a target-to-source affiliation A: P (E|F, A) ? |E| i=1 P (e i |e i?n+1 i?1 , C a i?k = f a i?k +m a i?k ?m ) where e i is the word being predicted; e i?n+1 i?1 is the string of n ? 1 previously generated words; C a i?k to the source context of m source words around f a i?k , the source word affiliated with e i?k . We refer to k as the offset parameter. We use the definition of word affiliation introduced in  Devlin et al. (2014) . When no source context is used, the model is equivalent to an n-gram language model, while an offset parameter of k = 0 reduces the model to the JM of  Devlin et al. (2014) . When k > 0, the JMO captures non-local context in the prediction of the next target word. More specifically, e i?k and e i , which are local on the target side, are affiliated to f a i?k and f a i which may be distant from each other on the source side due to non-monotone translation, even for k = 1. The offset model captures reordering constraints by encouraging the predicted target word e i to fit well with the previous affiliated source word f a i?k and its surrounding words. We implement a separate feature for each value of k, and later train them jointly via multitask learning. As our experiments in Section 5.2.1 confirm, the historyaffiliated source context results in stronger SMT improvement than just increasing the number of surrounding words in JM. Fig.  1  illustrates the difference between JMO and JM. Assuming n = 3 and m = 1, then JM estimates P (e 5 |e 4 , e 3 , C a 5 = {f 6 , f 7 , f 8 }). On the other hand, for k = 1 , JMO k=1 estimates P (e 5 |e 4 , e 3 , C a 4 = {f 8 , f 9 , f 10 }). f 9 f 5 . . . e 5 e 6 e 4 e 7 e 3 . . . . . . C 7 = C a 5 . . . f 6 f 7 f 8 Figure  1 : Example to illustrate features. f 9 5 is the source segment, e 7 3 is the corresponding translation and lines refer to the alignment. We show hypothesis-enumerating features that look at f 7 and source-enumerating features that look at e 5 . We surround the source words affiliated with e 5 and its n-gram history with a bracket, and surround the source words affiliated with the history of e 5 with squares. 

 Source-Enumerating Features Source-Enumerating Features iterate over words in the source sentence, including unaligned words, and assign it a score depending on what aspect of translation they are modeling. A sourceenumerating feature can be formulated as follows: P (E|F, A) ? |F | j=1 P (Y j |C j = f j+m j?m ) where C a j is the source context (similar to the hypothesis-enumerating features above) and Y j is the label being predicted by the feature. We first describe pre-existing source-enumerating features: the lexical translation model, the orientation model and the fertility model, and then discuss a new feature: Translation Context Model (TCM), which is an extension of the lexical translation model. 

 Pre-existing Features Lexical Translation model (LTM) estimates the probability of translating a source word f j to a tar-get word l(f j ) = e b j given a source context C j , b j ? B is the source-to-target word affiliation as defined in  (Devlin et al., 2014) . When f j is translated to more than one word, we arbitrarily keep the left-most one. The target word vocabulary V is extended with a N U LL token to accommodate unaligned source words. Orientation model (ORI) describes the probability of orientation of the translation of phrases surrounding a source word f j relative to its own translation. We follow  (Setiawan et al., 2013)  in modeling the orientation of the left and right phrases of f j with maximal orientation span (the longest neighboring phrase consistent with alignment), which we denote by L j and R j respectively. Thus, o( f j ) = o L j (f j ), o R j (f j ) , where o L j and o R j refer to the orientation of L j and R j respectively. For unaligned f j , we set o(f j ) = o L j (R j ), the orientation of R j with respect to L j . Fertility model (FM) models the probability that a source word f j generates ?(f j ) words in the hypothesis. Our implemented model only distinguishes between aligned and unaligned source words (i.e., ?(f j ) ? {0, 1}). The generalization of the model to account for multiple values of ?(f i ) is straightforward. 

 Translation Context Model As with JMO in Section 2.1.1, we aim to capture translation phenomena that appear local on the target hypothesis but non-local on the source side. Here, we do so by extending the LTM feature to predict not only the translated word e b j , but also its surrounding context. Formally, we model P (e b j +d |C j ) and implemented each as a separate neural network-based feature. Note that TCM is equivalent to the LTM when d = 0. Because of word reordering, a given hypothesis word in l(f j ) might not be affiliated with f j or even to the words in C j . TCM can model non-local information in this way. P (l(f j )|C j ), where l(f j ) = e b j ?d , ? ? ? , e b j , ? ? ? e b j +d 

 Combined Model Since the feature label is undefined for unaligned source words, we make the model hierarchical, based on whether the source word is aligned or not, and thus arrive at the following formulation: P (l(f j )) ? P (ori(f j )) ? P (?(f j )) = ? ? ? ? ? ? ? ? ? ? ? P (? p (f j ) = 0) ? P (o L j (R j )) P (? p (f j ) ? 1) ? +d d =?d P (e b j +d ) ?P (o L j (f j ), o R j (f j )) We dropped the common context (C j ) for readability. We reuse Fig.  1  to illustrate the sourceenumerating features. Assuming d = 1, the scores associated with f 7 are P (?(f 7 ) ? 1|C 7 ) for the FM; P (e 4 |C 7 ) ? P (e 5 |C 7 ) ? P (e 6 )|C 7 ) for the TCM; and P (o(f 7 ) = o L 7 (f 7 ) = RA, o R 7 (f 7 ) = RA ) for the ORI(RA refers to Reverse Adjacent). L 7 and R 7 (i.e. f 6 and f 9 8 respectively), the longest neighboring phrase of f 7 , are translated in reverse order and adjacent to e 5 . 

 Tensor Neural Networks The second part of this work improves SMT by improving the neural network architecture. Neural Networks derive their strength from their ability to learn a high-level representation of the input automatically from data. This high-level representation is typically constructed layer by layer through a weighted sum linear operation and a non-linear activation function. With sufficient training data, neural networks often achieve state-of-the-art performance on many tasks. This stands in sharp contrast to other algorithms that require tedious manual feature engineering. For the features presented in this paper, the context words are fed to the network network with minimal engineering. We further strengthen the network's ability to learn rich interactions between its units by introducing tensors in the hidden layers. The multiplicative property of the tensor bares a close resemblance to collocation of context words which are useful in many natural language processing tasks. In conventional feedforward neural networks, the output of hidden layer l is produced by multiplying the output vector from the previous layer with a weight matrix (W l ) and then applying the activation function ? to the product. Tensor Neural Networks generalize this formulation by using a tensor U l of order 3 for the weights. The output of node k in layer l is computed as follows: h l [k] = ? h l?1 ? U l [k] ? h T l?1 where U l [k], the k-th slice of U l , is a square ma- trix. In our implementation, we follow  (Yu et al., 2012; Hutchinson et al., 2013)  and use a low-rank approximation of U l [k] = Q l [k] ? R l [k] T , where Q l [k], R l [k] ? R n?r . The output of node k becomes: h l [k] = ? h l?1 ? Q l [k] ? R l [k] T ? h T l?1 In our experiments, we choose r = 1, and also apply the non-linear activation function ? distributively. We arrive at the following three equations for computing the hidden layer outputs (0 < l < L): v l = ? (h l?1 ? Q l ) v l = ? (h l?1 ? R l ) h l = v l ? v l where h l?1 is double-projected to v l and v l , and the two projections are merged using the Hadamard element-wise product operator ?. This formulation allows us to use the same infrastructure of the conventional neural networks by projecting the previous layer to two different spaces of the same dimensions, then multiplying them element-wise. The only component that is different from conventional feedforward neural networks is the multiplicative function, which is trivially differentiable with respect to the learnable parameters. Figure  3 (b) illustrates the tensor architecture for two hidden layers. The tensor network can learn collocation features more easily. For example, it can learn a collocation feature that is activated only if h l?1 [i] col- locates with h l?1 [j] by setting U l [k][i][j] to some positive number. This results in SMT improvements as we describe in Section 5. 

 Multitask Learning The third part of this paper addresses the challenge of effectively learning a large number of neural network parameters without overfitting. The challenge is even larger for tensor network since they practically doubles the number of parameters. In this section, we propose to apply Multitask Learning (MTL) to partially address this issue. We implement MTL as parameter sharing among the networks. This effectively reduces the number of parameters, and more importantly, it takes advantage of parameters learned for one feature to better Input h 1 h 2 Input h 1 ? v 1 v 1 h 2 ? v 2 v 2 Input h 1 ? v 1 v 1 h 1 2 ? v 1 2 v 1 2 h M 2 ? v M 2 v M 2 ? ? ? ? ? ? W 1 W 2 Q 1 R 1 R 2 Q 2 R 1 Q 1 Q 1 2 R 1 2 Q M 2 R M 2 (a) (b) (c) Output Output Task 1 Task M Figure 2: The network architecture for (a) a conventional feedforward neural network, (b) tensor hidden layers, and (c) multitask learning with M features that share the embedding and first hidden layers (t = 1). learn the parameters of the other features. Another way of looking at this is that MTL facilitates regularization through learning the other tasks. MTL is suitable for SMT features as they model different but closely related aspects of the same translation process. MTL has long been used by the wider machine learning community  (Caruana, 1997)  and more recently for natural language processing  (Collobert and Weston, 2008; Collobert et al., 2011) . The application of MTL to machine translation, however, has been much less restricted, which is rather surprising since SMT features arise from the same translation task and are naturally related. We apply MTL for the features described in Section 2. We design all the features to also share the same neural network architecture (in this case, the tensor architecture described in Section 3) and the same input, thus resulting in two large neural networks: one for the hypothesis-enumerating features and another for the source-enumerating ones. This simplifies the implementation of MTL. Using this setup, it is possible to vary the number of shared hidden layers t from 0 (only sharing the embedding layer) to L ? 1 (sharing all the layers except the output). Note that in principle MTL is applicable to other set of networks that have different architecture or even different input set. With MTL, the training procedure is the same as that of standard neural networks. We use the back propagation algorithm, and use as the loss function the product of likelihood of each feature 1 : Loss = i M j log (P (Y j (X i ))) where X i is the training sample and Y j is one of the M models trained. We use the sum of log likelihoods since we assume that the features are independent.  Fig. 3(c)  illustrates MTL between M models sharing the input embedding layer and the first hidden layer (t = 1) compared to the separatelytrained conventional feedforward neural network and tensor neural network. 

 Experiments We demonstrate the impact of our work with extensive MT experiments on Arabic-English and Chinese-English translation for the DARPA BOLT Web Forum and the NIST OpenMT12 conditions. 

 Baseline MT System We run our experiments using a state-of-the-art string-to-dependency hierarchical decoder  (Shen et al., 2010) . The baseline we use includes a set of powerful features as follow: ? Forward and backward rule probabilities We use the MADA-ARZ tokenizer  (Habash et al., 2013)  for Arabic word tokenization. For Chinese tokenization, we use a simple longest-matchfirst lexicon-based approach. We align the training data using GIZA++  (Och and Ney, 2003) . For tuning the weights of MT features including the new features, we use iterative k-best optimization with an ExpectedBLEU objective function  (Rosti et al., 2010) , and decode the test sets after 5 tuning iteration. We report the lower-cased BLEU and TER scores. 

 BOLT Discussion Forum The bulk of our experiments is on the BOLT Web Discussion Forum domain, which uses data collected by the LDC. The parallel training data consists of all of the high-quality NIST training corpora, plus an additional 3 million words of translated forum data. The tuning and test sets consist of roughly 5000 segments each, with 2 independent references for Arabic and 3 for Chinese. 

 Effects of New Features We first look at the effects of the proposed features compared to the baseline system. Table  1  summarizes the primary results of the Arabic-English and Chinese-English experiments for the BOLT condition. We show the experimental results related to hypothesis-enumerating features (HypEn) in rows S 2 -S 5 , those related to source-enumerating features (SrcEn) in rows S 6 -S 9 , and the combination of the two in row S 10 . For all the features, we set the source context length to m = 5 (11-word window). For JM and JMO, we set the target context length to n = 4. For the offset parameter k of JMO, we use values 1 to 3. For TCM, we model one word around the translation (d = 1). Larger values of d did not result in further gains. The baseline is comparable to the best results of  (Devlin et al., 2014) . In rows S 3 to S 5 , we incrementally add a model with different offset source context, from k = 1 to k = 3. For AR-EN, adding JMOs with different offset source context consistently yields positive effects in BLEU score, while in ZH-EN, it yields positive effects in TER score. Utilizing all offset source contexts "+JMO k?3 " (row S 5 ) yields around 0.9 BLEU point improvement in AR-EN and around 0.3 BLEU in ZH-EN compared to the baseline. The JMO consistently provides better improvement compared to a larger JM context (row S 2 ), validating our hypothesis that using offset source context captures important non-local context. Rows S 6 to S 9 present the improvements that result from implementing pre-existing sourceenumerating SMT features as neural networks, and highlight the contribution of our translation context model (TCM). This set of experiments is orthogonal to the HypEn experiments (rows S 2 -S 5 ). Each pre-existing model has a modest positive cumulative effect on both BLEU and TER. We see this result as further confirming the current trend of casting existing SMT features as neural network since our baseline already contains such features. The next row present the results of adding the translation context model, with one word surrounding the translation (d = 1). As shown, TCM yields a positive effect of around 0.5 BLEU and TER improvements in AR-EN and around 0.2 BLEU and TER improvements in ZH-EN. Separately, the set of source-enumerating features and the set of target-enumerating features produce around 1.1 to 1.2 points BLEU gain in AR-EN and 0.3 to 0.5 points BLEU gain in ZH-EN. The combination of the two sets produces a complementary gain in addition to the gains of the individual models as Row (S 10 ) shows. The combined gain improves to 1.5 BLEU points in    

 Effects of Tensor Network and Multitask Learning We first analyze the impact of tensor architecture and MTL intrinsically by reporting the models' average log-likelihood on the validation sets (a subset of the test set) in Table  2 . As mentioned, we group the models to HypEn (JM and JMO k?3 ) and SrcEn (LTM, ORI,FERT and TCM) as we perform MTL on these two groups. Likelihood of these two groups in the previous subsection are in column "NN" (for Neural Network), which serves as a baseline. The application of the tensor architecture improves their likelihood as shown in column "Tensor" for both languages and models. Feat.   Table  2 : Sum of the average log-likelihood of the models in HypEn and SrcEn. t = 0 refers to MTL that shares only the embedding layer, while t = 1 shares the first hidden layer as well. L refers to the network's depth. Higher value is better. Independent MTL NN Tensor t = 0 t = 1 L = 2 L = The likelihoods of the MTL-related experiments are in columns with "MTL" header. We present two set of results. In the first set (column "MTL,t=0,L=2"), we run MTL for features from column "Tensor" by sharing the embedding layer only (t = 0). This allows us to isolate the impact of MTL in the presence of Tensors. Column "MTL,t=1,l=3" corresponds to the experiment that produces the best intrinsic result, where each model uses Tensors with three hidden layers (500x500x500, l = 3) and the models share the embedding and the first hidden layers (t = 1). MTL consistently gives further intrinsic gain compared to tensors. More sharing provides an extra gain for SrcEn as shown in the last column. Note that we only experiment with different l and t for SrcEn and not for HypEn because the models in HypEn have different input sets. In our experiments, further sharing and more hidden layers resulted in no further gain. In total, we see a consistent positive effect in intrinsic evaluation from the tensor networks and multitask learning. Moving on to MT evaluation, we summarize the experiments showing the impact of Tensors and MTL in Table  3 . For MTL, we use L = 3, t = 2 since it gives the best intrinsic score. Employing tensors instead of regular neural networks gives a significant and consistent positive impact for all models and language pairs. For the system with the baseline features, we use the tensor architecture for both the joint model and the lexical translation model of  Devlin et al. resulting     (Devlin et al., 2014) . 

 NIST OpenMT12 Our NIST system is compatible with the OpenMT12 constrained track, which consists of 10M words of high-quality parallel training for Arabic, and 25M words for Chinese. The n-gram LM is trained on 5B words of data from the English GigaWord. For test, we use the "Arabic-To-English Original Progress Test" (1378 segments) and "Chinese-to-English Original Progress Test + OpenMT12 Current Test" (2190 segments), which consists of a mix of newswire and web data. All test segments have 4 references. Our tuning set contains 5000 segments, and is a mix of the MT02-05 eval set as well as additional held-out parallel data from the training corpora. We report the experiments for the NIST condition in Table  4 . In particular, we investigate the impact of deploying our new features (column "Feat") and demonstrate the effects of the tensor architecture (column "Tensor") and multitask learning (column "MTL"). As shown the results are inline with the BOLT condition where we observe additive improvements from adding our new features, applying tensor network and multitask learning. On Arabic-English, we see a gain of 2.7 The features we propose in this paper address the major aspects of SMT modeling that have informed much of the research since the original IBM models  (Brown et al., 1993) : lexical translation, reordering, word fertility, and language models. Of particular relevance to our work are approaches that incorporate context-sensitivity into the models  (Carpuat and Wu, 2007) , formulate reordering as orientation prediction task  (Tillman, 2004 ) and that use neural network language models  (Bengio et al., 2003; Schwenk, 2010; Schwenk, 2012) , and incorporate source-side context into them  (Devlin et al., 2014; Auli et al., 2013; Le et al., 2012; Schwenk, 2012) . Approaches to incorporating source context into a neural network model differ mainly in how they represent the source sentence and in how long is the history they keep. In terms of representation of the source sentence, we follow  (Devlin et al., 2014)  in using a window around the affiliated source word. To name some other approaches,  Auli et al. (2013)  uses latent semantic analysis and source sentence embeddings learned from the recurrent neural network;  Sundermeyer et al. (2014)  take the representation from a bidirectional LSTM recurrent neural network; and  Kalchbrenner and Blunsom (2013)  employ a convolutional sentence model. For target context, recent work has tried to look beyond the classical n-gram history.  (Auli et al., 2013; Sundermeyer et al., 2014 ) consider an unbounded history, at the expense of making their model only applicable for N-best rescoring. Another recent line of research  (Bahdanau et al., 2014; Sutskever et al., 2014)  departs more radically from conventional feature-based SMT and implements the MT system as a single neural network. These models use a representation of the whole input sentence. We use a feedforward neural network in this work. Besides feedforward and recurrent net-works, other network architectures that have been applied to SMT include convolutional networks  (Kalchbrenner et al., 2014)  and recursive networks  (Socher et al., 2011) . The simplicity of feedforward networks works to our advantage. More specifically, due to the absence of a feedback loop, the feedforward architecture allows us to treat individual decisions independently, which makes parallelization of the training easy and the querying the network at decoding time straightforward. The use of tensors in the hidden layers strengthens the neural network model, allowing us to model more complex feature interactions like collocation, which has been long recognized as important information for many NLP tasks (e.g. word sense disambiguation  (Lee and Ng, 2002) ). The tensor formulation we use is similar to that of  (Yu et al., 2012; Hutchinson et al., 2013) . Tensor Neural Networks have a wide application in other field, but have only been recently applied in NLP  (Socher et al., 2013; Pei et al., 2014) . To our knowledge, our work is the first to use tensor networks in SMT. Our approach to multitask learning is related to work that is often labeled joint training or transfer learning. To name a few of these works,  Finkel and Manning (2009)  successfully train name entity recognizers and syntactic parsers jointly, and  Singh et al. (2013)  train models for coreference resolution, named entity recognition and relation extraction jointly. Both efforts are motivated by the minimization of cascading errors. Our work is most closely related to  Collobert and Weston (2008; Collobert et al. (2011) , who apply multitask learning to train neural networks for multiple NLP models: part-of-speech tagging, semantic role labeling, named-entity recognition and language model variations. 

 Conclusion This paper argues that a relatively simple feedforward neural network can still provides significant improvement to Statistical Machine Translation (SMT). We support this argument by presenting a multi-pronged approach that addresses modeling, architectural and learning aspects of pre-existing neural network-based SMT features. More concretely, we paper present a new set of neural network-based SMT features to capture important translation phenomena, extend feedforward neural network with tensor layers, and apply multi-task learning to integrate the SMT features more tightly. Empirically, all our proposals successfully produce an improvement over state-of-the-art machine translation system for Arabic-to-English and Chinese-to-English and for both BOLT web forum and NIST conditions. Building on the success of this paper, we plan to develop other neuralnetwork-based features, and to also relax the limiteation of current rule extraction heuristics by generating translations word-by-word. is the hypothesis word window around e b j . In practice, we decompose TCM further into +d d =?d 
