title
Regularized Context Gates on Transformer for Machine Translation

abstract
Context gates are effective to control the contributions from the source and target contexts in the recurrent neural network (RNN) based neural machine translation (NMT). However, it is challenging to extend them into the advanced Transformer architecture, which is more complicated than RNN. This paper first provides a method to identify source and target contexts and then introduce a gate mechanism to control the source and target contributions in Transformer. In addition, to further reduce the bias problem in the gate mechanism, this paper proposes a regularization method to guide the learning of the gates with supervision automatically generated using pointwise mutual information. Extensive experiments on 4 translation datasets demonstrate that the proposed model obtains an averaged gain of 1.0 BLEU score over a strong Transformer baseline.

Introduction An essence to modeling translation is how to learn an effective context from a sentence pair. Statistical machine translation (SMT) models the source context from the source-side of a translation model and models the target context from a target-side language model  (Koehn et al., 2003; Koehn, 2009; Chiang, 2005) . These two models are trained independently. On the contrary, neural machine translation (NMT) advocates a unified manner to jointly learn source and target context using an encoderdecoder framework with an attention mechanism, leading to substantial gains over SMT in translation quality  (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) . Prior work on attention mechanism  (Luong et al., 2015; Liu et al., 2016; Mi et al., 2016; Li et al., 2018; Elbayad et al., 2018; Yang et al., 2020)  have shown a better context representation is helpful to translation performance.  former obtain an unfaithful translation by wrongly translate "t ? q?u" into "play golf" because referring too much target context. By regularizing the context gates, the purposed method corrects the translation of "t ? q?u" into "play soccer". The light font denotes the target words to be translated in the future. For original Transformer, the source and target context are added directly without any rebalancing. However, a standard NMT system is incapable of effectively controlling the contributions from source and target contexts     2017 ) carefully designed context gates to dynamically control the influence from source and target contexts and observed significant improvements in the recurrent neural network (RNN) based NMT. Although Transformer  (Vaswani et al., 2017)  delivers significant gains over RNN for translation, there are still one third translation errors related to context control problem as described in Section 3.3. Obviously, it is feasible to extend the context gates in RNN based NMT into Transformer, but an obstacle to accomplishing this goal is the complicated archi-tecture in Transformer, where the source and target words are tightly coupled. Thus, it is challenging to put context gates into practice in Transformer. In this paper, under the Transformer architecture, we firstly provide a way to define the source and target contexts and then obtain our model by combining both source and target contexts with context gates, which actually induces a probabilistic model indicating whether the next generated word is contributed from the source or target sentence  (Li et al., 2019) . In our preliminary experiments, this model only achieves modest gains over Transformer because the context selection error reduction is very limited as described in Section 3.3. To further address this issue, we propose a probabilistic model whose loss function is derived from external supervision as regularization for the context gates. This probabilistic model is jointly trained with the context gates in NMT. As it is too costly to annotate this supervision for a large-scale training corpus manually, we instead propose a simple yet effective method to automatically generate supervision using pointwise mutual information, inspired by word collocation  (Bouma, 2009) . In this way, the resulting NMT model is capable of controlling the contributions from source and target contexts effectively. We conduct extensive experiments on 4 benchmark datasets, and experimental results demonstrate that the proposed gated model obtains an averaged improvement of 1.0 BLEU point over corresponding strong Transformer baselines. In addition, we design a novel analysis to show that the improvement of translation performance is indeed caused by relieving the problem of wrongly focusing on the source or target context. 

 Methodology Given a source sentence x = x 1 , ? ? ? , x |x| and a target sentence y = y 1 , ? ? ? , y |y| , our proposed model is defined by the following conditional probability under the Transformer architecture: 1 P (y | x) = |y| i=1 P (y i | y <i , x) = |y| i=1 P y i | c L i , (1) where y <i = y 1 , . . . , y i?1 denotes a prefix of y with length i ? 1, and c L i denotes the L th layer context in the decoder with L layers which is obtained from the representation of y <i and h L , i.e., the top layer hidden representation of x, similar to the original Transformer. To finish the overall definition of our model in equation 1, we will expand the definition c L i based on context gates in the following subsections. 

 Context Gated Transformer To develop context gates for our model, it is necessary to define the source and target contexts at first. Unlike the case in RNN, the source sentence x and the target prefix y <i are tightly coupled in our model, and thus it is not trivial to define the source and target contexts. Suppose the source and target contexts at each layer l are denoted by s l i and t l i . We recursively define them from c l?1 <i as follows. 2 t l i = rn ? ln ? att c l?1 i , c l?1 <i , s l i = ln ? att t l i , h L , ( 2 ) where ? is functional composition, att (q, kv) denotes multiple head attention with q as query, k as key, v as value, and rn as a residual network  (He et al., 2016) , ln is layer normalization  (Ba et al., 2016) , and all parameters are removed for simplicity. In order to control the contributions from source or target side, we define c l i by introducing a context gate z l i to combine s l i and t l i as following: c l i = rn ? ln ? ff (1 ? z l i ) ? t l i + z l i ? s l i (3) with z l i = ? ff t l i s l i , (4) where ff denotes a feedforward neural network, denotes concatenation, ?(?) denotes a sigmoid function, and ? denotes an element-wise multiplication. z l i is a vector  (Tu et al. (2017)  reported that a gating vector is better than a gating scalar). Note that each component in z l i actually induces a probabilistic model indicating whether the next generated word y i is mainly contributed from the source (x) or target sentence (y <i ) , as shown in Figure  1 . Remark It is worth mentioning that our proposed model is similar to the standard Transformer with boiling down to replacing a residual connection with a high way connection  (Srivastava et al., 2015; : if we replace (1 ? z l i ) ? t l i + z l i ? s l i in equation 3 by t l i + s l i , the proposed model is reduced to Transformer. 

 Regularization of Context Gates In our preliminary experiments, we found learning context gates from scratch cannot effectively reduce the context selection errors as described in Section 3.3. To address this issue, we propose a regularization method to guide the learning of context gates by external supervision z * i which is a binary number representing whether y i is contributed from either source (z * i = 1) or target sentence (z * i = 0). Formally, the training objective is defined as follows: = ? log P (y | x)+? l,i z * i max(0.5?z l i , 0) + (1 ? z * i ) max(z l i ? 0.5, 0) , (5) where z l i is a context gate defined in equation 4 and ? is a hyperparameter to be tuned in experiments. Note that we only regularize the gates during the training, but we skip the regularization during inference. Because golden z * i are inaccessible for each word y i in the training corpus, we ideally have to annotate it manually. However, it is costly for human to label such a large scale dataset. Instead, we propose an automatic method to generate its value in practice in the next subsection. 

 Generating Supervision z * i To decide whether y i is contributed from the source (x) or target sentence (y <i )  (Li et al., 2019) , a metric to measure the correlation between a pair of words ( y i , x j or y i , y k for k < i) is first required. This is closely related to a well-studied problem, i.e., word collocation  (Liu et al., 2009) , and we simply employ the pointwise mutual information (PMI) to measure the correlation between a word pair ?, ? following  Bouma (2009) : pmi (?, ?) = log P (?,?) P (?)P (?) = log Z + log C(?,?) C(?)C(?) , (6) where C (?) and C (?) are word counts, C (?, ?) is the co-occurrence count of words ? and ?, and Z is the normalizer, i.e., the total number of all possible (?, ?) pairs. To obtain the context gates, we define two types of PMI according to different C (?, ?) including two scenarios as follows. PMI in the Bilingual Scenario For each parallel sentence pair x, y in training set, C (y i , x j ) is added by one if both y i ? y and x j ? x. PMI in the Monolingual Scenario In the translation scenario, only the words in the preceding context of a target word should be considered. So for any target sentence y in the training set, C (y i , y k ) is added by one if both y i ? y and y k ? y <i . Given the two kinds of PMI for a bilingual sentence x, y , each z * i for each y i is defined as follows, z * i = 1 max j pmi(y i ,x j )>max k<i pmi(y i ,y k ) , (7) where 1 b is a binary function valued by 1 if b is true and 0 otherwise. In equation 7, we employ max strategy to measure the correlation between y i and a sentence (x or y <i ). Indeed, it is similar to use the average strategy, but we did not find its gains over max in our experiments. 

 Experiments The proposed methods are evaluated on NIST ZH?EN 3 , WMT14 EN?DE 4 , IWSLT14 DE?EN 5 and IWSLT17 FR?EN 6 tasks. To make our NMT models capable of open-vocabulary translation, all datasets are preprocessed with Byte Pair Encoding  (Sennrich et al., 2015) . All proposed methods are implemented on top of Transformer  (Vaswani et al., 2017)  which is the state-of-the-art NMT system. Case-insensitive BLEU score  (Papineni et al., 2002)  is used to evaluate translation quality of  ZH?EN, DE?EN and FR?EN.  For the fair comparison with the related work, EN?DE is evaluated with case-sensitive BLEU score. Setup details are described in Appendix A. 

 Tuning Regularization Coefficient In the beginning of our experiments, we tune the regularization coefficient ? on the DE?EN task. Table  2  shows the robustness of ?, because the translation performance only fluctuates slightly over various ?. In particular, the best performance  is achieved when ? = 1, which is the default setting throughout this paper. 

 Translation Performance Table  1  shows the translation quality of our methods in BLEU. Our observations are as follows: 1) The performance of our implementation of the Transformer is slightly higher than  Vaswani et al. (2017) , which indicates we are in a fair comparison. 2) The proposed Context Gates achieves modest improvement over the baseline. As we mentioned in Section 2.1, the structure of RNN based NMT is quite different from the Transformer. Therefore, naively introducing the gate mechanism to the Transformer without adaptation does not obtain similar gains as it does in RNN based NMT. 3) The proposed Regularized Context Gates improves nearly 1.0 BLEU score over the baseline and outperforms all existing related work. This indicates that the regularization can make context gates more effective in relieving the context control problem as discussed following. 

 Error Analysis To explain the success of Regularized Context Gates, we analyze the error rates of translation and context selection. Given a sentence pair x and y, the forced decoding translation error is defined as P (y i | y <i , x) < P (? i | y <i , x), where ?i arg max v P (v | y <i , x) and v denotes any to-ken in the vocabulary. The context selection error is defined as z * i (y i ) = z * i (? i ) , where z * i is defined in equation 7. Note that a context selection error must be a translation error but the opposite is not true. The example shown in Figure  1    As shown in Table  3 , the Regularized Context Gates significantly reduce the translation error by avoiding the context selection error. The Context Gates are also able to avoid few context selection error but cannot make a notable improvement in translation performance. It is worth to note that there is approximately one third translation error is related to context selection error. The Regularized Context Gates indeed alleviate this severe problem by effectively rebalancing of source and target context for translation.  means the model tends to trust its language model more than the source context, and we call this context imbalance bias of the freely learned context gate. Specifically, this bias will make the translation unfaithful for some source tokens. As shown in Table  4 , the Regularized Context Gates demonstrates more balanced behavior (0.51?0.5) over the source and target context with similar variance. 

 Statistics of Context Gates 

 Regularization in Different Layers To investigate the sensitivity of choosing different layers for regularization, we only regularize the context gate in every single layer. Table  5  shows that there is no significant performance difference, but all single layer regularized context gate models are slightly inferior to the model, which regularizes all the gates. Moreover, since nearly no computation overhead is introduced and for design simplicity, we adopt regularizing all the layers.   

 Effects on Long Sentences In  Tu et al. (2017) , context gates alleviate the problem of long sentence translation of attentional RNN based system  (Bahdanau et al., 2014) . We follow  Tu et al. (2017)  and compare the translation performances according to different lengths of the sentences. As shown in Figure  2 , we find Context Gates does not improve the translation of long sentences but translate short sentences better. Fortunately, the Regularized Context Gates indeed significantly improves the translation for both short sentences and long sentences. 

 Conclusions This paper transplants context gates from the RNN based NMT to the Transformer to control the source and target context for translation. We find  [0,10) [10,20) [20,30) [30,40) [40,50) [50,60) [60,130)  Length of Source Sentence that context gates only modestly improve the translation quality of the Transformer, because learning context gates freely from scratch is more challenging for the Transformer with the complicated structure than for RNN. Based on this observation, we propose a regularization method to guide the learning of context gates with an effective way to generate supervision from training data. Experimental results show the regularized context gates can significantly improve translation performances over different translation tasks even though the context control problem is only slightly relieved. In the future, we believe more work on alleviating context control problem has the potential to improve translation performance as quantified in Table  3 . w? j?ng ch?ng h? w? d? t?ng h?ng m?n y? q? t? q?u ? 

 Figure 1 : 1 Figure1: A running example to raise the context control problem. Both original and context gated Transformer obtain an unfaithful translation by wrongly translate "t ? q?u" into "play golf" because referring too much target context. By regularizing the context gates, the purposed method corrects the translation of "t ? q?u" into "play soccer". The light font denotes the target words to be translated in the future. For original Transformer, the source and target context are added directly without any rebalancing. 

 to deliver highly adequate translations as shown in Figure 1. As a result, Tu et al. ( 

 Figure 2 : 2 Figure 2: Translation performance on MT08 test set with respect to different lengths of source sentence. Regularized Context Gates significantly improves the translation of short and long sentences. 

 Table 1 : 1 Translation performances (BLEU). TheRNN based NMT et al., 2014)  is reported from the baseline model in Tu et al. (2017) . "params" shows the number of parameters of models when training ZH?EN except Vaswani et al. (2017)  is for EN?DE tasks. Models params ?10 6 ZH?EN MT05 MT06 MT08 EN?DE DE?EN FR?EN RNN based NMT 84 30.6 31.1 23.2 - - - Tu et al. (2017) 88 34.1 34.8 26.2 - - - Vaswani et al. (2017) 65 - - - 27.3 - - Ma et al. (2018) - 36.8 35.9 27.6 - - - Zhao et al. (2018) - 43.9 44.0 33.3 - - - Cheng et al. (2018) - 44.0 44.4 34.9 - - - Transformer 74 46.9 47.4 38.3 27.4 32.2 36.8 This Work Context Gates Regularized Context Gates 92 92 47.1 47.7 47.6 48.3 39.1 39.7 27.9 28.1 32.5 33.0 37.7 38.3 ? 0.1 0.5 1 2 10 BLEU 32.7 32.6 33.0 32.7 32.6 * Results are measured on DE?EN task. 

 Table 2 : 2 Translation performance over different regularization coefficient ?. 

 also demonstrates a context selection error indicating the translation error is related with the bad context selection. Models FER CER CE/FE Transformer 40.5 13.8 33.9 Context Gates 40.5 13.7 33.7 Regularized Context Gates 40.0 13.4 33.4 * Results are measured on MT08 of ZH?EN task. 

 Table 3 3 : Forced decoding translation error rate (FER), context selection error rate (CER) and the proportion of context selection errors over forced decoding trans- lation errors (CE/FE) of the original and context gated Transformer with or without regularization. 

 Table 4 summarizes the mean and variance of each context gate (every dimension of the context gate vectors) over the MT08 test set. It shows that learning context gates freely from scratch tends to pay more attention to target context (0.38 < 0.5), which Results are measured on MT08 of ZH?EN task. Models Mean Variance Context Gates 0.38 0.10 Regularized Context Gates 0.51 0.13 * 

 Table 4 : 4 Mean and variance of context gates 

 Table 5 : 5 Regularize context gates on different layers."N/A" indicates regularization is not added. "ALL" indicates regularization is added to all the layers. 

			 Throughout this paper, a variable in bold font such as x denotes a sequence while regular font such as x denotes an element which may be a scalar x, vector x or matrix X. 

			 For the base case, c 0 <i is word embedding of y<i. 

			 LDC2000T50, LDC2002L27, LDC2002T01, LDC2002E18, LDC2003E07, LDC2003E14, LDC2003T17, LDC2004T07 4 WMT14: http://www.statmt.org/wmt14/ 5 IWSLT14: http://workshop2014.iwslt.org/ 6 IWSLT17: http://workshop2017.iwslt.org/
