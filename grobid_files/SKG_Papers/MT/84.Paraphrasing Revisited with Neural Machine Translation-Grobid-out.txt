title
Paraphrasing Revisited with Neural Machine Translation

abstract
Recognizing and generating paraphrases is an important component in many natural language processing applications. A wellestablished technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by "pivoting" over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches.

Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content  (Madnani and Dorr, 2010) . Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion  (Riezler et al., 2007) , summarization  (Barzilay, 2003) , question answering  (Lin and Pantel, 2001) , semantic parsing  (Berant and Liang, 2014) , semantic role labeling  (Woodsend and Lapata, 2014) , and machine translation  (Callison-Burch et al., 2006) . Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT,  Koehn et al. (2003) ). The intuition behind  Bannard and Callison-Burch's (2005)  bilingual pivoting method is that two English strings e 1 and e 2 that translate to the same foreign string f can be assumed to have the same meaning. The method then pivots over f to extract e 1 , e 2 as a pair of paraphrases. Drawing inspiration from syntaxbased SMT, several subsequent efforts  (Callison-Burch, 2008; Ganitkevitch et al., 2011)  extended this technique to syntactic paraphrases leading to the creation of PPDB  (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014) , a largescale paraphrase database containing over a billion of paraphrase pairs in 23 different languages. In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks  (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015) . At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs  (Jean et al., 2015b; Luong et al., 2015) , using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space representations for phrases and sentences (aka embeddings) that can be usefully incorporated in downstream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and generate target paraphrases for a given source input. Due to the architecture of NMT, generation takes advantage of wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. In the remainder of the paper, we introduce our paraphrase model and experimentally compare it to the phrase-based pivoting approach. We evaluate the model's paraphrasing capability both intrinsically in a paraphrase detection task (i.e., decide the degree of semantic similarity between two sentences) and extrinsically in a generation task. Across tasks and datasets our results show that neural paraphrases yield superior performance when assessed automatically and by humans. 

 Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the underlying representation (surface form or syntax trees), and the acquisition method itself. For an overview of these issues we refer the interested reader to  Madnani and Dorr (2010) . We focus on bilingual pivoting methods and aspects of neural machine translation pertaining to our model. We also discuss related work on paraphrastic embeddings. Bilingual Pivoting Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and  Callison-Burch (2005) . Their method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e| f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar  (Ganitkevitch et al., 2011; Madnani et al., 2007)  as well as labeling paraphrases with linguistic annotations such as CCG categories  (Callison-Burch, 2008)  and partof-speech tags  (Zhao et al., 2008) . In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Neural Machine Translation There has been a surge of interest recently in repurposing sequence transduction neural network models for machine translation  (Sutskever et al., 2014) . Central to this approach is an encoder-decoder architecture implemented by recurrent neural networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism  (Bahdanau et al., 2014)  is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models  (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a) , we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. 

 Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases.  Wieting et al. (2015)  take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to  Socher et al. (2013) . In follow-up work  (Wieting et al., 2016) , they learn sentence embeddings based on supervision provided by PPDB. In our approach, embeddings are learned as part of the model and are available for any-length segments making use of no additional machinery beyond NMT itself. 

 Neural Paraphrasing In this section we present PARANET, our Paraphrasing model based on Neural Machine Translation. PARANET uses neural machine translation to first translate from English to a foreign pivot, which is then back-translated to English, producing a paraphrase. In the following, we briefly overview the basic encoder-decoder NMT framework and then discuss how it can be extended to paraphrasing. 

 NMT Background In the neural encoder-decoder framework for MT  (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015) , the encoder, a recurrent neural network (RNN), is used to compress the meaning of the source sentence into a sequence of vectors. The decoder, a conditional RNN language model, generates a target sentence word-by-word. For the language pair, an encoder takes in a source sentence X = {x 1 , ..., x T X }, as a sequence of linguistic symbols and produces a sequence of context vectors C = {h 1 , ...h T X }. PARANET uses a bidirectional RNN, where each context vector h t is the concatenation of the forward and the backward RNN's hidden states at time t. The decoder is a conditional RNN language model that produces, given the source sentence, a probability distribution over the translation. At each time step t , the decoder's hidden state is updated: z t = RNN(z t ?1 , y t ?1 , c t ) (1) The update uses the previous hidden state z t ?1 , the previous target symbol y t ?1 and the time dependent context c t , which is computed by an attention mechanism ? t,t over the source sentences' context vectors: c t = T x ? t=1 ? t ,t h t (2) ? t ,t ? e f (z t ?1 ,h t ) (3) g is a feedforward neural network with a softmax activation function in the output layer which returns the probability of the next target symbol. The probability of the target sentence Y = {y 1 , ..., y T X }, is the product of the probabilities of the symbols within the sentence: P(Y |X) = T Y ? t =1 P(y t |y <t , X) (4) 

 Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is not a translation path from the source language to the target. Instead, pivoting takes advantage of paths through an intermediate language. The idea dates back at least to  Kay (1997) , who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in traditional phrase-based SMT  (Wu and Wang, 2007; Utiyama and Isahara, 2007)  and more recently in neural MT systems  (Firat et al., 2016b) . In the case of paraphrasing, there is not a path from English to English. Instead, a path from English to French to English can be used. In other words, we translate a source sentence into a pivot language and then translate the pivot back into the source language. Pivoting using NMT ensures that the entire sentence is considered when choosing a pivot. The fact that contextual information is considered when translating, allows for a more accurate pivoted sentence. It also places greater emphasis on capturing the meaning of the sentence, which is a key part of paraphrasing. A naive approach to pivoting is one-to-one back-translation. The source English sentence E 1 , is translated into a single French sentence F. Next, F is translated back into English, giving a probability distribution over English sentences, E 2 . This translation distribution acts as the paraphrase distribution P(E 2 |E 1 , F): P(E 2 |E 1 , F) = P(E 2 |F) (5) One-to-one back-translating offers an easy way to paraphrase, because existing NMT systems can be used with no additional training or changes. However, there are several disadvantages; for example the French sentence F must fully capture the exact meaning of E 1 , as E 1 and E 2 are conditionally independent given F. Since there is rarely a clear one-to-one mapping between sentences in different languages, information about the source sentence can be lost, leading to inaccuracies in the paraphrase probabilities. To avoid this, we propose back-translating through multiple sentences within one and multiple foreign languages. Multi-pivoting PARANET pivots through the set of K-best translations F = {F 1 , ...F K } of E 1 . This ensures that multiple aspects (semantic and syntactic) of the source sentence are captured. Moreover, multiple pivots provide resilience against a single bad translation, which would prevent one-to-one back-translation from producing accurate paraphrase probabilities. Translating from multiple pivot sentences into one target sentence requires that the decoder be redefined.  Firat et al. (2016b)  propose several ways in which multiple pivot sentences can be incorporated into a NMT decoder. We extended their late averaging approach to incorporate weights. Consider the case of two pivot sentences from the same language, F 1 and F 2 . Each translation path individ- After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (  6 ). From this combined distribution a word is chosen, which is then given as input to each decoder. ually computes the distribution over the target vocabulary P(y t = w|y <t , F 1 ) and P(y t = w|y <t , F 2 ). Our late-weighted combination approach defines the path with respect to both translations as: P(y t = w|y <t , F 1 , F 2 ) = ? 1 P(y t = w|y <t , F 1 ) +? 2 P(y t = w|y <t , F 2 ) While  Firat et al. (2016b)  train a new model to capture these joint translations, we leave the model unchanged, instead treating PARANET as a meta encoder-decoder model (see Figure  1 ). Unlike late averaging, PARANET assigns weights ? to each pivot sentence. These weights are set to the initial translation probabilities P(F i |E 1 ), thus capturing the model's confidence in the accuracy of the translation: P(y t =w|y <t , F 1 , F 2 ) = P(F 1 |E 1 )P(y t =w|y <t ,F 1 ) +P(F 2 |E 1 )P(y t =w|y <t ,F 2 ) Which can be trivially extended to include all translations from the K-best list: (6) P(y t = w|y <t , F ) = ? K i=1 P(F i |E 1 ) ? P(y t = w|y <t , F i ) To ensure a probability distribution, we normalize the K-best list F , such that the translation probabilities sum to one. Multi-lingual Pivoting PARANET further expands on the multi pivot approach by pivoting not only over multiple sentences from one language, but also over multiple sentences from multiple languages. Multi-lingual pivoting has been recently shown to improve translation quality  (Firat et al., 2016b) , especially for low-resource language pairs. Here, we hypothesize that it will also lead to more accurate paraphrases. Multi-lingual pivoting requires a small extension to late-weighted combination. We illustrate with German as a second language. First, the source sentence is translated into a K-best list of French F Fr , and a K-best list of German F De . Late-weighted combination is then applied, producing P(y t = w|y <t , F Fr ) and P(y t = w|y <t , F De ). These two output distributions are averaged, producing a multi-sentence, multilingual paraphrase probability: P(y t = w|y <t , F Fr , F De ) = 1 2 P(y t = w|y <t , F Fr ) + P(y t = w|y <t , F De ) which is used to obtain probability distributions over sentences: P(E 2 |E 1 ) = T E 2 ? t =1 P(y t |y <t , F Fr , F De ) (7) This can be trivially generalized to multiple languages. In this paper we use up to three. 

 PARANET Applications The applications of PARANET are many and varied. We discuss some of these here and present detailed experimental evidence in Section 4. PARANET can be readily used for paraphrase detection (the task of analyzing two text segments and determining if they have the same meaning), by computing Equation (  7 ). In addition, it can identify which linguistic units are considered paraphrases and to what extent. PARANET's explanatory power stems from the attention mechanism inherent in the NMT systems. In encoder-decoder models, attention is used during each step of decoding to indicate which are the relevant source words. In our case, each word of the paraphrase attends to words within the pivot sentence and each word in the pivot sentence attends to words within the source sentence. By summing out the weighted pivot sentence, it is possible to see the attention from paraphrase to Two men sailing in a small boat couple sailing in a tiny sail boat  ?(E i 2 , E j 1 , F )= F ? F P(E 2 |E 1 , F)? T F ? m (? E 2 ,F i,m ? F,E 1 m, j ) (8) An example shown in Figure  2  where attention has successfully identified the semantically equivalent parts of two sentences. Beyond providing interpretable paraphrasing, attention scores can be used as features in both generation and classification tasks. Furthermore, PARANET can be readily used to perform text generation (via the NMT decoder) without additional resources or parameter estimation. It also learns phrase and sentence embeddings for free without any model adjustments or recourse to resources like PPDB. 

 Experiments We evaluated PARANET in several ways: (a) we examined whether the paraphrases learned by our model correlate with human judgments of paraphrase quality; (b) we assessed PARANET in paraphrase and similarity detection tasks; and (c) in a sentence-level paraphrase generation task. We first present details on how PARANET and comparison models were trained and then discuss our results. 

 Neural Machine Translation Training We used Groundhog 1 as the implementation of the NMT system for all experiments. We generally followed the settings and training procedure from previous work  (Bahdanau et al., 2014; Sennrich et al., 2016a) . As such, all networks have a hidden layer size of 1000, and an embedding layer size of 620. During training, we used Adadelta  (Zeiler, 2012) , a minibatch size of 80, and the training set was reshuffled between epochs. We trained a network for approximately 7 days on a single GPU, then the embedding layer was fixed and training continued, as suggested in  Jean et al. (2015a) , for 12 hours. Additionally, the softmax was calculated over a filtered list of candidate translations. Following  Jean et al. (2015a) , we set the common 1 github.com/sebastien-j/LV groundhog vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign  (Dyer et al., 2013) . In our experiments, we used up to six encoder-decoder NMT models (three pairs); English?French, French?English, English?Czech, Czech?English, English?German, German?English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN?DE, EN?CS, and EN?FR, respectively). For EN?DE and EN?CS, we also had access to back-translated monolingual training data  (Sennrich et al., 2016a) , which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES  (Koehn et al., 2007) . Rare words were split into sub-word units, following  Sennrich et al. (2016b) . BLEU scores for each NMT system can be seen in Table  1 . 

 Statistical Machine Translation Training Throughout our experiments we compare PARANET against a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT), which we henceforth refer to as PARASTAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems  (Durrani et al., 2011) . SMT translation models were implemented using both GIZA++  (Och and Ney, 2003)  and MOSES  (Koehn et al., 2007)  and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model  (Heafield, 2011) , trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table  1 . Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): and English to foreign (E?F) directions). P(E 2 |E 1 , F ) = F ? F P(E 2 |F)P(F|E 1 ) (9 

 Correlation with Human Judgments The PPDB 2.0 Human Evaluation data set is a sample of paraphrase pairs taken from PPDB which have been human annotated for semantic similarity  (Pavlick et al., 2015) . 26,455 samples were taken from range of syntactic categories, resulting in paraphrase candidates varying from single words to multi-word expressions. Each paraphrase pair was judged by five people on a 5-point scale. Ratings were then averaged giving each paraphrase pair a score between 1 and 5. Using this dataset we measure the correlation (Spearman ?) between (length normalized) PARANET probabilities (Equation (  7 )) assigned to paraphrase pairs and human judgments. Figure  3  shows correlation coefficients for all language pairs using a single foreign pivot and 200 pivots. Across all language combinations multiple pivots 2 achieve better correlations, with the German, Czech pair performing best with ? = 0.53. For comparison,  Pavlick et al. (2015)  report a correlation of ? = 0.41 using Equation (9) and PPDB  (Ganitkevitch et al., 2013) . The latter contains over 100 million paraphrases and was constructed over several English-to-foreign parallel corpora including Europarl v7  (Koehn, 2005)  which contains bitexts for the 19 European languages. Following  Pavlick et al. (2015) , we next developed a supervised scoring model. Specifically, we fit a decision tree regressor on the PPDB 2.0 dataset using the implementation provided in scikit-learn  (Pedregosa et al., 2011) . To improve accuracy and control over-fitting we built an ensemble of regression trees using the Extra-Trees algorithm  (Geurts et al., 2006)  which fits a number of randomized decision trees (a.k.a. extratrees) on various sub-samples of the dataset. In our experiments 1,000 trees were trained to minimize mean square error. The regressor was trained with the following basic features: sentence length, 2 Across tasks and datasets we find that multiple pivots outperform single pivots. We omit these comparisons from subsequent experiments for the sake of brevity.  pscore(E 2 , E 1 ) = P(E 2 |E 1 ) P(E 2 |E 1 ) + P(E 2 |E 2 ) (10) Two features reflect the alignment between candidate paraphrases. We built an alignment matrix according to Equation (8), and used the mean of the diagonal as feature. This acts as a proxy of how much movement there is between two paraphrases. The second feature is the number of unaligned words which we compute by calculating hard alignments between the two paraphrases. Regressors varied with respect to how P(E 2 |E 1 ) was computed, keeping the string based features the same. Equations (  7 ) and (  9 ) were used to calculate paraphrase probability for PARANET and PARASTAT, respectively. For both models beam search (with width set to 100) was used to generate the K-best list. For each language, the K-best list is the union of the 100-best list of E 1 and the 100-best list of E 2 , giving a maximum of 200 pivot sentences. As set out in  Pavlick et al. (2015)  evaluation is done using cross validation: in each fold, we hold out 200 phrases. Table  2  presents results for PARANET and PARASTAT using different languages as pivots. PARANET outperforms PARA-STAT across the board. Furthermore, despite using fewer features and pivot languages, it obtains a closer correspondence to human data compared to PPDB 2.0  (Pavlick et al., 2015) .  

 Model 

 Paraphrase Identification and Similarity The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs. By design, the dataset contains colloquial sentences representing informal language usage and sentence pairs which are lexically similar but semantically dissimilar. Sentence pairs were crawled from Twitter's trending topics and associated tweets (see  Xu et al. (2014)  for details). The shared task consists of a (binary) paraphrase identification subtask (i.e., determine whether two sentences are paraphrases) and an optional semantic similarity task (i.e., determine the similarity between two sentences on a scale of 1-5, where 5 means completely equivalent and 1 not equivalent). We trained a decision tree regressor on the PIT-2015 similarity dataset using the features described above. Once trained, the decision tree regressor can be readily applied to the semantic similarity subtask. For the paraphrase detection subtask, we use the same model and apply a threshold (optimized on the validation set) such that those pairs that are over this threshold are deemed paraphrases. Tables  3 and 4  present our results on the two subtasks together with previously published results. We evaluate system performance on the detection task using F1 (the harmonic mean of precision and recall). For semantic similarity, system outputs are compared by Pearson correlation against human scores. The first block in the tables summarize results for PARANET and PARA-STAT using different languages as pivots. The second block includes three baselines provided by the organizers of the shared task: a random baseline, a logistic regression baseline with minimal 

 Model PARASTAT   (Guo and Diab, 2012) . The last two rows show the highest scoring systems: ASOBEK  (Eyecioglu and Keller, 2015)  ranked 1st in the identification subtask and MITRE  (Zarrella et al., 2015)  in the similarity subtask. Whereas ASOBEK uses knowledge-lean features based on word and character n-gram overlap, MITRE is a combination of multiple systems including mixtures of string matching metrics, alignments using tweet-specific word representations, and recurrent neural networks. As can be seen, PARANET achieves better similarity and detection score than all baselines and PARASTAT, for any combinations of lan- guages. This is particularly impressive as the translation models were trained on very dissimilar data. Compared to the state of the art, PARANET fares worse, however our model was not particularly optimized on the PIT-2015 dataset which was merely used as a testbed for a fair comparison. It is thus reasonable to assume that taking into account more elaborate features (e.g., based on character embeddings) would improve performance. The highest semantic similarity score is obtained with PARANET trained using German data. The highest scoring paraphrase detection model was PARANET trained on French and Czech data. Interestingly, using multiple pivot languages seems to offer small improvements in most cases. The languages selected as pivots in our experiments were somewhat ad-hoc. We expect to get more mileage if these are selected from the same language family or with more linguistic insight (e.g., morphologically rich vs. poor). 

 Model 

 Semantic Textual Similarity In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. We present results on the Semeval-2015 English subtask which contains sentences from a wide range of domains, including newswire headlines, image descriptions, and answers from Q&A websites. The training/test sets consist of 11,250 and 3,000 sentence pairs, respectively. Sentence pairs are rated on a 1-5 scale, with 5 indicating they are completely equivalent. We used the decision tree regressor with the same features described in the previous section. Again, we experimented with one, two, and three languages as pivots, and compared PARANET and PARASTAT directly. Our results are summarized in Table  5 . The third block in the table presents a simple cosine-based baseline provided by the organizers (Tokencos) and the top-performing system (DLS@CU) which uses PPDB paraphrases to identify semantically similar words and word2vec embeddings trained on approximately 2.8 billion tokens  (Sultan et al., 2014) . PARANET outperforms PARASTAT on all languages and language combinations. Both systems outperform the Semeval baseline but are worse compared to the top scoring system. We see for PARANET Czech achieves the highest scores, this could be in part due to Czech non-strict word order, which allows paraphrases that are simple rearrangements not be penalized. 

 Paraphrase Generation Finally, we evaluated PARANET (and PARAS-TAT) in a paraphrase generation task. We created sentential paraphrases for three (parallel monolingual) datasets representative of different domains and genres: (a) the Multiple-Translation Chinese (MTC) corpus  (Huang et al., 2002)  contains news stories from three sources of journalistic Mandarin Chinese text translated into English by 4 translation agencies; we sampled 1,000 sentences for training and testing, respectively (each source sentence had an average of 4 paraphrases); (b) the Jules Vernes Twenty Thousand Leagues Under the Sea novel (Leagues) corpus  (Pang et al., 2003)  contains two English translations of the French novel; we sampled 500 sentences for training/testing (each source sentence had one paraphrase); and (c) the Wikianswers corpus  (Fader et al., 2013)  which contains questions taken from the website 3 wiki answers; we sampled 1,000 questions for training/testing (each question has on average 21 paraphrases). In order to select the best paraphrase candidate for a given input sentence, PARASTAT was optimized on the training set using Minimum Error Training  (MERT, Och and Ney (2003) ). MERT integrates automatic evaluation metrics such as BLEU into the training process to achieve optimal end-to-end performance. Naively optimizing for BLEU, however, will result in a trivial paraphrasing system heavily biased towards producing identity "paraphrases".  Sun and Zhou (2012)   sentence and rewards those close to the target: iBLEU(s, r s , c) = ?BLEU(c, r s ) ? (1 ? ?)BLEU(c, s) where s, is the source sentence, r s , is the target and c is the candidate paraphrase. (1 ? ?)BLEU(c, s), measures the originality of the candidate paraphrase, BLEU(c, r s ) measures semantic adequacy, and ? is a tuning parameter which balances the two. Sentence level BLEU is calculated using plus one smoothing  (Lin and Och, 2004) . PARANET relies on a relatively simple architecture which is trained end-to-end with the objective of maximizing the likelihood of the training data. Since evaluation metrics cannot be straightforwardly integrated into this training procedure, we reranked the k-best paraphrases obtained from PARANET using a simple classifier which favors sentences which are dissimilar to the source. Specifically, we trained a decision tree regression model with iBLEU as the target variable using the same features described in Section 4.4. Examples of paraphrases generated by PARANET are shown in the Appendix. System output was assessed automatically using iBLEU with human-written paraphrases as reference. In addition, we evaluated the generated text by eliciting human judgments via Amazon Mechanical Turk. We randomly selected 100 source sentences from each data set and generated output with PARANET and PARASTAT (using German as a pivot). We also included a randomly selected human paraphrase as a goldstandard. Workers (self-reported native English speakers) were asked to rank the three paraphrases from best to worst (ties were allowed) in order of semantic equivalence (does the paraphrase convey the same meaning as the source?) and fluency (is the description written in well-formed English?). Participants were explicitly told to give high ranks to output demonstrating a fair amount of paraphrasing and low ranks to trivial paraphrases (e.g., deletion of articles or punctuation). We collected 5 responses per input sentence. datasets. For the sake of brevity, we only show results with one pivot language since combinations performed slightly worse for both models. We set ? = 0.8 for iBLEU as we experimentally found it offers the best trade-off between semantic equivalence and dissimilarity. As an upperbound we also measure iBLEU amongst the gold paraphrases provided by humans. Again, we observe that PARANET has a slight advantage over PARASTAT in terms of iBLEU, however both systems tend to paraphrase less compared to the goldstandard. Table  7  shows the mean ranks given to these systems by human subjects. An Analysis of Variance (ANOVA) revealed a reliable effect of system type. Post-hoc Tukey tests showed that PARANET is significantly (p < 0.01) better than PARASTAT across datasets; PARANET is also significantly (p < 0.01) better than the the gold standard on both MTC and the Wikianswers dataset. We attribute this to the noisy nature of these two datasets which contain a wealth of paraphrases, a few of which are ungrammatical, contain typos or abbreviations leading to low scores among humans. 

 Conclusions In this work we presented PARANET, a neural paraphrasing model based on bilingual pivoting. Experimental results across several tasks (similarity prediction, paraphrase identification, and paraphrase generation) show that PARANET outperforms conventional paraphrasing methods. In the future, we plan to exploit the attention scores more directly for extracting paraphrase pairs (in analogy to PPDB) and as features for classification tasks (e.g., textual entailment). We would also like to investigate how PARANET can be adapted using reinforcement learning  (Ranzato et al., 2016)  to text generation tasks such as simplification and sentence compression. Wikianswers a. How many calories in a handful of strawberries? b. The number of calories in a handful of strawberries. a. Beauty is not in the eye of the beholder. b. Beauty is not in the mind of the viewer. a. What is the importance of employee satisfaction in an organization? b. What is the significance of staff satisfaction at an organisation? a. What is the difference between electrical power and electrical energy? b. What is the difference between electrical energy and electrical power? a. How many high tides happen at a given coast in any 24 hour period? b. How many high tides occur on a certain coast in 24 hours? a. What is a beverage that starts with the letter p? b. What is a drink that begins with the letter p? a. What Swiss mathematician and teacher was responsible for instituting the use of the symbol for ? in mathematical notation? b. What Swiss mathematicians and teachers were responsible for the introduction of the symbol for ? in math notation? a. How do you make a pina colada? b. How do you do a Pina colada? a. What is the difference between a captain and a skipper? b. What is the difference between being a captain and skipper?      Figure 1 : 1 Figure1: Late-weighted combination: two pivot sentences are simultaneously translated to one target sentence. Blue circles indicate the encoders, which individually encode the two source sentences. After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (6). From this combined distribution a word is chosen, which is then given as input to each decoder. 

 Figure 2 : 2 Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: 

 Leagues a. "Faith i should never have believed it," said Conseil. b. "Faith, I never would have believed", Conseil said. a. "I owed myself this revenge!" Said the Captain to the Canadian. b. "I am indebted to this revenge!" the captain told the Canadian. a. "Well, sir, you will only get your deserts." b. "Well, sir, you are only getting your deserts." a. "That's what I've been telling you Ned." b. "That's what I said, Ned." a. Very much embarrassed, after having vainly exhausted our speaking resources, I knew not what part to take, when Conseil said: "if master will permit me I will relate it in German." b. It was very embarrassing that I had used up our speaking time, and I did not know what to do, as Conseil said: "If the Masters allow me, I shall refer to German." a. Almost every day the panels in the lounge were open for some hours, and our eyes never tired of probing the mysteries of the underwater world. b. Almost every day, the panels opened in the lounge for a few hours, and our eyes never tired, the secrets of the underwater world. a. I bowed, and desired to be conducted to the cabin destined for me. b. I bow to and wish I headed to the cabin for me. a. I had one question left to address to Captain Nemo. b. I had a question left to Captain Nemo. a. "I have not the foggiest notion, Professor Aronnax." b. I have no idea, Professor Aronnax.Table 9: Sentences marked (a) are the input and (b) are PARANET paraphrases. 

 MTC a. China expresses strong dissatisfaction over the Japanese leader's move this time. b. China expresses a strong dissatisfaction over Japanese leader's move. a. We will accelerate the drafting of telecommunications legalization, amend the law of post and the regulations governing wireless telecommunications. b. We will speed up the design of telecommunications, change the law and regulations governing wireless telecommunication. a. Liu said: the poverty-stricken areas are badly hit in the first stage of this year's floods and many counties and cities are listed as the poorest ones in the country. b. Liu said: poverty-stricken areas are hit hard in the first phase of this year's flooding and many counties and towns are listed as the poorest in the country. a. (London, AP) The British government is working on resolving the increasingly serious problems of street crimes and will strengthen patrolling police. b. London, AP The British government is working to resolve the increasingly serious problems of street crime and will strengthen patrols. a. Kida said that the dead killed by the heat wave were mostly old people with heart diseases.b. Kida said the dead by heatwave were mostly old people with heart disease. a. Growth of Mobile Phone users in Mainland China to Slow Down. b. Growth of Mobile Phone users in Mainland China on Slow Down. a. The survey report said that in the first six months of last year 18 sandstorms struck Beijing and they all originated from Inner Mongolia where 60% of the land is desert. b. The report said that 18 sandstorms struck Beijing in the first six months of last year, and they were all from Inner Mongolia, where 60% of the desert is desert. a. The World Cup co-host by Japan and South Korea, will inaugurate on May 31. b. The World Cup, co-host Japan and South Korea, will open on May 31. a. Two days ago, President Bush seemed opposed to this idea when he held talks with Sharon. b. Two days ago President Bush opposed this idea when he talks to Sharon. a. Russia Faces Population Crisis. b. Russia's demographics problem. a. Computer Crimes Cost US billions of Dollars Last Year. b. Computer Crimes Cost American Billions of Dollars. a. However, many sports associations in Chile hope to cooperate with China not just for the 

 Table 5 : 5 Results on the Semeval-2015 semantic similarity dataset. Boldface indicates the best performing paraphrasing model. PARASTAT PARANET fr 0.657 0.682 de 0.666 0.678 cz 0.649 0.688 de, fr 0.665 0.684 de, cz 0.662 0.687 fr, cz 0.654 0.690 fr, cz, de 0.658 0.689 Tokencos 0.587 DLS@CU 0.801 

 Table 6 : 6 introduce iBLEU which we also adopt. iBLEU penalizes paraphrases which are similar to the source Mean iBLEU across three datasets. Model PARASTAT PARANET fr 0.280 0.299 de 0.282 0.295 cz 0.280 0.291 Gold 0.599 

 Table 6 6 summarizes our results across the three 

 Table 7 : 7 Mean Rankings given to paraphrases by human participants (a lower score is better). 

 Table 8 : 8 Sentences marked (a) are the input and (b) are PARANET paraphrases. 

 table tennis alone. b. However, many sports federations in Chile are hoping to collaborate with China, not only for the table tennis players. 

 Table 10 : 10 Sentences marked (a) are the input and (b) are PARANET paraphrases. 

			 http://wiki.answers.com/
