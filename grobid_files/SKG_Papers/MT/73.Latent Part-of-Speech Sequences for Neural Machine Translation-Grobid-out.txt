title
Latent Part-of-Speech Sequences for Neural Machine Translation

abstract
Learning target side syntactic structure has been shown to improve Neural Machine Translation (NMT). However, incorporating syntax through latent variables introduces additional complexity in inference, as the models need to marginalize over the latent syntactic structures. To avoid this, models often resort to greedy search which only allows them to explore a limited portion of the latent space. In this work, we introduce a new latent variable model, LaSyn, that captures the codependence between syntax and semantics, while allowing for effective and efficient inference over the latent space. LaSyn decouples direct dependence between successive latent variables, which allows its decoder to exhaustively search through the latent syntactic choices, while keeping decoding speed proportional to the size of the latent variable vocabulary. We implement LaSyn by modifying a transformer-based NMT system and design a neural expectation maximization algorithm that we regularize with part-of-speech information as the latent sequences. Evaluations on four different MT tasks show that incorporating target side syntax with LaSyn improves both translation quality, and also provides an opportunity to improve diversity.

Introduction Syntactic information has been shown to improve the translation quality in NMT models. On the source side, syntax can be incorporated in multiple ways -either directly during encoding  (Chen et al., 2018; Eriguchi et al., 2016) , or indirectly via multi-task learning to produce syntax informed representations  (Eriguchi et al., 2017; Baniata et al., 2018; Niehues and Cho, 2017; Zaremoodi et al., 2018) . On the target side, however, incorporating the syntax is more challenging due to the additional complexity in inference when decoding over latent states. To avoid this, existing methods resort to approximate inference over the latent states using a two-step decoding process  (G? et al., 2018; Wang et al., 2018a; Wu et al., 2017; Aharoni and Goldberg, 2017) . Typically, the first stage decoder produces a beam of latent states, which serve as conditions to feed into the second stage decoder to obtain the target words. Thus, training and inference in these models can only explore a limited sub-space of the latent states. In this work, we introduce LaSyn, a new target side syntax model that allows exhaustive exploration of the latent states to ensure a better translation quality. Similar to prior work, LaSyn approximates the co-dependence between syntax and semantics of the target sentences by modeling the joint conditional probability of the target words and the syntactic information at each position. However, unlike prior work, LaSyn eliminates the sequential dependence between the latent variables and simply infers the syntactic information at a given position based on the source text and the partial translation context. This allows LaSyn to search over a much larger set of latent state sequences. In terms of time complexity, unlike typical latent sequential models, LaSyn only introduces an additional term that is linear in the size of latent variable vocabulary and the length of the sentence. We implement LaSyn by modifying a transformer-based encoder-decoder model. The implementation uses a hybrid decoder that predicts two posterior distributions: the probability of syntactic choices at each position P (z n |x, y <n ), and the probability of the word choices at each position conditioned on each of the possible values for the latent states P (y n |z n , x, y <n ). The model cannot be trained by directly optimizing the data log-likelihood because of its non-convex property. We devise a neural expectation maximization (NEM) algorithm, whose E-step computes the posterior distribution of latent states under current model parameters, and M-step updates model parameters using gradients from back-propagation. Given some supervision signal for the latent variables, we can modify this EM algorithm to obtain a regularized training procedure. We use partsof-speech (POS) tag sequences, automatically generated by an existing tagger, as the source of supervision. Because the decoder is exposed to more latent states during training, it is more likely to generate diverse translation candidates. To obtain diverse sequences, we can decode the most likely translations for different POS tag sequences. This is a more explicit and effective way of performing diverse translation than other methods based on diverse or re-ranking beam search  (Vijayakumar et al., 2018; Li and Jurafsky, 2016) , or coarse codes planning  (Shu and Nakayama, 2018) . We evaluate LaSyn on four translation tasks. Evaluations show that LaSyn outperforms models that only use partial exploration of the latent states for incorporating target side syntax. A diversity based evaluation also shows that when using different POS tag sequences during inference, LaSyn produces more diverse and meaningful translations compared to existing models. 

 A Latent Syntax Model for Decoding In a standard sequence-to-sequence model, the decoder directly predicts the target sequence y conditioned on the source input x. The translation probability P (y|x) is modeled directly using the probability of each target word y n at time step n conditioned on the source sequence x and the current partial target sequence y <n as follows: P (y|x; ?) = N n=1 P (y n |x, y <n ; ?) (1) where, ? denotes the parameters of both the encoder and the decoder. In this work, we model syntactic information of target tokens using an additional sequence of variables, which captures the syntactic choices 1 at   (Wang et al., 2018a; Wu et al., 2017; Aharoni and Goldberg, 2017) . (c) LaSyn, our latent syntax model that uses non-sequential latent variables for exhaustive search of latent states. each time step. There are multiple ways of incorporating this additional information in a sequenceto-sequence model. An ideal solution should capture the codependence between syntax and semantics. In a sequential translation process, the word choices at each time step depend on both the semantics and the syntax of the words generated at the previous time steps. The same dependence also holds for the syntactic choices at each time step. Figure  1a  shows a graphical model that captures this full co-dependence between the syntactic variable sequence z 1 , . . . , z N and the output word sequence y 1 , . . . , y N . Such a model can be implemented using two decoders, one to decode syntax and the other to decode output words. The main difficulty, however, is that inference in this model is intractable since it involves marginalizing over the latent z sequences. To keep inference tractable, existing approaches treat syntactic choices z as observed sequential variables  (G? et al., 2018; Wang et al., 2018a; Wu et al., 2017; Aharoni and Goldberg, 2017) , as shown in Figure  1b . These models use a two-stage decoding process, where for each time step they first produce most likely latent state z n and then use this as input to a second stage that decodes words. However, this process is unsatisfactory in two respects. First, the inference of syntactic choices is still approximate as it does not explore the full space of z. Second, these models are not well-suited for controllable or diverse translations. Using such a model to decode from an arbitrary z sequence is a divergence from its training, where it only learns to decode from a limited space of z sequences. 

 Model Description Our goal is to design a model that allows for exhaustive search over syntactic choices. We introduce LaSyn, a new latent model shown in Fig-  ure 1c . The syntactic choices are modeled as true latent variables i.e., unobserved variables. Compared to the ideal model in Figure  1a , LaSyn includes two simplifications for tractability: (i) The dependence between successive syntactic choices is modeled indirectly, via the word choices made in the previous time steps.  (ii)  The word choice at each position depends only on the syntactic choice at the current position and the previous predicted words. Dependence on previous syntactic choices is modeled indirectly. Under this model, the joint conditional probability of the target word y n together with its corresponding latent syntactic choice z n 2 is given by: P (y n , z n |x, y <n ) = P (y n |z n , x, y <n ) ? P (z n |x, y <n ) (2) We implement LaSyn by modifying the Transformer-based encoder-decoder architecture  (Vaswani et al., 2017) . As shown in Figure  2 , LaSyn consists of a shared encoder for encoding source sentence x and a hybrid decoder that manages the decoding of the latent sequence z (left branch) and the target sentence y (right branch) separately. The encoder consists of the standard selfattention layer, which generates representations of each token in the source sentence x. The hybrid decoder consists of a self-attention layer that encodes the output generated thus far (i.e., the partial translation), followed by a inter-attention layer  

 Inference with Exhaustive Search for Latent States When using additional variables to model target side syntax, exact inference requires marginalizing over these additional variables. P (y|x) = z?F (z) P (y|z, x)P (z|x) (3) To avoid this exponential complexity, prior works use a two-step decoding process with models similar to the one shown in Figure  1b . They use greedy or beam search to explore a subset B(z) of the latent space to compute the posterior distribution as follows: P (y|x) z?B(z) P (y|z, x)P (z|x) (4) Finding the most likely translation using LaSyn also requires marginalizing over the latent states. However, because the latent states in LaSyn don't directly depend on each other, we can exhaustively search over the latent states. In particular, we can show that when y is fixed (observed), the {z n } N n=1 variables are d-separated  (Bishop, 2006)  i.e., are mutually independent. As a result, the time complexity for searching latent sequence z drops from |V z | N to N |V z |. The posterior distribution for the translation probability at a time step n can be computed as follows: P (y n |x, y <n ) = zn?F (zn) P (y n , z n |x, y <n ) = zn?F (zn) P (y n |z n , x, y <n ) ? P (z n |x, y <n ) (5) where, F (z n ) is the full search space of latent states z n and the joint probability is factorized as specified in Equation  2 . For decoding words, we use standard beam search 3 with P (y n , z n |x, y <n ) as the beam cost. With this inference scheme, we can easily control decoding for diversity, by feeding different z sequences to the right branch of the decoder and decode diverse y n by directly using P (y n |z n , x, y <n ) as the beam cost. Unlike the two-step decoding models which only evaluate a small number of z n values at each time step (constrained by beam size), LaSyn evaluates all possible values for z n at each time step, while avoiding the evaluation of all possible sequences. 

 Training with Neural Expectation Maximization The log-likelihood of LaSyn's parameters ? 4 computed on one training pair x, y ? D is given by: L(?) = log P (y|x; ?) = N n=1 log P (y n |x, y <n ; ?) = N n=1 log zn?Vz P (y n , z n |x, y <n ) (6) Directly optimizing the log-likelihood function (equation 6) with respect to model parameter ? is challenging because of the highly non-convex function P (y n , z n |x, y <n ) and the marginalization over z n . 5 Alternatively, we optimize the system parameters by Expectation Maximization (EM). Using Jensen's inequality, equation (  6 ) can be re-written as: L(?) = N n=1 log zn?Vz Q(z n ) P (y n , z n |x, y <n ) Q(z n ) ? N n=1 zn?Vz Q(z n ) log P (y n , z n |x, y <n ) Q(z n ) =L lower (Q, ?) (7) where L lower (Q, ?) is the lower bound of the loglikelihood and Q is any auxiliary probability distribution defined on z n . ? is omitted from the expression for simplicity. We set Q(z n ) = P (z n |x, y ?n ; ? old ), the probability of the latent state computed by the decoder (shown in the left branch in Figure  2 ). Substituting this in equation (  7 ), we obtain the lower bound as L lower (Q, ?) = N n=1 zn?Vz P (z n |x, y ?n ; ? old ) ? log P (y n , z n |x, y <n ; ?) ? P (z n |x, y ?n ; ? old ) ? log P (z n |x, y ?n ; ? old ) = Q(?, ? old ) + C (8) where Q(?, ? old ) = N n=1 zn?Vz P (z n |x, y ?n ; ? old ) ? log P (y n , z n |x, y <n ; ?) (9) EM algorithm for optimizing Q(?, ? old ) consists of two major steps. In the E-step, we compute the posterior distribution of z n with respect to ? old by ?(z n = i) = P (z n = i|x, y ?n ) = P (y n , z n = i|x, y <n ) zn=j P (y n , z n = j|x, y <n ) (10) where ?(z n = i) is the responsibility of z n = i given y n , and can be calculated by equation (  2 ). In the M-step, we aim to find the configuration of ? that would maximize the expected loglikelihood using the posteriors computed in the Estep. In conventional EM algorithm for shallow probabilistic graphical model, the M-step is generally supposed to have closed-form solution. However, we model the probabilistic dependencies by deep neural networks, where Q(?, ? old ) is highly non-convex and non-linear with respect to network parameters ?. Therefore, there exists no analytical solution to maximize it. However, since deep neural network is differentiable, we can update ? by taking a gradient ascent step: ? new = ? old + ? ?Q(?, ? old ) ? , (11) The resulting algorithm belongs to the class of generalized EM algorithms and is guaranteed (for a sufficiently small learning rate ?) to converge to a (local) optimum of the data log likelihood  (Wu, 1983) . 

 Regularized EM training The EM training we derived does not assume any supervision for the latent variables z. This can be seen as inferring the latent syntax of the target sentences by clustering the target side tokens into |V z | different categories. Given some tokenlevel syntactic information, we can modify the training procedure to regularize the generation of latent sequence P (z n |x, y <n ) such that true latent sequences have higher probabilities under the model. In this work, we consider parts-of-speech sequences of the target sentences for regularization. The regularized EM training objective is thus redefined as L total (?) = L lower (?) + ?L z (?), (12) where L lower (?) is the EM lower bound in equation (7) and L z (?) denotes cross entropy loss between P (z n |x, y <n ) and the true POS tag sequences and ? is a hyper-parameter that controls the impact of the regularization.  

 Evaluation We evaluate LaSyn on four translation tasks, including three with moderate sized datasets IWSLT 2014  (Cettolo et al., 2015)  German-English (De-En), English-German (En-De), English-French (En-Fr), and one with a relatively larger dataset, the WMT 2014 English-German (En-De). We describe the datasets in more details in the appendix. We compare against three types of baselines: (i) general Seq2Seq models that use no syntactic information, (ii) models that incorporate source side syntax directly, and multitask learning models which include syntax indirectly, and (iii) models that use syntax on the target side. We also define a LaSyn Empirical Upper Bound (EUB), which is our proposed model using true POS tag sequences for inference. We use BLEU as the evaluation metric  (Papineni et al., 2002)  for translation quality. For diverse translation evaluation, we utilize distinct-1 score  as the evaluation metric, which is the number of distinct unigrams divided by total number of generated words. For all translation tasks, we choose the base configuration of Transformer with d model = 512. During training, we choose Adam optimizer (Kingma and Ba, 2015) with ? 1 = 0.9, ? 2 = 0.98 with initial learning rate is 0.0002 with 4000 warm-up steps. We describe additional implementation and training details in the Appendix. 

 Results on IWSLT'14 Tasks Table  1  compares LaSyn versions against some of the state-of-the-art models on the IWSLT'14 dataset. LaSyn-K rows show results when varying the number of EM update steps per batch (K). On the De-En task, LaSyn provides a 1.7 points improvements over the Transformer baseline, demonstrating that the LaSyn's improvements come from incorporating target side syntax effectively. This result is also better than a transformer-based source side syntax model by 1.5 points. LaSyn results are also better than the published numbers for LSTM-based models that use multi-task learning for source side and models that uses target side syntax. Note that since the underlying architectures are different, we only present these results to show that the results with LaSyn are comparable with other models that have incorporated syntax. On the En-De task, our model achieves 29.2 in terms of BLEU score, with 2.6 points improvement over the Transformer baseline and 2.4 points improvement over Transformer-based Source Side Syntax model. Compared with NPMT  (Huang et al., 2018) , which is a BiLSTM based model, we achieve 3.84 point improvement. On the En-Fr task, our model set a new state-ofthe-art with a BLEU score of 40.6, which is 1.7 points improvement over the second best model which uses Transformer to incorporate source side syntax knowledge. Our model also surpasses the basic Transformer model by about 2.1 points. We notice that across all tasks, the performance of our model improves with number of EM update steps per batch (K). With larger K values, we get better lower bounds L lower (?) on each training batch, thus leading to better optimization. For update steps beyond K > 5, the performance does not improve any more. Last, the EUB row indicates the performance that can be obtained when feeding in the true POS tags. The large improvement here shows the potential for improvement when modeling target side syntax. Table  2  shows one example where LaSyn produces correct translations for a long input sentence. The output of LaSyn is close to the reference and the output of LaSyn when given the gold POS tag sequence is even better, demonstrating the benefits of modeling syntax. The transformer model however fails to decode the later portions of the long input accurately. 

 Speed We compare the speeds of our (un-optimized) implementation of LaSyn with a vanilla transformer with no latents in its decoder. Table  3  shows the training time per epoch, and the inference time for the whole test set. computed on the IWSLT'14 De-En task. When K = 1, LaSyn takes almost twice as much time as the vanilla transformer for training. Increasing K increases training time further. For inference, LaSyn takes close to four times as much time compared to the vanilla Transformer. In terms of complexity, LaSyn only adds a linear term (in POS tag size to the decoding complexity. Specifically, its decoding complexity is B ? O(m) ? O(|V z | ? N ) where B is beam size, m is a constant proportional to the tag set size and N is output size. As the table shows, empirically, our current implementation incurs m 4. We leave further optimizations for future work. 

 Diversity We compare the diversity of translations using distinct-1 score , which is simply the number of distinct unigrams divided by total number of generated words. We use our model to generate 10 translations for each source sentence of the test dataset. We then compare our results with baseline Transformer. The result is shown in Table  4 . Much like translation quality, LaSyn's diversity increases with number of EM updates and is better than diversity of the transformer and a source side encoder model. 

 Controlling Diversity with POS Sequences One of the main strengths of LaSyn is that it can generate translations conditioned on a given POS sequence. First, we present some examples that we generate by decoding over different POS tag sequences. Given a source sentence, we use SRC letztes jahr habe ich diese beiden folien gezeigt , um zu veranschaulichen , dass die arktische eiskappe , die fr annhernd drei millionen jahre die grsse der unteren 48 staaten hatte , um 40 prozent geschrumpft ist . REF last year i showed these two slides so that demonstrate that the arctic ice cap , which for most of the last three million years has been the size of the lower 48 states , has shrunk by 40 percent . Transformer last year , i showed these two slides to illustrate that the arctic ice caps that had the size of the lower 48 million states to 40 percent . LaSyn last year , i showed these two slides to illustrate that the arctic ice cap , which for nearly three million years had the size of the lower 48 states , was shrunk by 40 percent . LaSyn (groundtrue POS) last year i showed these two slides just to illustrate that the arctic ice cap , which for nearly about the last three million years has been the size of the lower 48 states , has shrunk by 40 percent .  LaSyn to provide the most-likely target pos tag sequence. Then, we obtain a random set of valid POS tag sequences that differ from this maximum likely sequence by some edit distance. For each of these randomly sampled POS tag sequences, we let LaSyn generate a translation that fits the sequence. Table  5  shows some example sentences. LaSyn is able to generate diverse translations that reflect the sentence structure implied by the input POS tags. However, in trying to fit the translation into the specified sequence, it deviates somewhat from the ideal translation. To understand how diversity plays against translation quality, we also conduct a small scale quantitative evaluation. We pick a subset of the test dataset, and for each source sentence in this subset, we sample 10 POS tag sequences whose edit distance to their corresponding Top-1 POS tag sequence equal to a specific value, we then use them  to decode W translations. We calculate their final BLEU and distinct-1 scores. The results are shown in Figure  :  3. As the edit distance increases, diversity increases dramatically but at the cost of translation quality. Since the POS tag sequence acts as a template for generation, as we move further from the most likely template, the model struggles to fit the content accurately. Understanding this tradeoff can be useful for re-ranking or other scoring functions. 

 Results on WMT'14 En-De To assess the impact on a larger dataset, we show results on the WMT'14 English-German in table 6. Compared to the previously reported systems, we see that our transformer implementation is a strong baseline. LaSyn produces small gains, with the best gain at K=5 -a BLEU score improvement of 0.6. This demonstrates that syntactic information can contribute more to the increase of translation quality on a smaller dataset. 

 Related Work Attention-based Neural Machine Translation (NMT) models have shown promising results in 

 Model BLEU BiRNN+GCN  (Bastings et al., 2017)  23.9 ConvS2S  (Gehring et al., 2017)  25.16 MoE  26.03 Transformer (base) 27.3 LaSyn (K=1) (base) 27.6 LaSyn (K=3) (base) 27.8 LaSyn (K=5) (base) 27.9 Table  6 : WMT'14 English-German results -shown are the BLEU scores of various models on TED talks translation tasks. We highlight the best model in bold. various large scale translation tasks  (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017)  using an Seq2Seq structure. Many Statistical Machine Translation (SMT) approaches have leveraged benefits from modeling syntactic information  (Chiang et al., 2009; Huang and Knight, 2006; Shen et al., 2008) . Recent efforts have demonstrated that incorporating syntax can also be useful in neural methods as well. One branch uses features on the source side to help improve the translation performance  Morishita et al., 2018; Eriguchi et al., 2016) .  explored linguistic features like lemmas, morphological features, POS tags and dependency labels and concatenate their embeddings with sentence features to improve the translation quality. In a similar vein,  Morishita et al. (2018)  and  Eriguchi et al. (2016) , incorporated hierarchical subword features and phrase structure into the source side representations. Despite the promising improvements, these approaches are limited in that the trained translation model requires the availability of external tools during inference -the source text needs to be processed first to extract syntactic structure  (Eriguchi et al., 2017) . Another branch uses multitask learning, where the encoder of the NMT model is trained to produce multiple tasks such as POS tagging, namedentity recognition, syntactic parsing or semantic parsing  (Eriguchi et al., 2017; Baniata et al., 2018; Niehues and Cho, 2017; Zaremoodi et al., 2018) . These can be seen as models that implicitly generate syntax informed representations during encoding. With careful model selection, these methods have demonstrate some benefits in NMT. The third branch directly models the syntax of the target sentence during decoding  (G? et al., 2018; Wang et al., 2018a; Wu et al., 2017; Aharoni and Goldberg, 2017; Bastings et al., 2017; .  Aharoni et al. (2017)  treated constituency trees as sequential strings and trained a Seq2Seq model to translate source sentences into these tree sequences.  Wang et al. (2018a) and Wu et al. (2017)  proposed to use two RNNs, a Rule RNN and a Word RNN, to generate a target sentence and its corresponding tree structure.  Gu et al. (2018)  proposed a model to translate and parse at the same time. However, apart from the complex tree structure to model, they all have a similar architecture as shown in Figure  1b , which limits them to only exploring a small portion of the syntactic space during inference. LaSyn uses simpler parts-of-speech information in a latent syntax model, avoiding the typical exponential search complexity in the latent space with a linear search complexity and is optimized by EM. This allows for better translation quality as well as diversity. Similar to our work,  (Shankar et al., 2018)  and  (Shankar and Sarawagi, 2019)  proposed a latent attention mechanism to further reduce the complexity of model implementation by taking a top-K approximation instead of the EM algorithm as in LaSyn. 

 Conclusion Modeling target-side syntax through true latent variables is difficult because of the additional inference complexity. In this work, we presented LaSyn, a latent syntax model that allows for efficient exploration of a large space of latent sequences. This yields significant gains on four translation tasks, IWSLT'14 English-German, German-English, English-French and WMT'14 English-German. The model also allows for better decoding of diverse translation candidates. This work only explored parts-of-speech sequences for syntax. Further extensions are needed to tackle tree-structured syntax information. Figure1: Target-side Syntax Models: (a) An ideal solution that captures full co-dependence between syntax and semantics. (b) A widely-used two-step decoding model (Wang et al., 2018a;  Wu et al., 2017; Aharoni and Goldberg, 2017) . (c) LaSyn, our latent syntax model that uses non-sequential latent variables for exhaustive search of latent states. 
