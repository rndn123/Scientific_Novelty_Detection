title
The RWTH Aachen Machine Translation System for WMT 2012

abstract
This paper describes the statistical machine translation (SMT) systems developed at RWTH Aachen University for the translation task of the NAACL 2012 Seventh Workshop on Statistical Machine Translation (WMT 2012). We participated in the evaluation campaign for the French-English and German-English language pairs in both translation directions. Both hierarchical and phrase-based SMT systems are applied. A number of different techniques are evaluated, including an insertion model, different lexical smoothing methods, a discriminative reordering extension for the hierarchical system, reverse translation, and system combination. By application of these methods we achieve considerable improvements over the respective baseline systems.

Introduction For the WMT 2012 shared translation task 1 RWTH utilized state-of-the-art phrase-based and hierarchical translation systems as well as an in-house system combination framework. We give a survey of these systems and the basic methods they implement in Section 2. For both the French-English (Section 3) and the German-English (Section 4) language pair, we investigate several different advanced techniques. We concentrate on specific research directions for each of the translation tasks and present the respective techniques along with the empirical results they yield: For the French?English task (Section 3.1), we apply a standard phrase-based system. 1 http://www.statmt.org/wmt12/ translation-task.html For the English?French task (Section 3.2), we augment a hierarchical phrase-based setup with a number of enhancements like an insertion model, different lexical smoothing methods, and a discriminative reordering extension. For the German?English (Section 4.3) and English?German (Section 4.4) tasks, we utilize morpho-syntactic analysis to preprocess the data (Section 4.1) and employ system combination to produce a consensus hypothesis from normal and reverse translations (Section 4.2) of phrase-based and hierarchical phrase-based setups. 

 Translation Systems 

 Phrase-Based System The phrase-based translation (PBT) system used in this work is an in-house implementation of the state-of-the-art decoder described in  (Zens and Ney, 2008) . We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distance-based distortion model, an n-gram target language model and three binary count features. The parameter weights are optimized with minimum error rate training (MERT)  (Och, 2003) . 

 Hierarchical Phrase-Based System For our hierarchical phrase-based translation (HPBT) setups, we employ the open source translation toolkit Jane  Vilar et al., 2012) , which has been developed at RWTH and is freely available for non-commercial use. In hierarchical phrase-based translation  (Chiang, 2007) , a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is carried out with a parsing-based procedure. The standard models integrated into our Jane systems are: phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, four binary count features, and an n-gram language model. Optional additional models comprise IBM model 1  (Brown et al., 1993) , discriminative word lexicon (DWL) models and triplet lexicon models  (Mauser et al., 2009) , discriminative reordering extensions  (Huck et al., 2011a) , insertion and deletion models , and several syntactic enhancements like preference grammars  and string-to-dependency features  (Peter et al., 2011) . We utilize the cube pruning algorithm  (Huang and Chiang, 2007)  for decoding and optimize the model weights with MERT. 

 System Combination System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines. The basic concept of RWTH's approach to machine translation system combination is described in  (Matusov et al., 2006; Matusov et al., 2008) . This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 

 Other Tools and Techniques We employ GIZA++  (Och and Ney, 2003)  to train word alignments. The two trained alignments are heuristically merged to obtain a symmetrized word alignment for phrase extraction. All language models (LMs) are created with the SRILM toolkit  (Stolcke, 2002)  and are standard 4-gram LMs with interpolated modified Kneser-Ney smoothing  (Kneser and Ney, 1995; Chen and Goodman, 1998) . We evaluate in truecase, using the BLEU  (Papineni et al., 2002)  and TER  (Snover et al., 2006)   3 French-English Setups We trained phrase-based translation systems for French?English and hierarchical phrase-based translation systems for English?French. Corpus statistics for the French-English parallel data are given in Table  1 . The LMs are 4-grams trained on the provided resources for the respective language (Europarl, News Commentary, UN, 10 9 , and monolingual News Crawl language model training data). 2 For French?English we also investigate a smaller English LM on Europarl and News Commentary data only. For English?French we experiment with additional target-side data from the LDC French Gigaword Second Edition (LDC2009T28), which is an archive of newswire text data that has been acquired over several years by the LDC.  3  The LDC French Gigaword v2 is permitted for constrained submissions in the WMT shared translation task. As a development set for MERT, we use newstest2009 in all setups. 

 Experimental Results French?English For the French?English task, the phrase-based SMT system (PBT) is set up using the standard models listed in Section 2.1. We vary the training data we use to train the system and compare the results. It should be noted that these setups do not use any English LDC Gigaword data for LM training at all. Our baseline system uses the Europarl and News Commentary data for training LM and phrase table. Corpus statistics are shown in the "EP+NC" section of Table  1 . This results in a performance of 24.7 points BLEU on newstest2011. Then we add the 10 9 as well as UN data and more monolingual English data from the News Crawl corpus to the data used for training the language model. This system obtains a score of 27.7 points BLEU on newstest2011. Our final system uses Europarl, News Commentary, 10 9 and UN data and News Crawl monolingual data for LM training and the Europarl, News Commentary and 10 9 data (Table  1 ) for phrase table training. Using these data sets the system reaches 29.1 points BLEU. The experimental results are summarized in Table 2. 

 Experimental Results English?French For the English?French task, the baseline system is a hierarchical phrase-based setup including the standard models as listed in Section 2.2, apart from the binary count features. We limit the recursion depth for hierarchical rules with a shallow-1 grammar  (de Gispert et al., 2010) . In a shallow-1 grammar, the generic non-terminal X of the standard hierarchical approach is replaced by two distinct non-terminals XH and XP . By changing the left-hand sides of the rules, lexical phrases are allowed to be derived from XP only, hierarchical phrases from XH only. On all right-hand sides of hierarchical rules, the X is replaced by XP . Gaps within hierarchical phrases can thus solely be filled with purely lexicalized phrases, but not a second time with hierarchical phrases. The initial rule is substituted with S ? XP ?0 , XP ?0 S ? XH ?0 , XH ?0 , (1) and the glue rule is substituted with S ? S ?0 XP ?1 , S ?0 XP ?1 S ? S ?0 XH ?1 , S ?0 XH ?1 . (2) The main benefit of a restriction of the recursion depth is a gain in decoding efficiency, thus allowing us to set up systems more rapidly and to explore more model combinations and more system configurations. The experimental results for English?French are given in Table  3 . Starting from the shallow hierarchical baseline setup on Europarl and News Commentary parallel data only (but Europarl, News Commentary, 10 9 , UN, and News Crawl data for LM training), we are able to improve translation quality considerably by first adopting more parallel (10 9 and UN) and monolingual (French LDC Gigaword v2) training resources and then employing several different models that are not included in the baseline already. We proceed with individual descriptions of the methods we use and report their respective effect in BLEU on the test sets. 10 9 and UN (up to +2.5 points BLEU) While the amount of provided parallel data from Europarl and News Commentary sources is rather limited (around 2M sentence pairs in total), the UN and the 10 9 corpus each provide a substantial collection of further training material. By appending both corpora, we end up at roughly 35M parallel sentences (cf. Table  1 ). We utilize this full amount of data in our system, but extract a phrase table with only lexical (i.e. nonhierarchical) phrases from the full parallel data. We add it as a second phrase table to the baseline system, with a binary feature that enables the system to reward or penalize the application of phrases from this table. LDC Gigaword v2 (up to +0.5 points BLEU) The LDC French Gigaword Second Edition (LDC2009T28) provides some more monolingual French resources. We include a total of 28.2M sentences from both the AFP and APW collections in our LM training data. insertion model (up to +0.4 points BLEU) We add an insertion model to the log-linear model combination. This model is designed as a means to avoid the omission of content words in the hypotheses. It is implemented as a phrase-level feature function which counts the number of inserted words. We apply the model in source-totarget and target-to-source direction. A targetside word is considered inserted based on lexical probabilities with the words on the foreign language side of the phrase, and vice versa for a source-side word. As thresholds, we compute individual arithmetic averages for each word from the vocabulary . noisy-or lexical scores (up to +0.4 points BLEU) In our baseline system, the t Norm (?) lexical scoring variant as described in  (Huck et al., 2011a)  is employed with a relative frequency (RF) lexicon model for phrase table smoothing. The single-word based translation probabilities of the RF lexicon model are extracted from wordaligned parallel training data, in the fashion of . We exchange the baseline lexical scoring with a noisy-or  (Zens and Ney, 2004 ) lexical scoring variant t NoisyOr (?). DWL (up to +0.3 points BLEU) We augment our system with phrase-level lexical scores from discriminative word lexicon (DWL) models  (Mauser et al., 2009; Huck et al., 2011a)  in both source-to-target and target-to-source direction. The DWLs are trained on News Commentary data only. IBM-1 (up to +0.1 points BLEU) On News Commentary and Europarl data, we train IBM model-1  (Brown et al., 1993)  lexicons in both translation directions and also use them to compute phrase-level scores. discrim. RO (up to +0.4 points BLEU) The modification of the grammar to a shallow-1 version restricts the search space of the decoder and is convenient to prevent overgeneration. In order not to be too restrictive, we reintroduce more flexibility into the search process by extending the grammar with specific reordering rules XP ? XP ?0 XP ?1 , XP ?1 XP ?0 XP ? XP ?0 XP ?1 , XP ?0 XP ?1 . (3) The upper rule in Equation (  3 ) is a swap rule that allows adjacent lexical phrases to be transposed, the lower rule is added for symmetry reasons, in particular because sequences assembled with these rules are allowed to fill gaps within hierarchical phrases. Note that we apply a length constraint of 10 to the number of terminals spanned by an XP . We introduce two binary indicator features, one for each of the two rules in Equation (  3 ). In addition to adding  these rules, a discriminatively trained lexicalized reordering model is applied . 

 German-English Setups We trained phrase-based and hierarchical translation systems for both translation directions of the German-English language pair. Corpus statistics for German-English can be found in Table  4 . The language models are 4-grams trained on the respective target side of the bilingual data as well as on the provided News Crawl corpus. For the English language model the 10 9 French-English, UN and LDC Gigaword Fourth Edition corpora are used additionally. For the 10 9 French-English, UN and LDC Gigaword corpora we apply the data selection technique described in (Moore and Lewis, 2010). We examine two different language models, one with LDC data and one without. All German?English systems are optimized on newstest2010. For English?German, we use newstest2009 as development set. The news-test2011 set is used as test set and the scores for new-stest2008 are included for completeness. 

 Morpho-Syntactic Analysis In order to reduce the source vocabulary size for the German?English translation, the German text is preprocessed by splitting German compound words with the frequency-based method described in  (Koehn and Knight, 2003) . To further reduce translation complexity of PBT, we employ the long-range part-of-speech based reordering rules proposed by  Popovi? and Ney (2006) . 

 Reverse Translation For reverse translations we need to change the word order of the bilingual corpus. For example, if we re-verse both source and target language, the original training example "der Hund mag die Katze . ? the dog likes the cat ." is converted into a new training example ". Katze die mag Hund der ? . cat the likes dog the". We call this type of modification of source or target language reversion. A system trained of this data is called reverse. This modification changes the corpora and hence the language model and alignment training produce different results. 

 Experimental Results German?English Our results for the German?English task are shown in Table  5 . For this task, we apply the idea of reverse translation for both the phrase-based and the hierarchical approach. It seems that the reversed systems perform slightly worse. However, when we employ system combination using both reverse translation setups (PBT reverse and HPBT reverse) and both baseline setups (PBT baseline and HPBT baseline), the translation quality is improved by up to 0.4 points in BLEU and 1.0 points TER compared to the best single system. The addition of LDC Gigaword corpora (+GW) to the language model training data of the baseline setups shows improvements in both BLEU and TER. Furthermore, with the system combination including these setups, we are able to report an improvement of up to 0.7 points BLEU and 1.0 points TER over the best single setup. Compared to the system combination based on systems which are not using the LDC Gigaword corpora, we gain 0.3 points in BLEU and 0.4 points in TER. 

 Experimental Results English?German Our results for the English?German task are shown in Table  6 . For this task, we first compare systems using one, two or three language models of different parts of the data. The language model for systems with only one language model is created with all monolingual and parallel data. A language model with all monolingual data and a language model with all parallel data is created for the systems with two language models. For the systems with three language models, we also split the parallel data in two parts consisting of either only Europarl data or only News Commentary data. For PBT the system with two language models performs best for all test sets. Further, we apply the idea of reverse translation for both the phrase-based and the hierarchical approach. The PBT reverse 2 LM systems perform slightly worse compared to PBT baseline 2 LM. The HPBT reverse 2 LM performs better compared to HPBT baseline 2 LM. When we employ system combination using both reverse translation setups (PBT reverse 2 LM and HPBT reverse 2 LM) and both baseline setups (PBT baseline 2 LM and HPBT baseline 2 LM), the translation quality is improved by up to 0.2 points in BLEU and 2.1 points in TER compared to the best single system. 

 Conclusion For the participation in the WMT 2012 shared translation task, RWTH experimented with both phrasebased and hierarchical translation systems. Several different techniques were evaluated and yielded considerable improvements over the respective base-line systems as well as over our last year's setups  (Huck et al., 2011b) . Among these techniques are an insertion model, the noisy-or lexical scoring variant, additional phrase-level lexical scores from IBM model 1 and discriminative word lexicon models, a discriminative reordering extension for hierarchical translation, reverse translation, and system combination. measures. French English EP + NC Sentences 2.1M Running Words 63.3M 57.6M Vocabulary 147.8K 128.5K Singletons 5.4K 5.1K + 10 9 Sentences 22.9M Running Words 728.6M 624.0M Vocabulary 1.7M 1.7M Singletons 0.8M 0.8M + UN Sentences 35.4M Running Words 1 113.5M 956.4M Vocabulary 1.9M 2.0M Singletons 0.9M 1.0M Table 1: Corpus statistics of the preprocessed French- English parallel training data. EP denotes Europarl, NC denotes News Commentary. In the data, numerical quan- tities have been replaced by a single category symbol. 
