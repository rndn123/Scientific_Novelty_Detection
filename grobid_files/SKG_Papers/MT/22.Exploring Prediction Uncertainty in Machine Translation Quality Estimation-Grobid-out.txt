title
Exploring Prediction Uncertainty in Machine Translation Quality Estimation

abstract
Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.

Introduction Quality Estimation (QE)  (Blatz et al., 2004; Specia et al., 2009 ) models aim at predicting the quality of automatically translated text segments. Traditionally, these models provide point estimates and are evaluated using metrics like Mean Absolute Error (MAE), Root-Mean-Square Error (RMSE) and Pearson's r correlation coefficient. However, in practice QE models are built for use in decision making in large workflows involving Machine Translation (MT). In these settings, relying on point estimates would mean that only very accurate prediction models can be useful in practice. A way to improve decision making based on quality predictions is to explore uncertainty estimates. Consider for example a post-editing scenario where professional translators use MT in an effort to speed-up the translation process. A QE model can be used to determine if an MT segment is good enough for post-editing or should be discarded and translated from scratch. But since QE models are not perfect they can end up allowing bad MT segments to go through for postediting because of a prediction error. In such a scenario, having an uncertainty estimate for the prediction can provide additional information for the filtering decision. For instance, in order to ensure good user experience for the human translator and maximise translation productivity, an MT segment could be forwarded for post-editing only if a QE model assigns a high quality score with low uncertainty (high confidence). Such a decision process is not possible with point estimates only. Good uncertainty estimates can be acquired from well-calibrated probability distributions over the quality predictions. In QE, arguably the most successful probabilistic models are Gaussian Processes (GPs) since they considered the state-ofthe-art for regression  Hensman et al., 2013) , especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by  Qui?onero-Candela et al. (2006) , which defined new evaluation metrics that take into account probability distributions over predictions. The remaining of this paper is organised as follows: ? In Section 2 we further motivate the use of GPs for uncertainty modelling in QE and revisit their underlying theory. We also propose some model extensions previously developed in the GP literature and argue they are more appropriate for the task. ? We intrinsically evaluate our proposed models in terms of their posterior distributions on training and test data in Section 3. Specifically, we show that differences in uncertainty modelling are not captured by the usual point estimate metrics commonly used for this task. ? As an example of an application for predicitive distributions, in Section 4 we show how they can be useful in scenarios with asymmetric risk and how the proposed models can provide better performance in this case. We discuss related work in Section 5 and give conclusions and avenues for future work in Section 6. While we focus on QE as application, the methods we explore in this paper can be applied to any text regression task where modelling predictive uncertainty is useful, either in human decision making or by propagating this information for further computational processing. 

 Probabilistic Models for QE Traditionally, QE is treated as a regression task with hand-crafted features. Kernel methods are arguably the state-of-the-art in QE since they can easily model non-linearities in the data. Furthermore, the scalability issues that arise in kernel methods do not tend to affect QE in practice since the datasets are usually small, in the order of thousands of instances. The most popular method for QE is Support Vector Regression (SVR), as shown in the multiple instances of the WMT QE shared tasks  (Callisonburch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015) . While SVR models can generate competitive predictions for this task, they lack a probabilistic interpretation, which makes it hard to extract uncertainty estimates using them. Bootstrapping approaches like bagging  (Abe and Mamitsuka, 1998 ) can be applied, but this requires setting and optimising hyperparameters like bag size and number of bootstraps. There is also no guarantee these estimates come from a well-calibrated probabilistic distribution. Gaussian Processes (GPs)  (Rasmussen and Williams, 2006 ) is an alternative kernel-based framework that gives competitive results for point estimates  Shah et al., 2013; Beck et al., 2014b) . Unlike SVR, they explicitly model uncertainty in the data and in the predictions. This makes GPs very applicable when well-calibrated uncertainty estimates are required. Furthermore, they are very flexible in terms of modelling decisions by allowing the use of a variety of kernels and likelihoods while providing efficient ways of doing model selection. Therefore, in this work we focus on GPs for probabilistic modelling of QE. In what follows we briefly describe the GPs framework for regression. 

 Gaussian Process Regression Here we follow closely the definition of GPs given by  Rasmussen and Williams (2006) . Let X = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )} be our data, where each x ? R D is a D-dimensional input and y is its corresponding response variable. A GP is defined as a stochastic model over the latent function f that generates the data X : f (x) ? GP(m(x), k(x, x )), where m(x) is the mean function, which is usually the 0 constant, and k(x, x ) is the kernel or covariance function, which describes the covariance between values of f at the different locations of x and x . The prior is combined with a likelihood via Bayes' rule to obtain a posterior over the latent function: p(f |X ) = p(y|X, f )p(f ) p(y|X) , where X and y are the training inputs and response variables, respectively. For regression, we assume that each y i = f (x i ) + ?, where ? ? N (0, ? 2 n ) is added white noise. Having a Gaussian likelihood results in a closed form solution for the posterior. Training a GP involves the optimisation of model hyperparameters, which is done by maximising the marginal likelihood p(y|X) via gradient ascent. Predictive posteriors for unseen x * are obtained by integrating over the latent function evaluations at x * . GPs can be extended in many different ways by applying different kernels, likelihoods and modifying the posterior, for instance. In the next Sections, we explain in detail some sensible modelling choices in applying GPs for QE. 

 Mat?rn Kernels Choosing an appropriate kernel is a crucial step in defining a GP model (and any other kernel method). A common choice is to employ the exponentiated quadratic (EQ) kernel 1 : k EQ (x, x ) = ? v exp(? r 2 2 ) , where r 2 = D i=1 (x i ? x i ) 2 l 2 i is the scaled distance between the two inputs, ? v is a scale hyperparameter and l is a vector of lengthscales. Most kernel methods tie all lengthscale to a single value, resulting in an isotropic kernel. However, since in GPs hyperparameter optimisation can be done efficiently, it is common to employ one lengthscale per feature, a method called Automatic Relevance Determination (ARD). The EQ kernel allows the modelling of nonlinearities between the inputs and the response variables but it makes a strong assumption: it generates smooth, infinitely differentiable functions. This assumption can be too strong for noisy data. An alternative is the Mat?rn class of kernels, which relax the smoothness assumption by modelling functions which are ?-times differentiable only. Common values for ? are the half-integers 3/2 and 5/2, resulting in the following Mat?rn kernels: k M32 = ? v (1 + ? 3r 2 ) exp(? ? 3r 2 ) k M52 = ? v 1 + ? 5r 2 + 5r 2 3 exp(? ? 5r 2 ) , where we have omitted the dependence of k M32 and k M52 on the inputs (x, x ) for brevity. Higher values for ? are usually not very useful since the resulting behaviour is hard to distinguish from limit case ? ? ?, which retrieves the EQ kernel  (Rasmussen and Williams, 2006, Sec. 4.2) . The relaxed smoothness assumptions from the Mat?rn kernels makes them promising candidates for QE datasets, which tend to be very noisy. We expect that employing them will result in a better models for this application. 

 Warped Gaussian Processes The Gaussian likelihood of standard GPs has support over the entire real number line. However, common quality scores are strictly positive values, which means that the Gaussian assumption is not ideal. A usual way to deal with this problem is model the logarithm of the response variables, since this transformation maps strictly positive values to the real line. However, there is no reason to believe this is the best possible mapping: a better idea would be to learn it from the data. Warped GPs  (Snelson et al., 2004 ) are an extension of GPs that allows the learning of arbitrary mappings. It does that by placing a monotonic warping function over the observations and modelling the warped values inside a standard GP. The posterior distribution is obtained by applying a change of variables: p(y * |x * ) = f (y * ) 2? 2 * exp f (y * ) ? ? * 2? * , where ? * and ? * are the mean and standard deviation of the latent (warped) response variable and f and f are the warping function and its derivative. Point predictions from this model depend on the loss function to be minimised. For absolute error, the median is the optimal value while for squared error it is the mean of the posterior. In standard GPs, since the posterior is Gaussian the median and mean coincide but this in general is not the case for a Warped GP posterior. The median can be easily obtained by applying the inverse warping function to the latent median: y med * = f ?1 (? * ). While the inverse of the warping function is usually not available in closed form, we can use its gradient to have a numerical estimate. The mean is obtained by integrating y * over the latent density: E[y * ] = f ?1 (z)N z (? * , ? 2 * )dz, where z is the latent variable. This can be easily approximated using Gauss-Hermite quadrature since it is a one dimensional integral over a Gaussian density. The warping function should be flexible enough to allow the learning of complex mappings, but it needs to be monotonic.  Snelson et al. (2004)  proposes a parametric form composed of a sum of tanh functions, similar to a neural network layer: f (y) = y + I i=1 a i tanh(b i (y + c i )), where I is the number of tanh terms and a, b and c are treated as model hyperparameters and optimised jointly with the kernel and likelihood hyperparameters. Large values for I allow more complex mappings to be learned but raise the risk of overfitting. Warped GPs provide an easy and elegant way to model response variables with non-Gaussian behaviour within the GP framework. In our experiments we explore models employing warping functions with up to 3 terms, which is the value recommended by  Snelson et al. (2004) . We also report results using the f (y) = log(y) warping function. 

 Intrinsic Uncertainty Evaluation Given a set of different probabilistic QE models, we are interested in evaluating the performance of these models, while also taking their uncertainty into account, particularly to distinguish among models with seemingly same or similar performance. A straightforward way to measure the performance of a probabilistic model is to inspect its negative (log) marginal likelihood. This measure, however, does not capture if a model overfit the training data. We can have a better generalisation measure by calculating the likelihood on test data instead. This was proposed in previous work and it is called Negative Log Predictive Density (NLPD)  (Qui?onero-Candela et al., 2006) : NLPD(?, y) = ? 1 n n i=1 log p(? i = y i |x i ). where ? is a set of test predictions, y is the set of true labels and n is the test set size. This metric has since been largely adopted by the ML community when evaluating GPs and other probabilistic models for regression (see Section 5 for some examples). As with other error metrics, lower values are better. Intuitively, if two models produce equally incorrect predictions but they have different uncertainty estimates, NLPD will penalise the overconfident model more than the underconfident one. On the other hand, if predictions are close to the true value then NLPD will penalise the underconfident model instead. In our first set of experiments we evaluate models proposed in Section 2 according to their negative log likelihood (NLL) and the NLPD on test data. We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson's r, which has recently proposed by  Graham (2015)  as a more robust alternative. 

 Experimental Settings Our experiments comprise datasets containing three different language pairs, where the label to predict is post-editing time: English-Spanish (en-es) This dataset was used in the WMT14 QE shared task  (Bojar et al., 2014) . It contains 858 sentences translated by one MT system and post-edited by a professional translator. French-English (fr-en) Described in  (Specia, 2011) , this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator. English-German (en-de) This dataset is part of the WMT16 QE shared task 2 . It was translated by one MT system for consistency we use a subset of 2, 828 instances post-edited by a single professional translator. As part of the process of creating these datasets, post-editing time was logged on an sentence basis for all datasets. Following common practice, we normalise the post-editing time by the length of the machine translated sentence to obtain postediting rates and use these as our response variables. Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER)  (Snover et al., 2006) . Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort  (Koponen et al., 2012) . Additionally, time is more directly applicable in real translation environments -where uncertainty estimates could be useful, as it relates directly to productivity measures. For model building, we use a standard set of 17 features from the QuEst framework . These features are used in the strong baseline models provided by the WMT QE shared tasks. While the best performing systems in the shared tasks use larger feature sets, these are mostly resource-intensive and languagedependent, and therefore not equally applicable to all our language pairs. Moreover, our goal is to compare probabilistic QE models through the predictive uncertainty perspective, rather than improving the state-of-the-art in terms of point predictions. We perform 10-fold cross validation instead of using a single train/test splits and report averaged metric scores. The model hyperparameters were optimised by maximising the likelihood on the training data. We perform a two-pass procedure similar to that in : first we employ an isotropic kernel and optimise all hyperparameters using 10 random restarts; then we move to an ARD equivalent kernel and perform a final optimisation step to fine tune feature lengthscales. Point predictions were fixed as the median of the distribution. 

 Results and Discussion Table  1  shows the results obtained for all datasets. The first two columns shows an interesting finding in terms of model learning: using a warping function drastically decreases both NLL and NLPD. The main reason behind this is that standard GPs distribute probability mass over negative values, while the warped models do not. For the fr-en and en-de datasets, NLL and NLPD follow similar trends. This means that we can trust NLL as a measure of uncertainty for these datasets. However, this is not observed in the en-es dataset. Since this dataset is considerably smaller than the others, we believe this is evidence of overfitting, thus showing that NLL is not a reliable metric for small datasets. In terms of different warping functions, using the parametric tanh function with 3 terms performs better than the log for the fr-en and en-de datasets. This is not the case of the en-es dataset, where the log function tends to perform better. We believe that this is again due to the smaller dataset size. The gains from using a Mat?rn kernel over EQ are less conclusive. While they tend to perform better for fr-en, there does not seem to be any difference in the other datasets. Different kernels can be more appropriate depending on the language pair, but more experiments are needed to verify this, which we leave for future work. The differences in uncertainty modelling are by and large not captured by the point estimate metrics. While MAE does show gains from standard to Warped GPs, it does not reflect the difference found between warping functions for fr-en. Pearson's r is also quite inconclusive in this sense, except for some observed gains for en-es. This shows that NLPD indeed should be preferred as a evaluation metric when proper prediction uncertainty estimates are required by a QE model. 

 Qualitative Analysis To obtain more insights about the performance in uncertainty modelling we inspected the predictive distributions for two sentence pairs in the fr-en dataset. We show the distributions for a standard GP and a Warped GP with a tanh3 function in Fig-  ure  1. In the first case, where both models give accurate predictions, we see that the Warped GP distribution is peaked around the predicted value, as it should be. It also gives more probability mass to positive values, showing that the model is able to learn that the label is non-negative. In the second case we analyse the distributions when both models make inaccurate predictions. We can see that the Warped GP is able to give a broader distribution in this case, while still keeping most of the mass outside the negative range. We also report above each plot in Figure  1  the NLPD for each prediction. Comparing only the Warped GP predictions, we can see that their values reflect the fact that we prefer sharp distributions when predictions are accurate and broader ones when predictions are not accurate. However, it is interesting to see that the metric also penalises predictions when their distributions are too broad, as it is the case with the standard GPs since they can not discriminate between positive and negative values as well as the Warped GPs. Inspecting the resulting warping functions can bring additional modelling insights. In Figure  2  we show instances of tanh3 warping functions learned from the three datasets and compare them with the log warping function. We can see that the parametric tanh3 model is able to learn nontrivial mappings. For instance, in the en-es case the learned function is roughly logarithmic in the low scales but it switches to a linear mapping after y = 4. Notice also the difference in the scales, which means that the optimal model uses a latent Gaussian with a larger variance.  

 Asymmetric Risk Scenarios Evaluation metrics for QE, including those used in the WMT QE shared tasks, are assumed to be symmetric, i.e., they penalise over and underestimates equally. This assumption is however too simplistic for many possible applications of QE. For example: ? In a post-editing scenario, a project manager may have translators with limited expertise in post-editing. In this case, automatic translations should not be provided to the translator unless they are highly likely to have very good quality. This can be enforced this by increasing the penalisation weight for underestimates. We call this the pessimistic scenario. ? In a gisting scenario, a company wants to automatically translate their product reviews so that they can be published in a foreign language without human intervention. The company would prefer to publish only the reviews translated well enough, but having more reviews published will increase the chances of selling products. In this case, having better recall is more important and thus only reviews with very poor translation quality should be discarded. We can accomplish this by heavier penalisation on overestimates, a scenario we call optimistic. In this Section we show how these scenarios can be addressed by well-calibrated predictive distributions and by employing asymmetric loss functions. An example of such a function is the asymmetric linear (henceforth, AL) loss, which is a generalisation of the absolute error: L(?, y) = w(? ? y) if ? > y y ? ? if ? ? y, where w > 0 is the weight given to overestimates. If w > 1 we have the pessimistic scenario, and the optimistic one can be obtained using 0 < w < 1. For w = 1 we retrieve the original absolute error loss. Another asymmetric loss is the linear exponential or linex loss  (Zellner, 1986) : L(?, y) = exp[w(? ? y)] ? (? ? y) ? 1 where w ? R is the weight. This loss attempts to keep a linear penalty in lesser risk regions, while imposing an exponential penalty in the higher risk ones. Negative values for w will result in a pessimistic setting, while positive values will result in the optimistic one. For w = 0, the loss approximates a squared error loss. Usual values for w tend to be close to 1 or ?1 since for higher weights the loss can quickly reach very large scores. Both losses are shown on Figure  3 . Figure  3 : Asymmetric losses. These curves correspond to the pessimistic scenario since they impose larger penalties when the prediction is lower than the true label. In the optimistic scenario the curves would be reflected with respect to the vertical axis. 

 Bayes Risk for Asymmetric Losses The losses introduced above can be incorporated directly into learning algorithms to obtain models for a given scenario. In the context of the AL loss this is called quantile regression  (Koenker, 2005) , since optimal estimators for this loss are posterior quantiles. However, in a production environment the loss can change over time. For instance, in the gisting scenario discussed above the parameter w could be changed based on feedback from indicators of sales revenue or user experience. If the loss is attached to the underlying learning algorithms, a change in w would require full model retraining, which can be costly. Instead of retraining the model every time there is a different loss, we can train a single probabilistic model and derive Bayes risk estimators for the loss we are interested in. This allows estimates to be obtained without having to retrain models when the loss changes. Additionally, this allows different losses/scenarios to be employed at the same time using the same model. Minimum Bayes risk estimators for asymmetric losses were proposed by  Christoffersen and Diebold (1997)  and we follow their derivations in our experiments. The best estimator for the AL loss is equivalent to the w w+1 quantile of the predictive distribution. Note that we retrieve the median when w = 1, as expected. The best estimator for the linex loss can be easily derived and results in: ? = ? y ? w? 2 y 2 where ? y and ? 2 y are the mean and the variance of the predictive posterior. 

 Experimental Settings Here we assess the models and datasets used in Section 3.1 in terms of their performance in the asymmetric setting. Following the explanation in the previous Section, we do not perform any retraining: we collect the predictions obtained using the 10-fold cross-validation protocol and apply different Bayes estimators corresponding to the asymmetric losses. Evaluation is performed using the same loss employed in the estimator (for instance, when using the linex estimator with w = 0.75 we report the results using the linex loss with same w) and averaged over the 10 folds. To simulate both pessimistic and optimistic scenarios, we use w ? {3, 1/3} for the AL loss and w ? {?0.75, 0.75} for the linex loss. The only exception is the en-de dataset, where we report results for w ? ?0.25, 0.75 for linex 3 . We also report results only for models using the Mat?rn52 kernel. While we did experiment with different kernels and weighting schemes 4 our findings showed similar trends so we omit them for the sake of clarity. 

 Results and Discussion Results are shown on Table  2 . In the optimistic scenario the tanh-based warped GP models give consistently better results than standard GPs. The log-based models also gives good results for AL but for linex the results are mixed except for en-es. This is probably again related to the larger sizes of the fr-en and en-de datasets, which allows the tanh-based models to learn richer representations. The pessimistic setting uses w = 3 for AL and w = ?0.75 for linex, except for English-German, where w = ?0.25. 

 English The pessimistic scenario shows interesting trends. While the results for AL follow a similar pattern when compared to the optimistic setting, the results for linex are consistently worse than the standard GP baseline. A key difference between AL and linex is that the latter depends on the variance of the predictive distribution. Since the warped models tend to have less variance, we believe the estimator is not being "pushed" towards the positive tails as much as in the standard GPs. This turns the resulting predictions not conservative enough (i.e. the post-editing time predictions are lower) and this is heavily (exponentially) pe-nalised by the loss. This might be a case where a standard GP is preferred but can also indicate that this loss is biased towards models with high variance, even if it does that by assigning probability mass to nonsensical values (like negative time). We leave further investigation of this phenomenon for future work. 

 Related Work Quality Estimation is generally framed as text regression task, similarly to many other applications such as movie revenue forecasting based on reviews  (Joshi et al., 2010; Bitvai and Cohn, 2015)  and detection of emotion strength in news headlines  (Strapparava and Mihalcea, 2008; Beck et al., 2014a)  and song lyrics  (Mihalcea and Strapparava, 2012) . In general, these applications are evaluated in terms of their point estimate predictions, arguably because not all of them employ probabilistic models. The NLPD is common and established metric used in the GP literature to evaluate new approaches. Examples include the original work on Warped GPs  (Snelson et al., 2004) , but also others like L?zaro-Gredilla (2012) and  Chalupka et al. (2013) . It has also been used to evaluate recent work on uncertainty propagation methods for neural networks  (Hern?ndez-Lobato and Adams, 2015) . Asymmetric loss functions are common in the econometrics literature and were studied by  Zellner (1986)  and  Koenker (2005) , among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles  (Newey and Powell, 1987) . This loss generalises the commonly used squared error loss. In terms of applications,  Cain and Janssen (1995)  gives an example in real estate assessment, where the consequences of under-and over-assessment are usually different depending on the specific scenario. An engineering example is given by  Zellner (1986)  in the context of dam construction, where an underestimate of peak water level is much more serious than an overestimate. Such real-world applications guided many developments in this field: we believe that translation and other language processing scenarios which rely on NLP technologies can heavily benefit from these advancements. 

 Conclusions This work explored new probabilistic models for machine translation QE that allow better uncertainty estimates. We proposed the use of NLPD, which can capture information on the whole predictive distribution, unlike usual point estimatebased metrics. By assessing models using NLPD we can make better informed decisions about which model to employ for different settings. Furthermore, we showed how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be beneficial in these settings. Uncertainty estimates can be useful in many other settings beyond the ones explored in this work. Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE  (Beck et al., 2013) . Exploratory analysis is another avenue for future work, where error bars can provide further insights about the task, as shown in recent work  (Nguyen and O'Connor, 2015) . This kind of analysis can be useful for tracking post-editor behaviour and assessing cost estimates for translation projects, for instance. Our main goal in this paper was to raise awareness about how different modelling aspects should be taken into account when building QE models. Decision making can be risky using simple point estimates and we believe that uncertainty information can be beneficial in such scenarios by providing more informed solutions. These ideas are not restricted to QE and we hope to see similar studies in other natural language applications in the future. Figure 1 : 1 Figure 1: Predictive distributions for two fr-en instances under a Standard GP and a Warped GP. The top two plots correspond to a prediction with low absolute error, while the bottom two plots show the behaviour when the absolute error is high. 

 Figure 2 : 2 Figure 2: Warping function instances from the three datasets. The vertical axis correspond to the latent warped values. The horizontal axis show the observed response variables, which are always positive in our case since they are post-editing times. 

			 Also known as Radial Basis Function (RBF) kernel. 

			 www.statmt.org/wmt16 

			 Using w = ?0.75 in this case resulted in loss values on the order of 10 7 . In fact, as it will be discussed in the next Section, the results for the linex loss in the pessimistic scenario were inconclusive. However, we report results using a higher w in this case for completeness and to clarify the inconclusive trends we found.4  We also tried w ? {1/9, 1/7, 1/5, 5, 7, 9} for the AL loss and w ? {?0.5, ?0.25, 0.25, 0.5} for the linex loss.
