title
Sentential Paraphrasing as Black-Box Machine Translation

abstract
We present a simple, prepackaged solution to generating paraphrases of English sentences. We use the Paraphrase Database (PPDB) for monolingual sentence rewriting and provide machine translation language packs: prepackaged, tuned models that can be downloaded and used to generate paraphrases on a standard Unix environment. The language packs can be treated as a black box or customized to specific tasks. In this demonstration, we will explain how to use the included interactive webbased tool to generate sentential paraphrases.

Introduction Monolingual sentence rewriting encompasses a variety of tasks for which the goal is to generate an output sentence with similar meaning to an input sentence, in the same language. The generated sentences can be called sentential paraphrases. Some tasks that generate sentential paraphrases include sentence simplification, compression, grammatical error correction, or expanding multiple reference sets for machine translation. For researchers not focused on these tasks, it can be difficult to develop a one-off system due to resource requirements. To address this need, we are releasing a black box for generating sentential paraphrases: machine translation language packs. The language packs consist of prepackaged models for the Joshua 6 decoder  (Post et al., 2015)  and a monolingual "translation" grammar derived from the Paraphrase Database (PPDB) 2.0  (Pavlick et al., 2015) . The PPDB provides tremendous coverage over English text, containing more than 200 million paraphrases extracted from 100 million sentences  (Ganitkevitch et al., 2013) . For the first time, any researcher with Java 7 and Unix (there are no other dependencies) can generate sentential paraphrases without developing their own system. Additionally, the language packs include a web tool for interactively paraphrasing sentences and adjusting the parameters. The language packs contain everything needed to generate sentential paraphrases in English: ? a monolingual synchronous grammar, ? a language model, ? a ready-to-use configuration file, ? the Joshua 6 runtime, so that no compilation is necessary, ? a shell script to invoke the Joshua decoder, and ? a web tool for interactive decoding and parameter configuration. The system is invoked by a single command, either on a batch of sentences or as an interactive server. Users can choose which size grammar to include in the language pack, corresponding to the PPDB pack sizes (S through XXXL). In the rest of the paper, we will describe the translation model and grammar, provide examples of output, and explain how the configuration can be adjusted for specific needs. 

 Language pack description Several different size language packs are available for download.  1  The components of the language packs are described below. Grammar Our approach to sentential paraphrasing is analogous to machine translation. As a translation grammar, we use PPDB 2.0, which contains 170-million lexical, phrasal, and syntactic paraphrases  (Pavlick et al., 2015) . Each language pack contains a PPDB grammar that has been packed into a binary form for faster computation  (Ganitkevitch et al., 2012) , and users can select which size grammar to use. The rules present in each grammar are determined by the PPDB 2.0 score, which indicates the paraphrase quality (as given by a supervised regression model) and correlates strongly with human judgments of paraphrase appropriateness  (Pavlick et al., 2015) . Grammars of different sizes are created by changing the paraphrase score thresholds; larger grammars therefore contain a wider diversity of paraphrases, but with lower confidences. Features Each paraphrase in PPDB 2.0 contains 44 features, described in  Ganitkevitch and Callison-Burch (2014)  and  Pavlick et al. (2015) . For each paraphrase pair, we call the input the original and the new phrase the candidate. Features can reflect just the candidate phrase or a relationship between the original and candidate phrases. Each of these features is assigned a weight, which guides the decoder's choice of paraphrases to apply to generate the final candidate sentence. All feature values are pre-calculated in PPDB 2.0. Decoding The language packs include a compiled Joshua runtime for decoding, a script to invoke it, and configuration files for different tuned models. There is also a web-based tool for interactively querying a server version of the decoder for paraphrases. We include a 5-gram Gigaword v.5 language model for decoding. One or more languagemodel scores are used to rank translation candidates during decoding. The decoder outputs the n-best candidate paraphrases, ranked by model score. 

 Models Each language pack has three pre-configured models to use either out of the box or as a starting point for further customization. There are tuned models for (1) sentence compression, (2) text simplification, and (3) a general-purpose model with handtuned weights. These models are distinguished only by the different weight vectors, and are selected by point the Joshua invocation script to the corresponding configuration file. 

 Tuned models We include two models that were tuned for (1) sentence compression and (2) simplification. The compression model is based on the work of  Ganitkevitch et al. (2011) , and uses the same features, tuning data, and objective function, PR ?CIS. The simplification model is described in  Xu et al. (2016) , and is optimized to the SARI metric. The system was tuned using the parallel data described therein as well as the Newsela corpus  (Xu et al., 2015) . There is no specialized grammar for these models; instead, the parameters were tuned to choose appropriate paraphrases from the PPDB. Sample output generated with these models is shown in Table  1 . 

 Hand-derived weights To configure the general-purpose model, which generates paraphrases for no specific task, we examined the output of 100 sentences randomly selected from each of three different domains: newswire (WSJ 0-1 (Marcus et al., 1993)), "simple" English (the Britannica Elementary corpus  (Barzilay and Elhadad, 2003) ), and general text (the WaCky corpus  (Baroni et al., 2009) ). We systematically varied the weights of the Gigaword LM and the PPDB 2.0 score features and selected values that yielded the best output as judged by the authors. The parameters selected for the generic language packs are weight lm = 10 and weight ppdb2 = 15, with all other weights are set to zero. Example output is shown in Table  1 . 

 User customization The language packs include configuration files with pre-determined weights that can be used on their own or as a jumping-off point for custom configurations. There are weights for each of the 44 PPDB 2.0 features as well as for the language model(s) used by the decoder. We encourage researchers to explore modifications to the model to suit their specific tasks, and we have clearly identified five aspects of the language packs that can be modified: 1. Alternate language models. The decoder can accept multiple LMs, and the packs include LMs es-Compression Orig: rice admits mistakes have been made by american administration in rebuilding iraq Gen: rice admits mistakes were made by american administration in rebuilding iraq Orig: partisanship is regarded as a crime , and pluralism is rejected , and no one in the shura council would seek to compete with the ruler or distort his image . Gen: partisanship is regarded as a crime and pluralism is rejected and none in the shura council would seek to compete with the ruler or distort his image . 

 Simplification Orig: fives is a british sport believed to derive from the same origins as many racquet sports . Gen: fives is a british sport thought to come from the same source as many racquet sports . Orig: in the soviet years , the bolsheviks demolished two of rostov 's principal landmarks -st alexander nevsky cathedral  ( 1908 )  and st george cathedral in nakhichevan ( 1783-1807 ) . Gen: in the soviet years , the bolsheviks destroyed two of rostov 's key landmarks -st alexander nevsky church ( 1908 ) and st george church in naxc ?ivan ( 1783-1807 ) . Generic Orig: because the spaniards had better weapons , cortes and his army took over tenochtitlan by 1521 . Gen: as the spaniards had better weapons , cortes and his men took over tenochtitlan by 1521 . Orig: it was eventually abandoned due to resistance from the population . Gen: it was later abandoned due to opposition from the population . timated over newswire text and "simple" English. Other user-provided LMs can be used for tasks targeting different domains of text. 2. Rank output with a custom metric. The nbest candidate sentences are chosen by their score according to a given metric (LM score for the generic model, and PR ?CIS and SARI for the tuned models), however other metrics can be used instead. 3. Manually adjust parameters. The weights of the features discussed in Section 3 can be adjusted, as well as other PPDB feature weights. The web tool (Figure  1 ) allows users to select the weights for all of the features and see the top-5 candidates generated with those weights. Some of the more interpretable features to target include the length difference and entailment relations between the phrase original and candidate, as well as formality and complexity scores of the candidate paraphrase. 4. Optimize parameters with parallel data. For tailoring machine translation to a specific task, the weights given to each feature can be optimized to a given metric over a tuning set of parallel data. This metric is commonly BLEU in machine translation, but it can be a custom metric for a specific task, such as PR ?CIS for compression  (Ganitkevitch et al., 2011)  or SARI for simplification  (Xu et al., 2016) . The user needs to provide a parallel dataset for tuning, ideally with about 2,000 thousand sentences. The pipeline scripts in the Joshua decoder have options for optimization, with the user specifying the language pack grammar and parallel tuning data. The configuration file included in the language pack can be used as a template for tuning. 

 Interactive tool Finally, we include a web tool that lets users interact with the decoder and choose custom weights (Figure  1 ). Once users have downloaded the tool kit, an included script lets them run the decoder as a server, and through the web interface they can type individual sentences and adjust model parameters. The interface includes an input text box (one sentence  For each output sentence, we report the Translation Edit Rate (TER), which is the number of changes needed to transform the output sentence into the input  (Snover et al., 2006) . This tool can be used to demonstrate and test a model or to hand-tune the model in order to determine the parameters for a configuration file to paraphrase a large batch of sentences. Detailed instructions for using the tool and shell scripts, as well as a detailed description of the configuration file, are available at the language pack home page: http://joshua-decoder.com/ language-packs/paraphrase/ 6 Related work Previous work has applied machine translation techniques to monolingual sentence rewriting tasks. The most closely related works used a monolingual para-phrase grammar for sentence compression  (Ganitkevitch et al., 2011)  and sentence simplification  (Xu et al., 2016) , both of which developed custom metrics and task-specific features. Various other MT approaches have been used for generating sentence simplifications, however none of these used a general-purpose paraphrase grammar  (Narayan and Gardent, 2014; Wubben et al., 2012, among others) . Another application of sentential paraphrases is to expand multiple reference sets for machine translation  (Madnani and Dorr, 2010) . PPDB has been used for many tasks, including recognizing textual entailment, question generation, and measuring semantic similarity. These language packs were inspired by the foreign language packs released with Joshua 6  (Post et al., 2015) . 

 Conclusion We have presented a black box for generating sentential paraphrases: PPDB language packs. The language packs include everything necessary for generation, so that they can be downloaded and invoked with a single command. This toolkit can be used for a variety of tasks: as a helpful tool for writing (what is another way to express a sentence?); generating additional training or tuning data, such as multiplereferences for machine translation or other text-totext rewriting tasks; or for changing the style or tone of a text. We hope their ease-of-use will facilitate future work on text-to-text rewriting tasks. Figure 1 : 1 Figure 1: A screen shot of the web tool. The number to the right of each output sentence is the TER. 
