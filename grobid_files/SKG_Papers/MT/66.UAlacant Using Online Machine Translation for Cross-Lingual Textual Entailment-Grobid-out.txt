title
UAlacant: Using Online Machine Translation for Cross-Lingual Textual Entailment

abstract
This paper describes a new method for crosslingual textual entailment (CLTE) detection based on machine translation (MT). We use sub-segment translations from different MT systems available online as a source of crosslingual knowledge. In this work we describe and evaluate different features derived from these sub-segment translations, which are used by a support vector machine classifier to detect CLTEs. We presented this system to the Se-mEval 2012 task 8 obtaining an accuracy up to 59.8% on the English-Spanish test set, the second best performing approach in the contest.

Introduction Cross-lingual textual entailment (CLTE) detection  (Mehdad et al., 2010)  is an extension of the textual entailment (TE) detection  (Dagan et al., 2006)  problem. TE detection consists of finding out, for two text fragments T and H in the same language, whether T entails H from a semantic point of view or not. CLTE presents a similar problem, but with T and H written in different languages. During the last years, many authors have focused on resolving TE detection, as solutions to this problem have proved to be useful in many natural language processing tasks, such as question answering  (Harabagiu and Hickl, 2006)  or machine translation (MT)  (Mirkin et al., 2009; Pad? et al., 2009) . Therefore, CLTE may also be useful for related tasks in which more than one language is involved, such as cross-lingual question answering or cross-lingual information retrieval. Although CLTE detection is a relatively new problem, it has already been tackled.  Mehdad et al. (2010)  propose to use machine translation (MT) to translate H from L H , the language of H, into L T , the language of T , and then use any of the state-of-the-art TE approaches. In a later work , the authors use MT, but in a more elaborate way. They train a phrase-based statistical MT (PBSMT) system  (Koehn et al., 2003)  translating from L H to L T , and use the translation table obtained as a by-product of the training process to extract a set of features which are processed by a support vector machine classifier  (Theodoridis and Koutroumbas, 2009, Sect. 3.7)  to decide whether T entails H or not.  Castillo (2011)  discusses another machine learning approach in which the features are obtained from semantic similarity measures based on WordNet  (Miller, 1995) . In this work we present a new approach to tackle the problem of CLTE detection using a machine learning approach, partly inspired by that of . Our method uses MT as a source of information to detect semantic relationships between T and H. To do so, we firstly split both T and H into all the possible sub-segments with lengths between 1 and L, the maximum length, measured in words. We then translate the set of sub-segments from T into L H , and vice versa, and collect all the sub-segment pairs in a single set. We claim that when T -side sub-segments match T and their corresponding Hside sub-segments match H, this reveals a semantic relationship between them, which can be used to determine whether T entails H or not. Note that MT is used as a black box, i.e. sub-segment translations may be collected from any MT system, and that our approach could even use any other sources of bilingual sub-sentential information. It is even possible to combine different MT systems as we do in our experiments. This is a key point of our work, since it uses MT in a more elaborate way than  Mehdad et al. (2010) , and it does not depend on a specific MT approach. Another important difference between this work and that of  is the set of features used for classification. The paper is organized as follows: Section 2 describes the method used to collect the MT information and obtain the features; Section 3 explains the experimental framework; Section 4 shows the results obtained for the different features combination proposed; the paper ends with concluding remarks. 

 Features from machine translation Our approach uses MT as a black box to detect parallelisms between the text fragments T and H by following these steps: 1. T is segmented in all possible sub-segments t m+p?1 m of length p with 1 ? p ? L and 1 ? m ? |T | ? p + 1, where L is the maximum sub-segment length allowed. Analogously, H is segmented to get all the possible sub-segments h n+q?1 n of length q, with 1 ? q ? L and 1 ? n ? |T | ? q + 1. 2. The sub-segments obtained from T are translated using all the available MT systems into L H . Analogously, the sub-segments from H are translated into L T , to generate a set of subsegment pairs (t, h). 3. Those pairs of sub-segments (t, h) such that t is a sub-string of T and h is a sub-string of H are annotated as sub-segment links. Note that it could be possible to use statistical MT to translate both T and H and then use word alignments to obtain the sub-segment links. However, we use this methodology to ensure that any kind of MT system can be used by our approach. As a result of this process, a sub-segment in T may be linked to more than one sub-segment in H, and vice versa. Based on these sub-segment links we have designed a set of features which may be used by a classifier for CLTE. 

 Basic features [Bas] We used a set of basic features to represent the information from the sub-segment links between T and H, which are computed as the fraction of words in each of them covered by linked sub-segments of length l ? [1, L]. We define the feature function F l (S), applied on a text fragment S (either T or H) as: F l (S) = Cov(S, l)/|S| where Cov(S, l) is a function which obtains the number of words in S covered by at least one sub-segment of length l which is part of a sub-segment link. An additional feature is computed to represent the total proportion of words in each text fragment: F total (S) = Cov(S, * )/|S| where Cov(S, * ) is the same as Cov(S, l) but using sub-segments of any length up to L. F total (S) provide information about overlapping that F l (S) cannot grasp. For instance, if we have F 1 (T ) = 0.5 and F 2 (T ) = 0.5, we cannot know if the sub-segments of l = 1 and l = 2 are covering the same or different words, so F total (S) represents the actual proportion of words covered in a text fragment S. These feature functions are applied both on T and H, thus obtaining a set of 2 * L + 2 features, henceforth Bas. 

 Extensions to the basic features Some extensions can be made to the basic features defined above by using additional external resources. In this section we propose two extensions. Separate analysis of function words and content words  [Spl] . In this case, features represent, separately, function words, with poor lexical information, and content words, with richer lexical and semantic information. In this way, F l (S) is divided into FF l (S) and CF l (S) defined as: FF l (S) = Cov F (S, l)/|FW(S)| and CF l (S) = Cov C (S, l)/|CW(S)| where FW(S) is a function that returns the function words in text fragment S and CW(S) performs the same task for content words. Analogously, Cov F (S, l) and Cov C (S, l) are versions of Cov(S, l) which only consider function and content words, respectively. This extension can be also be applied to F total (T ) and F total (H). The set of 4L + 4 features obtained in this way (henceforth Spl) allows the classifier to use the information from the most relevant words in T and H to detect entailment. Stemming  [Stm and SplStm] . Stemming can also be used when detecting the sub-segment links. Both the table of sub-segment pairs and the text fragment pair (T ,H) are stemmed before matching. In this way, conflicts of number or gender disagreement in the translations can be overcome in order to detect more sub-segment links. This new extension can be applied both to Bas, obtaining the set of features Stm, and to Spl, obtaining the set of features SplStm. Although lemmatization could have been used, stemming was preferred because it does not require the part-of-speech ambiguity to be solved, which may be difficult to solve when dealing with very short sub-segments. 

 Additional features Two additional features were defined unrelated with the basic features proposed. The first one, called here R, is the length ratio |T |/|H|. Intuitively we can guess that if H is much longer than T it is unlikely that T entails H. The second additional set features is the one defined by , so we will refer to it as M . The corresponding feature function computes, for the total number of sub-segments of a given length l ? [1, L] obtained from a text fragment S, the fraction of them which appear in a sub-segment link. It is applied both to H and T and is defined as: F l (S) = Linked l (S)/(|S| ? l + 1) where Linked l is the number of sub-segments from S with length l which appear in a sub-segment link. 

 Experimental settings The experiments designed for this task are aimed at evaluating the features proposed in Section 2. We evaluate our CLTE approach using the English-Spanish data sets provided in the task 8 of SemEval 2012  (Negri et al., 2012) . Datasets. Two datasets were provided by the organization of SemEval 2012 : a training set and a test set, both composed by a set of 500 pairs of sentences. CLTE detection is evaluated in both directions, so instances belong to one of these four classes: forward (the sentence in Spanish entails the one in English); backward (the sentence in English entails the one in Spanish); bidirectional (both sentences entail each other); and no entailment (neither of the sentences entails each other). For the whole data set, both sentences in each instance were tokenized using the scripts 1 included in the Moses MT system  (Koehn et al., 2007) . Each sentence was segmented to get all possible sub-segments which were then translated into the other language. External resources. We used three different MT systems to translate the sub-segments from English to Spanish, and vice versa: ? Apertium: 2 a free/open-source platform for the development of rule-based MT systems  (Forcada et al., 2011) . We used the English-Spanish MT system from the project's repository 3 (revision 34706). ? Google Translate: 4 an online MT system by Google Inc. ? Microsoft Translator: 5 an online MT system by Microsoft. External resources were also used for the extended features described in Section 2.2. We used the stemmer 6 and the stopwords list provided by the SnowBall project for Spanish 7 and English. 8 Classifier. We used the implementation of support vector machine included in the WEKA v.3.6.6 data mining software package  (Hall et al., 2009)  for multiclass classification, and a polynomial kernel. 

 Results and discussion We tried the different features proposed in Section 2 in isolation, and also different combinations of them. Table  1  reports the accuracy for the different features described in Section 2 on the test set using sub-segments with lengths up to L = 6. 9 As can be seen, the features providing the best results on accuracy are the SplStm features. In addition, results show that all versions of the basic features (Bas, Spl, Stm, and SplStm) provide better results than the M feature alone. Some combinations of features are also reported in Table  1 . Although many combinations were tried, we only report the results of the combinations of features performing best because of lack of space. Bas ? Spl ? Stm ? SplStm ? M ? R Bas ? Spl ? M ? R Apertium Ap. As can be seen, both feature combinations Bas ? Spl and Bas ? Stm obtain higher accuracy than the separated features. Combining all these features Bas ? Spl ? Stm ? SplStm provide even better results, thus confirming some degree of orthogonality between them. Combination Bas ? Spl ? M ? R obtains one of the best results, since it produces an improvement of almost 1% over combination Bas ? Spl ? Stm ? SplStm but using less than a half of features. Combining all the features provides the best accuracy as expected, so this seems to be the best combination for the task. Table  2  reports the results sent for the SemEval 2012 task 8. We chose feature combinations Bas ? Spl ? M ? R and Bas ? Spl ? Stm ? SplStm ? M ? R since they are the best performing combinations. We sent two runs of our method using all three MT systems described in Section 3 and two more runs using only sub-segment translations from Apertium. From the ten teams presenting systems for the contest, only one overcomes our best result. Even the results obtained using Apertium as the only MT system overcome seven of the ten approaches presented. This result confirms that state-of-the-art MT is a rich source of information for CLTE detection. 

 Concluding remarks In this paper we have described a new method for CLTE detection which uses MT as a black-box source of bilingual information. We experimented with different features which were evaluated with the datasets for task 8 of SemEval 2012. We obtained up to 59.8% of accuracy on the Spanish-English test set provided, becoming the second best performing approach of the contest. As future works, we are now preparing experiments for other pairs of languages and we plan to use weights to promote those translations coming from more-reliable MT systems. Table 2 : 2 Precision (P) and recall (R) obtained by our approach for each of the four entailment classes and total accuracy on the English-Spanish test set using different feature combinations and different MT systems: Apertium, and a combination ofApertium, Google Translate, and Microsoft Translator (Ap.+Go.+Mi.). +Go.+Mi. Apertium Ap.+Go.+Mi. P R P R P R P R Backward 64.3% 64.8% 64.5% 72.8% 59.1% 64.8% 57.3% 60.0% Forward 65.5% 57.6% 68.9 % 56.8% 59.8% 56.0% 58.7% 59.2% Bidirectional 57.7% 56.8% 56.6% 55.2% 43.7% 41.6% 42.5% 40.8% No-entailment 47.5% 53.6% 50.7% 54.4% 42.5% 43.2% 44.7% 44.0% Accuracy 58.2% 59.8% 51.4% 51.0% Feature set N f Accuracy Bas 14 50.0% Spl 28 56.0% Stm 14 49.6% SplStm 28 56.8% R 1 45.8% M 12 47.0% Bas ? Spl 42 56.6% Bas ? Stm 28 51.0% Bas ? Spl ? Stm ? SplStm 84 57.4% Bas ? Spl ? M ? R 41 58.2% Bas ? Spl ? Stm ? ? SplStm ? M ? R 97 59.8% 

 Table 1 : 1 Accuracy obtained by the system using the different feature sets proposed in Section 2 for the test set. N f is the number of features. 

			 http://bit.ly/H4LNux 2 http://www.apertium.org 3 http://bit.ly/HCbn8a 4 http://translate.google.com 5 http://www.microsofttranslator.com 6 http://bit.ly/H2HU97 7 http://bit.ly/JMybmL 8 http://bit.ly/Iwg9Vm 9 All the results in this section are computed with L = 6, which proved to be the value providing the best accuracy for the dataset available after trying different values of L.
