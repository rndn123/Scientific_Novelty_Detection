title
Using Spoken Word Posterior Features in Neural Machine Translation

abstract
A spoken language translation (ST) system consists of at least two modules: an automatic speech recognition (ASR) system and a machine translation (MT) system. In most cases, an MT is only trained and optimized using error-free text data. If the ASR makes errors, the translation accuracy will be greatly reduced. Existing studies have shown that training MT systems with ASR parameters or word lattices can improve the translation quality. However, such an extension requires a large change in standard MT systems, resulting in a complicated model that is hard to train. In this paper, a neural sequence-to-sequence ASR is used as feature processing that is trained to produce word posterior features given spoken utterances. The resulting probabilistic features are used to train a neural MT (NMT) with only a slight modification. Experimental results reveal that the proposed method improved up to 5.8 BLEU scores with synthesized speech or 4.3 BLEU scores with the natural speech in comparison with a conventional cascaded-based ST system that translates from the 1-best ASR candidates.

Introduction Spoken language translation is one innovative technology that allows people to communicate by speaking in their native languages. However, translating a spoken language, in other words, recognizing speech and then translating words into another language, is incredibly complex. A standard approach in speech-to-text translation systems requires effort to construct automatic speech recognition (ASR) and machine translation (MT), both of which are trained and tuned independently. ASR systems, which aim for the perfect transcription of utterances, are trained and tuned by minimizing the word error rate (WER)  [1] . MT outputs are optimized and automatically measured based on a wide variety of metrics. One of the standard methods is the BLEU metric. However, all the errors from the words in ASR outputs are treated uniformly without considering their syntactic roles, which are often critical for MT. Many studies have investigated the effectiveness of the WER metric of ASR on the whole speech translation pipeline  [2, 3, 4]  and verified that ASR errors that compose the WER metric do not contribute equally to the BLEU score of translation quality. Furthermore, most MT systems are only trained and optimized using error-free text data. Despite the fact that ASR technologies and their recognition rates have continued to improve, the occurrence of speech recognition errors remains inevitable. This is because there are many ambiguities due to a wide variety of acoustic and linguistic patterns produced by different speakers with various speaking styles and background noises. If the ASR engine makes mistakes, the translation accuracy will be significantly reduced. Thus, ignoring the existence of ASR errors while constructing a speech translation system is practically impossible. Previous research on traditional phrase-based MTs has attempted to train the ASR and MT parameters of the loglinear model to directly optimize the BLEU score of the translation metric of full speech translation systems  [3] . It allows the model to directly select recognition candidates that are easy to translate and improve the translation accuracy given an imperfect speech recognition. Ohgushi et al.  [5]  further elaborated various techniques in the context of the joint optimization of ASR and MT, including minimum error rate training (MERT)  [6] , pair-wise ranking optimization (PRO)  [7] , and the batch margin infused relaxed algorithm (MIRA)  [8] . Other studies directly performed translation on the lattice representations of the ASR output  [9, 10, 11] . The results showed that a better translation can be achieved by translating the lattices rather than with the standard cascade system that translated the single best ASR output. Recently, deep learning has shown great promise in many tasks. A sequence-to-sequence attention-based neural network is one type of architecture that offers a powerful model for machine translation and speech recognition  [12, 13] . Several studies revisited similar problems and proposed handling lattice inputs by replacing the encoder part with a lattice encoder to obtain a lattice-to-sequence model  [14, 15] . With these methods, robust translation to speech recognition errors became possible. However, this approach requires a large modification to standard NMT systems, resulting in a complicated model that is hard to train. Also, as the NMT takes word lattices as input, it might be difficult to backpropagate a translation error to the ASR part. An extreme case is to train the encoder-decoder architecture for end-to-end speech translation (ST) tasks, which directly translates speech in one language into text in another. Duong et al.  [16]  directly trained attentional models on par-allel speech data. But their work focused only on alignment performance. The works by Berard et al.  [17]  might be the first attempts that successfully build a full-fledged end-to-end attentional-based speech-to-text translation system. But they only performed with a small parallel French-English BTEC corpus, and their best results were behind the cascade baseline model. Later on, Weiss et al.  [18]  proposed a similar approach and conducted experiments on the Spanish Fisher and Callhome corpora of telephone conversations augmented with English translations. However, most of these works were only done for language pairs with similar syntax and word order (SVO-SVO), such as Spanish-English or French-English. For such languages, only local movements are sufficient for translation. Kano et al.  [19]  showed that direct attentional ST approach failed to handle English-Japanese language pairs with SVO versus SOV word order. In this research, we also focus on English-Japanese and we aim for a neural speech translation that is robust against speech recognition errors without requiring significant changes in the NMT structure. This can be considered as a simplified version of the one that directly performed translation on the lattice representations. But, instead of providing full lattice outputs, we perform a neural sequence-tosequence ASR as feature processing that is trained to produce word posterior features given spoken utterances. This might resemble the word confusion networks (WCNs)  [20]  that can directly express the ambiguity of the word hypotheses at each time point. The resulting probabilistic features are used to train NMT with just a slight modification. Such vectors are expected to express the ambiguity of speech recognition output candidates better than the standard way using the 1-best ASR outputs while also providing a simpler structure than the lattice outputs. During training, the approach also allows backpropagating the errors from NMT to ASR and performs join training. Here, we evaluate our proposed English-Japanese speech translation model using both synthesized and natural speech with various degrees of ASR errors. 

 Overview of Attention-based Speech Translation Our English-Japanese end-to-end speech translation system consists of ASR and MT modules that were constructed on standard attention-based, encoder-decoder neural network architecture  [21, 22] . 

 Basic Attentional Encoder-Decoder model An attentional encoder-decoder model consists of an encoder, a decoder, and attention modules. Given input sequence x = [x 1 , x 2 , ..., x N ] with length N , the encoder produces a sequence of vector representation h enc = (h enc 1 , h enc 2 , ..., h enc N ). Here, we used a bidirectional recurrent neural network with long short-term memory (bi-LSTM) units  [23] , which consist of forward and backward LSTMs. The forward LSTM reads the input sequence from x 1 to x N and estimates forward ? ? ? h enc , and the backward LSTM reads the input sequence in reverse order from x N to x 1 and estimates backward ? ? ? h enc . Thus, for each input x n , we obtain h enc n by summation forward ? ? ? h enc and backward ? ? ? h enc : h enc n = ? ? ? h enc n + ? ? ? h enc n . (1) The decoder, on the other hand, predicts target sequence y = [y 0 , y 1 , y 2 , ..., y M ] with length M by estimating conditional probability p(y|x). Here, we use uni-directional LSTM (forward only). Conditional probability p(y|x) is estimated based on the whole sequence of the previous output: p(y m |y <m , x) = softmax(W y hdec m ). (2) Decoder hidden activation vector hdec m is computed by applying linear layer W c over context information c m and current hidden state h dec m : hdec m = tanh(W c [c m ; h dec m ]). (3) Here, c m is in the context information of the input sequence when generating current output at time m. It is estimated by the attention module over encoder hidden states h enc n : c m = N n=1 a m (n) * h enc n , (4) where variable-length alignment vector a m is computed whose size equals length of input sequence x: a m = align(h enc n , h dec m ) (5) = softmax(dot(h enc n , h dec m )). This step assists the decoder to find relevant information on the encoder side based on the current decoder hidden states. Several variations calculate align(h enc n , h dec m ). Here, we simply use the dot product between the encoder and decoder hidden states. 

 Automatic Speech Recognition Speech recognition tasks estimate a word sequence given a sequence of speech features. Input sequence x = [x 1 , ..., x N ] is the input speech filter bank feature sequence of the source language, and target sequence y = [y 1 , ..., y M ] is the predicted corresponding word sequence in the source language. 

 Machine Translation Machine translation tasks estimate a word sequence of a target language given a word sequence of a source language. Input sequence x = [x 1 , ..., x N ] is the word sequence of the source language, and target sequence y = [y 1 , ..., y M ] is the predicted corresponding word sequence in the target language. Here, x n is a one-hot vector in the baseline or posterior vector in the proposed method, y m is the index representation of the words, and y 0 is an index representation of the target sequence's start. 

 Speech-to-text Translation Speech-to-text translation tasks estimate a word sequence of a target language given a sequence of speech features. Here, we use both the sequence-to-sequence ASR and MT systems. Output sequence y from ASR becomes input sequence x in an MT system. 

 Proposed method: NMT using Spoken Word Posterior Features  we utilize the posterior probability vectors before the argmax function: p(y m |y <m , x). (7) This way the vectors can still express the ambiguity of the speech recognition output candidates with probabilities. The resulting probabilistic features are then used to train the NMT with only a slight modification. We train the end-to-end NMT using the standard attention-based encoderdecoder neural network architecture described in the previous section. The only difference is in the input features. Instead of training the model with the one-hot vector of the most probable words, we utilize the posterior vectors obtained from the ASR. However, the dimension of input vector representation used in a standard one-hot vector and the proposed posterior vectors is the same. The overall architecture is illustrated in Fig.  2 .  

 Experiments We evaluated the performance of the proposed method on an English-Japanese translation task. To simulate the effect of various ASR errors, we first assessed it on synthesized speech and later applied it to natural speech. 

 Data set The experiments were conducted using a basic travel expression corpus (BTEC)  [24] : 

 ? Text corpus We used a BTEC English-Japanese parallel text corpus that consists of about 460k (BTEC1-4) training sentences and 500 sentences in the test set. ? Synthesized speech corpus Since corresponding speech utterances for the BTEC parallel text corpus are not available, we used Google text-to-speech synthesis  [25]  to generate a speech corpus of the BTEC1 source language (about 160k utterances). We used about 500 speech utterances in the test set. 

 ? Natural speech corpus We also evaluated with natural speech. In this case, we used the ATR English speech corpus  [26]  in our experiments. The text material was based on the basic travel expression domain. The speech corpus we used consisted of American, British, and Australian (AUS) English accents with about 120k utterances spoken by 100 speakers (50 males, 50 females) for each accent. The speech utterances were segmented into multiple frames with a 25-ms window size and a 10-ms step size. Then we extracted 23-dimension filter bank features using Kaldi's feature extractor  [27]  and normalized them to have zero mean and unit variance. 

 Models We further used the data to build a speech translation system with attention-based ASR and MT systems. The ASR and NMT share the same vocabulary (16,745 words). The dimensions of the distributed vector representation are smaller than vocabulary size (the size depends on the model settings). The hidden encoder and decoder layer consists of 500 nodes. A batch size of 32 and a dropout of 0.1 were also applied. For all systems, we used a learning rate of 0.0001 for the encoder and 0.0005 for the decoder and adopted Adam  [28]  to all the models. As we aim to have a neural speech translation that is robust against speech recognition errors without requiring significant changes in the NMT structure. We constructed three types of models that fit those requirements: ? Text-based machine translation system (upperbound) This is a text-to-text translation model from the source language to the target language. Here the BTEC English-Japanese parallel text corpus is used to train the model. 

 ? Baseline speech translation This speech-to-text translation model was created by cascading the ASR (speech-to-text) in the source language with a text-to-text MT module using 1-best ASR outputs. First, we pre-trained the NMT with the BTEC English-Japanese parallel text corpus and then fine-tuned the NMT model with a one-hot vector provided from the ASR. ? Proposed speech translation This speech-to-text translation model was created by cascading ASR (speech-to-text) in the source language with the text-to-text MT module using the ASR posterior vectors. First, we pre-trained the ASR with the speech of the source language and the NMT with the BTEC English-Japanese parallel text corpus. After that, we fine-tuned the parameter of both models by jointly training, where the posterior vector of ASR output is used as the NMT input. Note that the ASR systems used for the baseline and the proposed systems are the same. Also, all translation systems were tuned adequately, and the best model from training epochs was selected for each system. 

 Result 

 Speech Recognition System To simulate different degrees of ASR errors, we constructed an ASR model using synthesized speech with different numbers of training epochs, resulting in four different models with the following WERs: (1) System 1 (WER=15.17%), System 2 (WER=12.34%), System 3 (WER=11.05%), and System 4 (WER=8.82%). As a model that is trained with natural speech, our performance achieved a 24.98% WER. 

 Translation System As mentioned earlier, we compare three translation system: one for standard text-based machine translation, one for baseline speech translation with the cascade model, and one for our proposed speech translation.  The quality of those translation systems with the input of synthesized speech was evaluated using BLEU  [29]  and The proposed methods (System 3 and 4) exceed the text translation because the recognition candidates included in the posterior vector made it possible to correctly distinguish confusing words in the word embedding of the text translation. We will scrutinize this result in the next section. Next, the quality of the speech translation systems using natural speech was also evaluated using BLEU and shown in Fig.  4 . For the text translation, we provided the transcription of the natural speech, which is different than the text used in Fig.  3 . This system used the ASR model where WER is 24.98%. Importantly, unlike several published ASR systems using BTEC dataset, our ASR system only used the text transcription of the training set for the language model. Therefore, the ASR results reported in the paper could not reach state-of-the-art ASR performance. Nevertheless, the translation results are still convincing as evidence of the proposed framework's effectiveness. The proposed method improved the 4.3 BLEU score of the baseline model, confirming that the proposed method is also effective for natural speech. 

 Discussion Table  1  shows the sentence output examples in English-Japanese translation: (1) with ASR error, and (2) without ASR error. In the first example, to analyze the effect of ASR error, we compare the sentence output of the proposed model and the baseline (the cascase model). Here, ASR misrecognized "shoe" as "station". This error impacted the baseline (cascade system), where it translated "station" as "eki" (the correct translation for "shoe store" is "kutsuya"). How-  ever, in the proposed method, it was still able to translate it to "kutsuya". This might be because the ASR provided a posterior vector in which the recognition candidate and each a posteriori probability are weighted (Table  2 ). Here, "shoe" information was still contained in the posterior vector with only slightly lower probability than "station," and based on the context information, the machine translation translated the word as "kutsuya." In the second example, ASR provided a correct sentence. Here, we compare the sentence output of the proposed model and the text translation. Since the contexts of "perm" and "haircut" are close, the text translation mistakenly translated both "perm" and "haircut" into "paama" (Fig.  5  illustrates the text translation's alignment matrix). On the other hand, having a posterior vector as the input in the proposed model (see the attention matrix in Fig.  6 ) allowed NMT to correctly distinguish confusing words by the word embedding of the text translation. 

 Conclusions In this research, a speech translation system that is robust against speech recognition errors is obtained by using a posterior vector, which is a normalized vector that expresses the ambiguity of the speech recognition candidates, as the input of an NMT engine. The lower the WER of the ASR model is, the weaker the tendency of translation error becomes. Nevertheless, the whole test's accuracy surpassed the baseline. As a result, the posterior vector improved the BLEU score by 4.8 to 5.8 points over the baseline in the simulation experiment and improved it by 4.3 BLEU points over the baseline in the experiment using natural voice. By providing the probability of the speech recognition output candidates in speech translation, an optimal input selection for NMT was made. In the future, we will directly perform join training from ASR to NMT. Figure 1 : 1 Figure 1: Construction of spoken word posterior features 

 Figure 2 : 2 Figure 2: Proposed NMT architecture 

 Figure 3 : 3 Figure 3: Translation quality given synthesized speech input 

 Figure 4 : 4 Figure 4: Translation quality given natural speech input 

 Figure 5 :Figure 6 : 56 Figure 5: Attention matrix of text translation 

 Table 1 : 1 Examples of sentences output: (1) with ASR error, and (2) without ASR error. Example 1: With ASR error ASR reference Excuse me where is the closest shoe store? ASR result Excuse me where is the closest station store? Baseline Sumimasen ichiban chikai eki wa doko desuka? Proposed Sumimasen ichiban chikai kutsuya wa doko desuka? MT reference Sumimasen ichiban chikai kutsuya wa doko desuka? Example 2: Without ASR error ASR reference i d like to have a perm and a haircut please ASR result i d like to have a perm and a haircut please Text translation Paama to paama o onegai shitai nodesuga Proposed Paama to katto o onegaishimasu MT reference Paama to katto o onegaishimasu 

 Table 2 : 2 Posterior vector     Recognized Posterior station 0.439 shoe 0.321 change 0.086 cashier 0.036 always 0.016 

			 Proceedings of the 15 th International Workshop on Spoken Language Translation Bruges, Belgium, October 29-30, 2018
