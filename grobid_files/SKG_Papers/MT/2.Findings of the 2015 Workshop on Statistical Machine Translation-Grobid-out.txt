title
Findings of the 2015 Workshop on Statistical Machine Translation

abstract
This paper presents the results of the WMT15 shared tasks, which included a standard news translation task, a metrics task, a tuning task, a task for run-time estimation of machine translation quality, and an automatic post-editing task. This year, 68 machine translation systems from 24 institutions were submitted to the ten translation directions in the standard translation task. An additional 7 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had three subtasks, with a total of 10 teams, submitting 34 entries. The pilot automatic postediting task had a total of 4 teams, submitting 7 entries.

Introduction We present the results of the shared tasks of the Workshop on Statistical Machine Translation (WMT) held at EMNLP 2015. This workshop builds on eight previous WMT workshops  (Koehn and Monz, 2006; Callison-Burch et al., 2007 , 2009 , 2010 , 2011 , 2012 Bojar et al., 2013 Bojar et al., , 2014 . This year we conducted five official tasks: a translation task, a quality estimation task, a metrics task, a tuning task 1 , and a automatic postediting task. In the translation task ( ?2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data. We held ten translation tasks this year, between English and each of Czech, French, German, Finnish, and Russian. The Finnish translation tasks were new this year, providing a lesser resourced data condition on a challenging language pair. The system outputs for each task were evaluated both automatically and manually. The human evaluation ( ?3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed evaluations proportional to the number of tasks they entered. We made data collection more efficient and used TrueSkill as ranking method. The quality estimation task ( ?4) this year included three subtasks: sentence-level prediction of post-editing effort scores, word-level prediction of good/bad labels, and document-level prediction of Meteor scores. Datasets were released with English?Spanish news translations for sentence and word level, English?German news translations for document level. The first round of the automatic post-editing task ( ?5) examined automatic methods for correcting errors produced by an unknown machine translation system. Participants were provided with training triples containing source, target and human post-editions, and were asked to return automatic post-editions for unseen  (source, target)  pairs. This year we focused on correcting English?Spanish news translations. The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.  2  We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. 

 Overview of the Translation Task The recurring task of the workshop examines translation between English and other languages. As in the previous years, the other languages include German, French, Czech and Russian. Finnish replaced Hindi as the special language this year. Finnish is a lesser resourced language compared to the other languages and has challenging morphological properties. Finnish represents also a different language family that we had not tackled since we included Hungarian in  and 2009 (Callison-Burch et al., 2008 , 2009 . We created a test set for each language pair by translating newspaper articles and provided training data, except for French, where the test set was drawn from user-generated comments on the news articles. 

 Test data The test data for this year's task was selected from online sources, as before. We took about 1500 English sentences and translated them into the other 5 languages, and then additional 1500 sentences from each of the other languages and translated them into English. This gave us test sets of about 3000 sentences for our English-X language pairs, which have been either written originally written in English and translated into X, or vice versa. For the French-English discussion forum test set, we collected 38 discussion threads each from the Guardian for English and from Le Monde for French. See Figure  1  for an example. The composition of the test documents is shown in Table  1 . The stories were translated by the professional translation agency Capita, funded by the EU Framework Programme 7 project MosesCore, and by Yandex, a Russian search engine company.  3  All of the translations were done directly, and not via an intermediate language. 

 Training data As in past years we provided parallel corpora to train translation models, monolingual corpora to train language models, and development sets to tune system parameters. Some training corpora were identical from last year (Eu-3 http://www.yandex.com/ roparl 4 , United Nations, French-English 10 9 corpus, CzEng, Common Crawl, Russian-English parallel data provided by Yandex, Russian-English Wikipedia Headlines provided by CMU), some were updated (News Commentary, monolingual data), and new corpora was added (Finnish Europarl), Finnish-English Wikipedia Headline corpus). Some statistics about the training materials are given in Figure  2 . 

 Submitted systems We received 68 submissions from 24 institutions. The participating institutions and their entry names are listed in Table  2 ; each system did not necessarily appear in all translation tasks. We also included 1 commercial off-the-shelf MT system and 6 online statistical MT systems, which we anonymized. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 

 Human Evaluation Following what we had done for previous workshops, we again conduct a human evaluation campaign to assess translation quality and determine the final ranking of candidate systems. This section describes how we prepared the evaluation data, collected human assessments, and computed the final results. This year's evaluation campaign differed from last year in several ways: ? In previous years each ranking task compared five different candidate systems which were selected without any pruning or redundancy cleanup. This had resulted in a noticeable amount of near-identical ranking candidates in WMT14, making the evaluation process unnecessarily tedious as annotators ran into a fair amount of ranking tasks containing very similar segments which are hard to inspect. For WMT15, we perform redundancy cleanup as an initial preprocessing step and This is perfectly illustrated by the UKIP numbties banning people with HIV. You mean Nigel Farage saying the NHS should not be used to pay for people coming to the UK as health tourists, and saying yes when the interviewer specifically asked if, with the aforementioned in mind, people with HIV were included in not being welcome. You raise a straw man and then knock it down with thinly veiled homophobia. Every time I or my family need to use the NHS we have to queue up behind bigots with a sense of entitlement and chronic hypochondria. I think the straw man is yours. Health tourism as defined by the right wing loonies is virtually none existent. I think it's called democracy. So no one would be affected by UKIP's policies against health tourism so no problem. Only in UKIP La La Land could Carswell be described as revolutionary. Quoting the bollox The Daily Muck spew out is not evidence. Ah, shoot the messenger. The Mail didn't write the report, it merely commented on it. Whoever controls most of the media in this country should undead be shot for spouting populist propaganda as fact. I don't think you know what a straw man is. You also don't know anything about my personal circumstances or identity so I would be very careful about trying to eradicate a debate with accusations of homophobia. Farage's comment came as quite a shock, but only because it is so rarely addressed. He did not express any homophobic beliefs whatsoever. You will just have to find a way of getting over it. I'm not entirely sure what you're trying to say, but my guess is that you dislike the media reporting things you disagree with. It is so rarely addressed because unlike Fararge and his Thatcherite loony disciples who think aids and floods are a signal from the divine and not a reflection on their own ignorance in understanding the complexities of humanity as something to celebrate,then no. Language Sources (Number of Documents) Czech aktu?ln?.cz (4), blesk.cz (1), blisty.cz (1), ctk.cz (1), den?k.cz (1), e15.cz (1), iDNES.cz (19), ihned.cz (3), lidovky.cz (6), Novinky.cz (2), tyden.cz (1). English ABC News (4), BBC (6), CBS News (1), Daily Mail (1), Euronews (1), Financial Times (1), Fox News (2), Globe and Mail (1), Independent (1), Los Angeles Times (1), News.com Australia (9), Novinite (2), Reuters (2), Sydney Morning Herald (1), stv.tv (1), Telegraph (8), The Local (1), The Nation (1), UPI (1), Washington Post (3). 

 German Abendzeitung N?rnberg (1), Aachener Nachrichten (1), Der Standard (2), Deutsche Welle (1), Frankfurter Neue Presse (1), Frankfurter Rundschau (1), Generalanzeiger Bonn (2), G?ttinger Tageblatt (1), Haller Kreisblatt (1), Hellweger Anzeiger (1), Junge Welt (1), Kreisanzeiger (1), Mainpost (1), Merkur (3), Mittelbayerische Nachrichten (2), Morgenpost (1), Mitteldeutsche Zeitung (1), Neue Presse Coburg (1), N?rtinger Zeitung (1), OE24 (1), K?lnische Rundschau (1), Tagesspiegel (1), Volksfreund (1), Volksstimme (1), Wiener Zeitung (1), Westf?lische Nachrichten (2). Finnish Aamulehti (2), Etel?-Saimaa (1), Etel?-Suomen Sanomat (3), Helsingin Sanomat (13), Ilkka (7), Ilta-Sanomat (18), Kaleva (4), Karjalainen (2), Kouvolan Sanomat (1), Lapin Kansa (3), Maaseudun Tulevaisuus (1). Russian 168.ru (1), aif (6), altapress.ru (1), argumenti.ru (8), BBC Russian (1), dp.ru (2), gazeta.ru (4), interfax (2),  Kommersant (12) , lenta.ru (8), lgng (3), mk (5), novinite.ru (1), rbc.ru (1), rg.ru (2), rusplit.ru (1), Sport Express (6), vesti.ru (10). Table  1 : Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for each document in the test set, and the origlang tag indicates the original source language. 

 Europarl Parallel Corpus French ? English German ? English Czech ? English Finnish ? English Sentences 2,007,723 1,  920, 209 646, 605 1, 926, 114 Words 60, 125, 563 55, 642, 101 50, 486, 398 53, 008, 851 14, 946, 399 17, 376, 433 37, 814, 266 52, 723,   create multi-system translations. As a consequence, we get ranking tasks with varying numbers of candidate systems. To avoid overloading the annotators we still allow a maximum of five candidates per ranking task. If we have more multi-system translations, we choose randomly. A brief example should illustrate this more clearly: say we have the following two candidate systems: sysA="This, is 'Magic'" sysX="this is magic" After lowercasing, removal of punctuation and whitespace normalization, which are our criteria for identifying near-identical outputs, both would be collapsed into a single multisystem: sysA+sysX="This, is 'Magic'" The first representative of a group of nearidentical outputs is used as a proxy representing all candidates in the group throughout the evaluation. While there is a good chance that users would have used some of the stripped information, e.g., case to differentiate between the two systems relative to each other, the collapsed system's comparison result against the other candidates should be a good approximation of how human annotators would have ranked them individually. We get a near 2x increase in the number of pairwise comparisons, so the general approach seems helpful. ? After dropping external, crowd-sourced translation assessment in WMT14 we ended up with approximately seventy-five percent less raw comparison data. Still, we were able to compute good confidence intervals on the clusters based on our improved ranking approach. This year, due to the aforementioned cleanup, annotators spent their time more efficiently, resulting in an increased number of final ranking results. We collected a total of 542,732 individual "A > B" judgments this year, nearly double the amount of data compared to WMT14. ? Last year we compared three different models of producing the final system rankings: Expected Wins (used in WMT13), Hopkins and May (HM) and TrueSkill (TS). Overall, we found the TrueSkill method to work best which is why we decided to use it as our only approach in WMT15. We keep using clusters in our final system rankings, providing a partial ordering (clustering) of all evaluated candidate systems. Semantics remain unchanged to previous years: systems in the same cluster could not be meaningfully distinguished and hence are considered to be of equal quality. 

 Evaluation campaign overview WMT15 featured the largest evaluation campaign to date. Similar to last year, we decided to collect researcher-based judgments only. A total of 137 individual annotator accounts have been actively involved. Users came from 24 different research groups and contributed judgments on 9,669 HITs. Overall, these correspond to 29,007 individual ranking tasks (plus some more from incomplete HITs), each of which would have spawned exactly 10 individual "A > B" judgments last year, so we expected at least >290,070 binary data points. Due to our redundancy cleanup, we are able to get a lot more, namely 542,732. We report our inter/intra-annotator agreement scores based on the actual work done (otherwise, we'd artificially boost scores based on inferred rankings) and use the full set of data to compute clusters (where the inferred rankings contribute meaningful data). Human annotation effort was exceptional and we are grateful to all participating individuals and teams. We believe that human rankings provide the best decision basis for machine translation evaluation and it is great to see contributions on this large a scale. In total, our human annotators spent 32 days and 20 hours working in Appraise. The average annotation time per HIT amounts to 4 minutes 53 seconds. Several annotators passed the mark of 100 HITs annotated, some worked for more than 24 hours. We don't take this enormous amount of effort for granted and will make sure to improve the evaluation platform and overall process for upcoming workshops. 

 Data collection The system ranking is produced from a large set of pairwise judgments on the translation quality of candidate systems. Annotations are collected in an evaluation campaign that enlists participants in the shared task to help. Each team is asked to contribute one hundred "Human Intelligence Tasks" (HITs) per primary system submitted. Each HIT consists of three so-called ranking tasks. In a ranking task, an annotator is presented with a source segment, a human reference translation, and the outputs of up to five anonymized candidate systems, randomly selected from the set of participating systems, and displayed in random order. This year, we perform redundancy cleanup as an initial preprocessing step and create multisystem translations. As a consequence, we get ranking tasks with varying numbers of candidate outputs. There are two main benefits to this approach: ? Annotators are more efficient as they don't have to deal with near-identical translations which are notoriously hard to differentiate; and ? Potentially, we get higher quality annotations as near-identical systems will be assigned the same "A > B" ranks, improving consistency. As in previous years, the evaluation campaign is conducted using Appraise 5  (Federmann, 2012) , an open-source tool built using Python's Django framework. At the top of each HIT, the following instructions are provided: You are shown a source sentence followed by several candidate translations. Your task is to rank the translations from best to worst (ties are allowed). Annotators can decide to skip a ranking task but are instructed to do this only as a last resort, e.g., if the translation candidates shown on screen are clearly misformatted or contain data issues (wrong language or similar problems). Only a small number of ranking tasks has been skipped in WMT15. A screenshot of the Appraise ranking interface is shown in Figure  3 . Annotators are asked to rank the outputs from 1 (best) to 5 (worst), with ties permitted. Note that a lower rank is better. The joint rankings provided by a ranking task are then reduced to the fully expanded set of pairwise rankings produced by considering all n 2 ? 10 combinations of all n ? 5 outputs in the respective ranking task. 5 https://github.com/cfedermann/Appraise For example, consider the following annotation provided among outputs A, B, F, H, and J: 1 2 3 4 5 F ? A ? B ? J ? H ? As the number of outputs n depends on the number of corresponding multi-system translations in the original data, we get varying numbers of resulting binary judgments. Assuming that outputs A and F from above are actually near-identical, the annotator this year would see a shorter ranking task: 1 2 3 4 5 AF ? B ? J ? H ? Note that AF is a multi-system translation cover- ing two candidate systems. Both examples would be reduced to the following set of pairwise judgments: A > B, A = F, A > H, A < J B < F, B < H, B < J F > H, F < J H < J Here, A > B should be read is "A is ranked higher than (worse than) B". Note that by this procedure, the absolute value of ranks and the magnitude of their differences are discarded. Our WMT15 approach including redundancy cleanup allows to obtain these judgments at a lower cognitive cost for the annotators. This partially explains why we were able to collect more results this year. For WMT13, nearly a million pairwise annotations were collected from both researchers and paid workers on Amazon's Mechanical Turk, in a roughly 1:2 ratio. Last year, we collected data from researchers only, an ability that was enabled by the use of TrueSkill for producing the partial ranking for each task ( ?3.4). This year, based on our redundancy cleanup we were able to nearly double the amount of annotations, collecting 542,732. See Table  3  for more details. 

 Annotator agreement Each year we calculate annotator agreement scores for the human evaluation as a measure of The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized and displayed in random order), and is asked to rank these according to their translation quality, with ties allowed. the reliability of the rankings. We measured pairwise agreement among annotators using Cohen's kappa coefficient (?)  (Cohen, 1960) . If P (A) be the proportion of times that the annotators agree, and P (E) is the proportion of time that they would agree by chance, then Cohen's kappa is: ? = P (A) ? P (E) 1 ? P (E) Note that ? is basically a normalized version of P (A), one which takes into account how meaningful it is for annotators to agree with each other by incorporating P (E). The values for ? range from 0 to 1, with zero indicating no agreement and 1 perfect agreement. We calculate P (A) by examining all pairs of outputs 6 which had been judged by two or more judges, and calculating the proportion of time that they agreed that A < B, A = B, or A > B. In 6 regardless if they correspond to an individual system or to a set of systems ("multi-system") producing nearly identical translations other words, P (A) is the empirical, observed rate at which annotators agree, in the context of pairwise comparisons. As for P (E), it captures the probability that two annotators would agree randomly. Therefore: P (E) = P (A<B) 2 + P (A=B) 2 + P (A>B) 2 Note that each of the three probabilities in P (E)'s definition are squared to reflect the fact that we are considering the chance that two annotators would agree by chance. Each of these probabilities is computed empirically, by observing how often annotators actually rank two systems as being tied. Table  4  shows final ? values for inter-annotator agreement for WMT11-WMT15 while Table  5  details intra-annotator agreement scores, including the division of researchers (WMT13 r ) and MTurk (WMT13 m ) data. The exact interpretation of the kappa coefficient is difficult, but according to  Landis and Koch (1977) , 0-0.2 is slight, 0.2-0.4 is fair, 0.4-0.6 is moderate, 0.6-0.8 is substantial, and 0.8-1.0 is almost perfect.  The inter-annotator agreement rates improve for most language pairs. On average, these are the best scores we have ever observed in one of our evaluation campaigns, including in WMT11, where results were inflated due to inclusion of the reference in the agreement rates. The results for intra-annotator agreement are more mixed: some improve greatly (Czech and German) while others degrade (French, Russian). Our special language, Finnish, also achieves very respectable scores. On average, again, we see the best intra-annotator agreement scores since WMT11. It should be noted that the improvement is not caused by the "ties forced by our redundancy cleanup". If two systems A and F produced nearidentical outputs, they are collapsed to one multisystem output AF and treated jointly in our agreement calculations, i.e. only in comparison with other outputs. It is only the final TrueSkill scores that include the tie A = F . 

 Producing the human ranking The collected pairwise rankings are used to produce the official human ranking of the systems. For WMT14, we introduced a competition among multiple methods of producing this human ranking, selecting the method based on which could best predict the annotations in a portion of the collected pairwise judgments. The results of this competition were that (a) the competing metrics produced almost identical rankings across all tasks but that (b) one method, TrueSkill, had less variance across randomized runs, allowing us to make more confident cluster predictions. In light of these findings, this year, we produced the human ranking for each task using TrueSkill in the following fashion, following procedures adopted for WMT12: We produce 1,000 bootstrap-resampled runs over all of the available data. We then compute a rank range for each system by collecting the absolute rank of each system in each fold, throwing out the top and bottom 2.5%, and then clustering systems into equivalence classes containing systems with overlapping ranges, yielding a partial ordering over systems at the 95% confidence level. The full list of the official human rankings for each task can be found in Table  6 , which also reports all system scores, rank ranges, and clusters for all language pairs and all systems. The official interpretation of these results is that systems in the same cluster are considered tied. Given the large number of judgments that we collected, it was possible to group on average about two systems in a cluster, even though the systems in the middle are typically in larger clusters. In Figure  4  and 5, we plotted the human evaluation result against everybody's favorite metric BLEU (some of the outlier online systems are Table  5 : ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the human evaluation campaign. Scores are much higher for WMT15 which makes sense as we enforce annotation consistency through our initial preprocessing which joins near-identical translation candidates into multi-system entries. It seems that the focus on actual differences in our annotation tasks as well as the possibility of having "easier" ranking scenarios for n < 5 candidate systems results in a higher annotator agreement, both for inter-and intra-annotator agreement scores. not included to make the graphs viewable). The plots cleary suggest that a fair comparison of systems of different kinds cannot rely on automatic scores. Rule-based systems receive a much lower BLEU score than statistical systems (see for instance English-German, e.g., PROMT-RULE). The same is true to a lesser degree for statistical syntax-based systems (see English-German, UEDIN-SYNTAX) and online systems that were not tuned to the shared task (see Czech-English, CU-TECTO vs. the cluster of tuning task systems TT-*). 

 Quality Estimation Task The fourth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task  (Callison-Burch et al., 2012; Bojar et al., 2013 Bojar et al., , 2014 , with tasks including both sentence and word-level estimation, using new training and test datasets, and an additional task: document-level prediction. The goals of this year's shared task were: ? Advance work on sentence-and wordlevel quality estimation by providing larger datasets. ? Investigate the effectiveness of quality labels, features and learning methods for documentlevel prediction. ? Explore differences between sentence-level and document-level prediction. ? Analyse the effect of training data sizes and quality for sentence and word-level predic-  tion, particularly the use of annotations obtained from crowdsourced post-editing. Three tasks were proposed: Task 1 at sentence level (Section 4.3), Task 2 at word level (Section 4.4), and Task 3 at document level (Section 4.5). Tasks 1 and 2 provide the same dataset with English-Spanish translations generated by the statistical machine translation (SMT) system, while Task 3 provides two different datasets, for two language pairs: English-German (EN-DE) and German-English (DE-EN) translations taken from all participating systems in WMT13  (Bojar et al., 2013) . These datasets were annotated with different labels for quality: for Tasks 1 and 2, the labels were automatically derived from the post-editing of the machine translation output, while for Task 3, scores were computed based on reference translations using Meteor  (Banerjee and Lavie, 2005) . Any external resource, including additional quality estimation training data, could be used by participants (no distinction between open and close tracks was made). As presented in Section 4.1, participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 4.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were not made available this year as multiple MT systems were used to produce the datasets, especially for Task 3, including online and rulebased systems. Therefore, as a general rule, participants could only use black-box features. 

 Baseline systems Sentence-level baseline system: For Task 1, QUEST 7  (Specia et al., 2013)  was used to extract 17 MT system-independent features from the source and translation (target) files and parallel corpora: ? Number of tokens in the source and target sentences. ? Average source token length. ? Average number of occurrences of the target word within the target sentence. ? Number of punctuation marks in source and target sentences. ? Language model (LM) probability of source and target sentences based on models for the WMT News Commentary corpus. ? Average number of translations per source word in the sentence as given by IBM Model 1 extracted from the WMT News Commentary parallel corpus, and thresholded such that P (t|s) > 0.2/P (t|s) > 0.01. ? Percentage of unigrams, bigrams and trigrams in frequency quartiles 1 (lower frequency words) and 4 (higher frequency words) in the source language extracted from the WMT News Commentary corpus. ? Percentage of unigrams in the source sentence seen in the source side of the WMT News Commentary corpus. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKIT-LEARN toolkit.  8  The ?, and C parameters were optimised via grid search with 5-fold cross validation on the training set. We note that although the system is referred to as "baseline", it is in fact a strong system. It has proved robust across a range of language pairs, MT systems, and text domains for predicting various forms of post-editing effort  (Callison-Burch et al., 2012; Bojar et al., 2013 Bojar et al., , 2014 . Word-level baseline system: For Task 2, the baseline features were extracted with the MAR-MOT tool 9 . For the baseline system we used a number of features that have been found the most informative in previous research on word-level quality estimation. Our baseline set of features is loosely based on the one described in  (Luong et al., 2014) . It contains the following 25 features: ? Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), but the length of a sentence might influence the probability of a word being incorrect. ? Target token, its left and right contexts of one word. ? Source token aligned to the target token, its left and right contexts of one word. The alignments were produced with the force align.py script, which is part of cdec  (Dyer et al., 2010) . It allows to align new parallel data with a pre-trained alignment model built with the cdec word aligner (fast align). The alignment model was trained on the Europarl corpus  (Koehn, 2005) . ? Boolean dictionary features: whether target token is a stopword, a punctuation mark, a proper noun, a number. ? Target language model features: -The order of the highest order n-gram which starts or ends with the target token. -Backoff behaviour of the n-grams (t i?2 , t i?1 , t i ), (t i?1 , t i , t i+1 ), (t i , t i+1 , t i+2 ) , where t i is the target token (the backoff behaviour is computed as described in  (Raybaud et al., 2011) ). ? The order of the highest order n-gram which starts or ends with the source token. ? Boolean pseudo-reference feature: 1 if the token is contained in a pseudo-reference, 0 otherwise. The pseudo-reference used for this feature is the automatic translation generated by an English-Spanish phrase-based SMT system trained on the Europarl corpus with standard settings. 10 ? The part-of-speech tags of the target and source tokens. ? The number of senses of the target and source tokens in WordNet. We model the task as a sequence prediction problem and train our baseline system using the Linear-Chain Conditional Random Fields (CRF) algorithm with the CRF++ tool. 11 Document-level baseline system: For Task 3, the baseline features for sentence-level prediction were used. These are aggregated by summing or averaging their values for the entire document. Features that were summed: number of tokens in the source and target sentences and number of punctuation marks in source and target sentences. All other features were averaged. The implementation for document-level feature extraction is available in QUEST++  (Specia et al., 2015) .  12  These features were then used to train a SVR algorithm with RBF kernel using the SCIKIT-LEARN toolkit. The ?, and C parameters were optimised via grid search with 5-fold cross validation on the training set. 

 Participants Table 7 lists all participating teams submitting systems to any of the tasks. Each team was allowed up to two submissions for each task and language pair. In the descriptions below, participation in specific tasks is denoted by a task identifier. 

 DCU-SHEFF (Task 2): The system uses the baseline set of features provided for the task. Two pre-processing data manipulation techniques were used: data selection and data bootstrapping. Data selection filters out sentences which have the smallest proportion of erroneous tokens and are assumed to be the least useful for the task. Data bootstrapping enhances the training data with incomplete training sentences (e.g. the first k words of a sentence of the length N , where k < N ). This technique creates additional data instances and boosts the importance of errors occurring in the training data. The combination of these techniques doubled the F 1 score for the "BAD" class, as compared to a models trained on the entire dataset given for the task. The labelling was performed with a CRF model trained using the CRF++ tool, as in the baseline system. HDCL (Task 2): HDCL's submissions are based on a deep neural network that learns continuous feature representations from scratch, i.e. from bilingual contexts. The network was pre-trained by initialising the word lookuptable with distributed word representations, ID Participating team DCU-SHEFF Dublin City University, Ireland and University of Sheffield, UK  (Logacheva et al., 2015 ) HDCL Heidelberg University, Germany  (Kreutzer et al., 2015)  LORIA Lorraine Laboratory of Research in Computer Science and its Applications, France (Langlois, 2015) RTM-DCU Dublin City University, Ireland  (Bicici et al., 2015)  SAU-KERC Shenyang Aerospace University, China  (Shang et al., 2015)  SHEFF-NN University of Sheffield Team 1, UK  (Shah et al., 2015)  UAlacant Alicant University, Spain (Espl?-Gomis et al., 2015a) UGENT Ghent University, Belgium  (Tezcan et al., 2015 ) USAAR-USHEF University of Sheffield, UK and Saarland University, Germany  (Scarton et al., 2015a ) USHEF University of Sheffield, UK  (Scarton et al., 2015a)  HIDDEN Undisclosed and fine-tuned for the quality estimation classification task by back-propagating wordlevel prediction errors using stochastic gradient descent. In addition to the continuous space deep model, a shallow linear classifier was trained on the provided baseline features and their quadratic expansion. One of the submitted systems (QUETCH) relies on the deep model only, the other (QUETCHPLUS) is a linear combination of the QUETCH system score, the linear classifier score, and binary and binned baseline features. The system combination yielded significant improvements, showing that the deep and shallow models each contributes complementary information to the combination. LORIA (Task 1): The LORIA system for Task 1 is based on a standard machine learning approach where source-target sentences are described by numerical vectors and SVR is used to learn a regression model between these vectors and quality scores. Feature vectors used the 17 baseline features, two Latent Semantic Indexing (LSI) features and 31 features based on pseudo-references. The LSI approach considers source-target pairs as documents, and projects the TF-IDF wordsdocuments matrix into a reduced numerical space. This leads to a measure of similarity between a source and a target sentence, which was used as a feature. Two of these features were used based on two matrices, one from the Europarl corpus and one from the official training data. Pseudoreferences were produced by three online systems. These features measure the intersection between n-gram sets of the target sentence and of the pseudo-references. Three sets of features were extracted from each online system, and a fourth feature was extracted measuring the inter-agreement among the three online systems and the target system. HIDDEN (Task 3): This submission, whose creators preferred to remain anonymous, estimates the quality of a given document by explicitly identifying potential translation errors in it. Translation error detection is implemented as a combination of human expert knowledge and different language processing tools, including named entity recognition, part-of-speech tagging and word alignments. 

 RTM-DCU In particular, the system looks for patterns of errors defined by human experts, taking into account the actual words and the additional linguistic information. With this approach, a wide variety of errors can be de-tected: from simple misspellings and typos to complex lack of agreement (in genre, number and tense), or lexical inconsistencies. Each error category is assigned an "importance", again according to human knowledge, and the amount of error in the document is computed as the weighted sum of the identified errors. Finally, the documents are sorted according to this figure to generate the final submission to the ranking variant of Task 3. 

 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. It is similar to Task 1.2 in WMT14. HTER  (Snover et al., 2006b)  is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: ? Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. ? Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true HTER scores. Data The data is the same as that used for the WMT15 Automatic Post-editing task, 15 as kindly provided by Unbabel.  16  Source segments are tokenized English sentences from the news domain with at least four tokens. Target segments are tokenized Spanish translations produced by an online SMT system. The human post-editions are a manual revision of the target, collected using Unbabel's crowd post-editing platform. HTER labels were computed using the TERCOM tool 17 with default settings (tokenised, case insensitive, exact matching only), but with scores capped to 1. As training and development data, we provided English-Spanish datasets with 11, 271 and 1, 000 source sentences, their machine translations, post-editions and HTER scores, respectively. As test data, we provided an additional set of 1, 817 English-Spanish source-translations pairs produced by the same MT system used for the training data. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the same metrics as in previous years: ? Scoring: Mean Average Error (MAE) (primary metric, official score for ranking submissions), Root Mean Squared Error (RMSE). ? Ranking: DeltaAvg (primary metric) and Spearman's ? rank correlation. Additionally, we included Pearson's r correlation against the true HTER label, as suggested by  Graham (2015) . Statistical significance on MAE and DeltaAvg was computed using a pairwise bootstrap resampling (1K times) approach with 95% confidence intervals. 18 For Pearson's r correlation, we measured significance using the Williams test, as also suggested in  (Graham, 2015) . Results Table  8  summarises the results for the ranking variant of Task 1. They are sorted from best to worst using the DeltaAvg metric scores as primary key and the Spearman's ? rank correlation scores as secondary key. The results for the scoring variant are presented in Table  9 , sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Pearson's r coefficients for all systems against HTER is given in Table  10 . As discussed in  (Graham, 2015) , the results according to this metric can rank participating systems differently. In particular, we note the SHEF/GP submission, are which is deemed significantly worse than the baseline system according to MAE, but substantially better than the baseline according to Pearson's correlation.  Graham (2015)  argues that the use of MAE as evaluation score for quality estimation tasks is inadequate, as MAE is very sensitive to variance. This means that a system that outputs predictions with high variance is more likely to have high MAE score, even if the distribution follows that of the true labels. Interestingly, according to Pearson's correlation, the systems are ranked exactly in the same way as according to our DeltaAvg metric. The only difference is that the 4th place is now considered significantly different from the three winning submissions. She also argues that the significance tests used with MAE, based on randomised resampling, assume that the data is independent, which is not the case. Therefore, we apply the suggested Williams significance test for this metric. 

 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Often, the overall quality of a translated segment is significantly harmed by specific errors in a small proportion of words. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a binary distinction between 'GOOD' and 'BAD' tokens. The decision to bucket all error types together was made because of the lack of sufficient training data that could allow consideration of more fine-grained error tags. Data This year's word-level task uses the same dataset as Task 1, for a single language pair: English-Spanish. Each instance of the training, development and test sets consists of the following elements: ? Source sentence (English). ? Automatic translation (Spanish). ? Manual post-edition of the automatic translation. ? Word-level binary ("OK"/"BAD") labelling of the automatic translation. The binary labels for the datasets were acquired automatically with the TERCOM tool  (Snover et al., 2006b) .  19  This tool computes the edit distance between machine-translated sentence and its reference (in this case, its post-edited version). It identifies four types of errors: substitution of a word with another word, deletion of a word (word was omitted by the translation system), insertion of a word (a redundant word was added by the translation system), and word or sequence of words shift (word order error). Every word in the machine-translated sentence is tagged with one of these error types or not tagged if it matches a word from the reference. System ID DeltaAvg ? Spearman's ? ? English-Spanish  All the untagged (correct) words were tagged with "OK", while the words tagged with substitution and insertion errors were assigned the tag "BAD". The deletion errors are not associated with any word in the automatic translation, so we could not consider them. We also disabled the shift errors by running TERCOM with the option '-d 0'. The reason for that is the fact that searching for shifts introduces significant noise in the annotation. The system cannot discriminate be-tween cases where a word was really shifted and where a word (especially common words such as prepositions, articles and pronouns) was deleted in one part of the sentence and then independently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Evaluation Submissions were evaluated in terms of classification performance against the original labels. The main evaluation metric is the average F 1 for the "BAD" class. Statistical significance on F 1 for the "BAD" class was computed using approximate randomization tests.  20  Results The results for Task 2 are summarised in Table  12 . The results are ordered by F 1 score for the error (BAD) class. Using the F 1 score for the word-level estimation task has a number of drawbacks. First of all, we cannot use it as the single metric to evaluate the system's quality. The F 1 score of the class "BAD" becomes an inadequate metric when one is also interested in the tagging of correct words. In fact, a naive baseline which tags all words with the class "BAD" would yield 31.75 F 1 score for the "BAD" class in the test set of this task, which is close to some of the submissions and by far exceeds the baseline, although this tagging is uninformative. We could instead use the weighted F 1 score, which would lead to a single F 1 figure where every class is given a weight according to its frequency in the test set. However, we believe the weighted F 1 score does not reflect the real quality of the systems either. Since there are many more instances of the "GOOD" class than there are of the "BAD" class, the performance on the "BAD" class does not contribute much weight to the overall score, and changes in accuracy of error prediction on this less frequent class can go unnoticed. The weighted F 1 score for the strategy which tags all words as "GOOD" would be 72.66, which is higher than the score of many submissions. However, similar to the case of tagging all words as "BAD", this strategy is uninformative. In an attempt to find more intuitive ways of evaluating word-level tasks, we introduce a new metric called sequence correlation. It gives higher importance to the instances of the "BAD" class and is robust against uninformative tagging. The basis of the sequence correlation metric is the number of matching labels in the reference and the hypothesis, analogously to a precision metric. However, it has some additional features that are aimed at making it more reliable. We consider the tagging of each sentence separately as a sequence of tags. We divide each sequence into sub-sequences tagged by the same tag, for example, the sequence "OK BAD OK OK OK" will be represented as a list of 3 sub-sequences: [ "OK", "BAD", "OK OK OK" ]. Each subsequence has also the information on its position in the original sentence. The sub-sequences of the reference and the hypothesis are then intersected, and the number of matching tags in the corresponding subsequences is computed so that every sub-sequence can be used only once. Let us consider the following example: Reference: OK BAD OK OK OK Hypothesis: OK OK OK OK OK Here, the reference has three sub-sequences, as in the previous example, and the hypothesis consists of only one sub-sequence which coincides with the hypothesis itself, because all the words were tagged with the "OK" label. The precision score for this sentence will be 0.8, as 4 of 5 labels match in this example. However, we notice that the hypothesis sub-sequence covers two matching sub-sequences of the reference: word 1 and words 3-5. According to our metric, the hypothesis sub-sequence can be used for the intersection only once, giving either 1 of 5 or 3 of 5 matching words. We choose the highest value and get the score of 0.6. Thus, the intersection procedure downweighs the uninformative hypotheses where all words are tagged with one tag. In order to compute the sequence correlation we need to get the set of spans for each label in both the prediction and the reference, and then intersect them. A set of spans of each tag t in the string w is computed as follows:  Then the intersection of spans for all labels is: weighted F 1 F 1 F 1 System ID All Bad ? GOOD English-Spanish ? UAlacant/OnLine- Int(y, ?) = t?{0;1} ? t sy?St(y) s ? ?St(?) |s y ? s ?| Here ? t is the weight of a tag t in the overall result. It is inversely proportional the number of instances of this tag in the reference: ? t = |y| c t (?) where c t (?) is the number of words labelled with the label t in the prediction. Thus we give the equal importance to all tags. The sum of matching spans is also weighted by the ratio of the number of spans in the hypothesis and the reference. This is done to downweigh the system tagging if the number of its spans differs from the number of spans provided in the gold standard. This ratio is computed as follows: r(y, ?) = min( |y| |?| ; |?| |y| ) This ratio is 1 if the number of spans is equal for the hypothesis and the reference, and less than 1 otherwise. The final score for a sentence is produced as follows: SeqCor(y, ?) = r(y, ?) ? Int(y, ?) |y| (1) Then the overall sequence correlation for the whole dataset is the average of sentence scores. Table  13  shows the results of the evaluation according to the sequence correlation metric. The results for the two metrics are quite different: one of the highest scoring submissions according to the F 1 -BAD score is only the third under the sequence correlation metric, and vice versa: the submissions with the highest sequence correlation feature in 3rd place according to F 1 -BAD score. However, the system rankings produced by two metrics are correlated -their Spearman's correlation coefficient between them is 0.65. The sequence correlation metric gives preference to systems that use sequence labelling (modelling dependencies between the assigned tags). We consider this a desirable feature, as we are generally not interested in maximising the prediction accuracy for individual words, but in maximising the accuracy for word-level labelling in the context of the whole sentence. However, using the TER alignment to tag errors cannot capture "phraselevel errors", and each token is considered independently when the dataset is built. This is a fundamental issue with the current definition of the word-level quality estimation that we intend to address in future work. 

 Sequence Our intuition is that the sequence correlation metric should be closer to human perception of word-level QE than F 1 scores. The goal of wordlevel QE is to identify incorrect segments of a sentence -and the sequence correlation metric evaluates how good the segmentation of the sentence is into correct and incorrect phrases. A system can get very high F 1 score by (almost) randomly assigning a correct tag to a word, and giving very little information on correct and incorrect areas in the text. That was illustrated by the WMT14 wordlevel QE task results, where the baseline strategy that assigned tag "BAD" to all words had significantly higher F 1 score than any of the submissions. fundamental problem with the current task. I added a sentence about it at the end of the paragraph before this one. 

 Task 3: Predicting document-level quality Predicting the quality of units larger than sentences can be useful in many scenarios. For example, consider a user searching for information about a product on the web. The user can only find reviews in German but he/she does not speak the language, so he/she uses an MT system to translate the reviews into English. In this case, predictions on the quality of individual sentences in a translated review are not as informative as predictions on the quality of the entire review. With the goal of exploring quality estimation beyond sentence level, this year we proposed a document-level task for the first time. Due to the lack of large datasets with machine translated documents (by various MT systems), we consider short paragraphs as documents. The task consisted in scoring and ranking paragraphs according to their predicted quality. Data The paragraphs were extracted from the WMT13 translation task test data  (Bojar et al., 2013) , using submissions from all participating MT systems. Source paragraphs were randomly chosen using the paragraph markup in the SGML files. For each source paragraph, a translation was taken from a different MT system such as to select approximately the same number of instances from each MT system. We considered EN-DE and DE-EN as language pairs, extracting 1, 215 paragraphs for each language pair. 800 paragraphs were used for training and 415 for test. Since no human annotation exists for the quality of entire paragraphs (or documents), Meteor against reference translations was used as quality label for this task. Meteor was calculated using its implementation within the Asyia toolkit, with the following settings: match, tokenised and case insensitive  (Gim?nez and M?rquez, 2010) . Evaluation The evaluation of the paragraphlevel task was the same as that for the sentencelevel task. MAE and RMSE are reported as evaluation metrics for the scoring task, with MAE as official metric for systems ranking. For the ranking task, DeltaAvg and Spearman's ? correlation are reported, with DeltaAvg as official metric for systems ranking. To evaluate the significance of the results, bootstrap resampling (1K times) with 95% confidence intervals was used. Pearson's r correlation scores with the Williams significance test are also reported. Results Table  14  summarises the results of the ranking variant of Task 3. 21 They are sorted from best to worst using the DeltaAvg metric scores as primary key and the Spearman's ? rank correlation scores as secondary key. RTM-DCU submissions achieved the best scores: RTM-SVR was the winner for EN-DE, and RTM-FS-SVR for DE-EN. For EN-DE, the HIDDEN system did not show significant difference against the baseline. For DE-EN, USHEF/QUEST-DISC-BO, USAAR-USHEF/BFF and HIDDEN were not significantly different from the baseline. The results of the scoring variant are given in Table  15 , sorted from best to worst by using the MAE metric scores as primary key and the RMSE metric scores as secondary key. Again the RTM-DCU submissions scored the best for both lan-21 Results for MAE, RMSE and DeltaAvg are multiplied by 100 to improve readability. guage pairs. All systems were significantly better than the baseline. However, the difference between the baseline system and all submissions was much lower in the scoring evaluation than in the ranking evaluation. Following the suggestion in  (Graham, 2015) , Table  16  shows an alternative ranking of systems considering Pearson's r correlation results. The alternative ranking differs from the official ranking in terms of MAE: for EN-DE, RTM-DCU/RTM-FS-SVR is no longer in the winning group, while for DE-EN, USHEF/QUEST-DISC-BO and USAAR-USHEF/BFF did not show statistically significant difference against the baseline. However, as with Task 1 these results are the same as the official ones in terms of DeltaAvg. 

 Discussion In what follows, we discuss the main findings of this year's shared task based on the goals we had previously identified for it. 

 Advances in sentence-and word-level QE For sentence-level prediction, we used similar data and quality labels as in previous editions of the task: English-Spanish, news text domain and HTER labels to indicate post-editing effort. The main differences this year were: (i) the much larger size of the dataset, (ii) the way post-editing was performed -by a large number of crowdsourced translators, and (iii) the MT systems used -an online statistical system. We will discuss items (i) and (ii) later in this section. Regarding (iii), the main implication of using an online system was that one could not have access to many of the resources commonly used to extract features, such as the SMT training data and lexical tables. As a consequence, surrogate resources were used for certain features, including many of the baseline ones, which made them less effective. To avoid relying on such resources, novel features were explored, for example those based on deep neural network architectures (word embeddings and continuous space language models by SHEFF-NN) and those based on pseudo-references (n-gram overlap and agreement features by LORIA). While it is not possible to compare results directly with those published in previous years, for sentence level we can observe the following with respect to the corresponding task in WMT14 (Task 1.2): System ID DeltaAvg ? Spearman's ? ? English-German Table  15 : Official results for the scoring variant of the WMT15 quality estimation Task 3. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. ? In terms of scoring, according to the primary metric -MAE, in WMT15 all systems except one were significantly better than the baseline. In both WMT14 and WMT15 only one system was significantly worse than the baseline. However, in WMT14 four others (out of nine) performed no different than the baseline. This year, no system tied with the baseline: the remaining seven systems were significantly better than the baseline. One could say systems are consistently better this year. It is worth mentioning that the baseline remains the same, but as previously noted, the resources used to extract baseline features are likely to be less useful this year given the mismatch between the data used to produce them and the data used to build the online SMT system. ? In terms of ranking, in WMT14 one system was significantly worse than the baseline, and the four remaining systems were significantly better. This year, all eight submissions are significantly better than the baseline. This can once more be seen as progress from last year's results. These results as well as the general ranking of systems were also found following Pearson's correlation as metric, as System ID Pearson's r ? English-German suggested by  Graham (2015) . For the word level task, a comparison with the WMT14 corresponding task is difficult to perform, as in WMT14 we did not have a meaningful baseline. The baseline used then for binary classification was to tag all words with the label "BAD". This baseline outperformed all the submissions in terms of F 1 for the "BAD" class, but it cannot be considered an appropriate baseline strategy (see Section 4.4). This year the submissions were compared against the output of a real baseline system and the set of baseline features was made available to participants. Although the baseline system itself performed worse than all the submitted systems, some other systems benefited from adding baseline features to their feature sets (UAlacant, UGENT, HDCL). Considering the feature sets and methods used, the number of participants in the WMT14 wordlevel task was too small to draw reliable conclusion: four systems for English-Spanish and one system for all other three language pairs. The larger number of submissions this year is already a positive result: 16 submissions from eight teams. Inspecting the systems submitted this and last year, we can speculate about the most promising techniques. Last year's winning system used a neural network trained on pseudo-reference features (namely, features extracted from n-best lists) (Camargo de  Souza et al., 2014) . This year's winning systems are also based on pseudo-reference features (UAlacant) and deep neural network architectures (HDCL).  Luong et al. (2013)  had pre-viously reported that pseudo-reference features improve the accuracy of word-level predictions. The two most recent editions of this shared task seem to indicate that the state of the art in wordlevel quality estimation relies upon such features, as well as the ability to model the relationship between the source and target languages using large datasets. 

 Effectiveness of quality labels, features and learning methods for document-level QE The task of paragraph-level prediction received fewer submissions than the other two tasks: four submissions for the scoring variant and five for the ranking variant, for both language pairs. This is understandable as it was the first time the task was run. Additionally, paragraph-level QE is still fairly new as a task. However, we were able to draw some conclusions and learn valuable lessons for future research in the area. By and large, most features are similar to those used for sentence-level prediction. Discourseaware features showed only marginal improvements relative to the baseline system (USHEF systems for  EN-DE and DE-EN) . One possible reason for that is the way the training and test data sets were created, including paragraphs with only one sentence. Therefore, discourse features could not be fully explored as they aim to model relationships and dependencies across sentences, as well as within sentences. In future, data will be selected more carefully in order to consider only paragraphs or documents with more sentences. Systems applying feature selection techniques, such as USAAR-USHEF/BFF, did not obtain major improvements over the baseline. However, they provided interesting insights by finding a minimum set of baseline features that can be used to build models with the same performance as the entire baseline feature set. These are models with only three features selected as the best combination by exhaustive search. The winning submissions for both language pairs and variants -RTM-DCU -explored features based on the source and target side information. These include distributional similarity, closeness of test instances to the training data, and indicators for translation quality. External data was used to select "interpretants", which contain data close to both training and test sets to provide context for similarity judgements. In terms of quality labels, one problem observed in previous work on document-level QE  (Scarton et al., 2015b)  is the low variation of scores (in this case, Meteor) across instances of the dataset. Since the data collected for this task included translations from many different MT systems, this was not the case. Table  17  shows the average and standard deviation (STDEV) values for the datasets (both training and test set together). Although the variation is substantial, the average value of the training set is a good predictor. In other words, if we consider the average of the training set scores as the prediction value for all data points in the test set, we obtain results as good as the baseline system. For our datasets, the MAE figure for EN-DE is 10, and for DE-EN 7 -the same as the baseline system. We can only speculate that automatically assigned quality labels based on reference translations such as Meteor are not adequate for this task. Other automatic metrics tend to behave similarly to Meteor for documentlevel  (Scarton et al., 2015b) . Therefore, finding an adequate quality label for document-level QE remains an open issue. Having humans directly assign quality labels is much more complex than in the sentence and word level cases. Annotation of entire documents, or even paragraphs, becomes a harder, more subjective and much more costly task. For future editions of this task, we intend to collect datasets with human-targeted documentlevel labels obtained indirectly, e.g. through postediting. algorithms specifically targeted at document-level prediction. 

 No submission focused on exploring learning Differences between sentence-level and document-level QE The differences between sentence and documentlevel prediction have not been explored to a great extent. Apart from the discourse-aware features by USHEF, the baseline and other features explored by participating teams for document level prediction were simple aggregations of sentence level feature values. Also, none of the submitted systems use sentence-level predictions as features for paragraph-level QE. Although this technique is possible in principle, its effectiveness has not yet been proved.  (Specia et al., 2015)  report promising results when using word-level prediction for sentence-level QE, but inclusive results when using sentence-level prediction for document-level QE. They considered BLEU, TER and Meteor as quality labels, all leading to similar findings. Once more the use of inadequate quality labels for document-level prediction could have been the reason. No submission evaluated different machine learning algorithms for this task. The same algorithms as those used for sentence-level prediction were applied by all participating teams. 

 Effect of training data sizes and quality for sentence and word-level QE As it was previously mentioned, the post-editions used for this year's sentence and word-level tasks were obtained through a crowdsourcing platform where translators volunteered to post-edit machine translations. As such, one can expect that not all post-editions will reach the highest standards of professional translation. Manual inspection of a small sample of the data, however, showed that the post-editions were high quality, although stylistic differences are evident in some cases. This is likely due to the fact that different editors, with different styles and levels of expertise, worked on different segments. Another factor that may have influenced the quality of the post-editions is the fact that segments were fixed out of context. For word level, in particular, a potential issue is the fact that the labelling of the words was done completely automatically, using a tool for alignment based on minimum edit distance (TER). On the positive side, this dataset is much larger dataset than any we have used before for prediction at any level: nearly 12K segments for training/development, as opposed to maximum 2K in previous years. For sentence-level prediction we did not expect massive gains from larger datasets, as it has been shown that small amounts of data can be as effective or even more effective than the entire collection, if selected in a clever way  (Beck et al., 2013a,b) . However, it is well known that data sparsity is an issue for word-level prediction, so we expected a large dataset to improve results considerably for this task. Unfortunately, having access to a large number of samples did not seem to bring much improvement for word-level predictions accuracy. The main reason for that was the fact that the number of erroneous words in the training data was too small, as compared to the number of correct words: 50% of the sentences had zero incorrect words (15% of the sentences) or fewer than 15% incorrect words (35% of the sentences). Participants used various data manipulation strategies to improve results: filtering of the training data, as in DCU-SHEFF systems, alternative labelling of the data which discriminates between "OK" label in the beginning, middle, and end of a good segment, and insertion of additional incorrect words, as in SAU-KERC submissions. Additionally, most participants in the word-level task leveraged additional data in some way, which points to the need for even larger but more varied post-edited datasets in order to make significant progress in this task. 

 Automatic Post-editing Task This year WMT hosted for the first time a shared task on automatic post-editing (APE) for machine translation. The task requires to automatically correct the errors present in a machine translated text. As pointed out in  Parton et al. (2012)  and  Chatterjee et al. (2015b) , from the application point of view, APE components would make it possible to: ? Improve MT output by exploiting information unavailable to the decoder, or by per- forming deeper text analysis that is too expensive at the decoding stage; ? Cope with systematic errors of an MT system whose decoding process is not accessible; ? Provide professional translators with improved MT output quality to reduce (human) post-editing effort; ? Adapt the output of a general-purpose MT system to the lexicon/style requested in a specific application domain. The first pilot round of the APE task focused on the challenges posed by the "black-box" scenario in which the MT system is unknown and cannot be modified. In this scenario, APE methods have to operate at the downstream level (that is after MT decoding), by applying either rule-based techniques or statistical approaches that exploit knowledge acquired from human post-editions provided as training material. The objectives of this pilot were to: i) define a sound evaluation framework for the task, ii) identify and understand the most critical aspects in terms of data acquisition and system evaluation, iii) make an inventory of current approaches and evaluate the state of the art and iv) provide a milestone for future studies on the problem. 

 Task description Participants were provided with training and development data consisting of (source, target, human post-edition) triplets, and were asked to return automatic post-editions for a test set of unseen (source, target) pairs. 

 Data Training, development and test data were created by randomly sampling from a collection of English-Spanish (source, target, human postedition) triplets drawn from the news domain.  22  Instances were sampled after applying a series of data cleaning steps aimed at removing duplicates and those triplets in which any of the elements (source, target, post-edition) was either too long or too short compared to the others, or included tags or special problematic symbols. The main reason for random sampling was to induce some homogeneity across the three datasets and, in turn, to increase the chances that correction patterns learned from the training set can be applied also to the test set. The downside of losing information yielded by text coherence (an aspect that some APE systems might take into consideration) has hence been accepted in exchange for a higher error repetitiveness across the three datasets. Table  18  provides some basic statistics about the data. The training and development sets respectively consist of 11, 272 and 1, 000 instances. In each instance: ? The source (SRC) is a tokenized English sentence having a length of at least 4 tokens. This constraint on the source length was posed in order to increase the chances to work with grammatically correct full sentences instead of phrases or short keyword lists; ? The target (TGT) is a tokenized Spanish translation of the source, produced by an unknown MT system; ? The human post-edition (PE) is a manuallyrevised version of the target. PEs were collected by means of a crowdsourcing platform developed by the data provider. Test data (1, 817 instances) consists of (source, target) pairs having similar characteristics of those in the training set. Human post-editions of the test target instances were left apart to measure system performance. The data creation procedure adopted, as well as the origin and the domain of the texts pose specific challenges to the participating systems. As discussed in Section 5.4, the results of this pilot task can be partially explained in light of such challenges. This dataset, however, has three major advantages that made it suitable for the first APE pilot: i) it is relatively large (hence suitable to apply statistical methods), ii) it was not previously published (hence usable for a fair evaluation), iii) it is freely available (hence easy to distribute and use for evaluation purposes). 

 Evaluation metric System performance is evaluated by computing the distance between automatic and human post-editions of the machine-translated sentences present in the test set (i.e. for each of the 1,817 target test sentences). This distance is measured in terms of Translation Error Rate (TER)  (Snover et al., 2006a) , an evaluation metric commonly used in MT-related tasks (e.g. in quality estimation) to measure the minimum edit distance between an automatic translation and a reference translation.  23  Systems are ranked based on the average TER calculated on the test set by using the TERcom 24 software: lower average TER scores correspond to higher ranks. Each run is evaluated in two modes, namely: i) case insensitive and ii) case sensitive. Evaluation scripts to compute TER scores in both modalities have been made available to participants through the APE task web page.  25   

 Baseline The official baseline is calculated by averaging the distances computed between the raw MT output and the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.  26  Baseline results computed for both evaluation modalities (case sensitive/insensitive) are reported in Tables  20 and 21 . Monolingual translation as another term of comparison. To get further insights about the progress with respect to previous APE methods, participants' results are also analysed with respect to another term of comparison: a reimplementation of the state-of-the-art approach firstly proposed by  Simard et al. (2007) .  27  For this purpose, a phrase-based SMT system based on Moses  (Koehn et al., 2007)  is used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++  (Gao and Vogel, 2008)  for word alignment. For language modeling we used the  23  Edit distance is calculated as the number of edits (word insertions, deletions, substitutions, and shifts) divided by the number of words in the reference. Lower TER values indicate better MT quality. 24 http://www.cs.umd.edu/ ?snover/tercom/ 25 http://www.statmt.org/wmt15/ape-task.html 26 In this case, since edit distance is computed between each machine-translated sentence and its human-revised version, the actual evaluation metric is the human-targeted TER (HTER). For the sake of clarity, since TER and HTER compute edit distance in the same way (the only difference is in the origin of correct sentence used for comparison), henceforth we will use TER to refer to both metrics. 27 This is done based on the description provided in  Simard et al. (2007) . Our re-implementation, however, is not meant to officially represent such approach. Discrepancies with the actual method are indeed possible due to our misinterpretation or to wrong guesses about details that are missing in the paper.  KenLM toolkit  (Heafield, 2011)  for standard ngram modeling with an n-gram length of 5. Finally, the APE system was tuned on the development set, optimizing TER with Minimum Error Rate Training  (Och, 2003) . The results of this additional term of comparison, computed for both evaluation modalities (case sensitive/insensitive), are also reported in Tables  20 and 21 . For each submitted run, the statistical significance of performance differences with respect to the baseline and the re-implementation of  Simard et al. (2007)  is calculated with the bootstrap test  (Koehn, 2004) . 

 Participants Four teams participated in the APE pilot task by submitting a total of seven runs. Participants are listed in Table  19 ; a short description of their systems is provided in the following. Abu-MaTran. The Abu-MaTran team submitted the output of two statistical post-editing (SPE) systems, both relying on the MOSES phrase-based statistical machine translation toolkit  (Koehn et al., 2007)  and on sentence level classifiers. The first element of the pipeline, the SPE system, is trained on the automatic translation of the News Commentary v8 corpus from English to Spanish aligned with its reference. This translation is obtained with an out-of-thebox phrase-based SMT system trained on Europarl v7. Both translation and post-editing systems use a 5-gram Spanish LM with modified Kneser-Ney smoothed trained on News Crawl 2011 and 2012 with KenLM  (Heafield, 2011) . For the second element of the pipeline, a binary classifier to select the best translation between the given MT output or its automatic post-edition is used. Two different approaches are investigated: a 180-hand-craftedbased regression model trained with a Support Vector Machine (SVM) with a radial basis function kernel to estimate the sentence-level HTER score, and a Recurrent Neural Network (RNN) classifier using context word embeddings as input and classifying each word of a sentence as good or bad. An automatic translation to be post-edited is first decoded by our SPE system, then fed into one of the classifiers identified as SVM180feat and RNN. The HTER estimator selects the translation with the lower score while the binary word-level classifier selects the translation with the fewer amount of bad tags. The official evaluation of the shared task show an advantage of the RNN approach compared to SVM. FBK. The two runs submitted by FBK  (Chatterjee et al., 2015a)  are based on combining the statistical phrase-based post-editing approach proposed by  Simard et al. (2007)  and its most significant variant proposed by  B?chara et al. (2011) . The APE systems are built-in an incremental manner. At each stage of the APE pipeline, the best configuration of a component is decided and then used in the next stage. The APE pipeline begins with the selection of the best language model from several language models trained on different types and quantities of data. The next stage addresses the possible data sparsity issues raised by the relatively small size of the training data. Indeed, an analysis of the original phrase table obtained from the training set revealed that a large part of its entries is composed of instances that occur only once in the training. This has the obvious effect of collecting potentially unreliable "translation" (or, in the case of APE, correction) rules. The problem is exacerbated by the "context-aware" approach proposed by  B?chara et al. (2011) , which builds the phrase table by joining source and target tokens thus breaking down the co-occurrence counts into smaller numbers. To cope with this problem, a novel feature (neg-impact) is designed to prune the phrase table by measuring the usefulness of each translation. The higher is the value of the negimpact feature, the less useful is the translation option. After pruning, the final stage of the APE pipeline tries to raise the capability of the decoder to select the correct translation rule by the introduction of new task specific features integrated in   (Pal et al., 2015b)  Table  19 : Participants in the WMT15 Automatic Post-editing pilot task. the model. These features measure the similarity and the reliability of the translation options and help to improve the precision of the resulting APE system. LIMSI. For the first edition of the APE shared task LIMSI submitted two systems  (Wisniewski et al., 2015) . The first one is based on the approach of  Simard et al. (2007)  and considers the APE task as a monolingual translation between a translation hypothesis and its post-edition. This straightforward approach does not succeed in improving translation quality. The second submitted system implements a series of sieves, each applying a simple post-editing rule. The definition of these rules is based on an analysis of the most frequent error corrections and aims at: i) predicting word case; ii) predicting exclamation and interrogation marks; and iii) predicting verbal endings. Experiments with this approach show that this system also hurts translation quality. An in-depth analysis revealed that this negative result is mainly explained by two reasons: i) most of the post-edition operations are nearly unique, which makes very difficult to generalize from a small amount of data; and ii) even when they are not, the high variability of post-editing, already pointed out by  Wisniewski et al. (2013) , results in predicting legitimate corrections that have not been made by the annotators, therefore preventing from improving over the baseline. 

 USAAR-SAPE. The USAAR-SAPE system  (Pal et al., 2015b)  is designed with three basic components: corpus preprocessing, hybrid word alignment and a state-of-the-art phrase-based SMT system integrated with the hybrid word alignment. The preprocessing of the training corpus is carried out by stemming the Spanish MT output and the PE data using Freeling  (Padr and Stanilovsky, 2012) . The hybrid word alignment method combines different kinds of word alignment: GIZA++ word alignment with the grow-diag-final-and (GDFA) heuristic  (Koehn, 2010) , SymGiza++  (Junczys-Dowmunt and Szal, 2011) , the Berkeley aligner  (Liang et al., 2006) , and the edit distance-based aligners  (Snover et al., 2006a; Lavie and Agarwal, 2007) . These different word alignment tables  (Pal et al., 2013)  are combined by a mathematical union method. For the phrase-based SMT system various maximum phrase lengths for the translation model and n-gram settings for the language model are used. The best results in terms of BLEU  (Papineni et al., 2002)  score are achieved by a maximum phrase length of 7 for the translation model and a 5-gram language model. 

 Results The official results achieved by the participating systems are reported in Tables  20 and 21 . The seven runs submitted are sorted based on the average TER they achieve on test data. Table  20  shows the results computed in case sensitive mode, while Table  21  provides scores computed in the case insensitive mode. Both rankings reveal an unexpected outcome: none of the submitted runs was able to beat the baselines (i.e. average TER scores of 22.91 and 22.22 respectively for case sensitive and case insensitive modes). All differences with respect to such baselines, moreover, are statistically significant. In practice, this means that what the systems learned from the available data was not reliable enough to yield valid corrections of the test instances. A deeper discussion about the possible causes of this unexpected outcome is provided in Section 5.4. Unsurprisingly, for all participants the case insensitive evaluation results are slightly better than the case sensitive ones. Although the two rankings are not identical, none of the systems was particularly penalized by the case sensitive evaluation. Indeed, individual differences in the two modes are always close to the same value (? 0.7 TER difference) measured for the two baselines. In light of this, and considering the importance of case sensitive evaluation in some language settings (e.g. having German as target), future rounds of the task will likely prioritize this more strict evaluation mode. 

 ID Overall, the close results achieved by participants reflect the fact that, despite some small variations, all systems share the same underlying statistical approach of  Simard et al. (2007) . As anticipated in Section 5.1, in order to get a rough idea about the extent of the improvements over such state-of-the-art method, we replicated it and considered its results as another term of comparison in addition to the baselines. As shown in Tables 20 and 21, the performance results achieved by our implementation of  Simard et al. (2007)  are 23.839 and 23.130 in terms of TER for the respective case sensitive and insensitive evaluations. Compared to these scores, most of the submitted runs achieve better performance, with positive average TER differences that are always statistically significant. We interpret this as a good sign: despite the difficulty of the task, the novelties introduced by each system allowed to make significant steps forward with respect to a prior reference technique. 

 Discussion To better understand the results and gain useful insights about this pilot evaluation round, we perform two types of analysis. The first one is focused on the data, and aims to understand the possible reasons of the difficulty of the task. In particular, by analysing the challenges posed by the origin and the domain of the text material used, we try to find indications for future rounds of the APE task. The second type of analysis focuses on the systems and their behaviour. Although they share the same underlying approach and achieve similar results, we aim to check if interesting differences can be captured by a more fine grained analysis that goes beyond rough TER measurements. 

 Data analysis In this section we investigate the possible relation between participants' results and the nature of the data used in this pilot task (e.g. quantity, sparsity, domain and origin) . For this purpose, we take advantage of a new dataset -the Autodesk Post-Editing Data corpus 28 -which has been publicly released after the organisation of the APE pilot task. Although it was not usable for this first round, its characteristics make it particularly suitable for our analysis purposes. In particular: i) Autodesk data predominantly covers the domain of software user manuals (that is, a restricted domain compared to a general one like news), and ii) post-edits come from professional translators (that is, at least in principle, a more reliable source of corrections compared to crowdsourced workforce). To guarantee a fair comparison, English-Spanish (source, target, human postedition) triplets drawn from the Autodesk corpus are split in training, development and test sets under the constraint that the total number of target words and the TER in each set should be similar to the APE task splits. In this setting, performance differences between systems trained on the two datasets will only depend on the different nature of the data (e.g. domain). Statistics of the training sets are reported in APE task data are the same of Table  18 ). The impact of data sparsity. A key issue in most evaluation settings is the representativeness of the training data with respect to the test set used. In the case of the statistical approach at the core of all the APE task submissions, this issue is even more relevant given the limited amount of training data available. In the APE scenario, data representativeness relates to the fact that the correction patterns learned from the training set can be applied also to the test set (as mentioned in Section 5.1, in the data creation phase random sampling from an original data collection was applied for this purpose). From this point of view, dealing with restricted domains such as software user manuals should be easier than working with news data. Indeed, restricted domains are more likely to feature smaller vocabularies, be more repetitive (or, in other terms, less sparse) and, in turn, determine a higher applicability of the learned error correction patterns. To check the relation between task difficulty and data repetitiveness, we compared different monolingual indicators (i.e. number of types and lemmas, and repetition rate 29 -RR) computed on the APE and the Autodesk source, target and postedited sentences. Although both the datasets have the same amount of target tokens, Table  22  shows that the APE training set has nearly double of types and lemmas compared to the Autodesk data,  29  Repetition rate measures the repetitiveness inside a text by looking at the rate of non-singleton n-gram types (n=1. . .4) and combining them using the geometric mean. Larger value means more repetitions in the text. For more details see  Cettolo et al. (2014)  which indicates the presence of less repeated information. A similar conclusion can be drawn by observing that the Autodesk dataset has a repetition rate that is more than twice the value computed for the APE task data. This monolingual analysis does not provide any information about the level of repetitiveness of the correction patterns made by the post-editors, because it does not link the target and the post-edited sentences. To investigate this aspect, two instances of the re-implemented approach of  Simard et al. (2007)  introduced in Section 5.1 are respectively trained on the APE and the Autodesk training sets. We consider the distribution of the frequency of the translation options in the phrase table as a good indicator of the level of repetitiveness of the corrections in the data. For instance, a large number of translation options that appear just one or only few times in the data indicates a higher level of sparseness. As expected due to the limited size of the training set, the vast majority of the translation options in both phrase tables are singletons as shown in Table  23 . Nevertheless, the Autodesk phrase table is more compact (731k versus 1,066k) and contains 10% fewer singletons than the APE task phrase table. This confirms that the APE task data is more sparse and suggests that it might be easier to learn more applicable correction patterns from the Autodesk domain-specific data. To verify this last statement, the two APE systems are evaluated on their own test sets. As previously shown, the system trained on the APE task data is not able to improve over the performance achieved by a system that leaves all the test targets unmodified (see Table  20 ). On the contrary, starting from a baseline of 23.57, the system trained on the Autodesk data is able to reduce the TER by 3.55 points (20.02). Interestingly, the Autodesk APE system is able to correctly fix the target sentences and improve the TER by 1.43 points even with only 25% of the training data. These results confirm our intuitions about the usefulness of repetitive data and show that, at least in restricteddomain scenarios, automatic post-editing can be successfully used as an aid to improve the output of an MT system. 

 Professional vs. Crowdsourced post-editions Differently from the Autodesk data, for which the post-editions are created by professional translators, the APE task data contains crowdsourced MT corrections collected from unknown (likely non-  expert) translators. One risk, given the high variability of valid MT corrections, is that the crowdsourced workforce follows post-editing attitudes and criteria that differ from those of professional translators. Professionals tend to: i) maximize productivity by doing only the necessary and sufficient corrections to improve translation quality, and ii) follow consistent translation criteria, especially for domain terminology. Such a tendency will likely result in coherent and minimally post-edited data from which learning and drawing statistics is easier. This is not guaranteed by crowdsourced workers which do not have specific time or consistency constraints. This suggests that non-professional post-editions and the correction patterns learned from them will feature less coherence, higher noise and higher sparsity. To assess the potential impact of these issues on data representativeness (and, in turn, on the task difficulty), we analyse a subset of the APE test instances (221 triples randomly sampled) in which target sentences were post-edited by professional translators. The analysis focuses on TER scores computed between: 1. The target sentences and their crowdsourced post-editions (avg. TER = 26.02); 2. The target sentences and their professional post-editions (avg. TER = 23.85); 3. The crowdsourced post-editions and the professional ones, using the latter as references (avg. TER = 29.18). The measured values indicate an attitude of nonprofessionals to correct more often and differently from the professional translators. Interestingly, and similar to the findings of  Potet et al. (2012) , crowdsourced post-editions feature a distance from the professional ones that is even higher than the distance between the original target sentences and the experts' corrections (29.18 vs. 23.85). If we consider the output of professional translators as a gold standard (made of coherent and minimally post-edited data), these figures suggest a higher difficulty in handling crowdsourced corrections. Further insights can be drawn from the analysis of the word level corrections produced by the two translator profiles. To this aim, word insertions, deletions, substitutions and phrase shifts are extracted using the TERcom software similar to  Blain et al. (2012)  and  Wisniewski et al. (2013) . For each error type, the ratio between the number of edit operations and the total number of occurred errors operations performed is computed. This quantity provides us with a measure of the level of repetitiveness of the errors, with 100% indicating that all the error patterns are unique, and small values indicating that most of the errors are repeated. Our results show that non-experts have generally larger ratio values than the professional translators (insertion +6%, substitution +4%, deletion +4%). This seems to support our hypothesis that, independently from their quality, post-editions collected from non-experts are less coherent than those derived from professionals. It is unlikely that different crowdsourced workers will apply the same corrections in the same contexts. If this hypothesis holds, the difficulty of this APE pilot task could be partially ascribed to this unavoidable intrinsic property of crowdsourced data. This aspect, however, should be further investigated to draw definite conclusions. 

 System/performance analysis The TER results presented in Tables  20 and 21  evidence small differences between participants, but they do not shed light on the real behaviour of the systems. To this aim, in this section the submitted runs are analysed by taking into consideration the changes made by each system to the test instances (case sensitive evaluation mode). In particular, Table 24 provides the number of modified, improved and deteriorated sentences, together with the percentage of edit operations performed (insertions, deletions, substitutions, shifts). Looking at these numbers, the following conclusions can be drawn. Although it varies considerably between the submitted runs, the number of modified sentences is quite small. Moreover, a general trend can be observed: the best systems are the most conservative ones. This situation likely reflects the aforementioned data sparsity and coherence issues. A small fraction of the correction patterns found in the training set seems to be applicable also to the test set, and the risk of performing corrections that are either wrong, redundant, or different from those in the reference post-editions is rather high. From the system point of view, the context in which a learned correction pattern will be applied is crucial. For instance, the same word substitution (e.g. "house" ? "home") is not applicable in all contexts. While sometimes it will be necessary (Example 1: "The house team won the match"), in some contexts it is optional (Example 2: "I was in my house") or wrong (Example 3: "He worked for a brokerage house"). Unfortunately, the unnecessary word replacement in Example 2 (human posteditors would likely leave it untouched) would increase the TER of the sentence exactly as in the clearly wrong replacement in Example 3. From the evaluation point of view, not penalising such correct but unnecessary corrections is also crucial. Similar to MT, where a source sentence can have many valid translations, in the APE task a target sentence can have many valid posteditions. Indeed, nothing prevents that in our evaluation some correct post-editions are considered as "deteriorated" sentences simply because they differ from the human post-editions used as references. As in MT, this well known variability problem might penalise good systems, thus calling for alternative evaluation criteria (e.g. based on multiple references or sensitive to paraphrase matches). Interestingly, for all the systems the number of modified sentences is higher than the sum of the improved and the deteriorated ones. Such difference is represented by modified sentences for which the corrections do not yield TER variations. This grey area makes the evaluation problem related to variability even more evident. The analysis of the edit operations performed by each system is not particularly informative. Similar to the overall performance results, also the proportion of correction types they perform reflects the adoption of the same underlying statistical approach. The distribution of the four types of edit operations is almost identical, with a predominance lexical substitutions (55.7%-57.7%) and rather few phrasal shifts (8.0%-8.6%). 

 Lessons learned and outlook The objectives of this pilot APE task were to: i) define a sound evaluation framework for future rounds, ii) identify and understand the most critical aspects in terms of data acquisition and system evaluation, iii) make an inventory of current approaches, evaluate the state of the art and iv) provide a milestone for future studies on the problem. With respect to the first point, improving the evaluation is possible, but no major issues emerged or requested radical changes in future evaluation rounds. For instance, using multiple references or a metric sensitive to paraphrase matches to cope with variability in the post-editing would certainly help. Concerning the most critical aspects of the evaluation, our analysis highlighted the strong dependence of system results on data repetitiveness/representativeness. This calls into question the actual usability of text material coming from general domains like news and, probably, of post-editions collected from crowdsourced workers (this aspect, however, should be further investigated to draw definite conclusions). Nevertheless, it's worth noting that collecting a large, unpublished, public, domain-specific and professionalquality dataset is a hardly achievable goal that will always require compromise solutions. Regarding the approaches proposed, this first experience was a conservative but, at the same time, promising first step. Although participants performed the task sharing the same statistical approach to APE, the slight variants they explored allowed them to outperform the widely used monolingual translation technique. Moreover, results' analysis also suggests a possible limitation of this state-of-the-art approach: by always performing all the applicable correction patterns, it runs the risk of deteriorating the input translations that it was supposed to improve. This limitation, common to all the participating systems, is a clue of a major difference between the APE task and the MT framework. In MT the system must always process the entire source sentence by translating all of its words into the target language. In the APE scenario, instead, the system has another option for each word: keeping it untouched. A reasonable (and this year unbeaten) baseline is in fact a system that applies this conservative strategy for all the words. By raising this and other issues as promising research directions, attracting researchers' attention to a challenging applicationoriented task, and establishing a sound evaluation framework to measure future advancements, this pilot has substantially achieved its goals, paving the way for future rounds of the APE evaluation exercise.    .61 .57 .53 .51 .43 -.12 -.18 -.18 -.19 -.21 -.22 -.26 -.29 -.32 -.32 -.35  rank 1 2 3-4 3-4 5 6 7-9 7-10 7-11 8-11 9-11 12-13 13-15 13-15 13-15 15-16 Table  25 : Head to head comparison, ignoring ties, for Czech-English systems 

 A Pairwise System Comparisons by Human Judges Tables 25-34 show pairwise comparisons between systems for each language pair. The numbers in each of the tables' cells indicate the percentage of times that the system in that column was judged to be better than the system in that row, ignoring ties. Bolding indicates the winner of the two systems. Because there were so many systems and data conditions the significance of each pairwise comparison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences (rather than differences that are attributable to chance). In the following tables indicates statistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at p ? 0.01, according to the Sign Test. Each table contains final rows showing how likely a system would win when paired against a randomly selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ? 0.05). Gray lines separate clusters based on non-overlapping rank ranges.   score .68 .51 .50 .46 .42 .26 .20 .11 -.34 -.34 -.34 -.37 -.40 -.56 -.80  rank 1 2-3 2-3 4 5 6 7 8 9-11 9-11 9-11 12 13 14 15   .73 ? .74 ? .74 ? .68 ? .65 ? .66 ? .58 ? .63 ? .61 ? .60 ? .50 score .56 .31 .29 .25 .22 .14 .09 -.17 -.17 -.22 -.30 -.48 -.54  rank 1 2-3 2-4 3-5 4-5 6-7 6-7 8-10 8-10 9-10 11 12-13 12-13 Table 27: Head to head comparison, ignoring ties, for German-English systems  Figure 1 : 1 Figure 1: Example news discussion thread used in the French-English translation task. 

 Figure 2 : 2 Figure 2: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct words (case-insensitive) is based on the provided tokenizer. 

 Figure 3 : 3 Figure3: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized and displayed in random order), and is asked to rank these according to their translation quality, with ties allowed. 

 Figure 4 :Figure 5 : 45 Figure 4: Human evaluation scores versus BLEU scores for the German-English and Czech-English language pairs illustrate the need for human evaluation when comparing systems of different kind. Confidence intervals are indicated by the shaded ellipses. Rule-based systems and to a lesser degree syntax-based statistical systems receive a lower BLEU score than their human score would indicate. The big cluster in the Czech-English plot are tuning task submissions. 

 S t (w) = {w [b:e] }, ?i s.t. b i e : w i = t where w [b:e] is a substring w b , w b+1 , ..., w e?1 , w e . 

 Table 3 : 3 Amount of data collected in the WMT15 manual evaluation campagin. The final four rows report summary information from previous editions of the workshop. Note how many rankings we get for Czech language pairs. These include systems from the tuning shared task. Finnish, as a new language, sees a shortage of rankings for Finnnish?English Interest in French seems to have lowered this year with only seven systems. Overall, we see a nice increase in pairwise rankings, especially considering that we have dropped crowd-source annotation and are instead relying on researchers' judgments exclusively. 

 Table 6 : 6 Official results for the WMT15 translation task. Systems are ordered by their inferred system means, though systems within a cluster are considered tied. Lines between systems indicate clusters according to bootstrap resampling at p-level p ? .05. Systems with grey background indicate use of resources that fall outside the constraints provided for the shared task. Czech-English German-English English-German # score range system # score range system # score range system 1 0.619 1 ONLINE-B 1 0.567 1 ONLINE-B 1 0.359 1-2 UEDIN-SYNTAX 2 0.574 2 UEDIN-JHU 2 0.319 2-3 UEDIN-JHU 0.334 1-2 MONTREAL 3 0.532 3-4 UEDIN-SYNTAX 0.298 2-4 ONLINE-A 2 0.260 3-4 PROMT-RULE 0.518 3-4 MONTREAL 0.258 3-5 UEDIN-SYNTAX 0.235 3-4 ONLINE-A 4 0.436 5 ONLINE-A 0.228 4-5 KIT 3 0.148 5 ONLINE-B 5 -0.125 6 CU-TECTO 3 0.141 6-7 RWTH 4 0.086 6 KIT-LIMSI 6 -0.182 7-9 TT-BLEU-MIRA-D 0.095 6-7 MONTREAL 5 0.036 7-9 UEDIN-JHU -0.189 7-10 TT-ILLC-UVA 4 -0.172 8-10 ILLINOIS 0.003 7-11 ONLINE-F -0.196 7-11 TT-BLEU-MERT -0.177 8-10 DFKI -0.001 7-11 ONLINE-C -0.210 8-11 TT-AFRL -0.221 9-10 ONLINE-C -0.018 8-11 KIT -0.220 9-11 TT-USAAR-TUNA 5 -0.304 11 ONLINE-F -0.035 9-11 CIMS 7 -0.263 12-13 TT-DCU 6 -0.489 12-13 MACAU 6 -0.133 12-13 DFKI -0.297 13-15 TT-METEOR-CMU -0.544 12-13 ONLINE-E -0.137 12-13 ONLINE-E -0.320 13-15 TT-BLEU-MIRA-SP 7 -0.235 14 UDS-SANT -0.320 13-15 TT-HKUST-MEANT -0.358 15-16 ILLINOIS French-English # score range system 8 -0.400 9 -0.501 15 16 ILLINOIS IMS English-Czech 1 0.498 0.446 1-2 ONLINE-B 1-3 LIMSI-CNRS Finnish-English # score range system 0.415 1-3 UEDIN-JHU # score range system 1 0.686 1 CU-CHIMERA 2 0.275 4-5 MACAU 1 0.675 1 ONLINE-B 2 0.515 2-3 ONLINE-B 0.223 4-5 ONLINE-A 2 0.280 2-4 PROMT-SMT 0.503 2-3 UEDIN-JHU 3 -0.423 6 ONLINE-F 0.246 2-5 ONLINE-A 3 0.467 4 MONTREAL 4 -1.434 7 ONLINE-E 0.236 2-5 UU 4 0.426 5 ONLINE-A 0.182 4-7 UEDIN-JHU 5 6 7 8 -0.342 9-11 TT-DCU 0.261 6 UEDIN-SYNTAX 0.209 7 CU-TECTO 0.114 8 COMMERCIAL1 -0.342 9-11 TT-AFRL -0.346 9-11 TT-BLEU-MIRA-D 9 -0.373 12 TT-USAAR-TUNA 10 -0.406 13 TT-BLEU-MERT 11 -0.563 14 TT-METEOR-CMU 12 -0.808 15 TT-BLEU-MIRA-SP English-French # score range system 1 0.540 1 LIMSI-CNRS 2 0.304 2-3 ONLINE-A 0.258 2-4 UEDIN-JHU 0.215 3-4 ONLINE-B 3 -0.001 5 CIMS 4 -0.338 6 ONLINE-F 5 -0.977 7 ONLINE-E 0.160 0.144 0.081 3 -0.081 4 -0.177 5 -0.275 6 -0.438 12-13 LIMSI 5-7 ABUMATRAN-COMB 5-8 UEDIN-SYNTAX 7-8 ILLINOIS 9 ABUMATRAN-HFS 10 MONTREAL 11 ABUMATRAN -0.513 13-14 SHEFFIELD -0.520 13-14 SHEFF-STEM Russian-English # score range system 1 0.494 1 ONLINE-G 2 0.311 2 ONLINE-B 3 0.129 3-6 PROMT-RULE 0.116 3-6 AFRL-MIT-PB 0.113 3-6 AFRL-MIT-FAC 0.104 3-7 ONLINE-A 0.051 6-8 AFRL-MIT-H 0.010 7-10 LIMSI-NCODE -0.021 8-10 UEDIN-SYNTAX -0.031 8-10 UEDIN-JHU English-Russian # score range system 1 1.015 1 PROMT-RULE 2 0.521 2 ONLINE-G 3 0.217 3 ONLINE-B 4 0.122 4-5 LIMSI-NCODE 0.075 4-5 ONLINE-A 5 0.014 6 UEDIN-JHU 6 -0.138 7 UEDIN-SYNTAX 7 -0.276 8 USAAR-GACHA 8 -0.333 9 USAAR-GACHA 9 -1.218 10 ONLINE-F English-Finnish # score range system 1 1.069 1 ONLINE-B 2 0.548 2 ONLINE-A 3 0.210 3 UU 4 0.042 4 ABUMATRAN-COMB 5 -0.059 5 ABUMATRAN-COMB 6 -0.143 6-7 AALTO -0.184 6-8 UEDIN-SYNTAX -0.212 6-8 ABUMATRAN 7 -0.342 CMU 9 8 -0.929 10 CHALMERS 4 -0.218 11 USAAR-GACHA 5 -0.278 12 USAAR-GACHA 6 -0.781 13 ONLINE-F 

 Table 7 : 7 Participants in the WMT15 quality estimation shared task. 

 Table 8 : 8 Official results for the ranking variant of the WMT15 quality estimation Task 1. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to pairwise bootstrap resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. ? LORIA/17+LSI+MT+FILTRE 6.51 0.36 ? LORIA/17+LSI+MT 6.34 0.37 ? RTM-DCU/RTM-FS+PLS-SVR 6.34 0.37 ? RTM-DCU/RTM-FS-SVR 6.09 0.35 UGENT-LT3/SCATE-SVM 6.02 0.34 UGENT-LT3/SCATE-SVM-single 5.12 0.30 SHEF/SVM 5.05 0.28 SHEF/GP 3.07 0.28 Baseline SVM 2.16 0.13 System ID MAE ? RMSE ? English-Spanish ? RTM-DCU/RTM-FS+PLS-SVR 13.25 17.48 ? LORIA/17+LSI+MT+FILTRE 13.34 17.35 ? RTM-DCU/RTM-FS-SVR 13.35 17.68 ? LORIA/17+LSI+MT 13.42 17.45 ? UGENT-LT3/SCATE-SVM 13.71 17.45 UGENT-LT3/SCATE-SVM-single 13.76 17.79 SHEF/SVM 13.83 18.01 Baseline SVM 14.82 19.13 SHEF/GP 15.16 18.97 

 Table 9 : 9 Official results for the scoring variant of the WMT15 quality estimation Task 1. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. System ID Pearson's r ? ? LORIA/17+LSI+MT+FILTRE 0.39 ? LORIA/17+LSI+MT 0.39 ? RTM-DCU/RTM-FS+PLS-SVR 0.38 RTM-DCU/RTM-FS-SVR 0.38 UGENT-LT3/SCATE-SVM 0.37 UGENT-LT3/SCATE-SVM-single 0.32 SHEF/SVM 0.29 SHEF/GP 0.19 Baseline SVM 0.14 

 Table 10 : 10 Alternative results for the scoring variant of the WMT15 quality estimation Task 1. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to Williams test with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. 

 Table 11 . 11 Sentences Words % of "BAD" words Training 11,271 257,548 19.14 Dev 1,000 23,207 19.18 Test 1,817 40,899 18.87 

 Table 11 : 11 Datasets for Task 2. 

 Table 12 : 12 Official results for the WMT15 quality estimation Task 2. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to approximate randomization tests with 95% confidence intervals. Submissions whose results are statistically different from others according to the same test are grouped by a horizontal line. SBI-Baseline 71.47 43.12 78.07 ? HDCL/QUETCHPLUS 72.56 43.05 79.42 UAlacant/OnLine-SBI 69.54 41.51 76.06 SAU/KERC-CRF 77.44 39.11 86.36 SAU/KERC-SLG-CRF 77.4 38.91 86.35 SHEF2/W2V-BI-2000 65.37 38.43 71.63 SHEF2/W2V-BI-2000-SIM 65.27 38.40 71.52 SHEF1/QuEst++-AROW 62.07 38.36 67.58 UGENT/SCATE-HYBRID 74.28 36.72 83.02 DCU-SHEFF/BASE-NGRAM-2000 67.33 36.60 74.49 HDCL/QUETCH 75.26 35.27 84.56 DCU-SHEFF/BASE-NGRAM-5000 75.09 34.53 84.53 SHEF1/QuEst++-PA 26.25 34.30 24.38 UGENT/SCATE-MBL 74.17 30.56 84.32 RTM-DCU/s5-RTM-GLMd 76.00 23.91 88.12 RTM-DCU/s4-RTM-GLMd 75.88 22.69 88.26 Baseline 75.31 16.78 88.93 

 Table 13 : 13 Alternative results for the WMT15 quality estimation Task 2 according to the sequence correlation metric. The win- System ID Correlation English-Spanish ? SAU/KERC-CRF 34.22 ? SAU/KERC-SLG-CRF 34.09 ? UAlacant/OnLine-SBI-Baseline 33.84 UAlacant/OnLine-SBI 32.81 HDCL/QUETCH 32.13 HDCL/QUETCHPLUS 31.38 DCU-SHEFF/BASE-NGRAM-5000 31.23 UGENT/SCATE-HYBRID 30.15 DCU-SHEFF/BASE-NGRAM-2000 29.94 UGENT/SCATE-MBL 28.43 SHEF2/W2V-BI-2000 27.65 SHEF2/W2V-BI-2000-SIM 27.61 SHEF1/QuEst++-AROW 27.36 RTM-DCU/s5-RTM-GLMd 25.92 SHEF1/QuEst++-PA 25.49 RTM-DCU/s4-RTM-GLMd 24.95 Baseline 0.2044 ning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to approximate randomization tests with 95% confidence intervals. Submissions whose results are statistically different from others according to the same test are grouped by a horizontal line. 

 Table 14 : 14 Official results for the ranking variant of the WMT15 quality estimation Task 3. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap resampling (1K times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. ? RTM-DCU/RTM-SVR 7.62 ?0.62 RTM-DCU/RTM-FS-SVR 6.45 ?0.67 USHEF/QUEST-DISC-REP 4.55 0.32 USAAR-USHEF/BFF 3.98 0.27 Baseline SVM 1.60 0.14 HIDDEN 1.04 0.05 German-English ? RTM-DCU/RTM-FS-SVR 4.93 ?0.64 RTM-DCU/RTM-FS+PLS-SVR 4.23 ?0.55 USHEF/QUEST-DISC-BO 1.55 0.19 Baseline SVM 0.59 0.05 USAAR-USHEF/BFF 0.40 0.12 HIDDEN 0.12 ?0.03 System ID MAE ? RMSE ? English-German ? RTM-DCU/RTM-FS-SVR 7.28 11.96 ? RTM-DCU/RTM-SVR 7.5 11.35 USAAR-USHEF/BFF 9.37 13.53 USHEF/QUEST-DISC-REP 9.55 13.46 Baseline SVM 10.05 14.25 German-English ? RTM-DCU/RTM-FS-SVR 4.94 8.74 RTM-DCU/RTM-FS+PLS-SVR 5.78 10.70 USHEF/QUEST-DISC-BO 6.54 10.10 USAAR-USHEF/BFF 6.56 10.12 Baseline SVM 7.35 11.40 

 Table 16 : 16 Alternative results for the scoring variant of the WMT15 quality estimation Task 3. The winning submissions are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to the Williams test with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically significant level according to the same test. ? RTM-DCU/RTM-SVR 0.59 RTM-DCU/RTM-FS-SVR 0.53 USHEF/QUEST-DISC-REP 0.30 USAAR-USHEF/BFF 0.29 Baseline SVM 0.12 German-English ? RTM-DCU/RTM-FS-SVR 0.52 RTM-DCU/RTM-FS+PLS-SVR 0.39 USHEF/QUEST-DISC-BO 0.10 USAAR-USHEF/BFF 0.08 Baseline SVM 0.06 

 Table 17 : 17 Average metric scores for automatic metrics in the corpus for Task 3. EN-DE DE-EN AVG STDEV AVG STDEV Meteor (?) 0.35 0.14 0.26 0.09 

 Table 18 : 18 Data statistics. 

 Informatique pour la M?canique et les Sciences de l'Ing?nieur, France (Wisniewski et al., 2015)  USAAR-SAPE Saarland University, Germany & Jadavpur University, India ID Participating team Abu-MaTran Abu-MaTran Project (Prompsit) FBK Fondazione Bruno Kessler, Italy (Chatterjee et al., 2015a) LIMSI Laboratoire d' 

 Table 20 : 20 Official results for the WMT15 Automatic Post-editing task -average TER (?) case sensitive. Avg. TER ID Avg. TER Baseline 22.913 Baseline 22.221 FBK Primary 23.228 LIMSI Primary 22.544 LIMSI Primary 23.331 FBK Primary 22.551 USAAR-SAPE 23.426 USAAR-SAPE 22.710 LIMSI Contrastive 23.573 Abu-MaTran Primary 22.769 Abu-MaTran Primary 23.639 LIMSI Contrastive 22.861 FBK Contrastive 23.649 FBK Contrastive 22.949 (Simard et al., 2007) 23.839 (Simard et al., 2007) 23.130 Abu-MaTran Contrastive 24.715 Abu-MaTran Contrastive 23.705 

 Table 21 : 21 Official results for the WMT15 Automatic Post-editing task -average TER (?) case insensitive. 

 Table 22 22 (those concerning the 

 Table 22 : 22 WMT APe Task and Autodesk training data statistics. 

 Table 23 : 23 Phrase pair count distribution in two phrase tables built using the APE 2015 training and the Autodesk dataset. 

 Table 24 : 24 Number of test sentences modified, improved and deteriorated by each submitted run, together with the corresponding percentage of insertions, deletions, substitutions and shifts (case sensitive). Modified Improved Deteriorated Edit operations ID Sentences Sentences Sentences Ins Del Sub Shifts FBK Primary 276 64 147 17.8 17.8 55.9 8.5 LIMSI Primary 339 75 217 19.4 16.8 55.2 8.6 USAAR-SAPE 422 53 229 17.6 17.4 56.7 8.4 LIMSI Contrastive 454 61 260 17.4 19.0 55.3 8.3 Abu-MaTran Primary 275 8 200 17.7 17.2 56.8 8.2 FBK Contrastive 422 52 254 18.4 17.0 56.2 8.4 Abu-MaTran Contrastive 602 14 451 17.8 16.4 57.7 8.0 (Simard et al., 2007) 488 55 298 18.3 17.0 56.4 8.3 

 46 ? .52 .46 ? .39 ? .25 ? .21 ? .21 ? .21 ? .21 ? .20 ? .20 ? .19 ? .17 ? .16  ? .17 ? UEDIN-JHU .54 ? -.48 .47 .44 ? .26 ? .21 ? .22 ? .20 ? .21 ? .20 ? .19 ? .19 ? .19 ? .19  ? .19 ? UEDIN-SYNTAX .48 .52 -.51 .46 .28 ? .21 ? .22 ? .22 ? .21 ? .21 ? .19 ? .18 ? .20 ? .19 ? .17 ? MONTREAL .54 ? .53 .49 -.45 ? .28 ? .24 ? .25 ? .24 ? .24 ? .25 ? .24 ? .21 ? .20 ? .20 ? .23 ? ONLINE-A .61 ? .56 ? .54 .55 ? -.29 ? .24 ? .26 ? .25 ? .25 ? .24 ? .23 ? .22 ? .23 ? .23 ? .22 ? CU-TECTO .75 ? .74 ? .72 ? .72 ? .71 ? -.48 .47 .47 .46 ? .48 .44 ? .43 ? .43 ? .43 ? .41 ? TT-BLEU-MIRA-D .79 ? .79 ? .79 ? .76 ? .76 ? .52 -.51 .41 ? .43 .38 ? .43 ? .41 ? .39 ? .39 ? .43 ? TT-ILLC-UVA .79 ? .78 ? .78 ? .75 ? .74 ? .53 .49 -.48 .47 .45 .41 ? .45 .42 ? .40 ? .42 ? TT-BLEU-MERT .79 ? .80 ? .78 ? .76 ? .75 ? .53 .59 ? .52 -.51 .48 .44 ? .45 ? .41 ? .40 ? .41 ? TT-AFRL .79 ? .79 ? .79 ? .76 ? .75 ? .54 ? .57 .53 .49 -.49 .45 .43 ? .42 ? .42 ? .41 ? TT-USAAR-TUNA .80 ? .80 ? .79 ? .75 ? .76 ? .52 .62 ? .55 .52 .51 -.45 .45 ? .41 ? .41 ? .42 ? TT-DCU .80 ? .81 ? .81 ? .76 ? .77 ? .56 ? .57 ? .59 ? .56 ? .55 .55 -.47 .45 ? .44 ? .45 ? TT-METEOR-CMU .81 ? .81 ? .82 ? .79 ? .78 ? .57 ? .59 ? .55 .55 ? .57 ? .55 ? .53 -.48 .49 .48 TT-BLEU-MIRA-SP .83 ? .81 ? .80 ? .80 ? .77 ? .57 ? .61 ? .58 ? .59 ? .58 ? .59 ? .55 ? .52 -.53 .50 TT-HKUST-MEANT .84 ? .81 ? .81 ? .80 ? .77 ? .57 ? .61 ? .60 ? .60 ? .58 ? .59 ? .56 ? .51 .47 -.48 ILLINOIS .82 ? .81 ? .83 ? .77 ? .78 ? .59 ? .57 ? .58 ? .59 ? .59 ? .58 ? .55 ? .52 .50 .52 - score 

  ? .08 ? UEDIN-JHU .57 ? .50 -.51 .44 ? .39 ? .41 ? .35 ? .18 ? .18 ? .18 ? .18 ? .16 ? .13 ? .10 ? MONTREAL .56 ? .50 .49 -.46 ? .43 ? .39 ? .36 ? .22 ? .21 ? .21 ? .21 ? .19 ? .19 ? .16 ? ONLINE-A .62 ? .56 ? .56 ? .54 ? -.43 ? .40 ? .36 ? .20 ? .19 ? .20 ? .18 ? .17 ? .15 ? .12 ? UEDIN-SYNTAX .67 ? .60 ? .61 ? .57 ? .57 ? -.48 .43 ? .25 ? .25 ? .26 ? .25 ? .23 ? .23 ? .17 ? CU-TECTO .71 ? .62 ? .59 ? .61 ? .60 ? .52 -.44 ? .29 ? .30 ? .28 ? .28 ? .28 ? .23 ? .17 ? COMMERCIAL1 .73 ? .68 ? .65 ? .64 ? .64 ? .57 ? .56 ? -.29 ? .28 ? .28 ? .27 ? .27 ? .22 ? .18 ? TT-DCU .85 ? .84 ? .82 ? .78 ? .80 ? .75 ? .71 ? .71 ? -.52 .48 .45 ? .40 ? .36 ? .27 ? TT-AFRL .85 ? .83 ? .82 ? .79 ? .81 ? .75 ? .70 ? .72 ? .48 -.49 .46 .37 ? .33 ? .29 ? TT-BLEU-MIRA-D .85 ? .83 ? .82 ? .79 ? .80 ? .74 ? .72 ? .72 ? .52 .51 -.39 ? .36 ? .36 ? .27 ? TT-USAAR-TUNA .86 ? .83 ? .82 ? .79 ? .82 ? .75 ? .72 ? .73 ? .55 ? .54 .61 ? -.36 ? .37 ? .28 ? TT-BLEU-MERT .86 ? .84 ? .84 ? .81 ? .83 ? .77 ? .72 ? .73 ? .60 ? .63 ? .64 ? .64 ? -.39 ? .28 ? TT-METEOR-CMU .89 ? .87 ? .87 ? .81 ? .85 ? .77 ? .77 ? .78 ? .64 ? .67 ? .64 ? .63 ? .61 ? -.32 ? TT-BLEU-MIRA-SP .90 ? .92 ? .90 ? .84 ? .88 ? .83 ? .83 ? .82 ? .73 ? .71 ? .73 ? .72 ? .72 ? .68 ? - CU-CHIMERA ONLINE-B UEDIN-JHU MONTREAL ONLINE-A UEDIN-SYNTAX CU-TECTO COMMERCIAL1 TT-DCU TT-AFRL TT-BLEU-MIRA-D TT-USAAR-TUNA TT-BLEU-MERT TT-METEOR-CMU TT-BLEU-MIRA-SP CU- 

 Table 26 : 26 Head to head comparison, ignoring ties, for English-Czech systems ONLINE-B -.41 ? .43 ? .39 ? .39 ? .33 ? .38 ? .25 ? .26 ? .27 ? .26 ? .19 ? .22 ? UEDIN-JHU .59 ? -.51 .46 .45 ? .43 ? .44 ? .31 ? .33 ? .36 ? .30 ? .28 ? .27 ? ONLINE-A .57 ? .49 -.52 .53 .48 .44 ? .36 ? .32 ? .31 ? .28 ? .29 ? .26 ? UEDIN-SYNTAX .61 ? .54 .48 -.49 .48 .45 ? .23 ? .33 ? .34 ? .35 ? .27 ? .26 ? KIT .61 ? .55 ? .47 .51 -.47 .46 .35 ? .38 ? .36 ? .35 ? .26 ? .32 ? RWTH .67 ? .57 ? .52 .52 .53 -.46 .38 ? .39 ? .40 ? .36 ? .31 ? .35 ? MONTREAL .62 ? .56 ? .56 ? .55 ? .54 .54 -.42 ? .43 ? .41 ? .35 ? .32 ? .34 ? ILLINOIS .75 ? .69 ? .64 ? .77 ? .65 ? .62 ? .58 ? -.48 .49 .48 .38 ? .42 ? DFKI .74 ? .67 ? .68 ? .67 ? .62 ? .61 ? .57 ? .52 -.43 ? .46 .39 ? .37 ? ONLINE-C .73 ? .64 ? .69 ? .66 ? .64 ? .60 ? .59 ? .51 .57 ? -.46 .42 ? .39 ? ONLINE-F .74 ? .70 ? .72 ? .65 ? .65 ? .64 ? .64 ? .52 .54 .54 -.44 ? .40 ? MACAU .81 ? .72 ? .71 ? .73 ? .74 ? .69 ? .68 ? .62 ? .61 ? .58 ? .56 ? -.50 ONLINE-E .78 ? ONLINE-B UEDIN-JHU ONLINE-A UEDIN-SYNTAX KIT RWTH MONTREAL ILLINOIS DFKI ONLINE-C ONLINE-F MACAU ONLINE-E 

 Table 28 : 28 Head to head comparison, ignoring ties, for English-German systems .53 ? .54 ? .53 ? -.48 .39 ? .28 ? ONLINE-A .56 ? .55 ? .54 ? .52 -.38 ? .26 ? ONLINE-F .65 ? .63 ? .65 ? .61 ? .62 ? -.37 ? ONLINE-E .78 ? .75 ? .74 ? .72 ? .74 ? .63 ? - ONLINE-B LIMSI-CNRS UEDIN-JHU MACAU ONLINE-A ONLINE-F ONLINE-E ONLINE-B -.50 .49 .47 ? .44 ? .35 ? .22 ? LIMSI-CNRS .50 -.49 .46 ? .45 ? .37 ? .25 ? UEDIN-JHU .51 .51 -.47 ? .46 ? .35 ? .26 ? score .49 .44 .41 .27 .22 -.42 -1.43 rank 1-2 1-3 1-3 4-5 4-5 6 7 MACAU 

 Table 29 : 29 Head to head comparison, ignoring ties, for French-English systems .62 ? .55 ? .56 ? .54 ? -.45 ? .36 ? ONLINE-F .64 ? .63 ? .59 ? .60 ? .55 ? -.41 ? ONLINE-E .72 ? .68 ? .69 ? .69 ? .64 ? .59 ?score .54 .30 .25 .21 -.00 -.33 -. LIMSI-CNRS ONLINE-A UEDIN-JHU ONLINE-B CIMS ONLINE-F ONLINE-E LIMSI-CNRS -.45 ? .44 ? .45 ? .38 ? .36 ? .28 ? ONLINE-A .55 ? -.49 .48 .45 ? .37 ? .32 ? UEDIN-JHU .56 ? .51 -.48 .44 ? .41 ? .31 ? ONLINE-B .55 ? .52 .52 -.46 ? .40 ? .31 ? 97 rank 1 2-3 2-4 3-4 5 6 7 CIMS 

 Table 30 : 30 Head to head comparison, ignoring ties, for English-French systems 

			 The metrics and tuning tasks are reported in separate papers(Stanojevi? et al., 2015a,b). 

			 http://statmt.org/wmt15/results.html 

			 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 

			 https://github.com/lspecia/quest 

			 http://scikit-learn.org/ 9 https://github.com/qe-team/marmot 

			 http://www.statmt.org/moses/?n=Moses. Baseline 11 http://taku910.github.io/crfpp/ 

			 https://github.com/ghpaetzold/ questplusplus 

			 http://www.apertium.org 14 http://context.reverso.net/translation/ 

			 http://www.statmt.org/wmt15/ape-task.html 16 https://unbabel.com/ 17 http://www.cs.umd.edu/ ?snover/tercom/ 

			 http://www.quest.dcs.shef.ac.uk/wmt15_ files/bootstrap-significance.pl 

			 http://www.cs.umd.edu/ ?snover/tercom/ 

			 http://www.nlpado.de/ ?sebastian/software/ sigf.shtml 

			 The original triplets were provided by Unbabel (https: //unbabel.com/). 

			 The corpus (https://autodesk.app.box.com/ Autodesk-PostEditing) consists of parallel English source-MT/TM target segments post-edited into several languages (Chinese, Czech, French, German, Hungarian, Italian, Japanese, Korean, Polish, Brazilian Portuguese, Russian, Spanish) with between 30K to 410K segments per language.
