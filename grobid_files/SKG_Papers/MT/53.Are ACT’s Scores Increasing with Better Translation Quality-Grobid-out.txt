title
Are ACT's scores increasing with better translation quality?

abstract
This paper gives a detailed description of the ACT (Accuracy of Connective Translation) metric, a reference-based metric that assesses only connective translations. ACT relies on automatic word-level alignment (using GIZA++) between a source sentence and respectively the reference and candidate translations, along with other heuristics for comparing translations of discourse connectives. Using a dictionary of equivalents, the translations are scored automatically or, for more accuracy, semi-automatically. The accuracy of the ACT metric was assessed by human judges on sample data for English/French, English/Arabic, English/Italian and English/German translations; the ACT scores are within 2-5% of human scores. The actual version of ACT is available only for a limited language pairs. Consequently, we are participating only for the English/French and English/German language pairs. Our hypothesis is that ACT metric scores increase with better translation quality in terms of human evaluation.

Introduction Discourse connectives should preserve their sense during translation, as they are often ambiguous and may convey more than one sense depending on the inter-sentential relation (causality, concession, contrast or temporal). For instance, since in English can express temporal simultaneity, but also a causal sense. In this paper, we present results of different Machine Translation systems for English-to-French and English-to-German pairs. More specifically, we measure the quality of machine translations of eight English discourse connectives: although, even though, meanwhile, since, though, while, however, and yet, adopting different approaches. This quality is measured using a dedicated metric named ACT (Accuracy of Connective Translation), a reference-based metric that assesses only connective translations. The paper is organized as follows. In Section 2, we present the ACT metric and its error rate. In section 3, we compare the ACT metric to previous machine translation evaluation metrics. Finally, we present the results of the different English-to-German and English-to-French MT systems (Section 4). 

 ACT Metric We described the ACT metric in  (Hajlaoui and Popescu-Belis, 2013)  and  (Hajlaoui and Popescu-Belis, 2012) . Its main idea is to detect, for a given explicit source discourse connective, its translation in a reference translation and in a candidate translation. ACT then compares and scores these translations. To identify the translations, ACT first uses a dictionary of possible translations of each discourse connective type, collected from training data and validated by humans. If a reference or a candidate translation contains more than one possible translation of the source connective, alignment information is used to detect the correct connective translation. If the alignment information is irrelevant (not equal to a connective), it then compares the word position (word index) of the source connective alignment with the index in the translated sentence (candidate or reference) and the set of candidate connectives to disambiguate the connective's translation. Finally, the nearest connective to the alignment is taken. ACT proceeds by checking whether the reference translation contains one of the possible translations of the connective in question. After that, it similarly checks if the candidate translation contains a possible translation of the connective. Fi-nally, it checks if the reference connective found is equal (case 1), synonymous (case 2) or incompatible 1 (case 3) to the candidate connective. Discourse relations can be implicit in the candidate (case 4), or in the reference (case 5) translation or in both of them (case 6). These different comparisons can be represented by the following 6 cases: ? Case 1: same connective in the reference (Ref) and candidate translation (Cand). ? Case 2: synonymous connective in Ref and Cand. ? Case 3: incompatible connective in Ref and Cand. ? Case 4: source connective translated in Ref but not in Cand. ? Case 5: source connective translated in Cand but not in Ref. ? Case 6: the source connective neither translated in Ref nor in Cand. Based on the connective dictionary categorised by senses, ACT gives one point for identical (case 1) and equivalent translations (case 2), otherwise zero. ACT proposes a semi-automatic option by manually checking instances of case 5 and case 6 2 . ACT returns the ratio of the total number of points to the number of source connectives according to the three versions: (1) ACTa counts only case 1 and case 2 as correct and all others cases as wrong, (2) ACTa5+6 excludes case 5 and case 6 and (3) ACTm considers the correct translations found by manual scoring of case 5 and case 6 noted respectively case5corr and case6corr to better consider these implicit cases. ACT a = (| case1 | + | case2 |)/ 6 i=1 | casei | (1) ACT a5 + 6 = (| case1 | + | case2 |)/ 4 i=1 | casei | (2) ACT m = ACT a + (| case5corr | + | case6corr | / 6 i=1 | casei |) (3) 1 In terms of connective sense.  2  We do not check manually case 4 because we observed that its instances propose generally explicit translations that do not belong to our dictionary, it means the SMT system tends to learn explicit translations for explicit source connective. 

 Configurations of ACT metric As shown in Figure  1 , ACT can be configured to use an optional disambiguation module. Two versions of this disambiguation module can be used: (1) without training, which means without saving an alignment model and only using GIZA++ as alignment tool; (2) with training and saving an alignment model using MGIZA++ (a multithreaded version of GIZA++) trained on an external corpus to align the (Source, Reference) and the (Source, Candidate) data. Figure  1 : ACT architecture ACT is more accurate using the disambiguation module. We encourage to use the version without training since it only requires the installation of the GIZA++ tool. Based on its heuristics and on its connective dictionaries categorised by senses, ACT has a higher precision to detect the right connective when more than one translation is possible. The following example illustrates the usefulness of the disambiguation module when we have more than one possible translation of the source connective. Without disambiguation, ACT detects the same connective si in both target sentences (wrong case 1), while the right translation of the source connective although is bien que and m?me si respectively in the reference and the candidate sentence (case 2). Without disambiguation, case 1: Csrc= although, Cref = si, Ccand = si With disambiguation, case 2: Csrc= although (concession), Cref = bien que, Ccand = m?me si ? SOURCE: we did not have it so bad in ireland this time although we have had many serious wind storms on the atlantic . ? REFERENCE: cette fois-ci en irlande . ce n' ?tait pas si grave . bien que de nombreuses temp?tes violentes aient s?vi dans l' atlantique . ? CANDIDATE: nous n' ?tait pas si mauvaise en irlande . cette fois . m?me si nous avons eu vent de nombreuses graves temp?tes sur les deux rives de l' atlantique . In the following experiments, we used the recommended configuration of ACT (without training). 

 Error rate of the ACT metric ACT is a free open-source Perl script licensed under GPL v3 3 . It has a reasonable and acceptable error score when comparing its results to human judgements  (Hajlaoui and Popescu-Belis, 2013) . Its accuracy was assessed by human judges on sample data for English-to-French, English-to-Arabic, English-to-Italian and English-to-German translations; the ACT scores are within 2-5% of human scores. 

 Multilingual architecture of ACT Metric The ACT architecture is multilingual: it was initially developed for the English-French language pair, then ported to English-Arabic, English-Italian and English-German. The main resource needed to port the ACT metric to another language pair is the dictionary of connectives matching possible synonyms and classifying connectives by sense. To find these possible translations of a given connective, we proposed an automatic method based on a large corpus analysis  (Hajlaoui and Popescu-Belis, 2012) . This method can be used for any language pair. Estimating the effort that would have to be taken to port the ACT metric to new language pairs focusing on the same linguistic phenomena mainly depends on the size of parallel data sets containing the given source connective. The classification by sense depends also on the number of possible translations detected for a given source connective. This task is sometimes difficult, as some translations (target connectives) can be as ambiguous as the source connective. Native linguistic knowledge of the target language is therefore needed in order to complete a dictionary with the main meanings and senses of the connectives. We think that the same process and the same effort can be taken to adapt ACT to new linguistic phenomena  (verbs, pronouns, adverbs, etc) . 

 Related works ACT is different from existing MT metrics. The METEOR metric  (Denkowski and Lavie, 2011)  uses monolingual alignment between two translations to be compared: a system translation and a reference one. METEOR performs a mapping between unigrams: every unigram in each translation maps to zero or one unigram in the other translation. Unlike METEOR, the ACT metric uses a bilingual alignment (between the source and the reference sentences and between the source and the candidate sentences) and the word position information as additional information to disambiguate the connective situation in case there is more than one connective in the target (reference or candidate) sentence. ACT may work without this disambiguation. The evaluation metric described in  (Max et al., 2010)  indicates for each individual source word which systems (among two or more systems or system versions) correctly translated it according to some reference translation(s). This allows carrying out detailed contrastive analyses at the word level, or at the level of any word class (e.g. part of speech, homonymous words, highly ambiguous words relative to the training corpus, etc.). The ACT metric relies on the independent comparison of one system's hypothesis with a reference. An automatic diagnostics of machine translation and based on linguistic checkpoints  (Zhou et al., 2008) ,  (Naskar et al., 2011)  constitute a different approach from our ACT metric. The approach essentially uses the BLEU score to separately evaluate translations of a set of predefined linguistic checkpoints such as specific parts of speech, types of phrases (e.g., noun phrases) or phrases with a certain function word. A different approach was proposed by  (Popovic and Ney, 2011)  to study the distribution of errors over five categories (inflectional errors, reordering errors, missing words, extra words, incorrect lexical choices) and to examine the number of errors in each category. This proposal was based on the calculation of Word Error Rate (WER) and Positionindependent word Error Rate (PER), combined with different types of linguistic knowledge (base forms, part-of-speech tags, name entity tags, com-pound words, suffixes, prefixes). This approach does not allow checking synonym words having the same meaning like the case of discourse connectives. 

 ACT-based comparative evaluation We used the ACT metric to assess connective translations for 21 English-German systems and 23 English-French systems. It was computed on tokenized and lower-cased text using its second configuration "without training"  (Hajlaoui and Popescu-Belis, 2013) . Table  1  shows only ACTa scores for the English-to-German translation systems since ACTa5+6 gives the same rank as ACTa. Table  2  present the same for the English-to-French systems. We are not presenting ACTm either because we didn't check manually case 5 and case 6.  

 Conclusion The connective translation accuracy of the candidate systems cannot be measured correctly by current MT metrics such as BLEU and NIST. We therefore developed a new distance-based metric, ACT, to measure the improvement in connective translation. ACT is a reference-based metric that only compares the translations of discourse connectives. It is intended to capture the improvement of an MT system that can deal specifically with discourse connectives. ACT can be also used semi-automatically. Consequently, the scores reflect more accurately the improvement in translation quality in terms of discourse connectives. Theoretically, a better system should preserve the sense of discourse connectives. Our hypothesis is thus that ACT scores are increasing with better translation quality. We need access the human rankings of this task to validate if ACT's scores indeed correlate with overall translation quality rankings. Table 1 : 1 Metric scores for all En-De systems: ACTa and ACTa5+6 scores give the same rank; ACT V1.7. SD is the Standard Deviation. Metric System ACTa cu-zeman.2724 rbmt-3 TUBITAK.2633 KITprimary.2663 StfdNLPG.2764 JHU.2888 LIMSI-N-S-p.2589 0.720 Value Avg 0.772 0.772 0.746 0.737 0.733 0.728 online-G 0.720 Shef-wproa.2748 0.720 RWTHJane.2676 0.711 0.697 0.056 SD uedin-wmt13.2638 0.707 UppslaUnv.2698 0.707 online-A 0.698 rbmt-1 0.694 online-B 0.677 uedin-syntax.2611 0.672 online-C 0.664 FDA.2842 0.664 MES-reorder.2845 0.664 PROMT.2789 0.621 rbmt-4 0.513 
