title
Maximum Expected BLEU Training of Phrase and Lexicon Translation Models

abstract
This paper proposes a new discriminative training method in constructing phrase and lexicon translation models. In order to reliably learn a myriad of parameters in these models, we propose an expected BLEU score-based utility function with KL regularization as the objective, and train the models on a large parallel dataset. For training, we derive growth transformations for phrase and lexicon translation probabilities to iteratively improve the objective. The proposed method, evaluated on the Europarl German-to-English dataset, leads to a 1.1 BLEU point improvement over a state-of-the-art baseline translation system. In IWSLT 2011 Benchmark, our system using the proposed method achieves the best Chinese-to-English translation result on the task of translating TED talks.

Introduction Discriminative training is an active area in statistical machine translation (SMT) (e.g.,  Och et al., 2002 , 2003 , Liang et al., 2006 , Blunsom et al., 2008 , Chiang et al., 2009 , Foster et al, 2010 , Xiao et al. 2011 .  Och (2003)  proposed using a loglinear model to incorporate multiple features for translation, and proposed a minimum error rate training (MERT) method to train the feature weights to optimize a desirable translation metric. While the log-linear model itself is discriminative, the phrase and lexicon translation features, which are among the most important components of SMT, are derived from either generative models or heuristics  (Koehn et al., 2003 , Brown et al., 1993 . Moreover, the parameters in the phrase and lexicon translation models are estimated by relative frequency or maximizing joint likelihood, which may not correspond closely to the translation measure, e.g., bilingual evaluation understudy (BLEU)  (Papineni et al., 2002) . Therefore, it is desirable to train all these parameters to directly maximize an objective that directly links to translation quality. However, there are a large number of parameters in these models, making discriminative training for them non-trivial (e.g.,  Liang et al., 2006 , Chiang et al., 2009 .  Liang et al. (2006)  proposed a large set of lexical and Part-of-Speech features and trained the model weights associated with these features using perceptron. Since many of the reference translations are non-reachable, an empirical local updating strategy had to be devised to fix this problem by picking a pseudo reference. Many such non-desirable heuristics led to moderate gains reported in that work.  Chiang et al. (2009)  improved a syntactic SMT system by adding as many as ten thousand syntactic features, and used Margin Infused Relaxed Algorithm (MIRA) to train the feature weights. However, the number of parameters in common phrase and lexicon translation models is much larger. In this work, we present a new, highly effective discriminative learning method for phrase and lexicon translation models. The training objective is an expected BLEU score, which is closely linked to translation quality. Further, we apply a Kullback-Leibler (KL) divergence regularization to prevent over-fitting. For effective optimization, we derive updating formulas of growth transformation (GT) for phrase and lexicon translation probabilities. A GT is a transformation of the probabilities that guarantees strict non-decrease of the objective over each GT iteration unless a local maximum is reached. A similar GT technique has been successfully used in speech recognition  (Gopalakrishnan et al., 1991 , Povey, 2004 , He et al., 2008 . Our work demonstrates that it works with large scale discriminative training of SMT model as well. Our work is based on a phrase-based SMT system. Experiments on the Europarl German-to-English dataset show that the proposed method leads to a 1.1 BLEU point improvement over a strong baseline. The proposed method is also successfully evaluated on the IWSLT 2011 benchmark test set, where the task is to translate TED talks (www.ted.com). Our experimental results on this open-domain spoken language translation task show that the proposed method leads to significant translation performance improvement over a state-of-the-art baseline, and the system using the proposed method achieved the best single system translation result in the Chineseto-English MT track. 

 Related Work One best known approach in discriminative training for SMT is proposed by  Och (2003) . In that work, multiple features, most of them are derived from generative models, are incorporated into a log-linear model, and the relative weights of them are tuned discriminatively on a small tuning set. However, in practice, this approach only works with a handful of parameters. More closely related to our work,  Liang et al. (2006)  proposed a large set of lexical and Part-of-Speech features in addition to the phrase translation model. Weights of these features are trained using perceptron on a training set of 67K sentences. In that paper, the authors pointed out that forcing the model to update towards the reference translation could be problematic. This is because the hidden structure such as phrase segmentation and alignment could be abused if the system is forced to produce a reference translation. Therefore, instead of pushing the parameter update towards the reference translation (a.k.a. bold updating), the author proposed a local updating strategy where the model parameters are updated towards a pseudo-reference (i.e., the hypothesis in the n-best list that gives the best BLEU score). Experimental results showed that their approach outperformed a baseline by 0.8 BLEU point when using monotonic decoding, but there was no significant gain over a stronger baseline with a full-distortion model. In our work, we use the expectation of BLEU scores as the objective. This avoids the heuristics of picking the updating reference and therefore gives a more principal way of setting the training objective. As another closely related study,  Chiang et al. (2009)     , Tromble et al., 2008  and lattice-based MERT . In these earlier work, however, the phrase and lexicon translation models used remained unchanged. Another line of research that is closely related to our work is phrase table refinement and pruning.  Wuebker et al. (2010)  proposed a method to train the phrase translation model using Expectation-Maximization algorithm with a leave-one-out strategy. The parallel sentences were forced to be aligned at the phrase level using the phrase table and other features as in a decoding process. Then the phrase translation probabilities were estimated based on the phrase alignments. To prevent overfitting, the statistics of phrase pairs from a particular sentence was excluded from the phrase table when aligning that sentence. However, as pointed out by  Liang et al (2006) , the same problem as in the bold updating existed, i.e., forced alignment between a source sentence and its reference translation was tricky, and the proposed alignment was likely to be unreliable. The method presented in this paper is free from this problem. 

 Phrase-based Translation System The translation process of phrase-based SMT can be briefly described in three steps: segment source sentence into a sequence of phrases, translate each source phrase to a target phrase, re-order target phrases into target sentence  (Koehn et al., 2003) . In decoding, the optimal translation ? given the source sentence F is obtained according to ? = argmax ! ? ? ? (1) where ? ? ? = 1 ? ? ? ! log ? ! (?, ?) ! (2) and ? = ? ? ! log ? ! (?, ?) ! ! is the normalization denominator to ensure that the probabilities sum to one. Note that we define the feature functions {? ! (?, ?)} in log domain to simplify the notation in later sections. Feature weights ? = {? ! } are usually tuned by MERT. Features used in a phrase-based system usually include LM, reordering model, word and phrase counts, and phrase and lexicon translation models. Given the focus of this paper, we review only the phrase and lexicon translation models below. 

 Phrase translation model A set of phrase pairs are extracted from wordaligned parallel corpus according to phrase extraction rules  (Koehn et al., 2003) . Phrase translation probabilities are then computed as relative frequencies of phrases over the training dataset. i.e., the probability of translating a source phrase ? to a target phrase ? is computed by ? ? ? = ?(?, ?) ?(?) (3) where ?(?, ?) is the joint counts of ? and ?, and ?(?) is the marginal counts of ?. In translation, the input sentence is segmented into K phrases, and the source-to-target forward phrase (FP) translation feature is scored as: ? !" ?, ? = ? ? ! ? ! ! (4) where ? ! and ? ! are the k-th phrase in E and F, respectively. The target-to-source (backward) phrase translation model is defined similarly. 

 Lexicon translation model There are several variations in lexicon translation features  (Ayan and Dorr 2006 , Koehn et al., 2003 , Quirk et al., 2005 . We use the word translation table from IBM Model 1  (Brown et al., 1993)  and compute the sum over all possible word alignments within a phrase pair without normalizing for length  (Quirk et al., 2005) . The source-to-target forward lexicon (FL) translation feature is: ? !" ?, ? = ? ? !,! ? !,! ! ! ! (5) where ? !,! is the m-th word of the k-th target phrase ? ! , ? !,! is the r-th word in the k-th source phrase ? ! , and ?(? !,! |? !,! ) is the probability of translating word ? !,! to word ? !,! . In IBM model 1, these probabilities are learned via maximizing a joint likelihood between the source and target sentences. The target-to-source (backward) lexicon translation model is defined similarly. 

 Maximum Expected-BLEU Training 

 Objective function We denote by ? the set of all the parameters to be optimized, including forward phrase and lexicon translation probabilities and their backward counterparts. For simplification of notation, ? is formed as a matrix, where its elements {? !" } are probabilities subject to ? !" ! = 1. E.g., each row is a probability distribution. The utility function over the entire training set is defined as: ?(?) = ? ? (? ! , ? , ? ! |? ! , ? , ? ! ) ?(? ! , ? ! * ) ! !!! ! ! ,?,! ! (6) where N is the number of sentences in the training set, ? ! * is the reference translation of the n-th source sentence ? ! , and ? ! ? ?(? ! ) that denotes the list of translation hypotheses of ? ! . Since the sentences are independent with each other, the joint posterior can be decomposed: ? ? ? ! , ? , ? ! ? ! , ? , ? ! = ? ? ? ! ? ! ! !!! (7) and ? ? ? ! ? ! is the posterior defined in (2), the subscript ? indicates that it is computed based on the parameter set ?. ? ? is proportional (with a factor of N) to the expected sentence BLEU score over the entire training set, i.e., after some algebra, ?(?) = ? ? (? ! |? ! )?(? ! , ? ! * ) ! ! ! !!! In a phrase-based SMT system, the total number of parameters of phrase and lexicon translation models, which we aim to learn discriminatively, is very large (see Table  1 ). Therefore, regularization is critical to prevent over-fitting. In this work, we regularize the parameters with KL regularization. KL divergence is commonly used to measure the distance between two probability distributions. For the whole parameter set ? , the KL regularization is defined in this work as the sum of KL divergence over the entire parameter space: ?(? ! ||?) = ? !" ! log ? !" ! ? !" ! ! (8) where ? ! is a constant prior parameter set. In training, we want to improve the utility function while keeping the changes of the parameters from ? ! at minimum. Therefore, we design the objective function to be maximized as: ? ? = log? ? ? ? ? ?(? ! ||?) (9) where the prior model ? ! in our approach is the relative-frequency-based phrase translation model and the maximum-likelihood-estimated IBM model 1 (word translation model). ? is a hyperparameter controlling the degree of regularization. 

 Optimization In this section, we derived GT formulas for iteratively updating the parameters so as to optimize objective (9). GT is based on extended Baum-Welch (EBW) algorithm first proposed by  Gopalakrishnan et al. (1991)  and commonly used in speech recognition (e.g.,  He et al. 2008) . 

 Extended Baum-Welch Algorithm Baum-Eagon inequality  (Baum and Eagon, 1967)  gives the GT formula to iteratively maximize positive-coefficient polynomials of random variables that are subject to sum-to-one constants. Baum-Welch algorithm is a model update algorithm for hidden Markov model which uses this GT.  Gopalakrishnan et al. (1991)  extended the algorithm to handle rational function, i.e., a ratio of two polynomials, which is more commonly encountered in discriminative training. Here we briefly review EBW. Assuming a set of random variables ? = {? !" } that subject to the constraint that ii) Derive GT formula for ? ? ? !" ! = 1 ? !" = ? !" ! ?(?) ? !" ?!?! + ? ? ? !" ! ? !" ! ?(?) ? !" ?!?! ! + ? ( 11 ) where D is a smoothing factor. 

 GT of Translation Models Now we derive the GTs of translation models for our objective. Since maximizing ? ? is equivalent to maximizing ? ! ? , we have the following auxiliary function: ? ? = ?(?)? !!?!"(? ! ||?) (12) After substituting (2) and (  7 ) into (6), and drop optimization irrelevant terms in KL regularization, we have ? ? in a rational function form: ? ? = ? ? ? ? ? ?(?) (13) where ? ? = ? ! ! ! ? ! , ? ! ! ! !!! ! ! ,?,! ! , ? ? = ? !" !! !" ! ! ! , and ? ? = ? ! ! ! ? ! , ? ! ! !!! ? ? ! , ? ! * ! !!! ! ! ,?,! ! are all positive polynomials of ?. Therefore, we can follow the two steps of EBW to derive the GT formulas for ?. If we denote by ? !" the probability of translating the source phrase i to the target phrase Then, the updating formula is (derivation omitted): ? !" = ? !" (? ! , ?, ?, ?) ! ! ! + ? ? ? !" ? !" ! + ? ! ? !" ! ? !" (? ! , ?, ?, ?) ! ! ! ! + ? ? ? !" + ? ! ( 14 ) where ? !" = ?/? !" and ? !" ? ! , ?, ?, ? = ? ?! ? ! ? ! ? ? ? ! , ? ! * ? ? ! ? ? ?(? !,! = ?, ? !,! = ?) ! . In which ? ! ? takes a form similar to (6), but is the expected BLEU score for sentence n using models from the previous iteration. ? !,! and ? !,! are the kth phrases of ? ! and ? ! , respectively. The smoothing factor set of ? ! according to the Baum-Eagon inequality is usually far too large for practical use. In practice, one general guide of setting ? ! is to make all updated value positive. Similar to  (Povey 2004) , we set ? ! by ? ! = max (0, ? !" (? ! , ?, ?, ?) ! ! ! ! ) (15) to ensure the denominator of (  15 ) is positive. Further, we set a low-bound of ? ! as max ! { ! ! !" ! ! ,!,!,! ! ! ! ! !" ! } to guarantee the numerator to be positive. We denote by ? !" the probability of translating the source word i to the target word j. Then following the same derivation, we get the updating formula for forward lexicon translation model: ? !" = ? !" (? ! , ?, ?, ?) ! ! ! + ? ? ? !" ? !" ! + ? ! ? !" ! ? !" (? ! , ? ! , ?, ?) ! ! ! ! + ? ? ? !" + ? ! ( 16 ) where ? !" = ?/? !" and ? !" ? ! , ?, ?, ? = ? ?! ? ! ? ! ? ? ? ! , ? ! * ? ? ! ? ? ?(? !,!,! = ?) ! ! ? ?, ?, ?, ? , and ? ?, ?, ?, ? = ?(! !,!,! !!)!!(! !,!,! |! !,!,! ) ! !!(! !,!,! |! !,!,! ) ! , in which ? !,!,! and ? !,!,! are the r-th and m-th word in the k-th phrase of the source sentence ? ! and the target hypothesis ? ! , respectively. Value of ? ! is set in a way similar to (15). GTs for updating backward phrase and lexicon translation models can be derived in a similar way, and is omitted here. 

 Implementation issues 

 Normalizing ? The posterior ? ?! ? ! ? ! in the model updating formula is computed according to (2). In decoding, only the relative values of ? matters. However, the absolute value will affect the posterior distribution, e.g., an overly large absolute value of ? would lead to a very sharp posterior distribution. In order to control the sharpness of the posterior distribution, we normalize ? by its L1 norm: ? ! = ? ! |? ! | ! (17) 

 Computing the sentence BLEU sore The commonly used BLEU-4 score is computed by ?--4 = BP ? exp 1 4 log? ! ! !!! (18) In the updating formula, we need to compute the sentence-level ? ? ! , ? ! * . Since the matching count may be sparse at the sentence level, we smooth raw precisions of high-order n-grams by: ? ! = #(?--? ?) + ? ? ? ! ! #(?--?) + ? (19) where ? ! ! is the prior value of ? ! , ? is a smoothing factor usually takes a value of 5 and ? ! ! can be set by ? ! ! = ? !!! ? ? !!! ? !!! , for n = 3, 4 . ? ! and ? ! are estimated empirically. Brevity penalty (BP) also plays a key role. Instead of clip it at 1, we use a non-clipped BP, ? = ? (!! ! ! ) , for sentence-level BLEU 1 . We further scale the reference length, r, by a factor such that the total length of references on the training set equals that of the baseline output 2 . 

 Training procedure The parameter set ? is optimized on the training set while the feature ? are tuned on a small tuning set 3 . Since ? and ? affect the training of each other, we train them in alternation. I.e., at each iteration, we first fix ? and update ?, then we re-tune ? given the new ?. Due to mismatch between training and tuning data, the training process might not always Therefore, we need a validation set to determine the stop point of training. At the end, ? and ? that give the best score on the validation set are selected and applied to the test set. Fig.  1  gives a summary of the training procedure. Note that step 2 and 4 are parallelize-able across multiple processors.  

 Evaluation In evaluating the proposed method, we use two separate datasets. We first describe the experiments with the Europarl dataset  (Koehn 2002) , followed by the experiments with IWSLT-2011 task  (Federico et al., 2011) . 

 Experimental setup in the Europarl task In evaluating the proposed method, we use two separate datasets. First, we conduct experiments on the Europarl German-to-English dataset. The training corpus contains 751K sentence pairs, 21 words per sentence on average. 2000 sentences are provided in the development set. We use the first 1000 sentences for ? tuning, and the rest for validation. The test set consists of 2000 sentences. 3 Usually, the tuning set matches the test condition better, and therefore is preferable for ? tuning. To build the baseline phrase-based SMT system, we first perform word alignment on the training set using a hidden Markov model with lexicalized distortion  (He 2007) , then extract the phrase table from the word aligned bilingual texts  (Koehn et al., 2003) . The maximum phrase length is set to four. Other models used in the baseline system include lexicalized ordering model, word count and phrase count, and a 3-gram LM trained on the English side of the parallel training corpus. Feature weights are tuned by MERT. A fast beam-search phrasebased decoder  (Moore and Quirk 2007 ) is used and the distortion limit is set to four. Details of the phrase and lexicon translation models are given in Table  1 . This baseline achieves a BLEU score of 26.22% on the test set. This baseline system is also used to generate a 100-best list of the training corpus during maximum expected BLEU training. Translation model # parameters Phrase models (fore. & back.) 9.2 M Lexicon model (IBM-1 src-to-tgt) 12.9 M Lexicon model (IBM-1 tgt-to-src) 11.9 M Table  1 . Summary of phrase and lexicon translation models 

 Experimental results on the Europarl task During training, we first tune the regularization factor ? based on the performance on the validation set. For simplicity reasons, the tuning of ? makes use of only the phrase translation models. Table  2  reports the BLEU scores and gains over the baseline given different values of ?. The results highlight the importance of regularization. While ? = 5?10 !! gives the best score on the validation set, the gain is shown to be substantially reduced to merely 0.2 BLEU point when ? = 0, i.e., no regularization. We set the optimal value of ? = 5?10 !! in all remaining experiments. Fixing the optimal regularization factor ?, we then study the relationship between the expected 1. Build the baseline system, estimate { ?, ? }. 2. Decode N-best list for training corpus using the baseline system, compute ?(? ! , ? ! * ).      3 . Results on the Europarl German-to-English dataset. The BLEU measures from various settings of maximum expected BLEU training are compared with the baseline, where * denotes that the gain over the baseline is statistically significant with a significance level > 99%, measured by paired bootstrap resampling method proposed by  Koehn (2004) .   

 Experiments on the IWSLT2011 benchmark As the second evaluation task, we apply our new method described in this paper to the 2011 Chinese-to-English machine translation benchmark  (Federico et al., 2011) . The main focus of the IWSLT2011 Evaluation is the translation of TED talks (www.ted.com). These talks are originally given in English. In the Chinese-to-English translation task, we are provided with human translated Chinese text with punctuations inserted. The goal is to match the human transcribed English speech with punctuations. This is an open-domain spoken language translation task. The training data consist of 110K sentences in the transcripts of the TED talks and their translations, in English and Chinese, respectively. Each sentence consists of 20 words on average. Two development sets are provided, namely, dev2010 and tst2010. They consist of 934 sentences and 1664 sentences, respectively. We use dev2010 for ? tuning and tst2010 for validation. The test set tst2011 consists of 1450 sentences. In our system, a primary phrase table is trained from the 110K TED parallel training data, and a 3gram LM is trained on the English side of the parallel data. We are also provided additional outof-domain data for potential usage. From them, we train a secondary 5-gram LM on 115M sentences of supplementary English data, and a secondary phrase table from 500K sentences selected from the supplementary UN corpus by the method proposed by  Axelrod et al. (2011) . In carrying out the maximum expected BLEU training, we use 100-best list and tune the regularization factor to the optimal value of ? = 1?10 !! . We only train the parameters of the primary phrase table. The secondary phrase table and LM are excluded from the training process since the out-of-domain phrase table is less relevant to the TED translation task, and the large LM slows down the N-best generation process significantly. At the end, we perform one final MERT to tune the relative weights with all features including the secondary phrase table and LM. The translation results are presented in Table  4 . The baseline is a phrase-based system with all features including the secondary phrase table and LM. The new system uses the same features except that the primary phrase table is discriminatively trained using maximum expected-BLEU and GT optimization as described earlier in this paper. The results are obtained using the two-stage training schedule, including six iterations for training phrase translation models and two iterations for training lexicon translation models. The results in Table  4  show that the proposed method leads to an improvement of 1.2 BLEU point over the baseline. This gives the best single system result on this task.  

 Summary The contributions of this work can be summarized as follows. First, we propose a new objective function (Eq. 9) for training of large-scale translation models, including phrase and lexicon models, with more parameters than all previous methods have attempted. The objective function consists of 1) the utility function of expected BLEU score, and 2) the regularization term taking the form of KL divergence in the parameter space. The expected BLEU score is closely linked to translation quality and the regularization is essential when many parameters are trained at scale. The importance of both is verified experimentally with the results presented in this paper. Second, through non-trivial derivation, we show that the novel objective function of Eq. (  9 ) is amenable to iterative GT updates, where each update is equipped with a closed-form formula. Third, the new objective function and new optimization technique are successfully applied to two important machine translation tasks, with implementation issues resolved (e.g., training schedule and hyper-parameter tuning, etc.). The superior results clearly demonstrate the effectiveness of the proposed algorithm. , and assume ?(?) and ?(?) are two positive polynomial functions of ? , a GT of ? for the rational function ? ? = !(?) !(?) can be obtained through the following two steps: i) Construct the auxiliary function: ? ? = ? ? ? ? ? ! ? ? (10) where ? ! are the values from the previous iteration. Increasing f guarantees an increase of r, i.e., ? ? > 0 and ? ? ? ? ? = ! ! ? ? ? ? ? ? . 

 Figure 1 . 1 Figure 1. The max expected-BLEU training algorithm. 

 3. set ? = ?, ? ! = ?. 4. Max expected BLEU training a. Go through the training set. i. Compute ? ?! (? ! |? ! ) and ? ! (?) . ii. Accumulate statistics {?}. b. Update: ? ! ? ? by one iteration of GT. 5. MERT on the tuning set: ? ! ? ?. 6. Test on the validation set using { ?, ? }. 7. Go to step 3 unless training converges or reaches a certain number of iterations. 8. Pick the best { ?, ? } on the validation set. 

 sentence-level BLEU (Exp. BLEU) score of N-best lists and the corpus-level BLEU score of 1-best translations. The conjectured relationship between the two is important in justifying our use of the former as the training objective. Fig.2shows these two scores on the training set over training iterations. Since the expected BLEU is affected by ? strongly, we fix the value of ? in order to make the expected BLEU comparable across different iterations. From Fig.2it is clear that the expected BLEU score correlates strongly with the real BLEU score, justifying its use as our training objective. 

 Figure 2 . 2 Figure 2. Expected sentence BLEU and 1-best corpus BLEU on the 751K sentence of training data. 

 Figure 3 . 3 Figure 3. BLEU scores on the validation set as a function of the GT training iteration in two-stage training of both the phrase translation models (PT) and the lexicon models (LEX). The BLEU scores on training phrase models are shown in blue, and on training lexicon models in red. 

 Table 2 . 2 Results on degrees of regularizations. BLEU scores are reported on the validation set. ? denotes the gain over the baseline. Test on Validation Set ?% ?% Baseline 26.70 -- ? = 0 (no regularization) 26.91 +0.21 ? = 1?10 !! 27.31 +0.61 ? = 5?10 !! 27.44 +0.74 ? = 10?10 !! 27.27 +0.57 

			 This is to better approximate corpus-level BLEU, i.e., as discussed in (Chiang, et al., 2008) , the per-sentence BP might effectively exceed unity in corpus-level BLEU computation. 2 This is to focus the training on improving BLEU by improving n-gram match instead of by improving BP, e.g., this makes the BP of the baseline output already being perfect.
