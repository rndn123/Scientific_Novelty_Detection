title
Overview of the 6th Workshop on Asian Translation

abstract
This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including Ja?En, Ja?Zh scientific paper translation subtasks, Ja?En, Ja?Ko, Ja?En patent translation subtasks, Hi?En, My?En, Km?En, Ta?En mixed domain subtasks, Ru?Ja news commentary translation task, and En?Hi multi-modal translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 7 1 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated.

Introduction The Workshop on Asian Translation (WAT) is an open evaluation campaign focusing on Asian languages. Following the success of the previous workshops WAT2014-WAT2018  (Nakazawa et al., 2014 (Nakazawa et al., , 2015 (Nakazawa et al., , 2016 (Nakazawa et al., , 2017 (Nakazawa et al., , 2018 , WAT2019 brings together machine 1 One paper was withdrawn post acceptance and hence only 6 papers will be in the proceedings. translation researchers and users to try, evaluate, share and discuss brand-new ideas for machine translation. We have been working toward practical use of machine translation among all Asian countries. For the 6th WAT, we adopted new translation subtasks with Khmer?English and Tamil?English mixed domain corpora, 2 Rus-sian?Japanese news commentary corpus and English?Hindi multi-modal corpus 3 in addition to most of the subtasks of WAT2018. WAT is a unique workshop on Asian language translation with the following characteristics: ? Domain and language pairs WAT is the world's first workshop that targets scientific paper domain, and Chi-nese?Japanese and Korean?Japanese language pairs. In the future, we will add more Asian languages such as Vietnamese, Thai and so on. ? Evaluation method Evaluation is done both automatically and manually. Firstly, all submitted translation results are automatically evaluated using three metrics: BLEU, RIBES and AMFM. Among them, selected translation results are assessed by two kinds of human evaluation: pairwise evaluation and JPO adequacy evaluation. 

 Datasets 2.1 ASPEC ASPEC was constructed by the Japan Science and Technology Agency (JST) in collaboration with the National Institute of Information and Communications Technology (NICT). The corpus consists of a Japanese-English scientific paper abstract corpus (ASPEC-JE), which is used for ja?en subtasks, and a Japanese-Chinese scientific paper excerpt corpus (ASPEC-JC), which is used for ja?zh subtasks. The statistics for each corpus are shown in Table  1 . 

 ASPEC-JE The training data for ASPEC-JE was constructed by NICT from approximately two million Japanese-English scientific paper abstracts owned by JST. The data is a comparable corpus and sentence correspondences are found automatically using the method from  Utiyama and Isahara (2007) . Each sentence pair is accompanied by a similarity score calculated by the method and a field ID that indicates a scientific field. The correspondence between field IDs and field names, along with the Lang Train Dev DevTest Test-N zh-ja 1,000,000 2,000 2,000 5,204 ko-ja 1,000,000 2,000 2,000 5,230 en-ja 1,000,000 2,000 2,000 5,668 Lang Test-N1 Test-N2 Test-N3 Test-EP zh-ja 2,000 3,000 204 1,151 ko-ja 2,000 3,000 230 en-ja 2,000 3,000 668 -Table  2 : Statistics for JPC frequency and occurrence ratios for the training data, are described in the README file of ASPEC-JE. The development, development-test and test data were extracted from parallel sentences from the Japanese-English paper abstracts that exclude the sentences in the training data. Each dataset consists of 400 documents and contains sentences in each field at the same rate. The document alignment was conducted automatically and only documents with a 1-to-1 alignment are included. It is therefore possible to restore the original documents. The format is the same as the training data except that there is no similarity score.  fication (IPC) sections are chemistry, electricity, mechanical engineering, and physics. At WAT2019, the patent tasks has two subtasks: normal subtask and expression pattern subtask. Both subtasks use common training, development and development-test data for each language pair. The normal subtask for three language pairs uses four test data with different characteristics: ? test-N: union of the following three sets; ? test-N1: patent documents from patent families published between 2011 and 2013; ? test-N2: patent documents from patent families published between 2016 and 2017; and ? test-N3: patent documents published between 2016 and 2017 where target sentences are manually created by translating source sentences. The expression pattern subtask for zh?ja pair uses test-EP data. The test-EP data consists of sentences annotated with expression pattern categories: title of invention (TIT), abstract (ABS), scope of claim (CLM) or description (DES). The corpus statistics are shown in Table  2 . Note that training, development, development-test and test-N1 data are the same as those used in WAT2017. 

 TDDC Timely Disclosure Documents Corpus (TDDC) was constructed by Japan Exchange Group (JPX). The corpus was made by aligning the sentences manually from past Japanese and English timely disclosure documents in PDF format published by companies listed on Tokyo Stock Exchange (TSE). Timely Disclosure tasks focus on Japanese to English translation of sentences extracted from timely disclosure documents in order to avoid mistranslations that would confuse investors. TSE is one of the largest capital markets in the world that has over 3,600 companies listed as of the end of 2018. Companies are required to disclose material information including financial statements, corporate actions, and corporate governance policies to the public in a timely manner. These timely disclosure documents form an important basis for investment decisions, containing important figures (e.g., sales, profits, significant dates) and proper nouns (e.g., names of persons, places, companies, business and product). Since such information is critical for investors, mistranslations should be avoided and translations should be of a high quality. The corpus consists of Japanese-English sentence pairs, document hashes, and sentence hashes. A document hash is a hash of the Document ID, which is a unique identifier of the source document. A sentence hash is a hash of the Document ID and the Sentence ID, which is a unique identifier of the sentence in each source document. The corpus is partitioned into training,  development, development-test, and       4 . The sentence pairs in each data are identified in the same manner as that for ASPEC using the method from  (Utiyama and Isahara, 2007) . 

 IITB Corpus IIT Bombay English-Hindi Corpus contains English-Hindi parallel corpus as well as monolingual Hindi corpus collected from a variety of sources and corpora. This corpus had been developed at the Center for Indian Language Technology, IIT Bombay over the years. The corpus is used for mixed domain tasks hi?en. The statistics for the corpus are shown in Table 5. 

 ALT and UCSY Corpus The parallel data for Myanmar-English translation tasks at WAT2019 consists of two cor-  The ALT corpus has been manually segmented into words  (Ding et al., , 2019 , and the UCSY corpus is unsegmented. A script to tokenize the Myanmar data into writing units is released with the data. The automatic evaluation of Myanmar translation results is based on the tokenized writing units, regardless to the segmented words in the ALT data. However, participants can make a use of the segmentation in ALT data in their own manner. The detailed composition of training, development, and test data of the Myanmar-English translation tasks are listed in Table  6 . Notice that both of the corpora have been modified from the data used in WAT2018. 

 ALT and ECCC Corpus The parallel data for Khmer-English translation tasks at WAT2019 consists of two corpora, the ALT corpus and ECCC corpus.  The ALT corpus has been manually segmented into words , and the ECCC corpus is unsegmented. A script to tokenize the Khmer data into writing units is released with with the data. The automatic evaluation of Khmer translation results is based on the tokenized writing units, regardless to the segmented words in the ALT data. However, participants can make a use of the segmentation in ALT data in their own manner. The detailed composition of training, development, and test data of the Khmer-English translation tasks are listed in Table  7 . 

 Multi-Modal Task Corpus For English?Hindi multi-modal translation task we asked the participants to use the Hindi Visual Genome corpus  (HVG, Parida et al., 2019a,b) . The statistics of the corpus are given in Table  8 . One "item" in the original HVG consists of an image with a rectangular region highlighting a part of the image, the original English caption of this region and the Hindi reference translation. Depending on the track (see 2.8.1 below), some of these item components are available as the source and some serve as the reference or play the role of a competing candidate solution. HVG Training, D-Test and E-Test sections were accessible to the participants in advance. The participants were explicitly instructed not to consult E-Test in any way but strictly speaking, they could have used the reference translation (which would mean cheating from the evaluation point of view  was distributed to task participants and the target side was published only after output submission deadline. Note that the original Visual Genome suffers from a considerable level of noise. Some observed English grammar errors are illustrated in Figure  1 . We also took the chance and used our manual evaluation for validating the quality of the captions given the picture, see 8.4.1 below. The multi-modal task includes three tracks as illustrated in Figure  1:  2.8.1 Multi-Modal Task Tracks 1. Text-Only Translation (labeled "TEXT" in WAT official tables): The participants are asked to translate short English captions (text) into Hindi. No visual information can be used. On the other hand, additional text resources are permitted (but they need to be specified in the corresponding system description paper). 2. Hindi Captioning (labeled "HI"): The participants are asked to generate captions in Hindi for the given rectangular region in an input image. 3. Multi-Modal Translation (labeled "MM"): Given an image, a rectangular region in it and an English caption for the rectangular region, the participants are asked to translate the English text into Hindi. Both textual and visual information can be used.   

 EnTam Corpus For Tamil?English translation task we asked the participants to use the publicly available EnTam mixed domain corpus 4  (Ramasamy et al., 2012) . This corpus contains training, development and test sentences mostly from the news-domain. The other domains are Bible and Cinema. The statistics of the corpus are given in Table  9 . 

 JaRuNC Corpus For the Russian?Japanese task we asked participants to use the JaRuNC corpus 5  (Imankulova et al., 2019)   translation quality as well but this is beyond the scope of this years sub-task. Refer to Table  10  for the statistics of the in-domain parallel corpora. In addition we encouraged the participants to use out-of-domain parallel corpora from various sources such as KFTT, 6 JESC, 7 TED, 8 ASPEC, 9 UN, 10 Yandex 11 and Russian?English news-commentary corpus. 12 3 Baseline Systems Human evaluations of most of WAT tasks were conducted as pairwise comparisons between the translation results for a specific baseline system and translation results for each partic-ipant's system. That is, the specific baseline system was the standard for human evaluation. At WAT 2019, we adopted a neural machine translation (NMT) with attention mechanism as a baseline system. The NMT baseline systems consisted of publicly available software, and the procedures for building the systems and for translating using the systems were published on the WAT web page.  13  We also have SMT baseline systems for the tasks that started at WAT 2017 or before 2017. The baseline systems are shown in Tables  11, 12 , and 13. SMT baseline systems are described in the WAT 2017 overview paper  (Nakazawa et al., 2017) . The commercial RBMT systems and the online translation systems were operated by the organizers. We note that these RBMT companies and online translation companies did not submit themselves. Because our objective is not to compare commercial RBMT systems or online translation systems from companies that did not themselves participate, the system IDs of these systems are anonymous in this paper. 

 Training Data We used the following data for training the NMT baseline systems. ? All of the training data for each task were used for training except for the AS-PEC Japanese-English task. For the AS-PEC Japanese-English task, we only used train-1.txt, which consists of one million parallel sentence pairs with high similarity scores. ? All of the development data for each task was used for validation. 

 Tokenization We used the following tools for tokenization.  When we built BPE-codes, we merged source and target sentences and we used 100,000 fors option. We used 10 for vocabulary-threshold when subword-nmt applied BPE. 

 For EnTam, News Commentary ? The Moses toolkit for English and Russian only for the News Commentary data. ? Mecab 19 for Japanese segmentation. ? The EnTam corpus is not tokenized by any external toolkits. ? Both corpora are further processed by tensor2tensor's internal pre/postprocessing which includes sub-word segmentation. 

 For Multi-Modal Task ? Hindi Visual Genome comes untokenized and we did not use or recommend any specific external tokenizer. ? The standard OpenNMT-py sub-word segmentation was used for pre/postprocessing for the baseline system and each participant used what they wanted. 

 Baseline NMT Methods We used the following NMT with attention for most of the tasks. We used Transformer  (Vaswani et al., 2017 ) (Tensor2Tensor)) for the News Commentary and English?Tamil tasks and Transformer (OpenNMT-py) for the Multimodal task. 

 NMT with Attention We used OpenNMT  (Klein et al., 2017)  as the implementation of the baseline NMT systems of NMT with attention (System ID: NMT). We used the following OpenNMT configuration. ? encoder_type = brnn ? brnn_merge = concat ? src_seq_length = 150 ? tgt_seq_length = 150 ? src_vocab_size = 100000 ? tgt_vocab_size = 100000 ? src_words_min_frequency = 1 ? tgt_words_min_frequency = 1 The default values were used for the other system parameters. 

 Transformer (Tensor2Tensor) For the News Commentary and En-glish?Tamil tasks, we used tensor2tensor's 20 implementation of the Transformer  (Vaswani et al., 2017)  and use default hyperparameter settings corresponding to the "base" model for all baseline models. The baseline for the News Commentary task is a multilingual model as described in  Imankulova et al. (2019)  which is trained using only the in-domain parallel corpora. We use the token trick proposed by  (Johnson et al., 2017)  to train the multilingual model. As for the English?Tamil task, we train separate baseline models for each translation direction with 32,000 separate sub-word vocabularies. 

 Transformer (OpenNMT-py) For the Multimodal task, we used the Transformer model  (Vaswani et al., 2018)  as implemented in OpenNMT-py  (Klein et al., 2017 ) and used the "base" model with default parameters for the multi-modal task baseline. We have generated the vocabulary of 32k subword types jointly for both the source and target languages. The vocabulary is shared between the encoder and decoder. 4 Automatic Evaluation 

 Procedure for Calculating Automatic Evaluation Score We evaluated translation results by three metrics: BLEU  (Papineni et al., 2002) , RIBES  (Isozaki et al., 2010)  and AMFM  (Banchs et al., 2015) . BLEU scores were calculated using multi-bleu.perl in the Moses toolkit  (Koehn et al., 2007) . RIBES scores were calculated using RIBES.py version 1.02.4. 21 AMFM scores were calculated using scripts created by the technical collaborators listed in the WAT2019 web page.  22  All scores for each task were calculated using the corresponding reference translations. Before the calculation of the automatic evaluation scores, the translation results were tokenized or segmented with tokenization/segmentation tools for each language. For Japanese segmentation, we used three different tools: Juman version 7.0  (Kurohashi et al., 1994) , KyTea 0.4.6  (Neubig et al., 2011)  with full SVM model 23 and MeCab 0.996  (Kudo, 2005)  with IPA dictionary 2.7.0. 24 For Chinese segmentation, we used two different tools: KyTea 0.4.6 with full SVM Model in MSR model and Stanford Word Segmenter  (Tseng, 2005)  version 2014-06-16 with Chinese Penn Treebank (CTB) and Peking University (PKU) model.  25  For Korean segmentation, we used mecab-ko.  26  For Myanmar and Khmer segmentations, we used myseg.py 27 and kmseg.py 28 . For English and Russian tokenizations, we used tokenizer.perl 29 in the Moses toolkit. For Hindi and Tamil tokenizations, we used Indic NLP Library.  30  The detailed procedures for the automatic evaluation are shown on the WAT2019 evaluation web page.  

 Automatic Evaluation System The automatic evaluation system receives translation results by participants and automatically gives evaluation scores to the uploaded results. As shown in Figure  2 , the system requires participants to provide the following information for each submission: ? Human Evaluation: whether or not they submit the results for human evaluation; ? Publish the results of the evaluation: whether or not they permit to publish automatic evaluation scores on the WAT2019 web page. ? Task: the task you submit the results for; ? Used Other Resources: whether or not they used additional resources; and ? Method: the type of the method including SMT, RBMT, SMT and RBMT, EBMT, NMT and Other. Evaluation scores of translation results that participants permit to be published are disclosed via the WAT2019 evaluation web page. Participants can also submit the results for human evaluation using the same web interface. This automatic evaluation system will remain available even after WAT2019. Anybody can register an account for the system by the procedures described in the registration web page. 32 

 Additional Automatic Scores in Multi-Modal Task For the multi-modal task, several additional automatic metrics were run aside from the WAT evaluation server, namely: BLEU (now calculated by Moses scorer 33 ), characTER  (Wang et al., 2016) , chrF3  (Popovi?, 2015) , TER  (Snover et al., 2006) , WER, PER and CDER  (Leusch et al., 2006)   JPC{N,N1,N2,N3,EP}zh-ja ,JPC{N,N1,N2,N3}ja-zh, JPC{N,N1,N2,N3}ko-ja, JPC{N,N1,N2,N3}ja-ko, JPC{N,N1,N2,N3}en-ja, and JPC{N,N1,N2,N3}ja-en indicate the patent tasks with JPO Patent Corpus. JPCN1{zh-ja,ja-zh,ko-ja,ja-ko,en-ja,ja-en} are the same tasks as JPC{zh-ja,ja-zh,ko-ja,ja-ko,en-ja,ja-en} in WAT2015-WAT2017. AMFM is not calculated for JPC{N,N2,N3} tasks. 

 Human evaluation: If you want to submit the file for human evaluation, check the box "Human Evaluation". Once you upload a file with checking "Human Evaluation" you cannot change the file used for human evaluation. When you submit the translation results for human evaluation, please check the checkbox of "Publish" too. You can submit two files for human evaluation per task. One of the files for human evaluation is recommended not to use other resources, but it is not compulsory. 

 Other: Team Name, Task, Used Other Resources, Method, System Description (public) , Date and Time(JST), BLEU, RIBES and AMFM will be disclosed on the Evaluation Site when you upload a file checking "Publish the results of the evaluation". You can modify some fields of submitted data. Read "Guidelines for submitted data" at the bottom of this page. scores are lower, we reverse the score by taking 1 ? x and indicate this by prepending "n" to the metric name. With this modification, higher scores always indicate a better translation result. Also, we multiply all metric scores by 100 for better readability. 

 Back to top 

 Human Evaluation In WAT2019, we conducted three kinds of human evaluations: pairwise evaluation (Section 5.1) and JPO adequacy evaluation (Section 5.2) for text-only language pairs and a pairwise variation of direct assessment (Section 5.3) for the multi-modal task. 

 Pairwise Evaluation We conducted pairwise evaluation for participants' systems submitted for human evaluation. The submitted translations were evaluated by a professional translation company and Pairwise scores were given to the submissions by comparing with baseline translations (described in Section 3). 

 Sentence Selection and Evaluation For the pairwise evaluation, we randomly selected 400 sentences from the test set of each task. We used the same sentences as the last year for the continuous subtasks. Baseline and submitted translations were shown to annotators in random order with the input source sentence. The annotators were asked to judge which of the translations is better, or whether they are on par. 

 Voting To guarantee the quality of the evaluations, each sentence is evaluated by 5 different annotators and the final decision is made depending on the 5 judgements. We define each judgement j i (i = 1, ? ? ? , 5) as: j i = ? ? ? 1 if better than the baseline ?1 if worse than the baseline 0 if the quality is the same The final decision D is defined as follows using S = ? j i : D = ? ? ? win (S ? 2) loss (S ? ?2) tie (otherwise) 

 Pairwise Score Calculation Suppose that W is the number of wins compared to the baseline, L is the number of losses and T is the number of ties. The Pairwise score can be calculated by the following formula: P airwise = 100 ? W ? L W + L + T From the definition, the Pairwise score ranges between -100 and 100. 

 Confidence Interval Estimation There are several ways to estimate a confidence interval. We chose to use bootstrap resampling  (Koehn, 2004)  to estimate the 95% 5 All important information is transmitted correctly. (100%) 4 Almost all important information is transmitted correctly. (80%-) 3 More than half of important information is transmitted correctly. (50%-) 2 Some of important information is transmitted correctly. (20%-) 1 Almost all important information is NOT transmitted correctly. (-20%) Table  14 : The JPO adequacy criterion confidence interval. The procedure is as follows: 1. randomly select 300 sentences from the 400 human evaluation sentences, and calculate the Pairwise score of the selected sentences 2. iterate the previous step 1000 times and get 1000 Pairwise scores 3. sort the 1000 scores and estimate the 95% confidence interval by discarding the top 25 scores and the bottom 25 scores 

 JPO Adequacy Evaluation We conducted JPO adequacy evaluation for the top two or three participants' systems of pairwise evalution for each subtask.  35  The evaluation was carried out by translation experts based on the JPO adequacy evaluation criterion, which is originally defined by JPO to assess the quality of translated patent documents. 

 Sentence Selection and Evaluation For the JPO adequacy evaluation, the 200 test sentences were randomly selected from the 400 test sentences used for the pairwise evaluation. For each test sentence, input source sentence, translation by participants' system, and reference translation were shown to the annotators. To guarantee the quality of the evaluation, each sentence was evaluated by two annotators. Note that the selected sentences are the same as those used in the previous workshops except for the new subtasks at WAT2019. 

 Evaluation Criterion Table  14  shows the JPO adequacy criterion from 5 to 1. The evaluation is performed subjectively. "Important information" represents the technical factors and their relationships. The degree of importance of each element is also considered to evaluate. The percentages in each grade are rough indications for the transmission degree of the source sentence meanings. The detailed criterion is described in the JPO document (in Japanese). 36 

 Manual Evaluation for the Multi-Modal Task The evaluations of the three tracks of the multi-modal task follow the Direct Assessment (DA,  technique by asking annotators to assign a score from 0 to 100 to each candidate. The score is assigned using a slider with no numeric feedback, the scale is therefore effectively continuous. After a certain number of scored items, each of the annotators stabilizes in their predictions. The collected DA scores can be either directly averaged for each system and track (denoted "Ave"), or first standardized per annotator and then averaged ("Ave Z"). The standardization removes the effect of individual differences in the range of scores assigned: the scores are scaled so that the average score of each annotator is 0 and the standard deviation is 1. Our evaluation differs from the basic DA in the following respects: (1) we run the evaluation bilingually, i.e. we require the annotators to understand the source English sufficiently to be able to assess the adequacy of the Hindi translation, (2) we ask the annotators to score two distinct segments at once, while the original DA displays only one candidate at a time. The main benefit of bilingual evaluation is that the reference is not needed for the evalu- ation. Instead, the reference can be included among other candidates and the manual evaluation allows us to directly compare the performance of MT to human translators. The dual judgment (scoring two candidates at once) was added experimentally. The advantage is saving some of the annotators' time (they do not need to read the source or examine the picture again) and the chance to evaluate candidates also in terms of direct pairwise comparisons. In the history of WMT  (Bojar et al., 2016) , 5-way relative ranking was used for many years. With 5 candidates, the individual pairs may not be compared very precisely. With the single-candidate DA, pairwise comparisons cannot be used as the basis for system ranking. We believe that two candidates on one screen could be a good balance. For the full statistical soundness, the judgments should be independent of each other. This is not the case in our dual scoring, even if we explicitly ask people to score the candidates independent of each other. The full independence is however not assured even in the original approach because annotators will remember their past judgments. This year, WMT even ran DA with document context available to the annotators by scoring all segments from a given document one after another in their natural order. We thus dare to pretend independence of judgments when interpreting DA scores. The user interface for our annotation for each of the tracks is illustrated in Figure  3 , Figure  4 , and Figure  5 . In the "text-only" evaluation, one English text (source) and two Hindi translations (candidate 1 and 2) are shown to the annotators. In the "multi-modal" evaluation, the annotators are shown both the image and the source English text. The first question is to validate if the source English text is a good caption for the indicated area. For two translation candidates, the annotators are asked to independently indicate to what extent the meaning is preserved. The "Hindi captioning" evaluation shows only the image and two Hindi candidates. The annotators are reminded that the two captions should be treated independently and that each of them can consider a very different aspect of the region. 

 Participants Table  15  shows the participants in WAT2019. The table lists 25 organizations from various countries, including Japan, India, Myanmar, USA, Korea, China, France, and Switzerland. About 400 translation results by 25 teams were submitted for automatic evaluation and about 30 translation results by 8 teams were submitted for pairwise evaluation. We selected about 50 translation results for JPO adequacy evaluation. Table  16  shows tasks for which each team submitted results by the deadline. 

 Evaluation Results In this section, the evaluation results for WAT2019 are reported from several perspectives. Some of the results for both automatic and human evaluations are also accessible at the WAT2019 website.    17 and 18 . The weights for the weighted ?  (Cohen, 1968 ) is defined as |Evaluation1 ? Evaluation2|/4. The automatic scores for the multi-modal task along with the WAT evaluation server BLEU scores are provided in Table  20 . For each of the test sets (E-Test and C-Test), the scores are comparable across all the tracks (text-only, captioning or multi-modal translation) because of the underlying set of reference translations is the same. The scores for the captioning task will be however very low because captions generated independently of the English source caption are very likely to differ from the reference translation. For multi-modal task, Table  19  shows the manual evaluation scores for all valid system submissions. As mentioned above, we used the reference translation as if it was one of the competing systems, see the rows "Reference" in the table. The annotation was fully anonymized, so the annotators had no chance of knowing if they are scoring human translation or MT output.                         Table  15 : List of participants in WAT2019      Team ID ASPEC JPC TDDC JIJI NCPD EJ JE CJ JC EJ JE CJ JC Ko-J J-Ko JE EJ JE RJ JR TMU ? ? NTT ? ? ? NICT-2 ? ? ? NICT-5 ? ? ? srcb ? ? ? ? sarah ? ? ? ? ? ? ? ? ? KNU_Hyundai ? ? ? ? ? ? ? ? ? ryan ? ? ? ? ? ? AISTAI ? SYSTRAN ? ? NHK-NES ? ? geoduck ? ykkd ? Team ID Mixed-domain tasks Mutimodal task ALT IITB UFAL (EnTam) EV/CH EM ME E-Kh Kh-E EH HE ET TE EH NICT ? ? NICT-4 ? ? ? ? NICT-5 ? ? ? UCSYNLP ? ? UCSMNLP ? ? cvit ? ? ? ? sarah ? 683 ? NITSNLP ? PUP-IND ? FBAI ? ? LTRC-MT ? IDIAP ? NLPRL ? ? 

 Statistical Significance Testing of Pairwise Evaluation between Submissions Table  21  shows the results of statistical significance testing of aspec-ja-en subtasks, Table  22  shows that of JIJI subtasks, Table  23  shows that of TDDC subtasks. ?, ? and > mean that the system in the row is better than the system in the column at a significance level of p < 0.01, 0.05 and 0.1 respectively. Testing is also done by the bootstrap resampling as follows: 1. randomly select 300 sentences from the 400 pairwise evaluation sentences, and calculate the Pairwise scores on the selected sentences for both systems 2. iterate the previous step 1000 times and count the number of wins (W ), losses (L) and ties (T ) 3. calculate p = L W +L Inter-annotator Agreement To assess the reliability of agreement between the workers, we calculated the Fleiss' ?  (Fleiss et al., 1971)  values. The results are shown in Table  24 . We can see that the ? values are larger for X?J translations than for J?X translations. This may be because the majority of the workers for these language pairs are Japanese, and the evaluation of one's mother tongue is much easier than for other languages in general. The ? values for Hindi languages are relatively higt. This might be because the overall translation quality of the Hindi languages are low, and the evaluators can easily distinguish better translations from worse ones. 

 Findings In this section, we will show findings of some of the translation tasks. 

 TDDC In the results of both the automatic evaluation and the human evaluation, every system translated most sentences correctly. According to the human evaluation of the subtasks of 'Items' and 'Texts', all evaluators rated more than 70% of all the pairs at 4 or 5. Most of these high-rated pairs consist of typical terms and sentences from timely disclosure documents. This tasks focus on the accurate translation of figures, so the evaluation criteria confirmed there are no mistranslation in the typical sentences containing figures, such as unit of money and dates. However, uncommon sentences used in timely disclosure documents tend to be mistranslated. For example, uncommon proper nouns tended to be omitted or mistranslated to other meaning words, besides sentences which has complex and uncommon structures, generally long sentences, caused errors at dependency of subordinate clauses. In addition, some systems translated sentences without subjects into sentences with incorrect subjects. Japanese sentences often omit subjects and objects, which would normally be included in English. For example, a Japanese sentence, "? 27,000 ? ? "(Common shares of the Company, limited to a maximum of 27,000 shares), was translated to"(Unrelated company name) common stocks up to 27,000 shares". Moreover, there are some incorrect modifiers or determiners. In Japanese timely disclosure documents, there are many variable prefix for dates, such as"?" (this),"?" (this),"?" (next), and "?"(last). Some systems translated sentences containing these words with incorrect year. For example, a Japanese sentence contains "? 3 ? ?" (the end of third quarter of this fiscal year) was translated to"the end of the third quarter of FY 2016". In summary, the causes of these mistranslations are considered as follows: ? It is difficult for the systems to translate long sentence and proper nouns which TDDC does not contain. ? Some source sentences are unclear due to lack of subjects and/or objects, so these are not suitable for English translation. ? TDDC contains not semantically balanced pairs and the systems might be affected strongly by either of source pair sentences. On the other hand, some translations seem to be fitted to sentences of TDDC which are freely and omitted redundant expressions, but evaluators mark them as low scores, probably because they are not literal translations. This result implies that it is necessary to create another evaluation criterion, which evaluates the correctness of transmitting information to investors correctly. 

 English?Tamil Task We observed that most participants used transfer learning techniques such as fine-tuning and mixed fine-tuning for Tamil?English translation leading to reasonably high quality translations. However, English?Tamil translation is still poor and the main reason is the lack of helping parallel corpora. We expect that utilization of large in-domain monolingual corpora for backtranslation should help alleviate this problem. We will provide such corpora for next year's task. 

 News Commentary Task We only received 3 submissions for Rus-sian?Japanese translation and all submissions leveraged multilingualism and multi-step fine-tuning proposed by  Imankulova et al. (2019)  and showed that carefully choosing corpora and robust training can dramatically enhance the quality of NMT for language pairs   that have very small in-domain parallel corpora. For next year's task we expect more submissions where participants will leverage additional larger helping monolingual as well as bilingual corpora. 8.4 Multi-Modal Task 

 Validation of Source English Captions In the manual evaluation of multimodal track, our annotators saw both the picture and the source text (and the two scored candidates). We took this opportunity to double check the quality of the original HVG data. Prior to scoring the candidates, we asked our annotators to confirm that the source English text is a good caption for the indicated region of the image. The results in Table  25  indicate that for a surprisingly high number of items we did not receive any answer. This confirms that even non-anonymous annotators can easily provide sloppy evaluations. It is possible that part of these omissions can be attributed to our annotation interface which was showing all items on one page and relying on scrolling. Next time, we will show only one annotation item on each page and also consider highlighting unanswered questions. Strictly requiring an answer would not be always appropriate but we need to ensure that annotators are aware that they are skipping a question. Luckily, the bad source captions are not a frequent case, amounting to 1 or 2% of assessed examples. 

 Relation to Human Translation The bilingual style of evaluation of the multimodal task allowed us to evaluate the reference translations as if they were yet another competing MT system. Table  19  thus lists also the "Reference". Across the tracks and test sets (EV vs. CH), humans surpass MT candidates. One single exception is IDIAP run 2956 winning in textonly translation of the E-Test, but this is not confirmed on the C-Test (CH). The score of the anonymized system 683 on E-Test in multi-modal track (MM) has also almost reached human performance. These are not the first cases of MT performing on par with humans and we are happy to see this when targetting an Indian language. 8.4.3 Evaluating Captioning While the automatic scores are comparable across tasks, the Hindi-only captioning ("HI") must be considered separately. Without a source sentence, both humans and machines are very likely to come up with highly varying textual captions. The same image can be described in many different aspects. All our automatic metrics compare the candidate caption with the reference one generally on the basis of the presence of the same character sequences, words or n-grams. Candidates diverging from the reference will get a low score regardless of their actual quality. The automatic evaluation score for the "Hindi caption" is very very low as compared to other sub-tasks ("text-only" and "multimodal" translations) as can be seen in the Table 20. Even the human annotators couldn't give any score for most of the segments submitted from the "Hindi caption" entries due to the wrong caption generation. 

 Conclusion and Future Perspective This paper summarizes the shared tasks of WAT2019. We had 25 participants worldwide, and collected a large number of useful submissions for improving the current machine translation systems by analyzing the submissions and identifying the issues. For the next WAT workshop, we plan to conduct document-level evaluation using the new dataset with context for some translation subtasks and we would like to consider how to realize context-aware machine translation in WAT. Also, we are planning to do extrinsic evaluation of the translations.   
