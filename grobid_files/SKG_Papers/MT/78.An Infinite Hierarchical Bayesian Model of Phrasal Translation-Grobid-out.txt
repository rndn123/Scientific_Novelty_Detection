title
An Infinite Hierarchical Bayesian Model of Phrasal Translation

abstract
Modern phrase-based machine translation systems make extensive use of wordbased translation models for inducing alignments from parallel corpora. This is problematic, as the systems are incapable of accurately modelling many translation phenomena that do not decompose into word-for-word translation. This paper presents a novel method for inducing phrase-based translation units directly from parallel data, which we frame as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. Overall this leads to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. Our experiments on Arabic, Urdu and Farsi to English demonstrate improvements over competitive baseline systems.

Introduction The phrase-based approach  (Koehn et al., 2003)  to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems  (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006)  all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models  (Brown et al., 1993)  remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from which phrasal translation units are extracted using a heuristic. Although this approach demonstrably works, it suffers from a number of shortcomings. Firstly, many phrase-based phenomena which do not decompose into word translations (e.g., idioms) will be missed, as the underlying word-based alignment model is unlikely to propose the correct alignments. Secondly, the relationship between different phrase-pairs is not considered, such as between single word translations and larger multi-word phrase-pairs or where one large phrase-pair subsumes another. This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a 'cache' learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars  (Johnson et al., 2007a) , which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a Pitman-Yor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling  (Teh, 2006) . We are not the first to consider this idea;  Neubig et al. (2011)  developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.'s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Neubig et al.'s method, and a competitive phrase-based baseline. 

 Related Work Inversion transduction grammar (or ITG)  (Wu, 1997)  is a well studied synchronous grammar formalism. Terminal productions of the form X ? e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets ... , generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised  (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010)  and supervised  (Haghighi et al., 2009; Cherry and Lin, 2006)  settings, and for decoding  (Petrov et al., 2008) . Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment  (DeNero and Klein, 2010; Neubig et al., 2011) . The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to  Neubig et al. (2011)  which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with  Marcu and Wong (2002)  who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs  (Blunsom et al., 2009b; Levenberg et al., 2012) . Conceptually, our work could be readily adapted to general SCFGs using similar techniques. This work was inspired by adaptor grammars  (Johnson et al., 2007a) , a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation;  (Neubig et al., 2011 ) also developed a similar approach based around ITG using a Pitman-Yor Process prior. Our approach improves upon theirs in terms of the model and inference, and critically, this is borne out in our experiments where we show uniform improvements in translation quality over a baseline system, as compared to their almost entirely negative results. We believe that their approach had a number of flaws: For inference they use a beam-search, which may speed up processing but means that they are no longer sampling from the true distribution, nor a distribution with the same support as the posterior. Moreover they include a Metropolis-Hastings correction step, which is required to correct the samples to account for repeated substructures which will be otherwise underrepresented. Consequently their approach does not constitute a Markov Chain Monte Carlo sampler, but rather a complex heuristic. The other respect in which this work differs from  Neubig et al. (2011)  is in terms of model formulation. They develop an ITG which generates phrase-pairs as terminals, while we employ a more restrictive word-based model which forces the decomposition of every phrase-pair. This is an important restriction as it means that we jointly learn a word and phrase based model, such that word based phenomena can affect the phrasal structures. Finally our approach models separately the three different types of ITG production (monotone, swap and lexical emission), allowing for a richer parameterisation which the model exploits by learning different hyper-parameter values. 

 Model The generative process of the model follows that of ITG with the following simple grammar X ? [X X] | X X X ? e/f | e/? | ?/f , where [?] denotes monotone ordering and ? denotes a swap in one language. The symbol ? denotes the empty string. This corresponds to a simple generative story, with each stage being a nonterminal rewrite starting with X and terminating when there are no frontier non-terminals. A popular variant is a phrasal ITG, where the leaves of the ITG tree are phrase-pairs and the training seeks to learn a segmentation of the source and target which yields good phrases. We would not expect this model to do very well as it cannot consider overlapping phrases, but instead is forced into selecting between many competing -and often equally viable -options. Our approach improves over the phrasal model by recursively generating complete phrases. This way we don't insist on a single tiling of phrases for a sentence pair, but explicitly model the set of hierarchically nested phrases as defined by an ITG derivation. This approach is closer in spirit to the phrase-extraction heuristic, which defines a set of 'atomic' terminal phrase-pairs and then extracts every combination of these atomic phase-pairs which is contiguous in the source and target.  1  The generative process is that we draw a complete ITG tree, t ? P 2 (?), as follows: 1. choose the rule type, r ? R, where r ? {mono, swap, emit} 2. for r = mono (a) draw the complete subtree expansion, t = X ? [. . .] ? T M 3. for r = swap (a) draw the complete subtree expansion, t = X ? . . . ? T S 4. for r = emit (a) draw a pair of strings, (e, f ) ? E (b) set t = X ? e/f Note that we split the problem of drawing a tree into two steps: first choosing the top-level rule type and then drawing a rule of that type. This gives us greater control than simply drawing a tree of any type from one distribution, due to our parameterisation of the priors over the model parameters T M , T S and E. To complete the generative story, we need to specify the prior distributions for T M , T S and E. First, we deal with the emission distribution, E which we drawn from a Dirichlet Process prior E ? DP(b E , P 0 ). We restrict the emission rules to generate word pairs rather than phrase pairs. 2 For the base distribution, P 0 , we use a simple uniform distribution over word pairs, P 0 (e, f ) = ? ? ? ? ? ? 2 1 V E V F e = ?, f = ? ?(1 ? ?) 1 V F e = ?, f = ? ?(1 ? ?) 1 V E e = ?, f = ? , where the constant ? denotes the binomial probability of a word being aligned.  3  We use Pitman-Yor Process priors for the T M and T S parameters T M ? PYP(a M , b M , P 1 (?|r = mono)) T S ? PYP(a S , b S , P 1 (?|r = swap)) where P 1 (t 1 , t 2 |r) is a distribution over a pair of trees (the left and right children of a monotone or swap production). P 1 is defined as follows: 1. choose the complete left subtree t 1 ? P 2 , 2. choose the complete right subtree t 2 ? P 2 , 3. set t = X ? [t 1 t 2 ] or t = X ? t 1 t 2 depending on r This generative process is mutually recursive: P 2 makes draws from P 1 and P 1 makes draws from P 2 . The recursion is terminated when the rule type r = emit is drawn. Following standard practice in Bayesian models, we integrate out R, T M , T S and E. This means draws from P 2 (or P 1 ) are no longer iid: for any non-trivial tree, computing its probability under this model is complicated by the fact that the probability of its two subtrees are interdependent. This is best understood in terms of the Chinese Restaurant Franchise (CRF; ), which describes the posterior distribution after integrating out the model parameters. In our case we can consider the process of drawing a tree from P 2 as a customer entering a restaurant and choosing where to sit, from an infinite set of tables. The seating decision is based on the number of other customers at each table, such that popular tables are more likely to be joined than unpopular or empty ones. If the customer chooses an occupied table, the identity of the tree is then set to be the same as for the other customers also seated there. For empty tables the tree must be sampled from the base distribution P 1 . In the standard CRF analogy, this leads to another customer entering the restaurant one step up in the hierarchy, and this process can be chained many times. In our case, however, every new table leads to new customers reentering the original restaurant -these correspond to the left and right child trees of a monotone or swap rule. The recursion terminates when a table is shared, or a new table is labelled with a emit rule. 

 Inference The probability of a tree (i.e., a draw from P 2 ) under the model is P 2 (t) = P (r)P 2 (t|r) (1) where r is the rule type, one of mono, swap or emit. The distribution over types, P (r), is defined as P (r) = n T,? r + b T 1 3 n T,? + b T where n T,? are the counts over rules of types.  4  The second component in (1), P 2 (t|r), is defined separately for each rule type. For r = mono or r = swap rules, it is defined as P 2 (t|r) = n ? t,r ? K ? t,r a r n ? r + b r + K ? r a r + b r n ? r + b r P 1 (t 1 , t 2 |r) , (2) where n ? t,r is the count for tree t in the other training sentences, K ? t,r is the table count for t and n ? r and K ? r are the total count of trees and tables, respectively. Finally, the probability for r = emit is given by P 2 (t|r = emit) = n ? t,E + b E P 0 (e, f ) n ? r + b r , where t = X ? e/f . To complete the derivation we still need to define P 1 , which is formulated as P 1 (t 1 , t 2 ) = P 2 (t 1 )P 2 (t 2 |t 1 ) , where the conditioning of the second recursive call to P 2 reflects that the counts n ? and K ? may be affected by the first draw from P 2 . Although these two draws are assumed iid in the prior, after marginalising out T they are no longer independent. For this reason, evaluating P 2 (t) is computationally expensive, requiring tracking of repeated substructures in descendent sub-trees of t, which may affect other descendants. This results in an asymptotic complexity exponential in the number of nodes in the tree. For this reason we consider trees annotated with binary values denoting their table assignment, namely whether they share a table or are seated alone. Given this, the calculation is greatly simplified, and has linear complexity.  5  We construct an approximating ITG following the technique used for sampling trees from monolingual tree-substitution grammars . To do so we encode the first term from (2) separately from the second term (corresponding to draws from P 1 ). Summing together these two alternate paths -i.e., during inside inferencewe recover P 2 as shown in (2). The full grammar transform for inside inference is shown in Table  1 . The sampling algorithm closely follows the process for sampling derivations from Bayesian PCFGs  (Johnson et al., 2007b) . For each sentencepair, we first decrement the counts associated with its current tree, and then sample a new derivation. This involves first constructing the inside lattice using the productions in Table  1 , and then performing a top-down sampling pass. After sampling each derivation from the approximating grammar, we then convert this into its corresponding ITG tree, which we then score with the full model and accept or reject the sample using the Type X ? M P (r = mono) X ? S P (r = swap) X ? E P (r = emit) Base M ? [XX] K ? M a M +b M n ? M +b M S ? XX K ? S a S +b S n ? S +b S 

 Count For every tree, t, of type r = mono, with n t,M > 0: M ? sig(t) n ? t,M ?K ? t,M ar n ? M +b M sig(t) ? yield(t) 1 For every tree, t, of type r = swap, with n t,S > 0: S ? sig(t) n ? t,S ?K ? t,S a S n ? S +b S sig(t) ? yield(t) 1 Emit For every word pair, e/f in sentence pair, where one of e, f can be ?: E ? e/f P 2 (t) Table  1 : Grammar transformation rules for MAP inside inference. The function sig(t) returns a unique identifier for the complete tree t, and the function yield(t) returns the pair of terminal strings from the yield of t. Metropolis-Hastings algorithm. 6 Accepted samples then replace the old tree (otherwise the old tree is retained) and the model counts are incremented. This process is then repeated for each sentence pair in the corpus in a random order. 

 Experiments Datasets We train our model across three language pairs: Urdu?English (UR-EN), Farsi?English (FA-EN), and Arabic?English (AR-EN). The corpora statistics of these translation tasks are summarised in Table  2 . The UR-EN corpus comes from NIST 2009 translation evaluation.  7  The AR-EN training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). For FA-EN, we use TEP 8 Tehran English-Persian Parallel corpus  (Pilevar and Faili, 2011)  from 1600 movie subtitles. We tokenized this corpus, removed noisy single-word sentences, randomly selected the development and test sets, and used the rest of the corpus as the training set. We discard sentences with length above 30 from the datasets for all experiments. 9 Sampler configuration Samplers are initialised with trees created from GIZA++ alignments constructed using a SCFG factorisation method  (Blunsom et al., 2009a) . This algorithm represents the translation of a sentence as a large SCFG rule, which it then factorises into lower rank SCFG rules, a process akin to rule binarisation commonly used in SCFG decoding. Rules that cannot be reduced to a rank-2 SCFG are simplified by dropping alignment edges until they can be factorised, the net result being an ITG derivation largely respecting the alignments.  10  The blocked sampler was run 1000 iterations for UR-EN, 100 iterations for FA-EN and AR-EN. After each full sampling iteration, we resample all the hyper-parameters using slice-sampling, with the following priors: a ? Beta(1, 1), b ? Gamma(10, 0.1). Figure  1  shows the posterior probability improves with each full sampling iterations. The alignment probability was set to ? = 0.99. The sampling was repeated for 5 independent runs, and we present results where we combine the outputs of these runs. This is a form of Monte Carlo integration which allows us to represent the uncertainty in the posterior, while also representing multiple modes, if present. The time complexity of our inference algorithm is O(n 6 ), which can be prohibitive for large scale machine translation tasks. We reduce the complexity by constraining the inside inference to consider only derivations which are compatible  9  Hence the BLEU scores we get for the baselines may appear lower than what reported in the literature.  10  Using the factorised alignments directly in a translation system resulted in a slight loss in BLEU versus using the unfactorised alignments. Our baseline system uses the latter. showing the posterior probability improving with each full sampling iteration. Different colours denote independent sampling runs. q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q q qq q q q q q q q q q q q q q q q q q q q q 0 5 10 with high confidence alignments from GIZA++. 11 Figure  2  shows the sampling time with respect to the average sentence length, showing that our alignment-constrained sampling algorithm is better than the unconstrained algorithm with empirical complexity of n 4 . However, the time complexity is still high, so we set the maximum sentence length to 30 to keep our experiments practicable. Presumably other means of inference may be more efficient, such as Gibbs sampling  (Levenberg et al., 2012)  or auxiliary variable sampling ; we leave these extensions to future work. Baselines. Following  (Levenberg et al., 2012; Neubig et al., 2011) , we evaluate our model by using its output word alignments to construct a phrase table. As a baseline, we train a phrasebased model using the moses toolkit 12 based on the word alignments obtained using GIZA++ in both directions and symmetrized using the growdiag-final-and heuristic 13  (Koehn et al., 2003) . This alignment is used as input to the rule factorisation algorithm, producing the ITG trees with which we initialise our sampler. To put our results in the context of the previous work, we also compare against pialign  (Neubig et al., 2011) , an ITG algorithm using a Pitman-Yor process prior, as described in Section 2.  14  In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as  (Blunsom et al., 2009a) , and for FA-EN and UR-EN the English data are the target sides of the bilingual training data. We use minimum error rate training  (Och, 2003)   Table  3 : The BLEU scores for the translation tasks of three language pairs. The individual column show the average and 95% confidence intervals for 5 independent runs, whereas the combination column show the results for combining the phrase tables of all these runs. The baselines are GIZA++ alignments and those generated by the pialign  (Neubig et al., 2011)  bold: the best result.  

 Results Table  3  shows the BLEU scores for the three translation tasks UR/AR/FA?EN based on our method against the baselines. For our models, we report the average BLEU score of the 5 independent runs as well as that of the aggregate phrase table generated by these 5 independent runs. There are a number of interesting observations in Table  3 . Firstly, combining the phrase tables from independent runs results in increased BLEU scores, possibly due to the representation of uncertainty in the outputs, and the representation of different modes captured by the individual models. We believe this type of Monte Carlo model averaging should be considered in general when sampling techniques are employed for grammatical inference, e.g. in parsing and translation. Secondly, our approach consistently improves over the Giza++ baseline often by a large margin, whereas pialign under-performs the GIZA++ baseline in many cases. Thirdly, our model consistently outperforms pialign (except in AR-EN MT08 which is very close). This highlights the modeling and inference differences between our method and the pialign. 

 Analysis In this section, we present some insights about the learned grammar and the model hyper-parameters. Firstly, we start by presenting various statistics about different learned grammars. Figure  3  shows the fraction of rules with a given frequency for each of the three rule types. The three types of rule exhibit differing amounts of high versus low frequency rules, and all roughly follow power laws. As expected, there is a higher tendency to reuse high-frequency emissions (or single-word translation) compared to other rule types, which are the basic building blocks to compose larger rules (or phrases). Table  4  lists the high frequency monotone and swap rules in the learned grammar. We observe the high frequency swap rules capture reordering in verb clusters, preposition-noun inversions and adjective-noun reordering. Similar patterns are seen in the monotone rules, along with some common canned phrases. Note that "in Iraq" appears twice, once as an inversion in UR-EN and another time in monotone order for AR-EN. Secondly, we analyse the values learned for the model hyper-parameters; Figure  4 .(a) shows the posterior distribution over the hyper-parameter values. There is very little spread in the inferred values, suggesting the sampling chains may have converged. Furthermore, there is a large difference between the learned hyper-parameters for the monotone rules versus the swap rules. For the Pitman-Yor Process prior, the values of the hyper- } $ $ $ $ } $ $ $ $ $ } $ } } $ Table  5 : Good phrase pairs in the top-100 high frequency phrase pairs specific to the phrase tables coming from our method vs that of pialign for FA-EN and AR-EN translation tasks. parameters affects the rate at which the number of types grows compared to the number of tokens. Specifically, as the discount a or the concentration b parameters increases we expect for a relative increase in the number of types. If the number of observed monotone and swap rules were equal, then there would be a higher chance in reusing the monotone rules. However, the number of observed monotone and swap rules are not equal, as plotted in Figure  4 .(b). Similar results were observed for the other language pairs (figures omitted for space reasons). Thirdly, we performed a manual evaluation for the quality of the phrase-pairs learned exclusively by our method vs pialign. For each method, we considered the top-100 high frequency phrasepairs which are specific to that method. Then we asked a bilingual human expert to identify reasonably well phrase-pairs among these top-100 phrase-pairs. The results are summarized in Table 5, and show that we learn roughly twice as many reasonably good phrase-pairs for AR-EN and FA-EN compared to pialign. 

 Conclusions We have presented a novel method for learning a phrase-based model of translation directly from parallel data which we have framed as learning an inverse transduction grammar (ITG) using a recursive Bayesian prior. This has led to a model which learns translations of entire sentences, while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. We have presented a Metropolis-Hastings sampling algorithm for blocked inference in our non-parametric ITG. Our experiments on Urdu-English, Arabic-English, and Farsi-English translation tasks all demonstrate improvements over competitive baseline systems.  Figure 1 : 1 Figure 1: Training progress on the UR-EN corpus, showing the posterior probability improving with each full sampling iteration. Different colours denote independent sampling runs. 

 Figure 2 : 2 Figure2: The runtime cost of bottom-up inside inference and top-down sampling as a function of sentence length (UR-EN), with time shown on a logarithmic scale. Full ITG inference is shown with red circles, and restricted inference using the intersection constraints with blue triangles. The average time complexity for the latter is roughly O(l 4 ), as plotted in green t = 2 ? 10 ?7 l 4 . 

 Figure 3 : 3 Figure 3: Fraction of rules with a given frequency, using a single sample grammar (UR-EN). 

 Figure 4 : 4 Figure 4: (a) Posterior over the hyper-parameters, a M , a S , b M , b S , b E , b T , measured for UR-EN using samples 400-500 for 3 independent sampling chains, and the intersection constraints. (b) Posterior over the number of monotone and swap rules in the resultant grammars. The distribution for emission rules was also peaked about 147k rules. 

 Table 2 : 2 Corpora statistics showing numbers of parallel sentences and source and target words for the training sets. , which consists of conversational/informal text extracted 

			 Our technique considers the subset of phrase-pairs which are consistent with the ITG tree. 

			 Note that we could allow phrases here, but given the model can already reason over phrases by way of its hierarchical formulation, this is an unnecessary complication.3  We also experimented with using word translation probabilities from IBM model 1, based on the prior used by Levenberg et al. (2012) , however we found little empirical difference compared with this simpler uniform model. 

			 The conditioning on event and table counts, n ? , K ? is omitted for clarity. 

			 To support this computation, we track explicit table assignments for every training tree and their component subtrees. We also sample trees labelled with seating indicator variables. 

			 The full model differs from the approximating grammar in that it accounts for inter-dependencies between subtrees by recursively tracking the changes in the customer and table counts while scoring the tree. Around 98% of samples were accepted in our experiments.7 http://www.itl.nist.gov/iad/mig/tests/mt/2009 8 http://ece.ut.ac.ir/NLP/resources.htm 

			 These are taken from the final model 4 word alignments, using the intersection of the source-target and target-source models. These alignments are very high precision (but have low recall), and therefore are unlikely to harm the model.12 http://www.statmt.org/moses13  We use the default parameter settings in both moses and GIZA++.14 http://www.phontron.com/pialign
