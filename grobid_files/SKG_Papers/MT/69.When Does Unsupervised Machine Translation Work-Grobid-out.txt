title
When Does Unsupervised Machine Translation Work?

abstract
Despite the reported success of unsupervised machine translation, the field has yet to examine the conditions under which the methods succeed and fail. We conduct an extensive empirical evaluation using dissimilar language pairs, dissimilar domains, and diverse datasets. We find that performance rapidly deteriorates when source and target corpora are from different domains, and that stochasticity during embedding training can dramatically affect downstream results. We advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. Towards this goal, we release our preprocessed dataset to stress-test systems under multiple data conditions.

Introduction Machine translation (MT) has progressed rapidly since the advent of neural machine translation (NMT)  (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015;  and is better than ever for languages for which ample high-quality bitext exists. Conversely, MT for lowresource languages remains a great challenge due to a dearth of parallel training corpora and poor quality bitext from esoteric domains. To address this, several authors have proposed unsupervised MT techniques, which rely only on monolingual text for training (e.g.,  Ravi and Knight, 2011; Yang et al., 2018; Artetxe et al., 2018c; Hoshen and Wolf, 2018; Lample et al., 2018a,b; Artetxe et al., 2018b . Recent unsupervised MT results appear promising, but they primarily report results for the highresource languages for which traditional MT already works well. The limits of these methods are so far under-explored. For unsupervised MT to be a viable path for low-resource machine translation, the field must determine (1) if it works outside highly-controlled environments, and (2) how to effectively evaluate newly-proposed training paradigms to pursue those which are promising for real-world low-resource scenarios. Unsupervised MT methods must work (1) on different scripts and between dissimilar languages, (2) with imperfect domain alignment between source and target corpora, (3) with a domain mismatch between training data and the test set, and (4) on the low-quality data of real low-resource languages. These factors reflect the real-life challenges of lowresource translation. Our main contribution is an extensive analysis of unsupervised MT with regards to factors (1)-(3) above.  1  We find that (a) translation performance rapidly deteriorates when source and target corpora are from different domains, (b) stochasticity during word embedding training can dramatically affect downstream bilingual lexicon induction (BLI) and translation performance, and (c) like in the bilingual lexicon induction literature, unsupervised MT performance declines when source and target languages are dissimilar. While (4) is not the focus of this paper, we do observe very low performance on an authentic low-resource language pair, corroborating previous studies  (Guzm?n et al., 2019) . Finally, as there are no standard evaluation protocols to ensure that unsupervised MT systems are robust to the types of data anomalies ubiquitous in low-resource translation settings, we advocate for extensive empirical evaluation of unsupervised MT systems to highlight failure points and encourage continued research on the most promising paradigms. We first discuss related work in Section 2, followed by a detailed overview of the unsupervised MT architecture in Section 3. In Section 4, we discuss our research questions, followed by our evaluation methodology and datasets in Sections 5 and 6. Section 7 presents our findings, and Section 8 discusses the results. We conclude in Section 9. 

 Related Work Bilingual Lexicon Induction Unsupervised MT methods can be thought of as an end-to-end extension of work inducing bilingual lexicons from monolingual corpora. Bilingual lexicon induction (BLI) using non-parallel data has a rich history, beginning with corpus statistic and decipherment methods (e.g.,  Rapp, 1995; Fung, 1995; Knight, 2000, 2002; Haghighi et al., 2008) , continuing to modern neural methods to create crosslingual word embeddings (e.g.  Mikolov et al., 2013a; , see Ruder et al. (2019  for a survey) which form a critical component of stateof-the-art unsupervised MT systems.  S?gaard et al. (2018)  determine that monolingual embedding spaces of similar languages are not typically isomorphic as was previously believed, and that bilingual dictionary induction "depends heavily on... the language pair, the comparability of the monolingual corpora, and the parameters of the word embedding algorithms."  argue that unsupervised approaches are unsuccessful with dissimilar languages and domains, and that unsupervised performance has been overly lauded because the conditions under which they were compared with supervised baselines were inequitable. 

 Evaluation of Embedding Spaces While a modest body of literature has examined the quality of cross-lingual word embeddings (CLEs) by measuring performance on BLI,  evaluate on downstream natural language tasks, underlining the importance of fullsystem evaluation. The authors conclude that "the quality of CLE models is largely task-dependent and that overfitting the models to the BLI task can result in deteriorated performance in downstream tasks." Similarly,  Doval et al. (2019)  investigate cross-lingual natural language inference. Evaluation of Unsupervised  MT Liu et al. (2020)  helpfully re-define unsupervised machine translation into three distinct categories: (1) no bitext whatsoever, (2) the target language pair is linked through bitext via a pivot language, and (3) no linkage through a pivot language, but bitexts exists for *some* language and the target language. The authors analyze their multilingual pretraining method with respect to other similar training paradigms  (Conneau and Lample, 2019; Song et al., 2019)  and evaluate unsupervised MT performance when using backtranslation (Definition 1) or language transfer after finetuning on related bitext (Definition 3). In unsupervised MT with no bitext, Lample et al. (2018b) ablate their PBSMT system, finding that initial phrase table quality is critical and that performance suffers when the language model is trained with less data. They tweak their NMT embedding initialization method, such as using separatelytrained BPE instead of joint, and word embeddings instead of BPE token embeddings. They report the results of dropping part of their loss function and making minor changes to the NMT architecture on downstream BLEU score. Concurrently to our work,  Kim et al. (2020)  arrived at similar conclusions to us using a different autoencoder/duallearning unsupervised MT approach based on crosslingual language model pretraining  (Conneau and Lample, 2019) ; this complements our experiments and corroborates our results. 

 Background: Unsupervised MT Our experiments employ the models of  Artetxe et al. (2018b   Another approach to unsupervised MT involves pretraining a bilingual or multilingual model on monolingual text on a general task before finetuning on translation. Such methods include crosslingual language model pretraining  (Conneau and Lample, 2019) , masked sequence-to-sequence pretraining  (Song et al., 2019) , and multilingual denoising pretraining  (Liu et al., 2020) , and have shown promise. For instance,  Liu et al. (2020)  record the first good results on the low-resource Sinhala-English and Nepali-English pairs. While pretraining and multilingual methods are not the subject of this work, they warrant future evaluation.  Training begins with two monolingual corpora which are not necessarily related in any way (i.e. they are not assumed to be parallel nor comparable text). First, word embeddings are trained independently for each corpus, resulting in a source and a target embedding space. Specifically, after preprocessing,  Artetxe et al. (2018b)  train two statistical language models using KenLM  (Heafield, 2011) , one for the source language and one for the target. They use phrase2vec 4  (Artetxe et al., 2018b) , an extension of  Mikolov et al. (2013b) 's skip-gram model, 5 to generate phrase embeddings for 200,000 unigrams, 400,000 bigrams, and 400,000 trigrams. Next, source and target word embeddings are aligned into a common cross-lingual embedding space. They run VecMap 6  (Artetxe et al., 2018a)  which calculates a linear mapping of one space to another based on the intuition that phrases with similar meaning should have similar neighbors regardless of language. Given a matrix of source word embeddings X and target word embeddings Z which have been length-normalized, meancentered, then length-normalized again, VecMap calculates M x = XX T and M z = ZZ T . Each cell M x ij and M z ij is the cosine similarity between words X i and X j , and Z i and Z j , respectively. M x and M z are symmetric, and if the monolingual vector spaces were fully isometric, M x and M z would be identical besides rows and columns being permuted. Each row of M x and M z is a similarity distribution. To exploit this, each row of ? M x and 4 https://github.com/artetxem/ phrase2vec 5 https://github.com/tmikolov/word2vec 6 https://github.com/artetxem/vecmap ? M z is sorted (they find that using the square root works better empirically), and length-normalized, mean-centered, and length-normalized again. For each row i in sorted( ? M x ), they find the row j of sorted( ? M z ) that is its nearest neighbor, and assign X i = Z j in the initial translation dictionary D. A cell D ij = 1 if words X i , and Z j are translations of one another, and 0 otherwise. Next, there is an iterative process of calculating the optimal linear mappings and extracting an updated dictionary. For calculating the mapping, the goal is to find the linear transformations W x and W z which maximize the cosine similarity of the words that are translations of one another as defined by the dictionary D, over the entire dictionary: arg max Wx,Wz i j (D ij )((X i W x ) ? (Z j W z )) From there, they calculate M = XW x W T z Z T , whereby each cell in M is the cosine similarity of word X i and Z j after their transformations with W x and W z . To avoid poor local optima, they stochastically zero-out some cells of M with probability p = 0.9, decreasing over time. The final score for each potential translation candidate is calculated using Cross-domain Similarity Local Scaling (CSLS)  to mitigate the hubness problem. CSLS utilizes cosine similarity, which is taken from M . For each pair of words X i and Z j , the new dictionary cell D ij = 1 if the CSLS score between X i and Z j is the highest over all other words in Z, and D ij = 0 otherwise. The dictionary is created in both directions, and concatenated. Readers are directed to  Artetxe et al. (2018a)  for further details. The next step extracts an initial phrase-table for use in a SMT system. They use the softmax over the cosine similarity of the 100 nearest-neighbors of each source phrase embedding as the phrase translation probabilities. This is done in both directions: (f |e) = e (cos(e,f )/? ) f e (cos(e,f )/? ) For the target embedding with the highest cosine similarity, the phrases are aligned, and unigram translation probabilities are multiplied to become the lexical weighting. Combining the preliminary phrase table with a distortion penalty and language model produces the initial unsupervised phrase-based SMT system  (Koehn et al., 2007) . The SMT model weights are tuned using a variant of MERT  (Och, 2003)  designed for unsupervised scenarios, which uses 10,000 parallel sentences generated via backtranslation  (Sennrich et al., 2016a) . The SMT model then undergoes three rounds of iterative backtranslation.  extend their 2018 work by adding a critical "NMT hybridization" final step, which achieves significant gains over SMT alone. 7 An NMT system is trained using backtranslated output from SMT for one epoch. On the next epoch, a small number of sentences are backtranslated with the newly-trained NMT system and concatenated with a slightly smaller fraction of SMT-generated bitext. The procedure continues for 30 epochs, gradually increasing the percentage of synthetic training data created by the NMT system until all of the training data is NMT-generated. The NMT system is trained for an additional 30 epochs of iterative backtranslation using data generated fully by the NMT system of the previous epoch. The test set is translated with beam search using an ensemble of models saved at every tenth epoch (six total), resulting in BLEU scores of 33.2 and 26.4 (SacreBLEU  (Post, 2018) ) on newstest2014 for French-English and German-English, respectively. We run  Artetxe et al. (2018b 's implementation for our experiments. Specifically, neural models are Transformer-big  (Vaswani et al., 2017)  trained with fairseq  on one NVIDIA GeForce GTX 1080Ti GPU. Models use shared embeddings, the Adam optimizer with ? 1 = 0.9, ? 2 = 0.98  (Kingma and Ba, 2015) , label smoothing, initial learning rate of 1e-07 warming up for 4000 steps to 5e-04 before decaying, and dropout  (Srivastava et al., 2014 ) probability of 0.3. We set optimizer delay to 4 to simulate 4 GPUs. To elucidate the performance gap due to the unsupervised architecture, we build a standard supervised NMT system using the same neural architecture described above. We train until performance on the development set ceases to improve for 10 epochs. To parallel the unsupervised setup, we translate the test set using an ensemble of 6 models; We perform ensemble selection by performance on a validation set, selecting the best-performing checkpoint along with 5 previous checkpoints. 

 Research Questions Existing unsupervised translation methods work well on languages which are similar to each other, use the same Roman script, and have an ample amount of monolingual news data available (which matches the test set domain). Questions remain as to whether unsupervised methods will be useful on authentic low-resource settings where few or none of the aforementioned properties hold. Namely, does unsupervised MT work with: ? dissimilar languages? ? dissimilar source and target domains? ? diverse datasets? ? authentic low-resource language pairs? Such questions reflect the reality of authentic low-resource translation, and are those which must be adequately resolved for unsupervised MT to be a viable alternative to traditional translation methods for the most difficult language pairs. 

 Evaluation of Unsupervised MT We perform an extensive empirical evaluation of unsupervised MT. Our evaluation protocol stresstests an unsupervised MT system under varying conditions to reveal its points of strength and failure. Systems should be judged on how well they perform: (1) on dissimilar languages, (2) on increasingly divergent domains between source and target corpora, (3) on diverse datasets, and (4) on authentic low-resource language pairs where data quality is typically low. Namely, we: 1. Choose 2 language pairs, at least one of which where the source and target languages utilize different scripts. 2. Choose 3 datasets of different domains, at least one of which is parallel bitext. 3. Perform at least one experiment for each language pair under each of the following data conditions: ? Originally parallel ? Not originally parallel ? Different domain for source and target. 4. Choose 2 true low-resource language pairs. 5. Judge the system based on performance in all tested scenarios. The data conditions above are designed measure how well a system performs in regards to the research questions of Section 4. Namely, success on a variety of languages with different scripts and linguistic structure indicates robustness to dissimilar languages; success on multiple datasets of different domains indicates that the system is not specifically designed for one domain at the expense of others, and performs well even when training and test data do not match perfectly; Step #3 evaluates performance on increasingly divergent domains between source and target data; and Step #4 is the true test-whether the system succeeds on authentic low-resource language pairs. 

 Datasets Training datasets used in our reinvestigation of the unsupervised MT system presented in  are shown in Table  1 . We focus on Russian-English (Ru-En) and French-English (Fr-En) tasks and include as reference Sinhala-English (Si-En) and Nepali-English (Ne-En) as well. Following Section 5, we evaluate the same system under various ablated data setups: ? The "Supervised" condition is the standard MT training setup which uses parallel bitext. ? In the "Parallel" condition, an unsupervised MT system is trained on a corpus that was originally parallel (i.e. UN corpus), now being treated as two separate monolingual corpora. ? In contrast, the "Disjoint" setting splits data from a parallel corpus into two disjoint halves, using the first half of the source-side corpus and the second half of the target-side corpus. ? In the "Different Domain" (Diff. Dom.) setting, source and target monolingual corpora come from different domains. This is a realistic setting in low-resource scenarios, and is expected to be much more difficult than the "Disjoint" setting. ? "News crawl" (News) and "Common Crawl" (CC) settings determine whether the system can flexibly handle diverse datasets. Specifics of the datasets used are described in subsequent subsections. is the condition most similar to  (Artetxe et al., 2018b . Src (M) and Trg (M) columns are the token counts, in millions. "Supervised" count is in BPE tokens. All others are token counts for SMT (pre-BPE). the subsections below are before preprocessing, whereas Table  1  reflects the data remaining after the preprocessing procedure of  Artetxe et al. (2018b) . We will release the preprocessed data splits for others to compare their results with ours. 

 United Nations The United Nations Parallel Corpus (UN)  (Ziemski et al., 2016)   We additionally extract the first 100 million French and Russian tokens for CC experiments. 

 Preprocessing Training data is preprocessed separately for each unsupervised experiment as part of Artetxe et al. (2018b)'s training pipeline. Data is deduplicated, and tokenized and truecased using scripts from Moses  (Koehn et al., 2007) . Sentences with less than 3 tokens or more than 80 tokens are discarded, and sentences are shuffled. Ten thousand sentences are removed to form a development set. To begin the NMT phase, a joint BPE  (Sennrich et al., 2016b)  vocabulary of 32000 tokens is learned. Source-and target-side corpora are backtranslated using the final model from the SMT phase, and all data then has BPE applied.  8  For supervised experiments, training data is tokenized and truecased, and then a joint BPE (Sennrich et al., 2016b) vocabulary of 32000 tokens is learned. After applying BPE, the data is cleaned using Moses' clean-corpus-n.perl, discarding sentences under 3 and greater than 80 tokens. 

 Vocabulary Overlap of Training Sets A vocabulary of unigrams was collected for each target-side (English) corpus, which includes tokens that appear at least 10 times, for a maximum of 200,000 unigrams. Of approximately 144,000 such unique tokens between UN-A and UN-B from the Fr-En UN corpus, the corpora share 54.1%. These corpora are used in the Disjoint condition. The respective vocabulary overlap for UN-A and CC from the Diff. Dom condition for Fr-En is 25.7%. For UN-B vs. CC for Fr-En, they share 25.3%. Statistics are analogous for Ru-En. 

 Test and Validation Sets Ru-En models are tested on newstest2019. Fr-En models are tested on newstest2014. Supervised models use newstest2018 (Ru-En) or new-stest2013 (Fr-En) for validation. For Si-En and Ne-En, we use the Wikipedia dev and devtest sets from  Guzm?n et al. (2019) . 9 For supervised models, we select the ensemble with best performance on newstest2017 (Ru-En) or newstest2012 (Fr-En). 7 Reinvestigation of Artetxe et. al. Next, we set up a series of experiments to assess the questions posed in Section 4. Results are presented in Tables  3 and 4 . 

 Unsupervised Quality Loss The Supervised ("Sup.") column of Table  3  shows performance of a standard Transformer-big architecture on parallel bitext for Ru-En and Fr-En. Assuming that supervised translation will always outperform unsupervised, these scores represent a ceiling to quantify how much potential quality is lost using an unsupervised architecture. 

 Sup. Parallel Disjoint Diff. Dom. Corpus A / A A / A A / B A / CC* Ru-En 26.9 23.7 (-3.2) 21.2 (-5.7) 0.7 (-26.2) Fr-En 29.9 27.6 (-2.3) 27.0 (-2.9) 3.9 (-26.0) Table  3 : Unsupervised MT performance on a single run using the United Nations (UN) dataset. "Diff. Dom." uses UN data as source and Common Crawl (*) as target. "Sup." is supervised with UN parallel data. A / A refers to UN training dataset A used on the source and target sides, for example. Scored using Sacre-BLEU  (Post, 2018)  on newstest2019 (Ru-En) and new-stest2014 (Fr-En). . The supervised models and those in the Parallel column use the same datasets 10 and can therefore be directly compared. We observe a BLEU score drop of ?3.2 for Ru-En versus a drop of ?2.3 for Fr-En when using the unsupervised architecture. This minor quality loss represents a strong result for unsupervised MT; however, the question is whether the results will remain strong as we gradually make the monolingual corpora less similar. 

 Investigating Our Research Questions Does unsupervised machine translation work for: (1) Dissimilar language pairs? We conduct experiments in French and Russian into English. Whereas French and English share the same Roman script and common linguistic origin, Russian is a Slavic language that uses the Cyrillic script. The results presented in Tables  3 and 4  indicate that unsupervised MT is more difficult when writing script and language family differs. Across the board, we observe that the ?BLEU between supervised and unsupervised performance is wider for Ru-En than for Fr-En, particularly for News and Common Crawl datasets. For instance, whereas Fr-En loses 2.9 BLEU in the Supervised versus Disjoint setups (which use comparable data), Ru-En loses 5.7 BLEU. While we acknowledge that in general one should not compare BLEU scores across language pairs or datasets, this gap suggests that unsupervised MT may behave differently for different language pairs. (2) Dissimilar domains? We investigate the effects of domain similarity between source and target training corpora. For each language, we observe the difference in perfor-10 Differences in token count are due to the different preprocessing detailed in Section 6.4. mance on Table  3  of the Parallel, Disjoint, and Diff. Dom. columns. Because training data in the Parallel condition was originally parallel, these experiments have the highest possible domain match between source and target data. Since Disjoint data was extracted from the same corpus but was not parallel, source and target can be thought of as having very slightly different domains. We observe a minor performance drop between Parallel and Disjoint experiments, which is more pronounced for Ru-En. Examining the Diff. Dom. column, however, the performance contrast is stark. While both language pairs obtain respectable BLEU scores in the 20s when domains match in Parallel and Disjoint conditions, performance drops sharply when training set domains are mismatched-scoring 3.9 BLEU for Fr-En and 0.7 for Ru-En. (A subsequent run of Fr-En scored 17.4, addressed in Section 7.4). The fault is not with either side of the training corpus alone-Parallel/Disjoint experiments from Table  3  which use UN data alone and CC experiments in Table  4  which use Common Crawl data alone perform acceptably-it is when the two datasets are paired as source-target in Diff. Dom. conditions that performance rapidly deteriorates. (3) Diverse datasets?   (Post, 2018)  on newstest2019 (Ru-En), newstest2014 (Fr-En), and the FLoRes Wikipedia evaluation sets (Si-En, Ne-En)  (Guzm?n et al., 2019) . UN Table  4  shows the results of experiments using three different training datasets. News crawl matches the domain of the test set exactly. UN data has a moderate domain match with the test set, and CC matches the least. Not unexpectedly, most experiments where training and test domain match perform better than when there is a domain mismatch. The exception is the News experiment for Ru-En, where the model performs considerably worse than the UN condition despite having a stronger domain match. Notably, News has approx-imately 2-3x less data than UN for each language pair. We suspect that for Fr-En, the relative ease of unsupervised translation for this language pair allowed the strong domain match with the test set to outweigh the lower amount of data. On the other hand, the relative difficulty of unsupervised MT in Ru-En made the system suffer too greatly in the lower-resource condition, to where it could not compensate with domain match. (4) A true low-resource pair? Facebook recently released test sets for Sinhala-English and Nepali-English, true low-resource language pairs which not only lack bitext, but monolingual data is of poor quality. These languages do not share a script or language family with English, and the data is out-of-domain with the English data. This reflects a real-world low-resource scenario where we would hope to benefit from unsupervised MT. We observe extremely poor results in Table  4 , with Si-En achieving a BLEU score of 0.2, and 0.4 Ne-En.  Guzm?n et al. (2019)  achieve similarly poor results for these language pairs without using supplemental data from a related language. 

 BLEU During Training Figure  2  shows translation performance for the experiments in Tables  3 and 4  at various steps during the unsupervised machine translation pipeline. Most SMT models improve performance slightly as a result of unsupervised MERT tuning, and more substantially after three rounds of iterative backtranslation. Substantial improvement occurs as a result of NMT training for all models except the degenerate Diff. Dom conditions.  

 Training Stability One challenge with unsupervised methods is training stability: stochasticity during training can give  substantially different results due to the iterative bootstrap nature of the training process. In their analysis of unsupervised methods for generating CLEs,  note considerable instability in performance on BLI. Defining failure as having a mean average precision (MAP) of <0.05 on all training runs, Iterative Closest Point  (Hoshen and Wolf, 2018)  fails for ?21% of language pairs, Gromov-Wasserstein Alignment (Alvarez-Melis and Jaakkola, 2018) for ?46%, and MUSE  for ?54%. VecMap  (Artetxe et al., 2018a)  succeeds for all language pairs, leading  Glava? et al.  to deem it the most robust.  Artetxe et al. (2018a)  demonstrate their robustness over other methods in their work. When counting successful runs as achieving >5.0% accuracy, VecMap is successful 10/10 times for three language pairs. Hartmann et al. (  2019 ) also investigate instability in vector space alignment methods. After training phrase embeddings for each experiment, we run VecMap on the generated embedding spaces ten additional times and indeed find little fluctuation in BLI between runs. When rerunning the full pipeline for each experiment, however, we observe considerable instability in two experiments which dramatically affects downstream results. We build a gold-standard bilingual dictionary of 2000 word pairs from Wikipedia data  (Wo?k and Marasek, 2014)  available publicly on OPUS (Tiedemann, 2012), and run the first four steps of the unsupervised training procedure additional times for each experiment. Table  5  contains the summary results of 10-11 runs of each experiment. Tables  3 and 4  present the results of the single first run of each experiment. Whereas the majority have consistent accuracy on bilingual lexicon between runs as seen in   5 ). The unsupervised pipeline begins with preprocessing (deterministic, except shuffling and random selection of development set), language model training with KenLM  (Heafield, 2011)  (deterministic), followed by phrase embedding training using phrase2vec (non-deterministic), and then embedding space mapping with VecMap (nondeterministic). Because performance on reruns of VecMap alone was stable while holding the rest of the system constant, we must conclude that the dramatic instability is caused by either a poor embedding initialization from phrase2vec/word2vec, or VecMap's inability to handle certain monolingual vector space configurations. Apparently, the initial formation of monolingual vector spaces dramatically affects VecMap's ability to converge to a good solution, which in turn results in highly variable downstream translation performance. To observe the relationship between BLI accuracy and downstream BLEU score, we direct the reader to Figure  3 , where BLI accuracy after the VecMap phase of experiments from Tables  3 and 4  are displayed in relation to the final BLEU score. 

 Discussion Except in the Diff. Dom. condition, unsupervised MT performance for Fr-En is impressive and suggests that sentence alignment may not be required for successful MT under ideal conditions. Ru-En results are also impressive, but show that unsupervised MT still struggles when language pairs are dissimilar, especially when data amount is reduced. The gap between Disjoint and Diff. Dom. con- ditions is perhaps the most striking result in our experiments. It suggests that one cannot naively collect monolingual corpora without considering their relative domain similarity; this may be a challenge in low-resource conditions, where there is less flexibility with data sources. Vuli? et al. (  2019 ) make a similar claim about unsupervised CLEs, stating "UNSUPERVISED methods are able to yield a good solution only when there is no domain mismatch and for the pair with two most similar languages (English-Spanish), again questioning their robustness and portability to truly low-resource and more challenging setups". Furthermore, the extremely poor results of Ne-En and Si-En reflect the reality of low-resource translation; the compound negative effects of language dissimilarity, domain mismatch between monolingual corpora, domain mismatch with the test set, and low amounts of low-quality data. It is the "worst of all worlds"but reflects how current models might perform on the use cases for which they are needed. These challenges highlight the importance of evaluating unsupervised MT under varying realistic data conditions. Our evaluation is a step towards this goal, and identifies multiple areas for improvement. A critical step in state-of-the-art unsupervised MT is methods for creating CLEs. Several authors have pointed out that "mapping" methods like VecMap assume that monolingual vector spaces are structurally similar, but that this "approximate isomorphism assumption" is increasingly tenuous as languages and domains diverge  (S?gaard et al., 2018; Ormazabal et al., 2019; Patra et al., 2019) .  Patra et al. (2019)  find this for Fr-En and Ru-En specifically, the languages examined in this work.  Nakashole and Flauger (2018)  argue that while linearity may hold within local "neighborhoods" of the vector space, the global mapping is non-linear.  S?gaard et al. (2018)  use their eigenvector similarity metric to show a strong correlation between vector space similarity and BLI performance. Analysis of the CLEs from our experiments demonstrate a relationship between BLI performance and downstream BLEU on the translation task. Coupled with our empirical evidence, the works cited in this section suggest that nonisometric vector spaces lead to poor quality translation. Factors observed in our experiments that lead to lower quality translation can be attributable to a "weak isomorphism" between the monolingual vector spaces. Dissimilar languages means increasingly different distributional characteristics of words. Data from different domains naturally have different word frequencies and distributional characteristics, which become more pronounced as domains diverge. Because mapping methods rely on structural similarity of vector spaces, experiments using either UN or CC data alone had acceptable downstream performance, where as combining the datasets as source and target resulted in extremely poor translation. We observe the critical effect of word embedding initialization on BLI performance and downstream BLEU, suggesting that stochasticity during word embedding creation can cause resulting vector spaces to be more or less isomorphic. Finally, more data can give a more accurate distribution of words in comparison with the true distribution in the language, leading to a more realistic monolingual vector space. With less data, word embeddings are dependent on the smaller training sample, which may not match the test set or reflect true distributional properties of the language. Combining all of these negative factors likely leads to highly nonisomorphic monolingual embedding spaces, as demonstrated by the very poor Si-En and Ne-En results. 

 Conclusion & Future Work Progress in unsupervised MT has been impressive, achieving performance near its supervised counterparts under some scenarios. That said, evaluating current approaches under broader settings and datasets reveals that unsupervised MT struggles in realistic low-resource scenarios. As stated by  Lample et al. (2018b) , "It's an open question whether there are more effective instantiations of these principles [underlying recent successes in fully unsupervised MT] or other principles altogether". In this work, we find that there is room for improvement to become robust to (1) dissimilar languages pairs, (2) dissimilar domains, (3) diverse datasets, and (4) the low-quality data of true low-resource languages-factors ubiquitous in low-resource language pairs for which unsupervised MT is intended. We find that (a) performance rapidly declines when source and target corpora are from different domains, and (b) stochasticity during word embedding training can dramatically affect downstream translation results. The latter is a yet unexplored research area. Future work should also evaluate pretraining methods in bilingual and multilingual training contexts. Finally, we argue for extensive evaluation of unsupervised MT systems under varying data conditions to assess failure cases and encourage pursuit of promising paradigms. Doing so is a step towards solving the real-world problems of low-resource machine translation. as representative of state-ofthe-art for the class of unsupervised MT methods that bootstrap from cross-lingual word embeddings. Recent work such as Lample et al. (2018b) is based on similar concepts. For our purposes, unsupervised MT follows Liu et al. (2020)'s Definition (1) from Section 2, where no bitext exists. 
