title
Findings of the 2020 Conference on Machine Translation (WMT20)

abstract
This paper presents the results of the news translation task and the similar language translation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages.

Introduction The Fifth Conference on Machine Translation (WMT20) 1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences  (Koehn and Monz, 2006; , 2008 , 2010 , 2011 , 2012 Bojar et al., 2013 Bojar et al., , 2014 Bojar et al., , 2015 Bojar et al., , 2017 Barrault et al., 2019) . This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: ? automatic post-editing  (Chatterjee et al., 2020)  ? biomedical translation  (Bawden et al., 2020b)  ? chat translation  (Farajian et al., 2020)  ? lifelong learning  (Barrault et al., 2020)  1 http://www.statmt.org/wmt20/ ? metrics  (Mathur et al., 2020)  ? parallel corpus filtering  ? quality estimation  (Specia et al., 2020a)  ? robustness  (Specia et al., 2020b)  ? unsupervised and very low-resource translation  (Fraser, 2020)  In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data ("constrained" condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. The translation tasks covered a range of language families, and included both low-resource and high-resource pairs. System outputs for each task were evaluated both automatically and manually, but we only include the manual evaluation here. The human evaluation (Section 3) involves asking human judges to score sentences output by anonymized systems. We obtained large numbers of assessments from researchers who contributed evaluations proportional to the number of tasks they entered. In addition, we used Mechanical Turk to collect further evaluations, as well as a pool of linguists. This year, the official manual evaluation metric is again based on judgments of adequacy on a 100-point scale, a method (known as "direct assessment") that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives of WMT are to evaluate the state of the art in machine translation, to disseminate common test sets and public training data with published performance numbers, and to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.  2  We hope these datasets serve as a valuable resource for research into datadriven machine translation, automatic evaluation, or prediction of translation quality. News translations are also available for interactive visualization and comparison of differences between systems at http://wmt.ufal.cz/ using MT-ComparEval  (Sudarikov et al., 2016) . In order to gain further insight into the performance of individual MT systems, we organized a call for dedicated "test suites", each focusing on some particular aspect of translation quality. A brief overview of the test suites is provided in Section 4. Following the success of the first Similar Language Translation (SLT) task at WMT 2019 and the interest of the community in this topic  (Costajuss? et al., 2018; , we organize a second iteration of the SLT task at WMT 2020. The goal of the shared task is to evaluate the performance of state-of-the-art MT systems on translating between pairs of closely-related languages from the same language family. SLT 2020 features five pairs of similar languages from three language families: Indo-Aryan (Hindi and Marathi), Romance (Catalan, Spanish, and Portuguese), and South-Slavic (Croatian, Serbian, and Slovene). Translations were evaluated in both directions using three automatic metrics: BLEU, RIBES, and TER. Results and main findings of the SLT shared task are discussed in Section 5. 

 News Translation Task This recurring WMT task assesses the quality of MT on news domain text. As in the previous year, we included Chinese, Czech, German and Russian (into and out of English) as well as French-German. New language pairs for this year were Inuktitut, Japanese, Polish and Tamil (to and from 2 http://statmt.org/wmt20/results.html English). We also included the two language pairs from the corpus filtering task (Pashto?English and Khmer?English), to give participants the opportunity to build and test MT systems using the large noisy corpora released for that task. 

 Test Data As in previous years, the test sets consist (as far as possible) of unseen translations prepared specially for the task. The test sets are publicly released to be used as translation benchmarks in the coming years. Here we describe the production and composition of the test sets. The test sets differed along several dimensions, which we list in Table  1 . The differing aspects of the test sets are as follows: Domain Most test sets are drawn from the "news" domain, which means the source texts were extracted from online news websites, and the translations were produced specifically for the task. The Pashto?English and Khmer?English test sets were drawn from wikipedia and, as last year, the French?German test sets concentrated on EU-related news. Due to limited resources and data available, the Inuktitut?English test sets contain documentand sentence-aligned data collected from two domains: news and parliamentary. The news data were extracted from the Nunatsiaq News online news website. The parliamentary data were debates from the Nunavut Hansard that are more recent than the training corpus. Development? For new languages we released a development set, produced in the same way as the test set. Sentence-split? For some pairs, we did not sentence-split the source texts. In these cases, we extracted the text from the HTML source with paragraph breaks retained, and asked translators to maintain only the paragraph breaks. This was done in order to try to improve the quality of the human translation by allowing the translators more freedom. Some analysis of the paragraph-split pairs is presented in Section 2.1.1. Directional? For most language pairs the source-side of the test set is the original, and the target-side of the test set is the translation. This is in contrast to the situation up until 2018 when our test sets were constructed from both "sourceoriginal" and "target-original" parts. Where a development set is provided, it is a mixture of both "source-original" and "target-original" texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut?English. The consequences of directionality in test sets has been discussed recently in the literature  (Freitag et al., 2019; Laubli et al., 2020; , and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use "source-original" parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut?English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents is shown in Table  5 , and the size of the test sets in terms of sentence pairs and words is given in Figure  4 . We generally aimed for 1000 sentences for a new language pair, and 2000 sentences for a previously used language pair (since there was no need to create a development set for a previously-used language pair). For test sets where the source was not sentence-split (see below) we aimed for an equivalent to 2000 sentences, but in running words. In order to improve the consistency and quality of the test set translations, this year we prepared common translator briefs to be sent to each agency we used. We show the translator briefs in Appendix B (for sentence-split sources) and Appendix C (for paragraph-split sources). 

 Paragraph-split Test Sets For the language pairs English?Czech, English?German and English?Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the "translation shifts" identified by  Popovic (2019) , which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in Table  2 , where we used the Moses sentence splitter  to provide sentence boundaries. We can see that the number of sentences per paragraph is much lower for English, where in fact 70% of paragraphs only have single sentence. For Czech and German, the mean sentences per paragraph is quite similar (2.62 vs. 2.52). The main question though, is whether translators tended to preserve the sentence structure when translating. To determine this, we split both source paragraphs and translations into sentence, and aligned them using hunalign  (Varga et al., 2005)  with the bitextor dictionaries  (Espl?-Gomis, 2009) . In Table  4  we show the counts of 1-1 sentence alignments, as well as cases where the translator merged or split neighbouring sentences. Note that these counts are approximate, since they are affected by errors in the automatic splitting and alignment. Looking through examples of merges and splits, we see that most of them are relatively simple changes, where the translator has merged to clauses into a sentence, or split a sentence to clauses. Examples of such merges and splits are shown in Table  3 , where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators' treatment of paragraph-split data for future work. 

 Training Data As in past years we provided a selection of parallel and monolingual corpora for model training, and development sets to tune system parameters. Participants were permitted to use any of the provided corpora to train systems for any of the language pairs. As well as providing updates on many of the previously released data sets, we included several new data sets, mainly to support the new language pairs. These included Wikimatrix  (Schwenk et al., 2019) , which was added for all language pairs where it was available. The news commentary and europarl corpora that we have been using since the earliest news task now have "data sheets", describing the data sets in standardised format  (Costajuss? et al., 2020) . For Tamil-English, we additionally included some recently crawled multilingual parallel corpora from Indian government websites  (Haddow and Kirefu, 2020; Siripragada et al., 2020) , the Tanzil corpus  (Tiedemann, 2009)    14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,    34,801,119 39,197,172 113,445,806 118,077,685 -72,320,248 50,061,388     93,828,313 102,937,537 3,057,383 3,766,628 -58,615,891 68,249,384     3, 074, 921, 453 2, 872, 785, 485 333, 498, 145 1, 168, 529, 851 1, 422, 729, 881 Words 65, 104, 585, 881 65, 147, 123, 742 6, 702, 445, 552 23, 332, 529, 629 40, 639, 985, 955 Dist. 342, 149, 665 338, 410, 238 48, 788, 665 90, 497, 177 213, 298, 869 Chinese Inuktitut Tamil Pashto French Sent. 1, 672, 324, 647 296, 730 28, 828, 239 6, 558, 180 4, 898, 012, 480, 611 632, 363, 004 218, 412, 919 126, 364, 574, 513 16, 780, 006 23, 531, 044 363, 878, 959      1 : The characteristics of the test sets for the news tasks. We show the domain that the test set was drawn from, whether or not we released a development set this year, whether the texts were sentence-split before translation, and whether the direction of translation was preserved. For "directional" test sets, the entire source side of the test set was originally written in the source language, and then translated to the target language. Non-directional test sets are a mixture of "source-original" and "target-original" texts. Finally, we record whether or not the test set contained the original document boundaries.  Als R?ckzieher sei das aber nicht zu verstehen: "Ganz ?gypten ist der Tahrirplatz". But that should not be understood as a withdrawal. "All of Egypt is Tahrir square." "Ich f?hle mich unglaublich geehrt und dem?tig, neben JLo die Latino-Community zu repr?sentieren. "I feel incredibly honored and humbled to be next to J. Lo, representing the Latino community that is such an important force in the United States," Shakira shared in a video. Denn diese hat eine unglaubliche St?rke in den USA", teilte Shakira in einem Video mit. Man k?nne die Unternehmen zwar nicht von der Umsatzsteuer auf Sachspenden befreien, erkl?rte das Ministerium auf eine Frage der Gr?nen-Bundestagsfraktion, ?ber die die Zeitungen der Funke-Mediengruppe am Freitag berichteten. Although it is impossible to exempt companies from VAT on donations in kind, retailers could set the market value of unsaleable returns so low that they would need to pay no or only very little VAT, the Ministry explained in response to a question from the Greens parliamentary group, as reported in newspapers of the Funke media group on Friday. Die H?ndler k?nnten aber den Marktwert der unverk?uflichen Retouren so niedrig ansetzen, dass sie keine oder nur wenig Umsatzsteuer zahlen m?ssten. Table  4 : How the translators treated sentences when translating the paragraph-split texts. We sentence-split and automatically aligned source and translation. We show the number and percentage of sentences which were translated 1-1, as well as the number of times translators merged or split sentences when translating.  (Kunchukuttan et al., 2018)  and English and Tamil wikipedia dumps. The training corpus for Inuktitut?English is the recently released Nunavut Hansard Inuktitut-English Parallel Corpus 3.0  (Joanis et al., 2020) . For the Japanese?English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0  (Morishita et al., 2020) , a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC)  (Pryzant et al., 2017) , the Kyoto Free Translation Task (KFTT) corpus  (Neubig, 2011) , constructed from the Kyoto-related Wikipedia articles, and TED Talks  (Cettolo et al., 2012) . The monolingual data we provided was similar to last year's, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto?English and Khmer?English are shared with the Parallel Corpus Filtering Shared Task . The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative  by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. Some statistics about the training and test materials are given in Figures  1, 2 , 3 and 4. 

 Submitted Systems In 2020, we received a total of 153 submissions. The participating institutions are listed in Table  6  and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included online MT systems (originating from 4 services), which we anonymized as ONLINE-A,B,G,Z. This year we introduced a new submission tool, OCELoT 4 , replacing the matrix that has been used in most previous editions. Using OCELoT gave us more control over the submission and scoring process, for example we were able to limit the number of test submissions by each team, and we also displayed the submissions anonymously to avoid publishing any automatic scores. A screenshot of OCELoT is shown in Figure  5 . For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. 

 AFRL (Gwinnup and Anderson, 2020) AFRL-SYSCOMB20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL-FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017.  (Xv, 2020)  ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit us- 

 ARIEL XV 

 Team Institution 

 AFRL Air Force Research Laboratory  (Gwinnup and Anderson, 2020)  ARIEL XV Independent submission (Xv, 2020) CUNI Charles University  (Popel, 2020 (Popel, , 2018 Kocmi, 2020  Zoho Corporation (no associated paper) Table  6 : Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop.  

 Charles University (CUNI) CUNI-DOCTRANSFORMER  (Popel, 2020)  is similar to the sentence-level version (CUNI-T2T-2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018  (Popel, 2018) , also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to  Popel and Bojar (2018)  plus a novel concat-regime backtranslation with checkpoint averaging , tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction ("translationese") issues. For cs?en also a coreference preprocessing was used adding the female-gender pronoun where it was pro-dropped in Czech, referring to a human and could not be inferred from a given sentence. CUNI-TRANSFER  (Kocmi, 2020)  combines transfer learning from a high-resource language pair Czech-English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. CUNI-TRANSFORMER  (Popel, 2020)     Kim et al. (2020)  base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. 2.3.9 ETRANSLATION  (Oravecz et al., 2020)  ETRANSLATION mainly use the standard training pipeline of Transformer in Marian, using tagged back-translation and other features. Subword units are identified by SentencePiece. The paper describes the group's concern about computing resources and the practical utility of expensive features like ensembling 2 to 4 bigger models. Techniques that were ineffective in ETRANSLATION's case (e.g. right-to-left model for rescoring English?German or Unicode preprocessing for Japanese?English) are also described. 2.3.10 FACEBOOK AI  (Chen et al., 2020a)  FACEBOOK AI focus on low-resource language pairs involving Inuktitut and Tamil using two strategies: (1) exploiting all available data (parallel and monolingual from all languages) and (2) adapting the model to the test domain. For (1), FACEBOOK AI opt for non-constrained submission, using data derived from Common-Crawl to get strong translation models via iterative backtranslation and self-training and strong language models for noisy channel reranking. Multilingual language models are created using mBART across all the 13 languages of WMT20. For (2), the datasets are tagged for domain, finetuned on and further extended with in-domain data. 

 GRONINGEN GRONINGEN-ENIU  (Roest et al., 2020)  investigate the (1) importance of correct morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. GRONINGEN-ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning.  (Dhar et al., 2020)  study the effects of various techniques such as linguistically motivated segmentation, backtranslation, fine-tuning and word dropout on the English?Tamil News Translation task. Linguis-tically motivated subword segmentation does not consistently outperform the widely used Senten-cePiece segmentation despite the agglutinative nature of Tamil morphology. The authors also found that fully-fledged back-translation remains more competitive than its cheaper alternative. 

 GRONINGEN-ENTAM 2.3.12 GTCOM  GTCOM are unconstrained systems using mBART (Multilingual Bidirectional and Auto-Regressive Transformers), back-translation and forward-translation. Further gains are achieved using rules, language model and RoBERTa model to filter monolingual, parallel sentences and synthetic sentences. The vocabularies are created from both monolingual and parallel data. 2.3.13 HELSINKINLP  (Scherrer et al., 2020a)  HELSINKINLP for the Inuktitut-English news translation task focuses on the efficient use of monolingual and related bilingual corpora with multi-task learning as well as an optimized subword segmentation with sampling. 2.3.14 HUAWEI TSC  (Wei et al., 2020a)  HUAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT  (Zhang et al., 2017)   Transformer implemented in fairseq is used, with smaller than "base" models due to limited training data. 2.3.17 NICT-KYOTO  (Marie et al., 2020)  NICT-KYOTO is a combination of neural machine translation systems processed through nbest list reranking. The systems combined are Transformer-based trained with Marian and Fairseq with and without using tagged backtranslation. All the systems are constrained, and the final primary submission is selected on the basis of the BLEU score obtained on the official validation data.  NICT-RUI  NICT-RUI is closely related to SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear.  NIUTRANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of numbers, names and punctuation. 

 NICT-RUI 

 NIUTRANS For low-resource language pairs, multi-lingual seed models are used. 

 NRC (Knowles et al., 2020) The NRC systems are hybrids of Transformer models trained with Sockeye, with one ensembled system for news domain translation and one for Hansard domain translation. Data was preprocessed with language-specific punctuation and character preprocessing, tokenization, and BPE. They were trained with domain tagging, domainspecific finetuning, ensembles of 3 systems per domain, BPE-dropout (EN-IU), and tagged backtranslation (IU-EN).  (Shi et al., 2020)  OPPO train Marian for some language pairs and fairseq for others, relying on a number of mature techniques including careful corpus filtering, iterative forward and backward translation, finetuning on the original parallel data, ensembling of several different models, and complex reranking which uses forward (source-to-target) scorers, backward scorers (target-to-source) and language models (monolingual), each group again building upon ensembles and being applied left-to-right as well as right-to-left. 

 OPPO Each language pair received targeted attention, discussing training data properties, varying the process as needed and choosing from several possible final models.  (Molchanov, 2020)  PROMT BASELINE TRANSFORMER uses Mar-ianNMT, shared vocabulary, 16k BPE merge operations and it is trained on unconstrained data. 

 PROMT PROMT BASIC TRANSFORMER uses separate vocabs (16k source + 32k target), and tied embeddings. PROMT MULTILINGUAL 4-TO-EN is a multilingual system trained to translate from Croatian, Serbian, Slovak and Czech to English. It is a basic Transformer configuration with shared vocabulary. 

 PROMT MULTILINGUAL PL-EN is a Polish?English system trained jointly in both directions. It uses basic Transformer configuration and shared vocabulary. None of PROMT systems are constrained. 

 SJTU-NICT (Li et al., 2020) SJTU-NICT represents two different main approaches. For News Translation Task, (1) crosslingual language models (XLM) are used in an additional encoder to benefit from languageindependent sentence representations from both the source and target side for Polish?English. For English?Chinese, which includes documentlevel information, three-stage training is used to train Longformer (Transformer with attention extended to the full document). 

 SRPOL (Krubi ?ski et al., 2020) No short description provided. 2.3.25 TALP UPC  (Escolano et al., 2020)  No short description provided. 

 TENCENT TRANSLATION (Wu et al., 2020b) No short description provided. 

 THUNLP (no associated paper) No description provided. 2.3.28 TILDE  (Kri?lauks and Pinnis, 2020)  For WMT 2020, Tilde developed English?Polish (separate constrained and unconstrained submissions) and Polish?English (constrained only) NMT systems. Tilde experimented with morpheme splitting prior to byte-pair encoding, dual conditional cross-entropy filtering, samplingbased backtranslation of source-domain-adherent monolingual data, and right-to-left reranking. The submitted translations were produced using ensembles of Transformer base and Transformer big models, which were trained using back-translated data, and right-to-left re-ranking. 

 TOHOKU-AIP-NTT (Kiyono et al., 2020) TOHOKU-AIP-NTT used Transformer-based Encoder-Decoder model with 8 layers and feed forward dimension of 8192. Synthetic data were created via beam back-translation from monolingual data available for each language and incorporated to the training using tagged backtranslation. The bitext was oversampled so that the model saw the bitext and synthetic data in 1:1 ratio. After training, the model was finetuned with newstest corpus. An ensemble of four models was used to generate candidate translation, which were in turn re-ranked using scores from following components: (1) source-to-target right-to-left model, (2) target-to-source left-to-right model, (3) target-tosource right-to-left model, (4) masked language model (RoBERTa), and (5) uni-directional language model (Transformer-LM).  (Hernandez and Nguyen, 2020)  UBIQUS performed a single submission, based on an unconstrained multilingual setup. The approach consists of jointly training a traditional Transformer model on several agglutinative languages in order to benefit from them for the lowresource English-Inuktitut task. For that purpose, the dataset was extended with other linguistically near languages (Finnish, Estonian), as well as inhouse datasets introducing more diversity to the domain. 

 MULTILINGUAL-UBIQUS 

 UEDIN UEDIN  (Bawden et al., 2020a)  for the very low-resource English-Tamil involved exploring pretraining, using both language model objectives and translation using an unrelated high-resource language pair (German-English), and iterative backtranslation. For English-Inuktitut, UEDIN explored the use of multilingual systems. UEDIN-DEEN and UEDIN-ENDE  (Germann, 2020)  ensemble big transformer models trained in three stages: First, base transformer models were trained on available high-quality parallel data. These models were used to rank and select parallel data from crawled and automatically matched parallel data (Paracrawl, Commoncrawl, etc.). 2nd-generation big transformers were then trained on the combined parallel data. These models were used for back-translation. Original and back-translated data was then used to the final 3rdgeneration models. WMTBIOMEDBASELINE are the baseline systems from the Biomedical Translation Task. 

 YOLO (no associated paper) No description provided. 

 ZLABS-NLP ZLABS-NLP used SentencePiece for subword segmentation, otherwise the model including hyperparameters is the same as described by  Ott et al. (2018)  and implemented in FairSeq. Probably, OpenNMT-py was used during training (backtranslation for Tamil). 

 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the official ranking of systems taking part in the news translation task. This section describes how data for the human evaluation is prepared, the process of collecting human assessments, and computation of the official results of the shared task. 

 Direct Assessment Since running a comparison of direct assessments (DA,  Graham et al., 2013 Graham et al., , 2014  and relative ranking in 2016  and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0-100 rating scale.  5  No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid  Awad et al., 2019)  and multilingual surface realisation  (Mille et al., 2018 (Mille et al., , 2019 . 

 Source and Reference-based Evaluations The earlier DA evaluations that we performed were all referenced based, as described above, however in 2018 we trialled source-based evaluation for the first time, in English to Czech translation. In this configuration, the human assessor is shown the source input and system output only (with no reference translation shown). This approach has the advantage of freeing up the humangenerated reference translation so that it can be included in the evaluation to provide an estimate of human performance. As was the approach in WMT19, since we would like to restrict human assessors to only evaluate translation into their native language, we again restrict bilingual/sourcebased evaluation to evaluation of translation for out-of-English language pairs. This is especially relevant since we have a large group of volunteer human assessors with native language fluency in non-English languages and high fluency in English, while we generally lack the reverse, i.e. native English speakers with high fluency in non-English languages. 

 Translationese Prior to WMT19, all the test sets included a mix of sentence pairs that were originally in the source language, and then translated to the target language, and sentence pairs that were originally in the target language but translated to the source language. The inclusion of the latter "reversecreated" sentence pairs has been shown to introduce biases into the evaluations, particularly in terms of BLEU scores , so we avoid it where possible. As detailed in Sec-tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 

 Document Context Prior to WMT19, the issue of including document context was raised within the community  (L?ubli et al., 2018; Toral et al., 2018)  and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context "+DC" (with document context), and secondly, a variation that omitted document context "?DC" (without document context). This year, for language pairs for which document context was available in the test set, we therefore include this context when evaluating translations for systems. Although we include document context, ratings are nevertheless collected on the segment-level, motivated by the power analysis described in  and . The particular details on how document context is made available to assessors depends on the translation direction, as described in more detail in Sections 3.2 and 3.3 below for translation into English and out of English, resp. In the following, we use the following abbreviations to describe annotation style: SR+DC for translation direction where assessors rank individual segments (Segment Ranking, SR) and have access to the full document, SR?DC for translation directions where document context is not available and assessors see individual sentences in random order. Fully document-level evaluation (DR+DC, document-level ranking with document context available) as trialled last year where we asked for a single score given the whole document is problematic in terms of statistical power and inconclusive ties, as shown in ; , and we subsequently did not include this approach for any into-English language this year. As in previous years, the SR?DC annotation is organized into "HITs" (following the Mechanical Turk's term "human intelligence task"), each containing 100 screens.  

 Human Evaluation of Translation into-English A summary of the human evaluation configurations run this year in the news task for into-English language pairs is provided in Table  7 . In terms of the News translation task manual evaluation for into-English language pairs, a total of 654 turker accounts were involved. 6 654,583 translation assessment scores were submitted in total by the crowd, of which 166,868 were provided by workers who passed quality control. System rankings are produced from a large set of human assessments of translations, each of which indicates the absolute quality of the output of a system. Table  8  shows total numbers of human assessments collected in WMT20 for into-English language pairs contributing to final scores for systems. 7 

 Crowd Quality Control We run two configurations of DA, one with document context, segment-rating with document context (SR+DC), for languages for which this information was available and one without document context, for the remainder, segment rating without document context (SR-DC). We describe quality control details and both methods of ranking systems for into-English language pairs in detail below. Standard DA HIT Structure (SR?DC) In the standard DA HIT structure (without document context), three kinds of quality control translation pairs are employed as described in Table  9 : we repeat pairs expecting a similar judgment (Repeat Pairs), damage MT outputs expecting significantly worse scores (Bad Reference Pairs) and use references instead of MT outputs expecting high scores (Good Reference Pairs). For each of these three types, we include the MT output, along with its corresponding control. In total, 60 items in a 100-translation HIT serve in quality control checks but 40 of those are regular judgments of MT system outputs (we exclude assessments of bad references and ordinary reference translations when calculating final scores). The effort wasted for the sake of quality control is thus 20%. Also in the standard DA HIT structure, within each 100-translation HIT, the same proportion of translations are included from each participating system for that language pair. This ensures the final dataset for a given language pair contains roughly equivalent numbers of assessments for each participating system. This serves three purposes for making the evaluation fair. Firstly, for the point estimates used to rank systems to be reliable, a sufficient sample size is needed and the most efficient way to reach a sufficient sample size for all systems is to keep total numbers of judgments roughly equal as more and more judgments are collected. Secondly, it helps to make the evaluation fair because each system will suffer or benefit equally from an overly lenient/harsh human judge. Thirdly, despite DA judgments being absolute, it is known that judges "calibrate" the way they use the scale depending on the general observed translation quality. With each HIT including all participating systems, this effect is systems comprising human-generated reference translations used to provide human performance estimates. 

 Repeat Pairs: Original System output (10) An exact repeat of it (10); Bad Reference Pairs: Original System output (10) A degraded version of it (10); Good Reference Pairs: Original System output (10) Its corresponding reference translation (10). averaged out. Furthermore apart from quality control items, HITs are constructed using translations sampled from the entire set of outputs for a given language pair. 

 Document-Level DA HIT Structure (SR+DC) Collection of segment-level ratings with document context (Segment Rating + Document Context) involved constructing HITs so that each sentence belonging to a given document (produced by a single MT system) was displayed to and rated in turn by the human annotator. Quality control items for this set-up was carried out as follows with the aim of constructing a HIT with as close as possible to 100 segments in total: 1. All documents produced by all systems are pooled; 8 2. Documents are then sampled at random (without replacement) and assigned to the current HIT until the current HIT comprises no more than 70 segments in total; 3. Once documents amounting to close to 70 segments have been assigned to the current HIT, we select a subset of these documents to be paired with quality control documents; this subset is selected by repeatedly checking if the addition of the number of the segments belonging to a given document (as quality control items) will keep the total number of segments in the HIT below 100; if this is the case it is included; otherwise it is skipped until the addition of all documents has been checked. In doing this, the HIT is structured to bring the total number of segments as close as possible to 100 segments in total within a HIT but without selecting documents in any systematic way such as selecting them based on fewest segments, for example. 4. Once we have selected a core set of original system output documents and a subset of them to be paired with quality control versions for each HIT, quality control documents are automatically constructed by altering the sentences of a given document into a mixture of three kinds of quality control items used in the original DA segment-level quality control: bad reference translations, reference translations and exact repeats (see below for details of bad reference generation); 5. Finally, the documents belonging to a HIT are shuffled. 

 Construction of Bad References As in previous years, bad reference pairs were created automatically by replacing a phrase within a given translation with a phrase of the same length, randomly selected from n-grams extracted from the full test set of reference translations belonging to that language pair. This means that the replacement phrase will itself comprise a mostly fluent sequence of words (making it difficult to tell that the sentence is low quality without reading the entire sentence) while at the same time making its presence highly likely to sufficiently change the meaning of the MT output so that it causes a noticeable degradation. The length of the phrase to be replaced is determined by the number of words in the original translation, as follows: Translation # Words Replaced Length (N) in Translation 1 1 2-5 2 6-8 3 9-15 4 16-20 5 >20 N/4 

 Annotator Agreement When an analogue scale (or 0-100 point scale, in practice) is employed, agreement cannot be measured using the conventional Kappa coefficient, ordinarily applied to human assessment when judgments are discrete categories or preferences. Instead, to measure consistency we fil-ter crowd-sourced human assessors by how consistently they rate translations of known distinct quality using the bad reference pairs described previously. Quality filtering via bad reference pairs is especially important for the crowd-sourced portion of the manual evaluation. Due to the anonymous nature of crowd-sourcing, when collecting assessments of translations, it is likely to encounter workers who attempt to game the service, as well as submission of inconsistent evaluations and even robotic ones. We therefore employ DA's quality control mechanism to filter out low quality data, facilitated by the use of DA's analogue rating scale. Assessments belonging to a given crowdsourced worker who has not demonstrated that he/she can reliably score bad reference translations significantly lower than corresponding genuine system output translations are filtered out. A paired significance test is applied to test if degraded translations are consistently scored lower than their original counterparts and the p-value produced by this test is used as an estimate of human assessor reliability. Assessments of workers whose p-value does not fall below the conventional 0.05 threshold are omitted from the evaluation of systems, since they do not reliably score degraded translations lower than corresponding MT output translations. Table  10  shows the number of workers participating in the into-English translation evaluation who met our filtering requirement in WMT20 by showing a significantly lower score for bad reference items compared to corresponding MT outputs, and the proportion of those who simultaneously showed no significant difference in scores they gave to pairs of identical translations. We removed data from the non-reliable workers in all language pairs. 

 Human Evaluation of Translation out-of-English Human evaluation of out-of-English translations features a bilingual/source-based evaluation campaign that enlists the help of participants in the shared task. As usual, each team was asked to contribute around 8 hours annotation time, which we estimated at 16 HITs per each primary system submitted, with each HIT including 100 segment translations. Unfortunately, not all participating teams were able to provide requested number of assessments, hence, to collect the required number of assessments per MT system, we also employed external translators in a separate campaign. The contracted translators contributed with one third of total number of assessments. Both campaigns utilized document-level DA and were run for all out-of-English language pairs, which test sets include document-level segmentation. For English?Khmer, English?Pashto, French?German, and German?French, whose test sets do not provide document boundaries, segment-level DA evaluation without document context (SR-DC) was performed, enlisting the effort of translators. For English?Inuktitut, since we expected no participants to speak Inuktitut, the NRC hired native speakers through the Pirurvik Centre to conduct most of the DA evaluation. Due to the delays in starting the evaluation campaign, they were only able to complete the evaluation a few days before the conference, and could only annotate the news half of the test set. The Hansard half of the test set was not assessed in time for this report, but plans are being made to continue the evaluation after the conference. Updated rankings should be provided at a future date. In terms of the News translation task documentlevel manual evaluation for out-of-English language pairs, a total of 1,189 researcher/translator accounts were involved, and 248,597 translation assessment scores were contributed in total (with quality control pairs), including 18,108 document ratings. For the segment-level campaigns (i.e. English?Khmer, English?Pashto, German?French and French?German) we had 300 accounts and 65872 scores collected in total. Statistics per language pair are summarized in Table  11 . For data collection we again used the open-source Appraise 9  (Federmann, 2012) . The effort that goes into the manual evaluation campaign each year is impressive, and we are grateful to all participating individuals and teams for their work. 

 Document-Level Assessment This year's human evaluation for out-of-English language pairs features an improved documentlevel direct assessment configuration that extends the context span to entire documents for a more reliable machine translation evaluation (Castilho  ment on a screen. In the default scenario, an annotator scores individual segments one-by-one and, after scoring all of them, on the same screen, the annotator then judges the translation of the entire document displayed. Annotators can, however, revisit and update scores of previously assessed segments at any point of the annotation of the given document. 

 Quality Control For the document-level evaluation of out-of-English translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English?German (3 alternative reference translations, including 1 generated using the paraphrasing method of ) and English?Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a bet- ter quality of assessments than the crowd-sourced workers, only bad references are used as quality control items. Instead of sampling initial documents with close to 70 segments, we sample documents with 88 segments, and then a subset of documents with around 12 segments is selected to be converted into bad references. The remaining of the HIT creation process remains the same. 

 Producing the Human Ranking In all set-ups, similar to previous years, system rankings were arrived at in the following way. Firstly, in order to iron out differences in scoring strategies of distinct human assessors, human assessment scores for translations were first standardized according to each individual human assessor's overall mean and standard deviation score. This year all rankings for to-English translation were arrived at via segment ratings (SR?DC, SR+DC), average standardized scores for individual segments belonging to a given system were then computed, before the final overall DA score for a given system is computed as the average of its segment scores (Ave z in Table  12 ). Results are also reported for average scores for systems, computed in the same way but without any score standardization applied (Ave % in Table  12 ).  clusters according to Wilcoxon rank-sum test p < 0.05. For evaluation of English?Inuktitut insufficient data resulted in a small sample size of human assessments per system and as a result some systems that fall within the same cluster are likely to do so simply due to low statistical power . Human performance estimates arrived at by evaluation of human-produced reference translations are denoted by "HUMAN" in all tables. Note that "HUMAN-P" is a human-produced paraphrase of HUMAN-A, according to the method proposed by . Clusters are identified by grouping systems together according to which systems significantly outperform all others in lower ranking clusters, according to Wilcoxon rank-sum test. Appendix A shows the underlying head-to-head significance test official results for all pairs of systems. All data collected during the human evaluation is available at http://www.statmt.org/wmt20/ results.html. In terms of human and machine quality comparisons in results, it is clear from the sourcebased evaluation of English to German and English to Chinese translation that human translators vary in performance, with each human translator represented in a distinct cluster. Without taking from the significant achievement of systems that have tied with a human translator, this fact should be taken into account when drawing conclusions about human parity. A tie with a single human translator should not be interpreted as a tie with human performance in general. 

 Test Suites "Test Suites" have now become an established part of WMT News Translation. Their purpose is to complement the standard one-dimensional manual evaluation. Each test suite can focus on any aspect of translation quality and any subset of language pairs and MT systems. Anyone can propose their own test suite and take part, and we also try to solicit evaluation from past successful test suite teams to support some cross-year insight. Each team in the test suites track provides source texts (and optionally references) for any language pair that is being evaluated by WMT News Task. We shuffle these additional texts into the inputs of News Task and ship them as inputs to MT system developers jointly with the regular news texts. The shuffling happens at the document or sentence level as agreed with the test suite authors. (Shuffling at the level of sentences can lead to a very high number of documents in the final test set because each sentence is treated as a separate document.) MT system developers may decide to skip these documents based on their ID but most of them process test suites along with the main news texts. After collecting the output translations from all WMT News Task Participants, test suites translations are made available back to the test suite authors for evaluation. Test suite sentences do not go through the manual evaluation as described in Section 3. As in the previous years, test suites are not limited to the news domain, so News Task system may actually underperform on them. 

 Test Suite Details The following paragraphs briefly describe each of the test suites. Please refer to the respective paper for all the details of the evaluation. 

 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset  (Anastasopoulos et al., 2020) . The dataset provides manually created translations of COVID-19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table  15  outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepared to handle highly narrow-domain data. In addition, the variance of the output quality across languages and across domains highlights the importance of building MT systems that can generalize across domains. 

 Document Coherence Check via Markable Annotation  (Zouhar et al., 2020)  The test suite provided in 2020 by the ELITR project  (Zouhar et al., 2020)  follows upon  Vojt?chov? et al. (2019) . The focus this year is on "markables", i.e. mainly domain-specific terms that have to be translated consistently and unambiguously throughout the whole document (except news where style may require variation) to maintain lexical coherence. Manual annotation of the translation of markables is contrasted with manual annotation of fluency and adequacy and also BLEU scores. The test suite is limited to 4 English?Czech documents and 2 Czech?English documents, covering 215 markable occurrences across 4 different domains. The set of markables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, even rare errors such as bad disambiguation, overtranslation or disappearance of a term or its translation which conflicts with other terms in the document can be critical. The comparison of MT outputs with the reference (hidden among MT systems) in the evaluation is also interesting. Man-made errors were always marked as less severe than those of MT. The annotation also suggests that one of the documentlevel systems outperformed the reference in markable evaluation if error severity and frequency are weighted equally. Fluency and adequacy collected as average sentence-level scores (with access to the full documents of all systems) are curious, revealing perhaps more about the annotators than the MT systems.  The test suite by  focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. 

 Gender Coreference and Bias The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to correctly resolve the coreference link and transfer information from the pronoun to the noun in the antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified.  build upon the WinoMT  (Stanovsky et al., 2019)  test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. 

 Linguistic Evaluation of German-to-English  (Avramidis et al., 2020)  The test suite by DFKI covers 107 grammatical phenomena organized into 14 categories. Since 2018, the same set of phenomena are being tested annually  (Macketanz et al., 2018; Avramidis et al., 2019) . Automatic evaluation is complemented with 45 hours of human annotation. This year, the newcomers VOLCTRANS and TOHOKU-AIP-NTT perform particularly well in the tested phenomena, followed by the traditional systems UEDIN, ONLINE-B, ONLINE-G, and ONLINE-A. The generally good news is that systems which participated in both WMT19 and WMT20 show an improvement this year. Given that the test suite target side remains undisclosed, these scores can be deemed absolute, unlike the official DA scores which are only relative within each year and set of systems. The test suite allows to report these improvements per linguistic category and specifically for each MT system that participated in two consecutive years. The biggest improvements are observed in long distance dependencies or interrogatives, verb valency, ambiguity and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 4.1.5 Word Sense Disambiguation  (Scherrer et al., 2020b)  Scherrer et al. (  2020b ) is a followup of last year's evaluation  (Raganato et al., 2019) , assessing the ability of MT systems to disambiguate a word given its context of the sentence. The underlying MuCoW (multilingual contrastive word sense disambiguation) dataset contains approximately 2k to 4k sentences per language pair selected from large parallel corpora to contain particularly ambiguous words. This year, the focus was on language pairs that appeared both in WMT19 and WMT20 (and were available in the MuCoW dataset), namely English?Czech, English?German, and English?Russian. Comparing overall numbers across the years,  Scherrer et al. (2020b)  report that ambiguous words are correctly disambiguated in the majority of cases. Both precision (percentage of correct choices out of sentences where either good or bad expected translation was found) and recall (percentage of correct choices out of all sentences) are above 60 % and reaching 80 % for the best systems in a given language pair when mixing "in-domain" and "out-of-domain" evaluation. The "out-of-domain" synsets are those that are represented in the test suite with more than half of cases coming from the colloquial subtitle domain; other synsets are deemed "in-domain". The "in-domain" scores are generally higher, with precisions above 95 % for the best Czech and Russian systems. Across the years, no real improvement is however observed. Three cases suggest that training systems at the level of documents decreases their performance in this sentence-level evaluation (each sentence forms a separate document): DocTransformer vs. Transformer by CUNI in 2019 and 2020 and Microsoft document-level vs. sentence-level submission in 2019. 

 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects of the same language  (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-juss? et al., 2018; . To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish -Portuguese (Romance languages), Czech -Polish (Slavic languages), and Hindi -Nepali (Indo-Aryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. SLT 2020 features five pairs of similar languages from three different language families: Indo-Aryan, Romance, and South-Slavic. Translations were evaluated in both directions using automatic evaluation metrics presented in this section. 

 Data Training We have made available a number of data sources for the SLT shared task. Some training datasets were used in the previous editions of the WMT News Translation shared task and were updated (Europarl v10, News Commentary v15, Wiki Titles v2), while some corpora were newly introduced (JRC Acquis). The released parallel HI-MR dataset was collected from news  (Siripragada et al., 2020) , PMIndia  (Haddow and Kirefu, 2020)  and Indic Wordnet  (Bhattacharyya, 2010; Kunchukuttan, 2020a)  datasets. All data were initially combined, tokenized using indic-nlp tokenizer  (Kunchukuttan, 2020b)  and randomly shuffled. From the combined corpus, we randomly extracted 49,434 sentences for the training set and the rest are used as development and test sets. For the South-Slavic language pairs we used large datasets available from Opus  (Tiedemann and Nygaard, 2004)    11  , more precisely the OpenSubtitles, MultiParaCrawl, DGT and JW300 data. Different to the other language groups, for monolingual data web corpora of the three languages  (Ljube?i? and Erjavec, 2011; Ljube?i? and Klubi?ka, 2014; Erjavec et al., 2015)  were given to the participants. 

 Development and Test Data The development and test sets for Spanish-Catalan and Spanish-11 http://opus.nlpl.eu/ Portuguese language pairs were created from a corpus provided by Pangeanic 12 . First, we performed cleaning using CLEAN-CORPUS-N.PERL 13 script to retain sentences that have between 4 and 100 tokens. This narrowed the number of sentences to 1,287 and 1,535 in dev and test sets respectively. Finally, sentences containing metadata information were removed, which resulted in 1,283 and 1,495 sentences in dev and test sets respectively. The aforementioned shuffled combined HI-MR dataset, 1411 sentences are used for development set and 3882 for the test set. Finally, the test set was equally split into two different test sets: 1941 sentences used for HI to MR and 1941 sentences were used for MR to HI. For the Slovene-Croatian and Slovene-Serbian language pairs, development and test data were obtained from the Ciklopea translation agency 14 in form of a data donation from the Bisnode business intelligence company 15 . The data consists of public relations releases translated in various directions between the three languages. The data was cleaned, deduplicated and shuffled, resulting in 2,457 dev and 2,582 instances for the Slovene-Croatian pair, and 1,259 dev and 1,260 test instances in the Slovene-Serbian pair. Given that these translations sometimes form Slovene-Croatian-Serbian triangles, special care was invested in circumventing data leakage between development data on one side, and test data on the other, of the two language pairs. 

 Participants and Approaches The second edition of the WMT SLT task attracted 68 teams who signed up to participate in the competition and 18 of them submitted their system outputs. In the end of the competition, 14 teams submitted system description papers which are referred to in this report. Table  22  summarizes the participation across language pairs and translation directions and includes references to the 14 system description papers. Next we provide summaries for each of the entries we received: 

 A3-108 The team A3-108 submitted their system for HI-MR and MR-HI. The team initially      build SMT models for both language direction after three steps preprocessing: (i) default -in-dic_nlp_library  16  and moses tokenizer 17 , (ii) morfessor 18 and (iii) BPE  19  . These SMT models were used for back-translation. Finally, these backtranslation data were used to train their NMT system. ADAPT-DCU The ADAPT-DCU team participated in the SLT task on the Croatian-Slovene and Serbian-Slovene language pairs. The team's submissions were based on the Sockeye implementations of the Transformer, with a joint 32klarge BPE vocabulary for all three languages. The submission were regularly multilingual (having Slovene on one side and Croatian and Serbian on the other). The team used only OpenSubtitles bilingual training data, considering other available data to be too noisy. The basic implementation of the multilingual system was submitted as the second contrastive system, the multilingual implementation trained on filtered parallel data as the first contrastive system, while the primary submission included backtranslation of target monolingual data of segments similar to the development data. By performing n-gram-character-based filtering of training data the training time was cut in half with a minor improvement on the translation quality, while the largest improvements in translation quality were obtained by back-translating data similar to development data (between 8 and 14 BLEU points). f1plusf6 During preprocessing as Marathi and Hindi are rich in terms of morphology, Applied two way segmentation as preprocessing, first supervised and unsupervised word based morphological segmentation and then BPE based segmentation to tackle low-resource language pairs. The participants used shared vocab across training and utilised POS based features on the source side to create initial models for both directions. For preparing unsupervised back-translation parallel data they used aligned embedding space to generate word by word parallel sentences for both language directions. They also prepared initial models from the provided parallel data for back translation from monolingual data and pruned back-translation pairs based on perplexity score. Their model is based on Luong's attention on bi-LSTM network, copy attention on dynamically generated dictionary with label smoothing and dropouts to reduce overfitting. Fast-MT Fast-MT team submitted their NMT system where Transformers and Recurrent Attention models are effectively used. They combined the recurrence based layered encoder-decoder model with the Transformer model. Their submitted system for Indo-Aryan Language (Hindi to Marathi) pair is trained on the parallel corpus of the training dataset provided by the organizers. IIAI IIAI TEAM participate in both directions of the Hindi-Marathi translation task. Their primary submission is a transformer model trained on the released parallel and back-translated monolingual data. The team jointly learned BPE from the merged source-target corpus. After BPE, sentences were corrupted and reconstructed using the two ways:(i) 15% of the subwords in the sentence are randomly selected and masked, (ii) 15% of the subwords are randomly selected one by one and swapped with another randomly-selected subword in the sequence. 

 Team System Description Paper   22 : The teams that participated in the SLT 2020 task and their system description papers. IITDELHI Team IITDELHI participated in the SLT task on Hindi-Marathi and Spanish-Portuguese language pairs. The team's primary submission builds on fine-tuning over pretrained mBART. They used pre-trained weights of the mBART model  (Liu et al., 2019) , which is pretrained on large amounts of monolingual data for 25 languages including Spanish, however Portuguese is not there. The authors initialized a Transformer architecture with 12 encoder and decoder layers using the pre-trained weights, and then directly fine-tuned with the released training data. The authors conclude that mBART is helpful for transfer learning, even though the languages that are not available in the pre-trained model. 

 INFOSYS Infosys system for Hindi-Marathi (Primary) task is designed to learn the nuances of translation of this low resource language pair by taking advantage of the fact that the source (Hindi) and target (Marathi) languages are same alphabet languages. This system is an ensemble of FairSeq model built on anonymized parallel data and FairSeq back-translation model. The common words/tokens between source and target languages are anonymized during pre-processing upon which the FairSeq model is trained. The input statements during inferencing are anonymized based on the vocabulary of common tokens prepared during training and the predicted statements are deanonymized during post-processing accordingly. This improved the accuracy (BLEU) of FairSeq considerably. Pre-processing also applies traditional parallel corpus filtering techniques to clean parallel data followed by domain specific techniques. There were records containing multiple statements delimited by slashes, where the domain specific techniques are applied to transform them in to records that retain only the matching single statement, identified based on its syntactic similarity with its parallel statement. Synthetic data generated with the mono-lingual (Marathi) data during FairSeq back-translation has unknown words (w.r.t parallel data vocabulary), resulting unknown words during prediction, which are downvoted while ensembling. IPN-CIC This team participated in the Spanish-Portuguese language pair. The systems used the Transformer architecture with a fine-tuning for domain adaptation. The team proposed experiments on the kind of tokens used (words and sub-word units) and the initialization of the word embeddings in the systems using either a random initialization or pre-trained word embeddings. NICT NICT participated in two language pairs: Hindi-Marathi and Spanish-Catalan, for both translation directions. Their primary submission is an unsupervised NMT system, initialized with a pre-trained cross-lingual language model (XLM), that has been trained using only the monolingual data provided by the organizers. They used the standard hyper-parameters for training XLM and unsupervised NMT. Their contrastive submission is the same but supervised NMT system trained on the combination of the released bilingual and monolingual data. NITS-CNLP NITS-CNLP system for HI-MR and MR-HI translation is based on cross-lingual language modelling with masked language modeling and translation language modeling. These language models were pre-trained on monolingual corpus and fine-tuned on parallel data following the architecture of  Conneau and Lample (2019)  and employing 6 layers with 8 attention heads and with 32 batch size, trained on a single GPU. NLPRL This system submitted by the NLPRL team for the HI-MR is based on the Transformer approach. The system were trained on only the released parallel corpus. The team used Sentence-Piece library for preprocessing and set vocabulary size of 5000 symbols for source and target bytepair encoding, respectively. 

 NLPRL-BHU The team participated in the HI ? MR language pair. The participants used byte pair encoding to preprocess the data and fairseq library with the GRU-transformer for training. NUIG-Panlingua-KMI The NUIG-Panlingua-KMI team explored phrase-based SMT, dependecy-based SMT method and neural method (used subword) for Hindi?Marathi language pair. NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium  (Forcada et al., 2009-11) . Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has been devoted to it and the nature of the languages itself (the closer, the better). 

 UBC-NLP The UBC-NLP team participated in the SLT task on all the available language pairs. The team regularly used all the parallel data and trained 6-layer Transformer models based on the Fairseq library. Only for the Slovene-Croatian language pair the team performed backtranslation, noticing a 3 BLEU point improvement in the results. This team obtained better results with bilingual than with multilingual models (training a single model for all language groups). UPCTALP The UPCTALP participated in the Romance pairs. This team made use of the Transformer architecture improved with multilingual, back-translation and fine-tuning techniques. Each of this techniques improved over the previous one. WIPRO-RIT WIPRO-RIT submitted their system to the SLT 2020 Indo-Aryan track. The presented system is a single multilingual NMT system based on the transformer architecture that can translate between multiple languages. The presented model is inspired from the model described in  Johnson et al. (2017) . WIPRO-RIT achieved competitive performance ranking 1 st in Marathi to Hindi and 2 nd in Hindi to Marathi translation among 22 systems.in Hindi to Marathi translation among 22 systems. 

 Results We present results for the three language families: three different language families: Indo-Aryan (Hindi -Marathi), Romance (Spanish -Catalan, Spanish -Portuguese), and South-Slavic (Slovene -Croatian, Slovene -Serbian), all of them in the two possible directions. Like last year edision, the second edition of the Similar Translation Task evaluation was also performed on automatic basis using BLEU  (Papineni et al., 2002) , RIBES  (Isozaki et al., 2010)  and TER  (Snover et al., 2006)  measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group i.e. Hindi-Marathi (in both directions). We received 22 submissions from 14 teams. The best systems (INFOSYS) based on BLEU for Hindi-Marathi achieved score 18.26, however based on other evaluation matric WIPRO-RIT achieved the best 62.45 RIBES and around 72 TER (see Table  23 ). While in the other direction Marathi-Hindi the best performing system (WIPRO-RIT) reached 24.53 of BLEU and 66.39 on TER, but based on RIBES score 66.83, IITDELHI performed the best (see Table  24 ). Similarly to the previous edition of the SLT shared task, participants could submit systems for the Spanish-Portuguese language pair (in both directions). The best systems for Spanish-to-Portuguese achieved over 32 BLEU and around 52 TER. While in the opposite direction (Portugueseto-Spanish) the best performing system reached 33.82 of BLEU, but its TER score was 52.41, which is higher than in the case of best performing Spanish-to-Portuguese systems. As the Spanish-Catalan dev and test sets were aligned with Spanish-Portuguese ones, we noticed that the best results for the Spanish-Catalan language pair are in general much better than for Spanish-Portuguese. For Spanish-to-Catalan the best system attained over 86 BLEU and below 8 TER. In the case of Catalan-to-Spanish, the best systems scored around 77 BLEU and less than 15 TER. A new language group in this year's SLT task is the group of (Western) South Slavic languages -Slovene, Croatian and Serbian, forming two language pairs -Slovene-Croatian and Slovene-Serbian, with one additional twist given the very high mutual intelligibility of Croatian and Serbian. The best systems for Slovene-to-Croatian achieved 36 BLEU and 43 TER, which is significantly worse than the results of the same bestperforming system in the opposite direction -43 BLEU and 36 TER. On the Slovene-Serbian pair a similar phenomenon can be observed -Slovene to Serbian achieving 39 BLEU and 40 TER, while the opposite direction achieves 47 BLEU and 33 TER. The reason for such a significant lack of symmetry is the better performance of the systems translating into Slovene, probably given that (Croatian and Serbian) multi-source translation (into Slovene) is simpler than multi-target translation, which was, finally, propagated to the backtranslation procedure, increasing the difference between the directions even further. 

 Summary In this section, we presented the results of the WMT SLT 2020 task. The second iteration of this competition featured data from five language pairs from three different language families: Hindi-Marathi; Spanish-Catalan and Spanish-Portuguese; Sloven-Croatian and Slovene-Serbian. We evaluated the systems translating in both directions of the language pair using three automatic metrics: BLEU, RIBES, and TER. We observed that the performance varies widely between language pairs. For example. the best performing systems trained to translate between Catalan and Spanish in both directions obtained significantly higher results than those trained to translate between other language pairs. In terms of participation, SLT received system submissions from 18 teams. In the end of the competition, 14 teams wrote system description papers that appear in the WMT proceedings. The list of teams with references to the respective system description paper is presented in Table  22 . Finally, short summaries of each entry, based on the description provided by the participants, were also presented in this section. 

 Conclusion This paper presented the results of WMT20 news translation and similar language translation shared tasks, as well as the extra test suites added to the news translation task. Our main findings rank participating systems in their sentence-level and document-level translation quality, as assessed in a large-scale manual evaluation using the method of Direct Assessment (DA). For out-of-English language pairs, DA was modified so that the context of the whole document is available while judging individual sentences and assessors are allowed to return to any sentence judgement within the document. As in previous years, the effect of translationese (translating from a source which itself was produced in translation) was avoided except lowerresourced Inuktitut?English, Pashto?English, Khmer?English, and German?French by creating reference translations always in the same direction as the MT systems are run. Furthermore, 8 out-of-English language pairs would not need human reference for our evaluation at all because the assessors are evaluating translation candidates bilingually, comparing them to the source             text (as opposed to the reference) in these language pairs. The reference translations are nevertheless included as evaluation, hidden among participating MT systems. This year, English?German included two independent reference translations and one humanproduced paraphrase, and English?Chinese included two references. Each of these translations ended up significantly differing in quality from the other ones. In German?English and also Chinese?English and English?Inuktitut, some MT systems fall in the same cluster with human translation. The observed variance of human translation quality however demands modesty before making any claims about human parity. The need for cautious interpretation of the results is also strengthened by the fact that even in English?German and English?Czech where human translation was seemingly significantly surpassed in 2018 and/or 2019, the result is not confirmed this year. Furthermore and similarly to previous year, a test suite this year again suggests that some aspects of translation are not handled by current systems at all. This year all MT systems fall into the gender bias trap  and they tend to make more severe errors than humans  (Zouhar et al., 2020) . The results of the task on similar language translation indicate that the performance when translating between pairs of closely-related languages is extremely varied across different language pairs. The best performing systems trained to translate between Catalan and Spanish, for example, obtained significantly higher results in both directions than those trained to translate between other language pairs in terms of BLEU, RIBES, and TER. human evaluation via Mechanical Turk. We would like to thank Roland Kuhn for advising on the Inuktitut?English task organization and the Nunavut Maligaliurvia (Legislative Assembly of Nunavut) and Nunatsiaq News for supplying all the Inuktitut?English parallel data. The organizers of the similar languages task would like to thank Ciklopea and Bisnode for the Croatian, Serbian, and Slovene data, and Pangeanic for the Catalan, Portuguese, and Spanish data. The work of the organizers of the similar languages task is supported in part by the Spanish Ministerio de Ciencia e Innovaci?n, through the postdoctoral senior grant Ram?n y Cajal and by the Agencia Estatal de Investigaci?n through the projects EUR2019-103819, PCIN-2017-079 and PID2019-107579RB-I00 / AEI / 10.13039/501100011033. 
