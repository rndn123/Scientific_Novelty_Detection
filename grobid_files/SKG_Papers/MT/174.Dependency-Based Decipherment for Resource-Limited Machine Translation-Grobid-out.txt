title
Dependency-Based Decipherment for Resource-Limited Machine Translation

abstract
We introduce dependency relations into deciphering foreign languages and show that dependency relations help improve the state-ofthe-art deciphering accuracy by over 500%. We learn a translation lexicon from large amounts of genuinely non parallel data with decipherment to improve a phrase-based machine translation system trained with limited parallel data. In experiments, we observe BLEU gains of 1.2 to 1.8 across three different test sets.

Introduction State-of-the-art machine translation (MT) systems apply statistical techniques to learn translation rules from large amounts of parallel data. However, parallel data is limited for many language pairs and domains. In general, it is easier to obtain non parallel data. The ability to build a machine translation system using monolingual data could alleviate problems caused by insufficient parallel data. Towards building a machine translation system without a parallel corpus,  Klementiev et al. (2012)  use non parallel data to estimate parameters for a large scale MT system. Other work tries to learn full MT systems using only non parallel data through decipherment  (Ravi and Knight, 2011; Ravi, 2013) . However, the performance of such systems is poor compared with those trained with parallel data. Given that we often have some parallel data, it is more practical to improve a translation system trained on parallel corpora with non parallel data.  Dou and Knight (2012)  successfully apply decipherment to learn a domain specific translation lexicon from monolingual data to improve out-ofdomain machine translation. Although their approach works well for Spanish/French, they do not show whether their approach works for other language pairs. Moreover, the non parallel data used in their experiments is created from a parallel corpus. Such highly comparable data is difficult to obtain in reality. In this work, we improve previous work by  Dou and Knight (2012)  using genuinely non parallel data, and propose a framework to improve a machine translation system trained with a small amount of parallel data. As shown in Figure  1 , we use a lexicon learned from decipherment to improve translations of both observed and out-of-vocabulary (OOV) words. The main contributions of this work are: ? We extract bigrams based on dependency relations for decipherment, which improves the state-of-the-art deciphering accuracy by over 500%. ? We demonstrate how to improve translations of words observed in parallel data by using a translation lexicon obtained from large amounts of non parallel data. ? We show that decipherment is able to find correct translations for OOV words. ? We use a translation lexicon learned by deciphering large amounts of non parallel data to improve a phrase-based MT system trained with limited amounts of parallel data. In experiments, we observe 1.2 to 1.8 BLEU gains across three different test sets. 

 Previous Work Motivated by the idea that a translation lexicon induced from non parallel data can be applied to MT, a variety of prior research has tried to build a translation lexicon from non parallel or comparable data  (Rapp, 1995; Fung and Yee, 1998; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Bergsma and Van Durme, 2011; Daum? and Jagarlamudi, 2011; Irvine and Callison-Burch, 2013b; Irvine and Callison-Burch, 2013a) . Although previous work is able to build a translation lexicon without parallel data, little has used the lexicon to improve machine translation. There has been increasing interest in learning translation lexicons from non parallel data with decipherment techniques  (Ravi and Knight, 2011; Dou and Knight, 2012; Nuhn et al., 2012) . Decipherment views one language as a cipher for another and learns a translation lexicon that produces a good decipherment. In an effort to build a MT system without a parallel corpus,  Ravi and Knight (2011)  view Spanish as a cipher for English and apply Bayesian learning to directly decipher Spanish into English. Unfortunately, their approach can only work on small data with limited vocabulary.  Dou and Knight (2012)  propose two techniques to make Bayesian decipherment scalable. First, unlike  Ravi and Knight (2011) , who decipher whole sentences,  Dou and Knight (2012)  decipher bigrams. Reducing a ciphertext to a set of bigrams with counts significantly reduces the amount of cipher data. According to  Dou and Knight (2012) , a ciphertext bigram F is generated through the following generative story: ? Generate a sequence of two plaintext tokens e 1 e 2 with probability P (e 1 e 2 ) given by a language model built from large numbers of plaintext bigrams. ? Substitute e 1 with f 1 and e 2 with f 2 with probability P (f 1 |e 1 ) ? P (f 2 |e 2 ). The probability of any cipher bigram F is: P (F ) = e 1 e 2 P (e 1 e 2 ) 2 i=1 P (f i |e i ) Given a corpus of N cipher bigrams F 1 ...F N , the probability of the corpus is: P (corpus) = N j=1 P (F j ) Given a plaintext bigram language model, the goal is to manipulate P (f |e) to maximize P (corpus). Theoretically, one can directly apply EM to solve the problem  (Knight et al., 2006) . However, EM has time complexity O(N ? V 2 e ) and space complexity O(V f ? V e ), where V f , V e are the sizes of ciphertext and plaintext vocabularies respectively, and N is the number of cipher bigrams.  Ravi and Knight (2011)  apply Bayesian learning to reduce the space complexity. Instead of estimating probabilities P (f |e), Bayesian learning tries to draw samples from plaintext sequences given ciphertext bigrams. During sampling, the probability of any possible plaintext sample e 1 e 2 is given as:  P sample ( P bayes (f i |e i ) = ?P 0 (f i |e i ) + count(f i , e i ) ? + count(e i ) where P 0 is a base distribution, and ? is a parameter that controls how much we trust P 0 . count(f i , e i ) and count(e i ) record the number of times f i , e i and e i appear in previously generated samples respectively. At the end of sampling, P (f i |e i ) is estimated by: P (f i |e i ) = count(f i , e i ) count(e i ) However, Bayesian decipherment is still very slow with Gibbs sampling  (Geman and Geman, 1987) , as each sampling step requires considering V e possibilities.  Dou and Knight (2012)  solve the problem by introducing slice sampling  (Neal, 2000)  to Bayesian decipherment. 

 From Adjacent Bigrams to Dependency Bigrams A major limitation of work by  Dou and Knight (2012)  is their monotonic generative story for deciphering adjacent bigrams. While the generation process works well for deciphering similar languages (e.g. Spanish and French) without considering reordering, it does not work well for languages that are more different in grammar and word order (e.g. Spanish and English). In this section, we first look at why adjacent bigrams are bad for decipherment. Then we describe how to use syntax to solve the problem. The left column in Table  1  contains adjacent bigrams extracted from the Spanish phrase "misi?n de naciones unidas en oriente medio". The correct decipherment for the bigram "naciones unidas" should be "united nations". Since the deciphering model described by  Dou and Knight (2012)  does not consider word reordering, it needs to decipher the bigram into "nations united" in order to get the right word translations "naciones"?"nations" and "unidas"?"united". However, the English language model used for decipherment is built from English adjacent bigrams, so it strongly disprefers "nations united" and is not likely to produce a sensible decipherment for "naciones unidas". The Spanish bigram "oriente medio" poses the same problem. Thus, without considering word reordering, the model described by  Dou and Knight (2012)  is not a good fit for deciphering Spanish into English. However, if we extract bigrams based on dependency relations for both languages, the model fits better. To extract such bigrams, we first use dependency parsers to parse both languages, and extract bigrams by putting head word first, followed by the modifier.  1  We call these dependency bigrams. The right column in Table  1  lists examples of Spanish dependency bigrams extracted from the same Spanish phrase. With a language model built with English dependency bigrams, the same model used for deciphering adjacent bigrams is able to decipher Spanish dependency bigram "naciones(head) unidas(modifier)" into "nations(head) united(modifier)". We might instead propose to consider word reordering when deciphering adjacent bigrams (e.g. add an operation to swap tokens in a bigram). However, using dependency bigrams has the following advantages: ? First, using dependency bigrams avoids complicating the model, keeping deciphering efficient and scalable. ? Second, it addresses the problem of long distance reordering, which can not be modeled by swapping tokens in bigrams. Furthermore, using dependency bigrams allows us to use dependency types to further improve decipherment. Suppose we have a Spanish dependency bigram "accept?(verb) solicitud(object)". Then all of the following English dependency bigrams are possible decipherments: "accepted(verb) UN(subject)", "accepted(verb) government(subject)", "accepted(verb) request(object)". However, if we know the type of the Spanish dependency bigram and use a language model built with the same type in English, the only possible decipherment is "accepted(verb) request(object)". If we limit the search space, a system is more likely to find a better decipherment. 

 Deciphering Spanish Gigaword In this section, we compare dependency bigrams with adjacent bigrams for deciphering Spanish into English. 

 Data We use the Gigaword corpus for our decipherment experiments. The corpus contains news articles from different news agencies and is available in Spanish and English. We use only the AFP (Agence France-Presse) section of the corpus in decipherment experiments. We tokenize the corpus using tools that come with the Europarl corpus  (Koehn, 2005) . To shorten the time required for running different systems on large amounts of data, we keep only the top 5000 most frequent word types in both languages and replace all other word types with UNK. We also throw away lines with more than 40 tokens, as the Spanish parser  (Bohnet, 2010)  we use is slow when processing long sentences. After preprocessing, the corpus contains approximately 440 million tokens in Spanish and 350 million tokens in English. To obtain dependency bigrams, we use the Bohnet parsers  (Bohnet, 2010)  to parse both the Spanish and English version of the corpus. 

 Systems Three systems are evaluated in the experiments. We implement a baseline system, Adjacent, based on  Dou and Knight (2012) . The baseline system collects adjacent bigrams and their counts from Spanish and English texts. It then builds an English bigram language model using the English adjacent bigrams and uses it to decipher the Spanish adjacent bigrams. Dependency Types Group 1 Verb/Subject Group 2 Preposition/Preposition-Object, Noun/Noun-Modifier Group 3 Verb/Noun-Object We build the second system, Dependency, using dependency bigrams for decipherment. As the two parsers do not output the same set of dependency relations, we cannot extract all types of dependency bigrams. Instead, we select a subset of dependency bigrams whose dependency relations are shared by the two parser outputs. The selected dependency relations are: Verb/Subject, Verb/Noun-Object, Preposition/Object, Noun/Modifier. Decipherment runs the same way as in the baseline system. The third system, DepType, is built using both dependent bigrams and their dependency types. We first extract dependency bigrams for both languages, then group them based on their dependency types. As both parsers treat noun phrases dependent on "del", "de", and "of" as prepositional phrases, we choose to divide the dependency bigrams into 3 groups and list them in Table  2 . A separate language model is built for each group of English dependency bigrams and used to decipher the group of Spanish dependency bigrams with same dependency type. For all the systems, language models are built using the SRILM toolkit  (Stolcke, 2002) . For the Adjacent system, we use Good-Turing smoothing. For the other systems, we use a mix of Witten-Bell and Good-Turing smoothing. 

 Sampling Procedure In experiments, we find that the iterative sampling method described by  Dou and Knight (2012)  helps improve deciphering accuracy. We also find that combining results from different decipherments helps find more correct translations at each iteration. Thus, instead of using a single sampling process, we use 10 different sampling processes at each iteration. The details of the new sampling procedure are provided here: ? Extract dependency bigrams from parsing outputs and collect their counts. ? Keep bigrams whose counts are greater than a threshold ?. Then start 10 different randomly seeded and initialized sampling processes. Perform sampling. ? At the end of sampling, extract word translation pairs (f, e) from the final sample. Estimate translation probabilities P (e|f ) for each pair. Then construct a translation table by keeping translation pairs (f, e) seen in more than one decipherment and use the average P (e|f ) as the new translation probability. ? Lower the threshold ? to include more bigrams into the sampling process. Start 10 different sampling processes again and initialize the first sample using the translation pairs obtained from the previous step (for each Spanish token f, choose an English token e whose P (e|f ) is the highest). Perform sampling again. ? Repeat until ? = 1. 

 Deciphering Accuracy We choose the first 1000 lines of the monolingual Spanish texts as our test data. The data contains 37,505 tokens and 6556 word types. We use type accuracy as our evaluation metric: Given a word type f in Spanish, we find a translation pair (f, e) with the highest average P (e|f ) from the translation table learned through decipherment. If the translation pair (f, e) can also be found in a gold translation lexicon T gold , we treat the word type f as correctly deciphered. Let |C| be the number of word types correctly deciphered, and |V | be the total number of word types evaluated. We define type accuracy as |C| |V | . To create T gold , we use GIZA  (Och and Ney, 2003)  to align a small amount of Spanish-English parallel text (1 million tokens for each language), and use the lexicon derived from the alignment as our gold translation lexicon. T gold contains a subset of 4408 types seen in the test data, among which, 2878 are also top 5000 frequent word types. 

 Results During decipherment, we gradually increase the size of Spanish texts and compare the learning curves of three deciphering systems in Figure  2 . With 100k tokens of Spanish text, the performance of the three systems are similar. However, the learning curve of Adjacent plateaus quickly, while those of the dependency based systems soar up as more data becomes available and still rise sharply when the size of Spanish texts increases to 10 million tokens, where the DepType system improves deciphering accuracy of the Adjacent system from 4.2% to 24.6%. In the end, with 100 million tokens, the accuracy of the DepType system rises to 27.0%. The accuracy is even higher (41%), when evaluated against the top 5000 frequent word types only. 

 Improving Machine Translation with Decipherment In this section, we demonstrate how to use a translation lexicon learned by deciphering large amounts of in-domain (news) monolingual data to improve a phrase-based machine translation system trained with limited out-of-domain (politics) parallel data. 

 Data We use approximately one million tokens of the Europarl corpus  (Koehn, 2005)      3 . 

 Systems 

 Baseline Machine Translation System We build a state-of-the-art phrase-based MT system, PBMT, using Moses  (Koehn et al., 2007) . PBMT has 3 models: a translation model, a distortion model, and a language model. We build a 5gram language model using the AFP section of the English Gigaword. We train the other models using the Europarl corpus. By default, Moses uses the following 8 features to score a candidate translation: ? direct and inverse translation probabilities ? direct and inverse lexical weighting ? a language model score ? a distortion score ? phrase penalty ? word penalty The 8 features have weights adjusted on the tuning data using minimum error rate training (MERT)  (Och, 2003) . PBMT has a phrase table T phrase . During decoding, Moses copies out-of-vocabulary (OOV) words, which can not be found in T phrase , directly to output. In the following sections, we describe how to use a translation lexicon learned from large amounts of non parallel data to improve translation of OOV words, as well as words observed in T phrase . 

 Decipherment for Machine Translation To achieve better decipherment, we: ? Increase the size of Spanish ciphertext from 100 million tokens to 894 million tokens. ? Keep top 50k instead of top 5k most frequent word types of the ciphertext. ? Instead of seeding the sampling process randomly, we use a translation lexicon learned from a limited amount of parallel data as seed: For each Spanish dependency bigram f 1 , f 2 , where both f 1 and f 2 are found in the seed lexicon, we find the English sequence e 1 , e 2 that maximizes P (e 1 , e 2 )P (e 1 |f 1 )P (e 2 |f 2 ). Otherwise, for any Spanish token f that can be found in the seed lexicon, we choose English word e, where P (e|f ) is the highest as the initial sample; for any f that are not seen in the seed lexicon, we do random initialization. We perform 20 random restarts with 10k iterations on each and build a word-to-word translation lexicon T decipher by collecting translation pairs seen in at least 3 final decipherments with either P (f |e) ? 0.2 or P (e|f ) ? 0.2. 

 Improving Translation of Observed Words with Decipherment To improve translation of words observed in our parallel corpus, we simply use T decipher as an additional parallel corpus. First, we filter T decipher by keeping only translation pairs (f, e), where f is observed in the Spanish part and e is observed in the English part of the parallel corpus. Then we append all the Spanish and English words in the filtered T decipher to the end of Spanish part and English part of the parallel corpus respectively. The training and tuning process is the same as the baseline machine translation system PBMT. We denote this system as Decipher-OBSV. 

 Improving OOV translation with Decipherment As T decipher is learned from large amounts of indomain monolingual data, we expect that T decipher contains a number of useful translations for words not seen in the limited amount of parallel data (OOV words). Instead of copying OOV words directly to output, which is what Moses does by default, we try to find translations from T decipher to improve translation. During decoding, if a source word f is in T phrase , its translation options are collected from T phrase exclusively. If f is not in T phrase but in T decipher , the decoder will find translations from T decipher . If f is not in either translation table, the decoder just copies it directly to the output. We call this system Decipher-OOV. However, when an OOV's correct translation is same as its surface form and all its possible translations in T decipher are wrong, it is better to just copy OOV words directly to output. This scenario happens frequently, as Spanish and English share many common words. To avoid over trusting T decipher , we add a new translation pair (f, f ) for each source word f in T decipher if the translation pair (f, f ) is not originally in T decipher . For each newly added translation pair, both of its log translation probabilities are set to 0. To distinguish the added translation pairs from the others learned through decipherment, we add a binary feature ? to each translation pair in T decipher . The final version of T decipher has three feature scores: P (e|f ), P (f |e), and ?. Finally, we tune weights of the features in T decipher using MERT  (Och, 2003)  on the tuning set. 

 A Combined Approach In the end, we build a system Decipher-COMB, which uses T decipher to improve translation of both observed and OOV words with methods described in sections 5.2.3 and 5.2.4. 

 Results We tune each system three times with MERT and choose the best weights based on BLEU scores on tuning set. Table  4  shows that the translation lexicon learned from decipherment helps achieve higher BLEU scores across tuning and testing sets. Decipher-OBSV improves BLEU scores by as much as 1.2 points. We analyze the results and find the gain mainly comes from two parts. First, adding T decipher to small amounts of parallel corpus improves word level translation probabilities, which lead to better lexical weighting; second, T decipher contains new alternative translations for words observed in the parallel corpus. Moreover, Decipher-OOV also achieves better BLEU scores compared with PBMT across all tuning and test sets. We also observe that systems using T decipher learned by deciphering dependency bigrams leads to larger gains in BLEU scores. When decipherment is used to improve translation of both observed and OOV words, we see improvement in BLEU score as high as 1.8 points on the 2010 news test set. The consistent improvement on the tuning and different testing data suggests that decipherment is capable of learning good translations for a number of OOV words. To further demonstrate that our decipherment approach finds useful translations for OOV words, we list the top 10 most frequent OOV words from both the tuning set and testing set as well as their translations (up to three most likely translations) in Table  5 . P (e|f ) and P (f |e) are average scores over different decipherment runs. From the table, we can see that decipherment finds correct translations (bolded) for 7 out of the 10 most frequent OOV words. Moreover, many OOVs and their correct translations are homographs , which makes copying OOVs directly to the output a strong baseline to beat. Nonetheless, decipherment still finds enough correct translations to improve the baseline. 

 Conclusion We introduce syntax for deciphering Spanish into English. Experiment results show that using dependency bigrams improves decipherment accuracy by over 500% compared with the state-of-the-art approach. Moreover, we learn a domain specific translation lexicon by deciphering large amounts of monolingual data and show that the lexicon can improve a baseline machine translation system trained with limited parallel data.  Figure 1 : 1 Figure 1: Improving machine translation with decipherment (Grey boxes represent new data and process). Mono: monolingual; LM: language model; LEX: translation lexicon; TM: translation model. 
