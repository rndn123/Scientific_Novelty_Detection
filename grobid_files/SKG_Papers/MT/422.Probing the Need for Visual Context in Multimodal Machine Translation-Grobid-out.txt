title
Probing the Need for Visual Context in Multimodal Machine Translation

abstract
Current work on multimodal machine translation (MMT) has suggested that the visual modality is either unnecessary or only marginally beneficial. We posit that this is a consequence of the very simple, short and repetitive sentences used in the only available dataset for the task (Multi30K), rendering the source text sufficient as context. In the general case, however, we believe that it is possible to combine visual and textual information in order to ground translations. In this paper we probe the contribution of the visual modality to state-of-the-art MMT models by conducting a systematic analysis where we partially deprive the models from source-side textual context. Our results show that under limited textual context, models are capable of leveraging the visual input to generate better translations. This contradicts the current belief that MMT models disregard the visual modality because of either the quality of the image features or the way they are integrated into the model.

Introduction Multimodal Machine Translation (MMT) aims at designing better translation systems which take into account auxiliary inputs such as images. Initially organized as a shared task within the First Conference on Machine Translation (WMT16) , MMT has so far been studied using the Multi30K dataset , a multilingual extension of Flickr30K  (Young et al., 2014)  with translations of the English image descriptions into German, French and Czech  (Elliott et al., 2017; . The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows: (i) multimodal attention using convolutional features  (Caglayan et al., 2016; Calixto et al., 2016; Libovick? and Helcl, 2017; Helcl et al., 2018)  (ii) cross-modal interactions with spatially-unaware global features  (Calixto and Liu, 2017; Ma et al., 2017; Caglayan et al., 2017a; Madhyastha et al., 2017)  and (iii) the integration of regional features from object detection networks  (Huang et al., 2016; Gr?nroos et al., 2018) . Nevertheless, the conclusion about the contribution of the visual modality is still unclear:  Gr?nroos et al. (2018)  consider their multimodal gains "modest" and attribute the largest gain to the usage of external parallel corpora.  observe that their multimodal word-sense disambiguation approach is not significantly different than the monomodal counterpart. The organizers of the latest edition of the shared task concluded that the multimodal integration schemes explored so far resulted in marginal changes in terms of automatic metrics and human evaluation . In a similar vein, Elliott (2018) demonstrated that MMT models can translate without significant performance losses even in the presence of features from unrelated images. These empirical findings seem to indicate that images are ignored by the models and hint at the fact that this is due to representation or modeling limitations. We conjecture that the most plausible reason for the linguistic dominance is that -at least in Multi30K -the source text is sufficient to perform the translation, eventually preventing the visual information from intervening in the learning process. To investigate this hypothesis, we introduce several input degradation regimes (Section 2) and revisit state-of-the-art MMT models (Section 3) to assess their behavior under degraded regimes. We further probe the visual sensitivity by deliberately feeding features from unrelated images. Our results (Section 4) show that MMT models successfully exploit the visual modality when the linguistic context is scarce, but indeed tend to be less sensitive to this modality when exposed to complete sentences. 

 Input Degradation In this section we propose several degradations to the input language modality to simulate conditions where sentences may miss crucial information. We denote a set of translation pairs by D and indicate degraded variants with subscripts. Both the training and the test sets are degraded. Color Deprivation. We consistently replace source words that refer to colors with a special token [v] (D C in Table  1 ). Our hypothesis is that a monomodal system will have to rely on sourceside contextual information and biases, while a multimodal architecture could potentially capitalize on color information extracted by exploiting the image and thus obtain better performance. This affects 3.3% and 3.1% of the words in the training and the test set, respectively. Entity Masking. The Flickr30K dataset, from which Multi30K is derived, has also been extended with coreference chains to tag mentions of visually depictable entities in image descriptions  (Plummer et al., 2015) . We use these to mask out the head nouns in the source sentences (D N in Table  1 ). This affects 26.2% of the words in both the training and the test set. We hypothesize that a multimodal system should heavily rely on the images to infer the missing parts. Progressive Masking. A progressively degraded variant D k replaces all but the first k tokens of source sentences with  [v]  . Unlike the color deprivation and entity masking, masking out suffixes does not guarantee systematic removal of visual context, but rather simulates an increasingly low-resource scenario. Overall, we form 16 degraded variants D k (Table  1 ) where k ? {0, 2, . . . , 30}. We stop at D 30 since 99.8% of the sentences in Multi30K are shorter than 30 words with an average sentence length of 12 words. D 0 -where the only remaining information is the source sentence length -is an interesting case from two perspectives: a neural machine translation (NMT) model trained on it resembles a target language model, while an MMT model becomes an image captioner with access to "expected length information". Visual Sensitivity. Inspired by  Elliott (2018) , we experiment with incongruent decoding in order to understand how sensitive the multimodal systems are to the visual modality. This is achieved D a lady in a blue dress singing D C a lady in a [v] dress singing D N a [v] in a blue [v] singing D 4 a lady in a [v] [v] [v] D 2 a lady [v] [v] [v] [v] [v] D 0 [v] [v] [v] [v] [v] [v] [v] Table  1 : An example of the proposed input degradation schemes: D is the original sentence. by explicitly violating the test-time semantic congruence across modalities. Specifically, we feed the visual features in reverse sample order to break image-sentence alignments. Consequently, a model capable of integrating the visual modality would likely deteriorate in terms of metrics. 

 Experimental Setup Dataset. We conduct experiments on the English?French part of Multi30K. The models are trained on the concatenation of the train and val sets (30K sentences) whereas test2016 (dev) and test2017 (test) are used for early-stopping and model evaluation, respectively. For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities. We use word-level vocabularies of 9,951 English and 11,216 French words. We use Moses  (Koehn et al., 2007)  scripts to lowercase, normalize and tokenize the sentences with hyphen splitting. The hyphens are stitched back prior to evaluation. Visual Features. We use a ResNet-50 CNN  (He et al., 2016)  trained on ImageNet  (Deng et al., 2009)  as image encoder. Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a center crop of size 256x256. We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L 2 normalization along the depth dimension  (Caglayan et al., 2018) . For the non-attentive model, we use the 2048-dimensional global average pooled version (pool5) of the above convolutional features. Models. Our baseline NMT is an attentive model  with a 2-layer bidirectional GRU encoder  and a 2-layer conditional GRU decoder  (Sennrich et al., 2017) . The second layer of the decoder receives the output of the attention layer as input. For the MMT model, we explore the basic multimodal attention (DIRECT)  (Caglayan et al., 2016)  and its hierarchical (HIER) extension  (Libovick? and Helcl, 2017) . The former linearly projects the concatenation of textual and visual context vectors to obtain the multimodal context vector, while the latter replaces the concatenation with another attention layer. Finally, we also experiment with encoder-decoder initialization (INIT)  (Calixto and Liu, 2017; Caglayan et al., 2017a)  where we initialize both the encoder and the decoder using a non-linear transformation of the pool5 features. Hyperparameters. The encoder and decoder GRUs have 400 hidden units and are initialized with 0 except the multimodal INIT system. All embeddings are 200-dimensional and the decoder embeddings are tied  (Press and Wolf, 2016) . A dropout of 0.4 and 0.5 is applied on source embeddings and encoder/decoder outputs, respectively  (Srivastava et al., 2014) . The weights are decayed with a factor of 1e?5. We use ADAM  (Kingma and Ba, 2014)  with a learning rate of 4e?4 and mini-batches of 64 samples. The gradients are clipped if the total norm exceeds 1  (Pascanu et al., 2013) . The training is early-stopped if dev set ME-TEOR  (Denkowski and Lavie, 2014)  does not improve for ten epochs. All experiments are conducted with nmtpytorch 1  (Caglayan et al., 2017b) . 

 Results We train all systems three times each with different random initialization in order to perform significance testing with multeval  (Clark et al., 2011) . Throughout the section, we always report the mean over three runs (and the standard deviation) of the considered metrics. We decode the translations with a beam size of 12. Figure  1 : Entity masking: all masked MMT models are significantly better than the masked NMT (dashed). Incongruent decoding severely worsens all systems. The vanilla NMT baseline is 75.9 2 . We first present test2017 METEOR scores for the baseline NMT and MMT systems, when trained on the full dataset D (Table  2 ). The first column indicates that, although MMT models perform slightly better on average, they are not significantly better than the baseline NMT. We now introduce and discuss the results obtained under the proposed degradation schemes. Please refer to Table  5  and the appendix for qualitative examples. 

 Color Deprivation Unlike the inconclusive results for D, we observe that all MMT models are significantly better than NMT when color deprivation is applied (D C in Table  2 ). If we further focus on the subset of the test set subjected to color deprivation (247 sentences), the gain increases to 1.6 METEOR for HIER. For the latter subset, we also computed the average color accuracy per sentence and found that the attentive models are 12% better than the NMT (32.5?44.5) whereas the INIT model only brings 4% (32.5?36.5) improvement. This shows that more complex MMT models are better at integrating visual information to perform better. 

 Entity Masking The gains are much more prominent with entity masking, where the degradation occurs at a larger scale: Attentive MMT models show up to 4.2 ME-TEOR improvement over NMT (Figure  1 ). We observed a large performance drop with incongruent decoding, suggesting that the visual modality is Czech +1.4 (? 2.9) +1.7 (? 3.5) +1.7 (? 4.1) German +2.1 (? 4.7) +2.5 (? 5.9) +2.7 (? 6.5) French +3.4 (? 6.5) +3.9 (? 9.0) +4.2 (? 9.7) Table  3 : Entity masking results across three languages: all MMT models perform significantly better than their NMT counterparts (p-value ? 0.01). The incongruence drop applies on top of the MMT score. now much more important than previously demonstrated  (Elliott, 2018) . A comparison of attention maps produced by the baseline and masked MMT models reveals that the attention weights are more consistent in the latter. An interesting example is given in Figure  2  where the masked MMT model attends to the correct region of the image and successfully translates a dropped word that was otherwise a spelling mistake ("son"?"song"). Czech and German. In order to understand whether the above observations are also consistent across different languages, we extend the entity masking experiments to German and Czech parts of Multi30K. Table  3  shows the gain of each MMT system with respect to the NMT model and the subsequent drop caused by incongruent decoding 3 . First, we see that the multimodal benefits clearly hold for German and Czech, although the gains are lower than for French 4 . Second, when we compute the average drop from using incongruent images across all languages, we see how conservative the INIT system is (? 4.7) compared  Blinding ? 3.9 ? 2.9 ? 0.4 ? 0.5 ? 0.3 NMT ? 3.7 ? 2.6 ? 0.6 ? 0.2 ? 0.3 Table  4 : The impact of incongruent decoding for progressive masking: all METEOR differences are against the DIRECT model. The blinded systems are both trained and decoded using incongruent features. to HIER (? 6.1) and DIRECT (? 6.8). This raises a follow-up question as to whether the hidden state initialization eventually loses its impact throughout the recurrence where, as a consequence, the only modality processed is the text. 

 Progressive Masking Finally, we discuss the results of the progressive masking experiments for French. Figure  3  clearly shows that as the sentences are progressively degraded, all MMT systems are able to leverage the visual modality. When the multimodal task becomes image captioning at k=0, MMT models improve over the language-model counterpart by ?7 METEOR. Further qualitative examples show that the systems perform surprisingly well by producing visually plausible sentences (see Table  5  and the Appendix). To get a sense of the visual sensitivity, we pick the DIRECT models trained on four degraded variants and perform incongruent decoding. We notice that as the amount of linguistic information increases, the gap narrows down: the MMT system gradually becomes less perplexed by the incongruence or, put in other words, less sensitive to the visual modality (Table  4 ). 

 SRC: an older woman in [v][v][v][v][v][v][v][v][v][v][v] NMT: une femme ?g?e avec un t-shirt blanc et des lunettes de soleil est assise sur un banc (an older woman with a white t-shirt and sunglasses is sitting on a bank) MMT: une femme ?g?e en maillot de bain rose est assise sur un rocher au bord de l'eau (an older woman with a pink swimsuit is sitting on a rock at the seaside) REF: une femme ?g?e en bikini bronze sur un rocher au bord de l'oc?an (an older woman in bikini is tanning on a rock at the edge of the ocean) Table  5 : Qualitative examples from progressive masking, entity masking and color deprivation, respectively. Underlined and bold words highlight the bad and good lexical choices. MMT is an attentive system. SRC: a young [v] in [v] We then conduct a contrastive "blinding" experiment where the DIRECT models are not only fed with incongruent features at decoding time but also trained with them from scratch. The results suggest that the blinded models learn to ignore the visual modality. In fact, their performance is equivalent to NMT models. 

 Discussion and Conclusions We presented an in-depth study on the potential contribution of images for multimodal machine translation. Specifically, we analysed the behavior of state-of-the-art MMT models under several degradation schemes in the Multi30K dataset, in order to reveal and understand the impact of textual predominance. Our results show that the models explored are able to integrate the visual modality if the available modalities are complementary rather than redundant. In the latter case, the primary modality (text) sufficient to accomplish the task. This dominance effect corroborates the seminal work of  Colavita (1974)  in Psychophysics where it has been demonstrated that visual stimuli dominate over the auditory stimuli when humans are asked to perform a simple audiovisual discrimination task. Our investigation using source degradation also suggests that visual grounding can increase the robustness of machine translation systems by mitigating input noise such as errors in the source text. In the future, we would like to devise models that can learn when and how to integrate multiple modalities by taking care of the complementary and redundant aspects of them in an intelligent way.     Figure 2 : 2 Figure 2: Baseline MMT (top) translates the misspelled "son" while the masked MMT (bottom) correctly produces "enfant" (child) by focusing on the image. 

 Figure 3 : 3 Figure 3: Multimodal gain in absolute METEOR for progressive masking: the dashed gray curve indicates the percentage of non-masked words in the training set. 

 holding a tennis [v] NMT: un jeune garc ?on en bleu tenant une raquette de tennis (a young boy in blue holding a tennis racket) MMT: une jeune femme en blanc tenant une raquette de tennis REF: une jeune femme en blanc tenant une raquette de tennis (a young girl in white holding a tennis racket) SRC: little girl covering her face with a [v] towel NMT: une petite fille couvrant son visage avec une serviette blanche (a little girl covering her face with a white towel) MMT: une petite fille couvrant son visage avec une serviette bleue REF: une petite fille couvrant son visage avec une serviette bleue (a little girl covering her face with a blue towel) 

 Figure 4 : 4 Figure 4: Attention example from entity masking experiments: (a) Baseline MMT translates the misspelled "son" (song ? chanson) while (b) the masked MMT achieves a correct translation ([v]? enfant) by exploiting the visual modality. 

 Figure 5 : 5 Figure 5: Attention example from entity masking experiments where terrier, grass and fence are dropped from the source sentence: (a) Baseline MMT is not able to shift attention from the salient dog to the grass and fence, (b) the attention produced by the masked MMT first shifts to the background area while translating "on lush green [v]" then focuses on the fence. 

 Table 6 : 6 Color deprivation examples from the English?French models: bold indicates correctly predicted cases. The colors generated by the models are shown in English for the sake of clarity. SRC: a girl in [v] is sitting on a bench NMT: pink Init: pink Hier: black Direct: black SRC:a man dressed in[v] talking to a girl NMT: black Init: black Hier: white Direct: white SRC: a [v] dog sits under a [v] umbrella NMT: brown / blue Init: black / blue Hier: black / blue Direct: black / blue SRC: a woman in a [v] top is dancing as a woman and boy in a [v] shirt watch NMT: blue / blue Init: blue / blue Hier: red / red Direct: red / red SRC: three female dancers in [v] dresses are performing a dance routine NMT: white Init: white Hier: white Direct: blue(a) Baseline (non-masked) MMT 

			 github.com/lium-lst/nmtpytorch 

			 Since entity masking uses Flickr30K splits (Section 3) rather than our splits, the scores are not comparable to those from other experiments in this paper. 

			 For example, the INIT system for French (Figure1) surpasses the baseline (50.5) by reaching 53.9 (+3.4), which ends up at 47.4 (? 6.5) after incongruent decoding.4 This is probably due to the morphological richness of DE and CS which is suboptimally handled by word-level MT.
