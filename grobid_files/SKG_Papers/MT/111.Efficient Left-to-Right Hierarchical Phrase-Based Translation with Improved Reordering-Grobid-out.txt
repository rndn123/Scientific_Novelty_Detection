title
Efficient Left-to-Right Hierarchical Phrase-based Translation with Improved Reordering

abstract
Left-to-right (LR) decoding  (Watanabe et al., 2006b ) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n 2 b) for input of n words and beam size b, compared to O(n 3 ) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in  (Watanabe et al., 2006b) . Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model.

Introduction Hiero  (Chiang, 2007)  models translation using a lexicalized synchronous context-free grammar (SCFG) extracted from word aligned bitexts. Typically, CKY-style decoding is used for Hiero with time complexity O(n 3 ) for source input with n words. Scoring the target language output using a language model within CKY-style decoding requires two histories per hypothesis, one on the left edge of each span and one on the right, due to the fact that the target side is not generated in left to right order, but rather built bottom-up from sub-spans. This leads to complex problems in efficient language model integration and requires state reduction techniques  Heafield et al., 2013) . The size of a Hiero SCFG grammar is typically larger than phrase-based models extracted from the same data creating challenges in rule extraction and decoding time especially for larger datasets . In contrast, the LR-decoding algorithm could avoid these shortcomings such as faster time complexity, reduction in the grammar size and the simplified left-to-right language model scoring. It means LR decoding has the potential to replace CKY decoding for Hiero. Despite these attractive properties, we show that the original LR-Hiero decoding proposed by  (Watanabe et al., 2006b)  does not perform to the same level of the standard CKY Hiero with cube pruning (see Table  3 ). In addition, the current LR decoding algorithm does not obtain BLEU scores comparable to phrase-based or CKYbased Hiero models for different language pairs (see Table  4 ). In this paper we propose modifications to the LR decoding algorithm that addresses these limitations and provides, for the first time, a true alternative to the standard CKY Hiero algorithm that uses left-to-right decoding. We introduce a new extended version of the LR decoding algorithm presented in  (Watanabe et al., 2006b)  which is demonstrably more efficient than the CKY Hiero algorithm. We measure the efficiency of the LR Hiero decoder in a way that is independent of the choice of system and programming language by measuring the number of language model queries. Although more efficient, the new LR decoding algorithm suffered from lower BLEU scores compared to CKY Hiero. Our analysis of left to right decoding showed that it has more potential for search errors due to early pruning of good hypotheses. This is unlike bottom-up decoding (CKY) which keeps best hypotheses for each span. To address this issue, we introduce two novel features into the Hiero SMT model that deal with reordering and distortion. Our experiments show that LR decoding with these features using prefix lexi-calized target side rules equals the scores obtained by CKY decoding with prefix lexicalized target side rules and phrase-based translation system. It performs four times fewer language model queries on average, compare to CKY Hiero decoding with unrestricted Hiero rules: 6466.7 LM queries for CKY Hiero (with cube pruning) compared to 1500.45 LM queries in LR Hiero (with cube pruning). While translation quality suffers by only about 0.67 in BLEU score on average, across two different language pairs. 2 Left-to-Right Decoding for Hiero Hierarchical phrase-based SMT  (Chiang, 2005; Chiang, 2007 ) uses a synchronous context free grammar (SCFG), where the rules are of the form X ? ?, ? , where X is a non-terminal, ? and ? are strings of terminals and non-terminals.  Chiang (2007)  places certain constraints on the extracted rules in order to simplify decoding. This includes limiting the maximum number of nonterminals (rule arity) to two and disallowing any rule with consecutive non-terminals on the foreign language side. It further limits the length of the initial phrase-pair as well as the number of terminals and non-terminals in the rule. For translating sentences longer than the maximum phrase-pair length, the decoder relies on additional glue rules S ? X, X and S ? SX, SX that allows monotone combination of phrases. The glue rules are used when no rules could match or the span length is larger than the maximum phrase-pair length. 

 Rule Extraction for LR Decoding Left-to-right Hiero  (Watanabe et al., 2006b)  generates the target hypotheses left to right, but for synchronous context-free grammar (SCFG) as used in Hiero. The target-side rules are constrained to be prefix lexicalized. These constrained SCFG rules are defined as: X ? ?, b ? (1) where ? is a mixed string of terminals and nonterminals. b is a terminal sequence prefixed to the possibly empty non-terminal sequence ?. For the sake of simplicity, We refer to these type of rules as GNF rules 1 in this paper. Rule extraction is similar to Hiero, except any rules violating GNF form on the target side are excluded. Rule extraction considers each smaller source-target phrase pair within a larger phrase pair and replaces the spans with non-terminal X, yielding hierarchical rules. Figure  1 (a) shows a wordaligned German-English sentence with a phrase pair ihre arbeit noch nicht gemacht haben, have not yet done their work that will lead to a SCFG rule. Given other smaller phrases (marked by bars above the source side), we extract a GNF rule 2 : X ? X 1 noch nicht X 2 haben, have not yet X 2 X 1 (2) In order to avoid data sparsity and for better generalization,  Watanabe et al. (2006b)  adds four glue rules for each lexical rule f , ? which are analogous to the glue rules defined in  (Chiang, 2007)  (see above) except that these glue rules for LR decoding allow reordering as well. X ? f X 1 , ?X 1 X ? X 1 f X 2 , ?X 1 X 2 X ? X 1 f , ?X 1 X ? X 1 f X 2 , ?X 2 X 1 (3) It might appear that the restriction that target-side rules be GNF is a severe restriction on the coverage of possible hypotheses compared to the full set of rules permitted by the Hiero extraction heuristic. However there is some evidence in the literature that discontinuous spans on the source side in translation rules is a lot more useful than discontinuous spans in the target side (which is disallowed in the GNF). For instance,  (Galley and Manning, 2010 ) do an extensive study of discontinuous spans on source and target side and show that source side discontinuous spans are very useful but removing discontinuous spans on the target side only lowers the BLEU score by 0.2 points (using the Joshua SMT system on Chinese-English). Removing discontinuous spans means that the target side rules have the form: uX, Xu, XuX, XXu, or uXX of which we disallow Xu, XuX, XXu.  Zhang and Zong (2012)  also conduct a study on discontinuous spans on source and target side of Hiero rules and conclude that source discontinuous spans are always more useful than discontinuities on the target side with experiments on four language pairs (zh-en, fren, de-en and es-en). As we shall also see in our experimental results (see Table  4 ) we can get close to the BLEU scores obtained using the full set of Hiero rules by using only target lexicalized rules in our LR decoder. 

 LR-Hiero Decoding LR-Hiero decoding uses a top-down depth-first search, which strictly grows the hypotheses in target surface ordering. Search on the source side follows an Earley-style search  (Earley, 1970) , the dot jumps around on the source side of the rules based on the order of nonterminals on the target side. This search is integrated with beam search or cube pruning to efficiently find the k-best translations. Several important details about the algorithm of LR-Hiero decoding are implicit and unexplained in  (Watanabe et al., 2006b) . In this section we describe the LR-Hiero decoding algorithm in more detail than the original description in  (Watanabe et al.,  Algorithm 1: LR-Hiero Decoding 1: Input sentence: f = f 0 f 1 . . . f n 2: F = FutureCost(f ) (Precompute future cost for spans) 3: for i = 0, . . . , n do 4: S i = {} (Create empty stacks) 5: h 0 = ( s , [[0, n]], ?, F [0,n] ) (Initial hypothesis 4-tuple) 6: Add h 0 to S 0 (Push initial hyp into first Stack) 7: for i = 0, . . . , n ? 1 do for r ? R do 12: h = GrowHypothesis(h, r, [u, v], F) (New hypothesis) 13: Add h to S l , where l = |h cov | (Add new hyp to stack) 14: return arg max(S n ) 15: GrowHypothesis(h, r, [u, v], F) 16: h = (h t = ?, h s = h s , h cov = ?, h c = 0) 17: r X = {X j , X k , . . . |j k . . .} (Get NTs in surface order) 18: for each X in reverse(r X ) do 19: push(h s , span(X)) (Push uncovered spans to LIFO list) 20: h t = Concatenate(h t , r t ) 21: h cov = UpdateCoverage(h cov , r s ) 22: h c = ComputeCost(g(h ), F ?h cov ) 23: return h 2006b). We explain our own modified algorithm for LR decoding with cube pruning in Section 2.3. Algorithm 1 shows the pseudocode for LR decoding. Decoding the example in Figure  1 (b) is explained using a walk-through shown in Fig- ure 2. Each partial hypothesis h is a 4-tuple (h t , h s , h cov , h c ): consisting of a translation prefix h t , a (LIFO-ordered) list h s of uncovered spans, source words coverage set h cov and the hypothesis cost h c . The initial hypothesis is a null string with just a sentence-initial marker s and the list h s containing a span of the whole sentence, [0, n]. The hypotheses are stored in stacks S 0 , . . . , S n , where each stack corresponds to a coverage vector of same size, covering same number of source words  (Koehn et al., 2003) . At the beginning of beam search the initial hy- pothesis h 0 is added to the decoder stack S 0 (line 6 in Algoorithm 1). Hypotheses in each decoder stack are expanded iteratively, generating new hypotheses, which are added to the latter stacks corresponding to the number of source words covered. In each step it pops from the LIFO list h s , the span [u, v] of the next hypothesis h to be processed. All rules that match the entire span [u, v] are then obtained efficiently via pattern matching  (Lopez, 2007) . GetSpanRules addresses possible ambiguities in matched rules to the given span  [u, v] . For example, given a rule r, with source side r s : X 1 the X 2 and source phrase p : ok, the more the better . There is ambiguity in matching r to p. GetSpanRules returns a distinct matched rule for each possible matching. ? X ? schuler ihre arbeit noch nicht gemacht haben .? schuler ? X 1 1 ?ihre arbeit noch nicht gemacht haben .? schuler ? X 1 2 ?ihre arbeit noch nicht gemacht ? haben X 2 2 ?.? schuler X 1 3 ?ihre arbeit ? noch nicht ? X 2 3 ? gemacht? haben X 2 2 ?.? schuler ? X 1 3 ?ihre arbeit ? noch nicht gemacht haben X 2 2 ?.? schuler ihre arbeit noch nicht gemacht haben ? X 2 2 ?.? schuler ihre arbeit noch nicht gemacht haben . 1) X ? schuler X 1 / students X 1 ? 2) X ? X 1 heban X 2 / have X 1 X 2 ? 3 ) X ? X 1 noch nicht X 2 /not yet X 2 X 1 ? 4 ) X ? gemacht /done ? 5 ) X ? ihre arbeit / their work ? 6 ) X ? ./. ? [0,8] students [1,8 ] students have [1,6 ][7,8] students have not yet [5,6][1,3 ][7,8] students have not yet done [1,3 ][7, The GrowHypothesis routine creates a new candidate by expanding given hypothesis h using rule r and computes the complete hypothesis score including language model score. Since the target-side rules are in GNF, the translation prefix of the new hypothesis is obtained by simply concatenating the terminal prefixes of h and r in same order (line 20). UpdateCoverage updates source word coverage set using the source side of r. The h s list is built by pushing the non-terminal spans of rule r in a reverse order (lines 17 and 18). The reverse ordering maintains the left-to-right generation of the target side. In the walk-through in Figure  2 , the derivation process starts by expanding the initial hypothesis h 0 (first item in the right pane of Fig 2  )  with the rule (rule #1 in left pane) to generate a new partial candidate having a terminal prefix of s students (second item in right pane). The second item in the middle pane shows the current position of the parser employing Earley's dot notation, indicating that the first word has already been translated. Now the decoder considers the second hypothesis and pops the span  [1, 8] . It then matches the rule (#2) and pushes the spans [1, 6] and  [7, 8]  into the list h s in the reverse order of their appearance in the target-side rule. At each step the new hypothesis is added to the decoder stack S l depending on the number of covered words in the new hypothesis (line 13 in Algorithm 1). For pruning we use an estimate of the future cost 3 of the spans uncovered by current hypothesis together with the hypothesis cost. The future cost is precomputed (line 2 Algorithm 1) in a way similar to the phrase-based models  (Koehn et al., 2007)  using only the terminal rules of the grammar. The ComputeCost method (line 22 in Algorithm 1) uses the usual log-linear model and scores a hypothesis based on its different feature scores g(h ) and the future cost of the yet to be covered spans (F ?h cov ). Time complexity of left to right Hiero decoding with beam search is O(n 2 b) in practice where n is the length of source sentence and b is the size of beam  (Huang and Mi, 2010) . 

 LR-Hiero Decoding with Cube Pruning The Algorithm 1 presented earlier does an exhaustive search as it generates all possible partial translations for a given stack that are reachable from the hypotheses in previous stacks. However only a few of these hypotheses are retained, while majority of them are pruned away. The cube pruning technique  (Chiang, 2007)  avoids the wasteful generation of poor hypotheses that are likely to be pruned away by efficiently restricting the generation to only high scoring partial translations. We modify the cube pruning for LR-decoding that takes into account the next uncovered span to for R s ? R do 14: cube = [g hyps , R s ] 15: Add cube to cubeList 16: S i = Merge(cubeList, F) (Create stack S i and add new hypotheses to it, see Figure  3 ) 17: return arg max(S n ) 18: Merge(CubeList, F) return hypList be translated indicated by the Earley's dot notation. The Algorithm 2 shows the pseudocode for LR-decoding using cube pruning. The structure of stacks and hypotheses and computing the future cost is similar to Algorithm 1 (lines 1-5). To fill stack S i , it iterates over previous stacks (line 8 in Algorithm 2) 4 . All hypotheses in each stack S p (covering p words on the source-side) are first partitioned into a set of groups, {G}, based on their first uncovered span (line 9) 5 . Each group g is a 2-tuple (g span , g hyps ), where g hyps is a list of hypotheses which share the same first uncovered span g span . Rules matching the span g span are obtained from routine GetSpanRules, which are then grouped based on unique source side rules (i.e. each R s contains rules that share the same source side s but have different target sides). Each g hyps and possible R s 6 create a cube which is added to cubeList. 19: heapQ = {} 20: for each (H, R) in cubeList do 21: [u, v] = span of rule R 22: h = GrowHypothesis(h 1 , r 1 , [u, v], F) (from Algorithm In LR-Hiero, each hypothesis is developed with only one uncovered span, therefore each cube always has just two dimensions: (1) hypotheses with the same number of covered words and similar first uncovered span, (2) rules sharing the same source side. In Figure  3 (a), each group of hypotheses, g hyps , is shown in a green box (in stacks), and each rectangle on the top is a cube. Figure  3  is using the example in Figure  2 . The Merge routine is the core function of cube pruning which generates the best hypotheses from all cubes  (Chiang, 2007) . For each possible cube, (H, R), the best hypothesis is generated by calling GrowHypothesis(h 1 , r 1 , span, F) where h 1 and r 1 are the best hypothesis and rule in H and R respectively (line 22). Figure  3  (b) shows a more detailed view of a cube (shaded cube in Figure  3(a) ). Rows are hypotheses and columns are rules which are sorted based on their scores. The first best hypotheses, h , along with their score, h c and corresponding cube, (H, R) are placed in a priority queue, heapQ (triangle in Figure  3 ). Iteratively the best hypothesis is popped from the queue (line 26) and its neighbours in the cube are added to the priority queue (using GetN eighbours([H, Q])). It continues to generate all K best hypotheses. Using cube pruning technique, each stack is filled with K best hypotheses without generating all possible hypotheses in each cube. groups the hypotheses in a given stack based on their coverage vector. But this idea does not work in LRHiero decoding in which the expansion of each hypothesis is restricted to its first uncovered span. We have also tried another way of grouping hypotheses: group by all uncovered spans, hs. Our experiments did not show any significant difference between the final results (BLEU score), therefore we decided to stick to the simpler idea: using first uncovered span for grouping.   shows the derivation of the two best hypotheses from the cube. The best hypothesis of this cube which is likely created from the best hypothesis and rule (left top most entry) is popped at first step. Then, GetNeighbours calls GrowHypothesis to generate next potential best hypotheses of this cube (neighbours of the popped entry which are shaded in Figure  3(b) ). These hypotheses are added to the priority queue. In the next iteration, the best hypothesis is popped from all candidates in the queue and algorithm continues. 

 Features We use the following standard SMT features for the log-linear model of LR-Hiero: relative-frequency translation probabilities p(f |e) and p(e|f ), lexical translation probabilities p l (f |e) and p l (e|f ), a language model probability, word count and phrase count. In addition we also use the glue rule count and the two reordering penalty features employed by  Watanabe et al. (2006b; 2006a) . These features compute the height and width (span size of the entire subtree) of all subtrees which are backtraced in the derivation of a hypothesis. A non-terminal X i is pushed into the LIFO list of a partial hypothesis; it's backtrace refers to the set of NTs that must be popped before X i . In Figure  1 (b), X 2 has two subtrees X 3 and X 6 , where X 3 should be processed before X 6 . The subtree rooted at X 3 in Figure  1 (b) has a height of 2 and span [1, 6] having a width of 5. Similarly, X 4 should be backtraced before X 5 and has height and width of 1. Backtracing applies only for rules having at least two non-terminals. Thus the total height and width penalty for this derivation are 3 and 6 respectively. However, the height and width features do not distinguish between a rule that reorders the nonterminals in source and target from one that preserves the ordering. Rules #2 and #3 in Figure  2  are treated equally although they have different orderings. The decoder is thus agnostic to this difference and would not be able to exploit this effectively to control reordering and instead would rely on the partial LM score. This issue is exacerbated for glue rules, where the decoder has to choose from different possibilities without any way to favour one over the others. Instead of the rule #2, the decoder could use its reordered version X 1 haben X 2 , have X 2 X 1 leading to a poor translation. The features we introduce can be used to learn if the model should favour monotone translations at the cost of re-orderings or vice versa and hence can easily adapt to different language pairs. Further, our experiments (see Section 4) suggest that the features h and w are not sufficient by themselves to model reordering for language pairs exhibiting very different syntactic structure. 

 Distortion Features Our distortion features are inspired by their namesake in phrase-based system, with some modifications to adapt the idea for the discontiguous phrases in LR-Hiero grammar. Consider a rule r = ?, b ? , with the source term ? being a mixed string of terminals and nonterminals. Representing the non-terminal spans and each sequence of terminals in ? as distinct items, our distortion feature counts the total length of jumps between the items during Earley parsing. r : hf 1 X 1 f 2 X 2 f 3 , tX 2 X 1 i I = [`, f 1 , f 2 , f 3 , X 2 , X 1 , a] f 2 f 3 X 1 f 1 X 2 (a) r : ? X 1 noch nicht X 2 /not yet X 2 X 1 ? I =[(1, Figure  4  (a) explains the computation of our distortion feature for an example rule r. Let I = [I 0 , . . . , I k ] be the items denoting the terminal sequences and non-terminal spans with I 0 and I k being dummy items ( and in Fig) marking the left and right indices of the rule r in input sentence f . Other items are arranged by their realization order on the target-side with the terminal sequences preceding non-terminal spans. The items for the example rule are shown in Figure  4  (a). The distortion feature is computed as follows: d(r) = k j=1 |I L j ? I R j?1 | (4) where superscripts refer to position of left (L) and right (R) edge of each item in the source sentence f . These are then aggregated across the rules of a derivation D as: d = r?D d(r). For each item I j , we count the jump from the end of previous item to the beginning of the current. In Figure  4  (a) the jumps are indicated by the arrows above the rule. Figure  4  (b) shows an example of distortion computation for r 3 and phrase ihre arbeit noch nicht gemacht haben from Figure  2 . Since the glue rules are likely to be in the top levels (possibly with large distortion) of the derivation, we would want the decoder to learn the distortion for regular and glue rules separately. We thus use two distortion features for the two rule types and we call them d p and d g . These features do not directly model the sourcetarget reordering, but only capture the source-side jumps. Furthermore they apply for both monotone and reordering rules. We now introduce a new feature for exclusively modelling the reordering. 

 Reordering Feature This feature simply counts the number of reordering rules, where the non-terminals in source and target sides are reordered. Thus r = rule(D, ), where rule(D, ) is the number of reordering rules in D. Similar to width and height, this feature is applied for rule having at least two non-terminals. This feature is applied to regular and glue rules. 

 Experiments We conduct different types of experiments to evaluate LR-Hiero decoding developed by cube pruning and integrating new features into LR-Hiero system for two language pairs: German-English (de-en) and Czech-English (cs-en).Table  1  shows the dataset details. 

 System Setup In our experiments we use four baselines as well as our implementation of LR-Hiero (written in Python): ? Hiero: we used Kriya, our open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems . Kriya can obtain statistically significantly equal BLEU scores when compared with Moses  (Koehn et al., 2007)  for several language pairs  Callison-Burch et al., 2012) . ? Hiero-GNF: where we use Hiero decoder with the restricted LR-Hiero grammar (GNF rules). ? LR-Hiero: our implementation of LR-Hiero  (Watanabe et al., 2006b)  in Python. ? phrase-based: Moses  (Koehn et al., 2007)  ? LR-Hiero+CP: LR-Hiero decoding with cube pruning. We use a 5-gram LM trained on the Gigaword corpus and use KenLM  (Heafield, 2011)  for LM scoring during decoding. We tune weights by minimizing BLEU loss on the dev set through MERT  (Och, 2003)  and report BLEU scores on the test set. We use comparable pop limits in each of the decoders: 1000 for Moses and LR-Hiero and 500 with cube pruning for CKY Hiero and LR-Hiero+CP. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings so that the results are comparable. Table  2  shows how the LR-Hiero grammar is much smaller than CKY-based Hiero.  

 Time Efficiency Comparison To evaluate the performance of LR-Hiero decoding with cube pruning (LR-Hiero+CP), we compare it with three baselines: (i) CKY Hiero, (ii) CKY Hiero-GNF, and (iii) LR-Hiero (without cube pruning) with two different beam size 500 and 1000. When it comes to instrument timing results, there are lots of system level details that we wish to abstract away from, and focus only on the number of "edges" processed by the decoder. In comparison of parsing algorithms, the common practice is to measure the number of edges processed by different algorithms for the same reason  (Moore and Dowding, 1991) . By analogy to parsing algorithm comparisons, we compare the different decoding algorithms with respect to the number of calls made to the language model (LM) since that directly corresponds to the number of hypotheses considered by the decoder. A decoder is more time efficient if it can consider fewer translation hypotheses while maintaining the same BLEU score. All of the baselines use the same wrapper to query the language model, and we have instrumented the wrapper to count the statistics we need and thus we can say this is a fair comparison. For this experiment we use a sample set of 50 sentences taken from the test sets. Table  3  shows the results in terms of average number of language model queries and times in milliseconds. 

 Reordering Features To evaluate the new reordering features proposed to LR-Hiero (Section 3.2), LR-Hiero+CP with new features is compared to all baselines. Table  4  shows the BLEU scores of different models in two language pairs. The baseline  (Watanabe et al., 2006b)  model uses all the features mentioned therein but is All the reported results are obtained a single optimizer run. However we observed insignificant changes in different tuning runs in our experiments. We find a gain of about 1 BLEU point when we add a single distortion feature d and a further gain of 0.3 BLEU (not shown due to lack of space) when we split the distortion feature for the two rule types (d p and d g ). The last line in part two of Table  4  shows a consistent gain of 1.6 BLEU over the LR-Hiero baseline for both language pairs. It shows that LR-Hiero maintains the BLEU scores obtained by "phrase-based" and "CKY Hiero-GNF". We performed statistical significance tests using two different tools: Moses bootstrap resampling and MultEval  (Clark et al., 2011) . The difference between "LR-Hiero+CP+reordering feat" and three baselines: "phrase-based", "CKY Hiero-GNF", "LR-Hiero+reordering feat" are not statistically significant even for p-value of 0.1 for both tools. To investigate the impact of proposed reordering features with other decoder or models. We add these features to both Hiero and Hiero-GNF 7 . The last part of Table  4  shows the performance CKY decoder 7 Feature r is defined for SCFG rules and cannot be adopted to phrase-based translation systems; and Moses uses distortion feature therefore we omit Moses from this experiment. with different models (full Hiero and GNF) with the new reordering features in terms of BLEU score. The results show that these features are helpful in both models. Although, they do not make a big difference in Hiero with full model, they can alleviate the lack of non-GNF rules in Hiero-GNF.  Nguyen and Vogel (2013)  integrate traditional phrase-based features: distortion and lexicalized reordering into Hiero as well. They show that such features can be useful to boost the translation quality of CKY Hiero with the full rule set.  Nguyen and Vogel (2013)  compute the distortion feature in a different way, only applicable to CKY. The distortion for each cell is computed after the translation for nonterminal sub-spans is complete. In LR-decoding, we compute distortion for rules even though we are yet to translate some of the sub-spans. Thus our approach computes the distortion incrementally for the untranslated sub-spans which are later added. Unlike  (Nguyen and Vogel, 2013) , our distortion feature can be applied to both LR and CKY-decoding (Table  4 ). We have also introduced another reordering feature (Section 3.2) not proposed previously. 

 Conclusion and Future Work We provided a detailed description of left-to-right Hiero decoding, many details of which were only implicit in  (Watanabe et al., 2006b) . We presented an augmented LR decoding algorithm that builds on the original algorithm in  (Watanabe et al., 2006b)  but unlike that algorithm, using experiments over multiple language pairs we showed two new results: (i) Our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero and the original LR decoding algorithm in  (Watanabe et al., 2006b) . And, (ii) by introducing new distortion and reordering features for LR decoding we show that it maintains the BLEU scores obtained by phrasebased and CKY Hiero-GNF. CKY Hiero uses standard Hiero-style translation rules capturing better reordering model than prefix lexicalized target-side translation rules used in LR-Hiero. Our LR-decoding algorithm is 4 times faster in terms of LM calls while translation quality suffers by about 0.67 in BLEU score on average. Unlike  Watanabe et al. (2006b) , our new features can easily adapt to the reordering requirements of different language pairs. We also introduce the use of future cost in decoding algorithm which is an essential part in decoding. We have shown in this paper that left-to-right (LR) decoding can be considered as a potential faster alternative to CKY decoding for Hiero-style machine translation systems. In future work, we plan to apply lexicalized reordering models to LR-Hiero. It has been shown to be useful for Hiero in some languages therefore it is promising to improve translation quality in LR-Hiero which suffers from lack of modeling power of non-GNF target side rules. We also plan to extend the glue rules in LR-Hiero to provide a better reordering model. We believe such an extension would be very effective in reducing search errors and capturing better reordering models in language pairs involving complex reordering requirements like Chinese-English. Figure 1 : 1 Figure 1: (a): A word-aligned German-English sentence pair. The bars above the source words indicate phrasepairs having at least two words. (b): its corresponding left-to-right target derivation tree. Superscripts on the source non-terminals show the indices of the rules (see Fig 2) used in derivation. 
