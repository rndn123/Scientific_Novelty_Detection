title
T ÜB İTAK-B İLGEM German-English Machine Translation Systems for WMT'13

abstract
This paper describes T ?B ?TAK-B ?LGEM statistical machine translation (SMT) systems submitted to the Eighth Workshop on Statistical Machine Translation (WMT) shared translation task for German-English language pair in both directions. We implement phrase-based SMT systems with standard parameters. We present the results of using a big tuning data and the effect of averaging tuning weights of different seeds. Additionally, we performed a linguistically motivated compound splitting in the Germanto-English SMT system.

Introduction T ?B ?TAK-B ?LGEM participated for the first time in the WMT'13 shared translation task for the German-English language pairs in both directions. We implemented a phrase-based SMT system by using the entire available training data. In the German-to-English SMT system, we performed a linguistically motivated compound splitting. We tested different language model (LM) combinations by using the parallel data, monolingual data, and Gigaword v4. In each step, we tuned systems with five different tune seeds and used the average of tuning weights in the final system. We tuned our systems on a big tuning set which is generated from the last years'  (2008, 2009, 2010, and 2012)  development sets. The rest of the paper describes the details of our systems. 

 German-English 

 Baseline All available data was tokenized, truecased, and the maximum number of tokens were fixed to 70 for the translation model. The Moses open SMT toolkit  (Koehn et al., 2007)  was used with MGIZA++  (Gao and Vogel, 2008)  with the standard alignment heuristic grow-diag-final  (Och and Ney, 2003)  for word alignments. Good-Turing smoothing was used for phrase extraction. Systems were tuned on newstest2012 with MERT  (Och, 2003)  and tested on newstest2011. 4gram language models (LMs) were trained on the target side of the parallel text and the monolingual data by using SRILM  (Stolcke, 2002)  toolkit with Kneser-Ney smoothing  (Kneser and Ney, 1995)  and then binarized by using KenLM toolkit  (Heafield, 2011) . At each step, systems were tuned with five different seeds with latticesamples. Minimum Bayes risk decoding  (Kumar and Byrne, 2004)  and -drop-unknown parameters were used during the decoding. This configuration is common for all of the experiments decribed in this paper unless stated otherwise. Table  1  shows the number of sentences used in system training after the clean-corpus process. We trained two baseline systems in order to assess the effects of this year's new parallel data, commoncrawl. We first trained an SMT system by using only the training data from the previous WMT shared translation tasks that is europarl and news-commentary (Baseline1). As the second baseline, we also included the new parallel data commoncrawl only in the translation model (Base-line2). Then, we included commoncrawl corpus both to the translation model and the language model (Baseline3).  

 Data 

 Number of sentences 

 Bayesian Alignment In the original IBM models  (Brown et al., 1993) , word translation probabilities are treated as model parameters and the expectation-maximization (EM) algorithm is used to obtain the maximumlikelihood estimates of the parameters and the resulting distributions on alignments. However, EM provides a point-estimate, not a distribution, for the parameters. The Bayesian alignment on the other hand takes into account all values of the model parameters by treating them as multinomial-distributed random variables with Dirichlet priors and integrating over all possible values. A Bayesian approach to word alignment inference in IBM Models is shown to result in significantly less "garbage collection" and a much more compact alignment dictionary. As a result, the Bayesian word alignment has better translation performances and obtains significant BLEU improvements over EM on various language pairs, data sizes, and experimental settings  (Mermer et al., 2013) . We compared the translation performance of word alignments obtained via Bayesian inference to those obtained via EM algorithm. We used a a Gibbs sampler for fully Bayesian inference in HMM alignment model, integrating over all possible parameter values in finding the alignment distribution by using Baseline3 word alignments for initialization.  

 Development Data in Training Development data from the previous years (i.e. newstest08, newstest09, newstest10), though being a small set of corpus (7K sentences), is in-domain data and can positively affect the translation system. In order to make use of this data, we experimented two methods: i) adding the development data in the translation model as described in this section and ii) using it as a big tuning set for tuning the parameters more efficiently as explained in the next section. Similar to including the commoncrawl corpus, we first add the development data both to the training and language models by concatenating it to the biggest corpus europarl (DD(tm+lm)) and then we removed this corpus from the language models (DD(tm)). Results in Table  4  show that including the development data both the tranining and language model increases the performance in development set but decreases the performance in the test set. Including the data only in the translation model shows a very slight improvement in the test set.  

 System 

 Tuning with a Big Development Data The second method of making use of the development data is to concatenate it to the tuning set. As a baseline, we tuned the system with newstest12 as mentioned in Section 2.1. Then, we concatenated the development data of the previous years with the newstest12 and built a big tuning set. Finally, we obtained a tuning set of 10K sentences. We excluded the newstest11 as an internal test set to see the relative improvements of different systems. Table  5  shows the results of using a big tuning set. Tuning the system with a big tuning set resulted in a 0.13 BLEU points improvement.  

 System 

 Effects of Different Language Models In this set of experiments, we tested the effects of different combinations of parallel and monolingual data as language models. As the baseline, we trained three LMs, one from each parallel corpus as europarl, news-commentary, and commoncrawl and one LM from the monolingual data newsshuffled (Baseline3). We then trained two LMs, one from the whole parallel data and one from the monolingual data (2LMs).  

 German Preprocessing In German, compounding is very common. From the machine translation point of view, compounds increase the vocabulary size with high number of the singletons in the training data and hence decrease the word alignment quality. Moreover, high number of out-of-vocabulary (OOV) words in tuning and test sets results in several German words left as untranslated. A well-known solution to this problem is compound splitting. Similarly, having different word forms for a source side lemma for the same target lemma causes the lexical redundancy in translation. This redundancy results in unnecessary large phrase translation tables that overload the decoder, as a separate phrase translation entry has to be kept for each word form. For example, German definite determiner could be marked in sixteen different ways according to the possible combinations of genders, case and number, which are fused in six different tokens  (e.g., der, das, die, den, dem, des) . Except for the plural and genitive cases, all these forms are translated to the same English word "the". In the German preprocessing, we aimed both normalizing lexical redundancy and splitting German compounds with corpus driven splitting algorithm based on  Koehn and Knight (2003) . We used the same compound splitting and lexical redundancy normalization methods described in  Allauzen et al. (2010)  and Durgar El-Kahlout and Yvon (  2010 ) with minor in-house changes. We used only "addition" (e.g., -s, -n, -en, -e, -es) and "truncation" (e.g., -e, -en, -n) affixes for compound splitting. We selected minimum candidate length to 8 and minimum split length to 4. By using the Treetagger  (Schmid, 1994 ) output, we included linguistic information in compound splitting such as not splitting named entities and foreign words (CS1). We also experimented adding # as a delimiter for the splitted words except the last word (e.g., Finanzkrisen is splitted as finanz# krisen) (CS2). On top of the compound splitting, we applied the lexical redundancy normalization (CS+Norm1). We lemmatized German articles, adjectives (only positive form), for some pronouns and for nouns in order to remove the lexical redundancy (e.g., Bildes as Bild) by using the finegrained part-of-speech tags generated by RFTagger  (Schmid and Laws, 2008) . Similar to CS2, We tested the delimited version of normalized words (CS+Norm2). Table  7  shows the results of compound splitting and normalization methods. As a result, normalization on top of compounding did not perform well. Besides, experiments showed that compound word decomposition is crucial and helps vastly to improve translation results 0.43 BLEU points on average over the best system described in Section 2.5.  

 System 

 Average of Weights As mentioned in Section 2.1, we performed tuning with five different seeds. We averaged the five tuning weights and directly applied these weights during the decoding. Table  8  shows that using the average of several tuning weights performs better than each individual tuning (0.2 BLEU points).  

 System 

 Other parameters In addition to the experiments described in the earlier sections, we removed the -drop-unknown parameter which gave us a 0.5 BLEU points improvement. We also included the monotone-atpunctuation, -mp in decoding. We handled outof-vocabulary (OOV) words by lemmatizing the OOV words. Moreover, we added all development data in training after fixing the parameter weights as described in Section 2.7. Although each of these changes increases the translation scores each gave less than 0.1 BLEU point improvement. Table  9  shows the results of the final system after including all of the approaches except the ones described in Section 2.2 and 2.3.  

 System 

 English-German For English-to-German translation system, the baseline setting is the same as described in Section 2.1. We also added the items that showed positive improvement in the German to English SMT system such as using 2 LMs, tuning with five seeds and averaging tuning parameters, using -mp, and not using -drop-unknown.  

 Final System and Results Table  11  shows our official submission scores for German-English SMT systems submitted to the WMT'13. 

 System newstest13 De-En 25.60 En-De 19.28 Table  11 : German-English Official Test Submission. 

 Conclusion In this paper, we described our submissions to WMT'13 Shared Translation Task for German-English language pairs. We used phrase-based systems with a big tuning set which is a combination of the development sets from last four years. We tuned the systems on this big tuning set with five different tunes. We averaged these five tuning weights in the final system. We trained 4-gram language models one from parallel data and one from monolingual data. Moreover, we trained a 4-gram language model with Gigaword v4 for German-to-English direction. For Germanto-English, we performed a different compound splitting method instead of the Moses splitter. We obtained a 1.7 BLEU point increase for Germanto-English SMT system and a 0.5 BLEU point increase for English-to-German SMT system for the internal test set newstest2011. Finally, we submitted our German-to-English SMT system with a BLEU score 25.6 and English-to-German SMT system with a BLEU score 19.3 for the official test set newstest2013. Table 1 : 1 Parallel Corpus. Europarl 1908574 News-Commentary 177712 Commoncrawl 726458 
