title
PARANMT-50M: Pushing the Limits of Paraphrastic Sentence Embeddings with Millions of Machine Translations

abstract
We describe PARANMT-50M, a dataset of more than 50 million English-English sentential paraphrase pairs. We generated the pairs automatically by using neural machine translation to translate the non-English side of a large parallel corpus, following . Our hope is that PARANMT-50M can be a valuable resource for paraphrase generation and can provide a rich source of semantic knowledge to improve downstream natural language understanding tasks. To show its utility, we use PARANMT-50M to train paraphrastic sentence embeddings that outperform all supervised systems on every SemEval semantic textual similarity competition, in addition to showing how it can be used for paraphrase generation. 1

Introduction While many approaches have been developed for generating or finding paraphrases  (Barzilay and McKeown, 2001; Lin and Pantel, 2001; , there do not exist any freelyavailable datasets with millions of sentential paraphrase pairs. The closest such resource is the Paraphrase Database (PPDB;  Ganitkevitch et al., 2013) , which was created automatically from bilingual text by pivoting over the non-English language  (Bannard and Callison-Burch, 2005) . PPDB has been used to improve word embeddings  (Faruqui et al., 2015; Mrk?i? et al., 2016) . However, PPDB is less useful for learning sentence embeddings . In this paper, we describe the creation of a dataset containing more than 50 million sentential paraphrase pairs. We create it automatically by scaling up the approach of . We use neural machine translation (NMT) to translate the Czech side of a large Czech-English parallel corpus. We pair the English translations with the English references to form paraphrase pairs. We call this dataset PARANMT-50M. It contains examples illustrating a broad range of paraphrase phenomena; we show examples in Section 3. PARANMT-50M has the potential to be useful for many tasks, from linguistically controlled paraphrase generation, style transfer, and sentence simplification to core NLP problems like machine translation. We show the utility of PARANMT-50M by using it to train paraphrastic sentence embeddings using the learning framework of  Wieting et al. (2016b) . We primarily evaluate our sentence embeddings on the SemEval semantic textual similarity (STS) competitions from 2012-2016. Since so many domains are covered in these datasets, they form a demanding evaluation for a general purpose sentence embedding model. Our sentence embeddings learned from PARANMT-50M outperform all systems in every STS competition from 2012 to 2016. These tasks have drawn substantial participation; in 2016, for example, the competition attracted 43 teams and had 119 submissions. Most STS systems use curated lexical resources, the provided supervised training data with manually-annotated similarities, and joint modeling of the sentence pair. We use none of these, simply encoding each sentence independently using our models and computing cosine similarity between their embeddings. We experiment with several compositional architectures and find them all to work well. We find benefit from making a simple change to learning ("mega-batching") to better leverage the large training set, namely, increasing the search space of negative examples. In the supplementary, we evaluate on general-purpose sentence embedding tasks used in past work  (Kiros et al., 2015; Conneau et al., 2017) , finding our embeddings to perform competitively. Finally, in Section 6, we briefly report results showing how PARANMT-50M can be used for paraphrase generation. A standard encoderdecoder model trained on PARANMT-50M can generate paraphrases that show effects of "canonicalizing" the input sentence. In other work, fully described by  Iyyer et al. (2018) , we used PARANMT-50M to generate paraphrases that have a specific syntactic structure (represented as the top two levels of a linearized parse tree). We release the PARANMT-50M dataset, our trained sentence embeddings, and our code. PARANMT-50M is the largest collection of sentential paraphrases released to date. We hope it can motivate new research directions and be used to create powerful NLP models, while adding a robustness to existing ones by incorporating paraphrase knowledge. Our paraphrastic sentence embeddings are state-of-the-art by a significant margin, and we hope they can be useful for many applications both as a sentence representation function and as a general similarity metric. 

 Related Work We discuss work in automatically building paraphrase corpora, learning general-purpose sentence embeddings, and using parallel text for learning embeddings and similarity functions. Paraphrase discovery and generation. Many methods have been developed for generating or finding paraphrases, including using multiple translations of the same source material  (Barzilay and McKeown, 2001) , using distributional similarity to find similar dependency paths  (Lin and Pantel, 2001) , using comparable articles from multiple news sources  Dolan and Brockett, 2005; , aligning sentences between standard and Simple English Wikipedia  (Coster and Kauchak, 2011) , crowdsourcing  (Xu et al., 2014 (Xu et al., , 2015 Jiang et al., 2017) , using diverse MT systems to translate a single source sentence  (Suzuki et al., 2017) , and using tweets with matching URLs  (Lan et al., 2017) . The most relevant prior work uses bilingual corpora.  Bannard and Callison-Burch (2005)  used methods from statistical machine translation to find lexical and phrasal paraphrases in parallel text.  Ganitkevitch et al. (2013)  scaled up these techniques to produce the Paraphrase Database (PPDB). Our goals are similar to those of PPDB, which has likewise been generated for many languages  (Ganitkevitch and Callison-Burch, 2014)  since it only needs parallel text. In particular, we follow the approach of , who used NMT to translate the non-English side of parallel text to get English-English paraphrase pairs. We scale up the method to a larger dataset, produce state-of-the-art paraphrastic sentence embeddings, and release all of our resources. Sentence embeddings. Our learning and evaluation setting is the same as that of our recent work that seeks to learn paraphrastic sentence embeddings that can be used for downstream tasks  (Wieting et al., 2016b,a; . We trained models on noisy paraphrase pairs and evaluated them primarily on semantic textual similarity (STS) tasks. Prior work in learning general sentence embeddings has used autoencoders  (Socher et al., 2011; Hill et al., 2016) , encoder-decoder architectures  (Kiros et al., 2015; Gan et al., 2017) , and other sources of supervision and learning frameworks (Le and  Mikolov, 2014; Pham et al., 2015; Arora et al., 2017; Pagliardini et al., 2017; Conneau et al., 2017) . Parallel text for learning embeddings. Prior work has shown that parallel text, and resources built from parallel text like NMT systems and PPDB, can be used for learning embeddings for words and sentences. Several have used PPDB as a knowledge resource for training or improving embeddings  (Faruqui et al., 2015; Wieting et al., 2015; Mrk?i? et al., 2016) . NMT architectures and training settings have been used to obtain better embeddings for words  (Hill et al., 2014a,b)  and words-in-context  (McCann et al., 2017) .  Hill et al. (2016)  evaluated the encoders of Englishto-X NMT systems as sentence representations.  adapted trained NMT models to produce sentence similarity scores in semantic evaluations. 

 The PARANMT-50M Dataset To create our dataset, we used back-translation of bitext . We used a Czech-English NMT system to translate Czech sentences  from the training data into English. We paired the translations with the English references to form English-English paraphrase pairs. We used the pretrained Czech-English model from the NMT system of . Its training data includes four sources: Common Crawl, CzEng 1.6  (Bojar et al., 2016) , Europarl, and News Commentary. We did not choose Czech due to any particular linguistic properties.  found little difference among Czech, German, and French as source languages for backtranslation. There were much larger differences due to data domain, so we focus on the question of domain in this section. We leave the question of investigating properties of back-translation of different languages to future work. 

 Choosing a Data Source To assess characteristics that yield useful data, we randomly sampled 100K English reference translations from each data source and computed statistics. Table  1  shows the average sentence length, the average inverse document frequency (IDF) where IDFs are computed using Wikipedia sentences, and the average paraphrase score for the two sentences. The paraphrase score is calculated by averaging PARAGRAM-PHRASE embeddings  (Wieting et al., 2016b)  for the two sentences in each pair and then computing their cosine similarity. The table also shows the entropies of the vocabularies and constituent parses obtained using the Stanford Parser .  2  Europarl exhibits the least diversity in terms of rare word usage, vocabulary entropy, and parse entropy. This is unsurprising given its formulaic and repetitive nature. CzEng has shorter sentences than the other corpora and more diverse sentence structures, as shown by its high parse entropy. In terms of vocabulary use, CzEng is not particularly more diverse than Common Crawl and News Commentary, though this could be due to the prevalence of named entities in the latter two. In Section 5.3, we empirically compare these data sources as training data for sentence embeddings. The CzEng corpus yields the strongest performance when controlling for training data size. Since its sentences are short, we suspect this helps ensure high-quality back-translations. A large portion of it is movie subtitles which tend to use a wide vocabulary and have a diversity of sentence structures; however, other domains are included as well. It is also the largest corpus, containing over 51 million sentence pairs. In addition to providing a large number of training examples for downstream tasks, this means that the NMT system should be able to produce quality translations for this subset of its training data. For all of these reasons, we chose the CzEng corpus to create PARANMT-50M. When doing so, we used beam search with a beam size of 12 and selected the highest scoring translation from the beam. It took over 10,000 GPU hours to backtranslate the CzEng corpus. We show illustrative examples in Table  2 . 

 Manual Evaluation We conducted a manual analysis of our dataset in order to quantify its noise level and assess how the Para. Score # Avg. Tri. Paraphrase Fluency Range (M) Overlap 1 2 3 1 2 3 (-0.1, 0.2] 4.0 0.00?0.0 92 6 2 1 5 94 (0.2, 0.4] 3.8 0.02?0.1 53 32 15 1 12 87 (0.4, 0.6] 6.9 0.07?0.1 22 45 33 2 9 89 (0.6, 0.8] 14.4 0.17?0.2 1 43 56 11 0 89 (0.8, 1.0] 18.0 0.35?0.2 1 13 86 3 0 97 Table  3 : Manual evaluation of PARANMT-50M. 100-pair samples were drawn from five ranges of the automatic paraphrase score (first column). Paraphrase strength and fluency were judged on a 1-3 scale and counts of each rating are shown. noise can be ameliorated with filtering. Two native English speakers annotated a sample of 100 examples from each of five ranges of the Paraphrase Score.  3  We obtained annotations for both the strength of the paraphrase relationship and the fluency of the translations. To annotate paraphrase strength, we adopted the annotation guidelines used by  Agirre et al. (2012) . The original guidelines specify six classes, which we reduced to three for simplicity. We combined the top two into one category, left the next, and combined the bottom three into the lowest category. Therefore, for a sentence pair to have a rating of 3, the sentences must have the same meaning, but some unimportant details can differ. To have a rating of 2, the sentences are roughly equivalent, with some important information missing or that differs slightly. For a rating of 1, the sentences are not equivalent, even if they share minor details. For fluency of the back-translation, we use the following: A rating of 3 means it has no grammatical errors, 2 means it has one to two errors, and 1 means it has more than two grammatical errors or is not a natural English sentence. Table  3  summarizes the annotations. For each score range, we report the number of pairs, the mean trigram overlap score, and the number of times each paraphrase/fluency label was present in the sample of 100 pairs. There is noise but it is largely confined to the bottom two ranges which together comprise only 16% of the entire dataset. In the highest paraphrase score range, 86% of the pairs possess a strong paraphrase relationship. The annotations suggest that PARANMT-50M contains approximately 30 million strong paraphrase pairs, and that the paraphrase score is a good indi-cator of quality. At the low ranges, we inspected the data and found there to be many errors in the sentence alignment in the original bitext. With regards to fluency, approximately 90% of the backtranslations are fluent, even at the low end of the paraphrase score range. We do see an outlier at the second-highest range of the paraphrase score, but this may be due to the small number of annotated examples. 

 Learning Sentence Embeddings To show the usefulness of the PARANMT-50M dataset, we will use it to train sentence embeddings. We adopt the learning framework from  Wieting et al. (2016b) , which was developed to train sentence embeddings from pairs in PPDB. We first describe the compositional sentence embedding models we will experiment with, then discuss training and our modification ("megabatching"). Models. We want to embed a word sequence s into a fixed-length vector. We denote the tth word in s as s t , and we denote its word embedding by x t . We focus on three model families, though we also experiment with combining them in various ways. The first, which we call WORD, simply averages the embeddings x t of all words in s. This model was found by  Wieting et al. (2016b)  to perform strongly for semantic similarity tasks. The second is similar to WORD, but instead of word embeddings, we average character trigram embeddings  (Huang et al., 2013) . We call this TRIGRAM.  Wieting et al. (2016a)  found this to work well for sentence embeddings compared to other n-gram orders and to word averaging. The third family includes long short-term memory (LSTM) networks  (Hochreiter and Schmidhuber, 1997) . We average the hidden states to produce the final sentence embedding. For regularization during training, we scramble words with a small probability . We also experiment with bidirectional LSTMs (BLSTM), averaging the forward and backward hidden states with no concatenation.  4  Training. The training data is a set S of paraphrase pairs s, s and we minimize a margin-based loss (s, s ) = max(0, ? ? cos(g(s), g(s )) + cos(g(s), g(t))) where g is the model (WORD, TRIGRAM, etc.), ? is the margin, and t is a "negative example" taken from a mini-batch during optimization. The intuition is that we want the two texts to be more similar to each other than to their negative examples. To select t we choose the most similar sentence in some set. For simplicity we use the mini-batch for this set, i.e., t = argmax t : t ,? ?S b \{ s,s } cos(g(s), g(t )) where S b ? S is the current mini-batch. Modification: mega-batching. By using the mini-batch to select negative examples, we may be limiting the learning procedure. That is, if all potential negative examples in the mini-batch are highly dissimilar from s, the loss will be too easy to minimize. Stronger negative examples can be obtained by using larger mini-batches, but large mini-batches are sub-optimal for optimization. Therefore, we propose a procedure we call "mega-batching." We aggregate M mini-batches to create one mega-batch and select negative examples from the mega-batch. Once each pair in the mega-batch has a negative example, the megabatch is split back up into M mini-batches and training proceeds. We found that this provides more challenging negative examples during learning as shown in Section 5.5. Table  6  shows results for different values of M , showing consistently higher correlations with larger M values. 

 Experiments We now investigate how best to use our generated paraphrase data for training paraphrastic sentence embeddings. 

 Evaluation We evaluate sentence embeddings using the Sem-Eval semantic textual similarity (STS) tasks from 2012 to 2016  (Agirre et al., 2012 (Agirre et al., , 2013 (Agirre et al., , 2014 (Agirre et al., , 2015 (Agirre et al., , 2016  and the STS Benchmark  (Cer et al., 2017) . Given two sentences, the aim of the STS tasks is to predict their similarity on a 0-5 scale, where 0 indicates the sentences are on different topics and 5 means they are completely equivalent. over each year of the STS tasks from 2012-2016. We use the small (250-example) English dataset from SemEval 2017  (Cer et al., 2017)  as a development set, which we call STS2017 below. The supplementary material contains a description of a method to obtain a paraphrase lexicon from PARANMT-50M that is on par with that provided by PPDB 2.0. We also evaluate our sentence embeddings on a range of additional tasks that have previously been used for evaluating sentence representations  (Kiros et al., 2015) . 

 Experimental Setup For training sentence embeddings on PARANMT-50M, we follow the experimental procedure of  Wieting et al. (2016b) . We use PARAGRAM-SL999 embeddings  (Wieting et al., 2015)  to initialize the word embedding matrix for all models that use word embeddings. We fix the mini-batch size to 100 and the margin ? to 0.4. We train all models for 5 epochs. For optimization we use Adam  (Kingma and Ba, 2014)  with a learning rate of 0.001. For the LSTM and BLSTM, we fixed the scrambling rate to 0.3. 5 

 Dataset Comparison We first compare parallel data sources. We evaluate the quality of a data source by using its backtranslations paired with its English references as training data for paraphrastic sentence embeddings. We compare the four data sources described in Section 3. We use 100K samples from each corpus and trained 3 different models on each: WORD, TRIGRAM, and LSTM. CzEng is diverse in terms of both vocabulary and sentence structure. It has significantly shorter sentences than the other corpora, and has much more training data, so its translations are expected to be better than those in the other corpora.  found that sentence length was the most important factor in filtering quality training data, presumably due to how NMT quality deteriorates with longer sentences. We suspect that better translations yield better data for training sentence embeddings. 

 Data Filtering Since the PARANMT-50M dataset is so large, it is computationally demanding to train sentence embeddings on it in its entirety. So, we filter the data to create a training set for sentence embeddings. We experiment with three simple methods: (1) the length-normalized translation score from decoding, (2) trigram overlap , and (3) the paraphrase score from Section 3. Trigram overlap is calculated by counting trigrams in the reference and translation, then dividing the number of shared trigrams by the total number in the reference or translation, whichever has fewer. We filtered the back-translated CzEng data using these three strategies. We ranked all 51M+ paraphrase pairs in the dataset by the filtering measure under consideration and then split the data into tenths (so the first tenth contains the bottom 10% under the filtering criterion, the second contains those in the bottom 10-20%, etc.). We trained WORD, TRIGRAM, and LSTM models for a single epoch on 1M examples sampled from each of the ten folds for each filtering criterion. We averaged the correlation on the STS2017 data across models for each fold. Table  5  shows the results of the filtering methods. Filtering based on the paraphrase score produces the best data for training sentence embeddings. We randomly selected 5M examples from the top two scoring folds using paraphrase score fil-   

 Effect of Mega-Batching Table  6  shows the impact of varying the megabatch size M when training for 5 epochs on our 5M-example training set. For all models, larger mega-batches improve performance. There is a smaller gain when moving from 20 to 40, but all models show clear gains over M = 1. Table  7  shows negative examples with different mega-batch sizes M . We use the BLSTM model and show the negative examples (nearest neighbors from the mega-batch excluding the current training example) for three sentences. Using larger mega-batches improves performance, presumably by producing more compelling negative examples for the learning procedure. This is likely more important when training on sentences than   (Conneau et al., 2017 ) 4096 70.6 C-PHRASE (Pham et al., 2015  63.9 GloVe  (Pennington et al., 2014)  300 40.6 word2vec  (Mikolov et al., 2013)  300 56.5 sent2vec  (Pagliardini et al., 2017)  700 75.5 Related Work (Supervised) Dep. Tree LSTM  (Tai et al., 2015)  71.2 Const. Tree LSTM  (Tai et al., 2015)  71.9 CNN  (Shao, 2017)  78.4 Table  9 : Results on STS Benchmark test set. prior work on learning from text snippets  (Wieting et al., 2015 (Wieting et al., , 2016b Pham et al., 2015) . 

 Model Comparison Table  8  shows results on the 2012-2016 STS tasks and Table  9  shows results on the STS Benchmark. Table  10 : The means (over all 25 STS competition datasets) of the absolute differences in Pearson's r between each pair of models. aware on the 2012-2016 STS datasets. Note that the large improvement over BLEU and METEOR suggests that our embeddings could be useful for evaluating machine translation output. Overall, our individual models (WORD, TRI-GRAM, LSTM) perform similarly. Using 300 dimensions appears to be sufficient; increasing dimensionality does not necessarily improve correlation. When examining particular STS tasks, we found that our individual models showed marked differences on certain tasks. Table  10  shows the mean absolute difference in Pearson's r over all 25 datasets. The TRIGRAM model shows the largest differences from the other two, both of which use word embeddings. This suggests that TRIGRAM may be able to complement the other two by providing information about words that are unknown to models that rely on word embeddings. We experiment with two ways of combining models. The first is to define additive architectures Target Syntax Paraphrase original with the help of captain picard, the borg will be prepared for everything. (SBARQ(ADVP)(,)(S)(,)(SQ)) now, the borg will be prepared by picard, will it? (S(NP)(ADVP)(VP)) the borg here will be prepared for everything. original you seem to be an excellent burglar when the time comes. (S(SBAR)(,)(NP)(VP)) when the time comes, you'll be a great thief. (S('')(UCP)('')(NP)(VP)) "you seem to be a great burglar, when the time comes." you said. Table  11 : Syntactically controlled paraphrases generated by the SCPN trained on PARANMT-50M. that form the embedding for a sentence by adding the embeddings computed by two (or more) individual models. All parameters are trained jointly just like when we train individual models; that is, we do not first train two simple models and add their embeddings. The second way is to define concatenative architectures that form a sentence embedding by concatenating the embeddings computed by individual models, and again to train all parameters jointly. In Table  8 and Table 9 , these combinations show consistent improvement over the individual models as well as the larger LSTM and BLSTM. Concatenating WORD and TRIGRAM results in the best performance on average across STS tasks, outperforming the best supervised systems from each year. We have released the pretrained model for these "WORD, TRIGRAM" embeddings. In addition to providing a strong baseline for future STS tasks, these embeddings offer the advantages of being extremely efficient to compute and being robust to unknown words. We show the usefulness of PARANMT by also reporting the results of training the "WORD, TRI-GRAM" model on SimpWiki, a dataset of aligned sentences from Simple English and standard English Wikipedia  (Coster and Kauchak, 2011) . It has been shown useful for training sentence embeddings in past work . However, Table  8  shows that training on PARANMT leads to gains in correlation of 3 to 6 points compared to SimpWiki. 

 Paraphrase Generation In addition to powering state-of-the-art paraphrastic sentence embeddings, our dataset is useful for paraphrase generation. We briefly describe two efforts in paraphrase generation here. We have found that training an encoder-decoder model on PARANMT-50M can produce a paraphrase generation model that canonicalizes text. For this experiment, we used a bidirectional LSTM encoder and a two-layer decoder original overall, i that it's a decent buy, and am happy that i own it. paraphrase it's a good buy, and i'm happy to own it. original oh, that's a handsome women, that is. paraphrase that's a beautiful woman. with soft attention over the encoded states  (Bahdanau et al., 2015) . The attention computation consists of a bilinear product with a learned parameter matrix. Table  12  shows examples of output generated by this model, showing how the model is able to standardize the text and correct grammatical errors. This model would be interesting to evaluate for automatic grammar correction as it does so without any direct supervision. Future work could also use this canonicalization to improve performance of models by standardizing inputs and removing noise from data. PARANMT-50M has also been used for syntactically-controlled paraphrase generation; this work is described in detail by  Iyyer et al. (2018) . A syntactically controlled paraphrase network (SCPN) is trained to generate a paraphrase of a sentence whose constituent structure follows a provided parse template. A parse template contains the top two levels of a linearized parse tree. Table  11  shows example outputs using the SCPN. The paraphrases mostly preserve the semantics of the input sentences while changing their syntax to fit the target syntactic templates. The SCPN was used for augmenting training data and finding adversarial examples. We believe that PARANMT-50M and future datasets like it can be used to generate rich paraphrases that improve the performance and robustness of models on a multitude of NLP tasks. 

 Discussion One way to view PARANMT-50M is as a way to represent the learned translation model in a mono-lingual generated dataset. This raises the question of whether we could learn an effective sentence embedding model from the original parallel text used to train the NMT system, rather than requiring the intermediate step of generating a paraphrase training set. However, while  Hill et al. (2016)  and  used trained NMT models to produce sentence similarity scores, their correlations are considerably lower than ours (by 10% to 35% absolute in terms of Pearson). It appears that NMT encoders form representations that do not necessarily encode the semantics of the sentence in a way conducive to STS evaluations. They must instead create representations suitable for a decoder to generate a translation. These two goals of representing sentential semantics and producing a translation, while likely correlated, evidently have some significant differences. Our use of an intermediate dataset leads to the best results, but this may be due to our efforts in optimizing learning for this setting  (Wieting et al., 2016b; . Future work will be needed to develop learning frameworks that can leverage parallel text directly to reach the same or improved correlations on STS tasks. 

 Conclusion We described the creation of PARANMT-50M, a dataset of more than 50M English sentential paraphrase pairs. We showed how to use PARANMT-50M to train paraphrastic sentence embeddings that outperform supervised systems on STS tasks, as well as how it can be used for generating paraphrases for purposes of data augmentation, robustness, and even grammar correction. The key advantage of our approach is that it only requires parallel text. There are hundreds of millions of parallel sentence pairs, and more are being generated continually. Our procedure is immediately applicable to the wide range of languages for which we have parallel text. We release PARANMT-50M, our code, and pretrained sentence embeddings, which also exhibit strong performance as general-purpose representations for a multitude of tasks. We hope that PARANMT-50M, along with our embeddings, can impart a notion of meaning equivalence to improve NLP systems for a variety of tasks. We are actively investigating ways to apply these two new resources to downstream applications, including machine translation, question answering, and additional paraphrase generation tasks. Table 1 : 1 Statistics of 100K-samples of Czech-English parallel corpora; standard deviations are shown for averages. Dataset Avg. Length Avg. IDF Avg. Para. Score Vocab. Entropy Parse Entropy Total Size Common Crawl 24.0?34.7 7.7?1.1 0.83?0.16 7.2 3.5 0.16M CzEng 1.6 13.3?19.3 7.4?1.2 0.84?0.16 6.8 4.1 51.4M Europarl 26.1?15.4 7.1?0.6 0.95?0.05 6.4 3.0 0.65M News Commentary 25.2?13.9 7.5?1.1 0.92?0.12 7.0 3.4 0.19M Reference Translation Machine Translation so, what's half an hour? half an hour won't kill you. well, don't worry. i've taken out tons and tons of guys. lots of guys. don't worry, i've done it to dozens of men. it's gonna be ...... classic. yeah, sure. it's gonna be great. greetings, all! hello everyone! but she doesn't have much of a case. but as far as the case goes, she doesn't have much. it was good in spite of the taste. despite the flavor, it felt good. 
