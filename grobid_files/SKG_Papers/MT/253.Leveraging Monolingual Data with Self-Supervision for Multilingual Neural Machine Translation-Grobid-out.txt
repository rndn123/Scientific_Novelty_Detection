title
Leveraging Monolingual Data with Self-Supervision for Multilingual Neural Machine Translation

abstract
Over the last few years two promising research directions in low-resource neural machine translation (NMT) have emerged. The first focuses on utilizing high-resource languages to improve the quality of low-resource languages via multilingual NMT. The second direction employs monolingual data with selfsupervision to pre-train translation models, followed by fine-tuning on small amounts of supervised data. In this work, we join these two lines of research and demonstrate the efficacy of monolingual data with self-supervision in multilingual NMT. We offer three major results: (i) Using monolingual data significantly boosts the translation quality of lowresource languages in multilingual models. (ii) Self-supervision improves zero-shot translation quality in multilingual models. (iii) Leveraging monolingual data with self-supervision provides a viable path towards adding new languages to multilingual models, getting up to 33 BLEU on WMT ro-en translation without any parallel data or back-translation.

Introduction Recent work has demonstrated the efficacy of multilingual neural machine translation (multilingual NMT) on improving the translation quality of low-resource languages  (Firat et al., 2016; Aharoni et al., 2019)  as well as zero-shot translation  (Ha et al., 2016; Johnson et al., 2017; Arivazhagan et al., 2019b) . The success of multilingual NMT on low-resource languages relies heavily on transfer learning from high-resource languages for which copious amounts of parallel data is easily accessible. However, existing multilingual NMT approaches often do not effectively utilize the abundance of monolingual data, especially in lowresource languages. On the other end of the spectrum, self-supervised learning methods, consuming only monolingual data, have achieved great success on transfer learning  (Devlin et al., 2019)  and unsupervised NMT  (Lample et al., 2018; Artetxe et al., 2018)  without fully benefiting from the rich learning signals offered by the bilingual data of multiple languages. In this work, we propose to combine the beneficial effects of multilingual NMT with the selfsupervision from monolingual data. Compared with multilingual models trained without any monolingual data, our approach shows consistent improvements in the translation quality of all languages, with greater than 10 BLEU points improvements on certain low-resource languages. We further demonstrate improvements in zero-shot translation, where our method has almost on-par quality with pivoting-based approaches, without using any alignment or adversarial losses. The most interesting aspect of this work, however, is that we introduce a path towards effectively adding new unseen languages to a multilingual NMT model, showing strong translation quality on several language pairs by leveraging only monolingual data with self-supervised learning, without the need for any parallel data for the new languages.   (Song et al., 2019)  given its success on unsupervised and low-resource NMT, and adapt it to the multilingual setting. 

 Adapting MASS for multilingual models MASS adapts the masked de-noising objective  (Devlin et al., 2019; Raffel et al., 2019)  for sequenceto-sequence models, by masking the input to the encoder and training the decoder to generate the masked portion of the input. To utilize this objective function for unsupervised NMT,  Song et al. (2019)  enhance their model with additional improvements, including language embeddings, target language-specific attention context projections, shared target embeddings and softmax parameters and high variance uniform initialization for target attention projection matrices 1 . We use the same set of hyper-parameters for self-supervised training as described in  (Song et al., 2019) . However, while the success of MASS relies on the architectural modifications described above, we find that our multilingual NMT experiments are stable even in the absence of these techniques, thanks to the smoothing effect of multilingual joint training. We also forego the separate source and target language embeddings in favour of pre-pending the source sentences with a < 2xx > token  (Johnson et al., 2017) . We train our models simultaneously on supervised parallel data using the translation objective and on monolingual data using the MASS objective. To denote the target language in multilingual NMT models we prepend the source sentence with the < 2xx > token denoting the target language. 1 Verified from open-source Github implementation. 3 Experimental Setup 

 Datasets We use the parallel and monolingual training data provided with the WMT corpus, for 15 languages to and from English. The amount of parallel data available ranges from more than 60 million sentence pairs as in En-Cs to roughly 10k sentence pairs as in En-Gu. We also collect additional monolingual data from WMT news-crawl, newscommentary, common-crawl, europarl-v9, newsdiscussions and wikidump datasets in all 16 languages including English.  2  The amount of monolingual data varies from 2 million sentences in Zh to 270 million in De. The distribution of our parallel and monolingual data is depicted in Figure  1 . 

 Data Sampling Given the data imbalance across languages in our datasets, we use a temperature-based data balancing strategy to over-sample low-resource languages in our multilingual models  (Arivazhagan et al., 2019b) . We use a temperature of T = 5 to balance our parallel training data. When applicable, we sample monolingual data uniformly across languages since this distribution is not as skewed. For experiments that use both monolingual and parallel data, we mix the two sources at an equal ratio (50% monolingual data with self-supervision and 50% parallel data). 

 Architecture and Optimization All experiments are performed with the Transformer architecture  (Vaswani et al., 2017)  using the open-source Tensorflow-Lingvo implementation  (Shen et al., 2019) . Specifically, we use the Transformer Big model containing 375M parameters (6 layers, 16 heads, 8192 hidden dimension) (Chen et al., 2018) and a shared source-target Sen-tencePiece model (SPM) 3  (Kudo and Richardson, 2018) . We use a vocabulary size of 32k for the bilingual models and 64k for the multilingual mod-   

 Using Monolingual Data for Multilingual NMT We evaluate the performance of the models using SacreBLEU  (Post, 2018)  on standard WMT validation and test sets  (Papineni et al., 2002) . The performance of our bilingual baselines for all 30 English-centric language pairs are reported in Table 1. We compare the performance of bilingual models, multilingual models trained with just supervised data for 30 language pairs (15 languages to and from English) and multilingual models trained with a combination of supervised and monolingual data in Figure  2 . High-Resource Translation Our results suggest that a single multilingual model is able to match the quality of individual bilingual models with a gap of less than 2 BLEU points for most high-resource languages, with the exception of Chinese (Zh). The slight quality regression is not surprising, given the large number of languages competing for capacity within the same model  (Arivazhagan et al., 2019b) . We find that adding additional monolingual data improves the multilingual model quality across the board, even for high-resource language pairs. Low-Resource Translation From Figure  2 , we observe that our supervised multilingual NMT model significantly improves the translation quality for most low and medium-resource languages compared with the bilingual baselines. Adding additional monolingual data leads to an additional im-provement of 1-2 BLEU for most medium-resource languages. For the lowest-resource languages like Kazakh (kk), Turkish (tr) and Gujarati (gu), we can see that multilingual NMT alone is not sufficient to reach high translation quality. The addition of monolingual data has a large positive impact on very low resource languages, significantly improving quality over the supervised multilingual model. These improvements range from 3-5 BLEU in the en?xx direction to more than 5 BLEU for the xx?en translation. 

 Zero-Shot Translation We next evaluate the effect of training on additional monolingual data on zero-shot translation in multilingual models. Table  2  demonstrates the zero-shot performance of our multilingual model that is trained on 30 language pairs, and evaluated on French(fr)-German(de) and German(de)-Czech(cs), when trained with and without monolingual data. To compare with the existing work on zero-shot translation, we also evaluate the performance of multilingual models trained on just the relevant languages (en-fr-de for fr-de translation, en-cs-de for cs-de translation). We observe that the additional monolingual data significantly improves the quality of zero-shot translation, often resulting in 3-6 BLEU increase on all zero-shot directions compared to our multilingual baseline. We hypothesize that the additional monolingual data seen during the selfsupervised training process helps better align representations across languages, akin to the smoothing effect in semi-supervised learning  (Chapelle et al., 2010) . We leave further exploration of this intriguing phenomenon to future work. 7.9 Table  2 : Zero-shot performance on non-English centric language pairs. We compare with pivot-based translation and two recent approaches from  Arivazhagan et al. (2019a)  and  Kim et al. (2019) . The translation quality between these language pairs when parallel data is available is also provided as a baseline. 4 lang. is a multilingual model trained on 4 language pairs (2 languages to and from English), while 30 lang. is our multilingual model trained on all English-centric language pairs. fr en en fr de en en de ro en en ro lt en en lt lv en en lv hi en en hi Multilingual NMT 34.9 37.5 28.  

 Adding New Languages to Multilingual NMT Inspired by the effectiveness of monolingual data in boosting low-resource language translation quality, we continue with a stress-test in which we completely remove the available parallel data from our multilingual model, one language at a time, in order to observe the unsupervised machine translation quality for the missing language. Results of this set of experiments are detailed in Table  3 . We find that simply adding monolingual data for a new language to the training procedure of a multilingual model is sufficient to obtain strong translation quality for several languages, often attaining within a few BLEU points of the fully supervised multilingual baseline, without the need for iterative back-translation. We also notice significant quality improvements over models trained with just self-supervised learning using monolingual data for a variety of languages. On WMT ro-en, the performance of our model exceeds XLM  (Conneau and Lample, 2019 ) by over 1.5 BLEU and matches bilingual MASS  (Song et al., 2019) , without utilizing any back-translation. This suggests that jumpstarting the iterative back-translation process from multilingual models might be a promising avenue to supporting new languages. 

 Related Work Our work builds on several recently proposed techniques for multilingual NMT and self-supervised representation learning. While massively multilingual models have obtained impressive quality improvements for low-resource languages as well as zero-shot scenarios  (Aharoni et al., 2019; Arivazhagan et al., 2019a) , it has not yet been shown how these massively multilingual models could be extended to unseen languages, beyond the pipelined approaches  (Currey and Heafield, 2019; Lakew et al., 2019) . On the other hand, self-supervised learning approaches have excelled at down-stream cross-lingual transfer  (Devlin et al., 2019; Raffel et al., 2019; , but their success for unsupervised NMT  (Conneau and Lample, 2019; Song et al., 2019)  currently lacks robustness when languages are distant or monolingual data domains are mismatched  (Neubig and Hu, 2018; Vuli? et al., 2019) . We observe that these two lines of research can be quite complementary and can compensate for each other's deficiencies. 

 Conclusion and Future Directions We present a simple framework to combine multilingual NMT with self-supervised learning, in an effort to jointly exploit the learning signals from multilingual parallel data and monolingual data. We demonstrate that combining multilingual NMT with monolingual data and self-supervision (i) improves the translation quality for both low and highresource languages in a multilingual setting, (ii) leads to on-par zero-shot capability compared with competitive bridging-based approaches and (iii) is an effective way to extend multilingual models to new unseen languages. Future work should explore techniques like iterative back-translation  (Hoang et al., 2018)  for further improvement and scaling to larger model capacities and more languages  (Arivazhagan et al., 2019b; Huang et al., 2019)    Figure 1 : 1 Figure 1: Number of parallel and monolingual training samples in millions for each language in WMT training corpora. 
