title
Training with Adversaries to Improve Faithfulness of Attention in Neural Machine Translation

abstract
Can we trust that the attention heatmaps produced by a neural machine translation (NMT) model reflect its true internal reasoning? We isolate and examine in detail the notion of faithfulness in NMT models. We provide a measure of faithfulness for NMT based on a variety of stress tests where model parameters are perturbed and measuring faithfulness based on how often the model output changes. We show that our proposed faithfulness measure for NMT models can be improved using a novel differentiable objective that rewards faithful behaviour by the model through probability divergence. Our experimental results on multiple language pairs show that our objective function is effective in increasing faithfulness and can lead to a useful analysis of NMT model behaviour and more trustworthy attention heatmaps. Our proposed objective improves faithfulness without reducing the translation quality and it also seems to have a useful regularization effect on the NMT model and can even improve translation quality in some cases.

Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability)  (Herman, 2017; Lage et al., 2019)  and faithfulness  (Lipton, 2018; Jacovi and Goldberg, 2020b) , (b) interpretation of the neural model components  Vig and Belinkov, 2019) , (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.)  (Ribeiro et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020) , and (d) evaluating different explanation methods from different perspectives je to moor?v z?kon za posledn?ch sto let je to moor?v z?kon za posledn?ch sto let it's moore's law for the last century it's moore's law for the last century 0.00 1.00 attention weights Figure  1 : An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended.  (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020) .  Jacovi and Goldberg (2020b)  emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) because faithfulness of both correct and incorrect decisions made by the model are equally important, (4) No model is "inherently" faithful. We need to measure faithfulness not as a binary aspect of a model but rather as a gray-scale measure. Aligned with these criteria, we study faithfulness of attention in NMT, the extent to which it can reflect the true internal reasoning behind a prediction (Figure  1 ). We make the following contributions: ? We propose a measure for quantifying faithfulness in NMT. ? We introduce a novel learning objective based on probability divergence that rewards faithful behavior and which can be included in the training objective for NMT. ? We provide empirical evidence that we can improve faithfulness in an NMT model. Our approach results in more a more faithful NMT model while producing better BLEU scores. We chose to study the impact of faithfulness in NMT because it is under-studied in terms of interpretability. Most previous work has focused on document or sentence-based classification tasks where attention models are not as directly useful as in NMT models. Attention is also more challenging in terms of faithfulness in the context of NMT models due to the substantial impact of the decoder component. 1 

 Faithfulness in NMT Models Intuitively, a faithful explanation should reflect the true internal reasoning of the model. Although there is no formal definition for faithfulness, a common approach in the community is to design stress tests to perturb the model parameters chosen in such a way that the model's decision should change if the model is faithful  (Jacovi and Goldberg, 2020b) . A common stress test is the erasure test in which the most-relevant part of the input is removed  (Arras et al., 2017) . In the context of NMT, at decoding time step t the attention component assigns attention weights ? t , attending to the source word at position m t = argmax i ? t [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model's prediction at the time step t  (Tu et al., 2016; Mi et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018) . The erasure stress test for evaluating faithfulness offered by ? t is done by setting ? t [m t ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faith-fulness. In this paper we consider three intuitive stress test cases: ? ZeroOutMax  (Arras et al., 2017 ): Here we remove attention from the most important token according to the attention weights by setting ? t [m t ] = 0. ? Uniform  (Moradi et al., 2019) : In this stress test all attention weights are set to be equal, ? t = 1 m 1, where m is the length of the source sentence. This is to confuse the model about which part of the input is the most important one. ? RandomPermute (Jain and Wallace, 2019): The attention weights are randomly permuted until a change in the model output is observed. We ensure that m t , the most important token according to attention, is always changed. We set ? t = random permute(? t ) such that argmax i ? t [i] = m t Many prior studies of attention  (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019)  have used a binary measure: either attention is faithful or it is not. These studies typically are about whether attention has the potential to be useful in terms of accuracy and faithful in terms of model behaviour. In many cases, especially in the case of NMT models, attention is clearly useful and by and large it must be faithful. The question is can we measure the faithfulness and improve faithfulness. It is more natural to have a gray-scale notion of faithfulness for evaluation  (Jacovi and Goldberg, 2020b) . Following this reasoning, we define F (M ) as faithfulness of attention heatmaps in model M as the following equation: F (M ) = # of tokens passing stress tests # of tokens (1) F (M ) is a number between 0 to 1 measuring the percentage of output tokens during inference which passed the stress tests. This metric can also be regarded as a measure of trust we can assign to the attention heatmap to fully reflect the internal reasoning of the NMT model. 

 Approach The conventional objective function in a sequenceto-sequence task is a cross-entropy loss F acc :   F acc (?) = 1 |S| (X,Y )?S log p(Y |X; ?) (2) 95 F = F acc + ? f aith F f aith (3) F f aith is an additional component that rewards the model for having more faithful attention. The parameter ? f aith regulates the trade-off between between faithfulness and accuracy objectives. 

 Divergence-based Faithfulness Objective Consider a predictive model g ? in which an intermediate calculation is later employed to justify predictions: ? = arg max y p(y|x) = arg max y g ? (x, IC(x), y) (4 ) where IC(x) is the intermediate calculation on the input. A concrete example for IC(x) would be the context vector calculated by the attention mechanism. Hypothesis If there exists an intermediate calculation IC (x) that conveys a contradictory post-hoc attention compared to IC(x), then IC(x) cannot be regarded as faithful for predicting ?. If IC(x) is faithful, we expect the model to diverge from predicting ? when IC (x) is employed instead. Based on our hypothesis, we propose a divergence-based objective which mimics behavior of a faithful explanation under stress test: F f aith = log p(?|x, IC (x)) (5) Here IC (x) is a stress test. This objective promotes reduction in output probability under an adversarial intermediate calculation (Figure  2 ). It is worth noting that this objective can be potentially employed in models where outputs are modeled as soft probabilities and thus is not limited to NMT. To put model under various stress tests we manipulate the context vector during training time by changing the attention weights and feed it to the decoder to calculate the probability. More precisely: F f aith = ? zom log p(?|x, IC zom (x)) + ? uni log p(?|x, IC uni (x)) + ? perm log p(?|x, IC perm (x)) (6) where IC zom , IC uni and IC perm are ZeroOut-Max, Uniform and RandomPermute methods (see Sec. 2) to manipulate attention weights, respectively. ? {method} parameters regulate the contribution of each objective. We use the term F all when all ? {method} s in Eq. (  6 ) are non-zero. Moreover, we use the term F {method} when ? {method} is set to 1 and other regularization weights are zero. 

 Experimental Setup We use the Czech-English (Cs-En) dataset from IWSLT2016and the German-English (De-En) dataset from IWSLT2014.We used Moses  (Koehn et al., 2007)  to tokenize the dataset. We use Open-NMT  (Klein et al., 2017)  as our translation framework. We employ a 2 layer LSTM-based encoderdecoder  (Sutskever et al., 2014; Cho et al., 2014)  model with global attention  (Luong et al., 2015) . We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. The baseline model is trained using Eqn. (  2 ) and we call it F baseline . We refer to the objective as F all when ? zom , ? uni , and ? perm are set to 0.5, 0.375, and 0.125 respectively. ? f aith is set to 1. 

 Results and Discussion 

 Impact on faithfulness To measure the effectiveness of the proposed objectives, we choose the best model in terms of provided faithfulness but within the 0.5 BLEU score of the maximum achieved BLEU score in the validation set. The reason is that we prefer a model that is both accurate and with faithful attention-based explanations. Table  1  and 2 show the performance of the different faithfulness objective functions when generating content words and function words across different attention manipulation methods in the Czech-English (Cs-En) and German-English (De-En) datasets respectively. Results indicate that the proposed divergence-based objective has been effective in increasing the faithfulness metric. F all is the most effective objective for increasing faithfulness when all stress tests are included in Eq. (1). When using F all , faithfulness of attention-based explanations for content words is increased 78% to 89%, while that of the function words is from 33% to 82%(see All column in Table  1 ). The same reductions are from 76% to 89% for content works and from 32% to 86% for function words in De-En dataset. These results establish the effectiveness of our proposed objectives to increase the faithfulness metric. It is worth noting that increase in faithfulness of attention-based explanations for function words is much more than that of content words. This can be attributed to the fact the function words are mostly generated using the target-side information in the decoder  (Tu et al., 2017; Moradi et al., 2019)     

 Effect of training with single adversary on passing other stress tests An interesting observation in Table  1  and 2 is that training with an adversary has positive effects on the model for passing stress tests from other types of adversaries. As an example, in Table  1  the column Uniform is the faithfulness metric when only Uniform test is employed in Eq. (1). When using this metric, we can observe that training a model with F perm increased faithfulness from 90% to 95% for content words and from 48% to 97% for function words. We can see such effect in Table  2  as well. This observation indicates that training with each adversary can be beneficial for making model tolerant against other types of stress tests. It seems that training with each adversary strengthens the dependence of the decoder on the attention component which can be beneficial for passing other stress tests. 

 Regularization Effect The model checkpoints used in Tables  1 and 2  were selected based on maximum increase in faithful-src es ist alles hier es ist alles online ref it 's all here it 's all on the web base it 's all right it 's all online . ours it 's all here it 's all online . src sie dr?ngten wasser aus dem land heraus und hinaus in den fluss ref they pushed water off the land and out into the river base they kept running water from the land and out in the river ours they pushed water out of the country and out in the river . src anstatt hunderte von kilometern entfernt im norden ref instead of hundreds of miles away in the north base instead of hundreds of miles away from north america ours instead of hundreds of miles away from north src es ist alles hier es ist alles online ref it 's all here it 's all on the web base it 's all right it 's all online . ours it 's all here it 's all online . src sie dr?ngten wasser aus dem land heraus und hinaus in den fluss ref they pushed water off the land and out into the river base they kept running water from the land and out in the river ours they pushed water out of the country and out in the river . src anstatt hunderte von kilometern entfernt im norden ref instead of hundreds of miles away in the north base instead of hundreds of miles away from north america ours instead of hundreds of miles away from north ness without sacrificing accuracy. To investigate if the proposed objective can have a general positive side effect in terms of accuracy, we train three independent models using the F baseline and F all objectives. Table  3  contains the average BLEU score of the trained models. It indicates that the model trained with F all , has +0.7 and +0.4 increase in BLEU score compared to the baseline for the Czech-English and German-English language pairs respectively. Objective BLEU% Table  3 : BLEU score of the baseline and the model trained with F all . Pairwise bootstrap resampling  (Koehn, 2004)  resulted in a p-value < 0.01. 

 Cs-En Improved BLEU scores for the faithful model can be due to two reasons: 1) the faithfulness objective can be seen as a regularization term which prevents the model from relying too much on the target-side context and the implicit language model in the decoder, which results in increased contribution of attention on the decoder and reducing some bias in the model. 2) penalizing the model for the lack of connection between justification and prediction forces the model to learn better translations by forcing it to justify each output in a right answer for the right reason paradigm. Figure  3  shows some examples of how our proposed model can produce better translations. 

 Related Work While several studies have focused on understanding the semantic notions captured by attention  (Ghader and Monz, 2017; Vig and Belinkov, 2019; Clark et al., 2019) , evaluating attention as an interpretability approach has garnered a lot of interest. From the faithfulness perspective,  (Jain and Wallace, 2019; Serrano and Smith, 2019)  show that for instances in a data set there can be adversarial attention heatmaps that do not change the output of the text classifier. In other words, adversarial attention leads to no decision flip in each instance. They use this to claim that attention heatmaps are not to be trusted, or unfaithful.  Wiegreffe and Pinter (2019)  argue against per-instance modifications at test time for two reasons: 1) in classification tasks attention may not be useful so perturbing attention is misleading. This is not true for NMT since attention is very useful in NMT. 2) they train an adversarial attention model (e.g. uniform attention) chosen to produce attention weights distant from the original attention weights while at the same time trying to minimize classification error. They show that such adversarial attention models are not as accurate as models with attention. In our work we acknowledge that attention is useful and faithful to some extent and we aim to improve faithfulness of NMT models. While most of these works provide evidence that attention weights are not always faithful,  Moradi et al. (2019)  confirm similar observations on the unfaithful nature of attention in the context of NMT models.  Li et al. (2020)  is one of the few papers examining attention models in NMT. However, they are focused on the task of analyzing attention weights from fidelity perspective which is different from faithfulness. While prior works have mostly failed to explicitly distinguish faithfulness from plausibility in their arguments,  Jacovi and Goldberg (2020a,b)  focus on formalizing faithfulness and addressing evaluation of faithfulness separately from plausibility respectively. 

 Conclusion In this paper, we proposed a method for quantifying faithfulness of NMT models. To optimize faithfulness we have defined a novel objective function that rewards faithful behavior through probability divergence. Unlike previous work, our method does not use prior knowledge or extraneous data. We also show that the additional constraint in the training objective for NMT does not harm translation quality and in some cases we see some better translations presumably due to the regularization effect of our faithfulness objective. Faithfulness Objective In an effort to develop a model that is right for right reason, Ross et al. (2017) change the loss function of their classifier to model both right answers and right reasons instead of only the former. They achieve this by introducing a regularizing term that tends to shrink irrelevant gradients. In a similar spirit, we change our objective to account for the NMT model's faithfulness as well as the cross-entropy score against the reference translations: 
