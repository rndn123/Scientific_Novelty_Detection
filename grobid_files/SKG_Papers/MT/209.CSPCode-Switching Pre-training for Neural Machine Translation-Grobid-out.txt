title
CSP: Code-Switching Pre-training for Neural Machine Translation

abstract
This paper proposes a new pre-training method, called Code-Switching Pre-training (CSP for short) for Neural Machine Translation (NMT). Unlike traditional pre-training method which randomly masks some fragments of the input sentence, the proposed CSP randomly replaces some words in the source sentence with their translation words in the target language. Specifically, we firstly perform lexicon induction with unsupervised word embedding mapping between the source and target languages, and then randomly replace some words in the input sentence with their translation words according to the extracted translation lexicons. CSP adopts the encoderdecoder framework: its encoder takes the codemixed sentence as input, and its decoder predicts the replaced fragment of the input sentence. In this way, CSP is able to pre-train the NMT model by explicitly making the most of the cross-lingual alignment information extracted from the source and target monolingual corpus. Additionally, we relieve the pretrainfinetune discrepancy caused by the artificial symbols like [mask]. To verify the effectiveness of the proposed method, we conduct extensive experiments on unsupervised and supervised NMT. Experimental results show that CSP achieves significant improvements over baselines without pre-training or with other pre-training methods.

Introduction Neural machine translation  (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015)  which typically follows the encoder-decoder framework, directly applies a single neural network to transform the source sentence into the target sentence. With tens of millions of trainable parameters in the NMT model, translation tasks are usually datahungry, and many of them are low-resource or even zero-resource in terms of training data. Following the idea of unsupervised and self-supervised pre-training methods in the NLP area  (Peters et al., 2018; Radford et al., 2018 Radford et al., , 2019 Devlin et al., 2019; Yang et al., 2019) , some works are proposed to improve the NMT model with pretraining, by making full use of the widely available monolingual corpora  (Lample and Conneau, 2019; Song et al., 2019b; Edunov et al., 2019; Rothe et al., 2019; Clinchant et al., 2019) . Typically, two different branches of pre-training approaches are proposed for NMT: model-fusion and parameterinitialization. The model-fusion approaches seek to incorporate the sentence representation provided by the pretrained model, such as BERT, into the NMT model  (Yang et al., 2019b; Clinchant et al., 2019; Weng et al., 2019; Zhu et al., 2020; Lewis et al., 2019; . These approaches are able to leverage the publicly available pre-trained checkpoints in the website but they need to change the NMT model to fuse the sentence embedding calculated by the pre-trained model. Large-scale parameters of the pre-trained model significantly increase the storage cost and inference time, which makes it hard for this branch of approaches to be directly used in production. As opposed to model-fusion approaches, the parameter-initialization approaches aim to directly pre-train the whole or part of the NMT model with tailored objectives, and then initialize the NMT model with pre-trained parameters  (Lample and Conneau, 2019; Song et al., 2019b) . These approaches are more production-ready since they keep the size and structure of the model same as standard NMT systems. While achieving substantial improvements, these pre-training approaches have two main cons. Firstly, as pointed out by  Yang et al. (2019) , the artificial symbols like  [mask]  used by these approaches during pre-training are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Secondly, while each pre-training step only involves sentences from the same language, these approaches are unable to make use of the cross-lingual alignment information contained in the source and target monolingual corpus. We argue that, as a cross-lingual sequence generation task, NMT requires a tailored pre-training objective which is capable of making use of cross-lingual alignment signals explicitly, e.g., word-pair information extracted from the source and target monolingual corpus, to improve the performance. To address the limitations mentioned above, we propose Code-Switching Pre-training (CSP) for NMT. We extract the word-pair alignment information from the source and target monolingual corpus automatically, and then apply the extracted alignment information to enhance the pre-training performance. The detailed training process of CSP can be presented in two steps: 1) perform lexicon induction to get translation lexicons by unsupervised word embedding mapping  (Artetxe et al., 2018a; Conneau et al., 2018) ; 2) randomly replace some words in the input sentence with their translation words in the extracted translation lexicons and train the NMT model to predict the replaced words. CSP adopts the encoder-decoder framework: its encoder takes the code-mixed sentence as input, and its decoder predicts the replaced fragments based on the context calculated by the encoder. By predicting the sentence fragment which is replaced on the encoder side, CSP is able to either attend to the remaining words in the source language or to the translation words of the replaced fragment in the target language. Therefore, CSP trains the NMT model to: 1) learn how to build the sentence representation for the input sentence as the traditional pre-training methods do; 2) learn how to perform cross-lingual translation with extracted word-pair alignment information. In summary, we mainly make the following contributions: ? We propose the code-switching pre-training for NMT, which makes full use of the crosslingual alignment information contained in source and target monolingual corpus to improve the pre-training for NMT. ? We conduct extensive experiments on super-vised and unsupervised translation tasks. Experimental results show that the proposed approach consistently achieves substantial improvements. ? Last but not least, we find that CSP can successfully handle the code-switching inputs. 2 Related works There have also been works on applying prespecified translation lexicons to improve the performance of NMT.  Hokamp and Liu (2017)  and  Post and Vilar (2018)  proposed an altered beam search algorithm, which took target-side pre-specified translations as lexical constraints during beam search.  Song et al. (2019a)  investigated a data augmentation method, making code-switched training data by replacing source phrases with their target translations according to the pre-specified translation lexicons. Recently, motivated by the success of unsupervised cross-lingual embeddings,  Artetxe et al. (2018b) ,  Lample et al. (2018a)  and  Yang et al. (2018)  applied the pre-trained translation lexicons to initialize the word embeddings of the unsupervised NMT model.  applied translation lexicons to unsupervised domain adaptation in NMT. In this paper, we apply the translation lexicons automatically extracted from the monolingual corpus to improve the pre-training of NMT. 

 CSP In this section, we firstly describe how to build the shared vocabulary for the NMT model; then we present the way extracting the probabilistic translation lexicons; and we introduce the detailed training process of CSP finally. 

 Shared sub-word vocabulary This paper processes the source and target languages with the same shared vocabulary created through the sub-word toolkits, such as Sentence-Piece (SP) and Byte-Pair Encoding (BPE)  (Sennrich et al., 2016b) . We learn the sub-word splits on the concatenation of the sentences equally sampled from the source and target corpus. The motivation behind is two-fold: Firstly, with processing the source and target languages by the shared vocabulary, the encoder of the NMT model is able to share the same vocabulary with the decoder. Sharing the vocabulary between the encoder and decoder makes it possible for CSP to replace the source words in the input sentence with their translation words in the target language. Secondly, as pointed out by  Lample and Conneau (2019) , the shared vocabulary greatly improves the alignment of embedding spaces. 

 Probabilistic translation lexicons Recently, some works successfully learned translation equivalences between word pairs from two monolingual corpus and extracted translation lexicons  (Artetxe et al., 2018a; Conneau et al., 2018) . Following  Artetxe et al. (2018a) , we utilize unsu-pervised word embedding mapping to extract probabilistic translation lexicons with monolingual corpus only. The probabilistic translation lexicons in this paper are defined as one-to-many source-target word translations. Specifically, giving separate source and target word embeddings, i.e., X e and Y e trained on source and target monolingual corpus X and Y , unsupervised word embedding mapping utilizes self-learning or adversarial-training to learn a mapping function f (X) = W X, which transforms source and target word embeddings to a shared embedding space. With word embeddings in the same latent space, we measure the similarities between source and target words with the cosine distance of word embeddings. Then, we extract the probabilistic translation lexicons by selecting the top k nearest neighbors in the shared embedding space. Formally, considering the word x i in the source language, its top k nearest neighbor words in the target language, denoted as y i1 , y i2 , . . . , y ik are extracted as its translation words, and the corresponding normalized similarities s i1 , s i2 , . . . , s ik are defined as the translation probabilities. 

 Training process of CSP CSP only requires monolingual data to pre-train the NMT model. Given an unpaired source sentence x ? X, where x = (x 1 , x 2 , . . . , x m ) is the source sentence with m tokens, we denote x [u:v] as the sentence fragment of x from u to v where 0 < u < v < m, and denote x \u:v as modified version of x where its fragment from position u to v are replaced with their translation words according to the probabilistic translation lexicons. Formally, x \u:v is represented as: x \u:v = (x 1 , . . . , x u?1 , y u , . . . , y v , x v+1 . . . , x m ) (1) where x \u:v [u:v] = (y u , . . . , y v ) is sampled based on the extracted probabilistic translation lexicons presented on Section 3.2. Here, we take the replacing process from x u to y u as an example. Considering the source word x u , its top k translation words y u1 , y u2 , . . . , y uk and the translation probabilities s u1 , s u2 , . . . , s uk , y u is calculated as: y u = y uj (1 ? j ? k) (2) where y uj is decided by performing multinomial sampling on the distribution defined by translation probabilities s u1 , s u2 , . . . , s uk . With higher translation probability s uj , the translation word y uj is more likely to be selected.   

 Encoder Decoder Attention 

 ? " ? $ % ? & % ? ' % ? ( % --- - ? $ ? $" % ? $" % ? & ? $* % ? $* % ? $+ % ? $+ % ? ? ? &" % ? &" % ? &* % ? & L(?; X) = 1 |X| x?X logP (x [u:v] |x \u:v ; ?) = 1 |X| x?X log v t=u P (x t |x <t , x \u:v ; ?) (3) Figure  1  shows an example for CSP training, where the original source sentence (x 1 , x 2 , x 3 , x 4 , x 5 , x 6 , x 7 ) with the fragment (x 3 , x 4 , x 5 , x 6 ) being replaced with their translation words, i.e., (y 3 , y 4 , y 5 , y 6 ) sampled from the extracted probabilistic translation lexicons. The encoder takes the code-mixed source sentence as input, and the decoder only predicts the replaced fragment (x 3 , x 4 , x 5 , x 6 ). 

 Experiments and Results This section describes the experimental details about CSP pre-training and fine-tuning on the supervised and unsupervised NMT tasks. To test the effectiveness and generality of CSP, we conduct extensive experiments on English-German, English-French and Chinese-to-English translation tasks. 

 CSP pre-training Model configuration We choose Transformer as the basic model structure. Following the base model in  Vaswani et al. (2017) , we set the dimension of word embedding as 512, dropout rate as 0.1 and the head number as 8. To be comparable with previous works, we set the model as 4-layer encoder and 4-layer decoder for unsupervised NMT, and 6-layer encoder and 6-layer decoder for supervised NMT. The encoder and decoder share the same word embeddings. 

 Datasets and pre-processing Following the work of  Song et al. (2019b) , we use the monolingual data sampled from WMT News Crawl datasets for English, German and French, with 50M sentences for each language. 2 For Chinese, we choose 10M sentences from the combination of LDC and WMT2018 corpora. For each translation task, the source and target languages are jointly tokenized into sub-word units with BPE  (Sennrich et al., 2016b) . The vocabulary is extracted from the tokenized corpora and shared by the source and target languages. For English-German and English-French translation tasks, we set the vocabulary size as 32k. For Chinese-English, the vocabulary size is set as 60k since few tokens are shared by Chinese System en-de de-en en-fr fr-en zh-en and English. To extract the probabilistic translation lexicons, we utilize the monolingual corpora described above to train the embeddings for each language independently by using word2vec  (Mikolov et al., 2013)  . We then apply the public implementation of the method proposed by  Artetxe et al. (2017)  to map the source and target word embeddings to a shared-latent space.  4  Training details We replace the consecutive tokens in the source input with their translation words sampled from the probabilistic translation lexicons, with random start position u. Following  Song et al. (2019b) , the length of the replaced fragment is empirically set as roughly 50% of the total number of tokens in the sentence, and the replaced tokens in the encoder will be the translation tokens 80% of the time, a random token 10% of the time and an unchanged token 10% of the time.  5  In the extracted probabilistic translation lexicons, we only keep top three translation words for each source word and also investigate how the number of translation words produces an effect on the training process. All of the models are implemented on Py-Torch and trained on 8 P40 GPU cards.  6  We use Adam optimizer with a learning rate of 0.0005 for pre-training. 

 Fine-tuning on unsupervised NMT In this section, we describe the experiments on the unsupervised NMT, where we only utilize monolingual data to fine-tune the NMT model based on the pre-trained model. Experimental settings For the unsupervised English-German and English-French translation tasks, we take the similar experimental settings to  Lample and Conneau (2019) ;  Song et al. (2019b) . Specifically, we randomly sample 5M monolingual sentences from the monolingual data used during pre-training and report BLEU scores on WMT14 English-French and WMT16 English-German. For fine-tuning on the unsupervised Chinese-to-English translation task, we also randomly sample 1.6M monolingual sentences for Chinese and English respectively similar to  Yang et al. (2018) . We take N IST 02 as the development set and report the BLEU score averaged on the test sets N IST 03, N IST 04 and N IST 05. To be consistent with the baseline systems, we apply the script multi-bleu.pl to evaluate the translation performance for all of the translation tasks. Baseline systems We take the following four strong baseline systems. English, English-to-French and Chinese-to-English unsupervised translation tasks, with as high as +0.7 BLEU points improvement in German-to-English translation task. In French-to-English translation direction, CSP also achieves comparable results with the SOTA baseline of  Song et al. (2019b) . In Chinese-to-English translation task, CSP even achieves +1.1 BLEU points improvement compared to the reproduced result of  Song et al. (2019b) . These results indicate that fine-tuning unsupervised NMT on the model pre-trained by CSP consistently outperforms the previous unsupervised NMT systems with or without pre-training. 

 Fine-tuning on supervised NMT This section describes our experiments on supervised NMT where we fine-tune the pre-trained model with bilingual data. Experimental settings For supervised NMT, we conduct experiments on the publicly available data sets, i.e., WMT14 English-French, WMT14 English-German and LDC Chinese-to-English corpora, which are used extensively as benchmarks for NMT systems. We use the full WMT14 English-German and WMT14 English-French corpus as our training sets, which contain 4.5M and 36M sentence pairs respectively. For Chinese-to-English translation task, our training data consists of 1.6M sentence pairs randomly extracted from LDC corpora.  7  All of the sentences are encoded with the same BPE codes utilized in pre-training. Baseline systems For supervised NMT, we consider the following three baseline systems.  8  The first one is the work of  Vaswani et al. (2017) , 7 LDC2002L27,LDC2002T01,LDC2002E18,LDC2003E07, LDC2004T08,LDC2004E12,LDC2005T10 8 Since model-fusion approaches incorporate too much extra parameters, it is not fair to take them as baselines here. We leave the comparison between CSP and mode-fusion approaches in the appendix C. which achieves SOTA results on WMT14 English-German and English-French translation tasks. The other two baseline systems are proposed by  Lample and Conneau (2019)  and  Song et al. (2019b) , both of which fine-tune the supervised NMT tasks on the pre-trained models. Furthermore, we compare with the back-translation method which has shown its great effectiveness on improving NMT model with monolingual data  (Sennrich et al., 2016a) . Specifically, for each baseline system, we translate the target monolingual data used during pre-training back to the source language by a reversely-trained model, and get the pseudo-parallel corpus by combining the translation with its original data. 9 At last, the training data which includes pseudo and parallel sentence pairs is shuffled and used to train the NMT system. Results The experimental results on supervised NMT are presented at Table  2 . We report the BLEU scores on English-to-German, English-to-French and Chinese-to-English translation directions. For each translation task, we report the BLEU scores for the standard NMT model and the model trained with back-translation respectively. As shown in Table  2 , compared to the baseline system without pre-training  (Vaswani et al., 2017) , the proposed model achieves +1.6 and +0.7 BLEU points improvements on English-to-German and English-to-French translation directions respectively. Even compared to stronger baseline system with pretraining  (Song et al., 2019b) , we also achieve +0.5 and +0.4 BLEU points improvements respectively on these two translation directions. On Chineseto-English translation task, the proposed model achieves +0.7 BLEU points improvement compared to the baseline system of  Song et al. (2019b) . With back-translation, the proposed model still outperforms all of the baseline systems. Experimental results above show that fine-tuning the supervised NMT on the model pre-trained by CSP achieves substantial improvements over previous supervised NMT systems with or without pre-training. Additionally, it has been verified that CSP is able to work together with back-translation. 

 Analysis 

 Study the number of translation words In CSP, the probabilistic translation lexicons only keep the top k translation words for each source word. For each word in the translation lexicons, the number of translation words k is viewed as an important hyper-parameter and can be set carefully during the process of pre-training. A natural question is that how much of translation words do we need to keep for each source word? Intuitively, if k is set as a small number, the model may lose its generality since each source word can be replaced with only a few translation words, which severely limits the diversity of the context. And if otherwise, the accuracy of the extracted probabilistic translation lexicons may get significantly diminished, which shall introduce too much noise for pre-training. Therefore, there is a trade-off between the generality and accuracy. We investigate this problem by studying the translation performance of unsupervised NMT with different k, where we vary k from 1 to 10 with the interval 2. We observe both the performance of CSP after pre-training and the translation performance after fine-tuning on the unsupervised NMT tasks, including the English-to-German and English-to-French translation directions. For each translation direction, we firstly present the perplexity (PPL) score of the pre-trained model averaged on the monolingual validation sets of the source and target languages.  10  And then we show the BLEU score of the finetuned model on the bilingual validation set. Figure  2  (a) and (c) illustrate the PPL score of the pretrained model and BLEU score of the fine-tuned unsupervised NMT model respectively on Englishto-German translation. Figure  2 (b ) and (d) present the PPL and BLEU score respectively for Englishto-French translation. From Figure  2 , it can be seen that, when k is set around 3, the pre-trained model achieves the best validation PPL scores on both of the English-to-German and English-to-French translation directions. Similarly, CSP also achieves the best BLEU scores on the unsupervised translation tasks when k is set around 3. 

 Ablation study To understand the importance of different components of the model pre-trained by CSP, we perform an ablation study by training multiple versions of the supervised NMT model with some components initialized randomly: the word embeddings, the encoder, the attention module between the encoder and decoder, and the decoder. Experiments are conducted on English-to-German and English-to-French translation tasks. All models are trained without back-translation and results are reported in Table  3 . We can find that the two most critical components are the pre-trained encoder and attention module. It shows that CSP enhances NMT not only on the ability of building sentence representation for the input sentence, but also on the ability of aligning the source and target languages with the help of word-pair alignment information. Additionally, the experimental results indicate that the pre-trained decoder shows little effect on the translation performance. This is mainly because the decoder only predicts the source-side words during pre-training but predicts the target-side words during fine-tuning. This pretrain-finetune mismatch makes the pre-trained decoder less helpful for performance improvement. 

 System en-de en-fr No pre-trained embeddings 28.4 38.5 No pre-trained encoder 27.9 38.2 No pre-trained attention module 28.1 38.3 No pre-trained decoder 28.8 38.8 Full model pre-trained by CSP 28.9 38.8 Table  3 : Ablation study on English-German and English-French translation tasks. The embeddings include the source-side and target-side word embeddings. 

 Code-switching translation Code-switching, which contains words from different languages in single input, has aroused more and more attention in NMT  (Johnson et al., 2017; Menacer et al., 2019) . In this section, we show that the proposed CSP is able to enhance the ability of the fine-tuned NMT model on handling the codeswitching input.    

 C Compared to model-fusion approaches In this section, we compare the proposed CSP with model-fusion approaches. We conduct experiments on supervised NMT where we fine-tune the pre-trained model with bilingual data. Experimental settings are identical to the settings in section 4.3. We report the performance of Englishto-German, English-to-French and Chinese-to-English translation tasks. Since  Zhu et al. (2020)  released their code which makes their results reproducible, we take their system as the baseline. To make the comparison more fair, we distill the model of  Zhu et al. (2020)  to a student model which has the same size and structure to standard NMT model. For knowledge distillation, we utilized the sequence-level knowledge distillation proposed by  Kim and Rush (2016) .  14  Experimental results are presented in Table  5 . We can find that, compared to the distilled student model of  Zhu et al. (2020) , CSP achieves better translation performance on two of three translation tasks. 

 D Case study for code-switching In this section, we compare the performance of different NMT systems by case study. We randomly select some examples of the code-switching inputs and get the outputs by feeding the code-switching inputs into different NMT systems. The results are presented in Table  6 . We can find that, for the two code-switching input sentences in Table  6 , the standard Transformer and the multi-lingual system are both easily to give insufficient translations with some semantic contents untranslated. We assume that this is mainly because these systems are weak in encoding the full context of the code-switching input. Compared to the baseline systems, our system gives more sufficient and fluent translations. This shows that CSP enhances the model's ability 13 https://github.com/bert-nmt/bert-nmt 14 While variant distillation methods have been proposed recently, we only test the most simple and standard one. 

 Source sentence ?But even this most loyalUS ally is opposed to attacking Baghdad. Output of Transformer cheney arrived kuwait after visiting all Gulf, But even this most employed US Output of Multi-lingual system Cheney arrived in Kuwait after visiting, but even this most loyal is opposed to attacking Baghdad. Output of our system Cheney arrived in Kuwait on Monday after visiting all Gulf countries, but even the most loyal US ally is opposed to attacking Baghdad. Reference cheney arrived kuwait on monday after visiting all other gulf states. however , even this most loyal ally to u.s. opposes an attack on baghdad . Source sentence ? Megawati will send a personal letter from Kim Dae Jung to Kim Jong Il? ? Output of Transformer as japan says, Megawati send a personal letter to Kim Jong, the south korea denied. Output of Multi-lingual system as for the news released in japan asahi that megawati will hand a letter from kim dae jung in his own handwriting to kim, the south korea denied this . Output of our system as for the news released in the japanese newspaper asahi that will hand a personal letter from kim dae jung in his own handwriting to kim jong , the south korean government denied . Reference as for the news released in the japanese newspaper asahi that megawati will hand a personal letter from kim dae jung in his own handwriting to kim jong -il , the south korean government denied this . on encoding code-switching inputs. Figure 1 : 1 Figure 1: The training example of our proposed CSP which randomly replaces some words in the source input with their translation words based on the probabilistic translation lexicons. Identical to MAS, the token '-' represents the padding in the decoder. The attention module represents the attention between the encoder and decoder 

 Figure 2 : 2 Figure 2: The performance of CSP with the probabilistic translation lexicons keeping different translation words for each source word, which includes: (a) the PPL score of the pre-trained English-to-German model; (b) the PPL score of the pre-trained English-to-French model; (c) the BLEU score of the fine-tuned unsupervised English-to-German NMT model; (d)the BLEU score of the fine-tuned unsupervised English-to-French NMT model. 

 Figure 3 : 3 Figure 3: The performance of CSP with different length of the replaced fragment, which includes: (a) the PPL score of the pre-trained English-to-German model; (b) the PPL score of the pre-trained English-to-French model; (c) the BLEU score of the fine-tuned unsupervised English-to-German NMT model; (d)the BLEU score of the fine-tuned unsupervised English-to-French NMT model. 

 Table 1 : 1 The translation performance of the fine-tuned unsupervised NMT models. To reproduce the results of Lample and Conneau (2019)  and Song et al. (2019b) , we directly run their released codes on the website.3     Yang et al. (2018) 10.86 14.62 16.97 15.58 14.52 Lample et al. (2018b) 17.16 21.0 25.14 24.18 - Lample and Conneau (2019) 27.0 34.3 33.4 33.3 - Song et al. (2019b) 28.1 35.0 37.5 34.6 - Lample and Conneau (2019) (our reproduction) 27.3 33.8 32.9 33.5 22.1 Song et al. (2019b) (our reproduction) 27.9 34.7 37.3 34.1 22.8 CSP and fine-tuning (ours) 28.7 35.7 37.9 34.5 23.9 

 Table 2 : 2 The translation performance of supervised NMT on English-German, English-French and Chinese-to-English test sets. (+ BT: trains the model with back-translation method.) Lample et al. (2018b) achieved state-of-the-art (SOTA) translation per- formance on unsupervised English-German and English-French translation tasks, by utilizing cross- lingual vocabulary, denoising auto-encoding and back-translation. Yang et al. (2018) proposed the weight-sharing architecture for unsupervised NMT and achieved SOTA results on unsupervised Chinese-to-English translation task. Lample and Conneau (2019) and Song et al. (2019b) are among the first endeavors to apply pre-training to unsuper- vised NMT, and both of them achieved substantial improvements compared to the methods without utilizing pre-training. ResultsTable1shows the experimental results on the unsupervised NMT. From Table1, we can find that the proposed CSP outperforms all of the previous works on English-to-German, German-to- 

 Table 4 : 4 The performance of Chinese-to-English translation on in-house code-switching test sets. 6 Conclusions and Future work This work proposes a simple yet effective pre- training approach, i.e., CSP for NMT, which ran- domly replaces some words in the source sentence with their translation words in the probabilistic translation lexicons extracted from monolingual corpus only. To verify the effectiveness of CSP, we investigate two downstream tasks, supervised and unsupervised NMT, on English-German, English- French and Chinese-to-English translation tasks. Experimental results show that the proposed ap- proach achieves substantial improvements over strong baselines consistently. Additionally, we show that CSP is able to enhance the ability of NMT on handling code-switching inputs. There are two promising directions for the future work. Firstly, we are interested in applying CSP to other 

 Table 6 : 6 Examples of the code-switching inputs and outputs of different NMT systems. 

			 To be used in production easily, these models need to be distilled into a student model with the structure and size same as standard NMT systems. 

			 In this paper, we lower-cased all of the case-sensitive languages by default, such as English, German and French. 

			 https://github.com/facebookresearch/ XLM https://github.com/microsoft/MASS 4 The configuration we used to run these open-source tool kits can be found in appendix A.5  We test different length of the replaced segment and report the results in the appendix B. We find similar results to Song et al. (2019b) .6  The code we used can be found in the attached file. 

			 We randomly select the target monolingual data with the same size to the bilingual data. 

			 For English-German translation, the monolingual validation set for English is built by including all English sentences in the bilingual English-German validation set, and the monolingual validation set for German is built in the same way. 

			 The two in-house code-switching test sets can be found in the attached files. 

			 https://github.com/artetxem/vecmap
