title
Subword Segmentation and a Single Bridge Language Affect Zero-Shot Neural Machine Translation

abstract
Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in which settings it will be effective, and what limits performance compared to a fully supervised system. In this paper, we investigate zero-shot performance of a multilingual EN?{FR,CS,DE,FI} system trained on WMT data. We find that zero-shot performance is highly unstable and can vary by more than 6 BLEU between training runs, making it difficult to reliably track improvements. We observe a bias towards copying the source in zero-shot translation, and investigate how the choice of subword segmentation affects this bias. We find that language-specific subword segmentation results in less subword copying at training time, and leads to better zero-shot performance compared to jointly trained segmentation. A recent trend in multilingual models is to not train on parallel data between all language pairs, but have a single bridge language, e.g. English. We find that this negatively affects zero-shot translation and leads to a failure mode where the model ignores the language tag and instead produces English output in zero-shot directions. We show that this bias towards English can be effectively reduced with even a small amount of parallel data in some of the non-English pairs.

Introduction Zero-shot translation has first been introduced by  Firat et al. (2016)  and refers to the ability of a multilingual NMT model to translate between all its source and target languages, even those pairs for which no parallel data was seen in training. In the simplest setting, all parameters in the network are shared between the different languages and the translation is guided only by special tags to indicate the desired output language  (Johnson et al., 2017; Ha et al., 2016) . While this capability is attractive because it is an alternative to building N 2 dedicated translation systems to serve N languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as  Arivazhagan et al. (2019) ,  Gu et al. (2019)  and  Zhang et al. (2020) , have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings. In this paper, we examine in detail the behavior of the multilingual model proposed by  Johnson et al. (2017)  on zero-shot translation directions. Our experiments show the following: ? Translation quality for zero-shot language pairs is highly unstable between different training runs, and between training checkpoints, which calls for more rigour to avoid false positive results. ? The incorrect copying of source text into the output is affected by the extent of subword copying at training time, and can be reduced by performing language-specific subword segmentation. ? English-centric models have a tendency to produce English text for non-English input. Multi-bridge models that include data from non-English pairs mitigate this problem. Overall, we observe improvements of 8.1 BLEU (15.9?24.0) on 6 zero-shot directions with simple changes to the multilingual training setup. 

 Related Work Our experiments are based on the multilingual model proposed by  (Johnson et al., 2017; Ha et al., 2016) : A single model is trained on multiple language pairs with a standard encoder-decoder architecture, all parameters in the network are shared for all languages, including the vocabulary. An artificial target language token determines the output language. We prefix this special token to the source sentence as in  Johnson et al. (2017) . The major advantage of this model lies in its simplicity, since it does not require changing the architecture or training objective. Several recent studies have explored approaches to improve generalization to zero-shot language pairs, for example through semi-supervised training  (Gu et al., 2019; Currey and Heafield, 2019; Zhang et al., 2020)  or alignment of encoder representations  (Arivazhagan et al., 2019) . Our study is concerned with data conditions that enable zero-shot generalization for multilingual NMT, specifically preprocessing and data settings. While initial work used separate encoders and decoders for different languages  (Firat et al., 2016) , sharing of encoder and decoder parameters was established by  Johnson et al. (2017) ;  Ha et al. (2016)  and has since been widely adopted.  Johnson et al. (2017)  use a shared subword segmentation model across languages, and this strategy is followed by later work (e.g.  Zhang et al., 2020) .  Ha et al. (2016)  do not share embeddings across languages, but use language-specific codes. We will show that both strategies cause errors. In terms of data settings, the number of languages involved in multilingual models has increased from 3-4  (Firat et al., 2016; Johnson et al., 2017)  to over 100 . The most popular setup are English-centric datasets, where the model is trained on translations between English and a number of other languages. A multiway parallel corpus between 5 languages has been provided for the IWSLT17 multilingual task  (Cettolo et al., 2012) . Results on this dataset show strong zero-shot generalization, close or even exceeding the supervised condition  (Lakew et al., 2017) , but multi-way parallel corpora are only available in small amounts and specific domains, so we investigate alternatives to English-centric models that do not rely on multi-way parallelism. 

 Data and Models Following  English?{French,Czech,German,Finnish} from WMT  (Barrault et al., 2019) . For all zero-shot language pairs, we sample test sets from OPUS  (Tiedemann, 2012) , see Table  1  for details. To indicate the target language, we prefix a language tag on the source side (e.g. <2en>). Following  Johnson et al. (2017) , we segment all data with a byte-pair encoding model trained jointly on the training data in all five languages  (Sennrich et al., 2016) , with a threshold of 32k BPE operations. All our systems are base Transformers  (Vaswani et al., 2017)  implemented in Sockeye  (Hieber et al., 2018) , trained with early stopping based on BLEU on a development set that consists in equal parts of parallel sentences from all trained translation directions. See Appendix A and B for training details. 

 Baseline Experiments BLEU 1 on zero-shot pairs is relatively unstable, see Fig.  1 : while BLEU on the trained pairs increases steadily during training (dashed lines), performance on unseen language pairs fluctuates considerably, as also observed by . Furthermore, multiple training runs result in relatively large differences in BLEU on the zero-shot directions. Across three training runs, average BLEU varies up to 0.24 points on trained language pairs (standard deviation: 0.12), but up to 6.28 BLEU on zero-shot pairs (standard deviation: 3.14) -see Table  2  for full results. We suspect that this fluctuation is due to the fact that the model is not op-   (Tiedemann, 2012) , all other corpora are part of the WMT19 translation shared task  (Barrault et al., 2019)  timized on zero-shot directions: models converge to different local minima that may be similarly good for trained pairs, but with no mechanism that stabilizes generalization to zero-shot pairs. If not stated otherwise, we will report the mean and standard deviation of three training runs with different seeds throughout the paper. As an alternative to zero-shot translation, we report results obtained via pivot translation through English (e.g. German-English-Czech). On our data set, this approach works better than zero-shot translation. Pivot translation is stable across training runs, with a standard deviation of 0.19. 

 Copy Bias and Language-Specific Subword Segmentation One failure mode we observe in zero-shot translation is over-copying of the input.  2  We suspect that for the translation of zero-shot directions, the model relies heavily on (sub-) words in the vocabulary that are shared between languages. To test this hypothesis, we train two models with languagespecific subword segmentation: a) a model with language-specific subword seg-  mentation and no vocabulary overlap. We limit BPE operations to 10k per language. b) similar to model a), with the exact same subword segmentation, but with vocabulary overlap. For model a), we remove any potential vocabulary overlap by adding a language identifier to each subword. For instance, consider the preposition in in German and English: instead of one token in, the network vocabulary has an entry for in#de# and an additional entry for in#en#. This corresponds to the language-specific coding introduced by  Ha et al. (2016) . For model b), we split words with the same language-specific BPE models as for a), but we allow vocabulary overlap, i.e. homographic forms in different languages are represented by a single entry in the network's vocabulary. This results in a vocabulary size of ?50k for model a), whereas for model b), the vocabulary amounts to a total of ?36k subwords. Table  3  shows that removing vocabulary overlap does not affect the trained language pairs greatly, however, the effect on the zero-shot directions is quite harsh: For the first evaluation of model a), we remove only the correct target language tag (i.e. homographic forms with wrong language tag count as wrong), while for the second evaluation, we remove all language tags from the translations (i.e. homographic forms in other languages count as correct). In the first case, the model averages at only 4.7 BLEU on zero-shot directions, however, the more lenient second evaluation results in better scores (12.7 BLEU). This difference is due to the fact that the no-overlap model tends to produce a lot of English subwords (marked by #en#), especially for proper names and numbers.  3  The second evaluation improves BLEU because the no-overlap model will often output the correct form, e.g. for proper names, if the word in the target language has the same spelling as in English. Model b), with language-specific BPE and overlapping vocabularies, represents a compromise between a fully shared representation and fully language-specific coding. We hypothesize that allowing some vocabulary overlap helps aligning the representation between sentences with the same  meaning in different languages, which is also supported by the effectiveness of cross-lingual pretraining with shared vocabularies for unsupervised MT and cross-lingual transfer  (Conneau and Lample, 2019) . We observe that models with jointly trained BPE develop a strong bias towards copying the input in zero-shot conditions. However, using language-specific BPE reduces the subword overlap between source and target sentences at training time, and consequently reduces this copying behavior at test time (see Table  4 ). Model b) indeed performs better (+5.1 BLEU) on the zero-shot directions than the original baseline with shared BPE (see Table  3 ). 

 Multi-Bridge Models A common issue in zero-shot translation is output in the wrong language. Previous work has addressed this with semi-supervised training  (Gu et al., 2019; Arivazhagan et al., 2019; Zhang et al., 2020) . We explore whether the recent trend to train English-centric models is to blame for this behavior. In most cases, the model will wrongly produce English output in zero-shot directions, since for all non-English languages, English was the only target language seen in training. We suspect that adding even a small amount of parallel data in pairs without English will improve Table  5 : BLEU for single-bridge baseline with language-specific BPE (see Table  3 ), and model trained with 350k pairs in de?cs and fi?fr (multi-bridge). Both models use language-specific BPE segmentation. generalization, make models more sensitive to the language tag, and reduce the amount of English translations in the zero-shot directions. To test this hypothesis, we collect a small amount of parallel data in German-Czech and Finnish-French 4 and train our model with the additional language pairs. This new model has seen all non-English languages paired with exactly one other non-English language, but it still has zero-shot directions in de?fr, fr?cs and de?fi. We use language-specific BPE segmentation and thus use the model with the best zero-shot performance from Table  3  as baseline. The results in Table  5  show that even a small amount of parallel data in non-English language pairs increases generalization to unseen translation directions. The increase in BLEU scores for the newly added pairs de?cs and fi?fr are expected, but the new model also performs better on cs?fr, de?fr and fi?de (+3.1 BLEU on average). Following  Zhang et al. (2020) , we use the Python Table  6 : Percentage of output produced in the correct target language (tgt), English, and the source language (src) in zero-shot translation according to automatic language identification. Models from Table  5 . version of langdetect 5 to estimate the number of translations in the correct language. Even though the amount of parallel data in de?cs and fi?fr was small compared to the directions with English (350k vs. 5 million sentence pairs), the new model is less likely to produce output in the wrong target language, as shown in Table  6 . 

 Comparison to Back-Translation and Encoder Alignment Previous work on the zero-shot generalization of multilingual NMT systems has proposed backtranslation or changes to the training objective to improve translation in unsupervised directions. While we consider our proposed solutions on the data side to be complementary, and easier to adopt widely, we still want to discuss the question how our solutions compare to previous work, and whether they can be combined. 

 Back-Translation Previous work has used fine-tuning with synthetic, back-translated data for translation directions that were unseen at training time  (Gu et al., 2019; Currey and Heafield, 2019; Zhang et al., 2020) . While this can mitigate the problem of producing output in the wrong language, this approach is sensitive to the zero-shot translation quality of back-translation.  6  We perform experiments following  Gu et al. (2019)  where we create synthetic corpora for all zero-resource directions via backtranslations (250k sentences per translation direction), and fine-tune our models on the concatena-  tion of this data, plus 250k sentence pairs per supervised translation direction. As base system for both back-translation and fine-tuning, we consider both our single-bridge and our multi-bridge system. As stopping criterion during fine-tuning, we use BLEU on the Finnish?German test set, one of the zero-resource language pairs. This leaves us with 5 translation directions that are still purely zero-resource.  et al. (2019)  propose to use cosine distance as an additional loss term for multilingual models. The cosine distance loss encourages the model to produce encoder representations for sentences in the source language that are similar to the encoder representation of the same sentence in the target language. This, directly and indirectly, rewards similarity of encoder representations across all languages. We implement cosine loss in Sockeye, but instead of normalising sequence lengths by max pooling like  Arivazhagan et al. (2019) , we average encoder states, as proposed by  Gouws et al. (2015) . We introduce a new hyperparameter ? that scales cosine distance (CD) loss w.r.t. the standard cross-entropy (CE): 

 Encoder Alignment 

 Arivazhagan L = (1 ? ?) * CE + ? * CD (1) We train models with ? = 0.5. As in our experiments with back-translation, we do not train from scratch, but fine-tune each of the single-bridge and multi-bridge models with a patience of 10. 7 7 In a new training run with random initialization, the encoder produces highly similar representations for all languages 

 Results Results are shown in Table  7 . The gains from using more than one bridge language and backtranslation are cumulative: Both the single-and the multi-bridge baseline improve with encoder alignment and back-translation, but the multi-bridge performs better overall in zero-resource directions. Aligning encoder representations leads to an increase of 0.8 BLEU for the zero-shot directions for the single bridge data. In the multi-bridge scenario however, the effect of the additional loss is smaller (+0.3 BLEU on average). Table  7  contains only results for models with language-specific subword segmentation; but preliminary experiments show that aligning encoder representations of one of the baselines from Table  2  with jointly trained BPE gives a similar result: Encoder alignment alone does not fix the underlying issue caused by vocabulary overlap and English-centric models, even though we observe an increase of ? 1.5 BLEU points in zero-shot directions over the baseline. Back-translation leads to an average improvement of 0.7 BLEU with singlebridge data, and 1.2 BLEU with multibridge data. On the fully supervised pairs English?{Czech,German,Finnish,French}, we observe a performance drop by 0.7-0.9 BLEU with back-translation. Again, back-translation alone does not seem to solve the issues of single-bridge setups, and the model benefits from additional supervised translation directions. On the 6 remaining zero-shot translation direcfrom the start.  Arivazhagan et al. (2019)  report that fine-tuning yields better results. tions, our pivoting baseline (Table  2 ) achieves an average BLEU of 23.7. Our best system with multi-bridge data and back-translation achieves 25.2, and thus outperforms our pivoting baseline by 1.5 BLEU. 

 Conclusions We analyze the importance of shared subwords in multilingual models and find that language-specific BPE segmentation helps to reduce the amount of untranslated segments in zero-shot directions. Furthermore, we explore whether the tendency to produce the wrong output language can be attributed to using English as the only bridge language, and show that even with a small amount of additional training data in non-English language pairs, generalization to unseen translation directions improves as the model is less likely to produce output in the wrong language. Compared to previous work, the methods we propose are easier to implement, since they only concern data collection and pre-processing, and result in higher gains for zero-shot directions. They are also compatible in principle with approaches that introduce new training objectives or model modifications, and we report best results when finetuning a multi-bridge model with back-translation for zero-resource translation directions. For future work, we are interested in testing the effects of subword regularization  (Kudo, 2018; Provilkov et al., 2020)  on zero-shot translation performance, and scaling multi-bridge setups to massively multilingual settings.  

 B Hyperparameters Table 2 : 2 Baseline (BLEU). Average and standard deviation of 3 training runs reported. For zero-shot directions, we compare direct zero-shot translation and pivot translation via English. 

 Table 3 : 3 Average BLEU for models with language specific BPE, with and without vocabulary overlap. BPE subwords words training set jointly trained lang.-specific 9.70% *5.70% 7.96% *5.70% translations jointly trained lang.-specific 24.82% 20.58% 6.97% 4.70% * identical 

 Table 4 : 4 Average word and subword overlap between source and target in training set, and in zero-shot translation output with jointly trained and language-specific BPE. 

 Table 7 : 7 Zero-resource translation performance (BLEU) with single-bridge and multi-bridge multilingual models, fine-tuned with a cosine loss to reward encoder representation alignment (+align), and back-translation for zeroresource translation directions (+bt). 

			 SacreBLEU (Post, 2018) : BLEU+c.mixed+#.1 +s.exp+t.13a+v.1.2.21. 

			 See also (Ha et al., 2017; Arivazhagan et al., 2019; Zhang et al., 2020) , who make similar observations in different settings. 

			 This essentially means that the strict evaluation gives us a more realistic estimate of the translation quality we can expect if the source and target language do not happen to share word forms, e.g. languages in different scripts. 

			 https://github.com/Mimino666/ langdetect 6 Unless back-translation is done via a pivot language, but note that Gu et al. (2019)  report slightly better results for direct zero-shot back-translation. 

			 https://github.com/awslabs/sockeye
