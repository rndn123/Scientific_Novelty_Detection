title
Task Alternation in Parallel Sentence Retrieval for Twitter Translation

abstract
We present an approach to mine comparable data for parallel sentences using translation-based cross-lingual information retrieval (CLIR). By iteratively alternating between the tasks of retrieval and translation, an initial general-domain model is allowed to adapt to in-domain data. Adaptation is done by training the translation system on a few thousand sentences retrieved in the step before. Our setup is time-and memory-efficient and of similar quality as CLIR-based adaptation on millions of parallel sentences.

Introduction Statistical Machine Translation (SMT) crucially relies on large amounts of bilingual data  (Brown et al., 1993) . Unfortunately sentence-parallel bilingual data are not always available. Various approaches have been presented to remedy this problem by mining parallel sentences from comparable data, for example by using cross-lingual information retrieval (CLIR) techniques to retrieve a target language sentence for a source language sentence treated as a query. Most such approaches try to overcome the noise inherent in automatically extracted parallel data by sheer size. However, finding good quality parallel data from noisy resources like Twitter requires sophisticated retrieval methods. Running these methods on millions of queries and documents can take weeks. Our method aims to achieve improvements similar to large-scale parallel sentence extraction approaches, while requiring only a fraction of the extracted data and considerably less computing resources. Our key idea is to extend a straightforward application of translation-based CLIR to an iterative method: Instead of attempting to retrieve in one step as many parallel sentences as possible, we allow the retrieval model to gradually adapt to new data by using an SMT model trained on the freshly retrieved sentence pairs in the translationbased retrieval step. We alternate between the tasks of translation-based retrieval of target sentences, and the task of SMT, by re-training the SMT model on the data that were retrieved in the previous step. This task alternation is done iteratively until the number of newly added pairs stabilizes at a relatively small value. In our experiments on Arabic-English Twitter translation, we achieved improvements of over 1 BLEU point over a strong baseline that uses indomain data for language modeling and parameter tuning. Compared to a CLIR-approach which extracts more than 3 million parallel sentences from a noisy comparable corpus, our system produces similar results in terms of BLEU using only about 40 thousand sentences for training in each of a few iterations, thus being much more time-and resource-efficient. 

 Related Work In the terminology of semi-supervised learning  (Abney, 2008) , our method resembles self-training and co-training by training a learning method on its own predictions. It is different in the aspect of task alternation: The SMT model trained on retrieved sentence pairs is not used for generating training data, but for scoring noisy parallel data in a translation-based retrieval setup. Our method also incorporates aspects of transductive learning in that candidate sentences used as queries are filtered for out-of-vocabulary (OOV) words and similarity to sentences in the development set in order to maximize the impact of translation-based retrieval. Our work most closely resembles approaches that make use of variants of SMT to mine comparable corpora for parallel sentences. Recent work uses word-based translation  (Munteanu and Marcu, 2005; Munteanu and Marcu, 2006) , fullsentence translation  (Abdul-Rauf and Schwenk, 2009; Uszkoreit et al., 2010) , or a sophisticated interpolation of word-based and contextual translation of full sentences  (Snover et al., 2008; Jehl et al., 2012;  to project source language sentences into the target language for retrieval. The novel aspect of task alternation introduced in this paper can be applied to all approaches incorporating SMT for sentence retrieval from comparable data. For our baseline system we use in-domain language models  (Bertoldi and Federico, 2009)  and meta-parameter tuning on in-domain development sets  (Koehn and Schroeder, 2007) . 

 CLIR for Parallel Sentence Retrieval 

 Context-Sensitive Translation for CLIR Our CLIR model extends the translation-based retrieval model of  Xu et al. (2001) . While translation options in this approach are given by a lexical translation table, we also select translation options estimated from the decoder's n-best list for translating a particular query. The central idea is to let the language model choose fluent, context-aware translations for each query term during decoding. For mapping source language query terms to target language query terms, we follow  Ture et al. (2012a; . Given a source language query Q with query terms q j , we project it into the target language by representing each source token q j by its probabilistically weighted translations. The score of target document D, given source language query Q, is computed by calculating the Okapi BM25 rank  (Robertson et al., 1998)  over projected term frequency and document frequency weights as follows: score(D|Q) = |Q| j=1 bm25(tf (qj, D), df (qj)) tf (q, D) = |Tq | i=1 tf (ti, D)P (ti|q) df (q) = |Tq | i=1 df (ti)P (ti|q) where T q = {t|P (t|q) > L} is the set of translation options for query term q with probability greater than L. Following  Ture et al. (2012a;  we impose a cumulative threshold C, so that only the most probable options are added until C is reached. Like  Ture et al. (2012a;  we achieved best retrieval performance when translation probabilities are calculated as an interpolation between (context-free) lexical translation probabilities P lex estimated on symmetrized word alignments, and (context-aware) translation probabilities P nbest estimated on the n-best list of an SMT decoder: P (t|q) = ?P nbest (t|q) + (1 ? ?)P lex (t|q) (1) P nbest (t|q) is the decoder's confidence to translate q into t within the context of query Q. Let a k (t, q) be a function indicating an alignment of target term t to source term q in the k-th derivation of query Q. Then we can estimate P nbest (t|q) as follows: P nbest (t|q) = n k=1 a k (t, q)D(k, Q) n k=1 a k (?, q)D(k, Q) (2) D(k, Q) is the model score of the k-th derivation in the n-best list for query Q. In our work, we use hierarchical phrase-based translation  (Chiang, 2007) , as implemented in the cdec framework  (Dyer et al., 2010) . This allows us to extract word alignments between source and target text for Q from the SCFG rules used in the derivation. The concept of self-translation is covered by the decoder's ability to use pass-through rules if words or phrases cannot be translated. 

 Task Alternation in CLIR The key idea of our approach is to iteratively alternate between the tasks of retrieval and translation for efficient mining of parallel sentences. We allow the initial general-domain CLIR model to adapt to in-domain data over multiple iterations. Since our set of in-domain queries was small (see 4.2), we trained an adapted SMT model on the concatenation of general-domain sentences and in-domain sentences retrieved in the step before, rather than working with separate models. Algorithm 1 shows the iterative task alternation procedure. In terms of semi-supervised learning, we can view algorithm 1 as non-persistent as we do not keep labels/pairs from previous iterations. We have tried different variations of label persistency but did not find any improvements. A similar effect of preventing the SMT model to "forget" general-domain knowledge across iterations is achieved by mixing models from current and previous iterations. This is accomplished in two ways: First, by linearly interpolating the translation option weights P (t|q) from the current and previous model with interpolation parameter ?. Second, by always using P lex (t|q) weights estimated from word alignments on S gen . We experimented with different ways of using the ranked retrieval results for each query and found that taking just the highest ranked document yielded the best results. This returns one pair of parallel Twitter messages per query, which are then used as additional training data for the SMT model in each iteration. 

 Experiments 

 Data We trained the general domain model M gen on data from the NIST evaluation campaign, including UN reports, newswire, broadcast news and blogs. Since we were interested in relative improvements rather than absolute performance, we sampled 1 million parallel sentences S gen from the originally over 5.8 million parallel sentences. We used a large corpus of Twitter messages, originally created by  Jehl et al. (2012) , as comparable in-domain data. Language identification was carried out with an off-the-shelf tool  (Lui and Baldwin, 2012) . We kept only Tweets classified as Arabic or English with over 95% confidence. After removing duplicates, we obtained 5.5 mil-lion Arabic Tweets and 3.7 million English Tweets (D trg ).  Jehl et al. (2012)  also supply a set of 1,022 Arabic Tweets with 3 English translations each for evaluation purposes, which was created by crowdsourcing translation on Amazon Mechanical Turk. We randomly split the parallel sentences into 511 sentences for development and 511 sentences for testing. All URLs and user names in Tweets were replaced by common placeholders. Hashtags were kept, since they might be helpful in the retrieval step. Since the evaluation data do not contain any hashtags, URLs or user names, we apply a postprocessing step after decoding in which we remove those tokens. 

 Transductive Setup Our method can be considered transductive in two ways. First, all Twitter data were collected by keyword-based crawling. Therefore, we can expect a topical similarity between development, test and training data. Second, since our setup aims for speed, we created a small set of queries Q src , consisting of the source side of the evaluation data and similar Tweets. Similarity was defined by two criteria: First, we ranked all Arabic Tweets with respect to their term overlap with the development and test Tweets. Smoothed per-sentence BLEU  (Lin and Och, 2004)  was used as a similarity metric. OOV-coverage served as a second criterion to remedy the problem of unknown words in Twitter translation. We first created a general list of all OOVs in the evaluation data under M gen (3,069 out of 7,641 types). For each of the top 100 BLEU-ranked Tweets, we counted OOV-coverage with respect to the corresponding source Tweet and the general OOV list. We only kept Tweets containing at least one OOV term from the corresponding source Tweet and two OOV terms from the general list, resulting in 65,643 Arabic queries covering 86% of all OOVs. Our query set Q src performed better (14.76 BLEU) after one iteration than a similar-sized set of random queries (13.39). 

 Experimental Results We simulated the full-scale retrieval approach by  Jehl et al. (2012)  with the CLIR model described in section 3. It took 14 days to run 5.5M Arabic queries on 3.7M English documents. In contrast, our iterative approach completed a single iteration in less than 24 hours. 1 In the absence of a Twitter data set for retrieval, we selected the parameters ? = 0.6 (eq.1), L = 0.005 and C = 0.95 in a mate-finding task on Wikipedia data. The n-best list size for P nbest (t|q) was 1000. All SMT models included a 5-gram language model built from the English side of the NIST data plus the English side of the Twitter corpus D trg . Word alignments were created using GIZA++  (Och and Ney, 2003) . Rule extraction and parameter tuning (MERT) was carried out with cdec, using standard features. We ran MERT 5 times per iteration, carrying over the weights which achieved median performance on the development set to the next iteration. Table  1  reports median BLEU scores on test of our standard adaptation baseline, the full-scale retrieval approach and the best result from our task alternation systems. Approximate randomization tests  (Noreen, 1989; Riezler and Maxwell, 2005)  showed that improvements of full-scale retrieval and task alternation over the baseline were statis-1 Retrieval was done in 4 batches on a Hadoop cluster using 190 mappers at once. tically significant. Differences between full-scale retrieval and task alternation were not significant.  2  Figure  1  illustrates the impact of ?, which controls the importance of the previous model compared to the current one, on median BLEU (a) and change of S in (b) over iterations. For all ?, few iterations suffice to reach or surpass full-scale retrieval performance. Yet, no run achieved good performance after one iteration, showing that the transductive setup must be combined with task alternation to be effective. While we see fluctuations in BLEU for all ?-values, ? = 0.1 achieves high scores faster and more consistently, pointing towards selecting a bolder updating strategy. This is also supported by plot (b), which indicates that choosing ? = 0.1 leads to faster stabilization in the pairs added per iteration (S in ). We used this stabilization as a stopping criterion. 

 Conclusion We presented a method that makes translationbased CLIR feasible for mining parallel sentences from large amounts of comparable data. The key of our approach is a translation-based high-quality retrieval model which gradually adapts to the target domain by iteratively re-training the underlying SMT model on a few thousand parallel sentences retrieved in the step before. The number of new pairs added per iteration stabilizes to a few thousand after 7 iterations, yielding an SMT model that improves 0.35 BLEU points over a model trained on millions of retrieved pairs. Figure 1 : 1 Figure 1: Learning curves for varying ? parameters. (a) BLEU scores and (b) number of new pairs added per iteration. 
