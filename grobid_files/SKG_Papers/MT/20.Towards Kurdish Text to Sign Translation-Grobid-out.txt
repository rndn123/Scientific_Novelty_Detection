title
Towards Kurdish Text to Sign Translation

abstract
The resources and technologies for sign language processing of resourceful languages are emerging, while the low-resource languages are falling behind. Kurdish is a multi-dialect language, and it is considered a low-resource language. It is spoken by approximately 30 million people in several countries, which denotes that it has a large community with hearing-impairments as well. This paper reports on a project which aims to develop the necessary data and tools to process the sign language for Sorani as one of the spoken Kurdish dialects. We present the results of developing a dataset in HamNoSys and its corresponding SiGML form for the Kurdish Sign lexicon. We use this dataset to implement a sign-supported Kurdish tool to check the accuracy of the sign lexicon. We tested the tool by presenting it to hearing-impaired individuals. The experiment showed that 100% of the translated letters were understandable by a hearing-impaired person. The percentages were 65% for isolated words, and approximately 30% for the words in sentences. The data is publicly available at https://github.com/KurdishBLARK/KurdishSignLanguage for non-commercial use under the CC BY-NC-SA 4.0 licence.

Introduction The studies on sign language processing have been emerging, but many areas are still unexplored  (Cormier et al., 2019) . As might be expected, this area of research has even not been initiated yet for many under-resourced languages. Kurdish, a multi-dialect language which is spoken by approximately 30 million people in different countries, is considered an under-resourced language  (Hassani, 2018) . It is also written in different scripts. The usage of the scripts changes according to the geographical situation  (Hassani and Medjedovic, 2016) . The current literature does not report on visible research on Kurdish Sign Language (KuSL) processing, nor are there any publicly available resources for this topic. This research focuses on text to sign conversion for the Sorani dialect of Kurdish. Sign language is the main communication method among the hearing-impaired community. This language is based on visual interaction rather than using sound. The interactions happen by manual and nonmanual signs and finger spelling  (Cooper et al., 2011) . Hand and body movement, shape, orientation and location are within manual signs  (Kelly et al., 2009) , while facial expressions, eye gaze, and shoulder movement are called non-manual signs  (Halawani, 2008) . Furthermore, the finger spelling is used to spell letters of certain words, for example, names and technical terms that do not have sign equivalents  (Liwicki and Everingham, 2009) . Normally, the communication between two hearingimpaired persons is smooth and understandable. The real challenge begins when a hearing person wants to interact with a hearing-impaired person  (Wazalwar and Shrawankar, 2017) . Generally, if the target hearing-impaired person is educated, they try to communicate by exchanging written texts. Otherwise, they turn to a human sign language interpreter as a recourse if available, or else perhaps they end up with serious miscommunication  (Wazalwar and Shrawankar, 2017) . Although the spoken Kurdish dialects use different lexicons  (Ahmadi et al., 2019) , the Kurdish Sign language, which is used in the Kurdistan Region of Iraq (KRI), uses the same lexicon among the hearingimpaired community regardless of the spoken dialect. While according to  Jepsen et al. (2015)  KuSL is not standardized, applying guidelines by  Mohammed (2007)  and using the Kurdish Sign dictionaries (Nashat  Salim et al., 2013; Ghazi Dizayee, 2000)  in the KRI education programs show some efforts towards KuSL standardization. We develop a Kurdish Sign lexicon using the Kurdish Sign Language Dictionary (KuSLD)  (Ghazi Dizayee, 2000) , which is used in KRI. Currently, no Kurdish Sign corpus is available, hence we aim at making Sorani texts sign-supported. That is, in the text conversion process we follow the spoken language and not the sign language structure. Sorani texts are mostly written in Persian-Arabic script (Hassani, 2018) hence we use the developed Kurdish Sign lexicon to make this type of the Sorani texts signsupported. The rest of this paper is organized as follows. Section 2. provides a brief background on sign language processing, Section 3. reviews the related work, Section 4. presents our approach, Section 5. illustrates the developed dataset, Section 6. discusses the results, finally, Section 7. concludes the paper. 

 Sign Language Processing Sign languages are considered as genuine languages that place them among the minority languages  (Senghas and Monaghan, 2002) . Since sign languages consist of visual gestures rather than voice as it is in spoken languages. The analysis and feature extraction of the former significantly differ from latter languages. However, for some languages, a variant of sign language also exists that follows the spoken/written language grammar, which is called sign-supported language  (Elliott et al., 2008) . The development of this variant is less challenging in the absence of required sign corpora and language models. The outcome could be used in various experimental and real-life occasions. Several approaches exist to process sign languages. In the following sections, we discuss those approaches which are more related to our current stage of research. 

 Notation Systems The sign visual gestures are normally denoted by special notations in order to be able to process them. Different notation systems are used to capture these gestures. The most popular ones are Stokoe, SignWriting, and HamNoSys. Stokoe was one of the earliest attempts for a sign language notation system  (McCarty, 2004 ). However, it was only concerned with manual sign representation, and it lacked any consideration for non-manual signs, such as eye gaze and shoulders movements, which are an essential entity to convey meaning by facial expression. SignWriting represents the signed gestures spatially in a 2D canvas  (Bouzid and Jemni, 2013b) . It is designed to facilitate communication among the hearingimpaired community. HamNoSys (Hamburg Notation System) is a phonetic translation system with iconicity, extensibility, and formal syntax characteristics used to denote sign languages  (Hanke, 2004) . A comparative analysis by  (Dhanjal and Singh, 2019)  concluded that HamNoSys is the most widely used notation system for a variety of sign languages. Ham-NoSys symbols are available as a Unicode font  (Hanke, 2004) . This Unicode font symbolizes manual sign gestures and allows the generation of the signs by dividing the description into the handshapes, orientations, locations, and actions. 

 Markup Languages To provide computer encoding for sign languages and to make their processing more efficient, several adoptions of the Extensive Markup Language (XML) have been suggested based on various sign notation systems. The Sign Writing Markup Language (SWML) is a markup language proposed by da Rocha Costa and Dimuro (2001) based on SignWriting. HamNoSys uses Signing Gesture Markup Language (SiGML), which gives a special XML tag to each Ham-NoSys symbol. These markup languages are used in different applications, for instance, to be given to a 3D avatar to animate the signs. 

 Related Work The only work on Kurdish Sign language processing that we were able to retrieve was by  Hashim and Alizadeh (2018)  wherein the researchers reported on their project on Kurdish Sign language recognition. That project focused on the recognition of Kurdish manual alphabets. Therefore, as literature does not report on active studies on Kurdish Sign language processing, we review the topic in the context of other languages.  Sugandhi and Kaur (2018)  introduced an online multilingual dictionary for avatar-based Indian Sign language. The system is designed to accept input from two languages English and Hindi. The input is transliterated into Hindi and then goes through the parser to be translated into Indian Sign Language (ISL). After extracting the root words of the input script, the target Hamburg Notations are retrieved from the database and converted into its corresponding SiGML. The generated SiGML is the input parameter for the Animation server, which uses Web Graphics Library (We-bGL) for the avatar representation.  Aouiti (2013)  proposed an approach to convert Arabic text into Arabic Sign language. The approach used an Arabic sentence/Sign language corpus as a core entity. The corpus includes Arabic sentences that were aligned with their corresponding sign representation. This helped to ensure that the represented sign refers to the real meaning of the input text. Afterward, the target sentence was syntactically and semantically analyzed by applying techniques, such as Morphological, Syntactic, Semantic, and Pragmatic analysis, which led to the generation of the glosses. The sign for each gloss was extracted from the corpus, which was sent to the avatar to be played.  Bouzid and Jemni (2013a)  developed an avatar-based system to enhance the usability and readability of notation systems for deaf people. The system was developed using SignWriting (SW) notation and its markup language. Their focus was to make the path easier for hearing-impaired people to understand and represent signs in a written format. Since SW is presented in a 2D format and it is easy to guess the target gestures from the written notations, this helps hearing-impaired people to learn different sign languages depending on the SW notations. SW is designed for daily communication purposes rather than linguistic and corpus development and processing. An automated reading system for SignWriting representation of Brazilian Sign language was introduced by  Stiehl et al. (2015) . They focused on SignWriting of several Brazilian signs and classified the symbols into several categories. Again, their purpose was to build a database of SignWriting representation for Brazilian Sign Language in order to involve hearing-impaired people into learning the notations and enable them to communicate with each other. This approach can also be used to have books, newspapers, dictionaries and such that are written in notation symbols and can be understood by hearing-impaired people or sign learn-ers. To summarize, we follow the approach of Sugandhi and Kaur (2018) because of two reasons. First, because HamNoSys is a proper method for corpus development, and second because SignWiriting is majorly used for the communication among the hearing-impaired community and not between the hearing-impaired community and people with no hearing difficulties. 

 Method We develop a dataset based on KuSLD. We also prepare and adapt a tool to translate Sorani texts into Kurdish Sign language to be animated by an avatar. We prepare the HamNoSys notations manually by analyzing the gestures available in KuSLD. We use the Ham2HPSG and eSIGN  (Hanke and Popescu, 2003)  to create the dataset and extract the SiGML codes. We implement a sign-supported tool based on the architecture that is shown in Figure  1 . In this architecture, the Language Model (LM), in its current form, is the developed dataset, which could be considered as the Kurdish (Sorani) sign lexicon. The input text goes through the tokenization process to extract the meaningful components from it. Similar to the existing sign-supported tools for other languages, the translation is word-by-word for the words that are found in the LM. Otherwise, the word will be replaced by a sequence of its letters in the sign language. Then the whole text is compiled into SiGML files, which will be sent to an avatar to be animated. We evaluate the tool by feeding it with input of four categories, namely alphabets, numbers, words, and sentences. The tool then plays the translation to the human individuals who are either hearing-impaired or Kurdish Sign language educators. As subjective understanding is not accurate  (Kipp et al., 2011) , we ask the testers to write down their understanding. We calculate the accuracy by the percentage of correctly understood cases for played alphabets, numbers, words, and sentences.  

 Developed Dataset The KuSLD consists of 2315 different sign gestures from 38 different categories. Our dataset, currently, consists of 20% of each category. However, we converted the alphabet and numbers completely. This adds more entries to the dataset, which sums up to approximately 560 entries. The KuSLD categories are listed in Table  1 . A sample for the prepared HamNoSys for Kurdish letters and words is shown in Figures  2 and 3 . We extracted the generated SiGML for the corresponding HamNoSys dataset, which was sent to an avatar to be animated. Two samples of extracted SiGML for the letter ?"?"? (B) and the word ?"?"? (University) in Kurdish are shown in Figures  4 and 5 . 

 Findings and Discussion We played a sample of letters of the prepared dataset to the hearing-impaired individuals. The test showed a 100% understanding of the test data. The results of playing words showed a 65% correct understanding of the played words. The accuracy of the tool for understanding sentences was approximately 30%. In the evaluation process, the person could recognize all shown letters successfully since they are clearly shown in the dictionary. On the other hand, the signs for the words had a lower evaluation outcome. The person could not understand some of the words. One reason for this was the usage of two different sign dictionaries in KRI. One of these dictionaries represents all signs based on the lexicon description, while the other (Ghazi Dizayee, 2000) uses vocal description for some of its entries. Our dataset was developed based on the latter. Both dictionaries are used interchangeably, but they provide different representations for specific signs depending on the context where they appear. This issue also affected sentence evaluation. Also, since we used a word by word translation, the hearing-impaired person was unable to understand the meaning of a majority of the sentences as a whole. Therefore, the sentence evaluation achieved low accuracy, which is typical for the sign-supported systems. 

 Conclusion We used HamNoSys to develop a sign dataset and its equivalent SiGML for Kurdish. We chose HamNoSys over SignWriting because of our plan to develop Kurdish Sign corpora in the future. Our developed dataset includes approximately 560 entries consisting of the alphabet, numbers, and words. We also implemented a tool to translate Sorani texts into the Kurdish Sign language, which could be animated by an avatar. We evaluated the tool by showing the animated output to hearing-impaired persons on the three aspects of understanding the sign gestures, namely letters, words, and sentences. The test showed a 100% understanding for the letters, a 65% for isolated words, and approxi-mately 30% for sentences. The main reasons for the low accuracy were the usage of more than one sign dictionary in the target community and the word-by-word translation of the input texts. As future work, we are targeting the development of a language model based on the grammar of the Kurdish Sign language. Additionally, we aim to add more entries to the developed dataset. Furthermore, we would like to include other Kurdish dialects in the dataset. 
