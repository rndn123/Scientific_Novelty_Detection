title
Unbabel's Participation in the WMT17 Translation Quality Estimation Shared Task

abstract
This paper presents the contribution of the Unbabel team to the WMT 2017 Shared Task on Translation Quality Estimation. We participated on the word-level and sentence-level tracks. We describe our two submitted systems: (i) STACKEDQE, a "pure" QE system, trained only on the provided training sets, which is a stacked combination of a feature-rich sequential linear model with a neural network, and (ii) FULLSTACKEDQE, which also stacks the predictions of an automatic post-editing system, trained on additional data. When evaluated on the English-German and German-English datasets, FULLSTACKEDQE achieved word-level F MULT 1 scores of 56.6% and 52.9%, and sentence-level correlation Pearson scores of 64.1% and 62.6%, respectively. Our system ranked second in both tracks, being statistically indistinguishable from the best system in the word-level track.

Introduction Quality estimation is the task of evaluating a translation system's quality without access to reference translations  (Blatz et al., 2004; Specia et al., 2013) . This paper describes the contribution of the Unbabel team to the Shared Task on Sentence-Level and Word-Level Quality Estimation (QE Tasks 1 and 2) at the 2017 Conference on Statistical Machine Translation (WMT 2017). In the word-level task, the goal is to predict the word-level quality of machine translated text, by assigning a label of OK or BAD to each word in the translation. The sentence-level task attempts to predict the HTER of each sentence, along with a ranking of the sentences. Two language pairs and domains are considered: English-German (IT domain) and German-English (medical domain). Our submission is largely based on the approach that we have recently proposed in  Martins et al. (2017) , which ensembles a "pure" quality estimation system with predictions derived from an automatic post-editing system. The focus was on developing a word-level system, and to use the word label predictions to predict the sentencelevel HTER. Our system architecture is described in full detail in the following sections. We first describe our "pure" QE system ( ?2), which consists of a neural model (NEURALQE) stacked into a linear feature-rich classifier (LINEARQE). Then, we train an APE system (using a large amount of artificial "roundtrip translations") and adapt it to predict word-level quality labels (yielding APEQE, ?3). We show that the pure and the APE-based QE system are highly complementary ( ?4): our best system is a stacked combination of LINEARQE, NEURALQE, and APEQE. By employing a simple word-to-sentence conversion, we adapt our systems to sentence-level QE. Overall, we achieve word-level F MULT 1 scores of 56.6% and 52.9% and sentence-level Pearson scores of 64.1% and 62.6% for English-German and German-English, respectively. The following external resources were used: part-of-speech tags and extra syntactic dependency information were obtained with Turbo-Tagger and TurboParser  (Martins et al., 2013) , 1 trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task  (Seddah et al., 2014) . For the neural models, we used pre-trained word embeddings from Polyglot  (Al-Rfou et al., 2013) . For our FULLSTACKEDQE submission, we also use additional data to train the APE-based QE systems: for English-German, the set of 500K artificial roundtrip translations provided by  Junczys-Dowmunt and Grundkiewicz (2016) , and, for German-English, the UFAL Medical Corpus provided in the WMT17 Biomedical Translation task. 

 Pure Quality Estimation We use the pure quality estimation system developed by the Unbabel team and described in  Martins et al. (2017) , which consists of an ensemble of a linear feature-based classifier with a neural network. We briefly describe the linear ( ?2.1) and neural ( ?2.2) components of our system, as well as their combination ( ?2.3). Further details are presented in  Martins et al. (2016 Martins et al. ( , 2017 . 

 Linear Sequential Model The linear component of our model is a discriminative feature-based sequential model (called LIN-EARQE). The system receives as input a tuple s, t, A , where s = s 1 . . . s M is the source sentence, t = t 1 . . . t N is the translated sentence, and A ? {(m, n) | 1 ? m ? M, 1 ? n ? N } is a set of word alignments. It predicts as output a sequence y = y 1 . . . y N , with each y i ? {BAD, OK}. This is done as follows: y = argmax y N i=1 w ? u (s, t, A, y i ) + N +1 i=1 w ? b (s, t, A, y i , y i?1 ). (1) Above, w is a vector of weights, ? u (s, t, A, y i ) are unigram features (depending only on a single output label), ? b (s, t, A, y i , y i?1 ) are bigram features (depending on consecutive output labels), and y 0 and y N +1 are special start/stop symbols. Table  1  shows the unigram and bigram features used in the LINEARQE system. We include features that depend on the target word and its aligned source word, as well as the context surrounding them.  2  We include also syntactic fea-2 Features involving the aligned source word are replaced tures to detect grammatically incorrect constructions. We use features that involve the dependency relation, the head word, and second-order sibling and grandparent structures. Features involving part-of-speech (POS) tags and syntactic information are obtained with TurboTagger and TurboParser  (Martins et al., 2013) . The feature weights are learned by running 50 epochs of the max-loss MIRA algorithm  (Crammer et al., 2006) , with regularization constant C ? {10 ?k } 4 k=1 and a Hamming cost function placing a higher penalty on false positives than on false negatives (c FP ? {0.5, 0.55, . . . , 0.95}, c FN = 1 ? c FP ), to account for the existence of fewer BAD labels than OK labels in the data. These values are tuned on the development set. 

 Neural System Next, we describe the neural component of our pure QE system, which we call NEURALQE. The architecture of NEURALQE is depicted in Figure  1 . We used  Keras (Chollet, 2015)  to implement our model. The system receives as input the source and target sentences s and t, their word-level alignments A, and their corresponding POS tags obtained from TurboTagger. The input layer follows a similar architecture as QUETCH  (Kreutzer et al., 2015) , with the addition of POS features. A vector representing each target word is obtained by concatenating the embedding of that word with those of the aligned word in the source.  3  The immediate left and right contexts for source and target words are also concatenated. We use We train the model with the RMSProp algorithm  (Tieleman and Hinton, 2012 ) by minimizing the cross-entropy with a linear penalty for BAD word predictions, as in  Kreutzer et al. (2015) . We set the BAD weight factor to 3.0. All hyperparameters are adjusted based on the development set. Target sentences are bucketed by length and then processed in batches (without any padding or truncation). Finally, we also trained 5 independent instances of NEURALQE with different random initializations and different data shuffles. We ensemble these systems by taking the averaged probability of each word being BAD. Tables  2-3  show the results. We observe that, for both language pairs, the neural model outperforms the linear model, and the ensemble of 5 neural systems achieves an extra boost (most noticeable in the English-German dataset). 

 Stacking Neural and Linear Models We now stack the NEURALQE system ( ?2.2) into the LINEARQE system ( ?2.1) as an ensemble strategy; we call the resulting system STACKEDQE. The individual instances of the neural systems are incorporated in the stacking architecture as different features, yielding STACKEDQE. In total, we have 5 predictions (probability values given by each NEURALQE system) for every word in the training, development and test datasets. These predictions are plugged as additional features in the LINEARQE model. As unigram features, we used one real-valued feature for every model prediction at each position, conjoined with the label. As bigram features, we used two real-valued features for every model prediction at the two positions, conjoined with the label pair. For the remainder of this paper, we will take STACKEDQE as our pure QE system. 

 APE-Based Quality Estimation To develop our APE-based QE system (APEQE), we followed a similar approach as the one described in Martins et al. (  2017 ), with a few minor differences, explained below. Junczys-Dowmunt and Grundkiewicz (  2016 ) applied neural translation models to the APE problem, treating different models as components in a log-linear model, allowing for multiple inputs (the source s and the translated sentence t) that were decoded to the same target language (post-edited translation p). Two systems were considered, one using s as the input (s ? p) and another using t as the input (t ? p). For English-German, we used the 500K artificial roundtrip translations provided by the shared task organizers, along with the original data from the shared task (oversampled 20 times, as in  Junczys-Dowmunt and Grundkiewicz (2016) ). For German-English, we only considered the s ? p machine translation system, trained from a subset of the UFAL Medical Corpus provided in the WMT17 Biomedical Translation task. This subset was obtained through crossentropy filtering. For this, we built an in-domain trigram language model from the English postedited training data. We then calculated crossentropy scores for the UFAL corpus according to the language model. We sorted the corpus by increasing cross-entropy and kept the first 500K sentences to be used as additional training data. To convert the resulting APE systems into word-level quality estimators, we need to turn the automatic post-edited sentences into word quality labels. This is done in a straightforward way by using the TERCOM software tool  (Snover et al., 2006)  4 with the default settings (tokenized, case insensitive, exact matching only, shifts disabled). This tool computes the HTER (the normalized edit distance) between the translated and post-edited sentence. As a by-product, it aligns the words in the two sentences, identifying substitution errors, word deletions (i.e. words omitted by the translation system), and insertions (redundant words in the translation). This is mapped deterministically into OK and BAD labels. Our approach for the shared task differs from Martins et al. (  2017 ) in which we skipped the QE-tuning step when training the log-linear APE model; instead, we kept the output of the s ? p and the t ? p systems, converted each to word-level quality labels, and then include the two predictions as additional features in the FULL-STACKEDQE system, described below. We denote the individual systems as APEQE s ? p and APEQE t ? p, and the combined system as FULLSTACKEDQE s, t ? p. 

 Full Stacked System Finally, we consider a larger stacked system where we stack both NEURALQE and APEQE into LIN-EARQE. This mixes pure QE with APE-based QE systems; we call the result FULLSTACKEDQE. The procedure is analogous to that described in ?2.3, with extra binary features for the APE-based word quality label predictions. For training, we used jackknifing to ensure the predictions on the training set are not biased. 

 Word-Level QE The performance of the FULLSTACKEDQE system on the English-German and German-English development datasets are shown in Tables  2-3 . For English-German, we compare with the system from  Martins et al. (2017) . For both language pairs, we can see that the APE-based and the pure QE systems are highly complementary. For English-German, the full combination of the linear, neural, and APE-based systems improves the scores with respect to the best individual system by about 5.5 points. There is a small improvement by including also a feature from APE t ? p, in addition to s ? p. For German-English, we observe an improvement of 4.9 points. 

 Sentence-Level QE We followed the same procedure of  Martins et al. (2017)  to convert word-level quality predictions to a sentence-level HTER prediction. For the APE system, we simply measured the HTER between the translated sentence t and the predicted corrected sentence p. For a pure QE system, we ap-  plied the following word-to-sentence conversion technique: (i) run a QE system to obtain a sequence of OK and BAD word quality labels; (ii) use the fraction of BAD labels as an estimate for HTER. Finally, to combine the APE and pure QE systems toward sentence-level QE, we simply take the average of the two HTER predictions above. Table  4  shows the results obtained with our pure QE system (STACKEDQE), with our APE-based system (APEQE), and with the combination of the two (FULLSTACKEDQE). We report also the performance of the system of  Martins et al. (2017)  for English-German, for comparison. 

 Final Results Finally, we show in Tables 5-6 the results obtained in the test set for our two submitted systems, STACKEDQE and FULLSTACKEDQE, in wordlevel and sentence-level quality estimation. As expected, the inclusion of the predictions made by the APE system gave a significant boost for the word-level task (>5 F MULT 1 points for English-German, and >6 points for German-English) and for the sentence-level task (>5 Pearson correlation points for English-German, >4 points for German-English). 

 Conclusions We have presented the contribution of the Unbabel team to the WMT 2017 Shared Task on Translation Quality Estimation. Our word-level system combines a pure quality estimation system, based on stacking a neural and feature-based linear model, and an APE-based quality estimation system, which uses the predictions of an automatic post-editing system to generate additional features. We applied a simple conversion strategy to obtain a sentence-level quality estimator based on the word-level one. The system is evaluated on two language pairs, English-German and German-English.  Figure 1 : 1 Figure 1: Architecture of our NEURALQE system. 

 Table 1 : 1 by NIL if the target word is unaligned. If there are multiple aligned source words, they are concatenated into a single feature. Features used in the LINEARQE system (see Martins et al., 2016 for a detailed description). Features marked with * are included in the WMT16 baseline system. Those marked with ? were proposed by Kreutzer et al. (2015) . Features Label Input (referenced by the ith target word) unigram  rich bigrams yi ? . . . yi ? yi?1 ? . . . all above yi+1 ? yi ? . . . WORD+SOURCEWORD, POSTAG+SOURCEPOSTAG syntactic yi ? . . . DEPREL, WORD+DEPREL HEADWORD/POSTAG+WORD/POSTAG LEFTSIBWORD/POSTAG+WORD/POSTAG RIGHTSIBWORD/POSTAG+WORD/POSTAG GRANDWORD/POSTAG+HEADWORD/POSTAG+WORD/POSTAG resulting vector representations. The following layers are then applied in se- quence: 1. Two feed-forward layers of size 400 with rec- tified linear units (ReLU; Nair and Hinton (2010)); 2. A layer with bidirectional gated recurrent units (BiGRU, Cho et al. (2014)) of size 200, where forward and backward vectors are con- catenated, trained with layer normalization (Ba et al., 2016); 3. Two feed-forward ReLU layers of size 200; 4. A BiGRU layer of size 100 with identical configuration to the previous BiGRU; 5. Two more feed-forward ReLU layers of sizes 100 and 50, respectively. * BIAS * WORD, LEFTWORD, RIGHTWORD * SOURCEWORD, SOURCELEFTWORD, SOURCERIGHTWORD * LARGESTNGRAMLEFT/RIGHT, SOURCELARGESTNGRAMLEFT/RIGHT * POSTAG, SOURCEPOSTAG ? WORD+LEFTWORD, WORD+RIGHTWORD ? WORD+SOURCEWORD, POSTAG+SOURCEPOSTAG simple bigram yi ? yi?1 ? . . . * BIAS the pre-trained 64-dimensional Polyglot word embeddings (Al-Rfou et al., 2013)  for English and German, and refine them during training. In addition to this, POS tags for each source and target word are also embedded and concatenated. POS embeddings have size 50 and are initialized as described by Glorot and Bengio (2010) . A dropout probability of 0.5 is applied to the As the output layer, a softmax transformation over the OK/BAD labels is applied. We provide ablation experiments in Martins et al. (2017)  to validate this architecture choice. 

 Table 3 : 3 Performance of the several word-level QE systems on the German-English development dataset. 

 Table 4 : 4 Pearson dev Pearson test 2016 Spearman dev Spearman test 2016 Performance of our sentence-level QE systems on the English-German and German-English datasets, as measured by the official evaluation script. We show the performance of Martins et al. (2017)  for comparison. Martins et al. (2017) 64.04 65.56 65.52 65.92 En-De STACKEDQE APEQE FULLSTACKEDQE 62.18 58.79 64.33 59.90 56.69 65.25 64.07 59.41 65.65 60.23 59.43 66.10 De-En STACKEDQE APEQE FULLSTACKEDQE 60.47 59.94 67.97 --- 59.74 57.68 62.45 --- F OK 1 F BAD 1 F MULT 1 EN-DE STACKEDQE EN-DE FULLSTACKEDQE 90.6 62.5 88.2 58.1 51.2 56.6 DE-EN STACKEDQE DE-EN FULLSTACKEDQE 94.1 56.2 93.6 49.7 46.6 52.9 

 Table 5 : 5 Performance of the submitted word-level systems on the test set. Pearson Spearman EN-DE STACKEDQE EN-DE FULLSTACKEDQE 58.9 64.1 61.0 65.2 DE-EN STACKEDQE DE-EN FULLSTACKEDQE 58.0 62.6 57 61 

 Table 6 : 6 Performance of the submitted sentencelevel systems on the test set. 

			 Publicly available on http://www.cs.cmu.edu/ ?ark/TurboParser/. 

			 For the cases in which there are multiple source words aligned to the same target word, the embeddings are averaged. 

			 http://www.cs.umd.edu/ ?snover/tercom.
