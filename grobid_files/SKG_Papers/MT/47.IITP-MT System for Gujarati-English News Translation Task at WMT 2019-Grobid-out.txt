title
IITP-MT System for Gujarati-English News Translation Task at WMT 2019

abstract
We describe our submission to WMT 2019 News translation shared task for Gujarati-English language pair. We submit constrained systems, i.e, we rely on the data provided for this language pair and do not use any external data. We train Transformer based subword-level neural machine translation (NMT) system using original parallel corpus along with synthetic parallel corpus obtained through back-translation of monolingual data. Our primary systems achieve BLEU scores of 10.4 and 8.1 for Gujarati?English and English?Gujarati, respectively. We observe that incorporating monolingual data through back-translation improves the BLEU score significantly over baseline NMT and SMT systems for this language pair.

Introduction In this paper, we describe the system that we submit to the WMT 2019 1 news translation shared task  (Bojar et al., 2019) . We participate in Gujarati-English language pair and submit two systems: English?Gujarati and Gujarati?English. Gujarati language belongs to Indo-Aryan language family and is spoken predominantly in the Indian state of Gujarat. It is a low-resource language as only a few thousands parallel sentences are available, which are not enough to train a neural machine translation (NMT) system as well statistical machine translation (SMT) system. Gujarati-English is a distant language pair and they have different linguistic properties including syntax, morphology, word order etc. English follows subject-verb-object order while Gujarati follows subject-object-verb order. 1 http://www.statmt.org/wmt19/ translation-task.html NMT  (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015)  has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly  (Koehn and Knowles, 2017) . However, back-translation  (Sennrich et al., 2016)  has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English?Gujarati and Gujarati?English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems  (Koehn et al., 2003) , and NMT systems are based on Transformer  (Vaswani et al., 2017)  architecture. Experiments show that NMT systems achieve BLEU  (Papineni et al., 2002)  scores of 10.4 and 8.1 for Gujarati?English and English?Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data. Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Gujarati translation pair, and finally, Section 6 concludes the work. 

 Transformer Architecture Recurrent neural network based encoder-decoder NMT architecture  (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015)  deals with input/output sentences word-by-word sequentially, which prevents the model from parallel computation.  Vaswani et al. (2017)  came up with a highly parallelizable architecture called Transformer which uses the self-attention to better encode a sequences. Self-attention is used in the architecture to calculate attention between a word and the other words in the sentence itself. Encoder and decoder both are stack of 6 identical layers. Each layer in encoder has two sub-layers: i. multi-head self attention mechanism and ii. position wise feed forward network. Each sub-layer is associated with residual connections, followed by layer normalization. Multi-head attention computes the attention multiple times for each word. Since their is no sequence to sequence encoding, positional encoding is used to encode the sequence information. 

 Task Description This task focuses on translating news domain corpus and this year, Gujarati language is introduced for the first time in a WMT shared task. Gujarati is a low-resource language and not many results have been reported in machine translation involving this language. Also, there was no standard test set for this language pair. So introduction of this language pair will help in further research for this language pair. As Gujarati does not have enough parallel data, the data that are provided for this shared task are mainly from WikiTitles which consists of only 11,671 parallel titles. Apart from that, few publicly available domain specific parallel data that are provided are: Bible corpus  (Christodouloupoulos and Steedman, 2015) ; a localization extracted from OPUS 2 ; parallel corpus extracted from Wikipedia; crawled corpus produced for this task; and monolingual Wikipedia dumps. 

 System Description We participated in Gujarati-English pair only and we submit for both directions: English?Gujarati and Gujarati?English. As Gujarati is a lowresource language and only a little amount of parallel data is available, we explore the backtranslation technique for this pair. Also our models are based on Transformer as it has become state of the art for machine translation for many language pairs. We train systems at subword level. For back-translation, we train a phrase-based SMT  (Koehn et al., 2003 ) system for each system in reverse direction. Using these SMT systems, monolingual sentences (for both Gujarati and English) are translated to create synthetic parallel data having original monolingual sentences at target and translated sentences at source side. These synthetic parallel data, along with the original parallel data are used to train a transformer based NMT system for each direction. The datasets that we use for training are shown in the Table  1 , which combine to a total of 155,798 parallel sentences. These parallel data are compiled from different sources. The compiled datasets are Bible 3 , govin-clean.guen.tsv 4 , opus.gu-en.tsv 5 , wikipedia.gu-en.tsv 6 and wikititles-v1.gu-en.tsv 7 . We use newsdev2019 for tuning the model, which has 1,998 parallel sentences.  

 Dataset 

 Sources 

 System 

 Experimental Setup We train phrase based statistical system (PB-SMT)  (Koehn et al., 2003)  as well as Transformer  (Vaswani et al., 2017)  based neural system for comparing their performance under low-resource conditions. In addition to that, PBSMT are used to genrate synthetic parallel data. PBSMT systems are trained only on original training data, while neural based models are trained on original training data (Transfomer in Table  2 ), and also with synthetic parallel data in addition to original data (Transfomer+Synth in Table  2 ). Synthetic parallel data are obtained through back-translation of a target monolingual corpus into source using PB-SMT system. We use Moses  (Koehn et al., 2007)  toolkit for PBSMT training and Sockeye  (Hieber et al., 2017)  toolkit for NMT training. Some preprocessing of data is required before using it for experiment. English data is tokenized using moses tokenizer, and truecased. For tokenizing Gujarati data, we use indic nlp library 8 . After tokeninzation and truecasing, we subword  (Sennrich et al., 2015)  all original data. We apply 10,000 BPE merge operations over English and Gujarati data independently. For back-translation of monolingual data, two PBSMT models English?Gujarati and Gujarati?English are trained over original available parallel subworded corpora. 4-gram lan-8 https://github.com/anoopkunchukuttan/indic nlp library guage model is trained using KenLM  (Heafield, 2011) . For word alignment, we use GIZA++  (Och and Ney, 2003)  with grow-diag-final-and heuristics. Model is tuned with Minimum Error Rate Training  (Och, 2003) . After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English?Gujarati and Gujarati?English PB-SMT model, respectively. We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati?English and English?Gujarati translation directions. Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction. We use the following hyperparameters values of Sockeye toolkit: 6 layers in both encoder and decoder, word embedding size of 512, hidden size of 512, maximum input length of 50 tokens, Adam optimizer, word batch size 1000, attention type is dot, learning rate of 0.0002. The rest of the hyper-parameters are set to the default values in Sockeye. We use early stopping criteria for terminating the training on the validation set of 1,998 parallel sentences. 

 Results The official automatic evaluation uses the following metrics: BLEU  (Papineni et al., 2002) , TER  (Snover et al., 2006) , CharactTER  (Wang et al., 2016) . The official scores are shown in the Table 2. Phrase-base SMT (PBSMT) obtains BLEU scores of 5.2 and 7.3 for English?Gujarati and Gujarati?Englsih, respectively. Whereas, baseline NMT (Transformer) obtains lower BLEU scores of 4.0 and 5.5 for the same directions. Though, SMT systems outperforms baseline NMT systems trained using small amount of original parallel data only. We observe from the   Synth) data obtained through back-translation of monolingual data, outperforms the baseline SMT systems with a margin of 2.9 and 3.1 BELU points. Also, as a result of augmenting backtranslated data with original training data, we obtain improvement of of 4.7 and 5.3 BLEU points over baseline NMT for English?Gujarati and Gujarati?English, respectively. The official preliminary human evaluation results are shown in the Table  3 . 

 Conclusion In this paper, we described our submission to the WMT 2019 News translation shared task for Gujarati-English language pair. This is the first time Gujarati language is introduced in a WMT shared task. We submit Transformer based NMT systems for English-Gujarati language pair. Since the number of parallel sentences in training set are very less and many sentences have length of only 2-3 tokens, BLEU scores for English-Gujarati pair using only available parallel corpus are very low (4.0 and 5.1 for English?Gujarati and Gujarati?English, respectively). So we use monolingual sentences for both languages to create synthetic parallel data through backtranslation, and merged them with original parallel data. We obtained improved BLEU scores of 8.1 and 10.4, respectively. Table 1 : 1 Training data sources and number of sentences. #Sentences Parallel Bible 7,807 govin-clean.gu-en.tsv 10,650 opus.gu-en.tsv 107,637 wikipedia.gu-en.tsv 18,033 wikititles-v1.gu-en.tsv 11,671 Total 155,798 Monolingual Gujarati (Wikipedia dump) 382,881 English (News crawl) 1,000,000 
