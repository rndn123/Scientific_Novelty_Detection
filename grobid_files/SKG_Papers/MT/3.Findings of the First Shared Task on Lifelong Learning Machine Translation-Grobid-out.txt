title
Findings of the First Shared Task on Lifelong Learning Machine Translation

abstract
A lifelong learning system can adapt to new data without forgetting previously acquired knowledge. In this paper, we introduce the first benchmark for lifelong learning machine translation. For this purpose, we provide training, lifelong and test data sets for two language pairs: English-German and English-French. Additionally, we report the results of our baseline systems, which we make available to the public. The goal of this shared task is to encourage research on the emerging topic of lifelong learning machine translation.

Introduction Lifelong learning can be defined as the ability to continually acquire new and retain previous knowledge. This ability characterizes humankind, but it is also reflected in several artificial intelligence systems  (Parisi et al., 2019; Biesialska et al., 2020) . There are many challenges that have to be solved in order to achieve this goal of continual adaptation, among which catastrophic forgetting  (French, 1999)  seems to be the most relevant. Lifelong learning is very useful in the area of machine translation (MT), as it allows MT systems to adapt to new vocabularies and topics, and produce accurate translations across time. Currently, there are no previous works that systematically try to solve the problem. This may be due to the lack of a benchmark to address the challenge  (Biesialska et al., 2020) . In this context, the main goal of the shared task on lifelong learning for MT is to develop systems that can self-adapt relying solely on domain expert data and are then freed from the necessity of machine learning expertise. What is more, this shared task also allows to investigate several MT research directions, such as: the continuous training/adaptation techniques; the preparation of additional pub-licly available corpora and evaluation sets; the active learning methods via a controlled simulated environment; the unsupervised adaptation of MT systems; the document-level approaches and the development and evaluation of MT systems across time. 

 Related work and tasks As mentioned in the introduction, there are not really any works in MT properly evaluating lifelong learning systems. However, there is a long history of studies in related tasks that are useful for addressing the lifelong learning objective. Domain adaptation is based on the premise that the system can adapt to a target domain known in advance. This has been widely studied earlier for statistical MT e.g.  (Koehn and Schroeder, 2007)  and, more recently, for neural MT e.g.  (Luong and Manning, 2015) ). Instance-based adaptation exploits similarity between training and inference instances  (Li et al., 2018) , also in unsupervised scenarios  (Farajian et al., 2017) . These studies have even led to the creation of adaptive MT commercial toolkits  (Federico, 2018) . Importantly, in this task there is no target domain data available. Unsupervised learning focuses on using monolingual corpora to train the translation system, without relying on any parallel corpora  (Artetxe et al., 2018; Lample et al., 2018) . Active learning aims at selecting the most useful source sentences from a monolingual set and query their translation. This selection needs to minimize the post-edited cost and maximize the improvement of a finetuned model  (Liu et al., 2018) . Interactive learning relies on a joint collaboration between a human and an MT system to obtain high-quality translations while reducing the human effort in the process  (Peris et al., 2016) . Our lifelong learning setting differs from both domain and instance-based adaptation, as it depends on the target data (called the lifelong data). The lifelong data set, unlike the training data, is unsupervised. Therefore, it is advisable to use techniques such as unsupervised learning, active learning, or interactive learning to approach the task. 3 Overview of the system and environment The toolchain developed to evaluate the autonomous systems is described in Figure  1 . It is made of four parts: ? the input datasets (purple on Figure  1 ), see section 3.1; ? the four blocks of the system (green on Figure  1  to be modified to include your own system), see appendix A for more details; ? the user simulation (orange on Figure  1 ), see section A.5; ? the evaluation blocks (blue on Figure  1 ), see section 3.2 Note that input datasets, user simulation and evaluation blocks are fixed and guarantee the reproducibility of the experiments. Participants are free to edit the four blocks of the system in order to include their own code. Once your code is included in this toolchain, the system will run automatically and the BEAT platform is responsible for managing the data exchanges between the different blocks of the architecture. Thus, you don't need to take care about the communication between blocks, especially, the interaction between the system and the user simulation is automatic. 

 Datasets Two different datasets are available: the training data and what we called lifelong data. The training data is used to train the preprocessing system (eventually) and the initial system in a supervised way. Source text along with the translation of all documents included in this set are available at any time during the lifelong MT process. Note that no development data is provided, meaning that it is up to the participants to decide how to split the training data into train and development (if one is needed). This year, we used the Europarl and NewsCommentary corpora as training data as they have document information along with their production dates. This represents between 50M and 58.6M words per language depending on the considered language pair (see details in Table  1 ). The lifelong data is available in a sequential manner: each document is processed one after the other to simulate the process along time. This data is unsupervised, meaning that no reference translation is provided (they correspond to the data to translate every day). The system has to provide translations for those documents that will be evaluated. We used the WMT14 English to French and English to German corpus as lifelong learning data. While this allows for comparison with systems that participated in WMT14 News translation shared task, one must keep in mind that the training data is much smaller than what was available for the shared task at the time. The aim here is to demonstrate the effectiveness of the continuous adaptation when compared to a baseline system that does not evolve (lower bound) and the best supervised system (retrained with all available data). In the future, we will extend the lifelong learning data to include that from 2014 up to the most recent one.  

 Evaluation The evaluation is performed in the mt evaluation and BLEU collate blocks. The first block is aimed at collecting scoring statistics for the document being currently processed. In our case, it will correspond to the BLEU modified n-gram precisions. The second block will aggregate those statistics along with the penalisation in order to provide a final score for the system. Each time the user simulation is asked for help,  S pen = S adapt + (S imp ? S cor ) with S adapt being the score of the adapted system and S imp and S cor are the scores of this system where all sentences requested to the user simulation are considered entirely wrong and correct, respectively. Note that in the case of BLEU, the brevity penalty is not impacted by this calculation, only the correct n-gram counts will be decreased proportionally to the sentence requested for translation. For more details, see  (Prokopalo et al., 2020) . 

 Baseline systems Integrating an NMT system in the BEAT platform requires to rethink the code so that everything is done in memory. We chose to use the nmtpytorch toolkit to implement the baseline systems  (Caglayan et al., 2017) . Our baseline systems consists of a 2-layer bidirectionnal GRU  encoder and a 2-layer Conditional GRU decoder  (Sennrich et al., 2017)  equipped with an attention mechanism  as implemented in nmtpytorch. Given a source sequence of embeddings X={x 1 , . . . , x S } and a target sequence of embeddings Y ={y 1 , . . . , y T }, the bidirectional encoder first computes the sequence of annotations corresponding to the concatenation of the hidden states of the two GRU A={a 1 , . . . , a S }. At a given timestep t of decoding, the output layer estimates the probability of the next target word y t as follows: d t = GRU(y t?1 , d t?1 ) c t = Attention(A, query ? d t ) (1) d t = GRU (c t , d t ) o t = tanh(W c c t + W d d t + W y y t?1 ) l t = W o (W b o t + b b ) + b o P (y t |X, Y <t ) = softmax(l t ) For a single training sample, we then maximise the joint likelihood of source and target sentences: L(X, Y ) = T t=1 log (P (y t |X, Y <t )) (2) 

 Adaptation techniques The first adaptation technique used is rather simple. It consists of selecting N sentences from training data that are the closest to the sentences in the document. The chosen similarity metric is the cosine between sentence embeddings obtained by a simple average of word embeddings, as described in  (Arora et al., 2017) . This data is then used to finetune the initial model for maximum 10 epochs with a learning rate of 0.00004, which is ten times smaller than during initial training of the model. Furthermore, we employed an active learning strategy as an adaptation method. In principle, there are two steps involved. Firstly, the model provides a translation for each document from the lifelong learning corpus. As the lifelong learning data are unsupervised; therefore, a quality estimation (QE) technique is used to evaluate the quality of the translations without any access to a reference translation. Every document is evaluated using sentence-level HTER scores  (Specia et al., 2018) . Secondly, an OpenKiwi QE model  (Kepler et al., 2019)  is used to rank the sentences according to their quality, and those with the worst HTER score are sent to the user simulation (active learning), which provides the correct translation of the selected sentences. This process implies the penalisation of the BLEU score as explained in Section 3. 

 Experimental setup The dimensions of embeddings and GRU hidden states are set to 128 and 256, respectively. The embeddings are shared in the decoder  (Press and Wolf, 2017) . We use ADAM (Kingma and Ba, 2014) as the optimiser and set the learning rate and mini-batch size to 0.0004 and 64, respectively. Regularisation is done by means of a weight decay of 1e?5 and the use of dropout on the embeddings, the source context and the output (set at 0.4)  (Srivastava et al., 2014) . We clip the gradients if the norm of the full parameter vector exceeds 1  (Pascanu et al., 2013) . The data is processed by a joint BPE model with 30k merge operations  (Sennrich et al., 2016a) . This leads to respectively 20.7k and 25.1k units for English and French and 17.2k and 26.5k units for English and German, respectively. We train each model for a maximum of 100 epochs and early stop the training if validation BLEU  (Papineni et al., 2002)  does not improve for 10 epochs (Figure  2 ). We also halve the learning rate if no improvement is obtained for three epochs. The number of learnable parameters is around 8.7M for En-Fr and 8.5M for En-De.  

 Results The results of the baseline systems and the adapted ones can be found in Table  2  Results show that a simple data selection method along with finetuning can provide a small improvement of the system's performance for English to French. German is known to be a more complicated language, as demonstrated by the lower results and the inefficient effect of the adaptation method. 

 Discussion and next year evaluation We can see that the task, given the very constrained data is very hard. A simple comparison with the results of the systems that participated in the WMT14 News translation task shows more than 10 BLEU points difference. We insist on the fact that the main goal of the challenge is to provide new methods to incrementally adapt the model to incoming documents. Without loss of generality, it is very probable that even with a better baseline system (trained on more data), the adapted models would exhibit a similar improvement. Many questions and challenges remain open as to how lifelong learning for MT should be implemented. Next year, we ought to push further the evaluation by improving the QE model in order to better select the sentences to be sent to the user Simulation (Active Learning module). Hence, this will require to reconsider how the systems are evaluated. This year, we introduced a way of penalising the systems but without corresponding results. We hope to have more participants bringing new ideas either by using the current baseline models (and avoiding the integration burden) or by integrating their own systems into the platform. (Marta R.  Costa-juss? and Magdalena Biesialska)  and LNE (Olivier Galibert) in the framework of the EU Chist-ERA funded ALLIES project. This work is is supported in part by the Spanish Ministerio de Ciencia e Innovaci?n, through the postdoctoral senior grant Ram?n y Cajal and by the Agencia Estatal de Investigaci?n through the projects EUR2019-103819, PCIN-2017-079 and PID2019-107579RB-I00 / AEI / 10.13039/501100011033. 

 A LLMT system This section describes the four different blocks that compose the LLMT system. The architecture of the system has been developed according to standard MT architectures. In order to facilitate the development of your system and to provide a baseline, a complete implementation of a LLMT system using nmtpytorch  (Caglayan et al., 2017)  is provided on the evaluation web page, see http://www.statmt.org/wmt20/ lifelong-learning-task.html for more details. General note: the prototypes of the process functions must not be changed! 

 A.1 Train and apply preprocessing This block is responsible for preparing the training data. Preprocessing may include tokenization, learning subword decomposition model, etc. It is also responsible for creating the source and target vocabularies that will be used by the system.  

 A.2 Train initial model The initial training of the system is implemented in the file algorithms/loicbarrault/mt train model/1.py. The process method is the main one. From this method, you can access all the training data from the train preprocessing block. This block outputs a model. The data is available through the data loader. In the provided baseline system, the processing consists of: tokenizing the data with Moses tokenizers , training and applying a BPE model with subword nmt  (Sennrich et al., 2016b) . As for the previous block, the output is written in the corresponding variable. Figure 2 : 2 Figure 2: Training loss and BLEU scores for the English?German MT system. 

 # this will be called each time the sync'd input has more data available to be processed d e f process( s e l f , data_loaders, outputs): (data, _, e n d _ d a t a _ i n d e x ) = data_loaders[0][0] data_dict = pickle.loads(data["train_data"].text.encode("latin1")) #HERE: USE YOUR SOFTWARE FUNCTIONS TO TRAIN A MODEL # The model is Pickled w i t h torch.save() and converted into a 1D-array of uint8 # Pass the model to the next block outputs['model'].write({'value': model}, e n d _ d a t a _ i n d e x ) # always return True, it signals BEAT to continue processing r e t u r n True 

 Table 1 : 1 Statistics of the newstest2014 English-French and English-German corpora. Training data (from 01.01.1996 to 31.12.2013) English French English German #Documents 15218 15472 #Segments 2308516 2246090 #Words 55.6M 58.6M 53.6M 50.4M Lifelong data (newstest2014) English French English German #Documents 176 164 #Segments 3003 3003 #Words 62.3k 69.6k 59.3k 55.1k 

 Flowchart of the lifelong learning MT system running on the BEAT platform. a penalisation is calculated based o the request. The final penalised score S pen corresponds to the following score: Training data WMT train data from 1996 to 2013 With document information Raw training data Train and apply preprocessing BPE model + Vocabularies Tokenized training data Training of initial model Initial model User simulation Lifelong data One document at a time! Lifelong learning loop One document at a time! WMT train and dev data from 2014 to 2020, source side Raw lifelong data Apply preprocessing Tokenized lifelong data A. B. C. Unsupervised adaptation Active Learning Interactive learning document Translated BLEU evaluation and penalisation Statistics BLEU collate only With document information Figure 1: 

 . English?French English?German Baseline SHEFFIELD 25.7 15.6 UPC 26.2 14.7 Data selection + finetuning SHEFFIELD 26.4 15.5 UPC 26.4 15.1 Table 2: BLEU scores on the newstest2014 English?French and English?German. 

 To do so, the entire training set is available at once (as in standard training protocol). The prepared training data is sent to the train initial model (sec. A.2) block while the subword model and vocabularies are sent to the apply preprocessing block (sec. A.3). d e f process( s e l f , data_loaders, outputs): # Get the training data data_loader = data_loaders[0] f o r i i n r a n g e (data_loader.count()): (data, _, e n d _ i n d e x ) = data_loader[i] ... data["train_source_raw"].text ... data["train_target_raw"].text ... data["train_file_info"] #Note: setup_for_nmtpytorch(data_loaders) does that for you #HERE: DO AS MUCH DATA PREPARATION AS YOU WISH #Create vocabulary and BPE or SPM model data_dict_tok, src_vocab, trg_vocab, subword_model = preprocess(data_dict, s e l f .source_language, s e l f .target_language, s e l f .min_freq, s e l f .short_list) data_dict_pickle = pickle.dumps(data_dict_tok).decode("latin1") #Write all the necessary outputs outputs['train_data_tokenized'].write({'text':data_dict_pickle}, e n d _ i n d e x ) outputs['source_vocabulary'].write({'text':src_vocab}, e n d _ i n d e x ) outputs['target_vocabulary'].write({'text':trg_vocab}, e n d _ i n d e x ) outputs['subword_model'].write({'text':subword_model}, e n d _ i n d e x ) # always return True, it signals BEAT to continue processing r e t u r n True 

			 https://docs.conda.io/en/latest/
