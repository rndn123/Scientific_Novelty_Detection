title
IPA and STOUT: Leveraging Linguistic and Source-based Features for Machine Translation Evaluation

abstract
This paper describes the UPC submissions to the WMT14 Metrics Shared Task: UPC-IPA and UPC-STOUT. These metrics use a collection of evaluation measures integrated in ASIYA, a toolkit for machine translation evaluation. In addition to some standard metrics, the two submissions take advantage of novel metrics that consider linguistic structures, lexical relationships, and semantics to compare both source and reference translation against the candidate translation. The new metrics are available for several target languages other than English. In the the official WMT14 evaluation, UPC-IPA and UPC-STOUT scored above the average in 7 out of 9 language pairs at the system level and 8 out of 9 at the segment level.

Introduction Evaluating Machine Translation (MT) quality is a difficult task, in which even human experts may fail to achieve a high degree of agreement when assessing translations. Conducting manual evaluations is impractical during the development cycle of MT systems or for transation applications addressed to general users, such as online translation portals. Automatic evaluation measures bring valuable benefits in such situations. Compared to manual evaluation, automatic measures are cheap, more objective, and reusable across different test sets and domains. Nonetheless, automatic metrics are far from perfection: when used in isolation, they tend to stress specific aspects of the translation quality and neglect others (particularly during tuning); they are often unable to capture little system improvements (enhancements in very specific aspects of the translation process); and they may make unfair comparisons when they are not able to reflect real differences among the quality of different MT systems  (Gim?nez, 2008) . ASIYA, the core of our approach, is an opensource suite for automatic machine translation evaluation and output analysis.  1  It provides a rich set of heterogeneous metrics and tools to evaluate and analyse the quality of automatic translations. The ASIYA core toolkit was first released in 2009  (Gim?nez and M?rquez, 2010a)  and has been continuously improved and extended since then  (Gonz?lez et al., 2012; Gonz?lez et al., 2013) . In this paper we first describe the most recent enhancements to ASIYA: (i) linguistic-based metrics for French and German; (ii) an extended set of source-based metrics for English, Spanish, German,  French, Russian, and Czech; and (iii)  the integration of mechanisms to exploit the alignments between sources and translations. These enhancements are all available in ASIYA since version 3.0. We have used them to prepare the UPC submissions to the WMT14 Metrics Task: UPC-IPA and UPC-STOUT, which serve the purpose of testing their usefulness in a real comparative setting. The rest of the paper is structured as follows. Section 2 describes the new reference-based metrics developed, including syntactic parsers for languages other than English. Section 3 gives the details of novel source-based metrics, developed for almost all the language pairs in this challenge. Section 4 explains our simple metrics combination strategy and analyses the results obtained with both approaches, UPC-IPA and UPC-STOUT, when applied to the WMT13 dataset. Finally, Section 5 summarises our main contributions. 

 Reference-based Metrics We recently added a new set of metrics to ASIYA, which estimate the similarity between reference (ref ) and candidate (cand) translations. The met-rics rely either on structural linguistic information (Section 2.1), on a semantic mapping (Section 2.2), or on word n-grams (Section 2.3). 

 Parsing-based Metrics Our initial set of parsing-based metrics is a followup of the proposal by  Gim?nez and M?rquez (2010b) : it leverages the structural information provided by linguistic processors to compute several similarity cues between two analyzed sentences. ASIYA includes plenty of metrics that capture syntactic and semantic aspects of a translation. New metrics based on linguistic structural information for French and German and upgraded versions of the parsers for English and Spanish are available since version 3.0. 2 In the WMT14 evaluation, we opt for metrics based on shallow parsing (SP), constituency parsing (CP), and dependency parsing (DPm)  3  . Measures based on named entities (NE) and semantic roles (SR) were used to analyse translations into English as well. The nomenclature used below follows the same patterns as in the ASIYA's manual  (Gonz?lez and Gim?nez, 2014) . The manual describes every family of metrics in detail. Next, we briefly depict the concrete metrics involved in our submissions to the WMT14 Shared Task. The set of SP metrics is available for English, German, French, Spanish and Catalan. They measure the lexical overlapping between parts-ofspeech elements in the candidate and reference translations. For instance, SP-Op(VB) measures the proportion of correctly translated verbs; and the coarser SP-Op(*) averages the overlapping between the words for each part of speech. We also use NIST  (Doddington, 2002)  to compute accumulated scores over sequences of n = 1..5 parts of speech (SP-pNIST). Similarly, CP metrics analyse similarities between constituent parse trees associated to candidate and reference translations. For instance, CP-STMi5 and CP-STM4 compute, respectively, the proportion of (individual) length-5 and accumulated up to length-4 matching sub-paths of the syntactic tree  (Liu and Gildea, 2005) . CP-Oc(*) computes the lexical overlap averaged over all the phrase constituents. Constituent trees are obtained using the parsers of  Charniak and Johnson (2005) , Bonsai v3.2  (Candito et al., 2010b) , and Berkeley Parser  (Petrov et al., 2006; Petrov and Klein, 2007)  for English, French, and German, respectively. Measures based on dependency parsing (DPm) -available for English and French thanks to the MALT parser  (Nivre et al., 2007) -capture the similarities between dependency tree items (i.e., heads and modifiers). The pre-trained models for French were obtained from the French Treebank  (Candito et al., 2010a)  and used to train the Bonsai parser, which in turn uses the MALT parser. For instance, DPm-HWCM w-3 retrieves average accumulated proportion of matching word-chains  (Liu and Gildea, 2005)  up to length 3; and DPm-HWCMi c-3 computes the proportion of matching category-chains of length 3. 

 Explicit-Semantics Metric Additionally, we borrowed a metric originally proposed in the field of Information Retrieval: explicit semantic analysis (ESA)  (Gabrilovich and Markovitch, 2007) . ESA is a similarity metric that relies on a large corpus of general knowledge to represent texts. Our knowledge corpora are composed of ? 100K Wikipedia articles from 2010 for the following target languages: English, French and German. In this case, ref and cand translations are both mapped onto the Wikipedia collection W . The similarities between each text and every article a ? W are computed on the basis of the cosine measure in order to compose a similarities vector that represents the text. That is: ref = {sim(ref, a) ?a ? W } , (1) cand = {sim(cand, a) ?a ? W } . (2) As the i-th elements in both ref and cand represent the similarity of ref and cand sentences to a common article, the similarity between ref and cand can be estimated by computing sim( ref, cand). 

 Language-Independent Resource-Free Metric We consider a simple characterisation based on word n-grams. Texts are broken down into overlapping word sequences of length n, with 1-word shifting. The similarity between cand and ref is computed on the basis of the Jaccard coefficient  (Jaccard, 1901) . We used this metric for the pairs English-Russian and Russian-English, considering n = 2 (NGRAM-jacTok2ngram). For the rest of the pairs we opt for the character-n-gram metrics described in Section 3.1, but they showed no positive results in the English-Russian pair during our tuning experiments. 

 Source-based Metrics We enhance our evaluation module by including a set of new metrics that compare the source text against the translations. The metrics can be divided into two subsets: those that do not require any external resources (Section 3.1) and those that depend on a parallel corpus (Section 3.2). 

 Language-Independent Resource-Free Metrics We opted for two characterisations that allow for the comparison of texts across languages without external resources nor language-related knowledge -as far as the languages use the same writing system.  4  The first characterisation is character n-grams; proposed by  McNamee and Mayfield (2004)  for cross-language information retrieval between European languages. Texts are broken down into overlapping character sequences of length n, with 1-character shifting. We opt for case-folded bigrams (NGRAM-cosChar2ngrams), as they allowed for the best performance across all the pairs (except for From/To Russian pairs) during tuning. The second characterisation (NGRAM-jacCognates) is based on the concept of cognateness; originally proposed for bitexts alignment  (Simard et al., 1992) . A word is a pseudo-cognate candidate if (i) it has only letters and |w| ? 4, (ii) it contains at least one digit, or (iii) it is a single punctuation mark. src and cand sentences are then represented as word vectors, containing only those words fulfilling one of the previous conditions. In the case of (i) the word is cut down to its leading four characters only. In both cases (character n-grams and cognateness) cand translations are compared against src sentences on the basis of the cosine similarity measure. 

 Parallel-Corpus Metrics We consider two metrics that make use of parallel corpora: length factor and alignment. The length factor (LeM) is rooted in the fact that the length of a text and its translation tend to preserve a certain length correlation. For instance, translations from English into Spanish or French tend to be longer than their source. Similar measures were proposed during the statistical machine translation early days, both considering characterand word-level lengths  (Gale and Church, 1993; Brown et al., 1991) .  Pouliquen et al. (2003)  defines the length factor as: (d ) = e ?0.5 |d | |dq | ? ? 2 , (3) where ? and ? represent the mean and standard deviation of the character lengths between translations of texts from L into L . This is a stochastic normal distribution that results in higher values as the length of the target text approaches the expected value given the source. Table  1  includes the values for each language pair, as estimated on the WMT13 parallel corpora. Note that this metric was not applied to Hindi-English since this language pair was not present in the WMT13 challenge. The last of our newly-added measures relies on the word alignments calculated between the sentence pairs src-cand and src-ref. We trained alignment models for each language pair using the Berkeley Aligner 5 , and devised three variants of an ALGN metric, which compute: (i) the proportion of aligned words between src and cand (AL-GNs); (ii) the proportion of aligned words between cand and ref, calculated as the combination of the alignments src-cand and src-ref (ALGNr); and (iii) the ratio of shared alignments between srccand and src-ref (ALGNp). 

 Experimental Results The tuning and selection of the different metrics to build UPC-IPA and UPC-STOUT was conducted considering the WMT13 Metrics Task dataset  (Mach?ek and Bojar, 2013) , and the resources distributed for the WMT13 Translation Task . Table  2  gives a complete list of these metrics grouped by families. First, we calculated the Pearson's correlation with the human judgements for all the metrics in the current version of the ASIYA repository, including standard MT evaluation metrics, such as METEOR  (Denkowski and Lavie, 2011) ,  GTM (Melamed et al., 2003) , -TERp-A  (Snover et al., 2009 ) (a variant of TER tuned towards adequacy), WER  (Nie?en et al., 2000)  and PER  (Tillmann et al., 1997) . We selected the best performing metrics (i.e., those resulting in high Pearson coefficients) in each family across all the From/To English translation language pairs, including the newly developed measures -even if they performed poorly compared to others (see This is how the UPC-STOUT metrics sets for both from English and To English translation pairs were composed 6 (see Table  3 ). Table  2 : Metrics considered in the experiments separated by families according to the type of grammatical items they use. The metric sets included in UPC-IPA are light versions of the UPC-STOUT ones. They were composed following different criteria, depending on the translation direction. Parsing-based measures were already available in the previous version of ASIYA when translating into Englishthey are known to be robust across domains and are usually good indicators of translation quality  (Gim?nez and M?rquez, 2007) . So, in order to assess the gain achieved with these measures with respect the new ones, UPC-IPA neglects the measures based on structural information obtained from parsers. In contrast, this distinction was not suitable for the From English pairs since the number of resources and measures varies for each language. Hence, in this latter case, UPC-IPA used only the subset of measures from UPC-STOUT that required no or little resources. In summary, when English is the target language, UPC-IPA uses the baseline evaluation metrics along with the length factor, alignmentsbased metrics, character n-grams, and ESA. In addition to the above metrics, UPC-STOUT uses the linguistic-based metrics over parsing trees, named entities, and semantic roles. When English is the source language, UPC-IPA relies on the basic collection of metrics and character n-grams only. UPC-STOUT includes the alignment-based metrics, length factor, ESA, and the syntactic parsers applied to both German and French. In all cases (metric sets and language pairs), the translation quality score is computed as the uniformly-averaged linear combination (ULC) of all the individual metrics for each sentence in the testset. Its calculation implies the normalization of heterogeneous scores (some of them are negative or unbounded), into the range [0, 1]. As a consequence, the scores of UPC-IPA and UPC-STOUT constitute a natural way of ranking different translations, rather than an overall quality estimation measure. We opt for this linear combination for simplicity. The discussion below suggests that a more sophisticated method for weight tuning (e.g., relying on machine learning methods) would be required for each language pair, domain and/or task since different metric families perform notably different for each subtask. We complete our experimentation by evaluating more configurations: BAS, a baseline with standard and commonly used MT metrics; SYN, the reference-based syntactic metrics; SEM, the reference-based semantic metrics; SRC, the source-based metrics; and the combination of BAS with every other configuration: BAS+SYN, BAS+SEM, and BAS+SRC. Their purpose is to evaluate the contribution of the newly developed sets of metrics with respect to the baseline. The composition of the different configurations is summarised in Tables  2 and 3 . Evaluation results are shown in Tables  4 and 5 . For each configuration and language pair, we show the correlation coefficients obtained at the systemand the segment-level, respectively. As customary with the WMT13 dataset, Pearson correlation was computed at the system-level, whereas Kendall's ? was used to estimate segment-level rank correlations. Additionally to the two submitted and seven extra configurations, we include the coefficients obtained with the Best and Worst systems reported in the official WMT13 evaluation for each language pair. Although the results of our two submitted systems are not radically different to each other, UPC-STOUT consistently outperforms UPC-IPA. The currently available version of ASIYA, including the new metrics, allows for a performance close to the top-performing evaluation measures in last year's challenge, even with our na?ve combination strategy. It is worth noting that no configuration behaves the same way throughout the different languages. In some cases (e.g., with the SRC configuration), the bad performance can be explained by the weaknesses of the necessary resources when computing certain metrics. When analysed in detail, the cause can be ascribed to different metric families in each case. As a result, it is clear that specific configurations are necessary for evaluating different languages and domains. We plan to approach these issues as part of our future work. When looking at the system-level figures, one can observe that the SEM set allows for a considerable improvement over the baseline system. The further inclusion of the SYN set -when available-, tends to increase the quality of the estimations, mainly when English is the source language. These properties impact on some of the UPC-STOUT configurations. In contrast, when looking at the segment-level scores, while the SYN measures still tend to provide some gain over the baseline, the SEM ones do not. Finally, it merits some attention the good results achieved by the baseline for translations into English. We may remark here that our baseline included also the best performing state-of-the-art metrics, including all the variants of METEOR, that reported good results in the WMT13 challenge. Tables  6 and 7  show the official results obtained by UPC-IPA and UPC-STOUT in WMT14.  8  The best and worst figures for each language pair are included for comparison -the worst performing submission at segment level is neglected as it seems to be a dummy  (Mach?ek and Bojar, 2014 to appear) . Both UPC-IPA and UPC-STOUT configurations resulted in different performances depending on the language pair. UPC-STOUT scored above the average for all the language pairs except for en-cs at both system and segment level, and en-ru at system level. Although the evaluation results are not directly comparable to the WMT13 ones, one can note that the results were notably better for pairs that involved Czech and Russian, and worse for those that involved French and German at system level. Analysing the impact of the evaluation methods and building comparable results in order to address a study on configurations for different languages is part of our future work. 

 Conclusions This paper describes the UPC submission to the WMT14 metrics for automatic machine translation evaluation task. The core of our evaluation system is ASIYA, a toolkit for MT evaluation. Besides the formerly available metrics in ASIYA, we experimented with new metrics for machine trans- As previous work on English as target language has proven, syntactic and semantic analysis can contribute positively to the evaluation of automatic translations. For this reason, we integrated a set of new metrics for different languages, aimed at evaluating a translation from different perspectives. Among the novelties, (i) new shallow metrics, borrowed from Information Retrieval, were included to compare the candidate translation against both the reference translation (monolingual comparison) and the source sentence (cross-language comparison), including explicit semantic analysis and the lexical-based characterisations character ngrams and pseudo-cognates; (ii) new parsers for other languages than English were applied to compare automatic and reference translation at the syntactic level; (iii) an experimental metric based on alignments; and (iv) a metric based on the correlation of the translations' expected lengths was included as well. Our preliminary experiments showed that the combination of these and standard MT evaluation metrics allows for a performance close to the best in last year's competition for some language pairs. The new set of metrics is already available in the current version of the toolkit ASIYA v3.0  (Gonz?lez and Gim?nez, 2014) . Our current efforts are focused on the exploitation of more sophisticated methods to combine the contributions of each metric, and the extension of the list of supported languages. Table 1 : 1 Length factor parameters as estimated on the WMT13 parallel corpora. pair ? ? pair ? ? en-cs 0.972 0.245 cs-en 1.085 0.273 en-de 1.176 0.926 de-en 0.961 0.463 en-fr 1.158 0.411 fr-en 0.914 0.313 en-ru 1.157 0.678 ru-en 1.069 0.668 

 Table 4 : 4 System-level Pearson correlation for automatic metrics over translations From/To English. WMT13 en-fr en-de en-es en-cs en-ru fr-en de-en es-en cs-en ru-en UPC-IPA 93.079 85.147 88.702 85.259 70.345 96.755 94.660 95.065 94.316 72.083 UPC-STOUT 94.274 90.193 73.314 84.743 70.544 96.916 96.208 96.704 96.666 74.050 BAS 92.502 84.251 90.051 86.584 67.655 95.777 96.506 95.98 96.539 71.536 SYN 95.68 87.297 96.965 n/a n/a 96.291 96.592 96.052 95.238 73.083 BAS+SYN 94.584 87.786 95.162 n/a n/a 96.684 97.057 96.101 96.402 72.800 SEM 89.735 83.647 35.694 95.067 n/a 95.629 96.601 98.021 96.595 76.158 BAS+SEM 92.254 87.005 47.321 89.107 n/a 96.337 97.534 97.568 97.371 74.804 SRC 14.465 -16.796 -22.466 -49.981 39.527 13.405 -51.371 71.64 -73.254 68.766 BAS+SRC 93.637 76.401 83.754 64.742 54.128 95.395 90.889 93.299 89.216 71.882 WMT13-Best 94.745 93.813 96.446 86.036 81.194 98.379 97.789 99.171 83.734 94.768 WMT13-Worst 78.787 -45.461 87.677 69.151 61.075 95.118 92.239 79.957 60.918 82.058 

 Table 5 : 5 Segment-level Kendall's ? correlation for automatic metrics over translations From/To English. WMT13 en-fr en-de en-es en-cs en-ru fr-en de-en es-en cs-en ru-en UPC-IPA 18.625 14.901 17.057 7.805 15.132 22.832 25.769 26.907 21.207 19.904 UPC-STOUT 19.488 15.012 17.166 8.545 15.279 23.090 27.117 26.848 21.332 19.100 BAS 19.477 13.589 16.975 8.449 15.599 24.060 28.259 28.381 23.346 20.983 SYN 16.554 14.970 16.444 n/a n/a 22.365 24.289 23.889 20.232 17.679 BAS+SYN 19.112 16.016 18.122 n/a n/a 23.940 28.068 27.988 23.180 19.659 SEM 12.184 9.249 10.871 3.808 n/a 17.282 19.083 20.859 15.186 14.971 BAS+SEM 19.167 13.291 15.857 7.732 n/a 22.024 25.788 26.360 21.427 19.117 SRC 2.745 2.481 1.152 1.992 5.247 2.181 1.154 8.700 -4.023 16.267 BAS+SRC 18.32 13.017 15.698 7.666 13.619 22.292 24.948 26.780 17.603 20.707 WMT13-Best 21.897 19.459 20.699 11.283 18.899 26.836 29.565 24.271 21.665 25.584 WMT13-Worst 16.753 13.910 3.024 4.431 13.166 14.008 14.542 14.494 9.667 13.178 

 Table 6 : 6 System-level Pearson correlation results in the WMT14 Metrics shared task en-fr en-de en-cs en-ru UPC-IPA 93.7 13.0 96.8 92.2 UPC-STOUT 93.8 14.8 93.8 92.1 WMT14-Best 95.9 19.8 98.8 94.2 WMT14-Worst 88.8 1.1 93.8 90.3 fr-en de-en hi-en cs-en ru-en UPC-IPA 96.6 89.4 91.5 82.4 80.0 UPC-STOUT 96.8 91.4 89.8 94.7 82.5 WMT14-Best 98.1 94.2 97.6 99.3 86.1 WMT14-Worst 94.5 76.0 41.1 74.1 -41.7 

 Table 7 : 7 Segment-level Kendall's ? correlation results in the WMT14 Metrics shared task en-fr en-de en-cs en-ru UPC-IPA 26.3 21.7 29.7 42.6 UPC-STOUT 27.8 22.4 28.1 42.5 WMT14-Best 29.7 25.8 34.4 44.0 WMT14-Worst 25.4 18.5 28.1 38.1 fr-en de-en hi-en cs-en ru-en UPC-IPA 41.2 34.1 36.7 27.4 32.4 UPC-STOUT 40.3 34.5 35.1 27.5 32.4 WMT14-Best 43.3 38.1 43.8 32.8 36.4 WMT14-Worst 31.1 22.5 23.7 18.7 21.2 lation evaluation, with especial focus on transla- tion from English into other languages. 

			 http://asiya.lsi.upc.edu 

			 Equivalent resources were previously available for English, Catalan, and Spanish. 3  ASIYA includes two dependency parsers; the m identifies the metrics calculated using the MALT parser. 

			 Previous research showed that transliteration is a good short-cut when dealing with different writing systems (Barr?n-Cede?o et al., 2014) . 

			 https://code.google.com/p/berkeleyaligner 

			 Parser-based measures are not present in Czech nor Russian as target languages, ALGN is not available for French pairs, and ESA is not applied to Russian as target. 

			 These are the full sets of measures for each configuration. However, each specific subset for From/To English can vary slightly depending on the available resources. 

			 At the time of submitting this paper, the evaluation results for WMT14 Metrics Task were provisional.
