title
Learning to Evaluate Translation Beyond English BLEURT Submissions to the WMT Metrics 2020 Shared Task

abstract
The quality of machine translation systems has dramatically improved over the last decade, and as a result, evaluation has become an increasingly challenging problem. This paper describes our contribution to the WMT 2020 Metrics Shared Task, the main benchmark for automatic evaluation of translation. We make several submissions based on BLEURT, a previously published metric which uses transfer learning. We extend the metric beyond English and evaluate it on 14 language pairs for which fine-tuning data is available, as well as 4 "zero-shot" language pairs, for which we have no labelled examples. Additionally, we focus on English to German and demonstrate how to combine BLEURT's predictions with those of YISI and use alternative reference translations to enhance the performance. Empirical results show that the models achieve competitive results on the WMT Metrics 2019 Shared Task, indicating their promise for the 2020 edition.

Introduction The recent progress in machine translation models has led researchers to question the use of ngram overlap metrics such as BLEU, which focus solely on surface-level aspects of the generated text, and thus may correlate poorly with human evaluation  (Papineni et al., 2002; Lin, 2004; Ma et al., 2019; Mathur et al., 2020; Belz and Reiter, 2006; Callison-Burch et al., 2006) . This has led to a surge of interest for more flexible metrics that use machine learning to capture semantic-level information  (Celikyilmaz et al., 2020) . Popular examples of such metrics include YISI-1  (Lo, 2019) , ESIM  (Mathur et al., 2019) , BERTSCORE  (Zhang et al., 2020) , the Sentence Mover's Similarity  (Zhao et al., 2019; Clark et al., 2019) , and BLEURT  (Sellam et al., 2020) . These metrics utilize contextual embeddings from large models such as BERT  (Devlin et al., 2019)  which have been shown to capture linguistic information beyond surface-level aspects  (Tenney et al., 2019) . The WMT Metrics 2020 Shared Task is the reference benchmark for evaluating these metrics in the context of machine translation. It tests the evaluation of systems that are to-English (X ? En) and to other languages (X ? Y), which requires a multilingual approach. An additional challenge for learned metrics is that human ratings are not available for all language pairs, and therefore, the models must use unlabeled data and perform zero-shot generalization. We describe several learned metrics based on BLEURT  (Sellam et al., 2020) , originally developed for English data. We first extend BLEURT to the multilingual setup, and show that our approach achieves competitive results on the WMT Metrics 2019 Shared Task.  1  We also present several simple BERT-based baselines, which we submit for analysis. Finally, we focus on English to German and enhance BLEURT's performance by combining its predictions with those of YISI  (Lo, 2019)  as well as by using alternative references. 

 Background and Notations Task Reference-based NLG evaluation seeks to assign a score to a triplet of sentences  (input, reference, candidate) , where input is a sentence in the source language, reference is a reference translation kept secret at inference time, and candidate is a translation produced by an MT system. Similar to BLEU  (Papineni et al., 2002)  and the previous editions of the WMT Metrics shared task, we omit the input and treat the task as a regression problem : we aim to learn a function f : (x, x) ? y that predicts a quality score y for a candidate sentence x = (x 1 , .., xp ) given a reference sentence x = (x 1 , .., x q ). The function is supervised on a corpus of N human ratings {(x i , xi , y i )} N n=1 . BLEURT Most experiments presented in this paper are based on BLEURT, a metric that leverages transfer learning to achieve high accuracy and increase robustness  (Sellam et al., 2020) . BLEURT is a BERT-based regression model  (Devlin et al., 2019) . It embeds sentence pairs into a fixed-width vector v BERT = BERT(x, x) with a pre-trained Transformer, and feeds this vector to a linear layer: ? = f (x, x) = W v BERT + b where W and b are the weight matrix and bias vector respectively. In its original (English) version, BLEURT is trained in three stages. (1) It is initialized from a publicly available BERT checkpoint. (2) The model is then "warmed up" by exposing it to millions of sentence pairs (x, x), obtained by randomly perturbing sentences from Wikipedia. During this phase, the model learns to predict a wide range of similarity scores that include existing metrics (BERTSCORE, BLEU, ROUGE), scores from an entailment model, and the likelihood that x was generated from x with a roundtrip translation by a given translation model. We denote this stage as mid-training. (3) In the final stage, the model is fine-tuned on human ratings from WMT Metrics  (Bojar et al., 2017; Ma et al., 2018 Ma et al., , 2019 , using a regression loss ? supervised = 1 N N n=1 y i ? 2 . We found that English BLEURT achieved competitive performance on four academic datasets, WebNLG  (Gardent et al., 2017) , and the WMT Metrics Shared Task years 2017 to 2019. 3 Extending BLEURT Beyond English 

 Modeling An approach to extend BLEURT would be to use MBERT, the public version of BERT pre-trained on 104 languages, and "mid-train" with non-English signals as described above. Yet, the evidence we gathered from early experiments were inconclusive. On the other hand, we did observe that models trained on several languages were often more accurate than monolingual models, possibly due to the larger amount of fine-tuning data. Thus, we opted for a simpler approach where we start with a multilingual BERT model and finetune it on all the human ratings data available for all languages (X ? Y and X ? En). In most cases, we found that such models could perform zero-shot evaluation: if a language Y does not have human ratings data, the metric can still perform evaluation in this target language as long as the base multilingual BERT model contains unlabeled data for Y, as observed in the past literature  (Karthikeyan et al., 2019; Pires et al., 2019) . We experiment with two pre-trained multilingual models: MBERT and MBERT-WMT, a custom multilingual variant of BERT. The MBERT-WMT model is larger that MBERT (24 Transformer layers instead of 12), and it was pre-trained on 19 languages of the WMT Metrics shared task 2015 to 2020. 

 Details of MBERT-WMT pre-training We trained MBERT-WMT model with an MLM loss  (Devlin et al., 2019) , using a combination of public datasets: Wikipedia, the WMT 2019 News Crawl  (Barrault et al.) , the C4 variant of Common Crawl  (Raffel et al., 2020) , OPUS (Tiedemann, 2012), Nunavut Hansard  (Joanis et al., 2020) , WikiTitles 2 , and ParaCrawl  (Espl?-Gomis et al., 2019) . We trained a new WordPiece vocabulary  (Schuster and Nakajima, 2012; Wu et al., 2016) , since the original vocabulary of mBERT does not support the alphabets of Pashto, Khmer and Inuktitut. The model was trained for 1 million steps with the LAMB optimizer  (You et al., 2020) , using the learning rate 0.0018 and batch size 4096 on 64 TPU v3 chips. 

 Experimental Setup Datasets At the time of writing, no human ratings data is available for WMT Metrics 2020. Therefore, we use the human ratings from WMT Metrics years 2015 to 2019 for both training and evaluation. We do so in two stages. In the first stage, we use 2015 to 2018 for training (216,541 sentence pairs in 8 languages), setting 10% aside for early stopping. We use 2019 as a development set, to choose hyper-parameters and to support high-level modeling decisions. In the second stage, we use 2015 to 2019, that is, all the data available, for training and uniformly sample 10% of the data for early stopping and hyper-parameter tuning. This adds 289,895 sentence pairs and 4 additional languages to our training set, approximately doubling the size of the training data. We report our results on the first setup, but submit our predictions to the shared task using the second setup. Hyper-parameters We run grid search on the learning rate and export the best model, using values {5e-6, 8e-6, 9e-6, 1e-5, 2e-5, 3e-5}. We use batch size 32 and evaluate the model every 1,000 steps on a 10% held-out data set to prevent over-fitting. During preliminary experiments, we additionally experimented with the batch size, dropout rate, frequency of continuous evaluation, balance of languages, pre-training schemes, Word-Piece vocabularies, and model architecture. 

 Additional Models and Baselines English BLEURT We fine-tune a new BLEURT checkpoint, following the methodology described above. The main difference with Sellam et al. (  2020 ) is that we incorporate the to-English ratings of year 2019, which were not previously available. Monolingual baselines based on BERT We experiment with three baselines and submit the results to the WMT Metrics Shared Task for analysis. BERT-L2-BASE and BERT-L2-LARGE are two regression models based on BERT and trained on to-English ratings. We use the same setup as English BLEURT, but we omit the mid-training phase. A similar approach was described in  Shimanaka et al. (2019) . BERT-CHINESE-L2 is similar to BERT-L2-BASE, but it uses BERT-CHINESE and it is fine-tuned on to-Chinese ratings. Other Systems We compare our setups to other state-of-the-art learned metrics: BERTSCORE  (Zhang et al., 2020) , and Yisi  (Lo, 2019)  all apply rules on top of BERT embeddings while ESIM  (Mathur et al., 2019)  is a neural sentence similarity model. PRISM  (Thompson and Post, 2020)  trains a multilingual translation model that is used as a zero-shot paraphrasing system. All the aforementioned systems take sentences pairs as input. Concurrent work has investigated incorporating the source with great success  (Rei et al., 2020) . We leave this line of research for future work. 

 Results Tables  1 and 2  show the results in the X ? En direction, at the segment-and system-level respectively. In the majority of cases, one of the BLEURT configurations yields the strongest results. The original BLEURT metric seems to perform better at the segment-level. At the system-level it may be dominated by PRISM (3 out of 7 language pairs) or by one of the simpler BERT-based models (4 out of 7 language pairs). Tables  3 and 4  present the results for the other languages. MBERT-WMT yields solid results at the segment-level (it achieves the highest correlations for 7 out of 11 language pairs), in particular for the "zero-shot" setups, En ? Gu, En ? Kk, and En ? Lt. It outperforms MBERT consistently, except for En ? Ru and En ? Zh where it lags behind the other metrics. The results are consistent at the system-level. 

 Strategy for the WMT Metrics Shared Task Based on these results, we make two "competitive" submissions. We present BLEURT as described above, which we ran on all the X ? En sentence pairs. Additionally, we submitted a multilingual system that combines MBERT-WMT (for all languages except Chinese) and BERT-CHINESE-L2 (for Chinese). We ran the multilingual system for all language pairs including to-English, as the large amount of non-English fine-tuning data made available in 2019 may benefit this setup too. We also release the predictions of BERT-BASE-L2, BERT-LARGE-L2, and MBERT for analysis. 

 Additional Improvements on English?German For English?German, the organizers of WMT20 provide three different reference translations: two standard references and one additional paraphrased reference. Given this novel setup, we investigate how to combine our predictions. Moreover, we use a similar framework to ensemble the predictions of different metrics. In particular, we average the predictions of BLEURT, YISI-1 and YISI-2. All three metrics are different in their approaches. While BLEURT and YISI-1 are reference-based metrics, YISI-2 is reference- Table  2 : System-level agreement with human ratings on the WMT19 Metrics Shared Task on the to-English language pairs. The metric is Pearson's correlation. The scores for YISI, YISI1-SRL, and ESIM come from  Ma et al. (2019) . The scores for BERTSCORE and PRISM come from  Thompson and Post (2020) . free and calculates its score by comparing translations only to the source sentence. BLEURT is fine-tuned on previous human ratings, while YISI-1 is based on the cosine similarity between BERT embeddings of the reference and the candidate. In the remainder of this section, we report BLEURT results using the MBERT-WMT setup unless specified otherwise. 3 

 Modifications to YiSi-1 Before combining BLEURT and YISI, we perform a series of modifications to YISI-1 and evaluate their impact on English?German. Experimental Setup All experimental results are summarized in Table  5 . We report both segment-level (DARR) and system-level (Kendall ? ) correlations. To replicate the multireference setup of 2020, we compute correlations  3  We use a different checkpoint from the one described in Section 4. The model was trained for 880K steps instead of 1 million, and it uses a sequence length of 256 tokens instead of 128. with the standard WMT references as well as the paraphrased reference from  Freitag et al. (2020) . Improving YiSi's Predictions Our baseline is similar to the YISI-1 submission from WMT 2019  (Lo, 2019) : we run YISI-1 with the public multilingual MBERT checkpoint. We then experiment with the underlying checkpoint. We continued pre-training MBERT on the in-domain German NewsCrawl dataset. The resulting model +pre-train NewsCrawl layer 9 increases the correlation for both reference translations. We improve the correlation further on the paraphrased reference by using the 8th instead of the 9th layer. Other experiments We tried pre-training BERT on forward translated sentences from German NewsCrawl, to adapt the word embeddings to MT outputs. We also trained a BERT model from scratch on the German NewsCrawl data. These experiments did not result in higher correlations with human ratings.     

 Combining BLEURT, YISI-1 and YISI-2 on Multiple References We describe our two submissions to WMT 2020, YISI-COMB and ALL-COMB, which result from our efforts to use multiple references for automatic evaluation. YISI-COMB is a multi-reference version of the YISI score  (Lo, 2019)  aimed at achieving better system-level correlations. ALL-COMB leverages metrics from BLEURT, YISI-1, and YISI-2 on multiple references to achieve better segment-level correlation. YISI-COMB YISI scores are F 1 scores of YISI precision and YISI recall. For the YISI-COMB submission, we take the minimum of the YISI recalls for the three different references as the multireference recall, and the maximum of the YISI precision as the multi-reference precision. Using the same notations as in  (Lo, 2019) , the final score is the F 1 of the recall and precision computed with ? = 0.7 (see Figure  1 ). This submission aims to maximize the system-level correlation. As shown in Table  5 , YISI-1 has the highest system-level correlation on paraphrased references. Given that we used ? = 0.7, YISI scores are quite similar to YISI recalls (when ? = 1.0, YISI scores are equal to YISI recalls). YISI-1 scores for paraphrased references are usually much lower than those of standard references, therefore taking the minimum recall is oftentimes equivalent to taking the YISI recall from the paraphrased references. Furthermore, we found that using the maximum precision, in combination with aggregating recalls, usually performs the best. Figure  1 : Correlations with respect to different ? settings for Yisi-1. The system-level correlation is highest when ? = 0.7, which is the ? we use for the submission. Table 1 : 1 Segment-level agreement with human ratings on the WMT19 Metrics Shared Task on the to-English language pairs. The metric is WMT's Direct Assessment metric, a robust variant of Kendall ? . The scores for YISI, YISI1-SRL, and ESIM come from Ma et al. (2019) . The scores for BERTSCORE and PRISM come from Thompson and Post (2020) . de-en fi-en gu-en kk-en lt-en ru-en zh-en avg YISI 0.164 0.347 0.312 0.440 0.376 0.217 0.426 0.326 YISI1-SRL 0.199 0.346 0.306 0.442 0.380 0.222 0.431 0.332 ESIM 0.167 0.337 0.303 0.435 0.359 0.201 0.396 0.314 BERTSCORE 0.176 0.345 0.320 0.432 0.381 0.223 0.430 0.330 PRISM 0.204 0.357 0.313 0.434 0.382 0.225 0.438 0.336 BLEURT Configurations, English-only BERT-L2-BASE 0.142 0.326 0.274 0.406 0.367 0.197 0.358 0.296 BERT-L2-LARGE 0.172 0.361 0.305 0.424 0.388 0.210 0.420 0.326 BLEURT 0.175 0.365 0.316 0.451 0.397 0.223 0.444 0.339 BLEURT Configurations, Multi-lingual MBERT 0.172 0.352 0.300 0.430 0.388 0.222 0.397 0.323 MBERT-WMT 0.187 0.363 0.306 0.439 0.398 0.226 0.425 0.335 de-en fi-en gu-en kk-en lt-en ru-en zh-en avg YISI 0.949 0.989 0.924 0.994 0.981 0.979 0.979 0.971 YISI1-SRL 0.950 0.989 0.918 0.994 0.983 0.978 0.977 0.969 ESIM 0.941 0.971 0.885 0.986 0.989 0.968 0.988 0.961 BERTSCORE 0.949 0.987 0.981 0.980 0.962 0.921 0.983 0.966 PRISM 0.954 0.983 0.764 0.998 0.995 0.914 0.992 0.943 BLEURT Configurations, English-only BERT-L2-BASE 0.938 0.992 0.930 0.992 0.991 0.976 0.997 0.974 BERT-L2-LARGE 0.940 0.987 0.819 0.992 0.990 0.985 0.993 0.958 BLEURT 0.943 0.989 0.865 0.996 0.995 0.984 0.990 0.966 BLEURT Configurations, Multi-lingual MBERT 0.937 0.976 0.863 0.984 0.978 0.959 0.978 0.954 MBERT-WMT 0.950 0.991 0.815 0.989 0.992 0.968 0.980 0.955 

 Table 3 : 3 Segment-level agreement with human ratings on the WMT19 Metrics Shared Task on non-English language pairs. The metric is WMT's Direct Assessment metric, a robust variant of Kendall ? . Languages without fine-tuning data are denoted in italics. The scores for YISI,YISI1-SRL, and ESIM come from et al. (2019). The scores for BERTSCORE and PRISM come from Thompson and Post (2020) . en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh de-cs de-fr fr-de avg YISI1 0.962 0.991 0.971 0.909 0.985 0.963 0.992 0.951 0.973 0.969 0.908 0.961 YISI1-SRL - 0.991 - - - - - 0.948 - - 0.912 - ESIM - 0.991 0.957 - 0.980 0.989 0.989 0.931 0.980 0.950 0.942 - BERTSCORE 0.981 0.990 0.970 0.922 0.981 0.978 0.989 0.925 0.969 0.971 0.899 0.961 PRISM 0.958 0.988 0.949 0.624 0.978 0.937 0.918 0.898 0.976 0.936 0.911 0.916 BLEURT Configurations BERT-CHINESE-L2 - - - - - - - 0.953 - - - - MBERT 0.942 0.987 0.953 0.949 0.982 0.950 0.947 0.949 0.972 0.970 0.924 0.957 MBERT-WMT 0.993 0.991 0.987 0.959 0.993 0.989 0.888 0.953 0.986 0.988 0.962 0.972 

 Table 4 : 4 System-level agreement with human ratings on the WMT19 Metrics Shared Task on non-English language pairs. The metric is Pearson's correlation. Languages without finetuning data are denoted in italics. The scores for YISI, YISI1-SRL, and ESIM come from Ma et al. (2019) . The scores for BERTSCORE and PRISM come from Thompson and Post (2020) . sys-level seg-level Ref Metric model Kendall ? DARR std BLEURT MBERT-WMT  ? 0.896 0.420 MBERT (WMT19 subm.) 0.810 0.351 std YiSi-1 +pre-train NewsCrawl layer 9 0.870 0.373 +pre-train NewsCrawl layer 8  ? 0.853 0.376 para BLEURT MBERT-WMT  ? 0.852 0.413 MBERT (WMT19 subm.) 0.844 0.316 para YiSi-1 +pre-train NewsCrawl layer 9 0.887 0.365 +pre-train NewsCrawl layer 8  ? 0.896 0.373 src YiSi-2 MBERT ? 0.307 0.106 2std+para YiSi-comb comb of 3 ( ? systems) all-comb avg of 7 ( ? &  ? systems) 0.905 0.878 0.399 0.454 

 Table 5 : 5 Agreement with human ratings on the WMT19 Metrics Shared Task for English?German. The first set of results are generated by using the standard reference translations for WMT 2019. The second set of results is generated by using the paraphrased reference translations. YiSi-2 is reference free and only uses the source sentences. 

			 https://linguatools.org/tools/ corpora/wikipedia-parallel-titles-corpora/
