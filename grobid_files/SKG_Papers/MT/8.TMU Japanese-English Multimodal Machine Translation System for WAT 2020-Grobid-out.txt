title
TMU Japanese-English Multimodal Machine Translation System for WAT 2020

abstract
We introduce our TMU system submitted to the Japanese?English Multimodal Task (constrained) for WAT 2020 (Nakazawa et al.,  2020). This task aims to improve translation performance with the help of another modality (images) associated with the input sentences. In a multimodal translation task, the dataset is, by its nature, a low-resource one. Our method used herein augments the data by generating noisy translations and adding noise to existing training images. Subsequently, we pretrain a translation model on the augmented noisy data, and then fine-tune it on the clean data. We also examine the probabilistic dropping of either the textual or visual context vector in the decoder. This aims to regularize the network to make use of both features while training. The experimental results indicate that translation performance can be improved using our method of textual data augmentation with noising on the target side and probabilistic dropping of either context vector.

Introduction In recent years, neural machine translation (NMT) has become the standard machine translation system owing to its high performance  Bahdanau et al., 2015; Luong et al., 2015) . However, NMT requires considerable parallel corpora for training; thus, it does not perform well in situations where low-resource data are present. To address this issue,  Sennrich et al. (2016a)  proposed back-translation that generates pseudo-parallel data by translating monolingual data in the target language. Multimodal machine translation (MMT) is a task whose purpose is to generate better translations with information from other modalities (such as images) related to the source sentences  (Specia et al., 2016) . Owing to the nature of MMT, which requires image information paired with sentences, the size of the available data is relatively small compared to that of text-only data. To overcome this issue, in this study, we augment training texts and images using several methods without external data. In our experiments, we pretrain the MMT decinit model (see Subsection 3.2) on the augmented training data and fine-tune it on the original training data to improve translation performance. Furthermore, to effectively utilize the features of both images and texts, we introduce the dropnet method  (Zhu et al., 2020)  into MMT models. It is expected to regularize the network training by probabilistically dropping one of the context vectors (textual or visual context vector) in the decoder. To the best of our knowledge, this is the first attempt incorporating the dropnet method into MMT with a recurrent neural network (RNN). Our main findings herein are as follows: ? Textual data augmentation are better than visual data augmentation for MMT. ? Placing noise on the target side of the augmented data is effective in improving translation performance in the English?Japanese direction. ? The use of the dropnet method leads to improvements in translation performance. 

 Related Work Several approaches to MMT have been proposed in the recent studies.  Caglayan et al. (2016)  and  proposed the doubly-attentive model wherein the encoder is a bi-directional gated recurrent unit (BiGRU)  (Cho et al., 2014)  that processes only the source sequence, and the decoder is a conditional GRU (CGRU) 1 that simultaneously pays attention to the source sequence and the spatial visual feature.  also used global visual features to initialize either the encoder or the decoder of the attention-based NMT. In the WMT 17 Shared Task on MMT, the models using global features were shown to be better than those using spatial features  (Caglayan et al., 2017a) . Additionally,  Gr?nroos et al. (2018)  adapted the Transformer  (Vaswani et al., 2017)  model to a multimodal setting and proposed concatenating the regional visual features encoded as a pseudo-word embedding to the word embeddings of the source sentence. According to them, however, the improvement achieved by incorporating visual information is modest, and they observed that external parallel data can significantly improve the performance. In contrast, we augmented training data without external data. With respect to research on data augmentation in NMT, in addition to the method mentioned in Section 1,  Fadaee et al. (2017)  generated synthetic sentence pairs containing low-frequency words by leveraging the language models trained on large monolingual corpora. Under simulated lowresource settings, their results showed that translations using this augmentation approach have more low-frequency words than those not using this approach, leading to improved performance.  Edunov et al. (2018)  investigated back-translation at a large scale for generating useful synthetic source sentences using several approaches. They obtained back-translated data via sampling and noisy beam outputs and added them to parallel corpora. They found that the above methods outperform the ones that generate synthetic sentences based on argmax inference (e.g., beam or greedy search), except in low-resource settings. 

 Model 

 NMT Model Our baseline NMT  (Caglayan et al., 2017a ) is an attentive encoder-decoder model, wherein the encoder is BiGRU, and the decoder is CGRU. Thus, our MMT models are based on RNNs. To generate synthetic data via textual data augmentation methods (see Subsection 4.1), we used the Transformer  (Vaswani et al., 2017)  model. 

 MMT Model with Decoder Initialization This MMT model initializes the hidden state of the decoder of our baseline NMT with global visual features  (Caglayan et al., 2017a) . This model's architecture is used for our baseline MMT as well as MMT models using augmented data. We denote this model as MMT decinit . 

 MMT Model with Double Attention In our MMT model with a double attention mechanism, the decoder part of our baseline NMT model is extended to be multimodal  (Caglayan et al., 2017a) . While decoding, this model individually pays attention to the source sentence and the image to obtain the textual and visual context vectors. Subsequently, it combines both context vectors to obtain the multimodal context vector. In our experiments, we also adopt a hierarchical attention mechanism to combine each context vector  (Libovick? and Helcl, 2017) . At each decoding step i, this attention combination projects each context vector into a common space (Equation  1 ) and computes another distribution with the projected context vectors (Equation  2 ). Then, we obtain the multimodal context vector by calculating the weighted average corresponding to each context vector (Equation  3 ). e (k) i = v T b tanh(W b s i + U (k) b c (k) i ), (1) ? (k) i = exp(e (k) i ) 2 n=1 exp(e (n) i ) , (2) c i,multimodal = 2 k=1 ? (k) i U (k) c c (k) i (3) where c (k) i is the k-th context vector (visual and textual), s i is the decoder hidden state, v b and W b are trainable parameters, and U We denote this model as MMT datt . In contrast to the MMT decinit models, we do not conduct experiments for MMT datt models using augmented data 2 . We incorporate the dropnet method (see Subsection 3.3) into only this model for regularization owing to its architecture that combines each context vector.  Zhu et al. (2020)  studied incorporating BERT  (Devlin et al., 2019)  into the 2 We conducted preliminary experiments for both MMT decinit and MMT datt models using augmented data, although our dataset used in those experiments had the different splits of the dataset given by WAT 2020. As a result, the baseline MMT datt model is inferior to the MMT decinit model; moreover, the baseline MMT datt model outperformed the ones pretrained on the augmented data. We therefore do not train MMT datt models using augmented data. 

 Dropnet method c i,multimodal = I(r i < p net 2 ) ? ? (1) i U (1) c c (1) i + I(r i > 1 ? p net 2 ) ? ? (2) i U (2) c c (2) i + I( p net 2 ? r i ? 1 ? p net 2 ) ? 2 k=1 ? (k) i U (k) c c (k) i (4) Transformer  (Vaswani et al., 2017)  model in the translation task. They proposed the dropnet method that intends to regularize the network training to fully utilize the features output from BERT and the conventional encoder. Specifically, at any layer l in the encoder during training, with probability p net /2 , the output from either BERT or the (l ? 1)th layer of the encoder is selected for computing the l-th layer's output, and with probability (1 ? p net ), both outputs are used for computing the l-th layer's output, where the dropnet rate p net ? [0, 1]. Similar to the above method, we attempt regularization by probabilistically dropping either the textual or the visual context vector. Our MMT datt model combines each context vector to obtain the multimodal context vector in the decoder. Therefore, we adapt the dropnet method to the decoder. At each decoding step i while training, with probability p net /2, either the textual context vector or the visual context vector is used for computing the multimodal context vector; with probability (1 ? p net ), both context vectors are used for the multimodal context vector (Equation  4 ). In Equation  4 , I(?) is the indicator function, r i is a random variable uni- formly sampled from [0, 1], c (k) i is the k-th context vector. 

 Data Augmentation Method 

 Textual Data Augmentation Sampling This method samples the hypothesis from the output distribution at each decoding step to generate synthetic parallel data  (Edunov et al., 2018) . Random noising This method was originally used to generate synthetic ungrammatical sentences in the grammatical error correction task  (Xie et al., 2018) . They penalized every hypothesis on the beam by adding noise r? to its hypothesis' score, where r is drawn from the uniform distribution on the interval [0, 1] during the beam search procedure, and ? controls the noise intensity. If ? is sufficiently large, this method is similar to the method that randomly shuffles the ranks of the hypotheses according to their scores. 

 Visual Data Augmentation We incorporate several image data augmentation methods based on computer vision tasks  (Shorten and Khoshgoftaar, 2019) . According to  Luke and Geoff (2018) , they augmented images in several ways in the image recognition task and demonstrated that the cropping method achieved the highest accuracy, followed by rotation. We likewise choose cropping (center cropping and random cropping) and rotation methods from the above results. Center cropping This method crops a center patch (256?256 size) of each image. Random cropping This method randomly selects a patch (256?256 size) of each image and crops it. Rotation This method rotates the images right or left on an axis between ?20 ? and 20 ? randomly. This range is useful for digit recognition tasks such as MNIST  (Shorten and Khoshgoftaar, 2019) . 

 Experimental Setup 

 Data For training and validation, we use the Flickr30k Entities Japanese dataset 3 for Japanese sentences, the Flickr30k Entities dataset 4 for English sentences, and the Flickr30k dataset 5 for images. For test data, we use both Japanese and English sentences provided by WAT 2020, and their associated images are in the Flickr30k dataset. The Japanese training data size is originally 59,566 sentences, but four sentences are missing; thus, we use 59,562 sentences (both Japanese and English) for training. We use Moses  (Koehn et al., 2007)  scripts to lowercase, normalize, and tokenize English sentences, and tokenized Japanese sentences using MeCab 6 with the IPA dictionary. The evaluation metric used is BLEU calculated by multibleu.perl 7 of Moses. We use the word-level vocabularies of 9,546 items for English and 11,235 items for Japanese. We also used byte pair encoding (BPE:  Sennrich et al., 2016b)  for vocabularies, but the word-level method demonstrated better results than BPE; therefore, we decide to use word-level vocabularies. To augment training texts, we train a textonly Transformer model on the original training texts. Subsequently, we translate the original English/Japanese training texts into Japanese/English texts as additional training texts using the trained model. We train the models with three different seeds and translate with the trained model having the highest score on the dev set among the three trained models. We generate noisy training sentences using the sampling method and the random noising method with each value ? = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20 (if ? = 0, the models simply translate English or Japaneses sentences without noise). For images, we augment images using Albumentations 8 , which is a library for image data augmentation. We extract the visual features from a pretrained CNN model, ResNet-50  (He et al., 2016) . The size of the spatial features extracted from the res4f relu layer is 14?14?1024, and the global features extracted from the pool5 layer are 2,048-dimensional features. 

 Model We conduct our experiments with the toolkit nmtpytorch version 4.0.0 9  (Caglayan et al., 2017b) , except during the textual data augmentation step. The encoder and decoder GRUs have 320 hidden dimensions, and word embeddings are 200 dimensions. We use the Adam (Kingma and Ba, 2015) optimizer with a learning rate of 4e-4, and a batch size of 64 for training and 32 for evaluation. We adopt the early stopping for training if the BLEU score of the dev set does not improve for ten epochs. The beam size is 12, and the total gradient norm is clipped to 1. We set the dropnet probability p net to 0.3. For the English?Japanese (En?Ja) direction, dropout  (Srivastava et al., 2014)  rates applied to source embeddings, source annotations, and pre-softmax activations are (0.4, 0.4, 0.6) for our NMT model, (0.3, 0.4, 0.5) for our MMT decinit model, and (0.4, 0.4, 0.4) for our MMT datt model, respectively. For the Japanese?English (Ja?En) direction, we set the dropout rates (0.4, 0.3, 0.4) for our NMT model, (0.5, 0.3, 0.4) for our MMT decinit model, and (0.4, 0.3, 0.3) for our MMT datt model. We use same dropout rates set on each model when both pretraining and fine-tuning. 

 Data augmentation We train a text-only Transformer model on the original training texts using the fairseq 10 toolkit for generating noisy texts. Both the encoder and the decoder have six blocks, and the input and output embeddings of the decoder are shared. The word embedding size and the hidden size is 512 dimensions. We optimize the models with Adam (? 1 = 0.9, ? 2 = 0.98). The learning rate is 4e-4 and the maximum number of tokens for each mini-batch is 4,096. The beam size is 5, and the total gradient norm is clipped to 5.0. We set the dropout rate of 0.1 for both directions. We repeated each experiment with three different seeds to calculate the BLEU score by averaging three scores on the dev set in order to select the best model for textual augmentation and fine-tuning as well as to ensemble the models. 

 Training 

 w/o augmented data Our baseline NMT, baseline MMT decinit and MMT datt models are trained on the original training data. 

 w/ augmented data For the MMT decinit models using augmented data, we first pretrain our MMT decinit model on augmented data obtained via the above-mentioned method (see Section 4). We pretrain three models with each different seed and select the best pretrained model. Thereafter, we fine-tune the selected pretrained model on clean data. the target side and clean texts on the source side during pretraining. We thus aim to utilize the visual feature more by smoothing attention to texts during decoding. Furthermore, we combine the noisy texts generated by different textual data augmentation methods for pretraining. For example, if we combine each generated data with a random noising method with ? = 1 and ? = 2, the mixed textual data comprise 119,124 sentences. 

 Augmented texts Augmented images In the case of using augmented images for pretraining, we use clean texts with associated augmented images which are center cropped, random cropped or rotated. When fine-tuning, both clean texts and clean images are used. 

 Results and Analysis En?Ja translation Table  1  shows the BLEU scores on the dev set for the English?Japanese direction. We found that using noisy data with the random noising method is effective for the En-Ja direction. For the "random noising" models on the target side, the model of random noising method with ? = 5 has achieved the highest score among the ones with other data augmentation methods, followed by ? = 1 and ? = 6. Therefore, we chose ? = 1, ? = 5, and ? = 6 for the "mix" model. "Ensemble (top 6 models)" ensembled of the six models ("Baseline MMT decinit ", "MMT decinit w/ random noising (? = 4; target)", "MMT decinit w/ random noising (? = 1; target)", "MMT decinit w/ sampling (target)", "MMT decinit w/ random noising (? = 4; target)" and "MMT decinit w/ random noising (? = 8; target)") MMT decinit , except for "sampling (target)". "random cropping" has the highest score among the three models using augmented visual data, but its score exceeds that of the baseline MMT decinit by only 0.09 points. Therefore, we did not conduct experiments using augmented visual data for the Ja?En direction. We show our published results for the En?Ja direction. In Table  3 , the single model has the highest score among its three models with each different seed. The model "Ensemble (top 6 models)" is an ensemble of the top six models in terms of the BLEU metric among all trained single models. It outperforms the other models in terms of the BLEU metric, but its RIBES scores are lower than those of "Ensemble (3 MMT datt models w/ dropnet)". Moreover, comparing "Baseline MMT decinit " and "MMT datt w/ dropnet", the RIBES score of the latter is higher than the former but not the BLEU metric. Additionally, we show the translation examples of several models in the appendix, focusing on the quality of "Ensemble (top 6 models)". A good translation of "Ensemble (top 6 models)" is in Table  7 , and a poor translation of it is in Table  8 .  2  shows the BLEU scores on the dev set for the Ja?En direction. We found that the noisy data did not have a positive effect on translation performance. We chose ? = 2 because the model pretrained on noisy data on the source side with the random noising method with ? = 2 is the best. Likewise, we chose ? = 4 of the random noising method, which achieves the best score among the models pretrained with the noisy data on the target side, followed by sampling. Hence, we used the noisy data (both on the target side) with random noising (? = 4) and sampling for pretraining the model "mix". Contrary to the En?Ja results, our baseline NMT model is 0.05 points higher than that of the baseline MMT decinit . "random noising (? = 4; target)" gains 0.03 points over the baseline MMT decinit ; however, it still does not reach the baseline NMT score. 

 Ja?En translation Table We present our published results for the Ja?En direction in does not achieve the best score in terms of both the BLEU and RIBES metrics. "Ensemble (3 baseline MMT decinit models)" surpasses the other models in terms of the BLEU metric, and "Ensemble (3 MMT datt models w/ dropnet)" is the best in terms of the RIBES metric. We show the translation examples of several models in the appendix, focusing on the quality of "Ensemble (top 6 models)". There are a good translation of "Ensemble (top 6 models)" in Table  9  and a poor translation of it in Table  10 . Dropnet Table  5  shows the results of MMT models with and without the dropnet method. Although the models trained both with and without the dropnet method have lower BLEU scores than the baseline MMT decinit , the model with dropnet gains 0.5 BLEU points for the En?Ja direction and 0.56 BLEU points for the Ja?En direction than the one without dropnet. For the RIBES metric, the model with dropnet achieves the best RIBES score in the En?Ja direction, but the baseline MMT decinit is the best model for the Ja?En direction. Moreover, the differences between the RIBES scores of each model are marginal; however, the models with dropnet are better than those without dropnet. These demonstrate that incorporating the dropnet method into MMT datt models can help improve the translation performance. 

 Source vs target noising We investigated how the score is affected by whether the noise data for pretraining is on the source or the target side. As indicated in Table  6 , we pretrain the models on the noisy data generated with thirteen different ? values (i.e., ? = 0, 1, 2,  3, 4, 5, 6, 7, 8, 9, 10, 15, 20)  and calculate the average of these thirteen models scores. This table shows that using the noisy data on the target side is better than on the source side for both directions while pretraining. Although it is unclear why the noisy data on the target side is effective, we infer this setting is useful under low-resource or MMT situations. 

 Conclusion We introduced several data augmentation methods to solve the low-resource problem in MMT. These methods have been shown to be useful for the En?Ja direction, but not for the Ja?En direction. The random noising method positively affects translation performance compared to other data augmentation methods. Furthermore, it is notable that adding noise on the target side is more effective than on the source side for textual data augmentation. We also adapted the dropnet method for the regularization of double attention mechanism for MMT. This method is effective compared to not using dropnet. For future work, we investigate why the noisy data on the target side is effective. We also explore other textual and visual data augmentation methods and mixing visual augmented data for pretraining; moreover, we research whether combining textual and visual augmented data improves the performance or not. Furthermore, we study how the MMT models with the dropnet method work. 
