title
Multi-level Structured Models for Document-level Sentiment Classification

abstract
In this paper, we investigate structured models for document-level sentiment classification. When predicting the sentiment of a subjective document (e.g., as positive or negative), it is well known that not all sentences are equally discriminative or informative. But identifying the useful sentences automatically is itself a difficult learning problem. This paper proposes a joint two-level approach for document-level sentiment classification that simultaneously extracts useful (i.e., subjective) sentences and predicts document-level sentiment based on the extracted sentences. Unlike previous joint learning methods for the task, our approach (1) does not rely on gold standard sentence-level subjectivity annotations (which may be expensive to obtain), and (2) optimizes directly for document-level performance. Empirical evaluations on movie reviews and U.S. Congressional floor debates show improved performance over previous approaches.

Introduction Sentiment classification is a well-studied and active research area  (Pang and Lee, 2008) . One of the main challenges for document-level sentiment categorization is that not every part of the document is equally informative for inferring the sentiment of the whole document. Objective statements interleaved with the subjective statements can be confusing for learning methods, and subjective statements with conflicting sentiment further complicate the document categorization task. For example, authors of movie reviews often devote large sections to (largely objective) descriptions of the plot  (Pang and Lee, 2004) . In addition, an overall positive review might still include some negative opinions about an actor or the plot. Early research on document-level sentiment classification employed conventional machine learning techniques for text categorization  (Pang et al., 2002) . These methods, however, assume that documents are represented via a flat feature vector (e.g., a bag-ofwords). As a result, their ability to identify and exploit subjectivity (or other useful) information at the sentence-level is limited. And although researchers subsequently proposed methods for incorporating sentence-level subjectivity information, existing techniques have some undesirable properties. First, they typically require gold standard sentence-level annotations  (McDonald et al. (2007) ,  Mao and Lebanon (2006) ). But the cost of acquiring such labels can be prohibitive. Second, some solutions for incorporating sentencelevel information lack mechanisms for controlling how errors propagate from the subjective sentence identification subtask to the main document classification task  (Pang and Lee, 2004) . Finally, solutions that attempt to handle the error propagation problem have done so by explicitly optimizing for the best combination of document-and sentence-level classification accuracy  (McDonald et al., 2007) . Optimizing for this compromise, when the real goal is to maximize only the document-level accuracy, can potentially hurt document-level performance. In this paper, we propose a joint two-level model to address the aforementioned concerns. We formulate our training objective to directly optimize for document-level accuracy. Further, we do not require gold standard sentence-level labels for training. Instead, our training method treats sentence-level labels as hidden variables and jointly learns to predict the document label and those (subjective) sentences that best "explain" it, thus controlling the propagation of incorrect sentence labels. And by directly optimizing for document-level accuracy, our model learns to solve the sentence extraction subtask only to the extent required for accurately classifying document sentiment. A software implementation of our method is also publicly available.  1  For the rest of the paper, we will discuss related work, motivate and describe our model, present an empirical evaluation on movie reviews and U.S. Congressional floor debates datasets and close with discussion and conclusions. 2 Related Work  Pang and Lee (2004)  first showed that sentencelevel extraction can improve document-level performance. They used a cascaded approach by first filtering out objective sentences and performing subjectivity extractions using a global min-cut inference. Afterward, the subjective extracts were converted into inputs for the document-level sentiment classifier. One advantage of their approach is that it avoids the need for explicit subjectivity annotations. However, like other cascaded approaches (e.g.,  Thomas et al. (2006) ,  Mao and Lebanon (2006) ), it can be difficult to control how errors propagate from the sentence-level subtask to the main document classification task. Instead of taking a cascaded approach, one can directly modify the training of flat document classifiers using lower level information. For instance,  Zaidan et al. (2007)  used human annotators to mark the "annotator rationales", which are text spans that support the document's sentiment label. These annotator rationales are then used to formulate additional constraints during SVM training to ensure that the resulting document classifier is less confident in classifying a document that does not contain the rationale versus the original document.  Yessenalina et al. (2010)  extended this approach to use automatically generated rationales. A natural approach to avoid the pitfalls associated with cascaded methods is to use joint twolevel models that simultaneously solve the sentencelevel and document-level tasks (e.g.,  McDonald et al. (2007) ,  Zaidan and Eisner (2008) ). Since these models are trained jointly, the sentence-level predictions affect the document-level predictions and vice-versa. However, such approaches typically require sentence-level annotations during training, which can be expensive to acquire. Furthermore, the training objectives are usually formulated as a compromise between sentence-level and documentlevel performance. If the goal is to predict well at the document-level, then these approaches are solving a much harder problem that is not exactly aligned with maximizing document-level accuracy. Recently, researchers within both Natural Language Processing (e.g.,  Petrov and Klein (2007) , ,  Clarke et al. (2010) ) and other fields (e.g.,  Felzenszwalb et al. (2008) , ) have analyzed joint multilevel models (i.e., models that simultaneously solve the main prediction task along with important subtasks) that are trained using limited or no explicit lower level annotations. Similar to our approach, the lower level labels are treated as hidden or latent variables during training. Although the training process is non-trivial (and in particular requires a good initialization of the hidden variables), it avoids the need for human annotations for the lower level subtasks. Some researchers have also recently applied hidden variable models to sentiment analysis, but they were focused on classifying either phrase-level  (Choi and Cardie, 2008)  or sentence-level polarity  (Nakagawa et al., 2010) . 

 Extracting Hidden Explanations In this paper, we take the view that each document has a subset of sentences that best explains its sentiment. Consider the "annotator rationales" generated by human judges for the movie reviews dataset  (Zaidan et al., 2007) . Each rationale is a text span that was identified to support (or explain) its parent document's sentiment. Thus, these rationales can be interpreted as (something close to) a ground truth labeling of the explanatory segments. Using a dataset where each document contains only its rationales, Algorithm 1 Inference Algorithm for (2) 1: Input: x 2: Output: (y, s) 3: s + ? argmax s?S(x) w T ?(x, +1, s) 4: s ? ? argmax s?S(x) w T ?(x, ?1, s) 5: if w T ?(x, +1, s + ) > w T ?(x, ?1, s ? ) then 6: Return (+1, s + ) 7: else 8: Return (?1, s ? ) 9: end if cross validation experiments using an SVM classifier yields 97.44% accuracy -as opposed to 86.33% accuracy when using the full text of the original documents. Clearly, extracting the best supporting segments can offer a tremendous performance boost. We are interested in settings where humanextracted explanations such as annotator rationales might not be readily available, or are imperfect. As such, we will formulate the set of extracted sentences as latent or hidden variables in our model. Viewing the extracted sentences as latent variables will pose no new challenges during prediction, since the model is expected to predict all labels at test time. We will leverage recent advances in training latent variable SVMs  to arrive at an effective training procedure. 

 Model In this section, we present a two-level document classification model. Although our model makes predictions at both the document and sentence levels, it will be trained (and evaluated) only with respect to document-level performance. We begin by presenting the feature structure and inference method. We will then describe a supervised training algorithm based on structural SVMs, and finally discuss some extensions and design decisions. Let x denote a document, y = ?1 denote the sentiment (for us, a binary positive or negative polarity) of a document, and s denote a subset of explanatory sentences in x. Let ?(x, y, s) denote a joint feature map that outputs features describing the quality of predicting sentiment y using explanation s for document x. We focus on linear models, so given a (learned) weight vector w, we can write the quality of predicting y (with explanation s) as F (x, y, s; w) = w T ?(x, y, s), (1) and a document-level sentiment classifier as h(x; w) = argmax y=?1 max s?S(x) F (x, y, s; w), (2) where S(x) denotes the collection of feasible explanations (e.g., subsets of sentences) for x. Let x j denote the j-th sentence of x. We propose the following instantiation of (1), w T ?(x, y, s) = 1 N (x) j?s y ? w T pol ? pol (x j ) + w T subj ? subj (x j ), (3) where the first term in the summation captures the quality of predicting polarity y on sentences in s, the second term captures the quality of predicting s as the subjective sentences, and N (x) is a normalizing factor (which will be discussed in more detail in Section 4.3). We represent the weight vector as w = w pol w subj , (4) and ? pol (x j ) and ? subj (x j ) denote the polarity and subjectivity features of sentence x j , respectively. Note that ? pol and ? subj are disjoint by construction, i.e., ? T pol ? subj = 0. We will present extensions in Section 4.5. For example, suppose ? pol and ? subj were both bag-of-words feature vectors. Then we might learn a high weight for the feature corresponding to the word "think" in ? subj since that word is indicative of the sentence being subjective (but not necessarily indicating positive or negative polarity). 

 Making Predictions Algorithm 1 describes our inference procedure. Recall from (2) that our hypothesis function predicts the sentiment label that maximizes (3). To do this, we compare the best set of sentences that explains a positive polarity prediction with the best set that explains a negative polarity prediction. We now specify the structure of S(x). In this paper, we use a cardinality constraint, S(x) = {s ? {1, . . . , |x|} : |s| ? f (|x|)}, (5) Algorithm 2 Training Algorithm for OP 1 1: Input: {(x 1 , y 1 ), . . . , (x N , y N )} //training data 2: Input: C //regularization parameter 3: Input: (s 1 , . . . , s N ) //initial guess 4: w ? SSVMSolve(C, {(x i , y i , s i )} N i=1 ) 5: while w not converged do 6: for i = 1, . . . , N do 7: s i ? argmax s?S(xi) w T ?(x i , y i , s) 8: end for 9: w ? SSVMSolve(C, {(x i , y i , s i )} N i=1 ) 10: end while 11: Return w where f (|x|) is a function that depends only on the number of sentences in x. For example, a simple function is f (|x|) = |x| ? 0.3, indicating that at most 30% of the sentences in x can be subjective. Using this definition of S(x), we can then compute the best set of subjective sentences for each possible y by computing the joint subjectivity and polarity score of each sentence x j in isolation, y ? w T pol ? pol (x j ) + w T subj ? subj (x j ), and selecting the top f (|x|) as s (or fewer, if there are fewer than f (|x|) that have positive joint score). 

 Training For training, we will use an approach based on latent variable structural SVMs . Optimization Problem 1. min w,?0 1 2 w 2 + C N N i=1 ? i (6) s.t. ?i : max s?S i w T ?(x i , y i , s) ? max s ?S(x i ) w T ?(x i , ?y i , s ) + 1 ? ? i (7) OP 1 optimizes the standard SVM training objective for binary classification. Each training example has a corresponding constraint (7), which is quantified over the best possible explanation of the training polarity label. Note that we never observe the true explanation for the training labels; they are the hidden or latent variables. The hidden variables are also ignored in the objective function. As a result, one can interpret OP 1 to be directly optimizing a trade-off between model complexity (as measured using the 2-norm) and document-level classification error in the training set. This has two main advantages over related training approaches. First, it solves the multi-level problem jointly as opposed to separately, which avoids introducing difficult to control propagation errors. Second, it does not require solving the sentence-level task perfectly, and also does not require precise sentence-level training labels. In other words, our goal is to learn to identify the informative (subjective) sentences that best explain the training labels to the extent required for good document classification performance. OP 1 is non-convex because of the constraints (7). To solve OP 1, we use the combination of the CCCP algorithm  (Yuille and Rangarajan, 2003)  with cutting plane training of structural SVMs , as proposed in . Suppose each constraint (  7 ) is replaced by w T ?(x i , y i , s i ) ? max s ?S(x i ) w T ?(x i , ?y i , s )+1? i , where s i is some fixed explanation (e.g., an initial guess of the best explanation). Then OP 1 reduces to a standard structural SVM, which can be solved efficiently . Algorithm 2 describes our training procedure. Starting with an initial guess s i for each training example, the training procedure alternates between solving an instance of the resulting structural SVM (called SSVMSolve in Algorithm 2) using the currently best known explanations s i (Line 9), and making a new guess of the best explanations (Line 7).  showed that this alternating procedure for training latent variable structural SVMs is an instance of the CCCP procedure  (Yuille and Rangarajan, 2003) , and so is guaranteed to converge to a local optimum. For our experiments, we do not train until convergence, but instead use performance on a validation set to choose the halting iteration. Since OP 1 is nonconvex, a good initialization is necessary. To generate the initial explanations, one can use an off-theshelf sentiment classifier such as OpinionFinder 2  (Wilson et al., 2005) . For some datasets, there exist documents with annotated sentences, which we can treat either as the ground truth or another (very good) initial guess of the explanatory sentences. 

 Feature Representation Like any machine learning approach, we must specify a useful set of features for the ? vectors described above. We will consider two types of features. Bag-of-words. Perhaps the simplest approach is to define ? using a bag-of-words feature representation, with one feature corresponding to each word in the active lexicon of the corpus. Using such a feature representation might allow us to learn which words have high polarity (e.g., "great") and which are indicative of subjective sentences (e.g., "opinion"). Sentence properties. We can incorporate many useful features to describe sentence subjectivity. For example, subjective sentences might densely populate the end of a document, or exhibit spatial coherence (so features describing previous sentences might be useful for classifying the current sentence). Such features cannot be compactly incorporated into flat models that ignore the document structure. For our experiments, we normalize each ? subj and ? pol to have unit 2-norm. Joint Feature Normalization. Another design decision is the choice of normalization N (x) in (3). Two straightforward choices are N (x) = f (|x|) and N (x) = f (|x|), where f (|x|) is the size constraint as described in (5). In our experiments we tried both and found the square root normalization to work better in practice; therefore all the experimental results are reported using N (x) = f (|x|). The appendix contains an analysis that sheds light on when square root normalization can be useful. 

 Incorporating Proximity Information As mentioned in Section 4.3, it is possible (and likely) for subjective sentences to exhibit spatial coherence (e.g., they might tend to group together). To exploit this structure, we will expand the feature space of ? subj to include both the words of the current and previous sentence as follows, ? subj (x, j) = ? subj (x j ) ? subj (x j?1 ) . The corresponding weight vector can be written as w subj = w subj w prevSubj . By adding these features, we are essentially assuming that the words of the previous sentence are predictive of the subjectivity of the current sentence. Alternative approaches include explicitly accounting for this structure by treating subjective sentence extraction as a sequence-labeling problem, such as in  McDonald et al. (2007) . Such structure formulations can be naturally encoded in the joint feature map. Note that the inference procedure in Algorthm 1 is still tractable, since it reduces to comparing the best sequence of subjective/objective sentences that explains a positive sentiment versus the best sequence that explains a negative sentiment. For this study, we chose not to examine this more expressive yet more complex structure. 

 Extensions Though our initial model (  3 ) is simple and intuitive, performance can depend heavily on the quality of latent variable initialization and the quality of the feature structure design. Consider the case where the initialization contains only objective sentences that do not convey any sentiment. Then all the features initially available during training are generated from these objective sentences and are thus useless for sentiment classification. In other words, too much useful information has been suppressed for the model to make effective decisions. To hedge against learning poor models due to using a poor initialization and/or a suboptimal feature structure, we now propose extensions that incorporate information from the entire document. We identify the following desirable properties that any such extended model should satisfy: (A) The model should be linear. (B) The model should be trained jointly. (C) The component that models the entire document should influence which sentences are extracted. The first property stems from the fact that our approach relies on linear models. The second property is desirable since joint training avoids error propagation that can be difficult to control. The third property deals with the information suppression issue. 

 Regularizing Relative to a Prior We first consider a model that satisfies properties (A) and (C). Using the representation in (4), we propose a training procedure that regularize w pol relative to a prior model. Suppose we have a weight vector w 0 which indicated the a priori guess of the contribution of each corresponding feature, then we can train our model using OP 2, Optimization Problem 2. min w,?0 1 2 w ? w 0 2 + C N N i=1 ? i s.t. ?i : max s?S i w T ?(x i , y i , s) ? max s ?S(x i ) w T ?(x i , ?y i , s ) + 1 ? ? i For our experiments, we use w 0 = w doc 0 , where w doc denotes a weight vector trained to classify the polarity of entire documents. Then one can interpret OP 2 as enforcing that the polarity weights w pol not be too far from w doc . Note that w 0 must be available before training. Therefore this approach does not satisfy property (B). 

 Extended Feature Space One simple way to satisfy all three aforementioned properties is to jointly model not only polarity and subjectivity of the extracted sentences, but also polarity of the entire document. Let w doc denote the weight vector used to model the polarity of entire document x (so the document polarity score is then w T doc ? pol (x)). We can also incorporate this weight vector into our structured model to compute a smoothed polarity score of each sentence via w T doc ? pol (x j ). Following this intuition, we propose the following structured model, w T ?(x, y, s) = y N (x) ? ? j?s w T pol ? pol (x j ) + w T doc ? pol (x j ) ? ? + 1 N (x) ? ? j?s w T subj ? subj (x j ) ? ? + y ? w T doc ? pol (x) where the weight vector is now w = ? ? w pol w subj w doc ? ? . Training this model via OP 1 achieves that w doc is (1) used to model the polarity of the entire document, and (2) used to compute a smoothed estimate of the polarity of the extracted sentences. This satisfies all three properties (A), (B), and (C), although other approaches are also possible. 

 Experiments 

 Experimental Setup We evaluate our methods using the Movie Reviews and U.S. Congressional Floor Debates datasets, following the setup used in previous work for comparison purposes. 3 Movie Reviews. We use the movie reviews dataset from  Zaidan et al. (2007)  that was originally released by  Pang and Lee (2004) . This version contains annotated rationales for each review, which we use to generate an additional initialization during training (described below). We follow exactly the experimental setup used in  Zaidan et al. (2007). 4  U.S. Congressional Floor Debates. We also use the U.S. Congressional floor debates transcripts from  Thomas et al. (2006) . The data was extracted from GovTrack (http://govtrack.us), which has all available transcripts of U.S. floor debates in the House of Representatives in 2005. As in previous work, only debates with discussions of "controversial" bills were considered (where the losing side had at least 20% of the speeches). The goal is to predict the vote ("yea" or "nay") for the speaker of each speech segment. For our experiments, we evaluate our methods using the speakerbased speech-segment classification setting as described in  Thomas et al. (2006) . 5 Since our training procedure solves a non-convex optimization problem, it requires an initial guess of the explanatory sentences. We use an explanatory set size (5) of 30% of the number of sentences in each document, L = 0.3 ? |x| , with a lower cap of 1. We generate initializations using OpinionFinder  (Wilson et al., 2005) , which were shown to be a reasonable substitute for human annotations in the Movie Reviews dataset  (Yessenalina et al., 2010) .  6  We consider two additional (baseline) methods for initialization: using a random set of sentences, and using the last 30% of sentence in the document. In the Movie Reviews dataset, we also use sentences containing human-annotator rationales as a final initialization option. No such manual annotations are available for the Congressional Debates. 

 Experimental Results We evaluate three versions of our model: the initial model (3) which we call SVM sle (SVMs for Sentiment classification with Latent Explanations), SVM sle regularized relative to a prior as described in the documents in the whole dataset contain only 1-3 sentences, making it an uninteresting setting to analyze with our model.  6  We select all sentences whose majority vote of Opinion-Finder word-level polarities matches the document's sentiment. If there are fewer than L sentences, we add sentences starting from the end of the document. If there are more, we remove sentences starting from the beginning of the document. Section 4.5.1 which we refer to as SVM sle w/ Prior, 7 and the feature smoothing model described in Section 4.5.2 which we call SVM sle f s . Due to the difficulty of selecting a good prior, we expect SVM sle f s to exhibit the most robust performance. Table  1  shows a comparison of our proposed methods on the two datasets. We observe that SVM sle f s provides both strong and robust performance. The performance of SVM sle is generally better when trained using a prior than not in the Movie Reviews dataset. Both extensions appear to hurt performance in the U.S. Congressional Floor Debates dataset. Using OpinionFinder to initialize our training procedure offers good performance across both datasets, whereas the baseline initializations exhibit more erratic performance behavior. 8 Unsurprisingly, initializing using human annotations (in the Movie Reviews dataset) can offer further improvement. Adding proximity features (as described in Section 4.4) in general seems to improve performance when using a good initialization, and hurts performance otherwise. Tables  2 and 3  show a comparison of SVM sle f s with previous work on the Movie Reviews and U.S. Congressional Floor Debates datasets, respectively. For the Movie Reviews dataset, we considered two settings: when human annotations are available, and when they are not (in which case we initialized using OpinionFinder). For the U.S. Congressional Floor Debates dataset we used only the latter setting, since there are no annotations available for this dataset. In all cases we observe SVM sle f s showing improved performance compared to previous results. Training details. We tried around 10 different values for C parameter, and selected the final model based on the validation set. The training procedure alternates between training a standard structural SVM model and using the subsequent model to re-label the latent variables. We selected the halting iteration of the training procedure using the validation set. When initializing using human annotations for the Movie Reviews dataset, the halting iteration is typically the first iteration, whereas the halting iteration is typically chosen from a later iteration  when initializing using OpinionFinder. Figure  1  shows the per-iteration overlap of extracted sentences from SVM sle f s models initialized using OpinionFinder and human annotations on the Movie Reviews training set. We can see that training has approximately converged after about 10 iterations.  9  We can also see that both models iteratively learn to extract sentences that are more similar to each other than their respective initializations (the overlap between the two initializations is 57%). This is an indicator that our learning problem, despite being non-convex and having multiple local optima, has a reasonably large "good" region that can be approached using different initialization methods. Varying the extraction size. Figure  2  shows how accuracy on the test set of SVM sle f s changes on the Movie Reviews dataset as a function of varying the extraction size f (|x|) from (5). We can see that performance changes smoothly 10 (and so is robust), and that one might see further improvement from more  9  The number of iterations required to converge is an upper bound on the number of iterations from which to choose the halting iteration (based on a validation set). 10 The smoothness will depend on the initialization. The discovery of embryonic stem cells is a major scientific breakthrough. Embryonic stem cells have the potential to form any cell type in the human body. This could have profound implications for diseases such as Alzheimer's, Parkinson's, various forms of brain and spinal cord disorders, diabetes, and many types of cancer. Ac-cording to the Coalition for the Advancement of Medical Research, there are at least 58 diseases which could potentially be cured through stem cell research. That is why more than 200 major patient groups, scientists, and medical research groups and 80 Nobel Laureates support the Stem Cell Research Enhancement Act. They know that this legislation will give us a chance to find cures to diseases affecting 100 million Americans. I want to make clear that I oppose reproductive cloning, as we all do. I have voted against it in the past. However, that is vastly different from stem cell research and as an ovarian cancer survivor, I am not going to stand in the way of science. Permitting peer-reviewed Federal funds to be used for this research, combined with public oversight of these activities, is our best assurance that research will be of the highest quality and performed with the greatest dignity and moral responsibility. The policy President Bush announced in August 2001 has limited access to stem cell lines and has stalled scientific progress. As a cancer survivor, I know the desperation these families feel as they wait for a cure. This congress must not stand in the way of that progress. We have an opportunity to change the lives of millions, and I hope we take it. I urge my colleagues to support this legislation. careful tuning of the size constraint. Examining an example prediction. Our proposed methods are not designed to extract interpretable explanations, but examining the extracted explanations might still yield meaningful information. Table  4  contains an example speech from the U.S. Congressional Floor Debates test set, with Latent Explanations found by SVM sle f s highlighted in boldface. This speech was made in support of the Stem Cell Research Enhancement Act. For comparison, Table  4  also shows the five least subjective sentences according to SVM sle f s . Notice that most of these "objective" sentences can plausibly belong to speeches made in opposition to bills that limit stem cell research funding. That is, they do not clearly indicate the speaker's stance towards the specific bill in question. We can thus see that our approach can indeed learn to infer sentences that are essential to understanding the document-level sentiment. 

 Discussion Making good structural assumptions simplifies the development process. Compared to methods that modify the training of flat document classifiers (e.g.,  Zaidan et al. (2007) ), our approach uses fewer parameters, leading to a more compact and faster train-ing stage. Compared to methods that use a cascaded approach (e.g.,  Pang and Lee (2004) ), our approach is more robust to errors in the lower-level subtask due to being a joint model. Introducing latent variables makes the training procedure more flexible by not requiring lower-level labels, but does require a good initialization (i.e., a reasonable substitute for the lower-level labels). We believe that the widespread availability of off-theshelf sentiment lexicons and software, despite being developed for a different domain, makes this issue less of a concern, and in fact creates an opportunity for approaches like ours to have real impact. One can incorporate many types of sentence-level information that cannot be directly incorporated into a flat model. Examples include scores from another sentence-level classifier (e.g.,  from Nakagawa et. al (2010))  or combining phrase-level polarity scores (e.g., from  Choi and Cardie (2008) ) for each sentence, or features that describe the position of the sentence in the document. Most prior work on the U.S. Congressional Floor Debates dataset focused on using relationships between speakers such as agreement  (Thomas et al., 2006; Bansal et al., 2008) , and used a global mincut inference procedure. However, they require all test instances to be known in advance (i.e., their formulations are transductive). Our method is not limited to the transductive setting, and instead exploits a different and complementary structure: the latent explanation (i.e., only some sentences in the speech are indicative of the speaker's vote). In a sense, the joint feature structure used in our model is the simplest that could be used. Our model makes no explicit structural dependencies between sentences, so the choice of whether to extract each sentence is essentially made independently of other sentences in the document. More sophisticated structures can be used if appropriate. For instance, one can formulate the sentence extraction task as a sequence labeling problem similar to  (McDonald et al., 2007) , or use a more expressive graphical model such as in  (Pang and Lee, 2004; Thomas et al., 2006) . So long as the global inference procedure is tractable or has a good approximation algorithm, then the training procedure is guaranteed to converge with rigorous generalization guarantees  (Finley and Joachims, 2008) . Since any formulation of the extraction subtask will suppress information for the main document-level task, one must take care to properly incorporate smoothing if necessary. Another interesting direction is training models to predict not only sentiment polarity, but also whether a document is objective. For example, one can pose a three class problem ("positive", "negative", "objective"), where objective documents might not necessarily have a good set of (subjective) explanatory sentences, similar to . 

 Conclusion We have presented latent variable structured models for the document sentiment classification task. These models do not rely on sentence-level annotations, and are trained jointly (over both the document and sentence levels) to directly optimize document-level accuracy. Experiments on two standard sentiment analysis datasets showed improved performance over previous results. Our approach can, in principle, be applied to any classification task that is well modeled by jointly solving an extraction subtask. However, as evidenced by our experiments, proper training does require a reasonable initial guess of the extracted ex-planations, as well as ways to mitigate the risk of the extraction subtask suppressing too much information (such as via feature smoothing). Figure 1 : 1 Figure 1: Overlap of extracted sentences from different SVM sle f s models on the Movie Reviews training set. 
