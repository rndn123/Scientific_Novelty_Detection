title
UMCC_DLSI-(SA): Using a ranking algorithm and informal features to solve Sentiment Analysis in Twitter

abstract
In this paper, we describe the development and performance of the supervised system UMCC_DLSI-(SA). This system uses corpora where phrases are annotated as Positive, Negative, Objective, and Neutral, to achieve new sentiment resources involving word dictionaries with their associated polarity. As a result, new sentiment inventories are obtained and applied in conjunction with detected informal patterns, to tackle the challenges posted in Task 2b of the Semeval-2013 competition. Assessing the effectiveness of our application in sentiment classification, we obtained a 69% F-Measure for neutral and an average of 43% F-Measure for positive and negative using Tweets and SMS messages.

Introduction Textual information has become one of the most important sources of data to extract useful and heterogeneous knowledge from. Texts can provide factual information, such as: descriptions, lists of characteristics, or even instructions to opinionbased information, which would include reviews, emotions, or feelings. These facts have motivated dealing with the identification and extraction of opinions and sentiments in texts that require special attention. Many researchers, such as  (Balahur et al., 2010; Hatzivassiloglou et al., 2000; Kim and Hovy, 2006; Wiebe et al., 2005)  and many others have been working on this and related areas. Related to assessment Sentiment Analysis (SA) systems, some international competitions have taken place. Some of those include: Semeval-2010 (Task 18: Disambiguating Sentiment Ambiguous Adjectives 1 ) NTCIR (Multilingual Opinion Analysis Task (MOAT 2 )) TASS 3 (Workshop on Sentiment Analysis at SEPLN workshop) and Semeval-2013 (Task 2 4 Sentiment Analysis in Twitter)  (Kozareva et al., 2013) . In this paper, we introduce a system for Task 2 b) of the Semeval-2013 competition. 

 Task 2 Description In participating in "Task 2: Sentiment Analysis in Twitter" of Semeval-2013, the goal was to take a given message and its topic and classify whether it had a positive, negative, or neutral sentiment towards the topic. For messages conveying, both a positive and negative sentiment toward the topic, the stronger sentiment of the two would end up as the classification. Task 2 included two sub-tasks. Our team focused on Task 2 b), which provides two training corpora as described in Table  3 , and two test corpora: 1) sms-test-input-B.tsv (with 2094 SMS) and 2) twitter-test-input-B.tsv (with 3813 Twit messages). The following section shows some background approaches. Subsequently, in section 3, we describe the UMCC_DLSI-(SA) system that was used in Task 2 b). Section 4 describes the assessment of the obtained resource from the Sentiment Classification task. Finally, the conclusion and future works are presented in section 5. 

 Background The use of sentiment resources has proven to be a necessary step for training and evaluating systems that implement sentiment analysis, which also include fine-grained opinion mining  (Balahur, 2011) . In order to build sentiment resources, several studies have been conducted. One of the first is the relevant work by  (Hu and Liu, 2004 ) using lexicon expansion techniques by adding synonymy and antonym relations provided by WordNet  (Fellbaum, 1998; Miller et al., 1990)  Another one is the research described by  (Hu and Liu, 2004; Liu et al., 2005)  which obtained an Opinion Lexicon compounded by a list of positive and negative opinion words or sentiment words for English (around 6800 words). A similar approach has been used for building WordNet-Affect  (Strapparava and Valitutti, 2004)  which expands six basic categories of emotion; thus, increasing the lexicon paths in WordNet. Nowadays, many sentiment and opinion messages are provided by Social Media. To deal with the informalities presented in these sources, it is necessary to have intermediary systems that improve the level of understanding of the messages. The following section offers a description of this phenomenon and a tool to track it. 

 Text normalization Several informal features are present in opinions extracted from Social Media texts. Some research has been conducted in the field of lexical normalization for this kind of text. TENOR  (Mosquera and Moreda, 2012 ) is a multilingual text normalization tool for Web 2.0 texts with an aim to transform noisy and informal words into their canonical form. That way, they can be easily processed by NLP tools and applications. TENOR works by identifying out-of-vocabulary (OOV) words such as slang, informal lexical variants, expressive lengthening, or contractions using a dictionary lookup and replacing them by matching formal candidates in a word lattice using phonetic and lexical edit distances. 

 Construction of our own Sentiment Resource Having analyzed the examples of SA described in section 2, we proposed building our own sentiment resource  (Guti?rrez et al., 2013)  by adding lexical and informal patterns to obtain classifiers that can deal with Task 2b of Semeval-2013. We proposed the use of a method named RA-SR (using Ranking Algorithms to build Sentiment Resources)  (Guti?rrez et al., 2013)  to build sentiment word inventories based on senti-semantic evidence obtained after exploring text with annotated sentiment polarity information. Through this process, a graph-based algorithm is used to obtain auto-balanced values that characterize sentiment polarities, a well-known technique in Sentiment Analysis. This method consists of three key stages:   

 Building contextual word graphs Initially, text preprocessing is performed by applying a Post-Tagging tool (using Freeling  (Atserias et al., 2006)  tool version 2.2 in this case) to convert all words to lemmas 5 . After that, all obtained lists of lemmas are sent to RA-SR, then divided into four groups: neutral, objective, positive, and negative candidates. As the first set  5  Lemma denotes canonic form of the words. of results, four contextual graphs are obtained: ?, ? , ?, and ? , where each graph includes the words/lemmas from the neutral, objective, positive and negative sentences respectively. These graphs are generated after connecting all words for each sentence into individual sets of annotated sentences in concordance with their annotations ( ? , ? , ?, ? ). Once the four graphs representing neutral, objective, positive and negative contexts are created, we proceed to assign weights to apply graph-based ranking techniques in order to autobalance the particular importance of each vertex ? ? into ?, ?, ? and ?. As the primary output of the graph-based ranking process, the positive, negative, neutral, and objective values are calculated using the PageRank algorithm and normalized with equation (1). For a better understanding of how the contextual graph was built see  (Guti?rrez et al., 2013) . 

 Applying a ranking algorithm To apply a graph-based ranking process, it is necessary to assign weights to the vertices of the graph. Words involved into ?, ?, ? and ? take the default of 1/N as their weight to define the weight of ? vector, which is used in our proposed ranking algorithm. In the case where words are identified on the sentiment repositories (see Table  4 ) as positive or negative, in relation to their respective graph, a weight value of 1 (in a range [0 ? 1] ) is assigned. ? represents the maximum quantity of words in the current graph. After that, a graph-based ranking algorithm is applied in order to structurally raise the graph vertexes' voting power. Once the reinforcement values are applied, the proposed ranking algorithm is able to increase the significance of the words related to these empowered vertices. The PageRank (Brin and Page, 1998) adaptation, which was popularized by  (Agirre and Soroa, 2009)  in Word Sense Disambiguation thematic, and which has obtained relevant results, was an inspiration to us in our work. The main idea behind this algorithm is that, for each edge between ?i and ?j in graph ?, a vote is made from ? i to ? j. As a result, the relevance of ? j is increased. On top of that, the vote strength from ? to ? depends on ? ? ? relevance. The philosophy behind it is that, the more important the vertex is, the more strength the voter would have. Thus, PageRank is generated by applying a random walkthrough from the internal interconnection of ?, where the final relevance of ? ? represents the random walkthrough probability over ? , and ending on ? ? . In our system, we apply the following configuration: dumping factor ? = 0.85 and, like in  (Agirre and Soroa, 2009)  we used 30 iterations. A detailed explanation about the PageRank algorithm can be found in  (Agirre and Soroa, 2009)  After applying PageRank, in order to obtain standardized values for both graphs, we normalize the rank values by applying the equation (  1 ), where ?(?) obtains the maximum rank value of ? vector (rankings' vector). ? ? = ? ? /?(?) (1) 

 Adjusting the sentiment polarity values After applying the PageRank algorithm on?, ? , ? and ? , having normalized their ranks, we proceed to obtain a final list of lemmas (named ? ) while avoiding repeated elements. ? is represented by ? ? lemmas, which would have, at that time, four assigned values: Neutral, Objective, Positive, and Negative, all of which correspond to a calculated rank obtained by the PageRank algorithm. At that point, for each lemma from ?, the following equations are applied in order to select the definitive subjectivity polarity for each one: ? = { ? ? ? ; ? > ? 0 ; ? (2) ? = { ? ? ? ; ? > ? 0 ; ? (3) Where ? is the Positive value and ? the Negative value related to each lemma in ?. In order to standardize again the ? and ? values and making them more representative in a [0?1] scale, we proceed to apply a normalization process over the ? and ? values. From there, based on the objective features commented by  (Baccianella et al., 2010) , we assume the same premise to establish an alternative objective value of the lemmas. Equation (  4 ) is used for that: ? = 1 ? |? ? ?| (4) Where ? represents the alternative objective value. As a result, each word obtained in the sentiment resource has an associated value of: positivity ( ? , see equation (  2 )), negativity ( ? , see equation (  3 )), objectivity (?_?, obtained by PageRank over ? and normalized with equation (  1 )), calculated-objectivity (?, now cited as ?_? ) and neutrality ( ? , obtained by PageRank over ? and normalized with equation (  1 )). 

 System Description The system takes annotated corpora as input from which two models are created. One model is created by using only the data provided at Semeval-2013 (Restricted Corpora, see Table  3 ), and the other by using extra data from other annotated corpora (Unrestricted Corpora, see Table  3 ). In all cases, the phrases are preprocessed using Freeling 2.2 pos-tagger  (Atserias et al., 2006)  while a dataset copy is normalized using TENOR (described in section 2.1). The system starts by extracting two sets of features. The Core Features (see section 3.1) are the Sentiment Measures and are calculated for a standard and normalized phrase. The Support Features (see section 3.2) are based on regularities, observed in the training dataset, such as emoticons, uppercase words, and so on. The supervised models are created using Weka 6 and a Logistic classifier, both of which the system uses to predict the values of the test dataset. The selection of the classifier was made after analyzing several classifiers such as: Support Vector Machine, J48 and REPTree. Finally, the Logistic classifier proved to be the best by increasing the results around three perceptual points. The test data is preprocessed in the same way the previous corpora were. The same process of feature extraction is also applied. With the aforementioned features and the generated models, the system proceeds to classify the final values of Positivity, Negativity, and Neutrality. 

 The Core Features The Core Features is a group of measures based on the resource created early (see section 2.2). The system takes a sentence preprocessed by Freeling 2.2 and TENOR. For each lemma of the analyzed sentence, ? , ? , ?_? , ?_?, and ? are calculated by using the respective word values assigned in RA-SR. The obtained values correspond to the sum of the corresponding values for each intersecting word between the analyzed sentence (lemmas list) and the obtained resource by RA-SR. Lastly, the aforementioned attributes are normalized by dividing them by the number of words involved in this process. Other calculated attributes are: ?_? , ?_? , ?_?_? , ?_?_? and ?_?. These attributes count each involved iteration for each feature type ( ? , ? , ?_? , ? and ? respectively, where the respective value may be greater than zero. Attributes ? and cnn are calculated by counting the amount of lemmas in the phrases contained in the Sentiment Lexicons (Positive and Negative respectively). All of the 12 attributes described previously are computed for both, the original, and the normalized (using TENOR) phrase, totaling 24 attributes. The Core features are described next. 

 Feature Name Description 

 ? Sum of respective value of each word. Counts the words contained in the Sentiment Lexicons for their respective polarities. 

 ? (to negative) Table  1 . Core Features 

 The Support Features The Support Features is a group of measures based on characteristics of the phrases, which may help with the definition on extreme cases. The emotPos and emotNeg values are the amount of Positive and Negative Emoticons found in the phrase. The exc and itr are the amount of exclamation and interrogation signs in the phrase. The following table shows the attributes that represent the support features:   

 Evaluation In the construction of the sentiment resource, we used the annotated sentences provided by the corpora described in Table  3 . The resources listed in Table  3  were selected to test the functionality of the words annotation proposal with subjectivity and objectivity. Note that the shadowed rows correspond to constrained runs corpora: tweeti-bsub.dist_out.tsv 8 (dist), b1_tweeti-objorneub.dist_out.tsv 9 (objorneu), twitter-dev-input-B.tsv 10 (dev). The resources from Table  3  that include unconstrained runs corpora are: all the previously mentioned ones, Computational-intelligence 11 (CI) and stno 12 corpora. The used sentiment lexicons are from the WordNetAffect_Categories 13 and opinion-words 14 files as shown in detail in Table  4 . Some issues were taken into account throughout this process. For instance, after obtaining a contextual graph ?, factotum words are present in most of the involved sentences (i.e., verb "to be"). This issue becomes very dangerous after applying the PageRank algorithm because the algorithm 7 Resources described in Table  4 .  8  Semeval-2013 (Task 2. Sentiment Analysis in Twitter, subtask b). 9 Semeval-2013 (Task 2. Sentiment Analysis in Twitter, subtask b). 10 http://www.cs.york.ac.uk/semeval-2013/task2/ 11 A sentimental corpus obtained applying techniques developed by GPLSI department. See (http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 12 NTCIR Multilingual Opinion Analysis Task (MOAT) http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 13 http://wndomains.fbk.eu/wnaffect.html 14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html strengthens the nodes possessing many linked elements. For that reason, the subtractions ? ? ? and ? ? ? are applied, where the most frequent words in all contexts obtain high values. The subtraction becomes a dumping factor. As an example, when we take the verb "to be", before applying equation (  1 ), the verb achieves the highest values in each subjective context graph ( ? and ?) namely, 9.94 and 18.67 rank values respectively. These values, once equation (  1 ) is applied, are normalized obtaining both ? = 1 and ? = 1 in a range [0...1]. At the end, when the following steps are executed (Equations (  2 ) and (  3 )), the verb "to be" achieves ? = 0 , ? = 0 and therefore ? = 1 . Through this example, it seems as though we subjectively discarded words that appear frequently in both contexts (Positive and Negative). Table  3 . Corpora used to apply RA-SR. Positive (P), Negative (N), Objective (Obj/O), Unknow (Unk), Total (T), Constrained (C), Unconstrained (UC). Sources P N T WordNet-Affects_Categories  (Strapparava and Valitutti, 2004)  629 907 1536 opinion-words  (Hu and Liu, 2004; Liu et al., 2005)  2006 4783 6789 Total 2635 5690 8325 Table  5 . Training dataset evaluation using crossvalidation (Logistic classifier (using 10 folds)). Constrained (Run1), Unconstrained (Run2), Correct(C), Incorrect (Inc). 

 The training evaluation In order to assess the effectiveness of our trained classifiers, we performed some evaluation tests. Table  5  shows relevant results obtained after applying our system to an environment (specific domain). The best results were obtained with the restricted corpus. The information used to increase the knowledge was not balanced or perhaps is of poor quality. 

 The test evaluation The test dataset evaluation is shown in Table  6 , where system results are compared with the best results in each case. We notice that the constrained run is better in almost every aspect. In the few cases where it was lower, there was a minimal difference. This suggests that the information used to increase our Sentiment Resource was unbalanced (high difference between quantity of tagged types of annotated phrases), or was of poor quality. By comparing these results with the ones obtained by our system on the test dataset, we notice that on the test dataset, the results fell in the middle of the effectiveness scores. After seeing these results (Table  5 and Table 6 ), we assumed that our system performance is better in a controlled environment (or specific domain). To make it more realistic, the system must be trained with a bigger and more balanced dataset. Table  6  shows the results obtained by our system while comparing them to the best results of Task 2b of Semeval-2013. In Table  5 , we can see the difference between the best systems. They are the ones in bold and underlined as target results. These results have a difference of around 20 percentage points. The grayed out ones correspond to our runs.  As we can see in the training and testing evaluation tables, our training stage offered more relevant scores than the best scores in Task2b  (Semaval-2013) . This means that we need to identify the missed features between both datasets (training and testing). For that reason, we decided to check how many words our system (more concretely, our Sentiment Resource) missed.   

 Conclusion and further work Based on what we have presented, we can say that we could develop a system that would be able to solve the SA challenge with promising results. The presented system has demonstrated election performance on a specific domain (see Table  5 ) with results over 80%. Also, note that our system, through the SA process, automatically builds sentiment resources from annotated corpora. For future research, we plan to evaluate RA-SR on different corpora. On top of that, we also plan to deal with the number of neutral instances and finding more words to evaluate the obtained sentiment resource. (I) Building contextual word graphs; (II) Applying a ranking algorithm; and (III) Adjusting the sentiment polarity values. These stages are shown in the diagram in Figure 1, which the development of sentimental resources starts off by giving four corpora of annotated sentences (the first with neutral sentences, the second with objective sentences, the third with positive sentences, and the last with negative sentences). 
