title
XLP at SemEval-2020 Task 9: Cross-lingual Models with Focal Loss for Sentiment Analysis of Code-Mixing Language

abstract
In this paper, we present an approach for sentiment analysis in code-mixed language on twitter defined in SemEval-2020 Task 9. Our team (referred as LiangZhao) employ different multilingual models with weighted loss focused on complexity of code-mixing in sentence, in which the best model achieved f1-score of 0.806 and ranked 1st of subtask-Sentimix Spanglish. The performance of method is analyzed and each component of our architecture is demonstrated.

Introduction Sentiment analysis is in the area of research that perform the automatic comprehension of the subjective information from user-generated data, which helps to gain the views on certain topics. Due to the rise of social media such as micro-blogs (e.g., Twitter) and the trend of global communications, they have accelerated the use of multilingual expressions, raising the concerns on code-mixing behavior  (Patwa et al., 2020) . To develop cross-lingual encoders that can encode any sentence into a shared embedding space, by using monolingual transfer learning, multilingual extensions of pretrained  (Lample et al., 2019)  encoders have been shown effective. As for code-mixed text, more complicated than cross-lingual sentence, it is crucial to consider the complexity of texts written in several different languages because different types of integration correlate with different social contexts  (Gualberto A. et al., 2016) . Sometimes, the user may post blogs in non-native language with grammar mistakes or even prefer to express the sentiment in the native language. The phenomena has encouraged the researchers to analyze the sentiment from multilingual code-mixed texts. Because Spanish and English share a lot of words with Latin roots, sometimes words with the same origin take a separate path in each language, or words with different origins resemble each other by coincidence, but have different meanings. For example, ?xito from Spanish means success, which resembles exit from English, with different meaning and sentiment. In the task, the number of words in a sentence vary from different languages dramatically. Intuitively, the language that has a bigger presence in the tweet would contain the sentiment of the sentence. To tackle the problem, we adopt the focal loss through calculating the ratio of each language in code-mixing text. The rest of the paper is structured as follows: Section 2 provides the detailed implementation method. Section 3 presents the results and performance of our models as well as experiment settings. Concluded remarks and future directions of our work are summarized in Section 4. 

 Implementation details 2.1 Preprocessing Normally, deep learning models have a simple data processing pipeline, while in the task data is very messy. Therefore we have used a more detailed method according to characteristic of the code-mixing data (URLs, emoji, hash symbol etc.) First, user name mentioned and URL are all removed because they are useless for sentiment prediction. Special characters like "RT" representing re-tweet is also deleted. Moreover, we also remove the hash Figure  1 : Illustration of our model symbol from hash-tags as it can be problematic for tokenizers to work with. As for non-text symbol like emoji and emoticon, we use the (emoji, 2019) library from python and emoticon dictionary from wiki (List of emoticons, 2020) respectively to transform the symbols to text. Next all characters into lowercase and stop words are removed. Afterwords, we employ fastBPE to generate and apply BPE codes to get post-BPE vocabulary using vocabulary of XLM model for 100 languages including Hindi, Spanish, and English. Sentence size is limited to 256. This is enough for nearly all of the tweets after processing. 

 Data augmentation In order to get more training data and based on the statistics of dataset, we have utilized machine translation  (Sennrich et al., 2016)  for generating more text to boost up the performance. After the original code-mixed text is translated to the target language Spanish, both source sentences and translated sentences are mixed to train a model. 

 Tested architectures 

 Pre-trained Models for Feature Encoding To extract valid representation features of tweet, two state-of-the-art pre-trained sentence embedding models are utilized. Details are deliberated in the following section. ? XLMs: We use pretrained embeddings made available by Facebook research  (Lample et al., 2019) , which is unsupervised that only relies on monolingual data, and support 100 languages including English and Spanish. After fine-tuning an XLM model on the training corpus, the model is still able to make accurate predictions at test time in code-mixed languages, for which there is not enough training data. This approach is usually referred to as "zero-shot cross-lingual classification". Based on the pretrained XLM model, the sentence is indexed by vocabulary and then independently fed into the pretrained transformer model, which is also optimized during training. The single column of last hidden layer of transformer model is used as the representation of sentence, fed into a projection layer using linear transformation. While for CNN model, all columns of last hidden layer are utilized as the sentence embedding. ? MUSE: MUSE are multilingual embeddings based on fastText  (Conneau et al., 2017) , available in different languages, where the words are mapped into the same vector space across languages. We use the average representations of all words in a sentence, which is modified during training as well. 

 Output layer Two models are examined with MUSE and XLM respectively: CNN based and linear layer based. ? Linear Classifier: The pretrained embeddings are just directly fed into a linear layer, also referred as fully connected layer and softmax afterwards to get the final predictions. 

 Optimized loss As analysis above, non-native English speaker may misuse English due to the culture differences and lack of vocabulary, and so on. The monolingual corpus in Spanish will be more accurate in the expression of the sentiment than multilingual. On the other hand, the quality of monolingual sample may be decreased due to error from augmentation data from translation. In view of data analysis of training and test corpus, we also found that the percentage of each language e.g., English and Spanish is biased. According to the statistics, the percentage of Spanish words is twice more than English in training dataset, and almost three times in valid and test data. The test data also have 560 monolingual sentences, in which half are in English and the other are in Spanish. In this case, the model is prone to learn the unbalanced semantic information. To benefit the gain from the samples and focus on the majority language model, we weighed the loss L W based on the complexity of code-mixing  (Gamb ack et al., 2014) .The formula is listed as followings, where ? is the percentage of Spanish words in a sentence, CE is the initial cross entropy, ? > 0 and ? is a constant positive scaling factor. To better explore the trend, weighted loss with different hyper-parameters is shown in shown in Figure  2 . L W = ? * CE * (1 ? ?) ? + ? * CE * ? ? The ? is a focusing parameter that control the loss. Larger values of ? correspond to large losses for low complexity of code-mixing sentences. When ? < 1, the model is prone to learn the multilingual data and on the contrary, if ? > 1, it's more likely to learn the monolingual data. When ? equals 1, the loss is just the cross entropy as default. 3 Experiments and Results evaluation 

 Dataset Subtask in Spanish of the SemEval-2020 task 9 is to predict the sentiment of a given code-mixed tweet. The sentiment labels are positive, negative, or neutral, and the code-mixed languages will be English-Spanish. Besides the sentiment labels, also the language labels at the word level are provided. The word-level language tags are en (English), spa (Spanish), hi (Hindi), mixed, and univ (e.g., symbols, @ mentions, hashtags).   

 Experiment Setup and Results Hyper-parameter optimization is performed using a simple grid search. All models are trained with 10 epochs with a batch size of 8 and an initial learning rate 0.000005 by Adam optimizer. The linear layers are dropped out with a probability of 0.5. Unless otherwise stated, default settings are used for other parameters. In the process of searching for optimal architecture and parameters, we experimented CNN and fully connected layer (marked as FC) respectively with MUSE and XLM. To explore and compare the optimal parameters ? and ?, as shown in Figure  3 , there is an obvious increasing tendency of f1 score until ? >1.5 when ? <= 1.0, and reaches the highest score as ? = 0.25 and second highest as ? = 1.0, which indicates that the model has found optimal parameters prone to high level of code-mixing data. Based on the results of validation set, to select best model, we expect that the best performance is always achieved in optimal parameters as above which are ?= 0.25 or ?= 1.0. The scores are summarized in Table  1 . XLM model with a fully connected layer achieved best when ?= 0.25, and from its class-wise scores, we conclude that the model performs best in classification of positive samples, while worst in neutral samples. The result can be caused by unbalanced distribution of data and complexity of code-mixing, such as the expression of positive sentiment mainly focused in specific language. CNN based model has not shown significant increase in performance compared to linear classifier. Figure 2 : 2 Figure 2: Weighted loss with different hyper-parameters 

 Figure 3 : 3 Figure 3: Mean f1 score for validation set of different hyper-parameters with XLM-FC 


 Table 1 : 1 Performance metrics of different models on validation and test sets. The average f1 scores of validation set are reported for ten runs using different random seeds to choose hyper-parameters, and the test scores are generated by using the trained model to predict on released labeled test data. 1. The trial data have 2000 tweets. 2. The train data have 15004 tweets. (They include trial data as well). 3. The train data after split have 12002 tweets.(used for training). 4. The validation data have 2998 tweets. 5. The test data have 3789 tweets. Moreover, 5000 back-translated data are added into training. Validation subset is used as an unbiased accuracy evaluation in order to fine-tune hyper parameters during training. To evaluate the performance of the system, Precision, Recall, and F-measure are measured. Precision Recall F1-score Positive 0.883 0.926 0.904 Negative 0.599 0.395 0.476 Neutral 0.181 0.209 0.194 Weighted - - 0.806 

 Table 2 : 2 Performance metrics of Class-wise Classification 

			 ConclusionIn this paper, we have introduced a novel approach with weighted loss of different multilingual models with weighted loss focused on complexity of code-mixing sentences for sentiment analysis task in SemEval-2020. The method is effective in situation where the distribution of different languages is unbalanced, and has a better control of language preference for sentiment by the level of how languages mix. Moreover, we conclude that the quality of word representations used has a significant impact on the performance of a model. Results indicate the potency of XLM on code-mixed lingual classification, leading to 4-5 % increase in f1 score compared to MUSE. In the future, we will continue to do model optimization and also try ensemble models.
