title
Tasty Burgers, Soggy Fries: Probing Aspect Robustness in Aspect-Based Sentiment Analysis

abstract
Aspect-based sentiment analysis (ABSA) aims to predict the sentiment towards a specific aspect in the text. However, existing ABSA test sets cannot be used to probe whether a model can distinguish the sentiment of the target aspect from the non-target aspects. To solve this problem, we develop a simple but effective approach to enrich ABSA test sets. Specifically, we generate new examples to disentangle the confounding sentiments of the non-target aspects from the target aspect's sentiment. Based on the SemEval 2014 dataset, we construct the Aspect Robustness Test Set (ARTS) as a comprehensive probe of the aspect robustness of ABSA models. Over 92% data of ARTS show high fluency and desired sentiment on all aspects by human evaluation. Using ARTS, we analyze the robustness of nine ABSA models, and observe, surprisingly, that their accuracy drops by up to 69.73%. We explore several ways to improve aspect robustness, and find that adversarial training can improve models' performance on ARTS by up to 32.85%. 1

Introduction Aspect-based sentiment analysis (ABSA) is an advanced sentiment analysis task that aims to classify the sentiment towards a specific aspect (e.g., burgers or fries in the review "Tasty burgers, and crispy fries."). The key to a strong ABSA model is it is sensitive to only the sentiment words of the target aspect, and therefore not be interfered by the sentiment of any non-target aspect. Although stateof-the-art models have shown high accuracy on existing test sets, we still question their robustness. Specifically, given the prerequisite that a model outputs correct sentiment polarity for the test example, we have the following questions: (Q1) If we reverse the sentiment polarity of the target aspect, can the model change its prediction accordingly? (Q2) If the sentiments of all non-target aspects become opposite to the target one, can the model still make the correct prediction? (Q3) If we add more non-target aspects with sentiments opposite to the target one, can the model still make the correct prediction? A robust ABSA model should both meet the prerequisite and have affirmative answers to all the questions above. For example, if a model makes the correct sentiment classification (i.e., positive) for burgers in the original sentence "Tasty burgers, and crispy fries", it should flip its prediction (to negative) when seeing the new context "Terrible burgers, but crispy fries". Hence, these questions together form a probe to verify if an ABSA model has high aspect robustness. Unfortunately, existing ABSA datasets have very limited capability to probe the aspect robustness. For example, the Twitter dataset  (Dong et al., 2014)  has only one aspect per sentence, so the model does not need to discriminate against non-target aspects. In the most widely used SemEval 2014 Laptop and Restaurant datasets  (Pontiki et al., 2014) , for 83.9% and 79.6% instances in the test sets, the sentiments of the target aspect, and all non-target aspects are all the same. Hence, we cannot decide whether models that make correct classifications attend only to the target aspect, because they may also wrongly look at the non-target aspects, which are confounding factors. Only a small portion of the test set can be used to answer our target questions proposed in the beginning. Moreover, when we test on the subset of the test set (59 instances in Laptop, and 122 instances in Restaurant) where the target aspect sentiment differs from all nontarget aspect sentiments (so that the confounding SubQ. Generation Strategy Example Prereq. SOURCE: The original sample from the test set Tasty burgers, and crispy fries. (Tgt: burgers) Q1 REVTGT: Reverse the sentiment of the target aspect Terrible burgers, but crispy fries. Q2 REVNON: Reverse the sentiment of the non-target aspects with originally the same sentiment as target Tasty burgers, but soggy fries. 

 Q3 ADDDIFF: Add aspects with the opposite sentiment from the target aspect Tasty burgers, crispy fries, but poorest service ever! factor is disentangled), the best model  (Xu et al., 2019a)  drops from 78.53% to 59.32% on Laptop and from 86.70% to 63.93% on Restaurant. This implies that the success of previous models may over-rely on the confounding non-target aspects, but not necessarily on the target aspect only. However, no datasets can be used to analyze the aspect robustness more in depth. We develop an automatic generation framework that takes as input the original test instances from SemEval 2014, and applies three generation strategies showed in Table  1 . New test instances generated by REVTGT, REVNON, and ADDDIFF can be used to answer the questions (Q1)-(Q3), respectively. The generated new instances largely overlap with the content and aspect terms of the original instances, but manage to disentangle the confounding sentiment polarity of non-target aspects from the target, as showed in the examples in Table  1 . In this way, we produce an "all-rounded" test set that can test whether a model robustly captures the target sentiment instead of other irrelevant clues. We enriched the laptop dataset by 294% from 638 to 1,877 instances and the restaurant dataset by 315% from 1,120 to 3,530 instances. By human evaluation, more than 92% of the new aspect robustness test set (ARTS) shows high fluency, and desired sentiment on all aspects. Our ARTS test set is in line with other recent works on NLP challenge sets  (McCoy et al., 2019; Gardner et al., 2020) . Using our new test set, we analyze the aspect robustness of nine existing models. Experiment results show that their performance degrades by up to 55.64% on Laptop and 69.73% on Restaurant. We also use our generation strategy to conduct adversarial training and find it improves aspect robustness by at least 11.87% and at most 35.37% across various models. The contributions of our paper are as follows: 1. We develop simple but effective automatic generation methods that generate new test in-stances (with over 92% accuracy by human evaluation) to challenge the aspect robustness. 2. We construct ARTS, a new test set targeting at aspect robustness for ABSA models, and propose a new metric, Aspect Robustness Score. 3. We probe the aspect robustness of nine models, and reveal up to 69.73% performance drop on ARTS compared with the original test set. 4. We provide several solutions to enhance aspect robustness for ABSA models, including adversarial training detailed in Section 5.4. 

 Data Generation As shown in Table  1 , we aim to build a systematic method to generate all possible aspect-related alternations, in order to remove the confounding factors in the existing ABSA data. In the following, we will introduce three different ways to disentangle the sentiment of the target aspect from sentiments of non-target aspects. 

 REVTGT The first strategy is to generate sentences that reverse the original sentiment of the target aspect. The word spans of each aspect's sentiment of Se-mEval 2014 data are provided by  (Fan et al., 2019a) . We design two methods to reverse the sentiment, and one additional step of conjunction adjustment on top of the two methods to polish the resulting sentence. 

 Strategy Example Flip Opinion It's light and easy to transport. ? It's heavy and difficult to transport. 

 Add Negation The menu changes seasonally. ? The menu does not change seasonally. 

 Adjust The food is good, and the decor is nice. Conjunctions ? The food is good, but the decor is nasty.  Flip Opinion Words Suppose we have the sentence "Tasty burgers and crispy fries," where the sentiment term for the target aspect is Tasty. We aim to generate a new sentence that flips the sentiment Tasty. A baseline approach is antonym replacement by looking up WordNet  (Miller, 1995) . However, due to polysemy, the simple lookup is very likely to derive an inappropriate antonym and cause incompatibility with the context. Among the retrieved set of antonyms, we only keep words with the same Part-of-Speech (POS) tag as original, using the stanza package 2 which takes the context into account by the state-of-the-art neural network-based model.  3  Lastly, in the case of multiple antonyms, we prioritize the words that are already in the existing vocabulary, and then randomly select an antonym from the candidate set. Add Negation As the above strategy of flipping by the antonym is constrained by whether appropriate antonyms are available. For those cases without suitable antonyms, including long phrases, we add negation according to the linguistic features. In most cases, the sentiment expression is an adjective or verb term, so we simply add negation (i.e., "not") in front of it. If the sentiment term is not an adjective or verb, we add negation to its closest verb. For example, in Table  2 , there are no available antonyms for "change" in the original example "The menu changes seasonally.", so we simply negate it as "The menu does not change seasonally." Adjust Conjunctions As pinpointed in Section 1, 79.6?83.9% of the original test data of SemEval 2014  (Pontiki et al., 2014)  have the same sentiment for all aspects. A possible result of reverting one aspect's sentiment is that the other aspects' sentiments will be opposite to the altered one. So we need to adjust the conjunctions for language fluency. If the two closest surrounding sentiments of a conjunction word have the same polarity, then cumulative conjunctions such as "and" should be applied; otherwise, we should adopt adversative conjunctions such as "but." In the example in Table 2, after flipping the sentiment, we derive the example "The food is good, and the decor is nasty" which is very unnatural, so we replace the conjunction "and" with "but," and thus generate the example "The food is good, but the decor is nasty." 

 REVNON Changing the target sentiment by REVTGT can test if a model is sensitive enough towards the targetaspect sentiment, but we need to further complement this probe by perturbing the sentiments of the non-target aspects (REVNON). As showed in Table  3 , for all the non-target aspects with the same sentiment as the target aspect's, we reverse their sentiments using the same method as REVTGT. And for all the remaining non-target aspects, whose sentiments are already opposite from the target sentiment, we exaggerate the extent by randomly adding an adverb (e.g., "very", "really" and "extremely") from a dictionary of adverbs of degree that is collected based on the training set. The resulting test example will be a solid proof of the ABSA quality, because only the target aspect has the desired sentiment, and all non-target aspects have been flipped to or exaggerated with the opposite sentiment. 

 ADDDIFF The first two strategies, REVTGT and REVNON, have explored how the sentiment changes of existing aspects will challenge an ABSA model, and ADDDIFF further investigate if adding more nontarget aspects can confuse the model. Moreover, the existing SemEval 2014 test sets have only on average 2 aspects per sentence, but the real-world applications can have more aspects. With these motivations, we develop ADDDIFF as follows. We first form a set of aspect expressions AspectSet 4 by extracting all aspect expressions from the entire dataset. Specifically, for each example in the dataset, we first identify each sentiment term (e.g., "reasonable" in "Food at a reasonable price") and then extract its linguistic branch as the aspect expression (e.g., "at a reasonable price") by pretrained constituency parsing  (Joshi et al., 2018) . Table  4  shows several examples of AspectSet in the restaurant domain. Using the AspectSet, we randomly sample 1-3 aspects that are not mentioned in the original test sample and whose sentiments are different from the target aspect's, and then append these to the end of the original example. For example, "Great food and best of all GREAT beer!" ADDDIFF ? ? ? "Great food and best of all GREAT beer, but management is less than accommodating, music is too heavy, and service is severely slow." In this way, ADDDIFF enables the advanced testing of whether the model will be confused when there are more irrelevant aspects with opposite sentiments. 

 ARTS Dataset 

 Overview Our source data is the most 5 widely used ABSA dataset, SemEval 2014 Laptop and Restaurant Reviews  (Pontiki et al., 2014) .  6  We follow  (Wang et al., 2016; Ma et al., 2017; Xu et al., 2019a)  to remove instances with conflicting polarity and only keep positive, negative, and neutral labels. We use the train-dev split as in  (Xu et al., 2019a)    

 Quality Inspection We conduct human evaluation to validate the generation quality of our ARTS dataset on two criteria: 1. Fluency: Does the generated sentence maintain the fluency of the source sentence? 2. Sentiment Correctness: Does the sentiment of each aspect have the desired polarity? ? REVTGT: Is the target sentiment reversed? ? REVNON: For non-target aspects with originally the same sentiment as the target, is it reversed? For the rest, are they exaggerated? ? ADDDIFF: Is the target sentiment unchanged? Each task is completed by two native-speaker judges. We first calculate the inter-agreement rate of the human annotators, and then resolve the divergent opinions on samples that they disagree with. We accept the samples that both judges considered as correct or are resolved to be correct after our check. Finally, we ask the annotators to fix the rejected samples by minimal edit which does not change the aspect term or the sentence meaning, but satisfies both criteria. Fluency Check The evaluation results on fluency are showed in Table  5 . Most samples (92.27% of Laptop and 92.35% of Restaurant test sets) are accepted as fluent text. The inter-agreement rate between the two human judges is also high, 91.10% and 92.69% on the two datasets. Sentiment Check We also evaluate the sentiment correctness of the generated text. Note that for REVNON, we count the samples with all "yes" answers as accepted samples. Overall, the acceptance rate of the generated samples is 93.93% on Laptop and 95.24% on Restaurant, along with interagreement rates of over 94.14% on both datasets. 

 Dataset Analysis After checking the quality of our enriched ARTS test set, we analyze the dataset characteristics and make comparisons with the original test sets. For general statistics, we can see from Table 6 that the sentence length in the new test set is on average 4 words more than the original, and the vocabulary size is also larger by around two hundred. For the label distribution, we can see that the new test set has an increasing number of all labels, and especially balances the ratio of positive-to-negative labels from the original 2.66 to 1.5 on Laptop, and from 3.71 to 1.77 on Restaurant. For the aspect-related challenge in the test set, the new test set, first of all, has a larger number of aspects per sentence than the original. Our test set also features the higher disentanglement of the target aspect from the non-target aspects that have the same sentiment as the target: the portion of samples with at least one non-target aspects of sentiments different from the target is 59?67%, and on average 45% higher than the original test sets. And the portion of the most challenging samples where all non-target aspects have sentiments different from the target one on the new test set is on average 30% more than that of the original test set. The average number of non-target aspects with opposite sentiments per sample in the new test set is on average 5 times that of the original set. 

 Aspect Robustness Score (ARS) As mentioned in Section 1, a model is considered to have high aspect robustness if it satisfies both the prerequisite and all three questions (Q1)-(  Q3 ). So we propose a novel metric, Aspect Robustness Score (ARS), that counts the correct classification of the source example and all its variations (REVTGT, REVNON, and ADDDIFF) as one unit of correctness. Then we apply the standard calculation of accuracy. Note that the three variations correspond to questions (Q1)-(Q3), respectively. 

 Evaluating ABSA Models We use our enriched test set as a comprehensive test on the aspect robustness of ABSA models. 

 Models For a comprehensive overview of the ABSA field, we conduct extensive experiments on models with a variety of neural network architectures. TD-LSTM:  (Tang et al., 2016a)  uses two Long Short-Term Memory Networks (LSTM) to encode the preceding and following contexts of the target aspect (inclusive) and concatenate the last hidden states of the two LSTMs to make the sentiment classification. AttLSTM: Wang et al. (  2016 ) apply an Attention-based LSTM on the concatenatation of the aspect and word embeddings of each token. GatedCNN: Xue and Li (2018) use a Gated Convolutional Neural Networks (CNN) that applies a Tanh-ReLU gating mechanism to the CNNencoded text with aspect embeddings. MemNet:  Tang et al. (2016b)  use memory networks to store the sentence as external memory and calculate the attention with the target aspect. GCN: Aspect-specific Graph Convolutional Networks (GCN)  (Zhang et al., 2019a)  first applies GCN over the syntax tree of the sentence and then imposes an aspect-specific masking layer on its top. BERT: Xu et al. (2019a) uses a BERT-based baseline  (Devlin et al., 2019)  and takes as input the concatenation of the aspect term and the sentence. BERT-PT:  Xu et al. (2019a)  post-train BERT on other review datasets such as Amazon laptop reviews  (He and McAuley, 2016)  and Yelp Dataset Challenge reviews, and finetune on ABSA tasks. CapsBERT:  (Jiang et al., 2019)  encode the sentence and the aspect term with BERT, and then feed it into Capsule Networks to predict the polarity. BERT-Sent: For more in-depth analysis, we also implement a sentence classification baseline. BERT-Sent takes as input sentences without aspect information, and predicts the "global" sentiment. We use it because if other ABSA models fails to pay attention to aspects, they will degenerate to a sentence classifier. If so, they will resemble BERT-Sent, which performs well on original tests and badly on ARTS. So BERT-Sent is a reference to check degenerated aspect-level models. 

 Implementation Details For all existing models, we use the authors' official implementation. For our self-proposed BERT-Sent, we use Adam optimizer with a learning rate of 5e-5, weight decay of 0.01, and batch size of 32. We apply the l 2 regularization with ? = 10 ?4 , and train 50 epochs. Note that the tokenization of the ASTS dataset is the same as the original SemEval 2014, as we prepared the new test set by inverting the NLTK tokenization rules we used when applying the generation strategies. 7 

 Results on ARTS We list the accuracy 8 of the nine models on the Laptop and Restaurant test sets in Table  7 . For Entire Test-New in Table  7 , accuracy is calculated using ARS. Supplementary to ARS, Table  7  also decomposes ARS into single-strategy scores (the right three columns) by splitting the entire ARTS test set into three subsets according to the corresponding data generation techniques. Each of the single-strategy scores explains from one perspective the reason for large performance drop in ARS, which will be elaborated later. Overall Performance On the entire test set, we can see that the accuracy of all models on the original test set is very high, achieving up to 78.53% on Laptop and 86.70% on Restaurant, but it drops drastically (?69%?25%) on our new test sets. Performance of Different Models From the overall performance on our new test set, we can see that BERT models on average are more robust to the aspect-targeted challenges that our new test set poses. The most effective model BERT-PT scores the best on both original accuracy and robustness. It has 53.29% ARS on Laptop and 59.29% on Restaurant. However, the accuracy of non-BERT models on average drops drastically to under 30% by over ?50%. Performance on Different Subsets We list in detail the performance of each model on the three subsets of our new test set: REVTGT, REVNON, and ADDDIFF. They correspond to the three questions (Q1)-(Q3). REVTGT on average induces the most performance drop, as it requires the model to pay precise attention to the target sentiment words. REVNON makes the performance of the sentence classifier BERT-Sent drops the most by up to ?45.93%, and the model CapsBERT also drops by up to ?39.26%. The last subset ADDDIFF causes most non-BERT models to drop significantly, indicating that these models are not robust enough against an increased number of non-target aspects, which should have been irrelevant. 

 Laptop vs. Restaurant The performance drop on Restaurant is higher than that on Laptop. There are two possible reasons: (1) the original performance on restaurant is higher, and (2) the new test set is more challenging in the Restaurant domain. We verify this by calculating the relative drop ( new?old old ) in addition to the reported absolute values of the change. The relative drop on Laptop is 64.76%, which is higher than 60.36% on Restaurant. For the laptop dataset, both the lower original performance and the larger relative decrease of performance might be due to the nature of the dataset. For example, Laptop restaurant has far fewer training data than Restaurant, which makes the models less accurate originally and weaker on ARTS. 

 Analysis 

 Variations of Generation Strategies Combining Multiple Strategies Each sample in the ARTS test set is generated by one of the three Table  7 : Model accuracy on Laptop and Restaurant data. We compare the accuracy on the Original and our New test sets (Ori ? New), and calculate the change of accuracy. Besides the Entire Test Set, we also list accuracy on subsets where the generation strategies REVTGT, REVNON and ADDDIFF can be applied. The accuracy of Entire Test-New is calculated using ARS. indicates whether the performance drop is statistically significant (with p-value ? 0.05 by Welch's t-test). strategies. However, it is also worth exploring whether combining several strategies can make a more challenging probe on the aspect robustness of ABSA models. As a case study, we analyze the model robustness against test samples generated by the combination of REVNON+ADDDIFF. By comparing the performance decrease caused  ADDDIFF with More Aspects Some strategies such as ADDDIFF can be parameterized by k, where k is the number of additional non-target aspects to be added. We select three models (the best, the worst, and an average-performing one), and plot their accuracy on test samples generated by ADDDIFF(k) on Laptop in Figure  1 . As k gets larger, the test samples become more difficult. The sentence classification baseline BERT-Sent drops drastically, BERT-PT remains high, and GCN lies in the middle. 

 How to Effectively Model the Aspect? An important usage of our ARTS is to understand what model components are key to aspect robustness. We list the aspect-specific mechanisms of all models according to the ascending order of their ARS on Laptop dataset in Table  9 . We can see that for BERT-based models, BERT-PT, which is further trained on large review corpora, gets the best accuracy and aspect robustness. More complicated structures like CapsBERT underperforms the basic BERT by 25.08%.  Table  9 : Models in the ascending order of their ARS on Laptop. We list their aspect-specific mechanisms, including concatenating the aspect and word embeddings (Asp+W Emb), position-aware mechanism for aspects (Posi-Aware), and attention using the aspect (Asp Att). We highlight for Posi-Aware as it is the most related to aspect robustness for non-BERT models. Among the non-BERT models, the aspect position-aware models TD-LSTM and GCN are the most robust, as they have a stronger sense of the location of the target aspect in a sentence. On the contrary, the other models with poorer robustness (9.87%?16.93% in Table  9 ) only use mechanisms such as aspect-based attention, or concatenating the aspect embedding to the word embedding. To summarize, the main takeaways are ? For BERT models, additional pretraining is the most effective. ? For non-BERT models, explicit positionaware designs lead to more aspect robustness. 

 Does a More Diverse Training Set Help? A recent dataset, Multi-Aspect Multi-Sentiment (MAMS)  (Jiang et al., 2019) , is collected from the same data source as the SemEval 2014 Restaurant dataset  (Ganu et al., 2009) . However, its sentences are more complicated, each having at least two aspects with different sentiment polarities. We use this dataset to inspect whether a stronger training set can help improve aspect robustness. Training and Testing on MAMS Table  10a  checks the aspect robustness of models trained on MAMS using the original MAMS test set (O?O) and the new test set that we produced by applying the same generation strategies to its test set (O?N). Models trained and tested on MAMS have a smaller decrease rate than those on the Restaurant dataset. This shows that a more challenging training set can make models more robust. 

 Training on MAMS and Testing on Restaurant As MAMS and Restaurant are collected from the same source data, we test whether MAMS-trained models perform well on the new test set of Restaurant (in the column "MAMS?N" of Table  10b ). We can see that all models trained on MAMS are more robust than those trained on the Restaurant dataset. For example, the accuracy of BERT and BERT-PT on the new test set is lifted up to 62.77%. 

 Does Adversarial Training Help? Although the MAMS described in Section 5.3 provides a training set with diversity, it remains difficult to improve aspect robustness for other domains, or future new datasets. Therefore, we propose a flexible method, adversarial training, for aspect robustness, which is applicable to any given dataset. We conducted adversarial training on the Laptop and Restaurant datasets, and analyze its effect in Table  10b . Specifically, for the column "Adv?N", we generated an additional training set by applying the three proposed strategies on training data, then trained models on the augmented data obtained by combining the original training data and this newly generated data, and finally evaluated on the ARTS test data. This practice follows Table  7  of  (Zhang et al., 2019b)  which is a similar stream of work as ours for the paraphrasing domain. In both Restaurant and Laptop domains, adversarial training (Adv?N) leads to significant performance improvement than only training on the original datasets (O?N). On the Restaurant datasets, adversarial training is even more effective than training on MAMS, because our generated data instances comprehensively covered all possible perturbations of the non-target aspects, and naturally collected datasets might not be comparable. 

 Error Analysis for Data Generation We analyze the error types in the subset of ARTS that was fixed by human judges.  as "the weight of the laptop is light?dark", and negation which causes grammatical errors (?1.1%). In future work, we can fix the latter by applying a grammatical error correction system on top of our generation. Also, REVTGT and REVNON cannot be applied to 1.4?6.6% instances with complicated sentiment expressions which rely on commonsense. For example, "a 2-hour wait" is negative bust too difficult to alter in our current generation framework. It needs more advanced models such as text style transfer  (Shen et al., 2017; Jin et al., 2019b) . 

 Related Work Robustness in NLP Robustness in NLP has attracted extensive attention in recent works  (Hsieh et al., 2019; . As a popular method to probe the robustness of models, adversarial text generation becomes an emerging research field in NLP. Techniques include adding extraneous text to the input  (Jia and Liang, 2016) , character-level noise  (Belinkov and Bisk, 2018; Ebrahimi et al., 2018) , and word replacement  (Alzantot et al., 2018; Jin et al., 2019a) . Using the adversarial generation techniques, new adversarial test sets are proposed for several tasks such as paraphrasing  (Zhang et al., 2019b)  and entailment  (Glockner et al., 2018; Mc-Coy et al., 2019) . Aspect-Based Sentiment Analysis ABSA has emerged as an active research area recently. Early works hand-craft sentiment lexicons and syntactic features for rule-based classifiers  (Vo and Zhang, 2015; Kiritchenko et al., 2014) . Recent neural network-based models use architectures such as LSTM  (Tang et al., 2016a) , CNN  (Xue and Li, 2018) , Attention mechanisms  (Wang et al., 2016) , Capsule Network  (Jiang et al., 2019) , and the pretrained model BERT  (Xu et al., 2019a) . Similar to the motivation in our paper, some work shows preliminary speculation that the current ABSA datasets might be downgraded to sentence-level sentiment classification  (Xu et al., 2019b) . 

 Conclusion In this paper, we proposed a simple but effective mechanism to generate test instances to probe the aspect robustness of the models. We enhanced the original SemEval 2014 test sets by 294% and 315% in laptop and restaurant domains. Using our new test set, we probed the aspect robustness of nine ABSA models, and discussed model designs and better training that can improve the robustness. Figure 1 : 1 Figure 1: Accuracy of BERT-PT, GCN, and BERT-Sent on the test samples in the laptop domain generated by ADDDIFF(k) where k varies from 1 to 5. 
