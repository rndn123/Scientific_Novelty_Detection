title
NILC USP: An Improved Hybrid System for Sentiment Analysis in Twitter Messages

abstract
This paper describes the NILC USP system that participated in SemEval-2014 Task 9: Sentiment Analysis in Twitter, a re-run of the SemEval 2013 task under the same name. Our system is an improved version of the system that participated in the 2013 task. This system adopts a hybrid classification process that uses three classification approaches: rule-based, lexiconbased and machine learning. We suggest a pipeline architecture that extracts the best characteristics from each classifier. In this work, we want to verify how this hybrid approach would improve with better classifiers. The improved system achieved an F-score of 65.39% in the Twitter message-level subtask for 2013 dataset (+ 9.08% of improvement) and 63.94% for 2014 dataset.

Introduction Twitter is an important platform of social communication. The analysis of the Twitter messages (tweets) offers a new possibility to understand social behavior. Understanding the sentiment contained in such messages showed to be very important to understand user behavior and also to assist market analysis  (Java et al., 2007; Kwak et al., 2010) . Sentiment analysis, the area in charge of studying how sentiments and opinions are expressed in texts, is usually associated with text classification This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ tasks. Sentiment classifiers are commonly categorized in two basic approaches: lexicon-based and machine learning approaches  (Taboada et al., 2011) . A lexicon-based classifier uses a lexicon to provide the polarity, or semantic orientation, of each word or phrase in the text. A machine learning classifier uses features (usually the vocabulary in the texts) obtained from labeled examples to classify the texts according to their polarity. In this paper, we present a hybrid system for sentiment classification in Twitter messages. Our system combines the lexicon-based and machine learning approaches, as well as uses simple rules to aid in the process. Our system participated in SemEval-2014 Task 9: Sentiment Analysis in Twitter  (Rosenthal et al., 2014) , a re-run for the Se-mEval 2013 task under the same name  (Nakov et al., 2013) . The task goal was to determine the sentiment contained in tweets. The task included two sub-tasks: a expression-level classification (Task A) and a message-level classification (Task B). Our system participated only in Task B, where, for a given message, it should classify it as positive, negative, or neutral. The system presented is an improved version of the system submitted for Semeval 2013. Our previous system had demonstrated that a hybrid approach could achieve good results (F-measure of 56.31%), even if we did not use the state-of-theart algorithms for each approach  (Balage Filho and Pardo, 2013) . In this way, this work aims to verify how much this hybrid system could improve in relation to the previous one by including modifications on both lexicon-based and machine learning approaches. 

 Related work The analysis of Tweets has gained lots of interest recently. One evidence is the expressive number of participants in the SemEval-2013 Task 2: Sentiment Analysis in Twitter  (Nakov et al., 2013) . There were a total of 149 submissions from 44 teams. The best performing system on twitter dataset for task B was reported by  Mohammad et al. (2013)  with an F-mesaure of 69.02%. Their system used a machine learning approach and a very rich feature set. They showed that the best results were achieved using a built-in positive and negative lexicon and a bag-of-words as features. Other important system in Semeval 2013 was reported by  Malandrakis et al. (2013) . The authors presented a hybrid system for twitter sentiment analysis combining two approaches: a hierarchical model based on an affective lexicon and a language modeling approach. The system achieved an F-mesaure of 60.14%. Most work in sentiment analysis uses either machine learning or lexicon-based techniques. However, few studies have shown promising results with the hybrid approach.  K?nig and Brill (2006)  proposed a hybrid classifier that uses human reasoning over automatically discovered text patterns to complement machine learning.  Prabowo and Thelwall (2009)  evaluated the effectiveness of different classifiers. Their study showed that the use of multiple classifiers in a hybrid manner could improve the effectiveness of sentiment analysis. 3 System Architecture Our system is described as a pipeline solution of four main processes: normalization, rule-based classification, lexicon-based classification and machine learning classification. This is the same architecture presented by our system in 2013. This pipeline architecture works as a back-off model. In this model, each classifier tries to classify the tweets by using the underlying approach. If a certain degree of confidence is achieved, the classifier will provide the final sentiment class for the message. Otherwise, the next classifier will continue the classification task. The last possibility is the machine learning classifier, responsible to deliver the class when the previous two could not achieve the confidence level. We decided to use this back-off model instead of a voting system, for example, due to the high precision achieved for the rule-based and the lexicon-based classifiers. The aim of this pipeline architecture is to improve the classification process. In Balage Filho and Pardo (2013), we have shown that this hybrid classification approach may outperform the individual approaches. In the following subsections, we detail the components of our system. In the next section, we explain how the confidence level was determined. 

 Normalization and Rule-based Classifier The normalization module is responsible for normalizing and tagging the texts. This module performs the following operations: ? Hashtags, urls and mentions are transformed into codes; ? Emoticons are grouped into representative categories (such as 'happy', 'sad', 'laugh') and are converted to particular codes; ? Part-of-speech tagging is performed by using the Ark-twitter NLP  (Owoputi et al., 2013)  The rule-based classifier is designed to provide rules that better impact the precision than the recall. In our 2014 system, we decided to use the same rule-based classifier from the 2013 system. The rules in this classifier only verify the presence of emoticons in the text. Empirically, we evidenced that the use of emoticons indicates the actual polarity of the message. In this module, we consider the number of positive and negative emoticons found in the text to determine its classification. 

 Lexicon-based Classifier The lexicon-based classifier is based on the idea that the polarity of a text can be given by the sum of the individual polarity values of each word or phrase present in the text. For this, a sentiment lexicon identifies polarity words and assigns polarity values to them (known as semantic orientations). In the 2013 system, we had used SentiStrength lexicon  (Thelwall et al., 2010) . In 2014, we improved our lexicon-based classifier by using a larger sentiment lexicon. We used the sentiment lexicon provided by Opinion-Lexicon  (Hu and Liu, 2004 ) and a list of sentiment hashtags provided by the NRC Hashtag Sentiment Lexicon  (Mohammad et al., 2013) . For dealing with negation, we used a handcrafted list of negative words. In our algorithm, the semantic orientations of each individual word in the text are added up. In this approach, the algorithm searches for each word in the lexicon and only the words that were found are returned. We associate the value +1 to the positive words, and -1 to the negative words. If a polarity word is negated, its value is inverted. This lexicon-based classifier assumes the signal of the final score as the sentiment class (positive or negative) and the score zero as neutral. 

 Machine Learning Classifier The machine learning classifier uses labeled examples to learn how to classify new instances. The features used for this 2014 system were completely changed from 2013 system. We inspired our machine learning module in the work reported by  Mohammad et al. (2013) . The features used by the classifier are: 1. unigrams, bigrams and trigrams 2. the presence of negation 3. the presence of three or more characters in the words 4. the sequence of three or more punctuation marks 5. the number of words with all letters in uppercase 6. the total number of each tag present in the text 7. the number of positive words computed by the lexicon-based method 8. the number of negative words computed by the lexicon-based method We use a Linear Kernel SVM classifier provided by the python sckit-learn library with C=0.005 1 . As we said in the previous section, our system is a pipeline of classifiers where each classifier may assign a sentiment class if it achieves a particular confidence threshold score. This confidence score is a fixed value set for each system in order to have a decision boundary. This decision was made by inspecting the results obtained for the development set. Tables  1 and 2  shows how the rule-based and lexicon-based classifiers perform for the development dataset in terms of score. The score obtained by the rule-based classifier consists of the difference between the number of positive emoticons and the number of negative emoticons found in the messages. The score obtained by the lexicon-based classifier represents the total semantic orientation obtained by the algorithm by adding up the semantic orientation for their lexicon. 

 Hybrid Approach and Tuning Inspecting Table  1 , for the best threshold, we adjusted the rule-based classifier boundary to decide when the score is different from zero. For values greater than zero, the classifier will assign the positive class and, for values below zero, the classifier will assign the negative class. For values equal to zero, the classifier will call the lexiconbased classifier.   2 , for the best threshold, we adjusted the lexicon-based classifier to assign the positive class when the total score is greater than 1 and negative class when the total score is below -2. For any other values, the classifier will call the machine learning classifier. As the machine learning classifier is responsible for the final stage, we did not have to decide any threshold for this classifier. However, we empirically identified a bias toward the positive class (the negative class was barely chosen). In order to correct this problem, we setup the machine learning classifier to decide for the negative class whenever the SVM score for this class is bigger than -0.4. Next section shows the results achieved for the Semeval test dataset. 

 Results Table  3  shows the results obtained by each individual classifier and by the hybrid classifier for the Twitter2014 messages in the testset. In the task, the systems were evaluated with the average Fscore obtained for positive and negative classes.  

 63.94 Table  4  shows the improvement of the system over the 2013 run. Unlike last year, we notice that the performance of this hybrid system is very close to the performance of the machine-learning. Table  5  shows the scores for each source in the testset. Last column shows our system rank among the 50 systems that participated in the competition. For the entire testing dataset, our algorithm had 503 (5%) examples classified by the rule-based classifier, 3204 (36%) by the lexicon-based classifier and 5280 (59%) by the machine learning classifier. 

 Conclusion We described our improved hybrid classification system used for Semeval-2014 Task 9: Sentiment Analysis in Twitter. This work showed that this hybrid classifier can be improved as its modules are too. However, we noticed that, improving the lexicon and machine learning modules, the overall score tends towards the machine learning score. The source code produced for the experiment is available at https://github.com/pedrobalage.  The organization from SemEval-2014 Task 9: Sentiment Analysis in Twitter provided four datasets for the task: a training dataset (TrainSet) with 9675 messages directly retrieved from Twitter; a development dataset (DevSet), with 1654 messages; the testing dataset from 2013 run, which was not used; and the testing dataset for 2014 1 Available at http://scikit-learn.org/ with 8987 messages. The 2014 testing dataset was composed of 5 different sources: ? Twitter2013: Twitter test data from 2013 run ? SMS2013: SMS test data from 2013 run ? Twitter2014: 2000 tweets ? LiveJournal2014: 2000 sentences from Live-Journal blogs ? Twitter2014Sarcasm: 100 tweets that contain sarcasm 
