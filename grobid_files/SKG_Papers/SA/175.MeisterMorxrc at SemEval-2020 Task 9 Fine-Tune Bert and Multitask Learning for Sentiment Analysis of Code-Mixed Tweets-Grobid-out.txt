title
MeisterMorxrc at SemEval-2020 Task 9: Fine-Tune Bert and Multitask Learning for Sentiment Analysis of Code-Mixed Tweets

abstract
Natural language processing (NLP) has been applied to various fields including text classification and sentiment analysis. In the shared task of sentiment analysis of code-mixed tweets, which is a part of the SemEval-2020 competition (Patwa et al., 2020), we preprocess datasets by replacing emoji and deleting uncommon characters and so on, and then fine-tune the Bidirectional Encoder Representation from Transformers(BERT) to perform the best. After exhausting top3 submissions, Our team MeisterMorxrc achieves an averaged F1 score of 0.730 in this task, and and our codalab username is MeisterMorxrc

Introduction Language is an indispensable and important part of human daily life. Natural language is everywhere as a most direct and simple tool of expression. Natural language processing is to transform the language used for human communication into a machine language that can be understood by machines. It is a model and algorithm framework for studying language capabilities. In recent years, NLP research has increasingly used new deep learning methods. As an important branch of artificial intelligence, language models are models that can estimate the probability distribution of a group of language units (usually word sequences). These models can be built at a lower cost and have significantly improved several NLP tasks, such as machine translation, speech recognition and parsing. The processing flow of natural language can be roughly divided into five steps: obtaining anticipation, preprocessing the corpus, characterizing, model training, and evaluating the effect of modeling. With the rapid development of the Internet, the frequency of online communication on social software such as Weibo, Twitter, and forums is getting higher and higher, and the Internet itself has also changed from "reading Internet" to "interactive Internet". The Internet has not only become an important source for people to obtain information, but also an important platform for people to express their opinions and share their own experiences and directly express their emotions. The achievements of NLP research laid a good foundation for text sentiment analysis. Text sentiment analysis is an important research branch in the field of natural language understanding, involving theories and methods in the fields of linguistics, psychology, artificial intelligence, etc. It mainly includes the processing of text sources, the subjective and objective classification of network text, and the subjective text Analysis and other steps. Due to the huge inclusiveness and openness of the Internet itself, it attracts users of different races, different languages, different cultural backgrounds and different religious beliefs to communicate with each other here. Therefore, mixed language sentiment classification will be an important research for NLP direction. 

 Related Work Sentiment analysis is a research with a long history that helps us understand the connections and relationships between objects. In recent years, many scholars have made great progess on sentiment analysis. A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level-whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Subsequently, the method described in a patent by  Volcani and Fogel (Volcani and Fogel, 2001) , looked specifically at sentiment and identified individual words and phrases in text with respect to different emotional scales. Many other subsequent efforts were less sophisticated, using a mere polar view of sentiment, from positive to negative, such as work by  Turney (Turney, 2002) , and Pang  (Pang et al., 2002)  who applied different methods for detecting the polarity of product reviews and movie reviews respectively. One can also classify a document's polarity on a multi-way scale, which was attempted by Pang  (Pang and Lee, 2005)  and Snyder  (Snyder and Benjamin, 2007) . But according to our findings, this research becomes particularly difficult in multilingual societies, especially in many code-mixed texts. Though some researchers have explored in the field, there is still a long way to go. Sharma and Srinivas explore various methods to normalize the text and judged the polarity of the statement as positive or negative using various sentiment resources  (Sharma et al., 2015) . Bhargava and Sharma develop a flexible and robust system for mining sentiments from code mixed sentences for English with combination of four other Indian languages (Tamil, Telugu, Hindi and Bengali)  (Bhargava et al., 2016) . Ghosh and Das extract sentiment (positive or negative) from Facebook posts in the form of code-mixed social media data using a machine learning approach  (Ghosh et al., 2017) . 

 Data and Methodology 

 Data Description This task is to predict the sentiment of the mixed tweets of a given code. The sentiment tags are divided into three categories: positive, negative and pertinent. Words are also given unique language tags, such as en (English), spa (Spanish), hi (Hindi), mixed and univ (for example, symbols, @mentioned, hashtags). The given data set is divided into training data set and validation data set, which contains emoticons, symbols, Spanish and English, Hindi and English mixed tweets. Since expressions and symbols cannot be directly put into classification and recognition, preprocessing is required to convert them into recognizable English phrases; for other languages mixed with English, we need to use English phrases to recognize them, and we need to label emotions. So use Multi-task to simultaneously recognize English words and perform emotion prediction. 

 Preprocessing We first process the data before feeding the data set to any model. In this section we will introduce the core methods and strategies of processing the raw data. Emoji Substitution -We design a function to map emoji unicode to replacement phrases. We treat these phrases as regular English phrases so their semantics can be preserved, especially when the dataset size is limited. Character Filtering -We also convert all the text into lower case. Since 'URL' does not have embedding representation in some pre-trained embedding and models, 'URL' is substituted by 'http'. Besides, we delete some uncommon characters without emotions such as '@' and 'https'. 

 Methodology Bert -The input part of BERT is a sequence where two sentences are connected. The two sentences are separated by a separator, and an identification symbol is added to the front and the end of each sentence to indicate the beginning and end of the sentence. For each word, BERT performs three different embedding operations, namely encoding the word position information, Word2vec encoding the word, and encoding the entire sentence. Vector stitching these three embedding results can get BERT input. In order to train the bidirectional feature and obtain the connection between the two sentences, BERT uses the Masked Language Model pre-training method, which randomly covers part of the words in the sentence, and uses the training model to predict this part of the word and the next sentence.This article uses the word vector pre-trained by the BERT method as the vector of the input short text. Because the mixed short texts of tweets are mostly replaced by English words in Spanish and Hindi words or Spanish and Hindi words in English, that is, the context is similar, so Spanish and English, Hindi and English The co-trained word vector is used as the vector for inputting short text. Fine-tune -The method NFT-TM refers to adding a complex network structure to the upper layer of the BERT model. During training, part of the convolutional layer of the pre-trained model is frozen, the parameters of the BERT are fixed, and only the upper task model network is trained separately. Matthew Peter and others from the Allen Institute for Artificial Intelligence in the United States compared the effects of the FT-TM and NFT-TM methods on the two pre-training models of ELMo and BERT. For BERT, the fine-tune effect is slightly better. Therefore, this experiment uses the NFT-TM method, followed by a simple specific task layer after the Bert model. During training, the BERT is fine-tuned according to the training sample set of the task. The practical results of this task show Excellent results. Multi-task -Multitask Learning is a derivation transfer learning method. The main tasks use domainspecific information possessed by the training signals of related tasks as a constant task. A machine learning method for inductive bias to improve the generalization performance of main tasks. Multi-task learning involves the simultaneous parallel learning of multiple related tasks and the simultaneous back propagation of gradients. Multiple tasks help each other learn through the underlying shared representation and improve the generalization effect. In this task, due to the mixed sentences of Spanish and English, Hindi and English, and the need to predict emotions.Therefore, the Multi-task model is used to identify English and predict emotion at the same time, reduce network overfitting and improve generalization effect. Adam -The Adam algorithm is different from the traditional stochastic gradient descent. Stochastic gradient descent keeps a single learning rate to update all the weights. The learning rate does not change during the training process, and Adam calculates the first and second moment estimates of the gradient design independent adaptive learning rates for different parameters. The Adam algorithm records the first moment of the gradient, that is, the average of all gradients in the past and the current gradient, so that each time the update is performed, the gradient of the last update and the current update will not differ much, that is, the gradient is smooth and stable can adapt to unstable objective functions. Adam recorded the second moment of the gradient, that is, the average of the square of the past gradient and the square of the current gradient, which reflects the environmental perception ability, and generates an adaptive learning rate for different parameters. This task uses the Adam algorithm to improve the overall efficiency of solving the problem, and performs more quickly and excellently when the gradient becomes sparse. Tfidf -We use Tf-Idf to digitize the text in the data set as a string, and then evaluate the importance of a word for a file set or for a corpus where it is located. The importance of a word increases in proportion to the number of times it appears in the document, but at the same time it decreases in inverse proportion to its frequency in the corpus. Both high-frequency words and low file frequencies in the file collection will produce a higher weight TF-IDF, so we use it to filter common words and retain important words, thereby improving the efficiency and accuracy of the overall task. The results on the official test sets for this task are presented in Table  1 . Our multi-task BERT model, fine-tuned on the provided dataset.After exhausting top3 submissions, we achieved an averaged F1 score of 0.730. 

 Experiment Results 

 System 

 Conclusion In this article, we introduce the results of SemEval-2020 Sentimix Task 9: Sentiment Analysis of Code-Mixed Tweets recognition and classification. The goal of this task is to determine and classify mixed languages, and label mixed sentences of English-Hindi and English-Spanish with positive, negative or neutral emotions. In the task, we first preprocess the data with Emoji Substitution and Character Filtering, convert the emoji recognition into recognizable English words, and then use the Fine-tune method on the processed data to access a specific network after the bert model Layer, through Multi-task and Adam to reduce the network overfitting and improve the generalization effect, improve the efficiency and accuracy of the overall task, and After exhausting top3 submissions, we achieved a score of 0.730 in this task. In the future, we will conduct more in-depth research on the entiment Analysis of Code-Mixed Tweets, and use other models and methods to practice and try to continuously improve the stability and accuracy of classification recognition. Table 1 : 1 The results of experiments F1(averaged) Fine-tuned Multi-task BERT 0.730 Fine-tuned BERT 0.687 Original BERT 0.589
