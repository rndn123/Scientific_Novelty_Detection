title
Memebusters at SemEval-2020 Task 8: Feature Fusion Model for Sentiment Analysis on Memes using Transfer Learning

abstract
In this paper, we describe our deep learning system used for SemEval 2020 Task 8: Memotion analysis. We participated in all the subtasks i.e Subtask A: Sentiment classification, Subtask B: Humor classification, and Subtask C: Scales of semantic classes. Similar multimodal architecture was used for each subtask. The proposed architecture makes use of transfer learning for images and text feature extraction. The extracted features are then fused together using stacked bidirectional Long Short Term Memory (LSTM) and Gated Recurrent Unit (GRU) model with attention mechanism for final predictions. We also propose a single model for predicting semantic classes (Subtask B) as well as their scales (Subtask C) by branching the final output of the post LSTM dense layers. Our model was ranked 5 in Subtask B and ranked 8 in Subtask C and performed nicely in Subtask A on the leader board. Our system makes use of transfer learning for feature extraction and fusion of image and text features for predictions.

Introduction A meme is an approach, concept, idea or style that spreads through social media within a society often with the goal of expressing a trend, topic, or significance represented by the meme  (Peirson and Tolunay, 2018) . Recent developments in social media analytics have shown a widespread increase in the usage of memes in expressing sentiment. Social media networks like Facebook, Instagram and Twitter usually get flooded with memes upon occurrence of any popular event. Memes are present in almost every form of media and they are constantly evolving considering the events happening around the world. The spread of memes on social media can be considered analogous to human genetics. Slightly altered versions of the same memes spread in social media in a similar way as copies of human genes pass through human lineage  (French, 2017) . They work as a medium for sharing humour on the pretext of cultural themes. They represent the sentiment of a community or culture with respect to the event involved. This is beneficial in gauging the sentiment of people. Memes concerning to political events may represent the political outlook of a community. Simultaneously, they can also be engineered to further political ideals, amplify echo chambers and alienate minorities. They are altogether a different means of communication and have become fundamental part of the current generation. Memes are uniquely multimodal, they contain images in conjugation with textual comments and conversations to add emphasis and provide additional meaning. Rarely, memes contain only the text or visual component. Thus, they pose strong challenge in identifying their semantics which may involve use of text as well as image features. Significant research has been carried out on sentiment classification of twitter data  (Rosenthal et al., 2017) , but they mostly confine to only the text. In comparison, multimodal analysis of social media content has very little research on it. Meme text generation  (Peirson and Tolunay, 2018)  on image templates provides some useful insights on use of deep learning on memes. The SemEval 2020 Task 8: Memotion analysis  (Sharma et al., 2020)  draws attention towards analyzing sentiment of memes on social media to extract the conveyed message and identify the semantics of the memes. In this paper we describe our deep learning model that competed in the SemEval 2020 Task 8: Memotion analysis  (Sharma et al., 2020) . Our proposed system makes use of transfer learning as a key component for extracting features from meme images and their Optical Character Recognition (OCR) extracted text. We use stacked bi-directional Long Short Term Memory (Bi-LSTM), GRU and attention mechanism for fusing the features of the two modalities into a single feature space. We use these combined features for classifying the humour class as well as quantify them on the given semantic scale. Our model performed well for Subtask B and Subtask C. We were ranked 5 th and 8 th on the official leader board. We propose a single multitask learning model for joint feature extraction which can then be used for multiple tasks dependent on similar feature space. This removes the need for training different systems for Subtask B and Subtask C. This brings down computational cost and complexity. Moreover, it advances the idea of transfer learning for tasks dependent on similar semantics. Our model was ranked 23 th in Subtask A. One possible reason for comparatively low performance could be the inability to differentiate between neutral and positive classes as they maybe remarkably similar. Our code is available online 1 for method replicability. 

 Background Understanding semantics of memes requires multimodal analysis, since they contain an image with some text. Occasionally it is an image without text that is symbolic of an emotion, such as surprise or joy  (French, 2017) , it poses an important challenge in recognizing their semantics. We need to take both components of a meme to analyse them. The study done in  (French, 2017)  focused on classifying the relationship between meme text and its corresponding image from social media. Sentiment classification using images combined with their label embeddings  (Graesser et al., 2017)  outperformed both only image and only text models. The text only model's performance was comparable with combined (image and text) model, which implies that textual data plays a crucial role while analysing multimodal data. Combining of the text and image modalities also poses a major challenge while performing analysis on meme data. Another challenge in performing analysis on such data is combining the two different modalities i.e text and image. Work done by  (Duong et al., 2017)  involves different fusion techniques for effectively combining the data and discusses their performance.  (Hu and Flaxman, 2018)  applied deep learning techniques to investigate the structure of emotions in multimodal data. They used transfer learning for extracting the features of each modality and concatenated them for further classification. Another important study on memes involved generating captions for meme templates  (Peirson and Tolunay, 2018) . It made use of image captioning model proposed by  (Vinyals et al., 2014)  which uses an encoder-decoder network. Work presented at  (Rosenthal et al., 2017) ,  (Zampieri et al., 2019)  provides useful insights for understanding the sentiment represented in text data. Memes have become an inherent part of modern-day world. Meagre research has been carried on memes and using combined text and images data for analysis. SemEval 2020 Task 8: Memotion analysis is an effort to bring research attention to this topic, it has three subtasks which we describe as follows: Subtask A (Sentiment Classification): Given a labelled dataset D of internet memes and their OCR extracted text, the objective of the task is to learn a classification function that can predict label L for a given meme, where L ? {negative, neutral, positive}. Subtask B (Humour classification): Given a labelled dataset D of internet memes and their OCR extracted text, the objective of the task is to learn a classification function that can identify the type of humour H for a given meme, where H ? {sarcastic, humorous, offensive, motivational}. Subtask C (Scales of semantic classes): Given a labelled dataset D of internet memes and their OCR extracted text, the objective of the task is to learn a classification function that can quantify the extent E to which a particular humour is expressed, where E ? {not, slightly, mildly, very}. Analysis of training set: Training set provided consisted of 6992 meme images and their OCR extracted text. The images were annotated as per the requirements of each subtask. 3 System Overview 

 High level Overview Figure  1  represents a high-level overview of our system. We process each modality separately to extract their individual features. Images are resized as per the requirement of the inception network and the text data is cleaned. The individual features are then fused into a single feature space as described in the following subsections. The fused features are then used for classification and quantification. 

 Text pre-processing The first step before feeding text data to any machine learning algorithm involves text pre-processing in order to clean the data. The extracted OCR text from the meme was cleaned and then processed. We followed the following pre-processing steps: 1. Website and URL removal: The text consisted of URL's from which the memes were extracted. We eliminated them since they do not provide valuable information. 2. Chat word conversion: Chat words like "LOL", "LMAO" etc. are extensively used for expressing emotions and can be helpful in recognizing the context. We converted them into their respective full forms. 3. Emoticon conversion: Emoticons are like emojis and are used in similar way. They are useful for increasing sentiment in text on social media. We converted emoticons to their respective meanings. Figure  1 : High level overview of our system 4. We further cleaned text data using ekphrasis  (Baziotis et al., 2017)  library: It normalizes the date's, time, numbers into a standard format. Annotates "hashtags", "allcaps", "elongated", "repeated", "emphasis", "censored". It performs hashtag segmentation and spelling correction on basis of twitter corpus. Lastly it tokenises the sentences. 

 Transfer Learning 

 Meme images: Inception network We used transfer learning for extracting features from meme images. For feature extraction from images, the pre-trained inception network  which was trained on imagenet  (Russakovsky et al., 2015)  dataset was used. Training a convolution network can be challenging as it is dependent on large datasets and requires testing different architectures before achieving satisfying performance. Transfer learning is a commendable approach as it decreases the computation required. The model is pre-trained on millions of images and it comprehends the fundamental composition of images. 2. Meme text: Global Vectors for Word Representation (GloVe) Word embeddings represent the semantic and syntactic meaning of the words as dense vector representations. It has improved the performance of several downstream tasks across various domains like text classification, machine comprehension etc.,  (Indurthi et al., 2019) . GloVe  (Pennington et al., 2014)  embeddings based on twitter corpus was used for encoding the words of each meme text. The choice of using embeddings trained on twitter corpus owes to the fact that it contains words from movies, TV show's and important events that are used for making memes. 

 Recurrent neural networks [LSTM's and GRU's] Recurrent neural networks (RNN) are effective for handling sequential information, they are called recurrent because they perform the same computation for each element, with the output being dependent on the previous computations. LSTM's  (Hochreiter and Schmidhuber, 1997)  and GRU  are frequently used RNNs. They overcome the problem of vanishing gradients which reduces the efficiency of earlier layers using several gates and cell states  (Bengio et al., 1994) . The cell states together with gates help in transporting relative information all the way down the sequences. The cell gates alter the sequence information. The collective effect of the memory cells along with gates aid the LSTM and GRU to learn long term dependencies.  

 Fusing image and text data The key challenge in the task was to merge the individual features obtained from text and images.  (Duong et al., 2017)  recommended a technique which used pooling to bring together features of different modalities of same dimension. The feature vectors of text and images were brought to same dimension. Text features are then modelled on images using LSTM's. A comparable method for image captioning using RNN's revealed encouraging results  (Vinyals et al., 2014) . The image feature vectors were passed as the initial states of the LSTM. This makes the LSTM aware of the image contents. We also made use of attention  with respect to the image feature vector for each time step of RNN to amplify the important words with respect to image. 

 Single model for humour classification as well as predicting it scale Subtask B and Subtask C required us to predict humour class H as well as quantify it on a scale E. Findings of  (Majumder et al., 2019)  show that multitask learning-based methods significantly outperform standalone sentiment and sarcasm classifiers indicating that sentiment classification and sarcasm detection are interrelated tasks. Subtask B and Subtask C for each humour class can be considered as inter-dependent. Hence, we use a single multitask learning model. 

 Model Description 

 Our model comprises of 4 main components: Image feature extraction: Image feature vectors were extracted using the inception network. We used the pre-final layer of the model for this purpose. Images were first resized to fit the requirements of the inception network. Text feature extraction: We used 100-dimensional GloVe embeddings for word level vector representation of the meme text content. We then passed the embeddings through a bi-directional LSTM to obtain a richer feature representation capable of understanding the overall context of the sentence. Feature fusion: This step involves combining the feature vectors from text and image. We use a bi-directional LSTM and attention for this task. The image feature vector dimension is reduced to the size of Bi-LSTM layer. We initialize the hidden state and cell state of Bi-LSTM with the image feature vector. Attention is calculated on text feature vectors with respect to image feature vector which is used as input for the LSTM for each timestamp. It amplifies the useful words with respect to the image and neglects the less important information. The outputs of the LSTM are then fed to a GRU. The output of the final timestamp of the GRU is then normalised using the batch normalisation layer . The output of the batch normalisation layers represents the fused feature vector. Classification and quantification: Once the fused feature vector is obtained the next step consists of simple dense layers for prediction. The output of normalization layer is branched into two separate dense layers one for humour classification and the other for quantifying it. Subtasks for which quantification scales were unavailable consisted of a single classification layer. Regularization: Due to the small size of the training set the model is easily prone to over-fitting. To overcome this problem, we used regularization. We used dropout  (Srivastava et al., 2014)  to randomly turn-off neurons in our neural network. Dropout prevents the adjacent neurons from learning similar features. It acts like ensemble learning by randomly shutting down some neurons at each iteration. For each training example only a sub-part of the network is utilized. Dropout is also used for the recurrent connections of RNN. In the final dense layer's, we used L2 regularization. It adds the sum of square of weights for these layers to the loss function. This makes sure that the value of weights remains relatively small during training so that the overall complexity of model does not increase leading to over-fitting. Class weights: The dataset classes were not equally balanced. This can add a significant bias to our model. In order to prevent it we used class weights to penalise the model more for less represented class. Let X be the vector containing counts of each class X i where i ? X. Then the weights for each class were given as: weight i = max(X) X i + max(X) 4 Experimental setup 

 Hyper-parameters We extracted 2048-dimensional image feature vectors from inception which were further reduced to 200 dimensions for fusion step using a dense layer. 100-dimension glove vectors were used for representing meme text. Bi-directional LSTM's of size 200 were used throughout the model. Feature fusion step used a GRU of size 64. A dropout of 0.2 was used on embedding layer. A dropout of 0.4 was used after first LSTM, dropout of 0.1 was used after the fusion step and dropout of 0.2 was used for recurrent connections of GRU and LSTM. L2 regularization of 0.001 is used in the final dense layers. All dense layers used 'relu' activation. The outputs of final layers used a softmax activation. 

 Training We trained all our models to minimize the cross-entropy loss. We used ADAM (Kingma and Ba, 2014) optimiser for backpropagation. Batch size of 200 was used and we trained our models for 120 epochs. Our models were developed on Keras (Chollet and others, 2015) using Tensorflow backend. We trained separate models for each humour class H. The details of branching the model for classification and quantification for each subtask is described as: 

 Subtask A The joint feature vector is fed into two different dense layers of three neurons (positive, negative, neutral) and five neurons (very negative, negative, neutral, positive, very positive). For prediction a layer with three neurons was used. 

 Subtask B and Subtask C The joint feature vector is fed into two different dense layers of size three and four. Subtask B required us to classify the meme as positive or negative for each class H, where H ? {sarcastic, humorous, offensive, motivational }. Subtask C required us to quantify the humour expressed. The quantification scale was E, where E ? {not, slightly, mildly, very}. For training we used two branches one for quantification meme into scale E. For the other branch we merged the "mild" and "very" semantic into a single class because "very" semantic scale was under-represented for each humour class. We trained separate classifiers for each humour class H. For motivational class quantification scales were not present so we trained it using a single dense layer after joint fusion. For Subtask B prediction, we used the 3 neuron branch and for Subtask C we used the 4 neuron branch. 

 Results and analysis Our model performed considerably well for Subtask B and Subtask C. Our team rank was 5 and 8. For Subtask A our rank was 23. The final F1 scores for our test set are given in the Our model performed considerably well for Subtask B and Subtask C. For Subtask A the lower performance can be due to difficulty in identifying the neutral class from the positive class. These two classes may have similar features which make it difficult for the model in identifying the correct classes. The difference in macro and micro F1 scores for Subtask A justifies the above premise. Another important aspect which we did not cover was the interdependence of humour classes H and sentiment L on each other. We trained independent models for each class for classification and quantification using similar architectures. We used transfer learning using only inception and GloVe models. Testing our architecture with separate models may lead to different results. One of the main problems which we came across was identifying sarcasm in images which is a key component of expressing sentiment and humour using memes. A lot of work has been done in identifying sentiment from text. Identifying sentiment in images that represent sarcasm is a tough task. Combining the two modalities improves the understanding of the memes. But better understanding of sentiments from images may further improve the results in future works. 

 Conclusion This paper describes our deep learning system that competed in all the subtasks of the SemEval 2020 Task 8: Memotion analysis. Transfer learning was used to extract features from image and text separately, employing the state-of-the-art models. We used LSTM with attention to combine the features of our model. A single model was used for classification as well as quantification for each H, where H ? {sarcastic, humorous, offensive, motivational}. We performed nicely in Subtask B and Subtask C. One important aspect which we would like to work on in the future is the interdependence of humour classes H on each other so that we can create a single model which utilises the interdependence of humour classes and perform classification as well as quantification for each H using a single combined model. Figure Figure 2: Architecture Diagram 
