title
IITB-Sentiment-Analysts: Participation in Sentiment Analysis in Twitter SemEval 2013 Task

abstract
We propose a method for using discourse relations for polarity detection of tweets. We have focused on unstructured and noisy text like tweets on which linguistic tools like parsers and POS-taggers don't work properly. We have showed how conjunctions, connectives, modals and conditionals affect the sentiments in tweets. We have also handled the commonly used abbreviations, slangs and collocations which are usually used in short text messages like tweets. This work focuses on a Web based application which produces results in real time. This approach is an extension of the previous work ).

Introduction Discourse relation is an important component of natural language processing which connects phrases and clauses together to establish a coherent relation. Linguistic constructs like conjunctions, connectives, modals, conditionals and negation do alter the sentiments of a sentence. For example, the movie had quite a few memorable moments but I still did not like it. The overall polarity of the sentence is negative even though it has one positive and one negative clause. This is because of the presence of the conjunction but which gives more weightage to the clause following the conjunction. Traditional works in discourse analysis use a discourse parser  (Marcu et al., 2003; Polanyi et al., 2004; Wolf et al., 2005; Welner et al., 2006; Narayanan et al., 2009; Prasad et al., 2010) . Many of these works and some other works in discourse  (Taboada et al., 2008; Zhou et al., 2011)  build on the Rhetorical Structure Theory (RTS) proposed by  Mann et al. (1988)  which tries to identify the relations between the nucleus and satellite in the sentence. Most of the work is based on well-structured text and the methods applied on that text is not suitable for the discourse analysis on micro-blogs because of the following reasons: 1. Micro-blogs like Twitter restricts a post (tweet) to be of only 140 characters. Thus, users do not use formal language to discuss their views. Thus, there are abundant spelling mistakes, abbreviations, slangs, collocations, discontinuities and grammatical errors. These differences cause NLP tools like POStaggers and parsers to fail frequently, as these tools are built for well-structured text. Thus, most of the methods described in the previous works are not well suited for discourse analysis on Micro-blogs like text. 

 2. The web-based applications require a fast response time. Using a heavy linguistic resource like parsing increases the processing time and slows down the application. Most of the previous work on discourse analysis does not take into consideration the conjunctions, connectives, modals, conditionals etc and are based on bag-of-words model with features like part-of-speech information, unigrams, bigrams etc. along with other domain-specific features like emoticons, hashtags etc. Our work harness the importance of discourse connectives like conjunctions, connectives, modals, conditionals etc and show that along with bag-ofwords model, it gives better sentiment classification accuracy. This work is the extension of . The roadmap for the rest of the paper is as follows: Section 2 studies the effect of discourse relations on sentiment analysis and identifies the critical ones. Section 3 talks about the semantic operators which influence the discourse relations. Section 4 discusses the lexicon based classification approach. Section 5 describes the feature engineering of the important features. Section 6 gives the list of experiments conducted and analysis of the results. Conclusion and Future Work is presented in Section 7. 

 Discourse Relations Critical for Sen- timent Analysis  showed that that the following discourse relations are critical for SA as all relations are not useful for SA.  

 Violated Expectations and Contrast: In Example 2, a simple bag-of-words feature based classifier will classify it as positive. However, it actually represents a negative sentiment. Such cases need to be handled separately. In Example 5, "memorable" has (+1) score and "not like" has (-1) score and overall polarity is 0 or objective whereas it should be negative as the final verdict following "but" is the deciding factor. These kinds of sentences refute the neighboring clause. They can be classified as Conj_Prev in which the clause preceding the conjunction is preferred and Conj_Fol in which the clause following the conjunction is preferred. 

 Conclusive or Inferential Conjunctions: These are the set of conjunctions, Conj_infer, that tend to draw a conclusion or inference. Hence, the discourse segment following them (subsequently in Example 11) should be given more weight. 

 Conditionals: In Example 3, "amazing" represent a positive sentiment. But the final polarity should be objective as we are talking of a hypothetical situation. Other Discourse Relations: Sentences under Cause-Effect, Similarity, Temporal Sequence, Attribution, Example, Generalization and Elaboration, provide no contrasting, conflicting or hy-pothetical information. They can be handled by taking a simple bag-of-words model. 

 Semantic Operators Influencing Discourse Relations There are connectives or semantic operators present in the sentences which influence the discourse relation within a sentence. For example, in the sentence the cannon camera may bad despite good battery life. The connective despite increases the weightage of the previous discourse element i.e. bad is weighted up but may introduces a certain kind of uncertainty which cannot be ignored. 1. (I did not study anything throughout the semester), so (I failed in the exams). 2. (Sourav failed to deliver in the penultimate test) despite (great expectations). 3. If (I had bought the amazing Nokia phone), I would not be crying). 

 (I love Cannon ) and (I also love Sony). 

 ( The movie had quite a few memorable moments) but (I still did not like it). 6. (The theater became interesting) after a while. 7. According (to the reviews), (the movie must be bad). 

 8. (Salman is a bad guy), for instance (he is always late). 9. In addition (to the bad battery life), (the camera is also very costly). 10. In general, (cameras from cannon (take great pictures). 11. (They were not in favour of that camera) and subsequently (decided not to buy it). 

 Table 1: Examples of Discourse Coherent Relations Similarity, in the sentence He gave his best in the movie, but still it was not good enough to win an Oscar. The connective but increases the weight of the following discourse i.e. good and win are weighted up but presence of negation operator also cannot be ignored. 

 Modals: Events that are happening or are bound to happen are called realis events. And those events that have possibly occurred or have some probability to occur in distant future are known as irrealis events. And it is important to distinguish between the two as it also alters the sentiments in a piece of text. Modals depict irrealis events and just cannot be handled by simple majority valence model. ) divided modals into two categories: Strong_Mod and Weak_Mod. Strong_Mod is the set of modals that express a higher degree of uncertainty in any situation. Weak_Mod is the set of modals that express lesser degree of uncertainty and more emphasis on certain events or situations. Like conditionals, sentences with strong modals express higher degree of uncertainty, thus discourse elements near strong modals are weighted down. Thus, in the previous example the cannon camera may bad despite good battery life bad is toned down.  

 Relations 

 Negation: The negation operator inverts the polarity of the sentence following it. Usually, to handle negation a window (typically 3-5 words) is considered and the polarities of all the words are reversed. We have considered the window size to be 5 and reverse the polarities of all the words within the window, till either a conjunction comes or window size exceeds. For example In the sentence He gave his best in the movie, but still it was not good enough to win an Oscar polarities of good and win are reversed. 

 Lexicon Based Classification We have used Senti-WordNet  (Esuli et al. 2006 ), Inquirer (Stone et. al 1996  and the Bing Liu sentiment lexicon  (Hu et al. 2004 ) to find out the word polarities. To compensate the bias effects introduced by the individual lexicons, we have used three different lexicons. The polarities of the reviews are given by ) ? ( ? ? * ? ? * ?(? ? )) ? ? ?=1 ? ?=1 ? ? ? ? = ? ? ? ? ? ? = 0 = ? ? ? 2 ? ? ? = 1 Above equation finds the weighted, signed polarity of a review. The polarity of each word, pol(w ij ) being +1 or -1, is multiplied with its discourse weight f ij and all the weighted polarities are added. Flip ij indicates if the polarity of w ij is to be negated. In case there is any conditional or strong modal in the sentence (indicated by ? ? = 1 ), then the polarity of every word in the sentence is toned down, by considering half of its assigned polarity ( +1 2 , ? 1 2 ) Thus, if good occurs in the user post twice, it will contribute a polarity of +1 ? 2 = +2 to the overall review polarity, if ? ? = 0. In the presence of a strong modal or conditional, it will contribute a polarity of +1 2 * 2 = +1. All the stop words, discourse connectives and modals are ignored during the classification phase, as they have a zero polarity in the lexicon. We have handled commonly used slangs, abbreviations and collocations by manually tagging them as positive, negative or neutral. 

 Feature Engineering The features specific for lexicon based classification for the task sentiment Analysis, identified in Section 2.4, are handled as follows: a) The words following the Conj_Fol (Table  2 ) are given more weightage. Hence their frequency count is incremented by 1. We follow a naive weighting scheme whereby we give a (+1) weightage to every word we consider important. In Example 5, "memorable" gets (+1) score, while "did not like" gets a (-2) score, making the overall score (-1) i.e. the example suggests a negative sentiment. b) The weightage of the words occurring before the Conj_Prev (Table  2 ) is increased by 1. In Example 2, "failed" will have polarity (-2) instead of (-1) and "great expectations" will have polarity (+1), making the overall polarity which conforms to the overall sentiment. c) The weightage of the words in the sentences containing conditionals (if) and strong modals (might, could, can, would, may) are toned down. e) The polarity of all words appearing within a window of 5 from the occurrence of a negation operator  (not, neither, nor, no, never)  and before the occurrence of a violating expectation conjunction is reversed. f) Exploiting sentence position information, the words appearing in the first k and last k sentences, are given more weightage. The value of k is set empirically. g) The Negation Bias factor is treated as a parameter which is learnt from a small set of negative polarity tagged documents. The frequency count of all the negative words (in a rule based system) is multiplied with this factor to give negative words more weightage than positive words. 

 Experiments and Evaluation For the lexicon-based approach, we performed two types of experiments-sentiment pertaining to a particular instance in a tweet (SemEval-2013 Task A) and generic sentiment analysis of a tweet (SemEval-2013 Task B). We treat both the tasks similarly. 

 Dataset We performed experiments on two Datasets: 1) SemEval-2013-task 2 Twitter Dataset A containing 4435 tweets without any external data. 2) SemEval-2013-task 2 Twitter Dataset B containing 3813 tweets without any external data. 

 Results on the Twitter Dataset A and B The system performs best for the positive class tweets as shown in Table  3  and Table  4  and performs badly for the negative class which is due to the fact that negative tweets can contain sarcasm which is a difficult phenomenon to capture. Also the results of the neutral category are very less which suggests that our system is biased towards subjective tweets and we wish to give the majority sentiment in the tweets.   

 Discussion The lexicon based classifier suffers from the problem of lexeme space where it is not able handle all the word senses. Also, short-noisy text like tweets often contain various spelling mistakes like great can be grt, g8t etc. or tomorrow can be tom, tomm, tommrrw etc. which will not be detected and handled properly. We suggest that a supervised approach comprising of the discourse features along with the bagof-words model and the sense based features will improve the results. 

 Conclusion and Future Work We have showed that discourse connectives, conjunctions, negations and conditionals do alter the sentiments of a piece of text. Most of the work on Micro-blogs like twitter is build on bagof-words model and does not incorporate discourse relations. We discussed an approach where we can incorporate discourse relations along-with bag-of-words model for a webapplication where parsers and taggers cannot be used as the results are required in real time. We need to take into consideration word senses and a supervised approach to use all the features collectively. Also, a spell checker would really help in the noisy text like in tweets. Table 1 1 pro- 
