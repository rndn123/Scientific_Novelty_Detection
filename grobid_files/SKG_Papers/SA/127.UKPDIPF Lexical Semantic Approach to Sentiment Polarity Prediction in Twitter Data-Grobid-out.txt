title
UKPDIPF: A Lexical Semantic Approach to Sentiment Polarity Prediction in Twitter Data

abstract
We present a sentiment classification system that participated in the SemEval 2014 shared task on sentiment analysis in Twitter. Our system expands tokens in a tweet with semantically similar expressions using a large novel distributional thesaurus and calculates the semantic relatedness of the expanded tweets to word lists representing positive and negative sentiment. This approach helps to assess the polarity of tweets that do not directly contain polarity cues. Moreover, we incorporate syntactic, lexical and surface sentiment features. On the message level, our system achieved the 8th place in terms of macroaveraged F-score among 50 systems, with particularly good performance on the Life-Journal corpus (F 1 =71.92) and the Twitter sarcasm (F 1 =54.59) dataset. On the expression level, our system ranked 14 out of 27 systems, based on macro-averaged F-score.

Introduction Microblogging sites, such as Twitter, have become an important source of information about current events. The fact that users write about their experiences, often directly during or shortly after an event, contributes to the high level of emotions in many such messages. Being able to automatically and reliably evaluate these emotions in context of a specific event or a product would be highly beneficial not only in marketing  (Jansen et al., 2009)  or public relations, but also in political sciences  (O'Connor et al., 2010) , disaster manage-This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ ment, stock market analysis  (Bollen et al., 2011)  or the health sector  (Culotta, 2010) . Due to its large number of applications, sentiment analysis on Twitter is a very popular task. Challenges arise both from the character of the task and from the language specifics of Twitter messages. Messages are normally very short and informal, frequently using slang, alternative spelling, neologism and links, and mostly ignoring the punctuation. Our experiments have been carried out as part of the SemEval 2014 Task 9 -Sentiment Analysis on Twitter  (Rosenthal et al., 2014) , a rerun of a SemEval-2013 Task 2  (Nakov et al., 2013) . The datasets are thus described in detail in the overview papers. The rerun uses the same training and development data, but new test data from Twitter and a "surprise domain". The task consists of two subtasks: an expression-level subtask (Subtask A) and a message-level subtask (Subtask B). In subtask A, each tweet in a corpus contained a marked instance of a word or phrase. The goal is to determine whether that instance is positive, negative or neutral in that context. In subtask B, the goal is to classify whether the entire message is of positive, negative, or neutral sentiment. For messages conveying both a positive and negative sentiment, the stronger one should be chosen. The key components of our system are the sentiment polarity lexicons. In contrast to previous approaches, we do not only count exact lexicon hits, but also calculate explicit semantic relatedness  (Gabrilovich and Markovitch, 2007)  between the tweet and the sentiment list, benefiting from resources such as Wiktionary and WordNet. On top of that, we expand content words (adjectives, adverbs, nouns and verbs) in the tweet with similar words, which we derive from a novel corpus of more than 80 million English Tweets gathered by the Language Technology group 1 at TU Darm-stadt. 

 Experimental setup Our experimental setup is based on an open-source text classification framework DKPro TC 2  (Daxenberger et al., 2014) , which allows to combine NLP pipelines into a configurable and modular system for preprocessing, feature extraction and classification. We use the unit classification mode of DKPro TC for Subtask A and the document classification mode for Subtask B. 

 Preprocessing We customized the message reader for Subtask B to ignore the first part of the tweet when the word but is found. This approach helps to reduce the misleading positive hits when a negative message is introduced positively (It'd be good, but). For preprocessing the data, we use components from DKPro Core 3 . Preprocessing is the same for subtasks A and B, with the only difference that in the subtask A the target expression is additionally annotated as text classification unit, while the rest of the tweet is considered to be a document context. We first segment the data with the Stanford Segmenter 4 , apply the Stanford POS Tagger with a Twitter-trained model  (Derczynski et al., 2013) , and subsequently apply the Stanford Lemmatizer  4 , TreeTagger Chunker (Schmid, 1994) , Stanford Named Entity Recognizer  (Finkel et al., 2005)  and Stanford Parser  (Klein and Manning, 2003)  to each tweet. After this linguistic preprocessing, the token segmentation of the Stanford tools is removed and overwritten by the ArkTweet Tagger  (Gimpel et al., 2011) , which is more suitable for recognizing hashtags and smileys as one particular token. Finally, we expand the tweet and proceed to feature extraction as described in detail in Section 3. 

 Classification We trained our system on the provided training data only, excluding the dev data. We use classifiers from the WEKA  (Hall et al., 2009)  toolkit, which are integrated in the DKPro TC framework. Our final configuration consists of a SVM-SMO classifier with a gaussian kernel. The optimal hyperparameters have been experimentally derived and finally set to C=1 and G=0.01. The resulting model was wrapped in a cost sensitive meta classifier from the WEKA toolkit with the error costs set to reflect the class imbalance in the training set. 

 Features used We now describe the features used in our experiments. For Subtask A (contextual polarity), we extracted each feature twice -once on the tweet level and once on the focus expression level. Only n-gram features were extracted solely from the expressions. For Subtask B (tweet polarity), we extracted features on tweet level only. In both cases, we use the Information Gain feature selection approach in WEKA to rank the features and prune the feature space with a threshold of T=0.005. 

 Lexical features As a basis for our similarity and expansion experiments (sections 3.4 and 3.5), we use the binary sentiment polarity lexicon by  Liu (2012)  augmented with the smiley polarity lexicon by  Becker et al. (2013)  and an additional swear word list 5 [further as Liu augmented ]. We selected this augmented lexicon for two reasons: firstly, it was the highest ranked lexical feature on the developmenttest and crossvalidation experiments, secondly it consists of two plain word lists and therefore does not introduce another complexity dimension for advanced feature calculations. We further measure lexicon hits normalized per number of tweet tokens for the following lexicons: Pennebaker's Linguistic Inquiry and Word Count (LIWC)  (Pennebaker et al., 2001) , the NRC Emotion Lexicon  (Mohammad and Turney, 2013) , the NRC Hashtag Emotion Lexicon  and the Sentiment140 lexicon . We use an additional lexicon of positive, negative, very positive and very negative words, diminishers, intensifiers and negations composed by  Steinberger et al. (2012) , where we calculate the polarity score as described in their paper. In a complementary set of features we combine each of the lexicons above with a list of weighted intensifying expressions as published by  Brooke (2009) . The intensity of any polar word found in any of the emotion lexicons used is intensified or diminished by a given weight if an intensifier (a bit, very, slightly...) is found within the preceding three tokens. Additionally, we record the overall counts of lexicon hits for positive words, negative words and the difference of the two. In one set of features we consider only lexicons clearly meant for binary polarity, while a second set of features also includes other emotions, such as fear or anger, from the NRC and the LIWC corpora. 

 Negation We handle negation in two ways. On the expression level (Subtask A) we rely on the negation dependency tag provided by the Stanford Dependency Parser. This one captures verb negations rather precisely and thus helps to handle emotional verb expressions such as like vs don't like. On the tweet level (all features of Subtask B and entiretweet-level features of Subtask A) we adopt the approach of  Pang et al. (2002) , considering as a negation context any sequence of tokens between a negation expression and the end of a sentence segment as annotated by the Stanford Segmenter. The negation expressions (don't, can't...) are represented by the list of invertors from Steinberger's lexicon  (Steinberger et al., 2012) . We first assign polarity score to each word in the tweet based on the lexicon hits and then revert it for the words lying in the negation context. This approach is more robust than the one of the dependency governor but is error-prone in the area of overlapping (cascaded) negation contexts. 

 N-gram features We extract the 5,000 most frequent word unigrams, bigrams and trigrams cleaned with the Snowball stopword list 6 as well as the same amount of skip-n-grams and character trigrams. These are extracted separately on the target expression level for subtask A and on document level for subtask B. On the syntactic level, we monitor the most frequent 5,000 part-of-speech ngrams with the size up to part-of-speech quadruples. Additionally, as an approximation for exploiting the key message of the sentence, we extract from the tweets a verb chunk and its left and right neighboring noun chunks, obtaining combinations such as we-go-cinema. The 1,000 most frequent chunk triples are then used as features similarly to ngrams.  

 Tweet expansion We expanded the content words in a tweet, i.e. nouns, verbs, adjectives and adverbs, with similar words from a word similarity thesaurus that was computed on 80 million English tweets from 2012 using the JoBim contextual semantics framework  (Biemann and Riedl, 2013) . Table  1  shows an example for a lexical expansion of the word awesome. The score was computed using left and right neighbor bigram features for the holing operation. The value hence shows how often the word appeared in the same left and right context as the original word. The upper limit of the score is set to 1,000. We then match the expanded tweet against the Liu augmented positive and negative lexicons. We assign to the lexicon hits of the expanded words their (contextual similarity) expansion score, using a score of 1,000 as an anchor-value for the original tweet, setting an expansion cut at 100. The overall tweet score is then normalized by the sum of word expansion scores. 

 Semantic similarity Tweet messages are short and each emotional word is very valuable for the task, even when it may not be present in a specific lexicon. Therefore, we calculate a semantic relatedness score between the tweet and the positive or negative word list. We use the ESA similarity measure  (Gabrilovich and Markovitch, 2007)  as implemented in the DKPro similarity software pack-age  (B?r et al., 2013) , calculated on English Wiktionary and WordNet as two separate concept spaces. The ESA vectors are freely available 7 . This way we obtain in total six features: sim(original tweet word list, positive word list), sim(original tweet word list, negative word list), difference between the two, sim(expanded tweet word list, positive word list), sim(expanded tweet word list, negative word list) and difference between the two. Our SemEval run was submitted using Word-Net vectors mainly for the shorter computation time and lower memory requirements. However, in our later experiments Wiktionary performed better. We presume this can be due to a better coverage for the Twitter corpus, although detailed analysis of this aspect is yet to be performed. 

 Other features Pak and Paroubek (2010) pointed out a relation between the presence of different part-of-speech types and sentiment polarity. We measure the ratio of each part-of-speech type to each chunk. We furthermore count the occurrences of the dependency tag for negation. We use the Stanford Named Entity Recognizer to count occurrence of persons, organizations and locations in the tweet. Additionaly, beside basic surface metrics, such as the number of tokens, characters and sentences, we measure the number of elongated words (such as coool) in a tweet, ratio of sentences ending with exclamation, ratio of questions and number of positive and negative smileys and their proportion. We capture the smileys with the following two regular expressions for positive, respectively negative ones:  ([/:{|] . We also separately measure the sentiment of smileys at the end of the tweet body, i.e. followed only by a hashtag, hyperlink or nothing. [<>]?[:;=8][-o * ']?[)]dDpPxXoO0 * }], [<>]?[:;=8][-o * ']?[ 

 Results In Subtask A, our system achieved an averaged F-score of 81.42 on the LiveJournal corpus and 79.67 on the Twitter 2014 corpus. The highest scores achieved in related work were 85.61 and 86.63 respectively. For subtask B, we scored 71.92 on LifeJournal and 63.77 on Twitter 2014, while the highest F-scores reported by related work were 74.84 and 70.96. 7 https://code.google.com/p/dkpro-similarity-asl/downloads/list Features with the highest Information Gain were the ones based on Liu augmented . Adding the weighted intensifiers of Brooke to the sentiment lexicons did not outperform the simple lexicon lookup. They were followed by features derived from the lexicons of Steinberger, which includes invertors, intensifiers and four polarity levels of words. On the other hand, adding the weighted intensifiers of Brooke to lexicons did not outperform the simple lexicon lookup. Overall, lexiconbased features contributed to the highest performance gain, as shown in Table  3 . The negation approach based on the Stanford dependency parser was the most helpful, although it tripled the runtime. Using the simpler negation context as suggested in  Pang et al. (2002)  performed still on average better than using none. When using WordNet, semantic similarity to lexicons did not outperform direct lexicon hits. Usage of Wiktionary instead lead to major improvement (Table  3 ), unfortunately after the Se-mEval challenge. Tweet expansion appears to improve the classification performance, however the threshold of 100 that we used in our setup was chosed too conservatively, expanding mainly stopwords with other stopwords or words with their spelling alternatives, resulting in a noisy, little valuable feature (expansion full in Table  3 ). Setting up the threshold to 50 and cleaning up both the tweet and the expansion with Snowball stopword list (expansion clean in Table  3 ), the performance increased remarkably. Amongst other prominent features were parts of lexicons such as LIWC Positive emotions, LIWC Affect, LIWC Negative emotions, NRC Joy, NRC Anger and NRC Disgust. Informative were also the proportions of nouns, verbs and adverbs, the exclamation ratio or number of positive and negative smileys at the end of the tweet.    Another challenge for the system was the nonstandard language in twitter with a large number of spelling variants, which was only partly captured by the emotion lexicons tailored for this domain. A twitter-specific lemmatizer, which would group all variations of a misspelled word into one, could help to improve the performance. The length of the negation context window does not suit all purposes. Also double negations such as I don't think he couldn't... can easily misdirect the polarity score. 

 Conclusion We presented a sentiment classification system that can be used on both message level and expression level with only small changes in the framework configuration. We employed a contextual similarity thesaurus for the lexical expansion of the messages. The expansion was not efficient without an extensive stopword cleaning, overweighting more common words and introducing noise. Utilizing the semantic similarity of tweets to lexicons instead of a direct match improves the score only with certain lexicons, possibly dependent on the coverage. Negation by dependency parsing was more beneficial to the classifier than the negation by keyword span annotation. Naive combination of sentiment lexicons was not more helpful than using individual ones separately. Among the common source of errors were laughing signs used in neutral messages and swearing used in positive messages. Even within Twitter, same words can have different polarity in different domains (lose weight, lose game, game with nice violent fights...). Deeper semantic insights are necessary to distinguish between polar words in context.  Table 1 : 1 Unsupervised expansion of 'awesome' Word Score Word (continued) Score awesome 1,000 fun 60 amazing 194 sexy 59 great 148 cold 59 cool 104 crazy 57 good 96 fantastic 56 best 93 bored 55 beautiful 93 excited 54 nice 87 true 53 funny 84 stupid 53 cute 81 gr8 52 perfect 70 entertaining 52 wonderful 67 favorite 52 lovely 66 talented 49 tired 65 other 49 annoying 63 depressing 48 Great 63 flawless 48 new 62 inspiring 47 hilarious 62 incredible 46 bad 61 complicated 46 hot 61 gorgeous 45 6 http://snowball.tartarus.org/algorithms/english/stop.txt 
