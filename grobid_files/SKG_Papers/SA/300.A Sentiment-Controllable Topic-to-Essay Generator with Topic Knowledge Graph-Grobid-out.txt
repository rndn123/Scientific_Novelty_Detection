title
A Sentiment-Controllable Topic-to-Essay Generator with Topic Knowledge Graph

abstract
Generating a vivid, novel, and diverse essay with only several given topic words is a challenging task of natural language generation. In previous work, there are two problems left unsolved: neglect of sentiment beneath the text and insufficient utilization of topicrelated knowledge. Therefore, we propose a novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational autoencoder (CVAE) framework. We firstly inject the sentiment information into the generator for controlling sentiment for each sentence, which leads to various generated essays. Then we design a Topic Knowledge Graph enhanced decoder. Unlike existing models that use knowledge entities separately, our model treats knowledge graph as a whole and encodes more structured, connected semantic information in the graph to generate a more relevant essay. Experimental results show that our SCTKG can generate sentiment controllable essays and outperform the state-of-theart approach in terms of topic relevance, fluency, and diversity on both automatic and human evaluation.

Introduction Topic-to-essay generation (TEG) task aims at generating human-like paragraph-level texts with only several given topics. It has plenty of practical applications, e.g., automatic advertisement generation, intelligent education, or assisting in keyword-based news writing  (Lepp?nen et al., 2017) . Because of its great potential in practical use and scientific research, TEG has attracted a lot of interest.  (Feng et al., 2018; Yang et al., 2019) . However, In TEG, two problems are left to be solved: the neglect of sentiment beneath the text and the insufficient utilization of topic-related knowledge. Love is a kind of emotion. Love experience is important to every one.... A well-performed essay generator should be able to generate multiple vivid and diverse essays when given the topic words. However, previous work tends to generate dull and generic texts. One of the reason is that they neglect the sentiment factor of the text. By modeling and controlling the sentiment of generated sentences, we can generate much more diverse and fascinating essays. As shown in Figure  1 , given the topic words "Love", "Experience" and "Emotion", the "without sentiment" model generates monotonous article. In contrast, the sentiment-attach model generates positive statements such as "fall in love with my boyfriend" when given the "positive" label, and generates negative phrases such as "addicted to smoking", "broke up" when given the "negative" label. In addition, sentiment control is especially essential in topicto-essay generation task, which aims to generate multiple sentences. As the number of sentences increases, the search space for generation model is exponentially enlarged by controlling the sentiment polarity for each of the sentence. Therefore, the ability to control sentiment is essential to improve discourse-level diversity for the TEG task. 

 It's been half a year since I fell in love As for the other problem, imagine that when we human beings are asked to write articles with some topics, we heavily rely on our commonsense knowledge related to the topics. Therefore, the proper usage of knowledge plays a vital role in the topic-to-essay generation. Previous state-of-the-art method  (Yang et al., 2019)  extracts topic-related concepts from a commonsense knowledge base to enrich the input information. However, they ignore the graph structure of the knowledge base, which merely refer to the concepts in the knowledge graph and fail to consider their correlation. This limitation leads to concepts being isolated from each other. For instance, given two knowledge triples (law, antonym, disorder) and (law, part of, theory), about the topic word law,  Yang et al. (2019)  simply uses the neighboring concepts disorder and theory as a supplement to the input information. However, their method fails to learn that disorder has opposite meaning with law while theory is a hypernym to law, which can be learned from their edges (correlations) in the knowledge graph. Intuitively, lacking the correlation information between concepts in the knowledge graph hinders a model from generating appropriate and informative essays. To address these issues, we propose a novel Sentiment-Controllable topic-to-essay generator with a Topic Knowledge Graph enhanced decoder, named SCTKG, which is based on the conditional variational auto-encoder (CVAE) framework. To control the sentiment of the text, we inject the sentiment information in the encoder and decoder of our model to control the sentiment from both sentence level and word level. The sentiment labels are provided by a sentiment classifier during training. To fully utilize the knowledge, the model retrieves a topic knowledge graph from a largescale commonsense knowledge base ConceptNet  (Speer and Havasi, 2012) . Different from  Yang et al. (2019) , we preserve the graph structure of the knowledge base and propose a novel Topic Graph Attention (TGA) mechanism. TGA attentively reads the knowledge graphs and makes the full use of the structured, connected semantic information from the graphs for a better generation. In the meantime, to make the generated essays more closely surround the semantics of all input topics, we adopt adversarial training based on a multi-label discriminator. The discriminator provides the reward to the generator based on the coverage of the output on the given topics. Our contributions can be summarized as follow: 1. We propose a sentiment-controllable topic-toessay generator based on CVAE, which can generate high-quality essays as well as control the sentiment. To the best of our knowledge, we are the first to control the sentiment in TEG and demonstrate the potential of our model to generate diverse essays by controlling the sentiment. 2. We equip our decoder with a topic knowledge graph and propose a novel Topic Graph Attention (TGA) mechanism. TGA makes the full use of the structured, connected semantic information from the topic knowledge graph to generate more appropriate and informative essays. 3. We conduct extensive experiments, showing that our model accurately controls the sentiment and outperforms the state-of-the-art methods both in automatic and human evaluations. 

 Task Formulation Traditional TEG task takes as input a topic sequence X = (x 1 , ? ? ? , x m ) with m words, and aims to generate an essay with M sentences (L 1 , ? ? ? , L M ) corresponding with topic sequence X. In this paper, we provide a sentiment sequence S = (s 1 , ? ? ? , s M ), each of which corresponds to a target sentence in essay. Each sentiment can be positive, negative, or neutral. Essays are generated in a sentence-by-sentence manner. The first sentence L 1 is generated only conditioned on the topic sequence X, then the model takes all the previous generated sentences as well as the topic sequence to generate the next sentence until the entire essay is completed. In this paper, we denote the previous sentences L 1:i?1 as context. 

 Model Description In this section, we describe an overview of our proposed model. Our SCTKG generator based on a CVAE architecture consists of an encoder and a topic knowledge graph enhanced decoder. The encoder encodes topic sequence, sentiment, and context and regards them as conditional variables c. Then a latent variable z is computed from c through a recognition network (during training) or prior network (during inference). The decoder attaches with a topic knowledge graph and sentiment label to generate the texts. At each decoding step, the TGA is used to enrich input topic information through effectively utilizing the topic knowledge graph. We adopt a two-stage training approach: (1) Train the SCTKG generator with the conventional CVAE loss; (2) After the first step is done, we introduce a topic label discriminator to evaluate the performance of SCTKG generator. We adopt adversarial training to alternately train the generator and the discriminator to further enhance the performance of the SCTKG generator. 

 SCTKG Generator 

 Encoder As shown in Figure  2 , the utterance encoder is a bidirectional GRU  (Chung et al., 2014)  to encode an input sequence into a fixed-size vector by concatenating the last hidden states of the forward and backward GRU. We use the utterance encoder to encode the topic sequence X into h x = [ ? ? h x , ? ? h x ], h x ? R d . d is the dimension of the vector. The next sequence L i is also encoded by utterance encoder into h i = [ ? ? h i , ? ? h i ], h i ? R d . For context encoder, we use a hierarchical encoding strategy. Firstly, each sentence in context L 1:i?1 is encoded by utterance encoder to get a fixed-size vector. By doing so, the context L 1:i?1 is encoded into h context = [h 1 , h 2 . . . , h i?1 ]. Then a single layer forward GRU is used to encode the sentence representations h context into a final state vector h c ? R d . Then the concatenation of h c , h x , e(s) is functionalized as the conditional vector c = [e(s); h c ; h x ]. e(s) is the embedding of sentiment label s. We assume that z follows a multivariate Gaussian distribution with a diagonal covariance matrix. Thus the recognition network q ? (z|h i , c) and the prior network p ? (z|c) follow N ?, ? 2 I and N ? , ? 2 I , respectively. I is identity matrix, and then we have ?, ? 2 = MLP recognition (h i , c), ? , ? 2 = MLP prior (c). (1) Additionally, we use a reparametrization trick (Kingma and Welling, 2013) to sample z from the recognition network during training and from prior network during testing. 

 Decoder A general Seq2seq model may tend to emit generic and meaningless sentences. To create more meaningful essays, we propose a topic knowledge graph enhanced decoder. The decoder is based on a 1-layer GRU network with initial state d 0 = W d [z, c, e(s)] + b d . W d and b d are trainable decoder parameters and e(s) is the sentiment embedding as mentioned above. As shown in Figure  2 , we equip the decoder with a topic knowledge graph to incorporate commonsense knowledge from Con-ceptNet 1 . ConceptNet is a semantic network which consists of triples R = (head; rel; tail). The head concept head has the relation rel with tail concept tail. We use word vectors to represent head and tail concepts and learn trainable vector r for relation rel , which is randomly initialized. Each word in the topic sequence is used as a query to retrieve a subgraph from ConceptNet and the topic knowledge graph is constituted by these subgraphs. Then we use the Topic Graph Attention (TGA) mechanism to read from the topic knowledge graph at each generation step. Topic Graph Attention. As previously stated, a proper usage of the external knowledge plays a vital role in our task. TGA takes as input the retrieved topic knowledge graph and a query vector q to produce a graph vector g t . We set q = [d t?1 ; c; z], where d t?1 represents the decoder hidden state for t ? 1 step. At each decoding step, we calculate the correlation score between each of the triples in the graph and q. Then we use the correlation score to compute the weighted sum of all the neighboring concepts 2 to the topic words to form the final graph vector g t . Neighboring concepts are entities that directly link to topic words. We formalize the computational process as follows: g t = N n=1 ? n o n , (2) ? n = exp (? n ) N j=1 exp (? j ) , (3) ? n = ? ? ? ? ? ? ? (W 1 q) tanh (W 2 r n + W 3 o n ) when o n ? S 1 , (W 1 q) tanh (W 2 r n + W 4 o n ) when o n ? S 2 (4) where o n is the embedding of n th neighboring concept and r n is the embedding of the relation for n th triple in the topic knowledge graph. W 1 , W 2 , W 3 , W 4 are weight matrices for query, relations, head entities and tail entities, respectively. S 1 contains the neighboring concepts which being the head concepts in their triples, while S 2 contains the neighboring concepts which being the tail concepts. The matching score ? n represents the correlation between the query q and neighbouring concept o n . Essentially, a graph vector g t is the weighted sum of the neighbouring concepts of the topic words. Note that we use different weight matrices to distinguish the neighboring concepts in different positions (in head or in tail). This distinction is necessary.  

 Topic Label Discriminator Another concern is that the generated texts should be closely related to the topic words. To this end, at the second training stage, a topic label discriminator is introduced to perform adversarial training with the SCTKG generator. In a max-min game, the SCTKG generator generates essays to make discriminator consider them semantically match with given topics. Discriminator tries to distinguish the generated essays from real essays. In detail, suppose there are a total of m topics, the discriminator produces a sigmoid probability distribution over (m + 1)classes. The score at (m + 1) th index represents the probability that the sample is the generated text. The score at the j th (i ? {1, ? ? ? , m}) index represents the probability that it belongs to the real text with the j th topic. Here the discriminator is a CNN  (Kim, 2014)  text classifier. 

 Training We introduce our two stage training method in this section. Stage 1: Similar to a conventional CVAE model, The loss of our SCTKG generator ?logp(Y |c) can be expressed as: ? L (?; ?; c; Y ) cvae = L KL + L decoder = KL (q ? (z|Y, c) p ? (z|c)) ? E q ? (z|Y,c) (log p D (Y |z, c)) . (5) Here, ? and ? are the parameters of the prior network and recognition network, respectively. Intuitively, L decoder maximizes the sentence generation probability after sampling from the recognition net-work, while L KL minimizes the distance between the prior and recognition network. Besides, we use the annealing trick and BOW-loss  (Zhao et al., 2017)  to alleviate the vanishing latent variable problem in VAE training. Stage 2: After trained the SCTKG generator with equation (  5 ), inspired by SeqGan  (Yu et al., 2017) , we adopt adversarial training between the generator and the topic label discriminator described in section 3.2. We refer reader to  Yu et al. (2017)  and  Yang et al. (2019)  for more details. 

 Experiments 

 Datasets We conduct experiments on the ZHIHU corpus  (Feng et al., 2018) . It consists of Chinese essays 3 whose length is between 50 and 100. We select topic words based on frequency and remove rare topic words. The total number of topic labels are set to 100. Sizes of the training set and the test set are 27,000 essays and 2500 essays. For tuning hyperparameters, we set aside 10% of training samples as the validation set. The sentence sentiment labels is required for our model training. To this end, we sample 5000 sentences from the dataset and annotated the data manually with three categories, i.e., positive, negative, neutral. This dataset was divided into a training set, validation set, and test set. We use an open-source Chinese sentiment classifier Senta 4 to finetune on our manually-label training set. This classifier achieves an accuracy of 0.83 on the test set. During training, the target sentiment labels s is computed by the sentiment classifier automatically. During inference, users can input any sentiment labels to control the sentiment for sentence generation. 

 Implementation Details We use the 200-dim pre-trained word embeddings provided by  and dimension of sentiment embeddings is 32. The vocabulary size is 50,000 and the batch size is 64. We use a manually tuning method to choose the hyperparameter values and the criterion used to select is BLEU  (Papineni et al., 2002a) . We use GRU with hidden size 512 for both encoder and decoder and the size of latent variables is 300. We implement the model with Tensorflow 5 . The number of parameters is 68M and parameters of our model were randomly initialized over a uniform distribution [-0.08,0.08]. We pre-train our model for 80 epochs with the MLE method and adversarial training for 30 epochs. The average runtime for our model is 30 hours on a Tesla P40 GPU machine, which adversarial training takes most of the runtime. The optimizer is Adam (Kingma and Ba, 2014) with 10 ?3 learning rate for pre-training and 10 ?5 for adversarial training. Besides, we apply dropout on the output layer to avoid over-fitting  (Srivastava et al., 2014)  (dropout rate = 0.2) and clip the gradients to the maximum norm of 10. The decoding strategy in this paper uses greedy search and average length of generated essays is 79.3. 

 Evaluation To comprehensively evaluate the generated essays, we rely on a combination of both automatic evaluation and human evaluation. Automatic Evaluation. Following previous work  (Yang et al., 2019) , we consider the following metrics 6 : BLEU: The BLEU score  (Papineni et al., 2002b ) is widely used in machine translation, dialogue, and other text generation tasks by measuring word overlapping between ground truth and generated sentences. Dist-1, Dist-2  (Li et al., 2015) : We calculate the proportion of distinct 1-grams and 2-grams in the generated essays to evaluate the diversity of the outputs. Consistency  (Yang et al., 2019) : An ideal essay should closely surround the semantics of all input topics. Therefore, we pre-train a multi-label classifier to evaluate the topic-consistency of the output. A higher "Consistency" score means the generated essays are more closely related to the given topics. Novelty  (Yang et al., 2019) : We calculated the novelty by the difference between output and essays with similar topics in the training corpus. A higher "Novelty" score means the output essays are more different from essays in the training corpus. Precision, Recall and Senti-F1: These metrics are used to measure sentiment control accuracy. If the sentiment label of the generated sentence is consistent with the ground truth, the generated result is right, and wrong otherwise. The sentiment label is predicted by our sentiment classifier mentioned above (see 4.1 for details about this classifier). Human Evaluation. We also perform human evaluation to more accurately evaluate the quality of generated essays. Each item contains the input topics and outputs of different models. Then, 200 items are distributed to 3 annotators, who have no knowledge in advance about the generated essays come from which model. Each annotator scores 200 items and we average the score from three annotators. They are required to score the generated essay from 1 to 5 in terms of three criteria: Novelty, Fluency, and Topic-Consistency. For novelty, we use the TF-IDF features of topic words to retrieve 10 most similar training samples to provide references for the annotators. To demonstrate the paragraph-level diversity of our model, we propose a Essay-Diversity criteria. Specifically, each model generates three essays with the same input topics, and annotators are required to score the diversity by considering the three essays together. 

 Baselines TAV  (Feng et al., 2018)  represents topic semantics as the average of all topic embeddings and then uses a LSTM to generate each word. Their work also includes the following two baselines. TAT  (Feng et al., 2018)  extends LSTM with an attention mechanism to model the semantic relatedness of each topic word with the generator's output. MTA  (Feng et al., 2018 ) maintains a topic coverage vector to guarantee that all topic information is expressed during generation through an LSTM decoder. CTEG  (Yang et al., 2019)  adopts commonsense knowledge and adversarial training to improve gen-eration. It achieves state-of-the-art performance on the topic-to-essay generation task. 

 Results and Analysis In this section, we introduce our experimental results and analysis from two part: the "text quality" and "sentiment control". Then we show case study of our model. 

 Results on Text Quality The automatic and human evaluation results are shown in Table  1 . We present three different versions of our model for a comprehensive comparison. (1)"SCTKG(w/o-Senti)" means we do not attach any sentiment label to the model. (  2 ) "SCTKG(Ran-Senti)" means we randomly set the sentiment label for each generated sentence. (3) "SCTKG(Gold-Senti)" means we set the golden sentiment label for the generated sentence. By investigating the results in Table  1 , we have the following observations: First, all versions of our SCTKG models outperform the baselines in all evaluation metrics (except the BLEU score of SCTKG(Ran-Senti)). This demonstrates that our SCTKG model can generate better essays than baseline models, whether uses the true sentiment, random sentiment or without any sentiment. Second, we can learn the superiority of the basic architecture of our model through the comparison between SCTKG(w/o-Senti) and the baselines. In human evaluation, SCTKG(w/o-Senti) outperform CTEG in topic-consistency, essay-diversity, and fluency by +0.15 (3.74 vs 3.89), +0.82 (3.08 vs 3.90), +0.12 (3.59 vs 3.71) respectively. Similar improvements can be also drawn from the automatic evaluation. The improvement in essay-diversity is the most significant. This improvement comes from our CVAE architecture because our sentence representation comes from the sampling from a continuous latent variable. This sampling operation introduces more randomness compared with baselines. Third, as previously stated, each model generates three essays and considers them as a whole when comparing the "E-div". When given the random and diverse sentiment label sequences, our SCTKG(Ran-Senti) achieves the highest "E-div" score (4.29). Consider that CVAE architecture has already improved the diversity compared with baselines. By randomizing the sentiment of each sentence, SCTKG(Ran-Senti) further boosts this improvement (from +0.82 to +1.21 compared with CTEG). This result demonstrates the potential of our model to generate discourse-level diverse essays by using diverse sentiment sequences, proving our claim in the introduction part. Fourth, when using the golden sentiment label, SCTKG(Gold-Senti) achieves the best performance in BLEU (11.02). However, we find the SCTKG(Gold-Senti) do not significantly outperforms other SCTKG models in other metrics. The results show the true sentiment label of the target sentence benefits SCTKG(Gold-Senti) to better fit in the test set, but there is no obvious help for other important metrics such as diversity and topicconsistency. Fifth, we find it interesting that when removing the sentiment label, the SCTKG(w/o-Senti) achieves the best topic-consistency score. We conceive that sentiment label may interfere with the topic information in the latent variable to some extent. But the effect of this interference is trivial. Comparing SCTKG(w/o-Senti) and SCTKG(Gold-Senti), the topic-consistency only drops 0.08 (3.89 vs 3.81) for human evaluation and 1.27 (43.84 vs 42.57) for automatic evaluation, which is completely acceptable for a sentiment controllable model. Ablation study on text quality. To understand how each component of our model contributes to the task, we train two ablated versions of our model: without adversarial training ("w/o AT") and without TGA ("w/o TGA"). Noted that in the "w/o TGA" experiment, we implement a memory network the same as  Yang et al. (2019)   bels. Table  2  presents the BLEU scores and human evaluation results of the ablation study. By comparing full model and "w/o TGA", we find that without TGA, the model performance drops in all metrics. In particularly, topicconsistency drops 0.27, which shows that by directly learning the correlation between the topic words and its neighboring concepts, concepts that are more closely related to the topic words are given higher attention during generation. Novelty drops 0.2, the reason is that TGA is an expansion of the external knowledge graph information. Therefore the output essays are more novel and informative. Fluency drops 0.37 because TGA benefits our model to choose a more suitable concept in the topic knowledge graph according to the current context. And the BLEU drops for 0.68 shows TGA helps our model to better fit the dataset by modeling the relations between topic words and neighboring concepts. By comparing full model and "w/o AT", we find that adversarial training can improve the BLEU, topic-consistency, and fluency. The reason is that the discriminative signal enhancing the topic consistency and authenticity of the generated texts. 

 Results on Sentiment Control In this section, we investigate whether the model accurately control the sentiment and how each component affects our sentiment control performance. We train three ablated versions of our model: without sentiment label in encoder, without sentiment label in decoder, and without TGA. We randomly sample 50 essays in our test set with 250 sentences. Instead of using golden sentiment labels, the sentiment labels are randomly given in this section. Predicting the golden sentiment is relatively simple because sometimes emotional labels can be directly derived from the coherence between contexts. We adopt a 

 Methods Precison Recall Senti-F1 more difficult experimental setting that aims to generate sentences following arbitrary given sentiment labels. The results are shown in Table  3 . We can learn that removing the sentiment label either from encoder or decoder leads to an obvious control performance decrease (-11% / -6% on Senti-F1) and the sentiment label in the encoder is the most important, since removing it leads to the most obvious decline (-11% Senti-F1). Although TGA does not directly impose sentiment information, it still helps to improve the control ability (4% in Senti-F1), which shows that learning correlations among concepts in topic knowledge graph strengthens the emotional control ability of the model. For instance, when given a positive label, the concepts related to the relation "desire of" are more likely to attach more attention, because the concepts with the relation "desire of" may represent more positive meaning. 

 Case Study Table  4  presents an example of our output essay with a random sentiment sequence. Positive sentences are shown in red and negative sentences are shown in blue. We can learn that the output es-say is not only closely related to the topic "Law" and "Education", but also corresponding with the randomly given sentiment label. Meanwhile, our model makes full use of commonsense knowledge with the help of TGA. For example, "high school student" and "right" are the neighboring concepts related to the topic words "Education" and "Law". 

 Related Work Topic-to-Text Generation. Automatically generating an article is a challenging task in natural language processing.  Feng et al. (2018)  are the first to propose the TEG task and they utilize coverage vector to integrate topic information.  Yang et al. (2019)  use extra commonsense knowledge to enrich the input information and adopt adversarial training to enhancing topic-consistency. However, both of them fail to consider the sentiment factor in the essay generation and fully utilize the external knowledge base. These limitations hinder them from generating high-quality texts. Besides, Chinese poetry generation is similar to our task, which can also be regarded as a topicto-sequence learning task.  adopt CVAE and adversarial training to generate diverse poetry.  use CVAE with hybrid decoders to generate Chinese poems. And  Yi et al. (2018)  use reinforcement learning to directly improve the diversity criteria. However, their models are not directly applicable to TEG task. Because they do not take knowledge into account, their models cannot generate long and meaningful unstructured essays. Controllable Text Generation. Some work has explored style control mechanisms for text generation tasks. For example,  Zhou and Wang (2017)  use naturally annotated emoji Twitter data for emotional response generation.  Wang and Wan (2018)  propose adversarial training to control the sentiment of the texts.  Chen et al. (2019)  propose a semi-supervised CVAE to generate poetry and deduce a different lower bound to capture generalized sentiment-related semantics. Different from their work, we inject sentiment label in both encoder and decoder of CVAE and prove that by modeling a topic knowledge graph can further enhance the sentiment control ability. 

 Conclusions In this paper, we make a further step in a challenging topic-to-essay generation task by proposing a novel sentiment-controllable topic-to-essay generator with a topic knowledge graph enhanced decoder, named SCTKG. To get better representation from external knowledge, we present TGA, a novel topic knowledge graph representation mechanism. Experiments show that our model can not only generate sentiment-controllable essays but also outperform competitive baselines in text quality. Figure 1 : 1 Figure1: Examples of comparison between the generated essays with sentiment control and without sentiment. We show the first two sentences for each generated essay and denote positive sentences in red and negative sentences in blue. Sentences without sentiment label are showed in black. 

 Figure 2 : 2 Figure 2: The architecture of our model. ? denotes the vector concatenation operation. Only the part with solid lines and the red dotted arrow is applied at inference, while the entire CVAE except the red dotted arrow part used in the training process. Sentiment label s with blue arrows denote sentiment control. Red solid lines denote TGA at each decoding step. The text generated by SCTKG generator feeds to topic label discriminator. The above m blue circle represents the probability that it belongs to the real text with the m input topics, and the green circle represents the given text is a generated text. 

 For instance, given two knowledge triples (Big Ben, part of, London) and (London, part of, England). Even though the concepts Big Ben and England are both neighboring concepts to London with the same relation part of, they have the different meaning with regard to London. We need to model this difference by W 3 and W 4 .Then the final probability of generating a word is computed byP t = softmax (W o [d t ; e(s); g t ] + b o ) ,where d t is the decoder state at t step andW o ? R d model ?|V | , b o ? R |V | aretrainable decoder parameters. d model is the dimension of [d t ; e(s); g t ] and |V | is vocabulary size. 

 Table 1 : 1 Automatic and human evaluation result. In human evaluation, Con., Nov., E-div., Flu. represent topicconsistency, novelty, essay-diversity, fluency, respectively. The best performance is highlighted in bold. Automatic evaluation Human evaluation Methods BLEU Consistency Novelty Dist-1 Dist-2 Con. Nov. E-div. Flu. TAV 6.05 16.59 70.32 2.69 14.25 2.32 2.19 2.58 2.76 TAT 6.32 9.19 68.77 2.25 12.17 1.76 2.07 2.32 2.93 MTA 7.09 25.73 70.68 2.24 11.70 3.14 2.87 2.17 3.25 CTEG 9.72 39.42 75.71 5.19 20.49 3.74 3.34 3.08 3.59 SCTKG(w/o-Senti) 9.97 43.84 78.32 5.73 23.16 3.89 3.35 3.90 3.71 SCTKG(Ran-Senti) 9.64 41.89 79.54 5.84 23.10 3.80 3.48 4.29 3.67 SCTKG(Gold-Senti) 11.02 42.57 78.87 5.92 23.07 3.81 3.37 3.94 3.75 

 which uses the concepts in ConceptNet but regardless of their correlation. All models uses golden sentiment la- Methods BLEU Con. Nov. E-div. Flu. Full model 11.02 3.81 3.37 3.94 3.75 w/o TGA 10.34 3.54 3.17 3.89 3.38 w/o AT 9.85 3.37 3.20 3.92 3.51 Table 2: Ablation study on text quality. "w/o AT" means without adversarial training. "w/o TGA" means withou TGA. Con., Nov., E-div., Flu. represent topic- consistency, novelty, essay-diversity, fluency, respec- tively. Full model represent SCTKG(Gold-Senti) in this table. 

 Table 4 : 4 Given topic "Law" and "Education", and randomly set sentiment label for each sentence. We generated an essay according to the topic and sentiment labels. "neu." represents neutral. "pos." represents positive and "neg." represents negative. We have translated the Chinese output into English. Full model 0.68 0.66 0.67 w/o Enc-senti 0.56 0.55 0.56 w/o Dec-senti 0.59 0.62 0.61 w/o TGA 0.62 0.64 0.63 Table 3: Ablation study on sentiment control. "w/o Enc-senti" means to remove the sentiment embed- ding in the encoder side and "w/o Dec-senti" means to remove from the decoder. Full model represents SCTKG(Ran-Senti) in this table. Input topics: Law Education Sentiment label: neu. pos. neg. neg. neu. Output essay: I am a senior high school student. I am in the best high school in our town. But bullying still exist on our campus. Teachers always ignore this phenomenon. What should we do to protect our rights? 

			 https://conceptnet.io 

			 As shown in Figure2, in the topic knowledge graph, red circles denote the topic words and blue circles denote their neighboring concepts. Since we have already encoded topic information in the encoder, the graph vector gt in this section mainly focuses on the neighboring concept to assist the generation. 

			 The dataset can be download by https://pan. baidu.com/s/17pcfWUuQTbcbniT0tBdwFQ 4 https://github.com/baidu/Senta 

			 https://github.com/tensorflow/ tensorflow 6 https://github.com/libing125/CTEG
