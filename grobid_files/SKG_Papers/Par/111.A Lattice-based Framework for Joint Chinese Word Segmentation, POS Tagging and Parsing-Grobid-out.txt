title
A Lattice-based Framework for Joint Chinese Word Segmentation, POS Tagging and Parsing

abstract
For the cascaded task of Chinese word segmentation, POS tagging and parsing, the pipeline approach suffers from error propagation while the joint learning approach suffers from inefficient decoding due to the large combined search space. In this paper, we present a novel lattice-based framework in which a Chinese sentence is first segmented into a word lattice, and then a lattice-based POS tagger and a lattice-based parser are used to process the lattice from two different viewpoints: sequential POS tagging and hierarchical tree building. A strategy is designed to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results on Chinese Treebank show that our lattice-based framework significantly improves the accuracy of the three sub-tasks.

Introduction Previous work on syntactic parsing generally assumes a processing pipeline where an input sentence is first tokenized, POS-tagged and then parsed  (Collins, 1999; Charniak, 2000; Petrov and Klein, 2007) . This approach works well for languages like English where automatic tokenization and POS tagging can be performed with high accuracy without the guidance of the highlevel syntactic structure. Such an approach, however, is not optimal for languages like Chinese where there are no natural delimiters for word boundaries, and word segmentation (or tokenization) is a non-trivial research problem by itself. Errors in word segmentation would propagate to later processing stages such as POS tagging and syntactic parsing. More importantly, Chinese is a language that lacks the morphological clues that help determine the POS tag of a word. For example, ? ( " investigate/investigation") can either be a verb ("investigate") or a noun ("investigation"), and there is no morphological variation between its verbal form and nominal form. This contributes to the relatively low accuracy (95% or below) in Chinese POS tagging when evaluated as a stand-alone task  (Sun and Uszkoreit, 2012) , and the noun/verb ambiguity is a major source of error. More recently, joint inference approaches have been proposed to address the shortcomings of the pipeline approach.  Qian and Liu (2012)  proposed a joint inference approach where syntactic parsing can provide feedback to word segmentation and POS tagging and showed that the joint inference approach leads to improvements in all three sub-tasks. However, a major challenge for joint inference approach is that the large combined search space makes efficient decoding and parameter estimation very hard. In this paper, we present a novel lattice-based framework for Chinese. An input Chinese sentence is first segmented into a word lattice, which is a compact representation of a small set of high-quality word segmentations. Then, a lattice-based POS tagger and a lattice-based parser are used to process the word lattice from two different viewpoints. We next employ the dual decomposition method to exploit the complementary strengths of the tagger and parser, and encourage them to predict agreed structures. Experimental results show that our lattice-based framework significantly improves the accuracies of the three sub-tasks 

 The Lattice-based Framework Figure  1  gives the organization of the framework. There are four types of linguistic structures: a Chinese sentence, the word lattice, tagged word sequence and parse tree of the Chinese sentence. An example for each structure is provided in Figure  2 . We can see that the terminals and preterminals of a parse tree constitute a tagged word sequence. Therefore, we define a comparator between a tagged word sequence and a parse tree: if they contain the same word sequence and POS tags, they are equal, otherwise unequal. Figure  1  also shows the workflow of the framework. First, the Chinese sentence is segmented into a word lattice using the word segmentation system. Then the word lattice is fed into the lattice-based POS tagger to produce a tagged word sequence and into the latticebased parser to separately produce a parse tree . We then compare with to see whether they are equal. If they are equal, we output as the final result. Otherwise, the guidance generator generates some guidance orders based on the difference between and , and guides the tagger and the parser to process the lattice again. This procedure may iterate many times until the tagger and parser predict equal structures. The motivation to design such a framework is as follows. First, state-of-the-art word segmentation systems can now perform with high accuracy. We can easily get an F1 score greater than 96%, and an oracle (upper bound) F1 score greater than 99% for the word lattice  (Jiang et al., 2008) . Therefore, a word lattice provides us a good enough search space to allow sufficient interaction among word segmentation, POS tagging and parsing systems. Second, both the lattice-based POS tagger and the lattice-based parser can select word segmentation from the word lattice and predict POS tags, but they do so from two different perspectives. The lattice-based POS tagger looks at a path in a word lattice as a sequence and performs sequence labeling based on linear local context, while the lattice-based parser builds the parse trees in a hierarchical manner. They have different strengths with regard to word segmentation and POS tagging. We hypothesize that exploring the complementary strengths of the tagger and parser would improve each of the sub-tasks. We build a character-based model  (Xue, 2003)  for the word segmentation system, and treat segmentation as a sequence labeling task, where each Chinese character is labeled with a tag. We use the tag set provided in  Wang et al. (2011)  and use the same feature templates. We use the Maximum Entropy (ME) model to estimate the feature weights. To get a word lattice, we first generate N-best word segmentation results, and then compact the N-best lists into a word lattice by collapsing all the identical words into one edge. We also assign a probability to each edge, which is calculated by multiplying the tagging probabilities of each character in the word. The goal of the lattice-based POS tagger is to predict a tagged word sequence for an input word lattice : =argmax ? ( ) ? ( ) where ( ) represents the set of all possible tagged word sequences derived from the word lattice . ( ) is used to map onto a global feature vector, and is the corresponding weight vector. We use the same non-local feature templates used in  Jiang et al. (2008)  and a similar decoding algorithm. We use the perceptron algorithm  (Collins, 2002)  for parameter estimation.  Goldberg and Elhadad (2011)  proposed a lattice-based parser for Heberw based on the PCFG-LA model  (Matsuzaki et al., 2005) . We adopted their approach, but found the unweighted word lattice their parser takes as input to be ineffective for our Chinese experiments. Instead, we use a weighted lattice as input and weigh each edge in the lattice with the word probability. In our model, each syntactic category is split into multiple subcategories [ ] by labeling a latent annotation . Then, a parse tree is refined into [ ], where X is the latent annotation vector for all non-terminals in . The probability of [ ] is calculated as: ( [ ]) = ( [ ] ? [ ] [ ]) ? ( [ ] ? ) ? ( ) where the three terms are products of all syntactic rule probabilities, lexical rule probabilities and word probabilities in [ ] respectively. 

 Combined Optimization Between The Lattice-based POS Tagger and The Lattice-based Parser We first define some variables to make it easier to compare a tagged word sequence with a parse tree . We define as the set of all POS tags. For , we define ( , , )=1 if contains a POS tag ? spanning from the i-th character to the j-th character, otherwise ( , , ) = 0. We also define ( , ,#) = 1 if contains the word spanning from the i-th character to the j-th character, otherwise ( , ,#) = 0. Similarly, for , we define ( , , ) =1 if contains a POS tag ? spanning from the i-th character to the j-th character, otherwise ( , , ) = 0. We also define ( , ,#) = 1 if contains the word spanning from the i-th character to the j-th character, otherwise ( , ,#) = 0. Therefore, and are equal, only if ( , , ) = ( , , ) for all ? [0, ] , ? [ +1, ] and ? ?#, otherwise unequal. Our framework expects the tagger and the parser to predict equal structures and we formulate it as a constraint optimization problem: The dual decomposition (a special case of Lagrangian relaxation) method introduced in Komodakis et al. (  2007 ) is suitable for this problem. Using this method, we solve the primal constraint optimization problem by optimizing the dual problem. First, we introduce a vector of Lagrange multipliers ( , , ) for each equality constraint. Then, the Lagrangian is formulated as: ( , , ) = ( ) + ( ) + ( , , )( ( , , )? ( , , )) , , By grouping the terms that depend on and , we rewrite the Lagrangian as ( , , ) = ( ) + ( , , ) ( , , ) , , + ( ) ? ( , , ) ( , , ) , , Then, the dual objective is ( ) =max , ( , , ) =max ( ) + ( , , ) ( , , ) , , + max ( ) ? ( , , ) ( , , ) , , The dual problem is to find min ( ). We use the subgradient method  (Boyd et al., 2003)  to minimize the dual. Following  Rush et al. (2010) , we define the subgradient of ( ) as: ( , , ) = ( , , ) ? ( , , ) for all ( , , ) Then, adjust  ( , , )  as follows: ( , , ) = ( , , ) ? ( ( , , ) ? ( , , )) where >0 is a step size. Algorithm 1 presents the subgradient method to solve the dual problem. The algorithm initializes the Lagrange multiplier values with 0 (line 1) and then iterates many times. In each iteration, the algorithm finds the best ( ) and ( ) by running the lattice-based POS tagger (line 3) and the lattice-based parser (line 4). If ( ) and ( ) share the same tagged word sequence (line 5), then the algorithm returns the solution (line 6). Otherwise, the algorithm adjusts the Lagrange multiplier values based on the differences between ( ) and ( ) (line 8). A crucial point is that the argmax problems in line 3 and line 4 can be solved efficiently using the original decoding algorithms, because the Lagrange multiplier can be regarded as adjustments for lexical rule probabilities and word probabilities. 

 Experiments We conduct experiments on the Chinese Treebank Version 5.0 and use the standard data split Algorithm 1: Combined Optimization 1: Set ( ) ( , , )=0, for all ( , , ) 2: For k=1 to K 3: ( ) ? argmax ( ) + ? ( ) ( , , ) ( , , ) , , 4: ( ) ? argmax ( ) ? ? ( ) ( , , ) ( , , ) , , 5: If ( ) ( , , ) = ( ) ( , , ) for all ( , , ) 6: Return ( ( ) , ( ) ) 7: Else 8: ( ) ( , , ) = ( ) ( , , ) ? ( ( ) ( , , ) ? ( ) ( , , ))  (Petrov and Klein, 2007) . The traditional evaluation metrics for POS tagging and parsing are not suitable for the joint task. Following with  Qian and Liu (2012) , we redefine precision and recall by computing the span of a constituent based on character offsets rather than word offsets. 

 Performance of the Basic Sub-systems We train the word segmentation system with 100 iterations of the Maximum Entropy model using the OpenNLP toolkit. Table  1  shows the performance. It shows that our word segmentation system is comparable with the state-of-the-art systems and the upper bound F1 score of the word lattice exceeds 99.6%. This indicates that our word segmentation system can provide a good search space for the lattice-based POS tagger and the lattice-based parser. To train the lattice-based POS tagger, we generate the word lattice for each sentence in the training set using cross validation approach. We divide the entire training set into 18 folds on average (each fold contains 1,000 sentences). For each fold, we segment each sentence in the fold into a word lattice by compacting 20-best segmentation list produced with a model trained on the other 17 folds. Then, we train the latticebased POS tagger with 20 iterations of the average perceptron algorithm. Table  2  presents the joint word segmentation and POS tagging performance and shows that our lattice-based POS tagger obtains results that are comparable with state-of-the-art systems. We implement the lattice-based parser by modifying the Berkeley Parser, and train it with 5 iterations of the split-merge-smooth strategy  (Petrov et al., 2006) . Table  3  shows the performance, where the "Pipeline Parser" represents the system taking one-best segmentation result from our word segmentation system as input and "Lattice-based Parser" represents the system taking the compacted word lattice as input. We find the lattice-based parser gets better performance than the pipeline system among all three subtasks. 

 Performance of the Framework For the lattice-based framework, we set the maximum iteration in Algorithm 1 as K = 20. The step size is tuned on the development set and empirically set to be 0.8. Table  4  shows the parsing performance on the test set. It shows that the lattice-based framework achieves improvement over the lattice-based parser alone among all three sub-tasks: 0.16 points for word segmentation, 1.19 points for POS tagging and 1.65 points for parsing. It also outperforms the lattice-based POS tagger by 0.65 points on POS tagging accuracy. Our lattice-based framework also improves over the best joint inference parsing system  (Qian and Liu, 2012 ) by 0.57 points. 

 Conclusion In this paper, we present a novel lattice-based framework for the cascaded task of Chinese word segmentation, POS tagging and parsing. We first segment a Chinese sentence into a word lattice, then process the lattice using a latticebased POS tagger and a lattice-based parser. We also design a strategy to exploit the complementary strengths of the tagger and the parser and encourage them to predict agreed structures. Experimental results show that the lattice-based framework significantly improves the accuracies of the three tasks. The parsing accuracy of the framework also outperforms the best joint parsing system reported in the literature.  ?Figure 1 : 1 Figure 2: Linguistic structure examples. 
