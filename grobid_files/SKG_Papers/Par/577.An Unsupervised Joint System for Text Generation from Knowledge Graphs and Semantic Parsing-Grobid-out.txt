title
An Unsupervised Joint System for Text Generation from Knowledge Graphs and Semantic Parsing

abstract
Knowledge graphs (KGs) can vary greatly from one domain to another. Therefore supervised approaches to both graph-to-text generation and text-to-graph knowledge extraction (semantic parsing) will always suffer from a shortage of domain-specific parallel graphtext data; at the same time, adapting a model trained on a different domain is often impossible due to little or no overlap in entities and relations. This situation calls for an approach that (1) does not need large amounts of annotated data and thus (2) does not need to rely on domain adaptation techniques to work well in different domains. To this end, we present the first approach to unsupervised text generation from KGs and show simultaneously how it can be used for unsupervised semantic parsing. We evaluate our approach on WebNLG v2.1 and a new benchmark leveraging scene graphs from Visual Genome. Our system outperforms strong baselines for both text?graph conversion tasks without any manual adaptation from one dataset to the other. In additional experiments, we investigate the impact of using different unsupervised objectives. 1

Introduction Knowledge graphs (KGs) are a general-purpose approach for storing information in a structured, machine-accessible way  (Van Harmelen et al., 2008) . They are used in various fields and domains to model knowledge about topics as different as lexical semantics  (Fellbaum, 2005; van Assem et al., 2006) , common sense  (Speer et al., 2017; Sap et al., 2019) , biomedical research  (Wishart et al., 2018)  and visual relations in images . This ubiquity of KGs necessitates interpretability because diverse users -both experts and nonexperts -work with them. Even though, in prin-ciple, a KG is human-interpretable, non-experts may have difficulty making sense of it. Thus, there is a need for methods, such as automatic natural language generation ("graph?text"), that support them. Semantic parsing, i.e., the conversion of a text to a formal meaning representation, such as a KG, ("text?graph") is equally important because it makes information that only exists in text form accessible to machines, thus assisting knowledge base engineers in KG creation and completion. As KGs are so flexible in expressing various kinds of knowledge, separately created KGs vary a lot. This unavoidably leads to a shortage of training data for both graph?text tasks. We therefore propose an unsupervised model that (1) easily adapts to new KG domains and (2) only requires unlabeled (i.e., non-parallel) texts and graphs from the target domain, together with a few fact extraction heuristics, but no manual annotation. To show the effectiveness of our approach, we conduct experiments on the latest release (v2.1) of the WebNLG corpus  (Shimorina and Gardent, 2018)  and on a new benchmark we derive from Visual Genome . While both of these datasets contain enough annotations to train supervised models, we evaluate our unsupervised approach by ignoring these annotations. The datasets are particularly well-suited for our evaluation as both graphs and texts are completely humangenerated. Thus for both our tasks, models are evaluated with natural, i.e., human-generated targets. Concretely, we make the following contributions: (1) We present the first unsupervised non-template approach to text generation from KGs (graph?text). (  2 ) We jointly develop a new unsupervised approach to semantic parsing that automatically adjusts to a target KG schema (text?graph). (3) In contrast to prior unsupervised graph?text and text?graph work, our model does not re-quire manual adaptation to new domains or graph schemas. (4) We provide a thorough analysis of the impact of different unsupervised objectives, especially the ones we newly introduce for text?graph conversion. (5) We create a new large-scale dataset for text?graph transformation tasks in the visual domain. 2 Related Work graph ? text. Our work is the first attempt at fully unsupervised text generation from KGs. In this respect it is only comparable to traditional rule-or template-based approaches  (Kukich, 1983; McRoy et al., 2000) . However, in contrast to these approaches, which need to be manually adapted to new domains and KG schemas, our method is generally applicable to all kinds of data without modification. There is a large body of literature about supervised text generation from structured data, notably about the creation of sports game summaries from statistical records  (Robin, 1995; Tanaka-Ishii et al., 1998) . Recent efforts make use of neural encoderdecoder mechanisms  (Wiseman et al., 2017; Puduppully et al., 2019) . Although text creation from relational databases is related and our unsupervised method is, in principle, also applicable to this domain, in our work we specifically address text creation from graph-like structures such as KGs. One recent work on supervised text creation from KGs is  (Bhowmik and de Melo, 2018) . They generate a short description of an entity, i.e., a single KG node, based on a set of facts about the entity. We, however, generate a description of the whole KG, which involves multiple entities and their relations.  Koncel-Kedziorski et al. (2019)  generate texts from whole KGs. They, however, do not evaluate on human-generated KGs but automatically generated ones from the scientific information extraction tool SciIE  (Luan et al., 2018) . Their supervised model is based on message passing through the topology of the incidence graph of the KG input. Such graph neural networks  (Kipf and Welling, 2017; Veli?kovi? et al., 2018)  have been widely adopted in supervised graph-to-text tasks  (Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019 Ribeiro et al., , 2020 . Even though  Marcheggiani and Perez-Beltrachini (2018)  report that graph neural networks can make better use of graph input than RNNs for supervised learning, for our unsuper-vised approach we follow the line of research that uses RNN-based sequence-to-sequence models  Sutskever et al., 2014)  operating on serialized triple sets  (Gardent et al., 2017b; Trisedya et al., 2018; Gehrmann et al., 2018; Castro Ferreira et al., 2019; Fan et al., 2019) . We make this choice because learning a common semantic space for both texts and graphs by means of a shared encoder and decoder is a central component of our model. It is a nontrivial, separate research question whether and how encoder-decoder parameters can effectively be shared for models working on both sequential and non-sequential data. We thus leave the adaptation of our approach to graph neural networks for future work. text ? graph. Converting a text into a KG representation, our method is an alternative to prior work on open information extraction  (Niklaus et al., 2018)  with the advantage that the extractions, though trained without labeled data, automatically adjust to the KGs used for training. It is therefore also related to relation extraction in the unsupervised  (Yao et al., 2011; Marcheggiani and Titov, 2016; Simon et al., 2019)  and distantly supervised setting  (Riedel et al., 2010; Parikh et al., 2015) . However, these systems merely predict a single relation between two given entities in a single sentence, while we translate a whole text into a KG with potentially multiple facts. Our text?graph task is therefore most closely related to semantic parsing  (Kamath and Das, 2019 ), but we convert statements into KG facts whereas semantic parsing typically converts a question into a KG or database query.  Poon and Domingos (2009)  proposed the first unsupervised approach. They, however, still need an additional KG alignment step, i.e., are not able to directly adjust to the target KG. Other approaches overcome this limitation but only in exchange for the inflexibility of manually created domain-specific lexicons  (Popescu et al., 2004; Goldwasser et al., 2011) . Poon (2013)'s approach is more flexible but still relies on preprocessing by a dependency parser, which generally means that language-specific annotations to train such a parser are needed. Our approach is endto-end, i.e., does not need any language-specific preprocessing during inference and only depends on a POS tagger used in the rule-based text?graph system to bootstrap training. Unsupervised sequence generation. Our unsu-pervised training regime for both text?graph tasks is inspired by  (Lample et al., 2018b) . They used self-supervised pretraining and backtranslation for unsupervised translation from one language to another. We adapt these principles and their noise model to our tasks, and introduce two new noise functions specific to text?graph conversion. 

 Preliminaries 

 Data structure We formalize a KG as a labeled directed multigraph (V, E, s, t, l) where entities are nodes V and edges E represent relations between entities. The lookup functions s, t : E ? V assign to each edge its source and target node. The labeling function l assigns labels to nodes and edges where node labels are entity names and edge labels come from a predefined set R of relation types. An equivalent representation of a KG is the set of its facts. A fact is a triple consisting of an edge's source node (the subject), the edge itself (the predicate), and its target node (the object). So the set of facts F of a KG can be obtained from its edges: F := { (s(e), e, t(e)) | e ? E } . Applying l to all triple elements and writing out F in an arbitrary order generates a serialization that makes the KG accessible to sequence models otherwise used only for text. This has the advantage that we can train a sequence encoder to embed text and KGs in the same semantic space. Specifically, we serialize a KG by writing out its facts separated with end-of-fact symbols (EOF) and elements of each fact with special SEP symbols. We thus define our task as a sequence-to-sequence (seq2seq) task. 

 Scene Graphs The Visual Genome (VG) repository is a large collection of images with associated manually annotated scene graphs; see Fig.  1 . A scene graph formally describes image objects with their attributes, e.g., (hydrant, attr, yellow), and their relations to other image objects, e.g.,  (woman, in, shorts) . Each scene graph is organized into smaller subgraphs, known as region graphs, representing a subpart of a more complex larger picture that is interesting on its own. Each region graph is associated with an English text, the region description. Texts and graphs were not automatically produced from each other, but were collected from crowdworkers who were presented an image region and then generated text and graph. So although the graphs were not specifically designed to closely resemble the texts, they describe the same image region. This semantic correspondence makes scene graph?text conversion an interesting and challenging problem because text and graph are not simple translations of each other. Scene graphs are formalized in the same way as other KGs: V here contains image objects and their attributes, and R contains all types of visual relationships and the special label attr for edges between attribute and non-attribute nodes. Fig.  2  shows an example. VG scene graphs have been used before for traditional KG tasks, such as KG completion  (Wan et al., 2018) , but we are the first to use them for a text?graph conversion dataset. 

 Approaches 

 Rule-based systems We propose a rule-based system as unsupervised baseline for each of the text?graph tasks. Note that they both assume that the texts are in English. R graph?text . From a KG serialization, we remove SEP symbols and replace EOF symbols by the word and. The special label attr is mapped to is. This corresponds to a template-based enumeration of all KG facts. See Table  5  for an example. R text?graph . After preprocessing a text with NLTK's default POS tagger  (Loper and Bird, 2004 ) and removing stop words, we apply two simple heuristics to extract facts: (1) Each verb becomes a predicate; is creates facts with predicate attr. The content words directly before and after such a predicate word become subject and object. (2) Adjectives a form attributes, i.e., build facts of the form (X, attr, a) where X is filled with the first noun after a. These heuristics are similar in nature to a rudimentary parser. See Table  8  for an example. 

 Neural seq2seq systems Our main system is a neural seq2seq architecture. We equip the standard encoder-decoder model with attention  and copy mechanism  (Gu et al., 2016) . Allowing the model to directly copy from the source to the target side is beneficial in data to text generation  (Wiseman et al., 2017; Puduppully et al., 2019) . The encoder (resp. decoder) is a bidirectional (resp. unidirectional) LSTM  (Hochreiter and Schmidhuber, 1997) . Dropout  (Hinton et al., 2012 ) is applied at the input of both encoder and decoder  (Britz et al., 2017) . We combine this model with the following concepts: Multi-task model. In unsupervised machine translation, systems are trained for both translation directions  (Lample et al., 2018b) . In the same way, we train our system for both conversion tasks text?graph, sharing encoder and decoder. To tell the decoder which type of output should be produced (text or graph), we initialize the cell state of the decoder with an embedding of the desired output type. The hidden state of the decoder is initialized with the last state of the encoder as usual. Noisy source samples.  Lample et al. (2018a)  introduced denoising auto-encoding as pretraining and auxiliary task to train the decoder to produce well-formed output and make the encoder robust to noisy input. The training examples for this task consist of a noisy version of a sentence as source and the original sentence as target. We adapt this idea and propose the following noise functions for the domains of graphs and texts: swap, drop, blank, repeat, rule. Table  1  describes their behavior. swap, drop and blank are adapted from  (Lample et al., 2018a)  with facts in graphs taking the role of words in text. As order should be irrelevant in a set of facts, we drop the locality constraint in the swap permutation for graphs by setting k = +?. Denoising samples generated by repeat requires the model to learn to remove redundant information in a set of facts. In the case of text, repeat mimics a behavior often observed with insufficiently trained neural models, i.e., repeating words considered important. Unlike the other noise functions, rule does not "perturb" its input, but rather noisily backtranslates it. We will see in Section 7 that bootstrapping with these noisy translations is essential. We consider two fundamentally different noise injection regimes: (1) The composed noise setting is an adaptation of  Lample et al. (2018a) 's noise model (blank?drop?swap) where our newly introduced noise functions rule and repeat are added to the start and end of the pipeline, i.e., all data samples are treated equally with the same noise function C comp := repeat?blank?drop?swap?rule. Figure  3  shows an example. (2) In the sampled noise setting, we do not use all noise functions at once but sample a single one per data instance. 

 Training regimes We denote the sets of graphs and corresponding texts by G and T . The set of available supervised examples (x, y) ? G ? T is called S ? G ? T . P g and P t are probabilistic models that generate, conditioned on any input, a graph (g) or a text (t). Unsupervised training. We first obtain a language model for both graphs and text by training one epoch with the denoising auto-encoder objective: L denoise = E x?G [? log P g (x|C(x))] + E y?T [? log P t (y|C(y))] where C ? C comp for composed noise and C ? {swap, blank, drop, repeat, rule} for sampled noise. In this pretraining epoch only, we use all possible noise functions individually on all available data. As sampled noise incorporates five different noise functions and composed noise only one, this results in five times more pretraining samples for sampled noise than for composed noise. In subsequent epochs, we additionally consider L back as training signal: L back = E x?G [? log P g (x|z * (x))] + E y?T [? log P t (y|w * (y))] z * (x) = arg max z P t (z|x) w * (y) = arg max w P g (w|y) This means that, in each iteration, we apply the current model to backtranslate a text (graph) to obtain a potentially imperfect graph (text) that we can use as noisy source with the clean original input being the target. This gives us a pseudo-parallel training instance for the next iteration -recall that we address unsupervised generation, i.e., without access to parallel data. The total loss in these epochs is L back + L denoise , where now L denoise only samples one possible type of noise independently for each data instance. Supervised training. Our intended application is an unsupervised scenario. For our two datasets, however, we have labeled data (i.e., a "parallel corpus") and so can also compare our model to its supervised variant. Although supervised performance is generally better, it serves as a reference point and gives us an idea of the impact of supervision as opposed to factors like model architecture and hyperparameters. The supervised loss is simply defined as follows: L sup = E (x,y)?S [? log P t (y|x) ? log P g (x|y)] 5 Experiments 

 Data For our experiments, we randomly split the VG images 80/10/10 into train/val/test. We then remove all graphs from train that also occur in one of the images in val or test. Finally, we unify graph serialization duplicates with different texts to single instances with multiple references for graph?text and proceed analogously with text duplicates for text?graph. For WebNLG v2.1, we use the data splits as provided. Following  (Gardent et al., 2017a) , we resolve the camel case of relation names and remove underscores from entity names in a preprocessing step. For both datasets, the order of facts in graph serializations corresponds to the order of triples in the original dataset. Because of VG's enormous size and limited computation power, we additionally create a closed-domain ball   (Lin et al., 2018) ; the CHRF++ script is from  (Popovi?, 2017b) . sports subset of VG, called VG ball , which we can use to quickly conduct additional experiments (see Section 7). We identify all images where at least one region graph contains at least one fact that mentions an object ending with ball and take all regions from them (keeping data splits the same). In contrast to alternatives like random subsampling, we consider this domain-focused construction more realistic. Table  2  shows relevant statistics for all datasets. While VG and WebNLG have similar statistics, VG is around 70 times larger than WebNLG, which makes it an interesting benchmark for future research, both supervised and unsupervised. Apart from size, there are two important differences: (1) The VG graph schema has been freely defined by crowd workers and thus features a large variety of different relations. (2) The percentage of graph tokens occurring in the text, a measure important for the text?graph task, is lower for VG than for WebNLG. Thus, VG graphs contain more details than their corresponding texts, which is a characteristic feature of the domain of image captions: they mainly describe the salient image parts. 

 Training details We train all models with the Adam optimizer (Kingma and Ba, 2015) for maximally 30 epochs. We stop supervised models early when L sup does not decrease on val for 10 epochs. Unsupervised models are stopped after 5 iterations on VG because of its big size and limited computational resources. All hyperparameters and more details are described in Appendices A and B. Our implementation is based on AllenNLP  (Gardner et al., 2017) . In unsupervised training, input graphs and texts are the same as in supervised training -only the gold target sides are ignored. While it is an artificial setup to split paired data and treat them as V 100 is a 100-size random sample from val. All results are computed with scripts from  (Lin et al., 2018) . unpaired, this not only makes the supervised and unsupervised settings more directly comparable, but also ensures that the text data resemble the evaluation texts in style and domain. For the purpose of experiments on a benchmark, this seems appropriate to us. For a concrete use case, it would be an important first step to find adequate texts that showcase the desired language style and that are about a similar topic as the KGs that are to be textualized. As KGs are rarely the only means of storing information, e.g., in an industrial context, such texts should not be hard to come by in practice.   4  shows how performance of our unsupervised model changes at every backtranslation iteration, measured in BLEU  (Papineni et al., 2002) , a common metric for natural language generation. For model selection, we adopt the two methods proposed by  Lample et al. (2018b) , i.e., a small validation set (we take a 100-size random subset of val, called V 100 ) and a fully unsupervised criterion (U) where BLEU compares an unlabeled sample with its back-and-forth translation. We confirm their finding that U is not reliable for neural  text generation models whereas V 100 correlates better with performance on the larger test sets. We use V 100 for model selection in the rest of this paper. Quantitative evaluation. Table  3  shows BLEU, METEOR  (Banerjee and Lavie, 2005)  and CHRF++  (Popovi?, 2017a)  for our unsupervised models and the rule baseline R graph?text , which is in many cases, i.e., if parallel graph-text data are scarce, the only alternative. First, we observe that R graph?text performs much better on WebNLG than VG, indicating that our new benchmark poses a tougher challenge. Second, our unsupervised models consistently outperform this baseline on all metrics and on both datasets, showing that our method produces textual descriptions much closer to human-generated ones. Third, noise composition, the general default in unsupervised machine translation, does not always perform better than noise sampling. Thus, it is worthwhile to try different noise settings for new tasks or datasets. 

 Results and Discussion Surprisingly, supervised and unsupervised models perform nearly on par. Real supervision does not seem to give much better guidance in training than our unsupervised regime, as measured by our three metrics on two different datasets. Some metric-dataset combinations even favor one of the unsupervised models. Our qualitative observations provide a possible explanation for that. Qualitative observations. Taking a look at example generations (Table  5 ), we also see qualitatively how much easier it is to grasp the content of our natural language summarization than reading through a simple enumeration of KG facts. We find that the unsupervised model (c) seems to output the KG information in a more complete manner than its supervised counterpart (d). The supervision probably introduces a bias present in the training data that image captions focus on salient image parts and therefore the supervised model is encouraged to omit information. As it never sees a corresponding text-graph pair together, the unsupervised model cannot draw such a conclusion. 

 Graph extraction from texts We evaluate semantic parsing (text?graph) performance by computing the micro-averaged F1 score of extracted facts. If there are multiple reference graphs (cf. Section 5.1), an extracted fact is considered correct if it occurs in at least one reference graph. For the ground truth number of facts to be extracted from a given text, we take the maximum number of facts of all its reference graphs. Model selection. Table  6  shows that (compared to text generation quality) U is more reliable for text?graph performance. For sampled noise, it correctly identifies the best iteration, whereas for composed noise it chooses second best. In both noise settings, V 100 perfectly chooses the best model. Quantitative observations. Table  7  shows a comparison of our unsupervised models with two rule-based systems, our R text?graph and the highly domain-specific Stanford Scene Graph Parser (SSGP) by  Schuster et al. (2015) . We choose these two baselines to adequately represent the state of the art in the unsupervised setting. Recall from Section 2 that the only previous unsupervised works either cannot adapt to a target graph schema (open information extraction), which means their precision and recall of retrieved facts is always 0, or have been created for SQL query generation from natural language questions  (Poon, 2013) , a related task that is yet so different that an adaptation to triple set generation from natural language statements is nontrivial. While rule-based systems do not automatically adapt to new graph schemas either, R text?graph and SSGP were at least designed with the scene graph domain in mind. Although SSGP was not optimized to match the scene graphs from VG, its rules were still engineered to cover typical idiosyncrasies of textual image descriptions and corresponding scene graphs. Besides, we evaluate it with lemmatized reference graphs because it only predicts lemmata as predicates. All this gives it a major advantage over the other presented systems but it is nonetheless outperformed by our best unsupervised model -even on VG. This shows that our automatic method can beat even hand-crafted domain-specific rules. Both R text?graph and SSGP fail to predict any fact from WebNLG. The DBpedia facts from WebNLG often contain multi-token entities while R text?graph only picks single tokens from the text. Likewise, SSGP models multi-token entities as two nodes with an attr relation. This illustrates the importance of automatic adaptation to the target KG. Although our system uses R text?graph during unsupervised training and is similarly not adapted to the WebNLG dataset, it performs significantly better. Supervision helps more on WebNLG than on VG. The poor performance of R text?graph on WebNLG is probably a handicap for unsupervised learning. Qualitative observations. Table  8  shows example facts extracted by different systems. R text?graph and SSGP are both fooled by the proximity of the noun pants and the verb play whereas our model correctly identifies man as the subject. It, however, fails to identify shirt as an entity and associates the two attributes colorful and white to pants. Only the supervised model produces perfect output. 

 Noise and translation completeness Sampled noise only creates training pairs that either are complete rule-based translations or reconstruction pairs from a noisy graph to a complete graph or a noisy text to a complete text. In contrast, composed noise can introduce translations from a noisy text to a complete graph or vice versa and thus encourage a system to omit input information (cf. Fig.  3 ). This difference is mirrored nicely in the results of our unsupervised systems for both tasks: composed noise performs better on VG where omit-ted information in an image caption is common and sampled noise works better on WebNLG where the texts describe their graphs completely. 

 Noise Ablation Study Our unsupervised objectives are defined by different types of noise models. Hence, we examine their impact in a noise ablation study. Table  9  shows results for text?graph and graph?text on the validation splits of VG ball and WebNLG. For both datasets and tasks, introducing variation via noise functions is crucial for the success of unsupervised learning. The model without noise (i.e., C(x) = x) fails completely as do all models lacking rule as type of noise, the only exception being the only-drop system on WebNLG. Even though drop seems to work equally well in this one case, the simple translations delivered by our rulebased systems clearly provide the most useful information for the unsupervised models -notably in combination with the other noise functions: removing rule and keeping all other types of noise (cf. "sample all but rule" and "comp. all but rule") performs much worse than leaving out drop. We hypothesize that our two rule systems provide two important pieces of information: (1) R graph?text helps distinguish data format tokens from text tokens and (2) R text?graph helps find probable candidate words in a text that form facts for the data output. As opposed to machine translation, where usually every word in a sentence is translated into a fluent sentence in the target language, identifying words that probably form a fact is more important in data-to/from-text generation. We moreover observe that our unsupervised models always improve on the rule-based systems even when rule is the only type of noise: graph?text BLEU increases from 6.2/18.3 to 19.5/37.4 on VG ball /WebNLG and text?graph F1 from 14.4/0.0 to 18.5/31.0. Finally, our ablation study makes clear that there is no best noise model for all datasets and tasks. We therefore recommend experimenting with both different sets of noise functions and noise injection regimes (sampled vs. composed) for new data. 

 Conclusion We presented the first fully unsupervised approach to text generation from KGs and a novel approach to unsupervised semantic parsing that automatically adapts to a target KG. We showed the effectiveness of our approach on two datasets, WebNLG v2.1 and a new text?graph benchmark in the visual domain, derived from Visual Genome. We quantitatively and qualitatively analyzed our method on text?graph conversion. We explored the impact of different unsupervised objectives in an ablation study and found that our newly introduced unsupervised objective using rule-based translations is essential for the success of unsupervised learning. Table  10 : BLEU scores on WebNLG for our unsupervised models evaluated for graph?text at different iterations. U is calculated on all unlabeled data used for training. V 100 is a 100-size random sample from val. All results are computed with scripts from  (Lin et al., 2018) . Table 11: F1 scores on WebNLG for our unsupervised models evaluated for text?graph at different iterations. U is calculated on all unlabeled data used for training. V 100 is a 100-size random sample from val. Figure 1 :Figure 2 : 12 Figure1: Region graphs and textual region descriptions in Visual Genome (VG). Image regions serve as common reference for text and graph creation but are disregarded in our work. We solely focus on the pairs of corresponding texts and graphs. Illustration adapted from. 
