title
UDapter: Language Adaptation for Truly Universal Dependency Parsing

abstract
Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality. However, crosslanguage interference and restrained model capacity remain major obstacles. To address this, we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules. This approach enables to learn adapters via language embeddings while sharing model parameters across languages. It also allows for an easy but effective integration of existing linguistic typology features into the parsing network. The resulting parser, UDapter, outperforms strong monolingual and multilingual baselines on the majority of both high-resource and lowresource (zero-shot) languages, showing the success of the proposed adaptation approach. Our in-depth analyses show that soft parameter sharing via typological features is key to this success. 1

Introduction Monolingual training of a dependency parser has been successful when relatively large treebanks are available  (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017) . However, for many languages, treebanks are either too small or unavailable. Therefore, multilingual models leveraging Universal Dependency annotations  (Nivre et al., 2018)  have drawn serious attention  (Zhang and Barzilay, 2015; Ammar et al., 2016; Kondratyuk and Straka, 2019) . Multilingual approaches learn generalizations across languages and share information between them, making it possible to parse a target language without supervision in that language. Moreover, multilingual models can be faster to train and easier to maintain than a large set of monolingual models. However, scaling a multilingual model over a high number of languages can lead to sub-optimal results, especially if the training languages are typologically diverse. Often, multilingual neural models have been found to outperform their monolingual counterparts on low-and zero-resource languages due to positive transfer effects, but underperform for high-resource languages  (Johnson et al., 2017; Arivazhagan et al., 2019; Conneau et al., 2020) , a problem also known as "the curse of multilinguality". Generally speaking, a multilingual model without language-specific supervision is likely to suffer from over-generalization and perform poorly on high-resource languages due to limited capacity compared to the monolingual baselines, as verified by our experiments on parsing. In this paper, we strike a good balance between maximum sharing and language-specific capacity in multilingual dependency parsing. Inspired by recently introduced parameter sharing techniques  (Platanios et al., 2018; Houlsby et al., 2019) , we propose a new multilingual parser, UDapter, that learns to modify its language-specific parameters including the adapter modules, as a function of language embeddings. This allows the model to share parameters across languages, ensuring generalization and transfer ability, but also enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database  which supports 3718 languages including all languages represented in UD. While the importance of typological features for cross-lingual parsing is known for both non-neural  (Naseem et al., 2012; Zhang and Barzilay, 2015)  and neural approaches  (Ammar et al., 2016; Scholivet et al., 2019) , we are the first to use them effectively as direct input to a neural parser, without manual selection, over a large number of languages in the context of zero-shot parsing where gold POS labels are not given at test time. In our model, typological features are crucial, leading to a substantial LAS increase on zero-shot languages and no loss on high-resource languages when compared to the language embeddings learned from scratch. We train and test our model on the 13 syntactically diverse high-resource languages that were used by  Kulmizev et al. (2019) , and also evaluate it on 30 genuinely low-resource languages. Results show that UDapter significantly outperforms stateof-the-art monolingual  (Straka, 2018)  and multilingual  (Kondratyuk and Straka, 2019)  parsers on most high-resource languages and achieves overall promising improvements on zero-shot languages. Contributions We conduct several experiments on a large set of languages and perform thorough analyses of our model. Accordingly, we make the following contributions: 1) We apply the idea of adapter tuning  (Rebuffi et al., 2018; Houlsby et al., 2019)  to the task of universal dependency parsing. 2) We combine adapters with the idea of contextual parameter generation  (Platanios et al., 2018) , leading to a novel language adaptation approach with state-of-the art UD parsing results. 3) We provide a simple but effective method for conditioning the language adaptation on existing typological language features, which we show is crucial for zero-shot performance. 

 Previous Work This section presents the background of our approach. Multilingual Neural Networks Early models in multilingual neural machine translation (NMT) designed dedicated architectures  (Dong et al., 2015; Firat et al., 2016)  whilst subsequent models, from  Johnson et al. (2017)  onward, added a simple language identifier to the models with the same architecture as their monolingual counterparts. More recently, multilingual NMT models have focused on maximizing transfer accuracy for low-resource language pairs, while preserving high-resource language accuracy  (Platanios et al., 2018; Neubig and Hu, 2018; Aharoni et al., 2019; Arivazhagan et al., 2019) , known as the (positive) transfer -(negative) interference trade-off. Another line of work builds massively multilingual pre-trained language mod-els to produce contextual representation to be used in downstream tasks  (Devlin et al., 2019; Conneau et al., 2020) . As the leading model, multilingual BERT (mBERT) 2  (Devlin et al., 2019)  which is a deep self-attention network, was trained without language-specific signals on the 104 languages with the largest Wikipedias. It uses a shared vocabulary of 110K WordPieces  (Wu et al., 2016) , and has been shown to facilitate cross-lingual transfer in several applications  (Pires et al., 2019; Wu and Dredze, 2019) . Concurrently to our work,  Pfeiffer et al. (2020)  have proposed to combine language and task adapters, small bottleneck layers  (Rebuffi et al., 2018; Houlsby et al., 2019) , to address the capacity issue which limits multilingual pre-trained models for cross-lingual transfer. Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages  Nivre et al., 2018)  has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser  (Zeman and Resnik, 2008;  on one or more source languages by using either gold or predicted POS labels  (Tiedemann, 2015)  and applied it to target languages. Building on this, later work used additional features such as typological language properties  (Naseem et al., 2012) , syntactic embeddings  (Duong et al., 2015) , and cross-lingual word clusters  (T?ckstr?m et al., 2012) . Among lexicalized approaches, Vilares et al. (  2016 ) learns a bilingual parser on a corpora obtained by merging harmonized treebanks.  Ammar et al. (2016)  trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT  (Devlin et al., 2019) , zero-shot transfer in dependency parsing was investigated  (Wu and Dredze, 2019; Tran and Bisazza, 2019) . Finally  Kondratyuk and Straka (2019)  trained a multilingual parser on the concatenation of all available UD treebanks. Language Embeddings and Typology Conditioning a multilingual model on the input language is studied in NMT  (Ha et al., 2016; Johnson et al., 2017) , syntactic parsing  (Ammar et al., 2016)  and language modeling  ( ?stling and Tiedemann, 2017) . The goal is to embed language information in real-valued vectors in order to enrich internal representations with input language for multilingual models. In dependency parsing, several previous studies  (Naseem et al., 2012; Zhang and Barzilay, 2015; Ammar et al., 2016; Scholivet et al., 2019)  have suggested that typological features are useful for the selective sharing of transfer information. Results, however, are mixed and often limited to a handful of manually selected features  (Fisch et al., 2019; Ponti et al., 2019) . As the most similar work to ours,  Ammar et al. (2016)  uses typological features to learn language embeddings as part of training, by augmenting each input token and parsing action representation. Unfortunately though, this technique is found to underperform the simple use of randomly initialized language embeddings ('language IDs'). Authors also reported that language embeddings hurt the performance of the parser in zero-shot experiments  (Ammar et al., 2016, footnote 30) . Our work instead demonstrates that typological features can be very effective if used with the right adaptation strategy in both supervised and zero-shot settings. Finally,  Lin et al. (2019)  use typological features, along with properties of the training data, to choose optimal transfer languages for various tasks, including UD parsing, in a hard manner. By contrast, we focus on a soft parameter sharing approach to maximize generalizations within a single universal model. 

 Proposed Model In this section, we present our truly universal dependency parser, UDapter. UDapter consists of a biaffine attention layer stacked on top of the pretrained Transformer encoder (mBERT). This is similar to  (Wu and Dredze, 2019; Kondratyuk and Straka, 2019) , except that our mBERT layers are interleaved with special adapter layers inspired by  Houlsby et al. (2019) . While mBERT weights are frozen, biaffine attention and adapter layer weights are generated by a contextual parameter generator  (Platanios et al., 2018)  that takes a language embedding as input and is updated while training on the treebanks. Note that the proposed adaptation approach is not restricted to dependency parsing and is in principle applicable to a range of multilingual NLP tasks. We will now describe the components of our model. 

 Biaffine Attention Parser The top layer of UDapter is a graph-based biaffine attention parser proposed by  Dozat and Manning (2017) . In this model, an encoder generates an internal representation r i for each word; the decoder takes r i and passes it through separate feedforward layers (MLP), and finally uses deep biaffine attention to score arcs connecting a head and a tail: h (head) i = MLP (head) (r i ) (1) h (tail) i = MLP (tail) (r i ) (2) s (arc) = Biaffine(H (head) , H (tail) ) (3) Similarly, label scores are calculated by using a biaffine classifier over two separate feedforward layers. Finally, the Chu-Liu/Edmonds algorithm  (Chu, 1965; Edmonds, 1967)  is used to find the highest scoring valid dependency tree. 

 Transformer Encoder with Adapters To obtain contextualized word representations, UDapter uses mBERT. For a token i in sentence S, BERT builds an input representation w i composed by summing a WordPiece embedding x i  (Wu et al., 2016 ) and a position embedding f i . Each w i ? S is then passed to a stacked self-attention layers (SA) to generate the final encoder representation r i : w i = x i + f i (4) r i = SA (w i ; ? (ad) ) (5) where ? (ad) denotes the adapter modules. During training, instead of fine-tuning the whole encoder network together with the task-specific top layer, we use adapter modules  (Rebuffi et al., 2018; Stickland and Murray, 2019; Houlsby et al., 2019) , or simply adapters, to capture both task-specific and language-specific information. Adapters are small modules added between layers of a pre-trained network. In adapter tuning, the weights of the original network are kept frozen, whilst the adapters are trained for a downstream task. Tuning with adapters was mainly suggested for parameter efficiency but they also act as an information module for the task or the language to be adapted  (Pfeiffer et al., 2020) . In this way, the original network serves as a memory for the language(s). In UDapter, following  Houlsby et al. (2019) , two bottleneck adapters with two feedforward projections and a GELU nonlinearity  (Hendrycks and Gimpel, 2016)  are inserted into each transformer layer as shown in  3) with a reasonable number of trainable parameters. 3 2) Adapters enable taskspecific as well as language-specific adaptation via CPG since it keeps backbone multilingual representations as memory for all languages in pre-training, which is important for multilingual transfer. 

 Contextual Parameter Generator To control the amount of sharing across languages, we generate trainable parameters of the model using a contextual parameter generator (CPG) function inspired by  Platanios et al. (2018) . CPG enables UDapter to retain high multilingual quality without losing performance on a single language, during multi-language training. We define CPG as a function of language embeddings. Since we only train adapters and the biaffine attention (i.e. adapter tuning), the parameter generator is formalized as {?  (ad)  , ? (bf ) } g  (m)  (l e ) where g (m) denotes the parameter generator with language embedding l e , and ?  (ad)  and ? (bf ) denote the parameters of adapters and biaffine attention respectively. We implement CPG as a simple linear transform of a language embedding, similar to Platanios et al.  (2018) , so that weights of adapters in the encoder and biaffine attention are generated by the dot product of language embeddings: g (m) (l e ) = (W (ad) , W (bf) ) ? l e (6) 3 Due to CPG, the number of adapter parameters is multiplied by language embedding size, resulting in a larger model compared to the baseline (more details in Appendix A.1). where l e ? R M , W (ad) ? R P (ad) ?M , W (bf) ? R P (bf) ?M , M is the language embedding size, P (ad) and P  (bf)  are the number of parameters for adapters and biaffine attention respectively.  4  An important advantage of CPG is the easy integration of existing task or language features. 

 Typology-Based Language Embeddings Soft sharing via CPG enables our model to modify its parsing decisions depending on a language embedding. While this allows UDapter to perform well on the languages in training, even if they are typologically diverse, information sharing is still a problem for languages not seen during training (zero-shot learning) as a language embedding is not available. Inspired by  Naseem et al. (2012)  and  Ammar et al. (2016) , we address this problem by defining language embeddings as a function of a large set of language typological features, including syntactic and phonological features. We use a multi-layer perceptron MLP  (lang)  with two feedforward layers and a ReLU nonlinear activation to compute a language embedding l e : l e = MLP (lang) (l t ) (7) where l t is a typological feature vector for a language consisting of all 103 syntactic, 28 phonological and 158 phonetic inventory features from the URIEL language typology database . URIEL is a collection of binary features zero-shot setup, i.e, without any training data.  6  The detailed treebank list is provided in Appendix A.3. For evaluation, the official CoNLL 2018 Shared Task script 7 is used to obtain LAS scores on the test set of each treebank. For the encoder, we use BERT-multilingualcased together with its WordPiece tokenizer. Since dependency annotations are between words, we pass the BERT output corresponding to the first wordpiece per word to the biaffine parser. We apply the same hyper-parameter settings as  Kondratyuk and Straka (2019) . Additionally, we use 256 and 32 for adapter size and language embedding size respectively. In our approach, pre-trained BERT weights are frozen, and only adapters and biaffine attention are trained, thus we use the same learning rate for the whole network by applying an inverse square root learning rate decay with linear warmup  (Howard and Ruder, 2018) . Appendix A.1 gives the hyper-parameter details. Baselines We compare UDapter to the current state of the art in UD parsing: [1] UUparser+BERT  (Kulmizev et al., 2019) , a graph-based BLSTM parser  (de Lhoneux et al., 2017; Smith et al., 2018)  using mBERT embeddings as additional features. [2] UDpipe  (Straka, 2018) , a monolingually trained multi-task parser that uses pretrained word embeddings and character representations. [3] UDify  (Kondratyuk and Straka, 2019) , the mBERTbased multi-task UD parser on which our UDapter is based, but originally trained on all language treebanks from UD. UDPipe scores are taken from  Kondratyuk and Straka (2019) . To enable a direct comparison, we also re-train UDify on our set of 13 high-resource languages both monolingually (one treebank at a time; monoudify) and multilingually (on the concatenation of languages; multi-udify). Finally, we evaluate two variants of our model: 1) Adapter-only has only task-specific adapter modules and no languagespecific adaptation, i.e. no contextual parameter generator; and 2) UDapter-proxy is trained without typology features: a separate language embedding is learnt from scratch for each in-training language, and for low-resource languages we use one from the same language family, if available, as proxy representation. Importantly, all baselines are either trained for a single language, or multilingually without any language-specific adaptation. By comparing UDapter to these parsers, we highlight its unique character that enables language specific parameterization by typological features within a multilingual framework for both supervised and zero-shot learning setup. 

 Results Overall, UDapter outperforms the monolingual and multilingual baselines on both high-resource and zero-shot languages. Below, we elaborate on the detailed results. High-resource Languages Labelled Attachement Scores (LAS) on the high-resource set are given in Table  1 . UDapter consistently outperforms both our monolingual and multilingual baselines in all languages, and beats the previous work, setting a new state of the art, in 9 out of 13 languages. Statistical significance testing 8 applied between UDapter and multi/mono-udify confirms that UDapter's performance is significantly better than the baselines in 11 out of 13 languages (all except en and it). Among directly comparable baselines, multiudify gives the worst performance in the typologically diverse high-resource setting. This multilingual model is clearly worse than its monolingually trained counterparts mono-udify: 83.0 vs 86.0. This result resounds with previous findings in multilingual NMT  (Arivazhagan et al., 2019)  and highlights the importance of language adaptation even when using high-quality sentence representations like those produced by mBERT. To understand the relevance of adapters, we also evaluate a model which has almost the same architecture as multi-udify except for the adapter modules and the tuning choice (frozen mBERT weights). Interestingly, this adapter-only model considerably outperforms multi-udify (85.0 vs 83.0), indicating that adapter modules are also effective in multilingual scenarios. Finally, UDapter achieves the overall best results, with consistent gains over both multi-udify and adapter-only, showing the importance of linguistically informed adaptation even for in-training languages. Low-Resource Languages Average LAS on the 30 low-resource languages are shown in column lr-avg of Table  1 . Overall, UDapter slightly outperforms the multi-udify baseline (36.5 vs 36.3), which shows the benefits of our approach on both in-training and zero-shot languages. For a closer look, Table  2  provides individual results for the 18 representative languages in our low-resource set. Here we find a mixed picture: UDapter outperforms multi-udify on 13 out of 18 languages 9 . Achieving improvements in the zero-shot parsing  9  LAS scores for all 30 languages are given in Appendix A.2. By significance testing, UDapter is significantly better than multi-udify on 16/30 low-resource languages, which is shown in Table  setup is very difficult, thus we believe this result is an important step towards overcoming the problem of positive/negative transfer trade-off. Indeed, UDapter-proxy results show that choosing a proxy language embedding from the same language family underperforms UDapter, apart from not being available for many languages. This indicates the importance of typological features in our approach (see ? 5.2 for further analysis). 

 Analysis In this section, we further analyse UDapter to understand its impact on different languages, and the importance of its various components. 

 Which languages improve most? Figure  2  presents the LAS gain of UDapter over the multi-udify baseline for each high-resource language along with the respective treebank training size. To summarize, the gains are higher for languages with less training data. This suggests that in UDapter, useful knowledge is shared among intraining languages, which benefits low resource languages without hurting high resource ones. For zero-shot languages, the difference between the two models is small compared to high-resource languages (+1.2 LAS). While it is harder to find a trend here, we notice that UDapter is typically beneficial for the languages not present in the mBERT training corpus: it outperforms multi-udify in 13 out of 22 (non-mBERT) languages. This suggests that typological feature-based adaptation leads to improved sentence representations when the pretrained encoder has not been exposed to a language.  

 How much gain from typology? UDapter learns language embeddings from syntactic, phonological and phonetic inventory features. A natural alternative to this choice is to learn language embeddings from scratch. For a comparison, we train a model where, for each in-training language, a separate language embedding (of the same size: 32) is initialized randomly and learned end-toend. For the zero-shot languages we use the average, or centroid, of all in-training language embeddings. As shown in Figure  4a , on the high-resource set, the models with and without typological features achieve very similar average LAS (87.3 and 87.1 respectively). On zero-shot languages, however, the use of centroid embedding performs very poorly: 9.0 vs 36.5 average LAS score over 30 languages. As already discussed in ? 4.1 (Table  2 ), using a proxy language embedding belonging to the same family as the test language, when available, also clearly underperforms UDapter. These results confirm our expectation that a model can learn reliable language embeddings for in-training languages, however typological signals are required to obtain a robust parsing quality on zero-shot languages. 

 How does UDapter represent languages? We start by analyzing the projection weights assigned to different typological features by the first layer of the language embedding network (see eq. 7). Figure  4b  shows the averages of normalized syntactic, phonological and phonetic inventory feature weights. Although dependency parsing is a syntactic task, the network does not only utilize syntactic features, as also observed by  Lin et al. (2019) , but exploits all available typological features to learn its representations. Next, we plot the language representations learned in UDapter by using t-SNE (van der Maaten and Hinton, 2008), which is similar to the analysis carried out by  Ponti et al. (2019, figure    8 ) using the language vectors learned by  Malaviya et al. (2017) . Figure  5  illustrates 2D vector spaces generated for the typological feature vectors l t (A) and the language embeddings l e learned by UDapter with or without typological features (B and C respectively). The benefits of using typological features can be understood by comparing A and B: During training, UDapter learns to project URIEL features to language embeddings in a way that is optimal for in-training language parsing quality. This leads to a different placement of the high-resource languages (red points) in the space, where many linguistic similarities are preserved (e.g. Hebrew and Arabic; European languages except Basque) but others are overruled (Japanese drifting away from Korean). Looking at the low-resource languages (blue points) we find that typologically similar languages tend to have similar embeddings to the closest highresource language in both A and B. In fact, most groupings of genetically related languages, such as the Indian languages (hi-cluster) or the Uralic ones (fi-cluster) are largely preserved across these two spaces. Comparing B and C where language embeddings are learned from scratch, the absence of typological features leads to a seemingly random space with no linguistic similarities (e.g. Arabic far away from Hebrew, Korean closer to English than to Japanese, etc.) and, therefore, no principled way to represent additional languages. Taken together with the parsing results of ? 4.1, these plots suggest that UDapter embeddings strike a good balance between a linguistically motivated representation space and one solely optimized for in-training language accuracy. 

 Is CPG really essential? In section 4.1 we observed that adapter tuning alone (that is, without CPG) improved the multilingual baseline in the high-resource languages, but worsened it considerably in the zero-shot setup. By contrast, the addition of CPG with typological features led to the best results over all languages. But could we have obtained similar results by simply increasing the adapter size? For instance, in multilingual MT, increasing overall model capacity of an already very large and deep architecture can be a powerful alternative to more sophisticated parameter sharing approaches  (Arivazhagan et al., 2019 ). To answer this question we train another adapteronly model with doubled size (2048 instead of the 1024 used in the main experiments). As seen in 3a, increase in model size brings a slight gain to the high-resource languages, but actually leads to a small loss in the zero-shot setup. This shows that adapters enlarge the per-language capacity for in-training languages, but at the same time they hurt generalization and zero-shot transfer. By contrast, UDapter including CPG which increases the model size by language embeddings (see Appendix A.1 for details), outperforms both adapter-only models, confirming once more the importance of this component. For our last analysis (Fig.  3b ), we study soft parameter sharing via CPG on different portions of the network, namely: only on the adapter modules 'cpg (adapters)' versus on both adapters and biaffine attention 'cpg (adap.+biaf.)' corresponding to the full UDapter. Results show that most of the gain in the high-resource languages is obtained by only applying CPG on the multilingual encoder. On the other hand, for the low-resource languages, typological feature based parameter sharing is most important in the biaffine attention layer. We leave further investigation of this result to future work. 

 Conclusion We have presented UDapter, a multilingual dependency parsing model that learns to adapt languagespecific parameters on the basis of adapter modules  (Rebuffi et al., 2018; Houlsby et al., 2019)  and the contextual parameter generation (CPG) method  (Platanios et al., 2018)  which is in principle applicable to a range of multilingual NLP tasks. While adapters provide a more general tasklevel adaptation, CPG enables language-specific adaptation, defined as a function of language embeddings projected from linguistically curated typological features. In this way, the model retains high per-language performance in the training data and achieves better zero-shot transfer. UDapter, trained on a concatenation of typologically diverse languages  (Kulmizev et al., 2019) , outperforms strong monolingual and multilingual baselines on the majority of both high-resource and low-resource (zero-shot) languages, which reflects its strong balance between per-language capacity and maximum sharing. Finally, the analyses we performed on the underlying characteristics of our model show that typological features are crucial for zero-shot languages. where no significant difference between UDapter and multi-udify by significance testing. For udapter-proxy, chosen proxy language is given between brackets. CTR means centroid language embedding. models are trained separately so the total number of parameters for 13 languages is 2.5B (13x191M). 
