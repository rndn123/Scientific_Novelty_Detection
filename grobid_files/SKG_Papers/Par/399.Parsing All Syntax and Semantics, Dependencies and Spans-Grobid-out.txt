title
Parsing All: Syntax and Semantics, Dependencies and Spans

abstract
Both syntactic and semantic structures are key linguistic contextual clues, in which parsing the latter has been well shown beneficial from parsing the former. However, few works ever made an attempt to let semantic parsing help syntactic parsing. As linguistic representation formalisms, both syntax and semantics may be represented in either span (constituent/phrase) or dependency, on both of which joint learning was also seldom explored. In this paper, we propose a novel joint model of syntactic and semantic parsing on both span and dependency representations, which incorporates syntactic information effectively in the encoder of neural network and benefits from two representation formalisms in a uniform way. The experiments show that semantics and syntax can benefit each other by optimizing joint objectives. Our single model achieves new state-of-the-art or competitive results on both span and dependency semantic parsing on Propbank benchmarks and both dependency and constituent syntactic parsing on Penn Treebank.

Introduction This work makes the first attempt to fill the gaps on syntactic and semantic parsing from jointly considering its representation forms and their linguistic processing layers. First, both span (constituent) and dependency are effective formal representations for both semantics and syntax, which have been well studied and discussed from both linguistic and computational perspective, though few works comprehensively considered the impact of either/both representation styles over the respective parsing  (Chomsky, 1981; Li et al., 2019b) . Second, as semantics is usually considered as a higher layer of linguistics over syntax, most previous studies focus on how the latter helps the former. Though there comes a trend that syntactic clues show less impact on enhancing semantic parsing since neural models were introduced . In fact, recent works  (He et al., 2017;  propose syntax-agnostic models for semantic parsing and achieve competitive and even state-of-the-art results. However, semantics may not only benefit from syntax which has been well known, but syntax may also benefit from semantics, which is an obvious gap in explicit linguistic structure parsing and few attempts were ever reported. To our best knowledge, few previous works focus on the relationship between syntax and semantic which only based on dependency structure  (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016) . To fill such a gap, in this work, we further exploit both strengths of the span and dependency representation of both semantic role labeling (SRL)  (Strubell et al., 2018)  and syntax, and propose a joint model 1 with multi-task learning in a balanced mode which improves both semantic and syntactic parsing. Moreover, in our model, semantics is learned in an end-to-end way with a uniform representation and syntactic parsing is represented as a joint span structure  (Zhou and Zhao, 2019)  relating to head-driven phrase structure grammar (HPSG)  (Pollard and Sag, 1994)  which can incorporate both head and phrase information of dependency and constituent syntactic parsing. We verify the effectiveness and applicability of the proposed model on Propbank semantic parsing 2 in both span style  (CoNLL-2005)    (Carreras and M?rquez, 2005)  and dependency style  (CoNLL-2009)    (Haji? et al., 2009)  and Penn Treebank (PTB)  (Marcus et al., 1993)  for both constituent and dependency syntactic parsing. Our empirical results show that semantics and syntax can indeed benefit each other, and our single model reaches new stateof-the-art or competitive performance for all four tasks: span and dependency SRL, constituent and dependency syntactic parsing. 

 Structure Representation In this section, we introduce a preprocessing method to handle span and dependency representation, which have strong inherent linguistic relation for both syntax and semantics. For syntactic representation, we use a formal structure called joint span following  (Zhou and Zhao, 2019)  to cover both constituent and head information of syntactic tree based on HPSG which is a highly lexicalized, constraint-based grammar  (Pollard and Sag, 1994) . For semantic (SRL) representation, we propose a unified structure to simplify the training process and employ SRL constraints for span arguments to enforce exact inference. 

 Syntactic Representation The joint span structure which is related to the HEAD FEATURE PRINCIPLE (HFP) of HPSG  (Pollard and Sag, 1994)  consists of all its children phrases in the constituent tree and all dependency arcs between the head and children in the dependency tree. For example, in the constituent tree of Figure  1 (a), Federal Paper Board is a phrase (1, 3) assigned with category NP and in dependency tree, Board is parent of Federal and Paper, thus in our joint span structure, the head of phrase (1, 3) is Board. The node S H (1, 9) in Figure  1 (b) as a joint span is: S H (1, 9) = { S H (1, 3) , S H (4, 8) , S H (9, 9), l(1, 9, <S>) , d(Board, sells) , d(., sells) }, where l(i, j, <S>) denotes category of span (i, j) with category S and d(r, h) indicates the dependency between the word r and its parent h. At last, the entire syntactic tree T being a joint span can be represented as: S H (T ) = {S H (1, 9), d(sells, root)} 3 . Following most of the recent work, we apply the PTB-SD representation converted by version 3.3.0 (5,7) (5,8) (4,8) (1,9) (1,3) (b) Joint span structure. Figure  1 : Constituent, dependency, and joint span structures from  (Zhou and Zhao, 2019) , which is indexed from 1 to 9 and assigned interval range for each node. The dotted box represents the same part. The special category # is assigned to divide the phrase with multiple heads. Joint span structure contains constitute phrase and dependency arc. Categ in each node represents the category of each constituent, and HEAD indicates the head word. of the Stanford parser. However, this dependency representation results in around 1% of phrases containing two or three head words. As shown in Figure  1 (a), the phrase (5,8) assigned with a category NP contains 2 head words of paper and products in dependency tree. To deal with the problem, we introduce a special category # to divide the phrase with multiple heads to meet the criterion that there is only one head word for each phrase. After this conversion, only 50 heads are errors in PTB. Moreover, to simplify the syntactic parsing algorithm, we add a special empty category ? to spans to binarize the n-ary nodes and apply a unary atomic category to deal with the nodes of the unary chain, which is popularly adopted in constituent syntactic parsing  (Stern et al., 2017; Gaddy et al., 2018)  (5,7) (5,8) (4,8) (1,9) (1,3) Federal Paper Board sells paper and wood products .  

 Semantic Representation Similar to the semantic representation of  (Li et al., 2019b) , we use predicate-argument-relation tuples Y ? P ? A ? R, where P = {w 1 , w 2 , ..., w n } is the set of all possible predicate tokens, A = {(w i , . . . , w j )|1 ? i ? j ? n} includes all the candidate argument spans and dependencies, and R is the set of the semantic roles and employ a null label to indicate no relation between predicate-argument pair candidate. The difference from that of  (Li et al., 2019b)  is that in our model, we predict the span and dependency arguments at the same time which needs to distinguish the single word span arguments and dependency arguments. Thus, we represent all the span arguments A = {(w i , . . . , w j )|1 ? i ? j ? n} as span S(i ? 1, j) and all the dependency arguments A = {(w i )|1 ? i ? n} as span S(i, i). We set a special start token at the beginning of sentence. 3 Our Model 

 Overview As shown in Figure  2 , our model includes four modules: token representation, self-attention encoder, scorer module, and two decoders. Using an encoder-decoder backbone, we apply self-attention encoder  (Vaswani et al.)  that is modified by position partition  (Kitaev and Klein, 2018) . We take multi-task learning (MTL) approach sharing the parameters of token representation and self-attention encoder. Since we convert two syntactic representations as joint span structure and apply uniform semantic representation, we only need two decoders, one for syntactic tree based on joint span syntactic parsing algorithm  (Zhou and Zhao, 2019) , another for uniform SRL. 

 Token Representation In our model, token representation x i is composed of characters, words, and part-of-speech (POS) representation. For character-level representation, we use CharLSTM  (Ling et al., 2015) . For word-level representation, we concatenate randomly initialized and pre-trained word embeddings. We concatenate character representation and word representation as our token representation x i =[x char ;x word ;x P OS ]. In addition, we also augment our model with BERT  (Devlin et al., 2019)  or XLNet  (Yang et al., 2019)  as the sole token representation to compare with other pre-training models. Since BERT and XLNet are based on sub-word, we only take the last sub-word vector of the word in the last layer of BERT or XLNet as our sole token representation x i . 

 Self-Attention Encoder The encoder in our model is adapted from  (Vaswani et al.)  and factor explicit content and position information in the self-attention process. The input matrices X = [x 1 , x 2 , . . . , x n ] in which x i is concatenated with position embedding are transformed by a self-attention encoder. We factor the model between content and position information both in self-attention sub-layer and feed-forward network, whose setting details follow  (Kitaev and Klein, 2018) . 

 Scorer Module Since span and dependency SRL share uniform representation, we only need three types of scores: syntactic constituent span, syntactic dependency head, and semantic role scores. We first introduce the span representation s ij for both constituent span and semantic role scores. We define the left end-point vector as concatenation of the adjacent token ? ? pl i = [ ? ? y i ; ? ? ? y i+1 ], which ? ? y i is constructed by splitting in half the outputs from the self-attention encoder. Similarly, the right endpoint vector is ? ? pr i = [ ? ? ? y i+1 ; ? ? y i ]. Then, the span representation s ij is the differences of the left and right end-point vectors s ij = [ ? ? pr j ? ? ? pl i ] 4 . Constituent Span Score We follow the constituent syntactic parsing  (Zhou and Zhao, 2019; Kitaev and Klein, 2018; Gaddy et al., 2018)  to train constituent span scorer. We apply one-layer feedforward networks to generate span scores vector, taking span vector s ij as input: S(i, j) = W 2 g(LN (W 1 s ij + b 1 )) + b 2 , where LN denotes Layer Normalization, g is the Rectified Linear Unit nonlinearity. The individual score of category is denoted by S categ (i, j, ) = [S(i, j)] , where [] indicates the value of corresponding the lth element of the score vector. The score s(T ) of the constituent parse tree T is obtained by adding all scores of span (i, j) with category : s(T ) = (i,j, )?T S categ (i, j, ). The goal of constituent syntactic parsing is to find the tree with the highest score: T = arg max T s(T ). We use CKY-style algorithm  (Gaddy et al., 2018)  to obtain the tree T in O(n 3 ) time complexity. This structured prediction problem is handled with satisfying the margin constraint: s(T * ) ? s(T ) + ?(T, T * ), where T * denotes correct parse tree, and ? is the Hamming loss on category spans with a slight modification during the dynamic programming search.  4  Since we use the same end-point span sij = [ ? ? prj ? ? ? pli] to represent the dependency arguments for our uniform SRL, we distinguish the left and right end-point vector ( ? ? pli and ? ? pri) to avoid having the zero vector as a span representation sij. The objective function is the hinge loss, J 1 (?) = max(0, max T [s(T )+?(T, T * )]?s(T * )). Dependency Head Score We predict a the possible heads and use the biaffine attention mechanism  (Dozat and Manning, 2017)  to calculate the score as follow: ? ij = h T i W g j + U T h i + V T g j + b, where ? ij indicates the child-parent score, W denotes the weight matrix of the bi-linear term, U and V are the weight vectors of the linear term, and b is the bias item, h i and g i are calculated by a distinct one-layer perceptron network. We minimize the negative log-likelihood of the golden dependency tree Y , which is implemented as a cross-entropy loss: J 2 (?) = ? (logP ? (h i |x i ) + logP ? (l i |x i , h i )) , where P ? (h i |x i ) is the probability of correct parent node h i for x i , and P ? (l i |x i , h i ) is the probability of the correct dependency label l i for the childparent pair (x i , h i ). Semantic Role Score To distinguish the currently considered predicate from its candidate arguments in the context, we employ one-layer perceptron to contextualized representation for argument a ij 5 candidates: a ij = g(W 3 s ij + b 1 ), where g is the Rectified Linear Unit nonlinearity and s ij denotes span representation. And predicate candidates p k is simply represented by the outputs from the self-attention encoder: p k = y k . For semantic role, different from  (Li et al., 2019b) , we simply adopt concatenation of predicates and arguments representations, and one-layer feedforward networks to generate semantic role score: ? r (p, a) = W 5 g(LN (W 4 [p k ; a ij ] + b 4 )) + b 5 , and the individual score of semantic role label r is denoted by: ? r (p, a, r) = [? r (p, a)] r . Since the total of predicate-argument pairs are O(n 3 ), which is computationally impractical. We apply candidates pruning method in  (Li et al., 2019b; . First of all, we train separate scorers (? p and ? a ) for predicates and arguments by two one-layer feedforward networks. Then, the predicate and argument candidates are ranked according to their predicted score (? p and ? a ), and we select the top n p and n a predicate and argument candidates, respectively: n p = min(? p n, m p ), n a = min(? a n, m a ), where ? p and ? a are pruning rate, and m p and m a are maximal numbers of candidates. Finally, the semantic role scorer is trained to optimize the probability P ? (?|s) of the predicateargument-relation tuples ?(p,a,r) ? Y given the sentence s, which can be factorized as: At last, we train our scorer for simply minimizing the overall loss: J 3 (?) = J overall (?) = J 1 (?) + J 2 (?) + J 3 (?). 

 Decoder Module 

 Decoder for Joint Span Syntax As the joint span is defined in a recursive way, to score the root joint span has been equally scoring all spans and dependencies in syntactic tree. During testing, we apply the joint span CKYstyle algorithm  (Zhou and Zhao, 2019) , as shown in Algorithm 1 to explicitly find the globally highest score S H (T ) of our joint span syntactic tree T 6 . Also, to control the effect of combining span and dependency scores, we apply a weight ? H 7 : s(i, j, ) = ? H S categ (i, j, ), d(i, j) = (1? H )? ij , 6 For further details, see  (Zhou and Zhao, 2019)  which has discussed the different between constituent syntactic parsing CKY-style algorithm, how to binarize the joint span tree and the time, space complexity.  7  We also try to incorporate the head information in constituent syntactic training process, namely max-margin loss Algorithm 1 Joint span syntactic parsing algorithm Input: sentence leng n, span and dependency score s(i, j, ), d(r, h), 1 ? i ? j ? n, ?r, h, Output: maximum value SH (T ) of tree T Initialization: sc[i][j][h] = si[i][j][h] = 0, ?i, j, h for len = 1 to n do for i = 1 to n ? len + 1 do j = i + len ? 1 if len = 1 then sc[i][j][i] = si[i][j][i] = max s(i, j, ) else for h = i to j do split l = max i?r<h { max r?k<h { sc[i][k][r]+ si[k + 1][j][h] } + d(r, h) } splitr = max h<r?j { max h?k<r { si[i][k][h]+ sc[k + 1][j][r] } + d(r, h) } sc[i][j][h] = max { split l , splitr }+ max =? s(i, j, ) si[i][j][h] = max { split l , splitr }+ max s(i, j, ) end for end if end for end for SH (T ) = max 1?h?n { sc[1][n][h] + d(h, root) } where ? H in the range of 0 to 1. In addition, we can merely generate constituent or dependency syntactic parsing tree by setting ? H to 1 or 0, respectively. Decoder for Uniform Semantic Role Since we apply uniform span for both dependency and span semantic role, we use a single dynamic programming decoder to generate two semantic forms following the non-overlapping constraints: span semantic arguments for the same predicate do not overlap  (Punyakanok et al., 2008) . 

 Experiments We evaluate our model on CoNLL-2009 shared task  (Haji? et al., 2009)  for dependency-style SRL, CoNLL-2005 shared task  (Carreras and M?rquez, 2005)  for span-style SRL both using the Propbank convention  (Palmer et al., 2005) , and English Penn Treebank (PTB)  (Marcus et al., 1993)  for constituent syntactic parsing, Stanford basic dependencies (SD) representation (de  Marneffe et al., 2006)  converted by the Stanford parser 8 for dependency syntactic parsing. We follow standard data splitting: for both two scores, but it makes the training process become more complex and unstable. Thus we employ a parameter to balance two different scores in joint decoder which is easily implemented with better performance. 8 http://nlp.stanford.edu/software/lex-parser.html semantic (SRL) and syntactic parsing take section 2-21 of Wall Street Journal (WSJ) data as training set, SRL takes section 24 as development set while syntactic parsing takes section 22 as development set, SRL takes section 23 of WSJ together with 3 sections from Brown corpus as test set while syntactic parsing only takes section 23. POS tags are predicted using the Stanford tagger  (Toutanova et al., 2003) . In addition, we use two SRL setups: end-to-end and pre-identified predicates. For the predicate disambiguation task in dependency SRL, we follow  and use the off-the-shelf disambiguator from  (Roth and Lapata, 2016) . For constituent syntactic parsing, we use the standard evalb 9 tool to evaluate the F1 score. For dependency syntactic parsing, following previous work  (Dozat and Manning, 2017) , we report the results without punctuations of the labeled and unlabeled attachment scores (LAS, UAS). 

 Setup Hyperparameters In our experiments, we use 100D GloVe  (Pennington et al., 2014)  pre-trained embeddings. For the self-attention encoder, we set 12 self-attention layers and use the same other hyperparameters settings as  (Kitaev and Klein, 2018) . For semantic role scorer, we use 512-dimensional MLP layers and 256-dimensional feed-forward networks. For candidates pruning, we set ? p = 0.4 and ? a = 0.6 for pruning predicates and arguments, m p = 30 and m a = 300 for max numbers of predicates and arguments respectively. For constituent span scorer, we apply a hidden size of 250-dimensional feed-forward networks. For dependency head scorer, we employ two 1024dimensional MLP layers with the ReLU as the activation function for learning specific representation and a 1024-dimensional parameter matrix for biaffine attention. In addition, when augmenting our model with BERT and XLNet, we set 2 layers of self-attention for BERT and XLNet. Training Details we use 0.33 dropout for biaffine attention and MLP layers. All models are trained for up to 150 epochs with batch size 150 on a single NVIDIA GeForce GTX 1080Ti GPU with Intel i7-7800X CPU. We use the same training settings as  (Kitaev and Klein, 2018)  and  (Kitaev et al., 2019) .   

 Joint Span Syntactic Parsing This subsection examines joint span syntactic parsing decoder 3.5 with semantic parsing both of dependency and span. The weight parameter ? H plays an important role to balance the syntactic span and dependency scores. When ? H is set to 0 or 1, the joint span parser works as the dependencyonly parser or constituent-only parser respectively. ? H set to between 0 to 1 indicates the general joint span syntactic parsing, providing both constituent and dependency structure prediction. We set the ? H parameter from 0 to 1 increased by 0.1 step as shown in Figure  3 . The best results are achieved when ? H is set to 0.8 which achieves the best performance of both syntactic parsing. In addition, we compare the joint span syntactic parsing decoder with a separate learning constituent syntactic parsing model which takes the same token representation, self-attention encoder and joint learning setting of semantic parsing on PTB dev set. The constituent syntactic parsing results are also converted into dependency ones by PTB-SD for comparison. Table  1  shows that joint span decoder benefit both of constituent and dependency syntactic parsing. Besides, the comparison also shows that the directly predicted dependencies from our model are better than those converted from the predicted constituent parse trees in UAS term. 

 Joint Learning Analysis Table  2  compares the different joint setting of semantic (SRL) and syntactic parsing to examine whether semantics and syntax can enjoy their joint learning. In the end-to-end mode, we find that constituent syntactic parsing can boost both styles of semantics while dependency syntactic parsing cannot. Moreover, the results of the last two rows indicate that semantics can benefit syntax simply by optimizing the joint objectives. While in the given predicate mode, both constituent and dependency syntactic parsing can enhance SRL. In addition, joint learning of our uniform SRL performs better than separate learning of either dependency or span SRL in both modes. Overall, joint semantic and constituent syntactic parsing achieve relatively better SRL results than the other settings. Thus, the rest of the experiments are done with multi-task learning of semantics and constituent syntactic parsing (wo/dep). Since semantics benefits both of two syntactic formalisms and two syntactic parsing can benefit each other, we also compare the results of joint learning with semantics and two syntactic parsing models (w/dep). 

 Syntactic Parsing Results In the wo/dep setting, we convert constituent syntactic parsing results into dependency ones by PTB-SD for comparison and set ? H described in 3.5 to UAS LAS  Dozat and Manning (2017)  95.74 94.08  Ma et al. (2018)  95.87 94.19  Strubell et al. (2018)  94.92 91.87  Fern?ndez-Gonz?lez and G?mez-Rodr?guez (2019)   Compared to the existing state-of-the-art models without pre-training, our performance exceeds  (Zhou and Zhao, 2019)  nearly 0.2 in LAS of dependency and 0.3 F1 of constituent syntactic parsing which are considerable improvements on such strong baselines. Compared with  (Strubell et al., 2018)  shows that our joint model setting boosts both of syntactic parsing and SRL which are consistent with  (Shi et al., 2016)  that syntactic parsing and SRL benefit relatively more from each other. We augment our parser with a larger version of BERT and XLNet as the sole token representation to compare with other models. Our single model in XLNet setting achieving 96.18 F1 score of constituent syntactic parsing, 97.23% UAS and 95.65% LAS of dependency syntactic parsing. 

 Semantic Parsing Results We present all results using the official evaluation script from the CoNLL-2005 and CoNLL-2009 shared tasks, and compare our model with previous state-of-the-art models in   

 Related Work In the early work of SRL, most of the researchers focus on feature engineering based on training corpus. The traditional approaches to SRL focused on developing rich sets of linguistic features templates and then employ linear classifiers such as SVM  (Zhao et al., 2009a) . With the impressive success of deep neural networks in various NLP tasks  (Luo and Zhao, 2020; Li et al., 2020; He et al., 2019; Luo et al., 2020b; Zhang et al., 2018a; Li et al., 2018a; Luo et al., 2020a; Zhang et al., 2019; Li et al., 2019a; Zhao and Kit, 2008; Zhao et al., 2009b Zhao et al., , 2013 , considerable attention has been paid to syntactic features  (Strubell et al., 2018; Kasai et al., 2019; .  (Lewis et al., 2015; Strubell et al., 2018; Kasai et al., 2019;  modeled syntactic parsing and SRL jointly,  (Lewis et al., 2015)  jointly modeled SRL and CCG parsing, and  (Kasai et al., 2019)  combined the supertags extracted from dependency parses with SRL . There are a few studies on joint learning of syntactic and semantic parsing which only focus on dependency structure  (Swayamdipta et al., 2016; Henderson et al., 2013; Shi et al., 2016) . Such as  (Henderson et al., 2013)  based on dependency structure only focus on shared representation without explicitly analyzing whether syntactic and semantic parsing can benefit each other. The ablation studies results show joint learning can benefit semantic parsing while the single syntactic parsing model was insignificantly worse (0.2%) than the joint model.  (Shi et al., 2016)  only made a brief attempt on Chinese Semantic Treebank to show the mutual benefits between dependency syntax and semantic roles. Instead, our work focuses on whether syntactic and semantic parsing can benefit each other both on span and dependency in a more general way. Besides, both span and dependency are effective formal representations for both semantics and syntax. On one hand, researchers are interested in two forms of SRL models that may benefit from each other rather than their separated development, which has been roughly discussed in  (Johansson and Nugues, 2008) .  is the first to apply span-graph structure based on contextualized span representations to span SRL and  (Li et al., 2019b)  built on these span representations achieves state-of-art results on both span and dependency SRL using the same model but training individually. On the other hand, researchers have discussed how to encode lexical dependencies in phrase structures, like lexicalized tree adjoining grammar (LTAG)  (Schabes et al., 1988)  and headdriven phrase structure grammar (HPSG)  (Pollard and Sag, 1994) . 

 Conclusions This paper presents the first joint learning model which is evaluated on four tasks: span and dependency SRL, constituent and dependency syntactic parsing. We exploit the relationship between semantics and syntax and conclude that not only syntax can help semantics but also semantics can improve syntax performance. Besides, we propose two structure representations, uniform SRL and joint span of syntactic structure, to combine the span and dependency forms. From experiments on these four parsing tasks, our single model achieves state-of-the-art or competitive results. Figure 2 : 2 Figure 2: The framework of our joint parsing model. 

 p?P,a?A,r?R ?logP ? (y (p,a,r) |s) = p?P,a?A,r?R ?log exp ?(p, a, r) r?R exp ?(p, a, r) where ? represents the model parameters, and ?(p, a, r) = ? p + ? a + ? r (p, a, r) is the score by the predicate-argument-relation tuple including predicate score ? p , argument score ? a and semantic role label score ? r (p, a, r). In addition, we fix the score of null label ?(p, a, ) = 0. 

 Figure 3 : 3 Figure 3: Syntactic parsing performance of different parameter ? H on PTB dev set. 

 . Federal 1 . . ROOT HEAD sells Categ < S > NNP 1 Dependency Head Score HEAD Board Categ < NP > HEAD sells Categ < VP > Paper NNP . Federal NNP 1 Paper NNP 2 Board NNP 3 sells VBZ 4 HEAD products Categ < NP > 9 . . . . Constituent Span Score 0 Label score 0 Joint Span Structure wood NN 7 Categ < # > paper NN and CC 5 6 HEAD paper products NNS 8 wood NN . Broadcast + products Predicate score . 0 Softmax NNS Semantic Role Score Argument score Span and Dependency SRL Input Token Representation Self-Attention Layers Scoring Layer Decoder Layer 

 Table 1 : 1 PTB dev set performance of joint span syntactic parsing. The converted means the corresponding dependency syntactic parsing results are from the corresponding constituent parse tree using head rules. Model F1 UAS LAS separate constituent converted dependency 93.98 ? 95.38 ? 94.06 separate dependency ? 95.80 94.40 joint span ?H = 1.0 93.89 ? ? joint span ?H = 0.0 ? 95.90 94.50 joint span ?H = 0.8 converted dependency 93.98 95.99 95.70 94.53 94.60 

 Table 2 : 2 Joint learning analysis on CoNLL-2005, CoNLL-2009, and PTB dev sets. System SEM span SEM dep SYN con SYN dep F 1 F 1 F 1 UAS LAS End-to-end SEM span 82.27 ? ? ? ? SEM dep ? 84.90 ? ? ? SEM span,dep 83.50 84.92 ? ? ? SEM span,dep , SYN con 83.81 84.95 93.98 ? ? SEM span,dep , SYN dep 83.13 84.24 ? 95.80 94.40 SYN con,dep ? ? 93.78 95.92 94.49 SEM span,dep , SYN con,dep 83.12 83.90 93.98 95.95 94.51 Given predicate SEM span 83.16 ? ? ? ? SEM dep ? 88.23 ? ? ? SEM span,dep 84.74 88.32 ? ? ? SEM span,dep , SYN con 84.46 88.40 93.78 ? ? SEM span,dep , SYN dep 84.76 87.58 ? 95.94 94.54 SEM span,dep , SYN con,dep 84.43 87.58 94.07 96.03 94.65 

 Table 4 : 4 Constituent syntactic parsing on WSJ test set 1 for generating constituent syntactic parsing only. 96.04 94.43 

 Table 5, 6. The upper part of the tables presents results from end-to-end mode System WSJ Brown P R F 1 P R F 1 End-to-end He et al. (2018a) 81.2 83.9 82.5 69.7 71.9 70.8 Li et al. (2019b) - - 83.0 - - - Strubell et al. (2018) 84.07 83.16 83.61 73.32 70.56 71.91 Strubell et al. (2018)* 85.53 84.45 84.99 75.8 73.54 74.66 Ours (wo/dep) 83.65 85.48 84.56 72.02 73.08 72.55 Ours (w/dep) 83.54 85.30 84.41 71.84 72.07 71.95 + Pre-training He et al. (2018a) 84.8 87.2 86.0 73.9 78.4 76.1 Li et al. (2019b) 85.2 87.5 86.3 74.7 78.1 76.4 Strubell et al. (2018) 86.69 86.42 86.55 78.95 77.17 78.05 Strubell et al. (2018)* 87.13 86.67 86.90 79.02 77.49 78.25 Ours (wo/dep) + BERT 86.77 88.49 87.62 79.06 81.67 80.34 Ours (w/dep) + BERT 86.46 88.23 87.34 77.26 80.20 78.70 Ours (wo/dep) + XLNet 87.65 89.66 88.64 80.77 83.92 82.31 Ours (w/dep) + XLNet 87.48 89.51 88.48 80.46 84.15 82.26 Given predicate Tan et al. (2017) 84.5 85.2 84.8 73.5 74.6 74.1 He et al. (2018a) - - 83.9 - - 73.7 Ouchi et al. (2018) 84.7 82.3 83.5 76.0 70.4 73.1 Strubell et al. (2018) 84.72 84.57 84.64 74.77 74.32 74.55 Strubell et al. (2018)* 86.02 86.05 86.04 76.65 76.44 76.54 Ours (wo/dep) 85.93 85.76 85.84 76.92 74.55 75.72 Ours (w/dep) 85.61 85.39 85.50 73.9 73.22 73.56 + Pre-training He et al. (2018a) - - 87.4 - - 80.4 Ouchi et al. (2018) 88.2 87.0 87.6 79.9 77.5 78.7 Li et al. (2019b) 87.9 87.5 87.7 80.6 80.4 80.5 Ours (wo/dep) + BERT 89.04 88.79 88.91 81.89 80.98 81.43 Ours (w/dep) + BERT 88.94 88.53 88.73 81.66 80.80 81.23 Ours (wo/dep) + XLNet 89.89 89.74 89.81 85.35 84.57 84.96 Ours (w/dep) + XLNet 89.62 89.82 89.72 85.08 84.84 84.96 

 Table 5 : 5 Span SRL results on CoNLL-2005 test sets. * represents injecting state-of-the-art predicted parses. System WSJ Brown P R F1 P R F1 End-to-end Li et al. (2019b) - - 85.1 - - - Ours (wo/dep) 84.24 87.55 85.86 76.46 78.52 77.47 Ours (w/dep) 83.73 86.94 85.30 76.21 77.89 77.04 + Pre-training He et al. (2018b) 83.9 82.7 83.3 - - - Cai et al. (2018) 84.7 85.2 85.0 - - 72.5 Li et al. (2019b) 84.5 86.1 85.3 74.6 73.8 74.2 Ours (wo/dep) + BERT 87.40 88.96 88.17 80.32 82.89 81.58 Ours (w/dep) + BERT 86.77 89.14 87.94 79.71 82.40 81.03 Ours (wo/dep) + XLNet 86.58 90.40 88.44 80.96 85.31 83.08 Ours (w/dep) + XLNet 86.35 90.16 88.21 80.90 85.38 83.08 Given predicate (Kasai et al., 2019) 89.0 88.2 88.6 78.0 77.2 77.6 Ours (wo/dep) 88.73 89.83 89.28 82.46 83.20 82.82 Ours (w/dep) 88.02 89.03 88.52 80.98 82.10 81.54 + Pre-training He et al. (2018b) 89.7 89.3 89.5 81.9 76.9 79.3 Cai et al. (2018) 89.9 89.2 89.6 79.8 78.3 79.0 Li et al. (2019b) 89.6 91.2 90.4 81.7 81.4 81.5 Kasai et al. (2019) 90.3 90.0 90.2 81.0 80.5 80.8 Lyu et al. (2019) - - 90.99 - - 82.18 Chen et al. (2019) 90.74 91.38 91.06 82.66 82.78 82.72 Cai and Lapata (2019) 91.7 90.8 91.2 83.2 81.9 82.5 Ours (wo/dep) + BERT 91.21 91.19 91.20 85.65 86.09 85.87 Ours (w/dep) + BERT 91.14 91.03 91.09 85.18 85.41 85.29 Ours (wo/dep) + XLNet 91.16 91.60 91.38 87.04 87.54 87.29 Ours (w/dep) + XLNet 90.80 91.74 91.27 86.43 87.25 86.84 

 Table 6 : 6 Dependency SRL results on CoNLL-2009 Propbank test sets.while the lower part shows the results of given predicate mode to compare to more previous works with pre-identified predicates. In given predicate mode, we simply replace predicate candidates with the gold predicates without other modification on the input or encoder. Span SRL Results Table 5 shows results on CoNLL-2005 in-domain (WSJ) and out-domain (Brown) test sets. It is worth noting that (Strubell et al., 2018) injects state-of-the-art predicted parses in terms of setting of (Dozat and Manning, 2017) at test time and aims to use syntactic information to help SRL. While our model not only excludes other auxiliary information during test time but also benefits both syntax and semantics. We ob- tain comparable results with the state-of-the-art method (Strubell et al., 2018) and outperform all recent models without additional information in test time. After incorporating with pre-training contextual representations, our model achieves new state-of-the-art both of end-to-end and given predi- cate mode and both of in-domain and out-domain. Dependency SRL Results Table 6 presents the results on CoNLL-2009. We obtain new state- of-the-art both of end-to-end and given predicate mode and both of in-domain and out-domain text. These results demonstrate that our improved uni- form SRL representation can be adapted to perform dependency SRL and achieves impressive perfor- mance gains. 

			 Our code : https://github.com/DoodleJZ/ParsingAll. 2 It is also called semantic role labeling (SRL) for the semantic parsing task over the Propbank. 

			 For dependency label of each word, we train a separated multi-class classifier simultaneously with the parser by optimizing the sum of their objectives. 

			 When i=j, it means a uniform representation of dependency semantic role. 

			 http://nlp.cs.nyu.edu/evalb/
