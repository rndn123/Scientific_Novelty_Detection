title
NJU-Parser: Achievements on Semantic Dependency Parsing

abstract
In this paper, we introduce our work on SemEval-2012 task 5: Chinese Semantic Dependency Parsing. Our system is based on MSTParser and two effective methods are proposed: splitting sentence by punctuations and extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement.

Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner's probabilistic dependency parser  (Eisner, 1996 ), McDonald's MSTParser (McDonald et al. 2005a McDonald et al. 2005b ) and Nivre's MaltParser  (Nivre, 2006) . Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance.  (Li et al., 2010 ) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of word as lemma and handling multi-level labels. Improvements could be achieved through the first two methods while not for the third. 

 Overview of Our System Our system is based on MSTParser which is one of the state-of-the-art parsers. MSTParser tries to obtain the maximum spanning tree of a sentence. For projective parsing task, it takes Eisner's algorithm  (Eisner, 1996)  to get the dependency tree in O(n 3 ) time. Meanwhile, Chu-Liu-Edmond's algorithm  (Chu and Liu, 1965)  is applied for non-projective task, which takes O(n 2 ) time. Three methods are adopted to MSTParser in our system: 1) Sentences are split into sub-sentences by commas and semicolons, for which there are two ways. Splitting sentences by all commas and semicolons is used in our primary system. In our contrast system, we use a classifier to determine whether a comma or semicolon can be used to split the sentence. In the primary and contrast system, the proto sentences and the subsentences are trained and tested separately and the outputs are merged in the end. 2) In a Chinese word, the last character usually contains main sense or semantic class. We treat the last character of the word as word lemma and find it gets a slightly improvement in the experiment. 3) An experiment trying to solve the problem of multi-level labels was conducted by parsing different levels separately and consequently merging the outputs together. The experiment results have shown that the first two methods could enhance the system performance while further improvements could be obtained through a combination of them in our subsubmitted systems. c) The second sub sentence of a) Figure  1 . An example of the split procedure. 

 Experiments 

 Split sentences by commas and semicolons It is observed that the performance decreases as the length of the sentences increases. Table  1  shows the statistical analysis on the data including SemEval-2012, Conll-07's Chinese corpus and a subset extracted from CTB using Penn2Malt. Long sentence can be split into sub-sentences to get better parsing result.   1 . Statistical analysis on the data. The CTB data is a subset extracted from CTB using Penn2Malt. Our work can be described as following steps: Step 1: Use MSTParser to parse the data. We name the result as "normal output". Step 2: Split train and test data by all commas and semicolons. The delimiters are removed in the sub sentences. For train data, a word's dependency relation is kept if the word's head is under the cov-er of the sub sentence. Otherwise, its head will be set to root and its label will be set to ROOT (ROOT is the default label of dependency arcs whose head is root). We define the word as "sentence head" if its head is root. "Sub-sentence head" indicates the sentence head of a sub-sentence. After splitting, there may be more than one sub-sentence heads in a sub-sentence. Figure  1  shows an example of the split procedure. Step 3: Use MSTParser to parse the data generated in step 2. We name the parsing result "split output". In split output, there may be more than one sub-sentences corresponding to a single sentence in normal output. Step 4: Merge the split output and the normal output. The outputs of sub-sentences are merged with delimiters restored. Dependency relations are recovered for all punctuations and sub-sentence heads in split output with relations in normal output. The sentence head of normal output is kept in final output. The result is called "merged split output". This step need to be consummated because it may result in a dependency tree not well formed with several sentence heads or even circles. The results of experiments on develop data and test data are showed in table 2. For develop data, an improvement of 0.85 could be obtained while 0.93 for test data, both on LAS. In step 2, there is an alternative to split the sentences, i.e., using a classifier to determine which comma and semicolon can be split. This method is taken in the contrast system. When applying the classifier, all commas and semicolons in train data w-4,w-3,w-2,w-1,w,w+1,w+2,w+3,w+4 p-4,p-3,p-2,p-1,p,p+1,p+2,p+3,p+4 wp-4,wp-3,wp-2,wp-1,wp wp+1,wp+2,wp+3,wp+4 w-4|w-3,w-3|w-2,w-2|w-1,w-1|w, w|w+1,w+1|w+2,w+2|w+3,w+3|w+4 p-4|p-3,p-3|p-2,p-2|p-1,p-1|p, p|p+1,p+1|p+2,p+2|p+3,p+3|p+4 first word of sub-sentence before the delimiter Table  3 . Features used in CRF++. w represents for word and p for PosTag. +1 means the index after current while -1 means before. 

 Extract last character of word as lemma In Chinese, the last character of a word usually contains main sense or semantic class, which indicates that it may represent the whole word. For example, " ? "(country) can represent " ? ? "(China) and " ? "(love) can represent " ? ?"(crazy love). The last character is used as lemma in the experiment, with an improvement of 0.27 for LAS on develop data and 0.24 on test data. Details of the scores are listed in table 2 as "lemma output". 

 Multi-level labels experiment A notable characteristic of SemEval-2012's data is multi-level labels. It introduces four kinds of multi-level labels which are s-X, d-X, j-X and r-X. The first level represents the basic semantic relation of the dependency while the second level shows the second import, except that s-X represents sub-sentence relation. The r-X label means that a verb modifies a noun and the relation between them is reverse. For example, in phrase "?(poor) ?(born) ? ? ?(star)", "?" is headed to "?" with label ragent. It means that "?" is the agent of "?". When a verbal noun is the head word and its child has indirect relation to it, the dependency is labeled with j-X. In phrase "?(school) ? (construction)", "?" is the head of "?" with label j-content. "?" is the content of "?". The d-X label means that the child modifies the head with an additional relation. For example, in phrase "?(technology) ?(enterprise)", "? ?" modifies "?" and the domain of "?" is "?". A heuristic method is tried in the experiment. The multi-level labels of d-X, j-X and r-X are separated into two parts for each level. For example, "d-content" will be separated to "d" and "content". For each part, MSTParser is used to train and test. We call the outputs "first-level output" and "second-level output". The outputs of each level and normal output are merged then. In our experiments, only the word satisfies the following conditions need to be merged: a) The dependency label in normal output is started with d-, j-or r-. b) The dependency label in first-level output is d, j or r. c) The heads in first-level output and secondlevel output are of the same. Otherwise, the dependency relation in normal output will be kept. There are also three ways in merging outputs: a) Label in first-level output and label in second-level output are merged. b) First level label in normal output and label in second-level output are merged. c) Label in first-level output and second level label in normal output are merged. Experiment has been done on develop data. In the experiment, 24% of the labels are merged and 92% of the new merged labels are the same as original. The results of three ways are listed in table 4. All of them get decline compared to normal output.  

 Combined experiment on split and lemma Improvements are achieved by first two methods in the experiment while a further enhancement is made with a combination of them in the submitted systems. The split method and lemma method are combined as primary system. The split method with CRF++ and lemma method are combined as contrast system. When combining the two methods, last character of the word is firstly extracted as lemma for train data and test data. Then the split or split with CRF++ method is used. The outputs of the primary system and contrast system are listed in table  2 . 

 Analysis and Discussion The contrast system presented in this paper finally got the second prize among nine systems. The primary system gets the third. There is an improvement of about one percent for both primary and contrast system. The following conclusions can be made from the experiments: 1) Parsing is more effective and accurate on short sentences. A word prefers to depend on another near to it. A sentence can be split to several sub sentences by commas and semicolons to get better parsing output. Result may be improved with a classifier to determine whether a comma or semicolon can be used to split the sentence. 2) Last character of word is a useful feature. In the experiment, the last character is coarsely used as lemma and a minor improvement is achieved. Much more language knowledge can be used in parsing. 3) The label set of the data is worthy to be reviewed. The meanings of the labels are not given in the task. Some of them are confusing especially the multi-level labels. The trying of training and testing multi-level labels separately by levels fails with a slightly decline of the score. Multi-level also causes too many labels: any single-level label can be prefixed to form a new multilevel label. It's a great problem for current parsers. Whether the label set is suitable to Chinese semantic dependency parsing should be discussed. 

 Conclusion and Future Work Three methods applied in NJU-Parser are described in this paper: splitting sentences by commas and semicolons, taking last character of word as lemma and handling multi-level labels. The first two get improvements in the experiments. Our primary system is a combination of the first two methods. The contrast system is the same as primary system except that it has a classifier implemented in CRF++ to determine whether a comma or a semicolon should be used to split the sentence. Both of the systems get improvements for about one percent on LAS. In the future, a better classifier should be developed to split the sentence. New method should be applied in merging split outputs to get a well formed dependency tree. And we hope there will be a better label set which are more capable of describing semantic dependency relations for Chinese. a) The proto sentence from train data b) The first sub sentence of a) 
