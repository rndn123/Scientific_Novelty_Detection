title
Nonparametric Bayesian Inference and Efficient Parsing for Tree-adjoining Grammars

abstract
In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.

Introduction There is a deep tension in statistical modeling of grammatical structure between providing good expressivity -to allow accurate modeling of the data with sparse grammars -and low complexity -making induction of the grammars (say, from a treebank) and parsing of novel sentences computationally practical. Tree-substitution grammars (TSG), by expanding the domain of locality of context-free grammars (CFG), can achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both. For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs  (Goodman, 2003) . Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar  (Cohn et al., 2009; Post and Gildea, 2009) . The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) "split them up", preventing their reuse, leading to less sparse grammars than might be ideal  (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992) . TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG)  (Joshi et al., 1975) . TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for "splicing in" of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments  (Joshi et al., 1975; XTAG Research Group, 2001) . Unfortunately, TAG's expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n 6 ), impractical as compared to CFG and TSG's O(n 3 ). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual  (Doran et al., 1997)  or heuristic techniques. For example, one can consider "outsourcing" the auxiliary trees  (Shieber, 2007) , use template rules and a very small number of grammar categories  (Hwa, 1998) , or rely on head-words and force lexicalization in order to constrain the problem  (Xia et al., 2001; Chiang, 2000; Carreras et al., 2008) . However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by  Cohn and Blunsom (2010)  argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work,  Shindo et al. (2011)  and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference. However TIG has the constraint of disallowing wrapping adjunction (coordination between material that falls to the left and right of the point of adjunction, such as parentheticals and quotations) as well as left adjunction along the spine of a right auxiliary tree and vice versa. In this work we formulate a blocked sampling strategy for TAG that is effective and efficient, and prove its superiority against the local Gibbs sampling approach. We show via nonparametric inference that TAG, which contains TSG as a subset, is a better model for treebank data than TSG and leads to improved parsing performance. TAG achieves this by using more compact grammars than TSG and by providing the ability to make finer-grained linguistic distinctions. We explain how our parameter refinement scheme for TAG allows for cubic-time CFG parsing, which is just as efficient as TSG parsing. Our presentation assumes familiarity with prior work on block sampling of TSG and TIG  (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012) . 

 Probabilistic Model In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP), each of which uses a base distribution P 0 that generates an initial tree by making stepwise decisions and concentration parameter ? c that controls the level of sparsity (size) of the generated grammars: G c ? DP(? c , P 0 (? | c)) We extend this model by adding specialized DPs for auxiliary trees G aux c ? DP(? aux c , P aux 0 (? | c)) Therefore, we have an exchangeable process for generating auxiliary tree a j given j ? 1 auxiliary trees previously generated p(a j | a <j ) = n c,a j + ? aux c P aux 0 (a j | c) j ? 1 + ? aux c (1) as for initial trees in TSG  (Cohn et al., 2009) . We must define base distributions for initial trees and auxiliary trees. P 0 generates an initial tree with root label c by sampling rules from a CFG P and making a binary decision at every node generated whether to leave it as a frontier node or further expand (with probability ? c )  (Cohn et al., 2009) . Similarly, our P aux 0 generates an auxiliary tree with root label c by sampling a CFG rule from P , flipping an unbiased coin to decide the direction of the spine (if more than a unique child was generated), making a binary decision at the spine whether to leave it as a foot node or further expand (with probability ? c ), and recurring into P 0 or P aux 0 appropriately for the off-spine and spinal children respectively. We glue these two processes together via a set of adjunction parameters ? c . In any derivation for every node labeled c that is not a frontier node or the root or foot node of an auxiliary tree, we determine the number (perhaps zero) of simultaneous adjunctions  (Schabes and Shieber, 1994)  by sampling a Geometric(? c ) variable; thus k simultaneous adjunctions would have probability (? c ) k (1 ? ? c ). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 

 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion  (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012) . As in previous work, we use a Goodman-transformed TAG as our proposal distribution  (Goodman, 2003)  that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution P aux 0 , and use the parsing algorithm described by  Shieber et al. (1995)  for computing inside probabilities under this TAG model. The algorithm is illustrated in Table  1  along with Figure  1 . Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down  (Johnson et al., 2007) . The N ? N ? N N N i . . . ? ? N 0 ? N 1 ? N 2 N 3 N 4 . . . ? ? N j ? N k N * ? N l ? N m N * ? Figure  1 : Example used for illustrating blocked sampling with TAG. On the left hand side we have a partial training tree where we highlight the particular nodes (with node labels 0, 1, 2, 3, 4) that the sampling algorithm traverses in post-order. On the right hand side is the TAG grammar fragment that is used to parse these particular nodes: one initial tree and two wrapping auxiliary trees where one adjoins into the spine of the other for full generality of our illustration. Grammar nodes are labeled with their Goodman indices (letters i, j, k, l, m). Greek letters ?, ?, ?, ? denote entire subtrees. We assume that a subtree in an auxiliary tree (e.g., ?) parses the same subtree in a training tree. sampler visits every node of the tree in post-order (O(n) operations, n being the number of nodes), visits every node below it as a potential foot (another O(n) operations), visits every mid-node in the path between the original node and the potential foot (if spine-adjunction is allowed) (O(log n) operations), and forms the appropriate chart items. The complexity is O(n 2 log n) if spine-adjunction is allowed, O(n 2 ) otherwise. 

 Parameter Refinement During inference, adjunction probabilities are treated simplistically to facilitate convergence. Only two parameters guide adjunction: ? c , the probability of adjunction; and p(a j | a <j , c) (see Equation  1 ), the probability of the particular auxiliary tree being adjoined given that there is an adjunction. In all of this treatment, c, the context of an adjunction, is the grammar category label such as S or NP, instead of a unique identifier for the node at which the adjunction occurs as was originally the case in probabilistic TAG literature. However it is possible to experiment with further refinement schemes at parsing time. Once the sampler converges on a grammar, we can reestimate its adjunction probabilities. Using the O(n 6 ) parsing algorithm  (Shieber et al., 1995)  we experimented with various refinements schemes -ranging from full node identifiers, to Goodman  ? N k [3-4] N * [4] and ? (1 ? ?c) ? ?(?) Nm[2-3] N * [3] and ? (1 ? ?c) ? ?(?) N l [1-3] ? and Nm[2-3] (1 ? ?c) ? ?(?) ?(Nm[2-3]) Naux[1-3] N l [1-3] nc,a l /(nc + ? aux c ) ?(N l [1-3]) N k [1-4] Naux[1-3] and N k [3-4] ?c ? ?(Naux[1-3]) ?(N k [3-4]) Nj [0-4] ? and N k [1-4] (1 ? ?c) ? ?(?) ?(N k [1-4]) Naux[0-4] Nj [0-4] nc,a j /(nc + ? aux c ) ?(Nj [0-4]) Ni[0] Naux[0-4] and Ni[4] ?c ? ?(Naux[0-4]) ?(Ni[4]) Table  1 : Computation of inside probabilities for TAG sampling. We create two types of chart items: (1) per-node, e.g., N i [?] denoting the probability of starting at an initial subtree that has Goodman index i and generating the subtree rooted at node ?, and (2) per-path, e.g., N j [?-?] denoting the probability of starting at an auxiliary subtree that has Goodman index j and generating the subtree rooted at ? minus the subtree rooted at ?. Above, c denotes the context of adjunction, which is the nonterminal label of the node of adjunction (here, N), ? c is the probability of adjunction, n c,a is the count of the auxiliary tree a, and n c = a n c,a is total number of adjunctions at context c. The function ?(?) retrieves the inside probability corresponding to an item. index identifiers of the subtree below the adjunction  (Hwa, 1998) , to simple grammar category labels -and find that using Goodman index identifiers as c is the best performing option. Interestingly, this particular refinement scheme also allows for fast cubic-time parsing, which we achieve by approximating the TAG by a TSG with little loss of coverage (no loss of coverage under special conditions which we find that are often satisfied) and negligible increase in grammar size, as discussed in the next section. 

 Cubic-time parsing MCMC training results in a list of sufficient statistics of the final derivation that the TAG sampler converges upon after a number of iterations. Basically, these are the list of initial and auxiliary trees, their cumulative counts over the training data, and their adjunction statistics. An adjunction statistic is listed as follows. If ? is any elementary tree, and ? is an auxiliary tree that adjoins n times at node ? of ? that is uniquely reachable at path p, we write ? p ? ? (n times). We denote ? alternatively as ? Now imagine that we end up with a small grammar that consists of one initial tree ? and two auxiliary trees ? and ?, and the following adjunctions occurring between them [p]. * q ! p " n m k # * p " i i i q ! i k # * m i " i # i i i # j j j q ! i j i j ! ij i (1) (2) (3) ? p ? ? (n times) ? p ? ? (m times) ? q ? ? (k times) as shown in Figure  2 . Assume that ? itself occurs l > n + m times in total so that there is nonzero probability of no adjunction anywhere within ?. Also assume that the node uniquely identified by ?[p] has Goodman index i, which we denote as i = G(?[p]). The general idea of this TAG-TSG approximation is that, for any auxiliary tree that adjoins at a node ? with Goodman index i, we create an initial tree out of it where the root and foot nodes of the auxiliary tree are both replaced by i. Further, we split the subtree rooted at ? from its parent and rename the substitution site that is newly created at ? as i as well. (See Figure  2 .) We can separate the foot subtree from the rest of the initial tree since it is completely remembered by any adjoined auxiliary trees due to the nature of our refinement scheme. However this method fails for adjunctions that occur at spinal nodes of auxiliary trees that have foot nodes below them since we would not know in which order to do the initial tree creation. However when the spine-adjunction relation is amenable to a topological sort (as is the case in Figure  2 ), we can apply the method by going in this order and doing some extra bookkeeping: updating the list of Goodman indices and redirecting adjunctions as we go along. When there is no such topological sort, we can approximate the TAG by heuristically dropping low-frequency adjunctions that introduce cycles.  1  The algorithm is illustrated in Figure  2 . In (1) we see the original TAG grammar and its adjunctions (n, m, k are adjunction counts). Note that the adjunction relation has a topological sort of ?, ?, ?. We process auxiliary trees in this order and iteratively remove their adjunctions by creating specialized initial tree duplicates. In (2) we first visit ?, which has adjunctions into ? at the node denoted ?[p] where p is the unique path from the root to this node. We retrieve the Goodman index of this node i = G(?[p]), split the subtree rooted at this node as a new initial tree ? i , relabel its root as i, and rename the newly-created substitution site at ?[p] as i. Since ? has only this adjunction, we replace it with initial tree version ? i where root/foot labels of ? are replaced with i, and update all adjunctions into ? as being into ? i . In (3) we visit ? which now has adjunctions into ? and ? i . For the ?[p] adjunction we create ? i the same way we created ? i but this time we cannot remove ? as it still has an adjunction into ? i . We retrieve the Goodman index of the node of adjunction j = G(? i [q]), split the subtree rooted at this node as new initial tree ? ij , relabel its root as j, and rename the newly-created substitution site at ? i [q] as j. Since ? now has only this adjunction left, we remove it by also creating initial tree version ? j where root/foot labels of ? are replaced with j. At this point we have an adjunctionfree TSG with elementary trees (and counts) ?(l), ? i (l), ? i (n), ? ij (n), ? i (m), ? j (k) where l is the count of initial tree ?. These counts, when they are normalized, lead to the appropriate adjunc- Although this algorithm increases grammar size, the sparsity of the nonparametric solution ensures that the increase is almost negligible: on average the final Goodman-transformed CFG has 173.9K rules for TSG, 189.2K for TAG. Figure  3  demonstrates the comparable Viterbi parsing times for TSG and TAG. 

 Evaluation We use the standard Penn treebank methodology of training on sections 2-21 and testing on section 23. All our data is head-binarized, all hyperparameters are resampled under appropriate vague gamma and beta priors. Samplers are run 1000 iterations each; all reported numbers are averages over 5 runs. For simplicity, parsing results are based on the maximum probability derivation (Viterbi algorithm). In Table  4 , we compare TAG inference schemes and TSG. TAG Gibbs operates by locally adding/removing potential adjunctions, similar to  Cohn et al. (2009) . TAG is the O(n 2 ) algorithm that disallows spine adjunction. We see that TAG has the best parsing performance, while TAG provides the most compact representation.   

 Conclusion We described a nonparametric Bayesian inference scheme for estimating TAG grammars and showed the power of TAG formalism over TSG for returning rich, generalizable, yet compact representations of data. The nonparametric inference scheme presents a principled way of addressing the difficult model selection problem with TAG. Our sampler has near quadratic-time efficiency, and our parsing approach remains context-free allowing for fast cubic-time parsing, so that our overall parsing framework is highly scalable. 2 There are a number of extensions of this work: Experimenting with automatically induced adjunction refinements as well as incorporating substitution refinements can benefit Bayesian TAG  (Shindo et al., 2012; Petrov et al., 2006) . We are also planning to investigate TAG for more context-sensitive languages, and synchronous TAG for machine translation.  Figure 2 : 2 Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end up with a larger yet adjunction-free TSG. 
