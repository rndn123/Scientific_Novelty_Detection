title
Semantic Parsing to Probabilistic Programs for Situated Question Answering

abstract
Situated question answering is the problem of answering questions about an environment such as an image or diagram. This problem requires jointly interpreting a question and an environment using background knowledge to select the correct answer. We present Parsing to Probabilistic Programs (P 3 ), a novel situated question answering model that can use background knowledge and global features of the question/environment interpretation while retaining efficient approximate inference. Our key insight is to treat semantic parses as probabilistic programs that execute nondeterministically and whose possible executions represent environmental uncertainty. We evaluate our approach on a new, publicly-released data set of 5000 science diagram questions, outperforming several competitive classical and neural baselines.

Introduction Situated question answering is a challenging problem that requires reasoning about uncertain interpretations of both a question and an environment together with background knowledge to determine the answer. To illustrate these challenges, consider the 8th grade science diagram questions in Figure  1 , which are motivated by the Aristo project  (Clark and Etzioni, 2016) . These questions require both computer vision to interpret the diagram and compositional question understanding. These components, being imperfect, introduce uncertainty that must be jointly reasoned about to avoid implausible interpretations. These uncertain interpretations must further  be combined with background knowledge, such as the definition of a "predator," to determine the correct answer. The challenges of situated question answering have not been completely addressed by prior work. Early "possible worlds" models  (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014)  were capable of compositional question understanding and using background knowledge, but did not jointly reason about environ-ment/question uncertainty. These models also used unscalable inference algorithms for reasoning about the environment, despite the lack of joint reasoning. More recent neural models  (Antol et al., 2015; Malinowski et al., 2015; Yang et al., 2015)  are incapable of using background knowledge and it remains unclear to what extent these models can represent compositionality in language. We present Parsing to Probabilistic Programs (P 3 ), a novel approach to situated question answering that addresses these challenges. It is motivated by two observations: (  1 ) situated question answering can be formulated as semantic parsing with an execution model that is a learned function of the environment, and (2) probabilistic programming is a natural and powerful method for specifying the space of permissible execution models and learning over it. In P 3 , we define a domain theory for the task as a probabilistic program, then train a joint loglinear model to semantically parse questions to logical forms in this theory and execute them in an environment. Importantly, the model includes global features over parsing and execution that enable it to avoid unlikely joint configurations. P 3 leverages semantic parsing to represent compositionality in language and probabilistic programming to specify background knowledge and perform linear-time approximate inference over the environment. We present an experimental evaluation of P 3 on a new data set of 5000 food web diagram questions (Figure  1 ). We compare our approach to several baselines, including possible worlds and neural network approaches, finding that P 3 outperforms both. An ablation study demonstrates that global features help the model achieve high accuracy. We also demonstrate that P 3 improves accuracy on a previously published data set. Finally, we have released our data and code to facilitate further research. 

 Prior Work Situated question answering is often formulated in terms of parsing both the question and environment into a common meaning representation where they can be combined to select the answer. This general approach has been implemented using different meaning representations: Possible world models use a logical meaning representation defined by a knowledge base schema. These models train a semantic parser to map questions to queries and an environment model to map environments to knowledge bases in this schema. Executing the queries against the knowledge bases produces answers. These models assume that the parser and environment model are independent and furthermore that the knowledge base consists of independent predicate instances  (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013; Malinowski and Fritz, 2014) . Despite these strong independence assumptions, these models have intractable inference. An exception is  Seo et al., (2015)  who incorporate hard constraints on the joint question/environment interpretation; however, this approach does not generalize to soft constraints or arbitrary logical forms. In some work only the environment model is learned  (Kollar et al., 2010; Tellex et al., 2011; Howard et al., 2014b; Howard et al., 2014a; Berant et al., 2014; Krishnamurthy and Mitchell, 2015) . Neural networks use a vector meaning representation that encodes both the question and environment as vectors. These networks have mostly been applied to visual question answering  (Antol et al., 2015) , where many architectures have been proposed  (Malinowski et al., 2015; Yang et al., 2015; Fukui et al., 2016) . It is unclear to what extent these networks can represent compositionality in language using their vector encodings. Dynamic Neural Module Networks  (Andreas et al., 2016a; Andreas et al., 2016b)  are the exception to the above generalization. This approach constructs a neural network to represent the meaning of the question via semantic parsing, then executes this network against the image to produce an answer. Our approach is similar except that we construct and execute a probabilistic program. Advantages of our approach are that it naturally represents the discrete structure of food webs and can use background knowledge. Preliminaries for our work are semantic parsing and probabilistic programming. Semantic parsing translates natural language questions into executable logical forms and has been used in applications such as question answering against a knowledge base  (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Liang et al., 2011; Kwiatkowski et al., 2013; Berant et al., 2013; Reddy et al., 2014; Yih et al., 2015; Xu et al., 2016) , direction following  (Chen and Mooney, 2011; , and information extraction  (Krishnamurthy and Mitchell, 2012; Choi et al., 2015) . Semantic parsing alone is insufficient for situated question answering because it does not interpret the environment; many of the above approaches use semantic parsing as a component in a larger model. Probabilistic programming languages extend programming languages with primitives for nondeterministic choice  (McCarthy, 1963; Goodman and Stuhlm?ller, 2014) . We express logical forms in a probabilistic variant of Scheme similar to Church  (Goodman et al., 2008) ; however, this paper uses Python-like pseudocode for clarity. The language has a single choice primitive called choose that nondeterministically returns one of its arguments. For example choose(1,2,3) can execute three ways, returning either 1, 2, or 3. Multiple calls to choose can be combined. For example, choose(1,2)+choose(1,2) adds two nondeterministically chosen values, and therefore has four executions that return 2, 3, 3 and 4. Each execution also has a probability; in our case, these probabilities are assigned by a trained model given the environment and not explicitly specified in the program. 3 Parsing to Probabilistic Programs (P 3 ) The P 3 model is motivated by two observations. The first is that situated question answering can be formulated as semantic parsing with an execution model that is a learned function of the environment. Consider the first question in Figure  1 . The meaning of this question could be represented by a logical form such as COUNT(?x.EATS(x, DEER)), which we could train a semantic parser to predict given a suitable domain theory of functions such as COUNT and EATS. However, the information required to execute this logical form and answer the question must be extracted from the diagram. Specifically, EATS(x, y) depends on whether an arrow is present between x and y, which we must train a vision model to determine. Thus, EATS should be a learned function of the environment. This first observation suggests a need for a formalism for representing uncertainty and performing learning over the domain theory's functions. Our second observation is that probabilistic program-ming is a natural fit for this task. In this paradigm, the domain theory is a probabilistic program that defines the information to be extracted from the environment by using choose. To a first approximation, the diagram question theory includes: def eats(x, y) choose(true, false) Logical forms are then probabilistic programs, each of whose possible executions represents a different interpretation of the environment. For example, executing EATS(LION, DEER) hits the choose in the above definition, resulting in two executions where the lion either eats or does not eat the deer. In the COUNT example above, each execution represents a different set of animals that eat the deer. To learn the correct environment interpretation, we train an execution model to assign a probability to each execution given features of the environment. Using probabilistic programming enables us to combine learned functions, such as EATS, with background knowledge functions, such as COUNT, and also facilitates inference. According to these observations, applying P 3 has two steps. The first step is to define an appropriate domain theory. This theory is the main design decision in instantiating P 3 and provides a powerful way to encode domain knowledge. The second step is to train a loglinear model consisting of a semantic parser and an execution model. This model learns to semantically parse questions into logical forms in the theory and execute them in the environment to answer questions correctly. We defer discussion of the diagram question domain theory to Section 4 and focus on the loglinear model in this section. 

 Model Overview The input to the P 3 model is a question and an environment and its output is a denotation, which is a formal answer to the question. P 3 is a loglinear model with two factors: a semantic parser and an execution model. The semantic parser scores syntactic parses and logical forms for the question. These logical forms are probabilistic programs with multiple possible executions (specified by the domain theory), each of which may return a different denotation. The execution model assigns a score to each of these executions given the environment. Formally, the model predicts a denotation ? for a question q in an environment v using three latent variables: P (?|v, q; ?) = e, ,t P (e, , t|v, q; ?)1(ret(e) = ?) P (e, , t|v, q; ?) = 1 Z q,v f ex (e, , v; ? ex )f p ( , t, q; ? p ) The model is composed of two factors. f p represents the semantic parser that scores logical forms and syntactic parse trees t given question q and parameters ? p . f ex represents the execution model. Given parameters ? ex , this factor assigns a score to a logical form and its execution e in environment v. The denotation ?, i.e., the formal answer to the question, is simply the value returned by e. Z q,v represents the model's partition function. The following sections describe these factors in more detail. 

 Semantic Parser The factor f p represents a Combinatory Categorial Grammar (CCG) semantic parser  (Zettlemoyer and Collins, 2005)  that scores logical forms for a question. Given a lexicon 1 mapping words to syntactic categories and logical forms, CCG defines a set of possible syntactic parses t and logical forms for a question q. Figure 3.2 shows an example CCG parse. f p is a loglinear model over parses ( , t): f p ( , t, q; ? p ) = exp{? T p ?( , t, q)} The function ? maps parses to feature vectors. We use a rich set of features similar to those for syntactic CCG parsing  (Clark and Curran, 2007) ; a full description is provided in an online appendix. 

 Execution Model The factor f ex is a loglinear model over the executions of a logical form given an environment. Logical forms in P 3 are probabilistic programs with a set of possible executions, where each execution e is a sequence, e = [e 0 , e 1 , e 2 , ..., e n ]. e 0 is the program's starting state, e i represents the state immediately after the ith call to choose, and e n is the state at termination. The score of an execution is: f ex (e, , v; ? ex ) = n i=1 exp{? T ex ?(e i?1 , e i , , v)} In the above equation, ? ex represents the model's parameters and ? represents a feature function that produces a feature vector for the difference between sequential program states e i?1 and e i given environment v and logical form . ? can include arbitrary features of the execution, logical form and environment, which is important, for example, to detect cycles in a food web (Section 4.3). 

 Inference P 3 is designed to rely on approximate inference: our goal is to use rich features to accurately make local decisions, as in linear-time parsers  (Nivre et al., 2006) . We perform approximate inference using a two-stage beam search. Given a question q, the first stage performs a beam search over CCG parses to produce a list of logical forms scored by f p . This step is performed by using a CKY-style chart parsing algorithm then marginalizing out the syntactic parses. The second stage performs a beam search over executions of each logical form. The space of possible executions of a logical form is a tree (Figure  4 .2) where each internal node represents a partial execution up to a choose call. The search maintains a beam of partial executions at the same depth, and each iteration advances the beam to the next depth, discarding the lowestscoring executions according to f ex to maintain a fixed size beam. This procedure runs in time linear to the number of choose calls. We implement the search by rewriting the probabilistic program into continuation-passing style, which allows choose to be implemented as a function that adds multiple continuations to the search queue; we refer the reader to  Goodman and Stuhlm?ller (2014)  for details. Our experiments use a beam size of 100 in the semantic parser, executing each of the 10 highest-scoring logical forms with a beam of 100 executions. 

 Training P 3 is trained by maximizing loglikelihood with stochastic gradient ascent. The training data {(q i , v i , c i )} n i=1 is a collection of questions q i and environments v i paired with supervision oracles c i . c i (e) = 1 for a correct execution e and c i (e) = 0 otherwise. The oracle c i can implement various kinds of supervision, including: (1) labeled denotations, by verifying the value returned by e and (2) labeled environments, by verifying each choice made by e. The oracle for diagram question answering combines both forms of supervision (Section 4.5). The objective function O is the loglikelihood of predicting a correct execution: O(?) = n i=1 log e,l,t c i (e)P (e, , t|q i , v i ; ?) We optimize this objective function using stochastic gradient ascent, using the approximate inference algorithm from Section 3.4 to estimate the necessary marginals. When computing the marginal distribution over correct executions, we filter each step of the beam search using the supervision oracle c i to improve the approximation. 

 Diagram Question Answering with P 3 As a case study, we apply P 3 to the task of answering food web diagram questions from an 8th grade science domain. A few steps are required to apply P 3 . First, we create a domain theory of food webs that represents extracted information from the diagram and background knowledge for the domain. Second, we define the features of the execution model that are used to learn how programs in the domain theory execute given a diagram. Third, we define a component to select a multiple-choice answer given a denotation. Finally, we define the supervision oracle used for training. 

 Food Web Diagram Questions We consider the task of answering food web diagram questions. The input consists of a diagram depicting a food web, a natural language question and a list of natural language answer options (Figure  1 ). The goal is to select the correct answer option. This task has many regularities that require global features: for example, food webs are usually acyclic and certain animals usually have certain roles (e.g., mice are herbivores). We have collected and released a data set for this task (Section 5.1). We preprocess the diagrams in the data set using a computer vision system that identifies candidate diagram elements  (Kembhavi et al., 2016) . This system extracts a collection of text labels (via OCR), arrows, arrowheads and objects, each with corresponding scores. It also extracts a collection of scored linkages between these elements. These extractions are noisy and contain many discrepancies such as overlapping text labels and spurious linkages. We use these extractions to define a set of candidate organisms (using the text labels), and also to define features of the execution model. 

 Domain Theory The domain theory is a probabilistic program encoding the information to extract from the environment as well as background knowledge about food webs. It represents the structure of a food web using two functions. These functions are predicates that invoke choose to return either true or false. The execution model learns to predict which of these values is correct for each set of arguments given the diagram. It furthermore has a collection of deterministic functions that encode domain knowledge, including definitions of animal roles such as HERBIVORE and a model of population change causation. Figure  4 .2 shows pseudocode for a portion of the domain theory. Food webs are represented using two functions over the extracted text labels: ORGANISM(x) indicates whether the label x is an organism (as opposed to, e.g., the diagram title); and EATS(x, y). The definitions of these functions invoke choose while remembering previously chosen values to avoid double counting probabilities when executing logical forms such as ORGANISM(DEER) ? ORGANISM(DEER). The remembered values are stored in a global variable that is also used to implement the supervision oracle. Deterministic functions such as CAUSE are defined in terms of these learned functions. The uses of choose in the domain theory create a tree of possible executions for every logical form. Figure  4   corresponds to the question "what happens to the snakes when the mice decrease?" This logical form is shorthand for the following program: filter(lambda f.cause( decrease(getOrganism("mice")), f(getOrganism("snakes"))), set(decrease, increase, unchanged)) Specifically, entities such as MICE are created by calling getOrganism and logical forms with functional types implicitly represent filters over the appropriate argument type. Executing this program first applies the filter predicate to decrease. Next, it evaluates getOrganism("mice"), which calls organism and encounters the first call to choose. This call is shown as the first branch of the tree in Figure  4 .2. The successful branch proceeds to evaluate getOrganism("snakes"), shown as the second branch. Finally, the successful branch evaluates cause, which calls eats twice, resulting in the final two branches. The value returned by each branch is determined by the causation model which performs some deterministic logic on the truth values of the two eats relations. 

 Execution Features The execution model uses three sets of features: instance features, predicate features, and denotation features. Instance features treat each predicate instance independently, while the remainder are global features of multiple predicate instances and the logical form. We provide a complete listing of features in an online appendix. Instance features fire whenever an execution chooses a truth value for a predicate instance. These features are similar to the per-predicate-instance features used in prior work to produce a distribution over possible worlds. For ORGANISM(x), our features are the vision model's extraction score for x and indicator features for the number of tokens in x. For EATS(x, y), our features are various combinations of the vision model's scores for arrows that may connect the text labels x and y. Predicate features fire based on the global assignment of truth values to all instances of a single predicate. The features for ORGANISM count occurrences of overlapping text labels among true instances. The features for EATS include cycle count features for various cycle lengths and arrow reuse features. The cycle count features help the model learn that food webs are typically, but not always, acyclic and the arrow reuse features aim to prevent the model from predicting two different EATS instances on the basis of a single arrow. Denotation features fire on the return value of an execution. There are two kinds of denotation features: size features that count the number of entities in denotations of various types and denotation element features for specific logical forms. The second kind of feature can be used to learn that the denotation of ?x.HERBIVORE(x) is likely to contain MOUSE, but unlikely to contain WOLF. 

 Answer Selection P 3 predicts a distribution over denotations for each question, which for our problem must be mapped to a distribution over multiple choice answers. Answer selection performs this task using string match heuristics and an LSTM  (Hochreiter and Schmidhuber, 1997) . The string match heuristics score each answer option given a denotation then select the highest scoring answer, abstaining in the case of a tie. The score computation depends on the denotation's type. If the denotation is a set of entities, the score is an approximate count of the number of entities in the denotation mentioned in the answer using a fuzzy string match. If the denotation is a set of change events, the score is a fuzzy match of both the change direction and the animal name. If the denotation is a number, string matching is straightforward. Applying these heuristics and marginalizing out denotations yields a distribution over answer options. A limitation of the above approach is that it does not directly incorporate linguistic prior knowledge about likely answers. For example, "snake" is usually a good answer to "what eats mice?" regardless of the diagram. Such knowledge is known to be essential for visual question answering  (Antol et al., 2015; Andreas et al., 2016b)  and important in our task as well. We incorporate this knowledge in a standard way, by training a neural network on question/answer pairs (without the diagram) and combining its predictions with the string match heuristics above. The network is a sequence LSTM that is applied to the question concatenated with each answer option a to produce a 50-dimensional vector v a for each answer. The distribution over answers is the softmax of the inner product of these vectors with a learned parameter vector w. For simplicity, we combine these two components using a 50/50 mix of their answer distributions. 

 Supervision Oracle The supervision oracle for diagram question answering combines supervision of both answers and environment interpretations. We assume that each diagram has been labeled with a food web. An execution is correct if and only if (1) all of the chosen values in the global variable encoding the food web are consistent with the labeled food web, and (  2 ) string match answer selection applied to its denotation chooses the correct answer. The first constraint guarantees that every logical form has at most one correct execution for any given diagram. 

 Evaluation Our evaluation compares P 3 to both possible worlds and neural network approaches on our data set of food web diagram questions. An ablation study demonstrates that both sets of global features improve accuracy. Finally, we demonstrate P 3 's generality by applying it to a previously-published data set, obtaining state-of-the-art results. Code, data and supplementary material for this paper are available at: http://www.allenai. org/paper-appendix/emnlp2016-p3 

 FOODWEBS Data Set FOODWEBS consists of ?500 food web diagrams and ?5000 questions designed to imitate actual questions encountered on 8th grade science exams. The train/validation/test sets contain ?300/100/100 diagrams and their corresponding questions. The data set has three kinds of annotations in addition to the correct answer for each question. First, each diagram is annotated with the food web that it depicts using ORGANISM and EATS. Second, each diagram has predictions from a vision system for various diagram elements such as arrows and text labels  (Kembhavi et al., 2016) . These are noisy predictions, not ground truth. Finally, each question is annotated by the authors with a logical form (or null if its meaning is not representable in the domain theory). These logical forms are not used to train P 3 but are useful to measure per-component error. We collected FOODWEBS by using a crowdsourcing process to expand a collection of real exam questions. First, we collected 89 questions from 4th and 8th grade exams and 500 food web diagrams us-ing an image search engine. Second, we generated questions for these diagrams using Mechanical Turk. Workers were shown a diagram and a real question for inspiration and asked to write a new question and its answer options. We validated each generated question by asking 3 workers to answer it, discarding questions where at least 2 did not choose the correct answer. We also manually corrected any ambiguous (e.g., two answer options are correct) and poorly-formatted (e.g., two answer options have the same letter) questions. The final data set has high quality: a human domain expert correctly answered 95 out of 100 randomly-sampled questions. 

 Baseline Comparison Our first experiment compares P 3 with several baselines for situated question answering. The first baseline, WORLDS, is a possible worlds model based on  Malinowski and Fritz (2014) . This baseline learns a semantic parser P ( , t|q) and a distribution over food webs P (w|v), then evaluates on w to produce a distribution over denotations. This model is implemented by independently training P 3 's CCG parser (on question/answer pairs and labeled food webs) and a possible-worlds execution model (on labeled food webs). The CCG lexicon for both P 3 and WORLDS was generated by applying PAL  (Krishnamurthy, 2016)  to the same data. Both models select answers as described in Section 4.4. We also compared P 3 to several neural network baselines. The first baseline, LSTM, is the textonly answer selection model described in Section 4.4. The second baseline, VQA, is a neural network for visual question answering. This model represents each image as a vector by using the final layer of a pre-trained VGG19 model  (Simonyan and Zisserman, 2014)  and applying a single fullyconnected layer. It scores answer options by using the answer selection LSTM to encode question/answer pairs, then computing a dot product between the text and image vectors. This model is somewhat limited because VGG features are unlikely to encode important diagram structure, such as the content of text labels. Our third baseline, DQA, is a neural network that rectifies this limitation  (Kembhavi et al., 2016) . It encodes the diagram predictions from the vision system as vectors and attends to them using the LSTM-encoded question vector to select an an- swer. This model is trained with question/answer pairs and diagram parses, which is roughly comparable to the supervision used to train P 3 . Table  5 .2 compares the accuracy of P 3 to these baselines. Accuracy is the fraction of questions answered correctly. LSTM performs well on this data set, suggesting that many questions can be answered without using the image. This result is consistent with results on visual question answering  (Antol et al., 2015) . The other neural network models have similar performance to LSTM, whereas both WORLDS and P 3 outperform it. We also find that P 3 outperforms WORLDS likely due to its global features, which we investigate in the next section. Given these results, we hypothesized that the neural models were largely memorizing common patterns in the text and were not able to interpret the diagram. We tested this hypothesis by running each model on a test set with unseen organisms created by reversing the organism names in every question and diagram (Table  5 .2, right column). As expected, the accuracy of LSTM is considerably reduced on this data set. VQA and DQA again perform similarly to LSTM, which is consistent with our hypothesis. In contrast, we find that the accuracies of WORLDS and P 3 are only slightly reduced, which is consistent with superior diagram interpretation abilities but ineffective LSTM answer selection. 

 Ablation Study We performed an ablation study to further understand the impact of LSTM answer selection and global features. Table  5 .2 shows the accuracy of P 3 trained without these components. We find that LSTM answer selection improves accuracy by 9 points, as expected due to the importance of linguistic prior knowledge. Global features improve accuracy by 7 points, which is roughly comparable to the delta between P 3 and WORLDS in Table  5 .2. 

 Component Error Analysis Our third experiment analyses sources of error by training and evaluating P 3 while providing the gold logical form, food web, or both as input. Table  5 .5 shows the accuracy of these three models. The final entry shows the maximum accuracy possible given our domain theory and answer selection. The larger accuracy improvement with gold food webs suggests that the execution model is responsible for more error than semantic parsing, though both components contribute. 

 SCENE Experiments Our final experiment applies P 3 to the SCENE data set of  Krishnamurthy and Kollar (2013) . In this data set, the input is a natural language expression, such as "blue mug to the left of the monitor," and the output is the set of objects in an image that the expression denotes. The images are annotated with a bounding box for each candidate object. The data set includes a domain theory that was automatically generated by creating a category and/or relation per word based on its part of speech. It also includes a CCG lexicon and image features. We use these resources, adding predicate and denotation features. Table  5 .5 compares P 3 to prior work on SCENE. The evaluation metric is exact match accuracy between the predicted and labeled sets of objects. We consider three supervision conditions: QA trains with question/answer pairs, QA+E further includes labeled environments, and QA+E+LF further includes labeled logical forms. We trained P 3 in the first two conditions, while prior work trained in the first and third conditions. KK2013 is a possible worlds model with a max-margin training objective. P 3 slightly outperforms in the QA condition and P 3 trained with labeled environments outperforms prior work trained with additional logical form labels. 

 Model 

 Conclusion Parsing to Probabilistic Programs (P 3 ) is a novel model for situated question answering that jointly reasons about question and environment interpretations using background knowledge to produce answers. P 3 uses a domain theory -a probabilistic program -to define the information to be extracted from the environment and background knowledge. A semantic parser maps questions to logical forms in this theory, which are probabilistic programs whose possible executions represent possible interpretations of the environment. An execution model scores these executions given features of the environment. Both the semantic parser and execution model are jointly trained in a loglinear model, which thereby learns to both parse questions and interpret environments. Importantly, the model includes global features of the logical form and executions, which help the model avoid implausible interpretations. We demonstrate P 3 on a challenging new data set of 5000 science diagram questions, where it outperforms several competitive baselines. 1. According to the given food chain, what is the number of organisms that eat deer? (A) 3 (B) 2 (C) 4 (D) 1 2. Which organism is both predator and prey? (A) Bark Beetles (B) Insect-eating birds (C) Deer (D) Hawks 3. Based on the given food web, what would happen if there were no insect-eating birds? (A) The grasshopper population would increase. (B) The grasshopper population would decrease. (C) There would be no change in grasshopper number. 
