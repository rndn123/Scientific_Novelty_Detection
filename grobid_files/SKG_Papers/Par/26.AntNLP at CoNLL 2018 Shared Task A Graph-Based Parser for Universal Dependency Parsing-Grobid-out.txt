title
AntNLP at CoNLL 2018 Shared Task: A Graph-based Parser for Universal Dependency Parsing

abstract
We describe the graph-based dependency parser in our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task. We use bidirectional lstm to get the word representation, then a bi-affine pointer networks to compute scores of candidate dependency edges and the MST algorithm to get the final dependency tree. From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26).

Introduction The focus of the CoNLL 2018 UD Shared Task is learning syntactic dependency parsers that can work over many typologically different languages, even low-resource languages for which there is little or no training data. The Universal Dependencies  (Nivre et al., 2017a,b)  treebank collection has 82 treebanks over 57 kinds of languages. In this paper we describe our system (AntNLP) submitted to the CoNLL 2018 UD Shared Task. Our system is based on the deep biaffine neural dependency parser  (Dozat and Manning, 2016) . The system contains a BiLSTM feature extractor for getting context-aware word representation and two biaffine classifiers to predict the head token of each word and the label between a head and its dependent. There are three main metrics for this task, LAS (labeled attachment score), MLAS (morphologyaware labeled attachment score) and BLEX (bilexical dependency score). From the official testing results, our system gets 70.90 LAS F1 score (rank 9/26), 55.92 MLAS (10/26) and 60.91 BLEX (8/26). In a word, Our system is ranked top 10 according to the three metrics described above. Additionally, in the categories of small treebanks, our system obtains the sixth place with a MLAS score of 63.73. Besides that, our system ranked tenth in the EPE 2018 campaign with a 55.71 F1 score. The rest of this paper is organized as follows. Section 2 gives a brief description of our overall system, including the system framework and parser architecture. In Section 3, 4 we describe our monolingual model and multilingual model. In Section 5, we briefly list our experimental results. 

 System Overview The CoNLL 2018 UD Shared Task aims to construct dependency trees based on raw texts, which means that the participants should not only build the parsing model, but also preprocess systems of the sentence segmentation, tokenization, POStagging and morphological analysis. We use preprocessors from the official UDPipe tool in our submission. The structure of the entire system is shown in Figure  1 . Our main focus is on building a graph-based parser. We implement a graph-based bi-affine parser following  Dozat and Manning (2016) . The parser architecture is shown in Figure  2 , which consists of the following components: ? Token representation, which produces the context independent representation of each token in the sentence. ? Deep Bi-LSTM Encoder, which produces the context-aware representation of each token in the sentence based on context. ? Bi-affine Pointer Networks, which assign probabilities to all possible candidate edges. We describe the three sub-modules in the following sections in detail. 

 Token representation Recent studies on dependency parsing show that densely embedded word representation could help to improve empirical parser performance. For example: Chen and Manning (2014) map words and POS tags to a d-dimensional vector space.  Dozat and Manning (2016)  use the pre-trained GloVe embeddings as an extra representation of the word.  Ma et al. (2018)  use Convolutional Neural Networks (CNNs) to encode character-level information of a word. The token representation module of our parser also uses dense embedding representations. Details on token embeddings are given in the following. ? Word e w i : The word embedding is randomly initialized from the normal distribution N (0, 1) (e w i ? R 100 ). ? Lemma e l i : The lemma embedding is randomly initialized from the normal distribution N (0, 1) (e l i ? R 100 ). ? Pre-trained Word e pw i : The FastText pretrained word embedding (e pw i ? R 300 ). We will not update e pw i during the training process. ? UPOS e u i : The UPOS-tag embedding is randomly initialized from the normal distribution N (0, 1) (e u i ? R 100 ). ? XPOS e x i : The XPOS-tag embedding is randomly initialized from the normal distribution N (0, 1) (e x i ? R 100 ). ? Char e c i : The character-level embedding is obtained by the character-level CNNs (e c i ? R 64 ). Our parser uses two kinds of token representations, one is a lexicalized representation of the monolingual model, another one is the delexicalized representation of the multilingual model. The lexicalized representation x l i of token w i is defined as: x l i = [e w i + e l i ; word e u i + e x i P OS ; e pw i ; e c i ] (1) and the delexicalized representation x d i of token w i is defined as: x d i = [e u i ; e x i ; e c i ] (2) In the following sections, we uses x i to represent x l i or x d i when the context is clear. 

 Deep Bi-LSTM Encoder Generally, the token embeddings defined above are context independent, which means that the sentence-level information is ignored. In recent years, some work shows that the deep BiLSTM can effectively capture the contextual information of words  (Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016; Ma et al., 2018) . In order to encode context features, we use a 3layer sentence level BiLSTM on top of x 1:n : h t = LSTM( h t?1 , x i , ?) h t = LSTM( h t+1 , x i , ?) v i = h i ? h i ? are the model parameters of the forward hidden sequence h. ? are the model parameters of the backward hidden sequence h. The vector v i is our final vector representation of ith token in s, which takes into account both the entire history h i and the entire future h i by concatenating h i and h i . 

 Biaffine Pointer Networks How to determine the probability of each dependency edge is an important part of the graphbased parser. The work of  Dozat and Manning (2016)  shows that the biaffine pointer (attention) networks can calculate the probability of each dependency edge well. Here we used a similar biaffine pointer network structure. In order to better represent the direction of the dependency edges, we use multi-layer perceptron (MLP) networks to learn each word as the representation of head and dependent words, rather than simply exchanging feature vectors. And we also separate the predictions of dependent edges and their labels. First, for each v i , we use two MLPs to define two pointers h arc i and s arc i , which is the representation of v i with respect to whether it is seen as a head or a modifier of an candidate edge. h arc i = MLP (arc) head (v i ) s arc i = MLP (arc) dep (v i ) Similarly, we use h rel i and s rel to describe x i when determine the relation label of a candidate edge. h rel i = MLP (rel) head (v i ) s rel i = MLP (rel) dep (v i ) We first use the arc-biaffine pointer networks to predict the probability of a dependency edge between any two words. For any two words w i and w j in a sentence, the probability p (arc) i?j that they form a dependency edge w i ? w j is as follows: a (arc) i?j = h arc i ? W (arc) ? s arc j + u (arc) ? s arc j p (arc) i?j = softmax(a (arc) i? * )[j] where ? arc = {W  (arc)  , u  (arc)  } are the model parameters, a  w i ? w j ? T is then computed. The definition of p (rel) i r ? ?j is as follows: a (rel) i * ? ?j = h rel i ? W (rel) ? s rel j + V (rel) ? h rel i + U (rel) ? s rel j p (rel) i r ? ?j = softmax(a (rel) i * ? ?j )[r] where ? rel = {W  (rel)  , V  (rel)  , U (rel) } are the model parameters, W (rel) is a 3-dimensional tensor. a  

 Training Details We train our model by minimizing the negative log likelihood of the gold standard (w i r(w i ) ? ? ? w h(w i ) ) arcs in all training sentences: J (arc) = ? 1 |? | S? N S i=1 log p (arc) i?h(w i ) J (rel) = ? 1 |? | S? N S i=1 log p (rel) i r(w i ) ? ? ?h(w i ) J = J (arc) + J (rel) where ? is the training set, h(w i ) and r(w i ) is w i 's gold standard head and relation within sentence S, and N s is the number of words in S.  

 Monolingual Model There are 82 treebanks in the CoNLL 2018 UD Shared Task, including 61 big treebanks, 5 PUD treebanks (additional parallel test sets), 7 small treebanks and 9 low-resource language treebanks. There are several languages in which there are many treebanks, such as en ewt, en gum and en lines in English. We combine training sets and development sets for multiple treebanks of the same language. And then just train a model for the language and make predictions on its different treebanks. For each language of UD version 2.2 sets  (Nivre et al., 2018; Zeman et al., 2018)  with both a training set and a development set, we train a parser using lexicalized token representation and only using its monolingual training set (no cross-lingual features) 1 . The architecture of the monolingual model is shown in Figure  3 . 

 Multilingual Model For 7 languages without a development set, we divide them into two classes based on the size of their training set, which can be fine-tuned (ga, sme) and can not be fine-tuned  (bxr, hsb, hy, kk, kmr) . For each language of UD version 2.2 sets (Nivre 

 Experimental Results We trained our system based on a Nvidia GeForce GTX Titan X. We used the official TIRA  (Potthast et al., 2014 ) to evaluate the system. We used Dynet neural network library to build our system  (Neubig et al., 2017) . The hyperparameters of the final system used for all the reported experiments are detailed in Table  5 . 

 Overall Results The main official evaluation results are given in Table  4 . And the   (arc)  500 Hidden units in M LP  (rel)  100 Learning rate 0.002 Optimization algorithm Adam Table  5 : Hyper-parameter values used in shared task. of 26 teams. Compared to the baseline obtained with UDPipe1.2  (Straka et al., 2016) , our system gained 5.10 LAS improvement on average. Our system shows better results on 7 small treebanks. Performance improvement are more obvious when considering only small treebanks(for example, our system ranked fourth best on ru taiga and sl sst). Besides that, our system ranked tenth in the EPE 3 2018 campaign with a 55.71 F1 score. 

 Discussion on Multilingual Model As described in section 4, we trained 46 crosslanguage models and selected the corresponding cross-language model for 7 languages that did not have a development set. Generally, cross-language models are trained in the language of the same family. However, apart from grammatical similarity, the language family division also considers the linguistic history, geographical location and other factors. We want to select a language's crosslanguage model to consider only grammatical similarity. So we use a cross-language model to predict the results in this language as a basis for selection. In table 3, the experimental results show that the Cross-language model with the best performance in hsb, hy, kk, and kmr languages comes from the same language family, while the Cross-language model with the best performance in bxr, ga, and sme is not from the same language family. Therefore, constructing a cross-language model according to the language of the same family is only applicable to some languages, not all of them. We've only chosen the best performing cross-language model at the moment. In the future, we will try to select the top-k cross-language model. 

 Conclusions In this paper, we present a graph-based dependency parsing system for the CoNLL 2018 UD Shared Task, which composed of a BiLSTMs feature extractor and a bi-affine pointer networks. The results suggests that a deep BiLSTM extractor and a bi-affine pointer networks is a way to achieve competitive parsing performances. We will continue to improve our system in our future work.  

 Corpus AntNLP Figure 1 : 1 Figure 1: The structure of the entire system. 

 Figure 2 : 2 Figure 2: The architecture of our parser system. 

 i?j is the computed score of the dependency edge. a (arc) i? * is a vector, and the k th dimension is the score of the dependency edge a(arc) i?k . p (arc)i?j is the j th dimension of normalization of the vector a (arc) i? * , meaning the probability of dependency edge w i ? w j .We obtain a dependency tree representation T of a complete graph p(arc) * ? * using the maximum spanning tree (MST) algorithm. The probability p (rel) i r ? ?j of the relation r of each dependency edge 

 , and the k th dimension is the score of the dependency edge w i k ? ? w j . 

 Figure 3 : 3 Figure 3: The architecture of our parser system. 

 Table 1 : 1 Table6shows the per-treebank LAS F1 results. Our system achieved 70.90 F1 (LAS) on the overall 82 tree banks, ranked 9 th out Corpus listed above are languages that don't have development set and the training set size is too small to be fine-tuned. "#Train" means the number of sentences. "Cross Language" means the language with the highest LAS score for corresponding origin language in our delexicalized cross-language model. Language #Train Cross language LAS Buryat (bxr) 19 Uyghur (ug) 27.45 Upper Sorbian (hsb) 23 Croatian (hr) 39.35 Armenian (hy) 50 Latvian (lv) 29.35 Kazakh (kk) 9 Turkish (tr) 23.44 Kurmanji (kmr) 19 Persian (fa) 26.03 Corpus #Total #Train #Dev Cross language LAS Fine-tune Irish (ga) 566 476 90 Hebrew (he) 36.13 68.49 North Sami (sme) 2464 1948 516 Swedish (sv) 36.13 63.00 

 Table 2 : 2 Corpus listed above are languages that don't have development set. Because the training set size is much bigger, we decide to divide the training set into two parts, one for training set and the other for development set. Origin language language family Cross Language Language family Buryat (bxr) Mongolic Uyghur (ug) Turkic Southeastern Upper Sorbian (hsb) IE Slavic Croatian (hr) IE Slavic Armenian (hy) IE Armenian Latvian (lv) IE Baltic Kazakh (kk) Turkic Northwestern Turkish (tr) Turkic Southwestern Kurmanji (kmr) IE Iranian Persian (fa) IE Iranian Irish (ga) IE Celtic Hebrew (he) Afro-Asiatic Semitic North Sami (sme) Uralic Sami Swedish (sv) IE Germanic 

 Table 3 : 3 language families and genera for origin language and cross language (IE = Indo-European). 2 Corpus FLAS Baseline Rank MLAS Baseline Rank BLEX Baseline Rank All treebanks(82) 70.90 65.80 9 55.92 52.42 10 60.91 55.80 8 Big treebanks(61) 79.61 74.14 12 65.43 61.27 11 70.34 64.67 9 PUD treebanks(5) 68.87 66.63 11 53.47 51.75 10 57.71 54.87 8 Small treebanks(7) 63.73 55.01 6 42.24 38.80 7 48.31 41.06 6 Low resource(9) 18.59 17.17 10 3.43 2.82 9 8.61 7.63 8 

 Table 4 : 4 Official experiment results with rank. (number): number of corpora. FLAS means F1 score of LAS. word/lemma dropout 0.33 upos/xpos tag dropout 0.33 char-CNN dropout 0.33 BiLSTM layers 3 BiLSTM hidden layer dimensions 400 Hidden units in M LP 

 Table 6 : 6 Official experiment results on each treebank. The results in table are F1(LAS). *: some corpus' name too long to display completely, using * to indicate omission. Rank Best Baseline Corpus AntNLP Rank Best Baseline af afr* 82.63 11 85.47 77.88 hy arm* 25.09 10 37.01 21.79 ar pad* 70.75 12 77.06 66.41 id gsd 76.86 16 80.05 74.37 bg btb 87.24 13 91.22 84.91 it isd* 89.14 12 92.00 86.26 br keb 10.06 16 38.64 10.25 it pos* 72.30 9 79.39 66.81 bxr bdt 19.53 1 1 12.61 ja gsd 72.82 17 83.11 72.32 ca anc* 89.28 13 91.61 85.61 ja mod* 12.94 22 28.33 22.71 cs cac 90.47 6 91.61 83.72 kk ktb 19.26 18 31.93 24.21 cs fic* 90.14 7 92.02 82.49 kmr mg 23.20 16 30.41 23.92 cs pdt 89.41 9 91.68 83.94 ko gsd 80.15 12 85.14 61.40 cs pud 84.76 6 86.13 80.08 ko kai* 85.01 11 86.91 70.25 cu pro* 68.23 13 75.73 65.46 la itt* 83.14 12 87.08 75.95 da ddt 80.56 10 86.28 75.43 la per* 60.99 5 72.63 47.61 de gsd 76.88 10 80.36 70.85 la pro* 66.24 12 73.61 59.66 el gdt 85.76 12 89.65 82.11 lv lvt* 75.56 12 83.97 69.43 en ewt 80.74 12 84.57 77.56 nl alp* 84.69 11 89.56 77.60 en gum 79.70 11 85.05 74.20 nl las* 82.04 8 86.84 74.56 en lin* 79.25 5 81.97 73.10 no bok* 89.19 7 91.23 83.47 en pud 84.60 9 87.89 79.56 no nyn* 88.26 9 90.99 82.13 es anc* 88.84 11 90.93 84.43 no nyn* 66.26 4 70.34 48.95 et edt 81.37 11 85.35 75.02 pcm nsc 18.30 6 30.07 12.18 eu bdt 79.01 11 84.22 70.13 pl lfg 91.16 13 94.86 87.53 fa ser* 83.98 11 88.11 79.10 pl sz 85.03 14 92.23 81.90 fi ftb 83.72 11 88.53 75.64 pt bos* 86.71 8 87.81 82.07 fi pud 85.50 10 90.23 80.15 ro rrt 84.92 8 86.87 80.27 fi tdt 83.18 11 88.73 76.45 ru syn* 90.20 10 92.48 84.59 fo oft 20.13 21 49.43 25.19 ru tai* 68.99 4 74.24 55.51 fr gsd 84.84 10 86.89 81.05 sk snk 81.14 12 88.85 75.41 fr seq* 85.32 10 89.89 81.12 sl ssj 83.26 12 91.47 77.33 fr spo* 70.96 8 75.78 65.56 sl sst 56.30 4 61.39 46.95 fro src* 83.13 12 87.12 79.27 sme gie* 57.15 13 69.87 56.98 ga idt 64.38 11 70.88 62.93 sr set 85.77 10 88.66 82.07 gl ctg 81.12 8 82.76 76.10 sv lin* 80.01 10 84.08 74.06 gl tre* 72.03 8 74.25 66.16 sv pud 76.54 10 80.35 70.63 got pro* 62.97 14 69.55 62.16 sv tal* 83.41 12 88.63 77.91 grc per* 70.76 9 79.39 57.75 th pud 0.36 18 13.70 0.70 grc pro* 73.82 9 79.25 67.57 tr ims* 59.68 12 66.44 54.04 he htb 61.43 12 76.09 57.86 ug udt 61.42 10 67.05 56.26 hi hdt* 90.44 11 92.41 87.15 uk iu 79.91 12 88.43 74.91 hr set 83.48 12 87.36 78.61 ur udt* 79.85 14 83.39 77.29 hsb ufa* 31.36 6 46.42 23.64 vi vtb 42.65 11 55.22 39.63 hu sze* 73.19 12 82.66 66.76 zh gsd 62.83 13 76.77 57.91 

			 In total, we trained 46 monolingual models. Zeman et al., 2018)  with both a training set and a development set, we train a parser using delexicalized token representation as a crosslanguage model. The architecture of the multilingual model is shown in Figure3. The training set of these 5 languages are then used as a development set to validate the performance of each cross-language model (see Table1). We select the best performance model as a cross-language model for the corresponding language. For both ga and sme, we manually divide the development set from the training set and fine-tune the crosslanguage model. Prediction and fine-tuning results are shown in the Table2. 

			 Information from http://universaldependencies.org. 3 http://epe.nlpl.eu
