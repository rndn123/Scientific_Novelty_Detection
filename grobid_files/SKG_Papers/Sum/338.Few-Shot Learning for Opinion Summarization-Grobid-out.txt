title
Few-Shot Learning for Opinion Summarization

abstract
Opinion summarization is the automatic creation of text reflecting subjective information expressed in multiple documents, such as user reviews of a product. The task is practically important and has attracted a lot of attention. However, due to the high cost of summary production, datasets large enough for training supervised models are lacking. Instead, the task has been traditionally approached with extractive methods that learn to select text fragments in an unsupervised or weakly-supervised way. Recently, it has been shown that abstractive summaries, potentially more fluent and better at reflecting conflicting information, can also be produced in an unsupervised fashion. However, these models, not being exposed to actual summaries, fail to capture their essential properties. In this work, we show that even a handful of summaries is sufficient to bootstrap generation of the summary text with all expected properties, such as writing style, informativeness, fluency, and sentiment preservation. We start by training a conditional Transformer language model to generate a new product review given other available reviews of the product. The model is also conditioned on review properties that are directly related to summaries; the properties are derived from reviews with no manual effort. In the second stage, we fine-tune a plug-in module that learns to predict property values on a handful of summaries. This lets us switch the generator to the summarization mode. We show on Amazon and Yelp datasets that our approach substantially outperforms previous extractive and abstractive methods in automatic and human evaluation.

Introduction Summarization of user opinions expressed in online resources, such as blogs, reviews, social media, or internet forums, has drawn much attention due to its potential for various information access applications, such as creating digests, search, and report Gold These shoes run true to size, do a good job supporting the arch of the foot and are well-suited for exercise. They're good looking, comfortable, and the sole feels soft and cushioned. Overall they are a nice, light-weight pair of shoes and come in a variety of stylish colors. 

 Ours These running shoes are great! They fit true to size and are very comfortable to run around in. They are light weight and have great support. They run a little on the narrow side, so make sure to order a half size larger than normal. Table  1 : Example summaries produced by our system and an annotator; colors encode its alignment to the input reviews. The reviews are truncated, and delimited with the symbol '||'. 

 Reviews generation  (Hu and Liu, 2004; Medhat et al., 2014; Angelidis and Lapata, 2018) . Although significant progress has been observed in supervised summarization in non-subjective single-document context, such as news articles  (Rush et al., 2015; Nallapati et al., 2016; Paulus et al., 2017; See et al., 2017; Liu et al., 2018) , modern deep learning methods rely on large amounts of annotated data that are not readily available in the opinion-summarization domain and expensive to produce. A key obstacle making data annotation expensive is that annotators need to consider multiple input texts when writing a summary, which is time-consuming. Moreover, annotation would have to be undertaken for multiple domains as online reviews are inherently multi-domain  (Blitzer et al., 2007)  and summarization systems can be domainsensitive  (Isonuma et al., 2017) . This suggests that it is unlikely that human-annotated corpora large enough for training deep models will be available. Recently, a number of unsupervised abstractive multi-document models were introduced (e.g., COPYCAT;  Bra?inskas et al. 2020 and MEANSUM; Chu and Liu 2019)  that are trained on large collections of unannotated product reviews. 1 However, unsurprisingly perhaps, since the models are not exposed to the actual summaries, they are unable to learn their key characteristics. For instance, MEAN-SUM  (Chu and Liu, 2019)  is prone to producing summaries that contain a significant amount of information that is unsupported by reviews; COPY-CAT generates summaries that are better aligned with reviews, yet they are limited in detail. Moreover, both systems, are trained mostly on subjectively written reviews, and as a result, tend to generate summaries in the same writing style. The main challenge in the absence of large annotated corpora lies in successful utilization of scarce annotated resources. Unlike recent approaches to language model adaptation for abstractive singledocument summarization  (Hoang et al., 2019; Raffel et al., 2019)  that utilize hundreds of thousands of summaries, our two annotated datasets consist of only 60 and 100 annotated data-points. It was also observed that a naive fine-tuning of multi-million parameter models on small corpora leads to rapid over-fitting and poor generalization  (Vinyals et al., 2016; Finn et al., 2017) . In this light, we propose a few-shot learning framework and demonstrate that even a tiny number of annotated instances is sufficient to bootstrap generation of the formal summary text that is both informative and fluent (see Table  1 ). To the best of our knowledge, this work is the first few-shot learning approach applied to summarization. In our work, we observe that reviews in a large unannotated collection vary a lot; for example, they differ in style, the level of detail, or how much they diverge from other reviews of the product in terms of content and overall sentiment. We refer to individual review characteristics and their relations to other reviews as properties (Ficler and Goldberg,  1  For simplicity, we use the term 'product' to refer to both Amazon products and Yelp businesses. 2017). While reviews span a large range of property values, only a subset of them is appropriate for summaries. For example, summaries should be close to the product's reviews in content, avoid using the first-person pronouns and agree with the reviews in sentiment. Our approach starts with estimating a property-aware model on a large collection of reviews and then adapts the model using a few annotator-created summaries, effectively switching the generator to the summarization regime. As we demonstrate in our experiments, the summaries do not even have to come from the same domain. More formally, we estimate a text model on a dataset of reviews; the generator is a Transformer conditional language model (CLM) that is trained with a 'leave-one-out' objective  (Besag, 1975; Bra?inskas et al., 2020)  by attending to other reviews of the product. We define properties of unannotated data that are directly related to the end task of summarization. Those properties are easy to derive from reviews, and no extra annotation effort is required. The CLM is conditioned on these properties in training. The properties encode partial information about the target review that is being predicted. We capitalize on that by fine-tuning parts of the model jointly with a tiny plug-in network on a handful of human-written summaries. The plug-in network is trained to output property values that make the summaries likely under the trained CLM. The plug-in has less than half a percent of the original model's parameters, and thus is less prone to over-fitting on small datasets. Nevertheless, it can successfully learn to control dynamics of a large CLM by providing property values that force generation of summaries. We shall refer to the model produced using the procedure as Few Shot Summarizer (FEWSUM). We evaluate our model against both extractive and abstractive methods on Amazon and Yelp human-created summaries. Summaries generated by our model are substantially better than those produced by competing methods, as measured by automatic and human evaluation metrics on both datasets. Finally, we show that it allows for successful cross-domain adaption. Our contributions can be summarized as follows: ? we introduce the first few-shot learning framework for abstractive opinion summarization; ? we demonstrate that the approach substantially outperforms extractive and abstractive models, both when measured with automatic metrics and in human evaluation; ? we release datasets with abstractive summaries for Amazon products and Yelp businesses. 2 

 Unsupervised Training User reviews about an entity (e.g., a product) are naturally inter-dependent. For example, knowing that most reviews are negative about a product's battery life, it becomes more likely that the next review will also be negative about it. To model inter-dependencies, yet to avoid intractabilities associated with undirected graphical models  (Koller and Friedman, 2009) , we use the leave-one-out setting  (Besag, 1975; Bra?inskas et al., 2020) . Specifically, we assume access to a large corpus of user text reviews, which are arranged as M groups {r 1:N } M j=1 , where r 1:N are reviews about a particular product that are arranged as a target review r i and N ? 1 source reviews r ?i = log G ? (r j i |E ? (r j ?i )) (1) Our model has an encoder-generator Transformer architecture  (Vaswani et al., 2017) , where the encoder E ? produces contextual representations of r ?i that are attended by the generator G ? , which in-turn is a conditional language model predicting the target review r i , estimated using teacherforcing  (Williams and Zipser, 1989 ). An illustration is presented in Fig.  1 . The objective lets the model exploit common information across reviews, such as rare brand names or aspect mentions. For example, in Fig.  1 , the generator can directly attend to the word vacuum in the source reviews to increase its prediction probability. Additionally, we condition on partial information about the target review r i using an oracle q(r i , r ?i ) as shown in Eq. 2. 1 M N M j=1 N i=1 log G ? (r j i |E ? (r j ?i ), q(r j i , r j ?i )) (2) We refer to this partial information as properties  (Ficler and Goldberg, 2017) , which correspond to text characteristics of r i or relations between r i and r ?i . For example, one such property can be the ROUGE score  (Lin, 2004)  between r i and r ?i , which indicates the degree of overlap between r i and r ?i . In Fig.  1 , a high ROUGE value can signal to the generator to attend the word vacuum in the source reviews instead of predicting it based on language statistics. Intuitively, while the model observes a wide distribution of ROUGE scores during training on reviews, during summarization in test time we can achieve a high degree of input-output text overlap by setting the property to a high value. We considered three types of properties. Content Coverage: ROUGE-1, ROUGE-2, and ROUGE-L between r i and r ?i signals to G ? how much to rely on syntactic information in r ?i during prediction of r i . Writing Style: as a proxy for formal and informal writing style, we compute pronoun counts, and create a distribution over 3 points of view and an additional class for cases with no pronouns; see Appendix 9.7 for details. Rating and Length Deviations: for the former, we compute the difference between r i 's rating and the average r ?i rating; in the latter case, we use the difference between r i 's length and the average r ?i length. 

 Novelty Reduction While summary and review generation are technically similar, there is an important difference that needs to be addressed. Reviews are often very diverse, so when a review is predicted, the generator often needs to predict content that is not present in source reviews. On the other hand, when a summary is predicted, its semantic content always matches the content of source reviews. To address this discrepancy, in addition to using the ROUGE scores, as was explained previously, we introduce a novelty reduction technique, which is similar to label smoothing  (Pereyra et al., 2017) . Specifically, we add a regularization term L, scaled by ?, that is applied to word distributions   Very sturdy vacuum ? r i < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 K N q + 4 W S m z E w t F C Q A p B y D P Z L o u k = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u K b Q M 2 F h G N B + Q H G F v s 5 c s 2 d s 7 d u e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y R S G P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y Y Z U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D G / 8 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u V b 1 6 t X b v V R p X e R x F O I N z u A Q P r q E B d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A F Q k o 2 7 < / l a t e x i t > r i < l a t e x i t s h a 1 _ b a s e 6 4 = " 2 K N q + 4 W S m z E w t F C Q A p B y D P Z L o u k = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u K b Q M 2 F h G N B + Q H G F v s 5 c s 2 d s 7 d u e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y R S G P z V H 8 Y s j b h C J q k x P c 9 N 0 M + o R s E k n 5 X 6 q e E J Z R M 6 4 j 1 L F Y 2 4 8 b P F q T N y Y Z U h C W N t S y F Z q L 8 n M h o Z M 4 0 C 2 x l R H J t V b y 7 + 5 / V S D G / 8 T K g k R a 7 Y c l G Y S o I x m f 9 N h k J z h n J q C W V a 2 F s J G 1 N N G d p 0 S j Y E b / X l d d K u V b 1 6 t X b v V R p X e R x F O I N z u A Q P r q E B d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A F Q k o 2 7 < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 4 G 1 1 P 1 2 t 0 P K F j c j i / a F b d B G 5 W q c = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u K b Q M 2 F h G N B + Q H G F v s 5 c s 2 d s 7 d u e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y R S G H T d b 6 e w s b m 1 v V P c L e 3 t H x w e l Y 9 P 2 i Z O N e M t F s t Y d w N q u B S K t 1 C g 5 N 1 E c x o F k n e C y e 3 c 7 z x x b U S s H n G a c D + i I y V C w S h a 6 U E P v E G 5 4 l b d B c g 6 8 X J S g R z N Q f m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 1 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k w i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 4 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p l G w I 3 u r L 6 6 R d q 3 r 1 a u 3 e q z S u 8 j i K c A b n c A k e X E M D 7 q A J L W A w g m d 4 h T d H O i / O u / O x b C 0 4 + c w p / I H z + Q P 7 o 4 2 D < / l a t e x i t > r 1 < l a t e x i t s h a 1 _ b a s e 6 4 = " 4 G 1 1 P 1 2 t 0 P K F j c j i / a F b d B G 5 W q c = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u K b Q M 2 F h G N B + Q H G F v s 5 c s 2 d s 7 d u e E c O Q n 2 F g o Y u s v s v P f u E m u 0 M Q H A 4 / 3 Z p i Z F y R S G H T d b 6 e w s b m 1 v V P c L e 3 t H x w e l Y 9 P 2 i Z O N e M t F s t Y d w N q u B S K t 1 C g 5 N 1 E c x o F k n e C y e 3 c 7 z x x b U S s H n G a c D + i I y V C w S h a 6 U E P v E G 5 4 l b d B c g 6 8 X J S g R z N Q f m r P 4 x Z G n G F T F J j e p 6 b o J 9 R j Y J J P i v 1 U 8 M T y i Z 0 x H u W K h p x 4 2 e L U 2 f k w i p D E s b a l k K y U H 9 P Z D Q y Z h o F t j O i O D a r 3 l z 8 z + u l G N 7 4 m V B J i l y x 5 a I w l Q R j M v + b D I X m D O X U E s q 0 s L c S N q a a M r T p l G w I 3 u r L 6 6 R d q 3 r 1 a u 3 e q z S u 8 j i K c A b n c A k e X E M D 7 q A J L W A w g m d 4 h T d H O i / O u / O x b C 0 4 + c w p / I H z + Q P 7 o 4 2 D < / l a t e x i t > ? ? ? ? Great vacuum ? r N < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 e a x J d 2 f T N 6 D C + z 8 T g q x 3 H E m a 6 w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u F l o G b K w k o v m A 5 A h 7 m 7 1 k y d 7 e s T s n h C M / w c Z C E V t / k Z 3 / x k 1 y h S Y + G H i 8 N 8 P M v C C R w q D r f j u F t f W N z a 3 i d m l n d 2 / / o H x 4 1 D J x q h l v s l j G u h N Q w 6 V Q v I k C J e 8 k m t M o k L w d j G 9 m f v u J a y N i 9 Y i T h P s R H S o R C k b R S g + 6 f 9 c v V 9 y q O w d Z J V 5 O K p C j 0 S 9 / 9 Q Y x S y O u k E l q T N d z E / Q z q l E w y a e l X m p 4 Q t m Y D n n X U k U j b v x s f u q U n F l l Q M J Y 2 1 J I 5 u r v i Y x G x k y i w H Z G F E d m 2 Z u J / 3 n d F M N r P x M q S Z E r t l g U p p J g T G Z / k 4 H Q n K G c W E K Z F v Z W w k Z U U 4 Y 2 n Z I N w V t + e Z W 0 a l X v s l q 7 9 y r 1 i z y O I p z A K Z y D B 1 d Q h 1 t o Q B M Y D O E Z X u H N k c 6 L 8 + 5 8 L F o L T j 5 z D H / g f P 4 A J 6 a N o A = = < / l a t e x i t > r N < l a t e x i t s h a 1 _ b a s e 6 4 = " 6 e a x J d 2 f T N 6 D C + z 8 T g q x 3 H E m a 6 w = " > A A A B 6 n i c b V A 9 S w N B E J 2 L X z F + R S 1 t F o M g F u E u F l o G b K w k o v m A 5 A h 7 m 7 1 k y d 7 e s T s n h C M / w c Z C E V t / k Z 3 / x k 1 y h S Y + G H i 8 N 8 P M v C C R w q D r f j u F t f W N z a 3 i d m l n d 2 / / o H x 4 1 D J x q h l v s l j G u h N Q w 6 V Q v I k C J e 8 k m t M o k L w d j G 9 m f v u J a y N i 9 Y i T h P s R H S o R C k b R S g + 6 f 9 c v V 9 y q O w d Z J V 5 O K p C j 0 S 9 / 9 Q Y x S y O u k E l q T N d z E / Q z q l E w y a e l X m p 4 Q t m Y D n n X U k U j b v x s f u q U n F l l Q M J Y 2 1 J I 5 u r v i Y x G x k y i w H Z G F E d m 2 Z u J / 3 n d F M N r P x M q S Z E r t l g U p p J g T G Z / k 4 H Q n K G c W E K Z F v Z W w k Z U U 4 Y 2 n Z I N w V t + e Z W 0 a l X v s l q 7 9 y r 1 i z y O I p z A K Z y D B 1 d Q h 1 t o Q B M Y D O E Z X u H N k c 6 L 8 + 5 8 L F o L T j 5 z D H / g f P 4 A J 6 a N o A = = < / l a t e x i t > ? ? ? ? Encoder States Oracle This ? vacuum hoover product Generator States q(r i , r i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " g 9 F 9 H I H e K C q j u s Y L y 0 9 K K 6 K + Q 5 U = " > A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h Q i 1 J P e i x 4 M V j B f s B b Q y b 7 a Z d u t m k u x u l h P 4 P L x 4 U 8 e p / 8 e a / c d v m o K 0 P B h 7 v z T A z z 4 8 5 U 9 q 2 v 6 3 c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 a q k o k Y Q 2 S c Q j 2 f G x o p w J 2 t R M c 9 q J J c W h z 2 n b H 9 3 M / P Y j l Y p F 4 l 5 P Y u q G e C B Y w A j W R n o Y l 6 X H K k h 6 6 Q W b n n v F k l 2 1 5 0 C r x M l I C T I 0 v O J X r x + R J K R C E 4 6 V 6 j p 2 r N 0 U S 8 0 I p 9 N C L 1 E 0 x m S E B 7 R r q M A h V W 4 6 v 3 q K z o z S R 0 E k T Q m N 5 u r v i R S H S k 1 C 3 3 S G W A / V s j c T / / O 6 i Q 6 u 3 Z S J O N F U k M W i I O F I R 2 g W A e o z S Y n m E 0 M w k c z c i s g Q S 0 y 0 C a p g Q n C W X 1 4 l r V r V u a z W 7 p x S v Z L F k Y c T O I U y O H A F d b i F B j S B g I R n e I U 3 6 8 l 6 s d 6 t j 0 V r z s p m j u E P r M 8 f N l + R m A = = < / l a t e x i t > q(r i , r i ) < l a t e x i t s h a 1 _ b a s e 6 4 = " g 9 F 9 H I H e K C q j u s Y L y 0 9 K K 6 K + Q 5 U = " > A A A B 9 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B a h Q i 1 J P e i x 4 M V j B f s B b Q y b 7 a Z d u t m k u x u l h P 4 P L x 4 U 8 e p / 8 e a / c d v m o K 0 P B h 7 v z T A z z 4 8 5 U 9 q 2 v 6 3 c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 a q k o k Y Q 2 S c Q j 2 f G x o p w J 2 t R M c 9 q J J c W h z 2 n b H 9 3 M / P Y j l Y p F 4 l 5 P Y u q G e C B Y w A j W R n o Y l 6 X H K k h 6 6 Q W b n n v F k l 2 1 5 0 C r x M l I C T I 0 v O J X r x + R J K R C E 4 6 V 6 j p 2 r N 0 U S 8 0 I p 9 N C L 1 E 0 x m S E B 7 R r q M A h V W 4 6 v 3 q K z o z S R 0 E k T Q m N 5 u r v i R S H S k 1 C 3 3 S G W A / V s j c T / / O 6 i Q 6 u 3 Z S J O N F U k M W i I O F I R 2 g W A e o z S Y n m E 0 M w k c z c i s g Q S 0 y 0 C a p g Q n C W X 1 4 l r V r V u a z W 7 p x S v Z L F k Y c T O I U y O H A F d b i F B j S B g I R n e I U 3 6 8 l 6 s d 6 t j 0 V r z s p m j u E P r M 8 f N l + R m A = = < / l a t e x i t > Attention ? ? ? ? Figure  1 : Illustration of the FEWSUM model that uses the leave-one-out objective. Here predictions of the target review r i is performed by conditioning on the encoded source reviews r ?i . The generator attends the last encoder layer's output to extract common information (in red). Additionally, the generator has partial information about r i passed by the oracle q(r i , r ?i ). produced by the generator G ? as shown in Eq. 3. 1 M N M j=1 N i=1 log G ? (r j i |E ? (r j ?i ), q(r j i , r j ?i )) ? ?L(G ? (r j i |E ? (r j ?i ), q(r j i , r j ?i )) (3) It penalizes assigning the probability mass to words not appearing in r ?i , as shown in Eq. 4, and thus steers towards generation of text that is more grounded in content of r ?i . L(G ? (r i |r ?i , q(r i , r ?i ))) = T t=1 w ?V (r ?i ) G ? (W t = w|r 1:t?1 i , r ?i , q(r i , r ?i )) (4) Here, T is the length of r i , and the inner sum is over all words that do not appear in the word vocabulary of r ?i . Intuitively, in Fig.  1 , the penalty could reduce the probability of the word hoover to be predicted as it does not appear in the source reviews. 

 Summary Adaptation Once the unsupervised model is trained on reviews, our task is to adapt it to generation of summaries. Here, we assume access to a small number of annotator-written summaries (s k , r k 1:N ) K k=1 where s is a summary for r 1:N input reviews. As we will show in Sec. 6.1, naive fine-tuning of the unsupervised model on a handful of annotated data-points leads to poor generalization. Instead, we capitalize on the fact that the generator G ? has observed a wide range of property values associated with r i during the unsupervised training phase. Intu-itively, some combinations of property values drive it into generation of text that has qualities of a summary while others of a review. However, we might not know values in advance that are necessary for generation of summaries. Furthermore, q(r i , r ?i ) cannot be applied at test time as it requires access to target texts. In the following section, we describe a solution that switches the generator to the summarization mode relying only on input reviews. 

 Plug-in Network We start by introducing a parametrized plug-in network p ? (r ?i ) that yields the same types of properties as q(r i , r ?i ). From a practical perspective, the plug-in should be input-permutation invariant and allow for an arbitrary number of input reviews  (Zaheer et al., 2017) . Importantly, the trainable plug-in can have a marginal fraction of the main model's parameters, which makes it less prone to over-fitting when trained on small datasets. We initialize the parameters of p ? (r ?i ) by matching its output to q(r i , r ?i ) on the unannotated reviews. Specifically, we used a weighted combination of distances as shown for one group of reviews in Eq. 5. N i=1 L l=1 ? l D l (p ? (r ?i ) l , q(r i , r ?i ) l ) (5) Here, D l (p ? (r ?i ) l , q(r i , r ?i ) l ) is a distance for the property l, and ? l is an associated weight. Specifically, we used L1 norm for Content Coverage, Rating and Length Deviations, and Kullback-Leibler divergence for Writing Style. For the plug-in network, we employed a multi-layer feed-forward network with multi-head attention modules over the encoded states of the source reviews at each layer, followed by a linear transformation, predicting property values. Note that the encoder is shared with the main model. 

 Fine-Tuning Unsurprisingly, perhaps, the network p ? being initialized on unannotated reviews inherits a strong bias towards outputting property values resulting in generation of reviews, which should not be appropriate for generating summaries. Fortunately, due to the simplicity of the chosen properties, it is possible to fine-tune p ? to match the output of q on the annotated data (s k , r k 1:N ) K k=1 using Eq. 5. An alternative is to optimize the plug-in to directly increase the likelihood of summaries under G ? while keeping all other parameters fixed.  3  As the generator is trained on unannotated reviews, it might not encounter a sufficient amount of text that is written as a summary, and that highly overlaps in content with the input reviews. We address that by unfreezing the attention module of G ? over input reviews and the plug-in p ? , and by maximizing the likelihood of summaries: 1 K K k=1 log G ? (s k |E ? (r k 1:N ), p ? (r k 1:N )) (6) This allows the system to learn an interaction between G ? and p ? . For example, what property values are better associated with summaries and how G ? should better respond to them. 4 Experimental Setup 

 Dataset For training we used customer reviews from Amazon  (He and McAuley, 2016)  and Yelp.  4  From the Amazon reviews we selected 4 categories: Electronics; Clothing, Shoes and Jewelry; Home and Kitchen; Health and Personal Care. We used a similar pre-processing schema as in  (Bra?inskas et al., 2020)  We obtained 480 human-written summaries (180 for Amazon and 300 for Yelp) for 8 reviews each, using Amazon Mechanical Turk (AMT). Each product/business received 3 summaries, and averaged ROUGE scores are reported in the following sections. Also, we reserved approximately 1 3 for testing and the rest for training and validation. The details are in Appendix 9.2. 

 Experimental Details For the main model, we used the Transformer architecture  (Vaswani et al., 2017)  with trainable length embeddings and shared parameters between the encoder and generator  (Raffel et al., 2019) . Subwords were obtained with BPE  (Sennrich et al., 2016)  using 32000 merges. Subword embeddings were shared across the model as a form of regularization  (Press and Wolf, 2017) . For a fair comparison, we approximately matched the number of parameters to COPYCAT  (Bra?inskas et al., 2020) . We randomly initialized all parameters with Glorot (Glorot and Bengio, 2010). For the plug-in network, we employed a multi-layer feed-forward network with multi-head attention modules over encoded states of the source review. After the last layer, we performed a linear projection to compute property values. Further, parameter optimization was performed using Adam (Kingma and Ba, 2014), and beam search with n-gram blocking  (Paulus et al., 2017)  was applied to our model and Copycat for summary generation. All experiments were conducted on 4 x GeForce RTX 2080 Ti. 

 Hyperparameters Our parameter-shared encoder-generator model used a 8-head and 6-layer Transformer stack. Dropout in sub-layers and subword embeddings dropout was both set to 0.1, and we used 1000 dimensional position-wise feed-forward neural networks. We set subword and length embeddings to 390 and 10 respectively, and both were concatenated to be used as input. For the plug-in network, we set the output dimension to 30 and internal feedforward network hidden dimensions to 20. We used a stack of 3 layers, and the attention modules with 3 heads at each layer. We applied 0.4 internal dropout and 0.15 attention dropout. Property values produced by the plug-in or oracle were concatenated with subword and length embeddings and linearly projected before being passed to the generator. In total, our model had approximately 25M parameters, while the plug-in network only 100K (i.e., less than 0.5 % of the main model's parameters). In all experiments, the hyperparameter tuning was performed based on the ROUGE-L score on Yelp and Amazon validation sets. 

 Baselines LEXRANK  (Erkan and Radev, 2004 ) is an unsupervised extractive graph-based algorithm selecting sentences based on graph centrality. Sentences represent nodes in a graph whose edges have weights denoting similarity computed with tf-idf. MEANSUM is an unsupervised abstractive summarization model  (Chu and Liu, 2019 ) that treats a summary as a discrete latent state of an autoencoder. The model is trained in a multi-task fashion with two objectives, one for prediction of reviews and the other one for summary-reviews alignment in the semantic space using the cosine similarity. COPYCAT is the state-of-the-art unsupervised abstractive summarizer  (Bra?inskas et al., 2020)  that uses continuous latent representations to model review groups and individual review semantics. It has an implicit mechanism for novelty reduction and uses a copy mechanism. As is common in the summarization literature, we also employed a number of simple summarization baselines. First, the CLUSTROID review was computed for each group of reviews as follows. We took each review from a group and computed ROUGE-L with respect to all other reviews. The review with the highest ROUGE score was selected as the clustroid review. Second, we sampled a RANDOM review from each group to be used as the summary. Third, we constructed the summary by selecting the leading sentences (LEAD) from each review of a group. 

 Evaluation Results Automatic Evaluation We report ROUGE F1 score  (Lin, 2004)  based evaluation results on the Amazon and Yelp test sets in Tables  3 and 4  For every criterion, a system's score is computed as the percentage of times it was selected as best, minus the percentage of times it was selected as worst  (Orme, 2009) . The scores range from -1 (unanimously worst) to +1 (unanimously best). The results are presented in Tables  5 and 6  for Amazon and Yelp, respectively. On the Amazon data, they indicate that our model is preferred across the board over the baselines. COPYCAT is preferred over LEXRANK in terms of fluency and non-redundancy, yet it shows worse results in terms of informativeness and overall sentiment preservation. In the same vein, on Yelp in   (2020) , the ROUGE metric can be insensitive to hallucinating facts and entities. We also investigated how well generated text is supported by input reviews. We split summaries generated by our model and COPYCAT into sentences. Then for each summary sentence, we hired 3 AMT workers to judge how well content of the sentence is supported by the reviews. Three following options were available. Full support: all the content is reflected in the reviews; Partial support: only some content is reflected in the reviews; No support: content is not reflected in the reviews. The results are presented in Table  7 . Despite not using the copy mechanism, that is beneficial for fact preservation  (Falke et al., 2019)  and generation of more diverse and detailed summaries (see Appendix), we score on par with COPYCAT. 

 Analysis 

 Alternative Adaptation Strategies We further explored alternative utilization approaches of annotated data-points, based on the same split of the Amazon summaries as explained in Sec. 4.1. First, we trained a model in an unsupervised learning setting (USL) on the Amazon reviews with the leave-one-out objective in Eq. 1. In this setting, the model has neither exposure to summaries nor the properties, as the oracle q(r i , r ?i ) is not used. Further, we considered two alternative settings how the pre-trained unsupervised model can be adapted on the gold summaries. In the first setting, the model is fine-tuned by predicting summaries conditioned on input reviews (USL+F). In the second one, similar to  Hoang et al. (2019) , we performed adaptation in a multi-tasking learning (MTL) fashion. Here, USL is further trained on a mixture of unannotated corpus review and gold summary batches with a trainable embedding indicating the task.  5  The results are presented in Table  8 . First, we observed that USL generates summaries that get the worst ROUGE scores. Additionally, the generated text tends to be informal and substantially shorter than an average summary, we shall discuss that in Sec. 6.2. Second, when the model is fine-tuned on the gold summaries (USL+F), it noticeably improves the results, yet they are substantially worse than of our proposed few-shot approach. It can be explained by strong influence of the unannotated data stored in millions of parameters that requires more annotated data-points to overrule. Finally, we observed that MTL fails to decouple the tasks, indicated by only a slight improvement over USL.  

 R1 

 Influence of Unannotated Data We further analyzed how plain fine-tuning on summaries differs from our approach in terms of capturing summary characteristics. For comparison, we used USL and USL+F, which are presented in Sec. 6.1. Additionally, we analyzed unannotated reviews from the Amazon training set. Specifically, we focused on text formality and the average word count difference (Len) from the gold summaries in the Amazon test set. As a proxy for the former, we computed the marginal distribution over points of view (POV), based on pronoun counts; an additional class (NoPr) was allocated to cases of no pronouns. The results are presented in Table  9 . First, we observed that the training reviews are largely informal (49.0% and 7.3% for 1st and 2nd POV, respectively). Unsurprisingly, the model trained only on the reviews (USL) transfers a similar trait to the summaries that it generates.  6  On the contrary, the gold summaries are largely formalindicated by a complete absence of the 1st and a marginal amount of 2nd POV pronouns. Also, an average review is substantially shorter than an average gold summary, and consequently, the generated summaries by USL are also shorter. Example summaries are presented in Table  10 . Further, we investigated how well USL+F, adapts to the summary characteristics by being ac- 

 Gold These shoes run true to size, do a good job supporting the arch of the foot and are well-suited for exercise. They're good looking, comfortable, and the sole feels soft and cushioned. Overall they are a nice, light-weight pair of shoes and come in a variety of stylish colors. 

 FewSum These running shoes are great! They fit true to size and are very comfortable to run around in. They are light weight and have great support. They run a little on the narrow side, so make sure to order a half size larger than normal. 

 USL+F This is my second pair of Reebok running shoes and they are the best running shoes I have ever owned. They are lightweight, comfortable, and provide great support for my feet. 

 USL This is my second pair of Reebok running shoes and I love them. They are the most comfortable shoes I have ever worn. tually fine-tuned on them. Indeed, we observed that USL+F starts to shift in the direction of the summaries by reducing the pronouns of the 1st POV and increasing the average summary length. Nevertheless, the gap is still wide, which would probably require more data to be bridged. Finally, we observed that our approach adapts much better to the desired characteristics by producing well-formed summary text that is also very close in length to the gold summaries. 

 Cross-Domain We hypothesized that on a small dataset, the model primarily learns course-grained features, such as common writing phrases, and their correlations between input reviews and summaries. Also, that they, in principle, could be learned from remotely related domains. We investigated that by fine-tuning the model on summaries that are not in the target domain of the Amazon dataset. Specifically, we matched data-point count for 3/4 domains of training and validation sets to the in-domain Amazon data experiment presented in Sec 5; the test set remained the same for each domain as in the in-domain experiment. Then, we fine-tuned the same model 5 times with different seeds per target domain. For comparison, we used the in-domain model which was used in Amazon experiments in Sec. 5. We computed the average ROUGE-L score per target domain, where overall ? was 0.0137. The results are reported in Table  11 . The results indicate that the models perform onpar on most of the domains, supporting the hypothesis. On the other hand, the in-domain model shows substantially better results on the health domain, which is expected, as, intuitively, this domain is the most different from the rest. 

 Related Work Extractive weakly-supervised opinion summarization has been an active area of research. LEXRANK  (Erkan and Radev, 2004 ) is an unsupervised extractive model. OPINOSIS  (Ganesan et al., 2010)  does not use any supervision and relies on POS tags and redundancies to generate short opinions. However, this approach is not well suited for the generation of coherent long summaries and, although it can recombine fragments of input text, it cannot generate novel words and phrases. Other earlier approaches  (Gerani et al., 2014; Di Fabbrizio et al., 2014)  relied on text planners and templates, which restrict the output text. A more recent extractive method of  Angelidis and Lapata (2018)  frames the problem as a pipeline of steps with different models for each step.  Isonuma et al. (2019)  introduce an unsupervised approach for single product review summarization, where they rely on latent discourse trees. The most related unsupervised approach to this work is our own work, COPYCAT  (Bra?inskas et al., 2020) . Unlike that work, we rely on a powerful generator to learn conditional spaces of text without hierarchical latent variables. Finally, in contract to MEANSUM  (Chu and Liu, 2019) , our model relies on inductive biases without explicitly modeling of summaries. A concurrent model DENOISESUM (Amplayo and Lapata, 2020) uses a syntactically generated dataset of source reviews to train a generator to denoise and distill common information. Another parallel work, OPINIONDIGEST  (Suhara et al., 2020) , considers controllable opinion aggregation and is a pipeline framework for abstractive summary generation. Our conditioning on text properties approach is similar to Ficler and Goldberg (  2017 ), yet we rely on automatically derived properties that associate a target to source, and learn a separate module to generate their combinations. Moreover, their method has not been studied in the context of summarization. 

 Conclusions In this work, we introduce the first to our knowledge few-shot framework for abstractive opinion summarization. We show that it can efficiently utilize even a handful of annotated reviews-summary pairs to train models that generate fluent, informative, and overall sentiment reflecting summaries. We propose to exploit summary related properties in unannotated reviews that are used for unsupervised training of a generator. Then we train a tiny plug-in network that learns to switch the generator to the summarization regime. We demonstrate that our approach substantially outperforms competitive ones, both abstractive and extractive, in human and automatic evaluation. Finally, we show that it also allows for successful cross-domain adaptation. 

 Appendices 

 Dataset Pre-Processing We selected only Amazon products and Yelp businesses with minimum of 10 reviews, and the minimum and maximum lengths of 20 and 70 words, respectively. Also, popular products/businesses above the 90 th percentile were removed. From each business/product we sampled 9 reviews without replacement to form groups of reviews. 

 Evaluation Data Split From the Amazon annotated dataset, We used 28, 12, 20 products for training, validation, and testing, respectively. On Yelp, we used 30, 30, 40 for training, validation, and testing, respectively. Both the automatic and human evaluation experiments were performed on the test sets. 

 Training Procedure First, to speed-up the training phase, we trained an unconditional language model for 13 epoch on the Amazon reviews with the learning rate (LR) set to 5 * 10 ?4 . On Yelp we trained it for 27 epochs with LR set to 7 * 10 ?4 . The language model was used to initialize both the encoder and generator of the main model. Subsequently, we trained the model using Eq. 2 for 9 epochs on the Amazon reviews with 6 * 10 ?5 LR, and for 57 epochs with LR set to 5 * 10 ?5 . Additionally, we reduced novelty using Eq. 4 by training the model further for 1 epoch with 10 ?5 LR and ? = 2 on Amazon; On Yelp we trained for 4 epochs, with 3 * 10 ?5 LR, and ? = 2.5. For the plugin network's initialization, as explained in Sec. 3.1, we performed optimization by output matching with the oracle for 11 epochs on the unannotated Amazon reviews with 1 * 10 ?5 LR. On Yelp we trained for 87 epochs with 1 * 10 ?5 Lastly, we fine-tuned the plugin network on the human-written summaries by output matching with the oracle 7 . On the Amazon data for 98 epochs with 7 * 10 ?4 , and for 62 epochs with 7 * 10 ?5 on Yelp. We set weights to 0.1, 1., 0.08, 0.5 for length deviation, rating deviation, POV, and ROUGE scores, respectively. Then fine-tuned the attention part of the model and the plug-in network jointly for 33 epochs with 1 * 10 ?4 on the Amazon data. And 23 epochs with 1 * 10 ?4 LR on Yelp.  7  We set rating deviation to 0 as summaries do not have associated human-annotated ratings. 

 Summary Annotation For summary annotation, we reused 60 Amazon products from  Bra?inskas et al. (2020)  and sampled 100 businesses from Yelp. We assigned 3 Mechanical Turk workers to each product/business, and instructed them to read the reviews and produce a summary text. We used the following instructions: ? The summary should reflect user common opinions expressed in the reviews. Try to preserve the common sentiment of the opinions and their details (e.g. what exactly the users like or dislike). For example, if most reviews are negative about the sound quality, then also write negatively about it. ? Please make the summary coherent and fluent in terms of sentence and information structure. Iterate over the written summary multiple times to improve it, and re-read the reviews whenever necessary. ? The summary should not look like a review, please write formally. ? Keep the length of the summary reasonably close to the average length of the reviews. ? Please try to write the summary using your own words instead of copying text directly from the reviews. Using the exact words from the reviews is allowed but do not copy more than 5 consecutive words from a review. 

 Human Evaluation Setup To perform the human evaluation experiments described in Sec 5, we hired workers with 98% approval rate, 1000+ HITS, Location: USA, UK, Canada, and the maximum score on a qualification test that we had designed. The test was asking if the workers were native English speakers, and was verifying that they correctly understood the instructions of both the best-worst scaling and content support tasks. 

 Best-Worst Scaling Details We performed human evaluation based on the Amazon and Yelp test sets using the AMT platform. We assigned workers to each tuple containing summaries from COPYCAT, our model, LEXRANK, and human annotators. Due to dataset size differences, we assigned 5 and 3 workers to each  tuple in the Amazon and Yelp test sets, respectively. We presented the associated reviews in a random order and asked the workers to judge summaries using the Best-Worst scaling (BWS)  (Louviere and Woodworth, 1991; Louviere et al., 2015)  that is known to produce more reliable results than ranking scales  (Kiritchenko and Mohammad, 2016) . The judgment criteria are presented where non-redundancy and coherence were taken from  Dang (2005) . Fluency: the summary sentences should be grammatically correct, easy to read and understand; Coherence: the summary should be well structured and well organized; Nonredundancy: there should be no unnecessary repetition in the summary; Informativeness: how much useful information about the product does the summary provide?; Sentiment: how well the sentiment of the summary agrees with the overall sentiment of the original reviews? 

 Points of View Summaries differ from reviews in terms of the writing style. Specifically, reviews are predominantly written informally, populated by pronouns such as I and you. In contrast, summaries are desirable to be written formally. In this work, we observed that a surprisingly simple way to achieve that is to condition the generator on the distribution over pronoun classes of the target review. We computed pronoun counts and produced the 4 class distributions: 1st, 2nd, 3rd person POV, and 'other' in case if no pronouns are present. Consider the example sentences in Table  12 . Here one can observe that the sentences of different pronoun classes differ in the style of writing and often the intention of the message: 1st POV sentences tend to provide clues about the personal experience of the user; 2nd POV sentences, on the other hand, commonly convey recommendations to a reader; 3rd POV and 'other' sentences often describe aspects and their associated opinions. 

 Gold Bennett Medical has poor customer service. Phone calls can take a long time to get answered and leaving voice mails tend to be fruitless. The products are overpriced and take a long time to be refilled. Using this medical supply company can be a hassle. 

 Ours This medical supply is the worst medical supply company in the valley. The customer service is horrible, the staff is rude, the wait times are long, and the service reps are not helpful at all. Do not recommend this company to anyone. 

 Copycat If I could give them 0 stars I would. The customer service is terrible, the staff is extremely rude and helpful. If you're looking for a new provider, this is the place to be. I will never use them again. 

 MeanSum Service is horrible, especially the manager. I have a lot of kids but not this place. Two months later I was able to go in and get a new one to go in the next day. They would tell me that they would have to do a lot of our water to be there to get a new one. But this is the first time I have dealt with him and we will never go back again. Thanks for your hard work, and I will never go anywhere else. Review 5 DON'T use this medical supply. Never enough staff to answer phone, so you'll need to leave messages. No return phone calls. I am unable to get my CPAP supplies every quarter without hours of calling / waiting / calling. Poor customer service. Will be moving to another medical supply as soon as I can. 

 Lexrank Review 6 Terrible experience. They have ridiculous price also bad customer services. You can get nebulizer machine around $50 at amazon, bennet medical charge you almost twice more expensive price. And breathing kit price was unbelievable too. Because of deduction, I had to pay them all out of my pocket whatever they charged. I don't recommand this medical company to anyone. 

 Review 7 Good luck getting a phone call back or someone to answer the phone without hanging up immediately. I have called over 20 times left 5 voicemails over the last 30 days, just to refill a mask perscription. This is an ongoing issue that is beyond frustrating. Not trying to destroy this businesses name just want the owners to implement some basic customer service skills. 

 Review 8 Always receive friendly customer service whenever we call or go into the location. My questions are always answered and I am very happy with the supplies we get from them. Great people providing a great service! Thank you for all you do! Table  13 : Example summaries produced by different systems on Yelp data. 

 Gold It is very clean and nice inside. Everyone is so kind and friendly. They do an amazing job on both nails and pedis. They get it done with speed and precision with a price that is very much affordable. They have the best customer service. 

 Ours This nail salon is very clean and the staff is very friendly. They have a wide variety of gel colors to choose from. The prices are very reasonable and they do a great job. The nail techs are very nice and do great work. 

 Copycat This is the best nail salon I have ever been to. Everyone is very friendly and professional. My nails look great and I'm glad I did! I will definitely be coming back to this place. 

 MeanSum The owner is so nice and accommodating. I went to get my nails done by a friend, and I was extremely happy with this salon. Everyone was very friendly and I was able to use them for nails. They did a great job on my nails and the best part about it was that it was a busy day but it was a treat! Highly recommend them. Lexrank I really enjoy coming here to get my nails done. B did an amazing job on my nails. Amazing service and nails. However B did an AMAZING job on my coffin chrome nails and Nancy was extremely helpful figuring out how I wanted my nails done too. Everyone is so friendly there too. Review 1 Tim and Tami always always always have the best customer service and do the best nails. I will NEVER go anywhere else. Even after weeks my nails look and feel as good as they did when I first got them done! I'm so dedicated I recommend and bring in all my friends! Review 2 Definitely my new nail salon! Everyone is so friendly and kind, I felt so welcomed! B did an amazing job on my nails. He made sure everything was perfect and happily changed something to make me happy. I would highly recommend this place to anyone who wants A + work at a totally affordable price. Love it!!:) Review 3 Amazing service and nails. This is the second time I have been here, they did a perfect job again. They get it done fast yet with precision. Everyone is so friendly there too. Best nail salon I have ever been too. I'm glad I found it. Review 4 I really enjoy coming here to get my nails done. They do a wonderful job on both pedis and nails. It is nice and clean inside. They are very friendly and welcoming. It is worth it to stop in and try it out. 

 Review 5 My first set of acrylics ever... I decided 27 years was a lot enough time to wait, and I'm SO happy with them. I'm not a huge nail person, and was glad to stumble upon this salon. My nail tech was quiet, clean, and very detail-oriented. Very pleased with my experience here and I recommend this place. Review 6 I called to make an appointment for later today for 3 adults and 2 kids and the man who answered the phone said 'we only have 2 techs today' we can't do that. Poor customer service and I never even went in. Review 7 Golden Nails has been my nail place for almost a year so it was surprising to see new management. However B did an AMAZING job on my coffin chrome nails and Nancy was extremely helpful figuring out how I wanted my nails done too. Definitely excited to keep coming back! Review 8 Seriously the best service I have ever gotten at a Tempe nail salon!! I walked in and they helped me right away. Nancy helped me pick the perfect color and was very honest and up front about everything! I wanted something very natural and using the dip method, I love my nails!!  perfect fit for me ... supply the support that I need ... are flexible and comfortable ... || ... It is very comfortable ... I enjoy wearing them running ... || ... running shoes ... felt great right out of the box ... They run true to size ... || ... my feet and feel like a dream ... Totally light weight ... || ... shoes run small ... fit more true to size ... fit is great! ... supports my arch very well ... || ... They are lightweight... usually wear a size women's 10 ... ordered a 10.5 and the fit is great! 

 H T d b 6 e w s b m 1 v V P c L e 3 t H x w e l Y 9 P 2 i Z O N e M t F s t Y d w N q u B S K t 1 C g 5 N 1 E c x o F k n e C y e 3 c 7 z x x b U S s H n G a c D + i I y V C w S h a 6 U E P x K B c c a v u A m S d e D m p Q I 7 m o 

 H T d b 6 e w s b m 1 v V P c L e 3 t H x w e l Y 9 P 2 i Z O N e M t F s t Y d w N q u B S K t 1 C g 5 N 1 E c x o F k n e C y e 3 c 7 z x x b U S s H n G a c D + i I y V C w S h a 6 U E P x K B c c a v u A m S d e D m p Q I 7 m o 

 Bennett Medical for Cpap supplies are horrible. Never enough staff to answer phone, so you'll need to leave messages. DON'T use this medical supply. If I could give Bennett Medical zero stars I would! Will be moving to another medical supply as soon as I can. Review 1 Bennett Medical for Cpap supplies are horrible. We have waited for three weeks to refill supplies and we are still waiting. This company does not have good customer service, you can only leave messages, and they never call back. If I could give Bennett Medical zero stars I would! Review 2 Teachers Health Trust, please look into the practice of the billing and filling of durable services. The mask cushions go for 45 to 50 days because of the lack of communication. The people in charge of billing are very argumentative and lack customer service. I will drop them after annual, because of my insurance obligations. Review 3 Fantastic service from Jocelyn at the front desk, we had a really hard time getting the right paperwork together from Drs but she stuck with us and helped us every step of the way, even calling to keep us updated and to update info we might have for her. Thanks Jocelyn. Review 4 I hardly ever write reviews, but I'd like to spare someone else from what I experienced. So a warning to the wise... If you like rude incompetent employees, almost an hour long wait for just picking up a phone order, and basically being treated like a second class citizen then look no further than Bennett Medical. 

 Table 2 : 2 Data statistics after pre-processing. The format in the cells is Businesses/Reviews and Products/Reviews for Yelp and Amazon, respectively. , details are presented in Appendix 9.1. For training, we partitioned business/product reviews to the groups of 9 reviews by sampling without replacement. Thus, for unsupervised training in Sec. 2, we conditioned on 8 reviews for each target review. The data-statistics are shown in Table 2. 

 Table 4 : 4 ROUGE scores on the Yelp test set. , respec- datasets. Also, the results are supported by qualitative improvements over other models, see examples in the Appendix. 

 Table 6 6 our model outperforms the other models. All pairwise differences between our model and other models are statistically significant at 

 Table 5 : 5 Human evaluation results in terms of the Best-Worst scaling on the Amazon test set. Fluency Coherence Non-Redundancy Informativeness Sentiment FewSum 0.1636 0.1429 0.0000 0.3793 0.3725 Copycat -0.2000 -0.0769 0.1053 -0.4386 -0.2857 LexRank -0.5588 -0.5312 -0.6393 -0.6552 -0.4769 Gold 0.5278 0.3784 0.4795 0.6119 0.4118 

 Table 6 : 6 Human evaluation results in terms of the Best-Worst scaling on the Yelp test set. Full (%) Partial (%) No (%) FewSum 43.09 34.14 22.76 Copycat 46.15 27.18 26.67 

 Table 7 : 7 Content support on the Amazon test set. p < 0.05, using post-hoc HD Tukey tests. The only exception is non-redundency on Yelp when comparing our model and COPYCAT (where our model shows a slightly lower score). Content Support As was observed by Falke et al. (2019); Tay et al. (2019); Bra?inskas et al. 

 Table 8 : 8 ROUGE scores on the Amazon test set for alternative summary adaptation strategies. R2 RL FewSum 0.3356 0.0716 0.2149 MTL 0.2403 0.0435 0.1627 USL+F 0.2823 0.0624 0.1964 USL 0.2145 0.0315 0.1523 Random 0.2500 0.0382 0.1572 1st 2nd 3rd NoPr Len Gold 0.0 1.7 60.0 38.3 0.0 FewSum 0.5 1.3 83.2 15.0 3.4 USL+F 29.7 0.0 45.3 25.0 -28.6 USL 56.7 0.0 43.3 0.0 -32.7 Reviews 49.0 7.3 35.6 8.1 -17.6 

 Table 9 : 9 Text characteristics of generated summaries by different models on the Amazon test set. 

 Table 10 : 10 Example summaries produced by models with different adaptation approaches. Domain In-domain Cross-domain Cloth 0.2188 0.2220 Electronics 0.2146 0.2136 Health 0.2121 0.1909 Home 0.2139 0.2250 Avg 0.2149 0.2129 

 Table 11 : 11 In and cross domain experiments on the Amazon dataset, ROUGE-L scores are reported. 

 Table 12 : 12 Examples of review sentences that contain only pronouns belonging to a specific class. 

 Table 14 : 14 Example summaries produced by different systems on Yelp data. 

			 Both the code and datasets are available at: https:// github.com/abrazinskas/FewSum 

			 We explored that option, and observed that it works similarly, yet leads to a slightly worse result.4 https://www.yelp.com/dataset/ challenge 

			 We observed that the 1:1 review-summary proportion works the best. 

			 As beam search, attempting to find the most likely candidate sequence, was utilized, opposed to a random sequence sampling, we observed that generated sequences had no cases of the 2nd POV pronouns and complete absence of pronouns (NoPr).
