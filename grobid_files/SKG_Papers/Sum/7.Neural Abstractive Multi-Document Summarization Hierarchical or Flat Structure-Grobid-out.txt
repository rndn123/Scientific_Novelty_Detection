title
Neural Abstractive Multi-Document Summarization: Hierarchical or Flat Structure?

abstract
With regards to WikiSum (Liu et al., 2018b)   that empowers applicative explorations of Neural Multi-Document Summarization (MDS) to learn from large scale dataset, this study develops two hierarchical Transformers (HT) that describe both the cross-token and crossdocument dependencies, at the same time allow extended length of input documents. By incorporating word-and paragraph-level multihead attentions in the decoder based on the parallel and vertical architectures, the proposed parallel and vertical hierarchical Transformers (PHT &VHT) generate summaries utilizing context-aware word embeddings together with static and dynamics paragraph embeddings, respectively. A comprehensive evaluation is conducted on WikiSum to compare PHT &VHT with established models and to answer the question whether hierarchical structures offer more promising performances than flat structures in the MDS task. The results suggest that our hierarchical models generate summaries of higher quality by better capturing crossdocument relationships, and save more memory spaces in comparison to flat-structure models. Moreover, we recommend PHT given its practical value of higher inference speed and greater memory-saving capacity. 1

Introduction With the promising results achieved by neural abstractive summarization on single documents  (See et al., 2017; Cao et al., 2018; Liu et al., 2018a; Gehrmann et al., 2018) , an increasing number of attempts are made to study abstractive multidocument summarization (MDS) using seq2seq models  (Liu et al., 2018b; Lebanoff et al., 2018; Fabbri et al., 2019; Liu and Lapata, 2019) . Compared with the single-document summarization, multi-document summarization places challenges 1 https://github.com/yema2018/wiki_sum in two primary aspects, that is representing large source documents and capturing cross-document relationships. To address the former issue,  Liu et al. (2018b)  adopts a two-stage approach by first selecting a list of important paragraphs from all documents in an extractive framework. Then a modified language model based on the Transformer-decoder with memory compressed attention (T-DMCA) is developed to conduct abstractive summarization after concatenating the extracted paragraphs to a flat sequence. Although the proposed flat structure of T-DMCA demonstrates both theoretical and practical soundness to learn long-term dependencies, it fails to implant the cross-document relationship in its summaries. On the other hand, the encoderdecoder structure that allows hierarchical inputs of multiple documents offers not only another solution to the long-text summarization problem  (Li et al., 2018; Zhang et al., 2019; Liu and Lapata, 2019)  but also allows cross-document information exchange in the produced summaries. In particular,  Liu and Lapata (2019)  proposes a Hierarchical Transformer with local and global encoder layers to represent cross-token and cross-paragraph information, which are both utilized later to enrich token embeddings. Summaries are then generated based on a vanilla Transformer  (Vaswani et al., 2017)  by concatenating enriched token embeddings from different documents to a flat sequence. Such Hierarchical Transformer though captures crossdocument relationships, the essentially-flat Transformer it adopts fails to learn dependencies of sequences longer than 2000 tokens according to  Liu et al. (2018b) . In this paper, we develop two novel hierarchical Transformers to address both the text-length and cross-document linkage problems in MDS. By introducing the word-level and paragraph-level multi-head attention mechanisms, our models are designed to learn both cross-token and cross- document relationships. The word-and paragraphlevel context vectors are then jointly used to generate target sequences in order to abandon the flat structure, thus to mitigate the long-dependency problem. In detail, both of the proposed hierarchical architectures are based on the Transformer encoder-decoder model  (Vaswani et al., 2017) , with context-aware word embeddings obtained from a shared encoder and cross-token linkages described by the word-level multi-head attention mechanism in the decoder. The difference lies in the way that the document-level information is handled. Based on the static 2 paragraph embeddings computed from the context-aware word embeddings, the parallel hierarchical Transformer (PHT) models crossdocument relationships with paragraph-level multihead attention parallel to the word-level multi-head attention. The paragraph attentions are then used to normalize the word attentions. On the other hand, the vertical hierarchical Transformer (VHT) stacks the paragraph-level attention layer on top of the word-level attention layer in order to learn the latent relationship between paragraphs with dynamic 3 paragraph embeddings from the previous layer. To evaluate the performance of the proposed models as well as to compare flat and hierarchical structures in the MDS task, we select several strong baselines covering abstractive models of flat strucuture (T-DMCA  (Liu et al., 2018b)  and Transformer-XL  (Dai et al., 2019) ) and of hierarchical structure  (Liu's hierachical Transformer (Liu and Lapata, 2019) ). A systematic analysis is conducted on the WikiSum dataset according 2 static means the embedding remains the same for different time steps in the decoder. 3 dynamic means the embeddings are dynamic for different time steps in the decoder. to four criteria including the models' abilities of capturing cross-document relationships, ROUGE evaluation, human evaluation and computational efficiency. The results show that PHT&VHT outperform other baselines significantly with memory space. 

 Related work Neural multi-document summarization Regarding to extractive models, neural networks are the most widely-used approach to model in-and cross-document knowledge with the objective to minimize the distance between the selected sentence set and the gold summary  (Cao et al., 2017; Ma et al., 2016; Nallapati et al., 2016; Yasunaga et al., 2017) . One representative study  (Yasunaga et al., 2017)  is to construct a graph of the document cluster based on the similarities between sentences. Graph Neural Network (GNN) (Kipf and Welling, 2016) is then employed to select salient sentences. Argued by  Liu and Lapata (2019) , self-attention is a better mechanism to learn the latent dependency among documents than GNNs. As for abstractive models, studies tend to extract important paragraphs from different documents followed by a abstractive seq2seq model to generate summaries  (Liu et al., 2018b; Liu and Lapata, 2019; Fabbri et al., 2019) . Additionally,  Chu and Liu (2019)  adopts an auto-encoder model to conduct MDS in an unsupervised way. Hierarchical neural network Hierarchical neural document models are applied in various fields of NLP such as document auto-encoder  or text classification . In the area of abstractive summarization,  Li et al. (2018)  extends a hierarchical RNN encoderdecoder  (Lin et al., 2015)  with the hybrid sentenceword attention. Instead of trainable attention machanisms,  Fabbri et al. (2019)  hires a hierarchical RNN with Maximal Marginal Relevance (MMR)  (Carbonell and Goldstein, 1998)  to represent the relationship between sentences.  Liu and Lapata (2019)  proposes a hierarchical Transformer by incorporating a global self-attention to represent cross-document relationships. Moreover,  Zhang et al. (2019)  constructs a hierarchical BERT  (Devlin et al., 2018)  to learn the context relationships among sentences by using other sentences to generate the masked sentence. 

 Hierarchical Transformer This paper proposes two hierarchical Transformers with parallel & vertical architectures, respectively. Section 3.1 explicitly explains the construction of the parallel hierarchical Transformer (PHT) and its application in MDS, whereas Section 3.2 places emphasis on explaining the structural differences between the vertical hierarchical Transformer (VHT) and PHT. 

 Parallel hierarchical Transformer 

 Encoder As shown in Figure  2 , the PHT encoder is shared by all paragraphs and consist of two major units, i.e. the transformer encoder and the Multi-head Attention Pooling layer, to obtain the token-and paragraph-embeddings. To be specific, contextaware word embeddings are first produced as the output of the transformer encoder based on the summation of word embeddings W and fixed positional encodings  (Vaswani et al., 2017) . C p = T ransE(W p + E p ) (1) where C p ? R n?d denotes context-aware word embeddings in the paragraph p and n is the paragraph length. We select the fixed encoding method rather than other learning models given that the former has the capacity to deal with sequences of arbitrary length. The context-aware word embedding is then used to generate paragraph embeddings as well as being a part of inputs to the PHT decoder. As the second step, the parallel architecture generates additional static paragraph embeddings to model cross-document relationships from the multihead attention pooling: head i p = HeadSplit(C p W 1 ) (2) ? i p = (Sof tmax(head i p W 2 )) T head i p (3) ? p = W 3 [? 0 p ; ? 1 p ; ? ? ?] (4) ? p := layerN orm(? p + F F N (? p )) (5) where W 1 ? R d?d , W 2 ? R d head ?1 and W 3 ? R d?d are linear transformation parameters, head i p ? R n?d head and ? i p ? R d head denote the i th attention head and paragraph embedding. These head embeddings are concatenated and fed to a two-layer feed forward network (FFN) with Relu activation function after linear transformation. The paragraph embedding is another input to the decoder, together with the context-aware word embedding. 

 Decoder The PHT decoder accepts three classes of inputs, namely the target summary, context-aware word embeddings in the p th paragraph C p ? R n?d where n is the length of the paragraph, and static paragraph embeddings ? ? R m?d where m is the number of paragraphs. Let X 1 ? R k?d denote the output of part I where k is the length of target sequence or the number of time steps. Note that both the word embedding and vocabulary in the decoder part I are shared with the encoder. Paragraph embeddings are added with the ranking encoding R generated by the positional encoding function  (Vaswani et al., 2017 ): 4 ? := ? + R (6) Different from the token-level ranking encoding  (Liu and Lapata, 2019) , we intend to incorporate the positional information of paragraphs to their embeddings. The PHT decoder consists of three parts. Similar to a vanilla Transformer  (Vaswani et al., 2017) , the first and last parts of the PHT decoder are the masked multi-head attention and the feed forward network, whereas the second part includes two parallel multi-head attentions to capture the inter-word and inter-paragraph relations. Paragraph-level multi-head attention: This self-attention mechanism is to create paragraphlevel context vectors that represent the latent crossparagraph relationships. The query is the output of part I: X 1 , whilst the key and value are static paragraph embeddings ?: X para , A para = M ultiHead(X 1 , ?, ?), (7) where X para ? R k?d is the paragraph-level context vector and A para ? R k?m denotes the attention weights of paragraphs 5 . Both X para and A para are comprised of representations of all time steps. Word-level multi-head attention: This shared self-attention mechanism is to output word-level context vectors which represent the cross-token dependency for each paragraph. Since there are m paragraphs, so the mechanism is implemented m times at each time step. The query of self attention  is X 1 , whilst the key and value are context-aware word embeddings C p . X word p = M ultiHead(X 1 , C p , C p ), (8) where X word p ? R k?d denotes the word-level context vectors of all time steps in the p th paragraph. The outputs X word ? R k?d?m are integrated by first being normalized by paragraph attentions A para , then propagated to subsequent layers after summation. X int = X word A para (9) where the dimension of A para is expanded to R k?m?1 and matrices are multiplied in the last two dimensions so X int ? R k?d . The output of part II: X 2 is written as: X 2 = LayerN orm(X 1 + X para + X int ). ( 10 ) With the outputs of part II, we are able to proceed to part III and compute the final probability distributions. 

 Vertical hierarchical Transformer The key difference between the parallel and the vertical architectures is the latter only passes contextaware word embeddings from the encoder to decoder part II without additional paragraph embeddings. Instead, the cross-document relationships in this architecture are modeled based on wordlevel context vectors by stacking the paragraphlevel multi-head attention vertically on top of the word-level multi-head attention. Vertical paragraph-level multi-head attention: Since the word-level context vectors X word t ? R m?d are the weighted summation of token embeddings in the paragraph at the t th time step, the VHT decoder regards them as dynamic paragraph embeddings, opposite to the static paragraph embeddings in PHT. According to Figure  3 , the dynamic paragraph embedding serves as the key and value of the vertical paragraph-level multihead attention after adding the ranking embeddings, and the query remains as the output of part I after separating in the time dimension, i.e., X 1 t ? R 1?d . X word t := X word t + R, (11) X para t = M ultiHead(X 1 t , X word t , X word t ), (12) where X para t ? R 1?d are concatenated to X para ? R k?d along time steps before passed to decoder part III with X 1 :   (Bojanowski et al., 2017 ) is adopted to tokenize our vocabulary to 32,000 subwords to better solve unseen words. X 2 = LayerN orm(X 1 + X para ). (13 

 Training configuration We apply a dropout rate of 0.3 to the output of each sub-layer and a warm-up Adam optimizer  (Vaswani et al., 2017)  with 16,000 warm-up steps. Given the limited computing resources (one 2080Ti), we stack 3-layers of encoder-decoder in both of our hierarchical Transformers with 256 hidden units, 1024 units in the feed-forward network and 4 headers. To demonstrate that our model has the potential to stack, 1-layer models are trained for comparison. All parameters are randomly initialized including token embeddings. All multi-layer models are trained for approximately 600,000 steps, while single-layer models for approximately 300,000 steps. Checkpoints are saved per 20,000 steps and the best-performing checkpoint on the validation set is used to generate the final summary. During the inference, the beam size is set as 5 and the average length normalization is used. The beam search is terminated til the length exceeds 200. In addition, we disallow repetition of trigrams and block two tokens (except the comma) before the current step to prevent degeneration situations such as Mike is good at cooking and cooking. 

 Baselines We compare the proposed hierarchical Transformers with the following baselines of different modeling natures. 

 Extractive model Lead is an extractive model that extracts the top K tokens from the concatenated sequence, given that K is the length of the corresponding gold summary. We combine paragraphs in order and place the title at the beginning of the concatenated sequence. Abstractive model with flat structure Flat Transformer (FT) is the vanilla Transformer encoder-decoder model  (Vaswani et al., 2017) . In this study, We adopt a 3-layers Transformer and truncate the flat sequence to 1600 tokens. T-DMCA  (Liu et al., 2018b ) is a Transformerdecoder model that splits a concatenated sequence into segments, and uses a Memory Compressed Attention to exchange information among them. We construct this model with 3 layers and 256 hidden states. The top 3000 tokens are truncated as inputs. Transformer-XL  (Dai et al., 2019)  is a language model that excels in handling excessively long sequences. This model improves the vanilla Transformer-decoder with the recurrent mechanism and relative positional encoding. We use 512 memory length and disable the adaptive softmax, with other hyper-parameters and token length remained the same as T-DMCA. Abstractive model with hierarchical structure Liu's Hierarchical Transformer (Liu's HT)  (Liu and Lapata, 2019)  uses a hierarchical structure to enrich tokens with information from other paragraphs before inputting to the flat Transformer. We use 3 local-attention layers and 3 global-attention layers introduced in  Liu and Lapata (2019) . Since this model is essentially based on the flat Transformer where token length should not exceed 2000, concatenated sequences are truncated to 1600 tokens. Parallel & Vertical Hierarchical Transformer (PHT/VHT) are models proposed in this paper. To verify that the models could be improved with deeper architectures, we train two 1-layer models to compare with the 3-layer models. We extract the top 30 paragraphs with 100 tokens per paragraph as inputs, and concatenate the title before the first paragraph. 

 Results 

 The ability of capturing cross-document relationships Cross-document relationships could be reflected by paragraph attentions. That is to say, if a model assigns higher attention weights to more important paragraphs and vice versa, the model is believed to have greater capacity of capturing cross-document relationships. To analytically assess the models' performance in this aspect, we use paragraph attentions of written summaries as the gold attention distribution, and its cosine similarity to the attention distribution of generated summaries as the evaluation metric. To model the paragraph attention of gold summaries, the normalized tf-idf similarities between the gold summary and each input paragraph are computed as the gold attention distribution. For non-hierarchical models, the summation of token weights in each paragraph are computed to indicate each paragraph's attention, whilst the hierarchical model returns the paragraph attention distribution directly from its paragraphlevel multi-head attention.   1  that hierarchical structures place significant improvements on the flat models in learning cross-document dependencies by assigning paragraph attentions in a way that is closer to the gold summaries. Moreover, VHT generates summaries of the greatest similarity 91.42% with the gold summaries, most likely due to its dynamic paragraph embedding architecture which allows more accurate representation of information that is continuously updated according to the changes of input targets. 

 ROUGE evaluation In this section, we adopt a widely-used evaluation metrics ROUGE  (Lin, 2004)  to evaluate the MDS models. ROUGE-1 & -2 and ROUGE-L F scores are reported in Table  2  assessing the informativeness and fluency of the summaries, respectively. As shown in Table  2 , the extractive model Lead exhibits overall inferior performance in comparison to the abstractive models, except that it produces a 0.11-higher ROUGE-L than the Flat Transformer. Although Liu's HT improves FT with a hierarchical structure, it fails to outperform the two extended flat models, i.e. T-DMCA and Transformer-XL, that are developed to learn with longer input of tokens. Moreover, T-DMCA and Transformer-XL, the two flat models based on the Transformer decoder, report comparable results in terms of the informativeness (ROUGE-1 & -2), whilst the latter outperforms the former by 0.41 in terms of the fluency (ROUGE-L). Further, the proposed hierarchical Transformers show promising ROUGE results. Profited from the pure hierarchical structure that enlarges the input length of tokens, PHT & VHT outperform Liu's HT in all domains of the ROUGE test. Moreover, the models' potential to be deepened is suggested by enhanced results of the 3-layer architecture over the 1-layer architecture. The ultimate 3-layer PHT & VHT surpass T-DMCA and Transformer-XL, the two flat models that also handle long input sequences of 3,000 tokens. Between the parallel and vertical architectures, PHT appears to be more informative in its summaries as it produces the highest ROUGE-1 & -2 among all models, whilst VHT is more fluent with the highest ROUGE-L. 

 Human evaluation To provide a better comparison between the hierarchical and the flat structures, we select 4 representative models with the best ROUGE performances, namely T-DMCA & Transformer-XL (flat structure), and PHT & VHT (hierarchical structure). The human evaluation is divided into two parts. The first part is to score multi-document summaries from four perspectives, including (A) Informativeness (Does the summary include all important facts in the gold summary), (B) Fluency (Is the summary fluent and grammatically-correct), (C) Conciseness (Does the summary avoid repetition and redundancy), (D) Factual consistency (Does the summary avoid common sense mistakes such as wrong date, wrong location, or anything else against facts). We specify five levels ranging from Very poor (1) to Very good (5) to assess criteria (A)-(C), and three levels of Much better (2), Better (1), and Hard to score (0) to assess criteria (D). Twenty examples are randomly selected from generated summaries. As shown in Table  3 , both Parallel and Vertical Hierarchical Transformer The second part of human evaluation is a side-byside preference test, which is comprised of thirty control groups of two sides. In each control group, Side A randomly places a summary generated by a flat model and side B places the corresponding summary generated by a hierarchical model. Assessors select their preferred side and briefly explain their reasons. Preference results show that the hierarchical class is approximately three times more likely to be chosen than the flat class, due to their overall accuracy and informativeness according to the assessors' comments.  6  It is interesting to note that the human evaluation suggests opposite results to the ROUGE test in terms of PHT&VHT's informativeness and fluency. The authors choose to place more trust on the quantitative measure, i.e. ROUGE, as it represents the quality of the entire sample rather than a limited segment of it. 

 Computational efficiency We assess the computational efficiency of the abstractive models in three aspects, namely the memory usage, parameter size and validation speed. We uniformly hire the 3-layers architecture and 1600 input tokens. In the experimental process, we increase the batch size until out of memory in a 2080ti GPU, and the model with the maximum batch size occupies the lowest memory space. To measure the parameter size, we count the number of parameters in the neural network. Finally, we run each trained model in the validation set (38,144 samples), and the average time consumed in each checkpoint is used to evaluate the efficiency of forward-propagating in the model. According to Table  4 , the hierarchical structure (the second panel) appears to be overall more memory-saving than the flat structure (the first panel), with higher requirements on the parameters. On the other hand, models based on the Transformer-decoder, i.e. Transformer-decoder, T-DMCA and Transformer-XL, demonstrate absolute superiority in reducing the parameter size. For the speed of forward-propagating, Transformer-XL dominates due to its recurrent mechanism, whereas VHT performs the worst in this aspect indicating the model's slow inference speed. Between the two proposed models, PHT is proven to outperform VHT in both the memory usage and inference speed, due to its parallel, rather than sequential, computation of the word & paragraph-level attention mechanisms. 

 Conclusion This paper proposes two pure hierarchical Transformers for MDS, namely the Parallel & Vertical Hierarchical Transformers (PHT & VHT). We experimentally confirm that hierarchical structure improves the quality of generated summaries over flat structure by better capturing cross-document relationships, at the same time saves more memory space. Given the similar performance of the two proposed models, we recommend PHT over VHT due to its practical value of higher inference speed and memory-saving capacity. Figure 1 : 1 Figure 1: Flat structure (top) -concatenating documents to a flat sequence. Hierarchical structure (bottom)-hierarchical input and representation of documents + modeling cross-document relationships. 
