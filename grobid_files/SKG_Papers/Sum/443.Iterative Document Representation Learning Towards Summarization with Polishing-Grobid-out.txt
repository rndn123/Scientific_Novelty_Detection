title
Iterative Document Representation Learning Towards Summarization with Polishing

abstract
In this paper, we introduce Iterative Text Summarization (ITS), an iteration-based model for supervised extractive text summarization, inspired by the observation that it is often necessary for a human to read an article multiple times in order to fully understand and summarize its contents. Current summarization approaches read through a document only once to generate a document representation, resulting in a sub-optimal representation. To address this issue we introduce a model which iteratively polishes the document representation on many passes through the document. As part of our model, we also introduce a selective reading mechanism that decides more accurately the extent to which each sentence in the model should be updated. Experimental results on the CNN/DailyMail and DUC2002 datasets demonstrate that our model significantly outperforms state-of-the-art extractive systems when evaluated by machines and by humans.

Introduction A summary is a shortened version of a text document which maintains the most important ideas from the original article. Automatic text summarization is a process by which a machine gleans the most important concepts from an article, removing secondary or redundant concepts. Nowadays as there is a growing need for storing and digesting large amounts of textual data, automatic summarization systems have significant usage potential in society. Extractive summarization is a technique for generating summaries by directly choosing a subset of salient sentences from the original document to constitute the summary. Most efforts made towards extractive summarization either rely on human-engineered features such as sentence length, word position, and frequency  (Cohen, 2002; Radev et al., 2004; Woodsend and Lapata, 2010; Yan et al., 2011a Yan et al., ,b, 2012  or use neural networks to automatically learn features for sentence selection  (Cheng and Lapata, 2016; Nallapati et al., 2016a) . Although existing extractive summarization methods have achieved great success, one limitation they share is that they generate the summary after only one pass through the document. However, in real-world human cognitive processes, people read a document multiple times in order to capture the main ideas. Browsing through the document only once often means the model cannot fully get at the document's main ideas, leading to a subpar summarization. We share two examples of this. (1) Consider the situation where we almost finish reading a long article and forget some main points in the beginning. We are likely to go back and review the part that we forget. (2) To write a good summary, we usually first browse through the document to obtain a general understanding of the article, then perform a more intensive reading to select salient points to include in the summary. In terms of model design, we believe that letting a model read through a document multiple times, polishing and updating its internal representation of the document can lead to better understanding and better summarization. To achieve this, we design a model that we call Iterative Text Summarization (ITS) consisting of a novel "iteration mechanism" and "selective reading module". ITS is an iterative process, reading through the document many times. There is one encoder, one decoder, and one iterative unit in each iteration. They work together to polish document representation. The final labeling part uses outputs from all iterations to generate summaries. The selective reading module we design is a modi-fied version of a Gated Recurrent Unit (GRU) network, which can decide how much of the hidden state of each sentence should be retained or updated based on its relationship with the document. Overall, our contribution includes: 1. We propose Iterative Text Summarization (ITS), an iteration based summary generator which uses a sequence classifier to extract salient sentences from documents. 2. We introduce a novel iterative neural network model which repeatedly polishes the distributed representation of document instead of generating that once for all. Besides, we propose a selective reading mechanism, which decides how much information should be updated of each sentence based on its relationship with the polished document representation. Our entire architecture can be trained in an end-to-end fashion. 3. We evaluate our summarization model on representative CNN/DailyMail corpora and benchmark DUC2002 dataset. Experimental results demonstrate that our model outperforms state-of-the-art extractive systems when evaluated automatically and by human. 

 Related Work Our research builds on previous works in two fields: summarization and iterative modeling. Text summarization can be classified into extractive summarization and abstractive summarization. Extractive summarization aims to generate a summary by integrating the most salient sentences in the document. Abstractive summarization aims to generate new content that concisely paraphrases the document from scratch. With the emergence of powerful neural network models for text processing, a vast majority of the literature on document summarization is dedicated to abstractive summarization. These models typically take the form of convolutional neural networks (CNN) or recurrent neural networks (RNN). For example,  Rush et al. (2015)  propose an encoder-decoder model which uses a local attention mechanism to generate summaries.  Nallapati et al. (2016b)  further develop this work by addressing problems that had not been adequately solved by the basic architecture, such as keyword modeling and capturing the hierarchy of sentenceto-word structures. In a follow-up work,  Nallapati et al. (2017)  propose a new summarization model which generates summaries by sampling a topic one sentence at a time, then producing words using an RNN decoder conditioned on the sentence topic. Another related work is by  See et al. (2017) , where the authors use "pointing" and "coverage" techniques to generate more accurate summaries. Despite the focus on abstractive summarization, extractive summarization remains an attractive method as it is capable of generating more grammatically and semantically correct summaries. This is the method we follow in this work. In extractive summarization,  Cheng and Lapata (2016)  propose a general framework for single-document text summarization using a hierarchical article encoder composed with an attention-based extractor. Following this,  Nallapati et al. (2016a)  propose a simple RNN-based sequence classifier which outperforms or matches the state-of-art models at the time. In another approach,  Narayan et al. (2018)  use a reinforcement learning method to optimize the Rouge evaluation metric for text summarization. The most recent work on this topic is  (Wu and Hu, 2018) , where the authors train a reinforced neural extractive summarization model called RNES that captures cross-sentence coherence patterns. Due to the fact that they use a different dataset and have not released their code, we are unable to compare our models with theirs. The idea of iteration has not been well explored for summarization. One related study is  Xiong et al. (2016) 's work on dynamic memory networks, which designs neural networks with memory and attention mechanisms that exhibit certain reasoning capabilities required for question answering. Another related work is  (Yan, 2016) , where they generate poetry with iterative polishing sn chema. Similiar method can also be applied on couplet generation as in  (Yan et al., 2016) . We take some inspiration from their work but focus on document summarization. Another related work is  (Singh et al., 2017) , where the authors present a deep network called Hybrid MemNet for the single document summarization task, using a memory network as the document encoder. Compared to them, we do not borrow the memory network structure but propose a new iterative architecture. 

 Methodology 

 Problem Formulation In this work, we propose Iterative Text Summarization (ITS), an iteration-based supervised model for extractive text summarization. We treat the extractive summarization task as a sequence labeling problem, in which each sentence is visited sequentially and a binary label that determines whether or not it will be included in the final summary is generated. ITS takes as input a list of sentences s = {s 1 , . . . , s ns }, where n s is the number of sentences in the document. Each sentence s i is a list of words: s i = {w i 1 , . . . , w i nw }, where n w is the word length of the sentence. The goal of ITS is to generate a score vector y = {y 1 , . . . , y ns } for each sentence, where each score y i ? [0, 1] denotes the sentence's extracting probability, that is, the probability that the corresponding sentence s i will be extracted to be included in the summary. We train our model in a supervised manner, using a corresponding gold summary written by human experts for each document in training set. We use an unsupervised method to convert the human-written summaries to gold label vector y = {y 1 , ..., y ns }, where y i ? {0, 1} denotes whether the i-th sentence is selected (1) or not (0). Next, during training process, the cross entropy loss is calculated between y and y , which is minimized to optimize y. Finally, we select three sentences with the highest score according to y to be the extracted summary. We detail our model below. 

 Model Architecture ITS is depicted in Fig.  1 . It consists of multiple iterations with one encoder, one decoder, and one iteration unit in each iteration. We combine the outputs of decoders in all iterations to generate the extracting probabilities in the final labeling module. Our encoder is illustrated in the shaded region in the left half of Fig.  1 . It takes as input all sentences as well as the document representation from the previous unit D k?1 , processes them through several neural networks, and outputs the final state to the iterative unit module which updates the document representation. Our decoder takes the form of a bidirectional RNN. It takes the representation of sentence generated by the encoder as input, and its initial state is the polished document representation D k . Our last module, the sentence labeling module, concatenates the hidden states of all decoders together to generate an integrated score for each sentence. As we apply supervised training, the objective is to maximize the likelihood of all sentence labels y = {y 1 , ..., y ns } given the input document s and model parameters ?: log p(y |s; ?) = ns i=1 log p(y i |s; ?) (1) 4 Our Model 

 Encoder In this subsection, we describe the encoding process of our model. For brevity, we drop the superscript k when focusing on a particular layer. All the W 's and b's in this section with different superscripts or subscripts are the parameters to be learned. Sentence Encoder: Given a discrete set of sentences s = {s 1 , . . . , s ns }, we use a word embedding matrix M ? R V ?D to embed each word w i in sentence s i into continuous space ?i , where V is the vocabulary size, D is the dimension of word embedding. The sentence encoder can be based on a variety of encoding schemes. Simply taking the average of embeddings of words in a sentence will cause too much information loss, while using GRUs or Long Short-Term Memory (LSTM) requires more computational resources and is prone to overfitting. Considering above, we select positional encoding described in  (Sukhbaatar et al., 2015)  as our sentence encoding method. Each sentence representation ?i is calculated by ?i = nw j=1 l j ? ?i j , where ? is element-wise multiplication, l j is a column vector computed as l j,d = (1? j nw )?( d D )(1? 2j nw ), l j,d denotes the d-th dimension of l j . Note that throughout this study, we use GRUs as our RNN cells since they can alleviate the overfitting problem as confirmed by our experiments. As our selective reading mechanism (which will be explained later) is a modified version of original GRU cell, we give the details of the GRU here. GRU is a gating mechanism in recurrent neural networks, introduced in  (Cho et al., 2014) . Their performance was found to be similar to that of LSTM cell but using fewer parameters as described in  (Hochreiter and Schmidhuber, 1997) . The GRU cell consists of an update gate vector ? ! " ! # ! $%# ? # $ ? ' $ ? ( $ ? # )%# ? ' $%# ? ( $%# Iterative Unit ? # # ? ' # ? ' # ! $ 4 ( 4 ' 4 # 4 ( 4 ' 4 # 4 ( 4 ' 4 # Labeling ? 5 # ? # ? ' 6 # ? ( GRU GRU GRU Encoder Encoder Encoder ? Decoder Decoder Decoder Selective-reading ? ? Iterative Unit Iterative Unit ? Encoder 4 # ! $%# MLP 6 # 4 # 4 ' 4 ( 4 # < 4 ' < 4 ( < ! $%# Figure  1 : Model Structure: There is one encoder, one decoder and one iterative unit (which is used to polish document representation) in each iteration. The final labeling part is used to generating the extracting probabilities for all sentences combining hidden states of decoders in all iterations. We take a document consists of three sentences for example here. u i , a reset gate vector r i , and an output vector h i . For each time step i with input x i and previous hidden state h i?1 , the updated hidden state h i = GRU(x i , h i?1 ) is computed by: u i = ?(W (u) x i + U (u) h i?1 + b (u) ) (2) r i = ?(W (r) x i + U (r) h i?1 + b (r) ) (3) hi = tanh(W (h) x i + r i ? U h i?1 + b (h) ) (4) h i = u i ? hi + (1 ? u i ) ? h i?1 (5) where ? is the sigmoid activation function, W (u) , W (r) , W (h) ? R n H ?n I , U (u) , U (r) , U ? R n H ?n H , n H is the hidden size, n I is the size of input x i . To further study the interactions and information exchanges between sentences, we establish a Bi-directional GRU (Bi-GRU) network taking the sentence representation as input: ? ? s i = GRU fwd (? i , ? ? ? s i?1 ) (6) ? ? s i = GRU bwd (? i , ? ? ? s i?1 ) (7) ? ? s i = ? ? s i + ? ? s i ( 8 ) where ?i is the sentence representation input at time step i, ? ? s i is the hidden state of the forward GRU at time step i, and ? ? s i is the hidden state of the backward GRU. This architecture allows information to flow back and forth to generate new sentence representation ? ? s i . 

 Document Encoder: We must initialize a document representation before polishing it. Generating the document representation from sentence representations is a process similar to generating the sentence representation from word embeddings. This time we need to compress the whole document, not just a sentence, into a vector. Because the information a vector can contain is limited, rather than to use another neural network, we simply use a non-linear transformation of the average pooling of the concatenated hidden states of the above Bi-GRU to generate the document representation, as written below: D 0 = tanh(W 1 n s ns i=1 [ ? ? s i ; ? ? s i ] + b) (9) where '[?;?]' is the concatenation operation. Selective Reading module: Now we can formally introduce the selective reading module in Fig.  1 . This module is a bidirectional RNN consisting of modified GRU cells whose input is the sentence representation ? ? s = { ? ? s 1 , ..., ? ? s ns }. In the original version of GRU, the update gate u i in Equation 2 is used to decide how much of hidden state should be retained and how much should be updated. However, due to the way u i is calculated, it is sensitive to the position and ordering of sentences, but loses information captured by the polished document representation. Herein, we propose a modified GRU cell that replace the u i with the newly computed update gate g i . The new cell takes in two inputs, the sentence representation and the document representation from the last iteration, rather than merely the sentence representation. For each sentence, the selective network generates an update gate vector g i in the following way: f i = [ ? ? s i ? D k?1 ; ? ? s i ; D k?1 ] (10) F i = W (2) tanh(W (1) f i + b (1) ) + b (2) (11) g i = exp(F i ) ns j=1 exp(F j ) (12) where ? ? s i is the i-th sentence representation, D k?1 is the document representation from last iteration. Equation 5 now becomes: h i = g i ? hi + (1 ? g i ) ? h i?1 (13) We use this "selective reading module" to automatically decide to which extent the information of each sentence should be updated based on its relationship with the polished document. In this way, the modified GRU network can grasp more accurate information from the document. 

 Iterative Unit After each sentence passes through the selective reading module, we wish to update the document representation D k?1 with the newly constructed sentence representations. The iterative unit (also depicted above in Fig.  1 ) is designed for this purpose. We use a GRU iter cell to generate the polished document representation, whose input is the final state of the selective reading network from the previous iteration, h ns and whose initial state is set to the document representation of the previous iteration, D k?1 . The updated document representation is computed by: D k = GRU iter (h ns , D k?1 ) (14) 

 Decoder Next, we describe our decoders, which are depicted shaded in the right part of Fig.  1 . Following most sequence labeling task  (Xue and Palmer, 2004; Carreras and M?rquez, 2005)  where they learn a feature vector for each sentence, we use a bidirectional GRU dec network in each iteration to output features so as to calculate extracting probabilities. For k-th iteration, given the sentence representation ? ? s as input and the document representation D k as the initial state, our decoder encodes the features of all sentences in the hidden state h k = {h k 0 , ..., h k ns }: h k i = GRU dec ( ? ? s h k i?1 ) (15) h k 0 = D k (16) 

 Sentence Labeling Module Next, we use the feature of each sentence to generate corresponding extracting probability. Since we have one decoder in each iteration, if we directly transform the hidden states in each iteration to extracting probabilities, we will end up with several scores for each sentence. Either taking the average or summing them together by specific weights is inappropriate and inelegant. Hence, we concatenate hidden states of all decoders together and apply a multi-layer perceptron to them to generate the extracting probabilities: y = W (4) tanh(W (3) [h 1 ; ...; h k ] + b (3) ) + b (4) (17) where y = {y 1 , ..., y ns }, y i is the extracting probability for each setence. In this way, we let the model learn by itself how to utilize the outputs of all iterations and assign to each hidden state a reliable weight. In section 6, we will show that this labeling method outperforms other methods. 

 Experiment Setup In this section, we present our experimental setup for training and estimating our summarization model. We first introduce the datasets used for training and evaluation, and then introduce our experimental details and evaluation protocol. 

 Datasets In order to make a fair comparison with our baselines, we used the CNN/Dailymail corpus which was constructed by  Hermann et al. (2015) . We used the standard splits for training, validation and testing in each corpus (90,266/1,220/1,093 documents for  CNN and 196, 557/12, 147/10, 396 for DailyMail) . We followed previous studies in using the human-written story highlight in each article as a gold-standard abstractive summary. These highlights were used to generate gold labels when training and testing our model using the greedy search method similar to  (Nallapati et al., 2016a) . We also tested ITS on an out-of-domain corpus, DUC2002, which consists of 567 documents. Documents in this corpus belong to 59 various clusters and each cluster has a unique topic. Each document has two gold summaries written by human experts of length around 100 words. 

 Implementation Details We implemented our model in Tensorflow  (Abadi et al., 2016) . The code for our models is available online 1 . We mostly followed the settings in  (Nallapati et al., 2016a)  and trained the model using the Adam optimizer  (Kingma and Ba, 2014)  with initial learning rate 0.001 and anneals of 0.5 every 6 epochs until reaching 30 epochs. We selected three sentences with highest scores as summary. After preliminary exploration, we found that arranging them according to their scores consistently achieved the best performance. Experiments were performed with a batch size of 64 documents. We used 100-dimension GloVe  (Pennington et al., 2014)  embeddings trained on Wikipedia 2014 as our embedding initialization with a vocabulary size limited to 100k for speed purposes. We initialized out-of-vocabulary word embeddings over a uniform distribution within  [-0.2,0,2] . We also padded or cut sentences to contain exactly 70 words. Each GRU module had 1 layer with 200-dimensional hidden states and with either an initial state set up as described above or a random initial state. To prevent overfitting, we used dropout after each GRU network and embedding layer, and also applied L2 loss to all unbiased variables. The iteration number was set to 5 if not specified. A detailed discussion about iteration number can be found in section 7. 

 Baselines On all datasets we used the Lead-3 method as a baseline, which simply chooses the first three sentences in a document as the gold summary. On DailyMail datasets, we report the performance of SummaRuNNer in  (Nallapati et al., 2016a)  and the model in  (Cheng and Lapata, 2016) , as well as a logistic regression classifier (LReg) that they used as a baseline. We reimplemented the Hybrid MemNet model in  (Singh et al., 2017)  as one of our baselines since they only reported the performance of 500 samples in their paper. Also,  Narayan et al. (2018)  released their code 2 for the REFRESH model, we used their code to produce Rouge recall scores on the DailyMail dataset as they only reported results on CNN/DailyMail joint dataset. Baselines on CNN dataset are similar. 1 https://github.com/yingtaomj/Iterati ve-Document-Representation-Learning-Tow ards-Summarization-with-Polishing 2 https://github.com/EdinburghNLP/Refr esh On DUC2002 corpus, we compare our model with several baselines such as Integer Linear Programming (ILR) and LReg. We also report the performance of the newest neural networks model including  (Nallapati et al., 2016a; Cheng and Lapata, 2016; Singh et al., 2017) . 

 Evaluation In the evaluation procedure, we used the Rouge scores, i.e. Rouge-1, Rouge-2, and Rouge-L, corresponding to the matches of unigram, bigrams, and Longest Common Subsequence (LCS) respectively, to estimate our model. We obtained our Rouge scores using the standard pyrouge package 3 . To compare with other related works, we used full-length F1 score on the CNN corpus, limited length of 75 bytes and 275 bytes recall score on DailyMail corpus. As for the DUC2002 corpus, following the official guidelines, we examined the Rouge recall score at the length of 75 words. All results in our experiment are statistically significant using 95% confidence interval as estimated by Rouge script. Schluter (2017) noted that only using the Rouge metric to evaluate summarization quality can be misleading. Therefore, we also evaluated our model using human evaluation. Five highly educated participants were asked to rank 40 summaries produced by four models: the Lead-3 baseline, Hybrid MemNet, ITS, and human-authored highlights. We chose Hybrid MemNet as one of the human evaluation baselines since its performance is relatively high compared to other baselines. Judging criteria included informativeness and coherence. Test cases were randomly sampled from DailyMail test set. 

 Experiment analysis Table  1  shows the performance comparison of our model with other baselines on the DailyMail dataset with respect to Rouge score at 75 bytes and 275 bytes of summary length. Our model performs consistently and significantly better than other models on 75 bytes, while on 275 bytes, the improvement margin is smaller. One possible interpretation is that our model has high precision on top rank outputs, but the accuracy is lower for lower rank sentences. In addition,  (Cheng and Lapata, 2016)   to create sentence-level extractive labels to train their model, while our model uses an unsupervised greedy approximation instead. We also examined the performance of our model on CNN dataset as listed in Table To compare with other models, we used full-length Rouge F1 metric as reported by  Narayan et al. (2018) . Results demonstrate that our model has a consistently best performance on different datasets. In Table  3 , we present the performance of ITS on the out of domain DUC dataset. Our model outperforms or matches other basic models including LReg and ILR as well as neural network baselines such as SummaRuNNer with respect to the ground truth at 75 bytes, which shows that our model can be adapted to different copora maintaining high accuracy. In order to explore the impact of internal structure of ITS, we also conducted an ablation study in Table  4 . The first variation is the same model without the selective reading module. The second one sets the iteration number to one, that is, a model without iteration process. The last variation is to apply MLP on the output from the last iteration instead of concatenating the hidden states of all decoders. All other settings and parameters are the same. Performances of these models are worse than that of ITS in all metrics, which demonstrates the preeminence of ITS. More importantly, by this controlled experiment, we can verify the contribution of different module in ITS. 

 Further discussion Analysis of iteration number: We did a broad sweep of experiments to further investigate the influence of iteration process on the generated summary quality. First, we studied the influence of iteration number. In order to make a fair comparison between models with different iteration number, we trained all models for same epochs without tuning. Fig.  2    the result of training the model for only one epoch outperforms the state-of-the-art in  (Singh et al., 2017) , which demonstrates that our selective reading module is effective. The fact that continuing this process increase the performance confirms that the iteration idea behind our model is useful in practice. Based on above observation, we set the default iteration number to be 5. Analysis of polishing process: Next, to fully investigate how the iterative process influences the extracting results, we draw heatmaps of the extracting probabilities for each decoder at each iteration. We pick two representative cases in Fig.  3 , where the x-axis represents the sentence index and y-axis is the iteration number, x-axis labels are omitted. The darker the color is, the higher the extracting probability is. In Fig.  3 (a), it can be seen that when the iteration begins, most sentences have similar probabilities. As we increase the number of iteration, some probabilities begin to fall and others saturate. This means that the model already has preferred sentences to select. Another interesting feature we found is that there is a tran-Models 1st 2nd 3rd 4th Lead-3 0.12 0.11 0.25 0.52 Hybrid MemNet 0.24 0.25 0.28 0.23 ITS 0.31 0.34 0.23 0.12 Gold 0.33 0.30 0.24 0.13 sitivity between iterations as shown in Fig.  3 (b). To be specific, the sentences which are not preferred by iteration 3 remain low probabilities in the next two iterations, while sentences with relatively high scores are still preferred by iteration 4 and 5. Human Evaluation: We gave human evaluators three system-generated summaries, generated by Lead-3, Hybrid MemNet, ITS, as well as the human-written gold standard summary, and asked them to rank these summaries based on summary informativeness and coherence. Table  5  shows the percentages of summaries of different models under each rank scored by human experts. It is not surprising that gold standard has the most summaries of the highest quality. Our model has the most summaries under 2nd rank, thus can be considered 2nd best, following are Hybrid MemNet and Lead-3, as they are ranked mostly 3rd and 4th. By case study, we found that a number of summaries generated by Hybrid MemNet have two sentences the same as ITS out of three, however, the third distinct sentence from our model always leads to a better evaluation result considering overall informativeness and coherence. Readers can refer to the appendix to see our case study. 

 Conclusion In this work, we introduce ITS, an iteration based extractive summarization model, inspired by the observation that it is often necessary for a human to read the article multiple times to fully understand and summarize it. Experimental results on CNN/DailyMail and DUC corpora demonstrate the effectiveness of our model.  Figure 2 :Figure 3 : 23 Figure 2: Relationship between number of iteration and Rouge score on DailyMail test dataset with respect to the ground truth at 75 bytes. 
