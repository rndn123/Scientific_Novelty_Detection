title
Quantifying the Limits and Success of Extractive Summarization Systems Across Domains

abstract
This paper analyzes the topic identification stage of single-document automatic text summarization across four different domains, consisting of newswire, literary, scientific and legal documents. We present a study that explores the summary space of each domain via an exhaustive search strategy, and finds the probability density function (pdf) of the ROUGE score distributions for each domain. We then use this pdf to calculate the percentile rank of extractive summarization systems. Our results introduce a new way to judge the success of automatic summarization systems and bring quantified explanations to questions such as why it was so hard for the systems to date to have a statistically significant improvement over the lead baseline in the news domain.

Introduction Topic identification is the first stage of the generally accepted three-phase model in automatic text summarization, in which the goal is to identify the most important units in a document, i.e., phrases, sentences, or paragraphs  (Hovy and Lin, 1999; Lin, 1999; Sparck-Jones, 1999 ). This stage is followed by the topic interpretation and summary generation steps where the identified units are further processed to bring the summary into a coherent, human readable abstract form. The extractive summarization systems, however, only employ the topic identification stage, and simply output a ranked list of the units according to a compression ratio criterion. In general, for most systems sentences are the preferred units in this stage, as they are the smallest grammatical units that can express a statement. Since the sentences in a document are reproduced verbatim in extractive summaries, it is theoretically possible to explore the search space of this problem through an enumeration of all possible extracts for a document. Such an exploration would not only allow us to see how far we can go with extractive summarization, but we would also be able to judge the difficulty of the problem by looking at the distribution of the evaluation scores for the generated extracts. Moreover, the high scoring extracts could also be used to train a machine learning algorithm. However, such an enumeration strategy has an exponential complexity as it requires all possible sentence combinations of a document to be generated, constrained by a given word or sentence length. Thus the problem quickly becomes impractical as the number of sentences in a document increases and the compression ratio decreases. In this work, we try to overcome this bottleneck by using a large cluster of computers, and decomposing the task into smaller problems by using the given section boundaries or a linear text segmentation method. As a result of this exploration, we generate a probability density function (pdf) of the ROUGE score  (Lin, 2004)  distributions for four different domains, which shows the distribution of the evaluation scores for the generated extracts, and allows us to assess the difficulty of each domain for extractive summarization. Furthermore, using these pdfs, we introduce a new success measure for extractive summarization systems. Namely, given a system's average score over a data set, we show how to calculate the per-centile rank of this system from the corresponding pdf of the data set. This allows us to see the true improvement a system achieves over another, such as a baseline, and provides a standardized scoring scheme for systems performing on the same data set. 

 Related Work Despite the large amount of work in automatic text summarization, there are only a few studies in the literature that employ an exhaustive search strategy to create extracts, which is mainly due to the prohibitively large search space of the problem. Furthermore, the research regarding the alignment of abstracts to original documents has shown great variations across domains  (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1999; Jing, 2002; Ceylan and Mihalcea, 2009) , which indicates that the extractive summarization techniques are not applicable to all domains at the same level. In order to automate the process of corpus construction for automatic summarization systems,  (Marcu, 1999)  used exhaustive search to generate the best Extract from a given (Abstract, Text) tuple, where the best Extract contains a set of clauses from Text that have the highest similarity to the given Abstract. In addition,  (Donaway et al., 2000)  used exhaustive search to create all the sentence extracts of length three starting with 15 TREC Documents, in order to judge the performance of several summary evaluation measures suggested in their paper. Finally, the study most similar to ours was done by  (Lin and Hovy, 2003) , who used the articles with less than 30 sentences from the DUC 2001 data set to find oracle extracts of 100 and 150 (?5) words. These extracts were compared against one summary source, selected as the one that gave the highest inter-human agreement. Although it was concluded that a 10% improvement was possible for extractive summarization systems, which typically score around the lead baseline, there was no report on how difficult it would be to achieve this improvement, which is the main objective of our paper. domains we used 50 documents and only one summary for each document, except for newswire where we used two summaries per document. For the newswire domain, we selected the articles and their summaries from the DUC 2002 data set, 1 . For the literary domain, we obtained 10 novels that are literature classics, and available online in text format. Further, we collected the corresponding summaries for these novels from various websites such as CliffsNotes (www.cliffsnotes.com) and SparkNotes (www.sparknotes.com), which make available human generated abstracts for literary works. These sources give a summary for each chapter of the novel, so each chapter can be treated as a separate document. Thus we evaluate 50 chapters in total. For the scientific domain, we selected the articles from the medical journal Autoimmunity Reviews 2 were selected, and their abstracts are used as summaries. Finally, for the legal domain, we gathered 50 law documents and their corresponding summaries from the European Legislation Website, 3 which comprises four types of laws -Council Directives, Acts, Communications, and Decisions over several topics, such as society, environment, education, economics and employment. Although all the summaries are human generated abstracts for all the domains, it is worth mentioning that the documents and their corresponding summaries exhibit a specific writing style for each domain, in terms of the vocabulary used and the length of the sentences. We list some of the statistical properties of each domain in Table  1 . 

 Description of the Data Set 

 Experimental Setup As mentioned in Section 1, an exhaustive search algorithm requires generating all possible sentence combinations from a document, and evaluating each one individually. For example, using the values from Table  1 , and assuming 20 words per sentence, we find that the search space for the news domain contains approximately 32 5 ? 50 = 10, 068, 800 summaries. The same calculation method for the scientific domain gives us 99 8 ? 50 = 8.56 ? 10 12 summaries. Obviously the search space gets much bigger for the legal and literary domains due to their larger text size. In order to be able to cope with such a huge search space, the first thing we did was to modify the ROUGE 1.5.5 4 Perl script by fixing the parameters to those used in the DUC experiments, 5 and also by modifying the way it handles the input and output to make it suitable for streaming on the cluster. The resulting script evaluates around 25-30 summaries per second on an Intel 2.33 GHz processor. Next, we streamed the resulting ROUGE script for each (document, summary) pair on a large cluster of computers running on an Hadoop Map-Reduce framework.  6  Based on the size of the search space for a (document, summary) pair, the number of computers allocated in the cluster ranged from just a few to more than one thousand. Although the combination of a large cluster and a faster ROUGE is enough to handle most of the documents in the news domain in just a few hours, a simple calculation shows that the problem is still impractical for the other domains. Hence for the scientific, legal, and literary domains, rather than considering each document as a whole, we divide them into sections, and create extracts for each section such that the length of the extract is proportional to the length of the section in the original document. For the legal and scientific domains, we use the given section boundaries (without considering the subsections for scientific documents). For the novels, we treat each chapter as a single document (since each chapter has its own summary), which is further divided into sections using a publicly available linear text segmentation algorithm by  (Utiyama and Isahara, 2001) .  7  In all cases, we let the algorithm pick the number of segments automatically. To evaluate the sections, we modified ROUGE further so that it applies the length constraint to the extracts only, not to the model summaries. This is due to the fact that we evaluate the extracts of each section individually against the whole model summary, which is larger than the extract. This way, we can get an overall ROUGE recall score for a document extract, simply by summing up the recall scores of each section extracts. The precision score for the entire document can also be found by adding the weighted precision scores for each section, where the weight is proportional to the length of the section in the original document. In our study, however, we only use recall scores. Note that, since for the legal, scientific, and literary domains we consider each section of a document independently, we are not performing a true exhaustive search for these domains, but rather solving a suboptimal problem, as we divide the number of words in the model summary to each section proportional to the section's length. However, we believe that this is a fair assumption, as it has been shown repeatedly in the past that text segmentation helps improving the performance of text summarization systems (yen  Kan et al., 1998; Nakao, 2000; Mihalcea and Ceylan, 2007) . 

 Exhaustive Search Algorithm Let E i k = S i 1 , S i 2 , ..., S i k be the i th extract that has k sentences, and generated from a document D with n sentences D = S 1 , S 2 , . . . , S n . Further, let len(S j ) give the number of words in sentence S j . We enforce that E i k satisfies the following constraints: len(E i k ) = len(S i 1 ) + . . . + len(S i k ) ? L len(E i k?1 ) = len(S i 1 ) + . . . + len(S i k?1 ) < L where L is the length constraint on all the extracts of document D. We note that for any E i k , the order of the sentences in E i k?1 does not affect the ROUGE scores, since only the last sentence may be chopped off due to the length constraint. 8 Hence, we start generating sentence combinations n r in lexicographic order, for r = 1...n, and for each combination E i k = S i 1 , S i 2 , ..., S i k where k > 1, we generate additional extracts E ? i k by successfully swapping S i j with S i k for j = 1, ..., k ? 1 and checking to see if the above constraints are still satisfied. Therefore from a combination with k sentences that satisfies the constraints, we might generate up to k ? 1 additional extracts. Finally, we stop the process either when r = n and the last combination is generated, or we cannot find any extract that satisfies the constraints for r. 

 Generating pdfs Once the extracts for a document are generated and evaluated, we go through each result and assign its recall score to a range, which we refer to as a bin. We use 1, 000 equally spaced bins between 0 and 1. As an example, a recall score of 0.46873 would be assigned to the bin [0.468, 0.469]. By keeping a count for each bin, we are in fact building a histogram of scores for the document. Let this histogram be h, and h[j] be the value in the j th bin of the histogram. We then define the normalized histogram h as: h[j] = N N i=1 h[j] h[j] (1) where N = 1, 000 is the number of bins in the histogram. Note that since the width of each bin is 1 N , the Riemann sum of the normalized histogram h is equal to 1, so h can be used as an approximation to the underlying pdf. As an example, we show the histogram h for the newswire document AP890323-0218 in Figure  1 . We combine the normalized histograms of all the documents in a domain in order to find the pdf for that domain. This requires multiplying the value of each bin in a document's histogram, with all the other possible combinations of bin values taken from each of the remaining histograms, and assigning the result to the average bin for each combina- tion. This can be done iteratively by keeping a moving average. We illustrate this procedure in Algorithm 1, where K represents the number of documents in a domain. Algorithm 1 Combine h i 's for i = 1, . . . , K to create h d , the histogram for domain d.   1 , is an approximation to the pdf for domain d. Furthermore, we used the round() function in line 9, which rounds a number to the nearest integer, as the bins are indexed by integers. Note that this rounding introduces an error, which is distributed uniformly due to the nature of the round() function. It is also possible to lower the affect of this error with higher resolutions (i.e. larger number of bins). In Figure  2 , we show a sample h d , obtained by combining 10 documents from the newswire do- main. 1: h d := {} 2: for i = 1 to N do 3: h d [i] := h 1 [i] 4: end for 5: for i = 2 to K do 6: h t = {} 7: for j = 1 to N do 8: for k = 1 to N do 9: a = round(((k * (i ? 1)) + j)/i) 10: h t [a] = h t [a] + (h d [k] * h i [j]) Recall from Section 4 that the documents in the literary, legal, and scientific domains are divided into sections either by using the given section boundaries or by applying a text segmentation algorithm, and the extracts of each section are then evaluated individually. Hence for these domains, we first calculate the histogram of each section individually, and then combine them to find the histogram of a document. The combination procedure for the section histograms is similar to Algorithm 1, except that in this case we do not keep a moving average, but rather sum up the bins of the sections. Note that when bin i and j are added, the resulting values should be expected to be half the times in bin i + j, and half the times in i + j ? 1. 

 Calculating Percentile Ranks Given a pdf for a domain, the success of a system having a ROUGE recall score of S could be simply measured by finding the area bounded by S. This gives us the percentile rank of the system in the overall distribution. Assuming 0 ? S ? 1, let S = ?N ? S?, then the formula to calculate the percentile rank can be simply given as:  P R(S) = 100 N b S i=1 h d [i] (2) 

 Results The ensemble distributions of ROUGE-1 recall scores per document are shown in Figure  3 . The ensemble distributions tell us that the performance of the extracts, especially for the news and the scientific domains, are mostly uniform for each document. This is due to the fact that documents in these domains, and their corresponding summaries, are written with a certain conventional style. There is however a little scattering in the distributions of the literary and the legal domains. This is an expected result for the literary domain, as there is no specific summarization style for these documents, but somehow surprising for the legal domain, where the effect is probably due to the different types of legal documents in the data set. The pdf plots resulting from the ROUGE-1 recall scores are shown in Figure  4 .  9  In order to analyze the pdf plots, and better understand their differences, Table  2  lists the mean (?) and the standard deviation (?) measures of the pdfs, as well as the average minimum and maximum scores that an extractive summarization system can get for each domain. By looking at the pdf plots and the minimum and maximum columns from Table  2 , we notice that for all the domains, the pdfs are long-tailed distributions. This immediately implies that most of the extracts in a summary space are clustered around the mean, which means that for automatic summarization systems, it is very easy to get scores around this range. Furthermore, we can judge the hardness of each domain by looking at the standard deviation values. A lower standard deviation indicates a steeper curve, which implies that improving a system would be harder. From the table, we can infer that the legal domain is the hardest while the newswire is the easiest. Comparing Table  2  with the values in Table  1 , we also notice that the compression ratio affects the performance differently for each domain. For example, although the scientific domain has the highest compression ratio, it has a higher mean than the literary and the newswire domains for ROUGE-1 and ROUGE-SU4 recall scores. This implies that although the abstracts of the medical journals are highly compressed, they have a high overlap with the document, probably caused by their writing style. This was in fact confirmed earlier by the experiments in  (Kupiec et al., 1995) , where it was found out that for a data set of 188 scientific articles, 79% of the sentences in the abstracts could be perfectly matched with the sentences in the corresponding documents. Next, we confirm our experiments by testing three different extractive summarization systems on our data set. The first system that we implement is called Random, and gives a random score between 1 and 100 to each sentence in a document, and then selects the top scoring sentences. The second system, Lead, implements the lead baseline method which takes the first k sentences of a document until the length limit is reached. Finally, the last system that we implement is TextRank, which uses a variation of the PageRank graph centrality algorithm in order to identify the most important sentences in a document  (Page et al., 1999; Erkan and Radev, 2004; Mihalcea and Tarau, 2004) . We selected TextRank as it has a performance competitive with the top systems participating in DUC '02  (Mihalcea and Tarau, 2004) . We would also like to mention that for the literary, scientific, and legal domains, the systems apply the algorithms for each section and each section is evaluated independently, and their resulting recall scores are summed up. This is needed in order to be consistent with our exhaustive search experiments. The ROUGE recall scores of the three systems are shown in Table  3 . As expected, for the literary and legal domains, the Random, and the Lead systems score around the mean. This is due to the fact that the leading sentences for these two domains do not indicate any significance, hence the Lead system just behaves like Random. However for the scientific and newswire domains, the leading sentences do have importance so the Lead system consistently outperforms Random. Furthermore, although TextRank is the best system for the literary, scientific, and legal domains, it gets outperformed by the Lead system on the newswire domain. This is also an expected result as none of the single-document summarization systems were able to achieve a statistically significant improvement over the lead baseline in the previous Document Understanding Conferences (DUC). The ROUGE scoring scheme does not tell us how much improvement a system achieved over another, or how far it is from the upper bound. Since we now have access to the pdf of each domain in our data set, see the difference more clearly between the two systems. Finally, when comparing two successful systems using percentile ranks, we believe the use of error reduction would be more beneficial. As a final note, we also randomly sampled extracts from documents in the scientific and legal domains, but this time without considering the section boundaries and without performing any segmentation. We kept the number of samples for each document equal to the number of extracts we generated from the same document using a divide-and-conquer approach. We evaluated the samples using ROUGE-1 recall scores, and obtained pdfs for each domain using the same strategy discussed earlier in the paper. The resulting pdfs, although they exhibit similar characteristics, they have mean values (?) around 10% lower than the ones we listed in Table  2 , which supports the findings from earlier research that segmentation is useful for text summarization. 

 Conclusions and Future Work In this paper, we described a study that explores the search space of extractive summaries across four different domains. For the news domain we generated all possible extracts of the given documents, and for the literary, scientific, and legal domains we followed a divide-and-conquer approach by chunking the documents into sections, handled each section independently, and combined the resulting scores at the end. We then used the distributions of the evaluations scores to generate the probability density functions (pdfs) for each domain. Various statistical properties of these pdfs helped us asses the difficulty of each domain. Finally, we introduced a new scoring scheme for automatic text summarization systems that can be derived from the pdfs. The new scheme calculates a percentile rank of the ROUGE-1 recall score of a system, which gives scores in the range [0-100]. This lets us see how far each system is from the upper bound, and thus make a better comparison among the systems. The new scoring system showed us that while there is a 20.1% gap between the upper bound and the lead baseline for the news domain, closing this gap is difficult, as the percentile rank of the lead baseline system, 99.99%, indicates that the system is already very close to the upper bound. Furthermore, except for the literary domain, the percentile rank of the TextRank system is also very close to the upperbound. This result does not suggest that additional improvements cannot be made in these domains, but that making further improvements using only extractive summarization will be considerably difficult. Moreover, in order to see these future improvements, a higher resolution (i.e. larger number of bins) will be needed when constructing the pdfs. In all our experiments we used the ROUGE  (Lin, 2004)  evaluation package and its ROUGE-1, ROUGE-2, and ROUGE-SU4 recall scores. We would like to note that since ROUGE performs its evaluations based on the n-gram overlap between the peer and the model summary, it does not take other summary quality metrics such as coherence and cohesion into account. However, our goal in this paper was to analyze the topic-identification stage only, which concentrates on selecting the right content from the document to include in the summary, and the ROUGE scores were found to correlate well with the human judgments on assessing the content overlap of summaries. In the future, we would like to apply a similar exhaustive search strategy, but this time with different compression ratios, in order to see the impact of compression ratios on the pdf of each domain. Furthermore, we would also like to analyze the high scoring extracts found by the exhaustive search, in terms of coherence, position and other features. Such an analysis would allow us to see whether these extracts exhibit certain properties which could be used in training machine learning systems. Figure 1 : 1 Figure 1: The normalized histogram h of ROUGE-1 recall scores for the newswire document AP890323-0218. 

 The resulting histogram h d , when normalized using Equation 

 Figure 2 : 2 Figure 2: An example pdf obtained by combining 10 document histograms of ROUGE-1 recall scores from the newswire domain. The x-axis is normalized to [0,1]. 

 Figure 3 :Figure 4 : 34 Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains, respectively from left to right. 

 Table 2 : 2 Statistical properties of the pdfs ROUGE-1 Domain ? ? max min Newswire 39.39 0.87 65.70 20.20 Literary 45.20 0.47 63.90 28.40 Scientific 45.99 0.68 71.90 24.20 Legal 72.82 0.28 82.40 62.80 ROUGE-2 Domain ? ? max min Newswire 11.57 0.79 37.40 1.60 Literary 5.41 0.34 16.90 1.80 Scientific 10.98 0.60 33.30 1.30 Legal 28.74 0.29 40.90 19.60 ROUGE-SU4 Domain ? ? max min Newswire 15.33 0.69 38.10 6.40 Literary 13.28 0.30 24.30 6.90 Scientific 16.13 0.50 35.80 6.20 Legal 35.63 0.25 45.70 28.70 

 Table 3 : 3 ROUGE recall scores of the Lead baseline, Tex-tRank, and Random sentence selector across domains ROUGE-1 Domain Random Lead TextRank Newswire 39.13 45.63 44.43 Literary 45.39 45.36 46.12 Scientific 45.75 47.18 49.26 Legal 73.04 72.42 74.82 ROUGE-2 Domain Random Lead TextRank Newswire 11.39 19.60 17.99 Literary 5.33 5.41 5.92 Scientific 10.73 12.07 12.76 Legal 28.56 28.92 31.06 ROUGE-SU4 Domain Random Lead TextRank Newswire 15.07 21.58 20.46 Literary 13.21 13.28 13.81 Scientific 15.92 17.12 17.85 Legal 35.41 35.55 37.64 

			 http://www-nlpir.nist.gov/projects/duc/data.html 2 http://www.elsevier.com/wps/product/cws home/622356 3 http://eur-lex.europa.eu/en/legis/index.htm 

			 http://berouge.com 5 -n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 6 http://hadoop.apache.org/ 

			 http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-1.211.tar.gz 

			 that we do not take the coherence of extracts into account, i.e. the sentences in an extract do not need to be sorted in order of their appearance in the original document. We also do not change the position of the words in a sentence. 

			 Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4, even if at a different scale. 

			 also accounts for the fact that even though we might have two very close ROUGE scores that are not statistically significant, their percentile rankings might differ quite a bit.
