title
Day 160: NLP Papers Summary -Extractive Summarization As Text Matching Objective and Contribution

abstract
We tackle extractive summarisation task as a semantic text matching problem rather than the common used sequence labelling problem. We proposed MATCHSUM, a novel summary-level framework that uses Siamese-BERT to match source document and candidate summaries in the semantic space. The idea is that a good summary should be semantically similar to the source document as a whole. This method achieved 44.41 ROUGE-1 score in CNN/DM dataset ? Natural Language Processing 365 ? ? ? ? ? ? ? 21/02/2022, 21:38 Day 160: NLP Papers Summary -Extractive Summarization as Text Matching -Ryan Ong https://ryanong.co.uk/2020/06/08/day-160-nlp-papers-summary-extractive-summarization-as-text-matching/ 2/11

and achieved similar results in other evaluation datasets. We also analyse the performance difference between sentence-level and summary-level extractive models. 

 Datasets We have six evaluation datasets as shown below and our evaluation metrics are ROUGE-1, ROUGE-2, and ROUGE-L. 

 MATCHSUM SIAMESE-BERT We use siamese-BERT, which consists of two BERTs with the same weight and a cosine similarity layer, to match document and candidate summary. We use BERT to encode both the document and candidate summaries and compute the similarity between the two embeddings. The basic idea is that gold summary has the highest matching score to the source document and good candidate summary should obtain high score. 

 CANDIDATES PRUNING To avoid having to score all possible candidates, we use a simple candidate pruning strategy. Sentence-level vs Summary-level Extractive Summarisation  

 Speci cally, we used a content 

 Results The results for CNN/DM are displayed below. As shown, our MATCHSUM outperformed all the other baseline models. We observed the best performance is achieved when when we change the encoder to RoBERTa-base. We believe this is due to RoBERTa was pre-trained using 63 million news articles.   

 Conclusion and Future Work In the future, we could work on different forms of matching models to further explore the performance of the proposed framework. In addition, a better understanding of the characteristics of the datasets we are dealing with would give us an advantage over which types of models to use.  
