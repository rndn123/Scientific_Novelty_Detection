title
Data Science Natural Language Processing NLP Papers Summary

abstract
The role of evaluation metrics is extremely important as they heavily guide the research progress of a particular eld. The goal of automatic evaluation metrics is to accurately evaluate generated summaries that's close to human judgements. The paper shows that there is a strong disagreement between evaluation metrics that behave similarly and the ones in higherscoring range. This disagreement means we don't know which metrics to trust when evaluating our generated summaries. The contributions of this paper is as follows: ? Natural Language Processing 365 ? ? ? ? ? ? ?

1. Introduce a methodology to study the evaluation metrics in high-scoring range and found that there are low / negative correlations between metrics. This work hopes to encourage researchers to collect more human annotations in the appropriate scoring range There aren't many manually annotated datasets and existing ones are created during shared tasks in 2008 and so annotated summaries are average compared to current level. This is illustrated in the gure below. As you can see, the score distribution of the ground-truth summaries (blue) differ from the score distribution of generated summaries by modern summarisation systems (red). There is no guarantee that evaluation metrics behave similar to human evaluation in the red distribution (high-scoring range) and the objective of this paper is to evaluate evaluation metrics in the high-scoring range, to assess if their ability to evaluate are consistent and correlates with human evaluation. The paper computes correlation between pairs of metrics in different scoring ranges that doesn't have human evaluation. 

 Experimental Setup -Data Generation The paper studies the following metrics: 1. ROUGE-2 (R-2). Bigram overlap between generated summaries and ground-truth  

 MEASURING CONSISTENT IMPROVEMENTS ACROSS METRICS Given a set of evaluation metrics, to measure consistent improvements across metrics (metrics agreeing with each other), we compute the following F/N ratio: 1. Select a summary s 2. Among the summaries, which are better than s for one metric (N) 3. Among the summaries, which are better than s for all metrics (F) The gure below shows this process repeated for 5000 random sample summaries. The results show that the proportion of consistent improvement (F/N ratio) decrease rapidly as the average score of summaries increases. By using multiple evaluation metrics that disagree with each other, it is very dif cult to identify high-scoring summaries with high con dence. 

 F divide by N to obtain the ratio 

 Conclusion and Future Work The disagreement between evaluation metrics in the high-scoring summaries means that it  NLP Papers Summary -Studying Summarization Evaluation Metrics in the Appropriate Scoring Ra? https://ryanong.co.uk/2020/04/18/day-109-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-appropriat? 3/10 2. ROUGE-L (R-L). Size of longest common subsequence between generated summaries and ground-truth 3. ROUGE-WE (R-WE). Soft matching based on cosine similarity and word embeddings 4. JS divergence (JS-2). Uses Jensen-Shannon divergence to measure difference between bigram distributions 5. S3. a metric that maximise its correlation with manual Pyramid annotations The authors use genetic algorithm for summarisation to generate summaries that optimise each metrics. The generated dataset (denoted W) consists of 160,523 summaries, with around 1763 summaries per metric. In order to focus on high-scoring summaries, we use LexRank to lter out summaries that underperform the benchmark. This led to the nal dataset (T) of around 102 summaries per topic after removing duplicates and ltering. Human judgment summaries are denoted as A. Correlation Analysis SIMPSON PARADOX From the gure above, we can see that for dataset A and W, there's a high correlation between evaluation metrics, with R-2 and JS-2 having the strongest correlation. This can be explain by the fact that they are both based on bigrams. R-L has the least correlation with the other ? ? 21/02/2022, 21:40 Day 109: NLP Papers Summary -Studying Summarization Evaluation Metrics in the Appropriate Scoring Ra? https://ryanong.co.uk/2020/04/18/day-109-nlp-papers-summary-studying-summarization-evaluation-metrics-in-the-appropriat? 4/10 metrics. However, in the high-scoring summaries (T), correlations between metrics are low and some are even negative. There's no global agreement between metrics to measure improvements when we examine summaries that are better than LexRank. In fact, the results show that this disagreement increases with higher-scoring summaries as shown in the gure below. This is known as the Simpson paradox whereby different conclusions are reached depending on which sub-populations you drawn from. The results tell us that our current evaluation metrics are good at distinguishing very bad summaries from very good summaries but aren't able to distinguish between high-scoring summaries. 
