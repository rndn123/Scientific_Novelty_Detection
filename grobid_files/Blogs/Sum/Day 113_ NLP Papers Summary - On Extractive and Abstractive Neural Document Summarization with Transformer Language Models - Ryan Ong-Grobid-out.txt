title
Day 113: NLP Papers Summary -On Extractive And Abstractive Neural Document Summarization With Transformer Language Models Objective and Contribution

abstract
Proposed an abstractive summarisation method on long documents. This is achieved through a two-step process of extractive and abstractive summarisation. The output of the extractive step is used to train the abstractive transformer language model. This extractive step has been shown to be very important towards the end summarisation results. In addition, the generated abstractive summaries are more abstractive than previous work that employed the copy mechanism and also yielded higher ROUGE scores. The contributions are: ? Natural Language Processing 365 ? ? ? ? ? ? ?

ABSTRACTIVE SUMMARISATION We trained a single transformer language model from scratch using "formatted" data. The  

 Abstract (ground-truth summary) 

 Rest of the paper. Serve to train language model to understand domain language For some datasets, the introduction section would be the entire document as there are no rest of the paper section.  

 Results and Analysis Table  2 and 4    Extractive summarisation. A hierarchical document model that either copy or classify sentences in the document to build extractive summary ? ? 21/02/2022, 21:40 Day 113: NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 3/13 2. Abstractive summarisation. The extractive summary as well as the document is used to condition the transformer language model EXTRACTIVE SUMMARISATION The extractive step involves sentence extraction using two different hierarchical document models: hierarchical seq2seq sentence pointer and sentence classi er. The goal is to lter out noisy sentences and extract important sentences to better train our transformer language model. The hierarchical seq2seq sentence pointer has an encoder-decoder architecture: 1. The encoder is a bidirectional LSTM at both the word and sentence level (hierarchical) 2. The decoder is an autoregressive LSTM The hierarchical encoder combines both the word and sentence-level directional LSTM. The token-level biLSTM encodes each sentence in the document to obtain the sentence embeddings. The sentence-level biLSTM encodes these sentence embeddings to obtain document representations. The decoder is an autoregressive LSTM that takes in the hidden state of the previously extracted sentence as input and predict the next sentence to be extract. Similar to the pointer network, the sentence classi er uses a hierarchical LSTM to encode the document and produce a sequence of sentence embeddings. The nal document representation is the average of these sentence embeddings. The nal document representation is concatenated to each sentence embedding and feed into a neural network with a sigmoid function to obtain the probability of each sentence to be included in the extractive summary. 

 transformer language model is GPT-2. Language models are trained by factorising joint distribution of words autoregressively. This inspires us to organise the training data in certain format where we put the ground-truth summary after the information the model would normally use to generate summaries. In this way, we model the joint distribution of document and summary during training and use the conditional distribution (given the document) to generate summary at inference. Therefore, the training data is formatted in 4 different sections: ? ? 21/02/2022, 21:40 Day 113: NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 4/13 1. Paper Introduction. Assumption that introduction should contain enough to generate the abstract 2. Extracted summary (from extractive summarisation) 

 Figure NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 5/13 

 showcase that our extractive models outperformed all previous extractive baselines on both arXiv and PubMed datasets. On the Newsroom dataset (table 6), our TLM outperformed the other abstractive model, Seq2Seq, by a massive margin and also outperformed the pointer-generator network. However, the Exconsumm model dominates the extractive and mixed results. NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 6/13 ? ? 21/02/2022, 21:40 Day 113: NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 7/13 The best performing TLM (TLM-I+E (G,M)) has outperformed previous abstractive results on most ROUGE scores metrics except on ROUGE-L. We believe this might be due to the fact that we don't have a copy mechanism in place, making it very challenging to get exact matches on large n-grams. The gure below supports this hypothesis as the copy mechanism of the discourse-aware model can copy up to 25-grams from the source document. In addition, the gure below also showcase that our TLM has generated more abstractive summaries than ? 21/02/2022, 21:40 Day 113: NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 8/13 previous work by the low percentage of n-grams overlap between generated summaries and source documents. We also measure the upper bound performance of our TLM (TLM-I+E (G,G)) by including the ground-truth extracted sentences in both training and testing. Lastly, the gure below showcase a qualitative results of summaries generated by our TLM. NLP Papers Summary -On Extractive and Abstractive Neural Document Summarization with Tran? https://ryanong.co.uk/2020/04/22/day-113-nlp-papers-summary-on-extractive-and-abstractive-neural-document-summarizatio? 9/13 Conclusion and Future Work The uency and coherency of the generated summaries are of strong level. However, there remains the problem of abstractive summaries generating imaginary / inaccurate content. Potential future work could focus more on factual correctness and coherency when evaluating summarisation models. Source: https://arxiv.org/pdf/1909.03186.pdf ? ?
