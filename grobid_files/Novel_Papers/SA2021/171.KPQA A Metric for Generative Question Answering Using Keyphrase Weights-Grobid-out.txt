title
KPQA: A Metric for Generative Question Answering Using Keyphrase Weights

abstract
In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQAmetric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics. Code for KPQA-metric will be available at https://github.com/ hwanheelee1993/KPQA.

Introduction Question answering (QA) has received consistent attention from the natural language processing community. Recently, research on QA systems has reached the stage of generating free-form answers, called GenQA, beyond extracting the answer to a given question from the context  (Yin et al., 2016; Song et al., 2017; Bauer et al., 2018; Nishida et al., 2019; Bi et al., 2019 Bi et al., , 2020 . However, as a bottleneck in developing GenQA models, there are no proper automatic metrics to evaluate generated answers . In evaluating a GenQA model, it is essential to consider whether a generated response correctly contains vital information to answer the question. There exist several n-gram similarity metrics such * This research was done while the author was affiliated with Adobe Research. Context : ... , this process, called hypothesis testing, consists of four steps. , ... Question : How many steps are involved in a hypothesis test? Reference Answer : Four steps are involved in a hypothesis test. Generated Answer : There are seven steps involved in a hypothesis test . Human Judgment : 0.063 BLEU-1 : 0.778 BLEU-1-KPQA : 0.057 ROUGE-L : 0.713 ROUGE-L-KPQA : 0.127 Figure  1 : An example from MS-MARCO  (Bajaj et al., 2016)  where widely used n-gram similarity metrics does not align with human judgments of correctness. On the other hand, our KPQA-metrics focus on the key information and give low scores to incorrect answers similar to humans. as BLEU  (Papineni et al., 2002)  and ROUGE-L  (Lin, 2004) , that measure the word overlaps between the generated response and the reference answer; however, these metrics are insufficient to evaluate a GenQA system  (Yang et al., 2018a; . For instance, in the example in Figure  1  from the MS-MARCO  (Bajaj et al., 2016) , the generated answer receives a high score on BLEU-1 (0.778) and ROUGE-L (0.713) due to the many overlaps of words with those in the reference. However, humans assign a low score of 0.063 on the scale from 0 to 1 due to the mismatch of critical information. As in this example, we find that existing metrics often fail to capture the correctness of the generated answer that considers the key information for the question. To overcome this shortcoming of the existing metrics, we propose a new metric called KPQAmetric for evaluating GenQA systems. To derive the metric, we first develop Keyphrase Predictor for Question Answering (KPQA). KPQA computes the importance weight of each word in both the generated answer and the reference answer by considering the question. By integrating the output from the KPQA, we compute the KPQA-metric in two steps: (1) Given a {question, generated answer, reference answer}, we compute importance weights for each question-answer pair {question, generated answer} and {question, reference answer} using a KPQA; (2) We then compute a weighted similarity score by integrating the importance weights into existing metrics. Our approach can be easily integrated into most existing metrics, including ngram similarity metrics and the recently proposed BERTScore  (Zhang et al., 2020) . Additionally, we newly create two datasets for assessing automatic evaluation metrics with regard to the correctness in the GenQA domain. We first generate answers using state-of-the-art GenQA models on MS-MARCO and AVSD  (Alamri et al., 2019)  where the target answers are natural sentences rather than short phrases. We then collect human judgements of correctness over the 1k generated answers for each dataset. In experiments on the human-evaluation datasets, we show that our KPQA-metrics have significantly higher correlations with human judgments than the previous metrics. For example, BERTScore-KPQA, one of our KPQA-integrated metrics, obtains Pearson correlation coefficients of 0.673 on MS-MARCO whereas the original BERTScore obtains 0.463. Further analyses demonstrate that our KPQA-metrics are robust to the question type and domain shift. Overall, our main contributions can be summarized as follows: ? We propose KPQA metric, an importance weighting based evaluation metric for GenQA. ? We collect high-quality human judgments of correctness for the model generated answers on MS-MARCO and AVSD, where those two GenQA datasets aim to generate sentence-level answers. We show that our proposed metric has a dramatically higher correlation with human judgments than the previous metrics for these datasets. ? We verify the robustness of our metric in various aspects such as question type and domain effect. ? We release the human-annotated benchmark dataset and pre-trained models to compute the KPQA-metric to the research community 1 . BERTScore is a recently proposed text evaluation metric that use pre-trained representations from BERT  (Devlin et al., 2019) . BERTScore first computes the contextual embeddings for given references and candidates independently with BERT, and then computes pairwise cosine similarity scores. When computing similarity, BERTScore adopts Inverse Document Frequency (IDF) to apply importance weighting. 

 Proposed Metric for Evaluating GenQA To build a better metric for GenQA, we first propose KPQA. By considering the question, the KPQA assigns different weights to each token in the answer sentence such that salient tokens receive a high value. We then integrate the KPQA into existing metrics to make them evaluate correctness as well. 

 KPQA For GenQA, we observe that each word has different levels of importance when assessing a gen-   

 Label Figure  3 : Overall architecture and an output example of KPQA. KPQA classifies whether each word in the answer sentences is in the answer span for a given question. We use the output probability KPW as an importance weight to be integrated into KPQA-metric. erated answer. As shown in Figure  1 , there exist keywords or keyphrases that are considered significant when evaluating the correctness of the answer. Additionally, some words, such as function words are mostly irrelevant to the correctness of the answer. Inspired by this observation, we introduce KPQA, which can predict the importance of each word when evaluating GenQA systems. As shown in Figure  3 , KPQA is a BERT-based  (Devlin et al., 2019)  classifier that predicts salient tokens in the answer sentences depending on the question. We regard it as a multi-class classification task where each token is a single class. To train KPQA, we first prepare extractive QA datasets such as SQuAD  (Rajpurkar et al., 2016) , which consist of {passage, question, answer-span}. We transform these datasets into pairs of {answer-sentences, question, answer-span}. We extract the answersentences that contain answer-span in the passage since these sentences are short summaries for the given question. Specifically, for a single-hop QA dataset such as SQuAD, we pick a single sentence that includes answer-span as the answer sentence. For the answers in a multi-hop QA dataset such as HotpotQA  (Yang et al., 2018b) , there are multiple supporting sentences for the single answer span. For these cases, we use SpanBERT  (Joshi et al., 2020)  to resolve the coreferences in the paragraphs and extract all of the supporting sentences to compose answer sentences. The {question, [SEP], answer-sentences} is then fed into the KPQA to classify the answer-span, which is a set of salient tokens, in the given answer-sentences considering the question. 

 KPQA Metric Since KPQA's training process allows KPQA to find essential words in the answer sentences to a given question, we use a pre-trained KPQA to get the importance weights that are useful for evaluating the correctness of generated answers in GenQA. The overall flow of our KPQA-metric is described in Figure  2 . We describe how we combine these weights with existing metrics to derive the KPQAmetric. We first compute the importance weights for a given question Q = (q 1 , ..., q l ), reference answer X = (x 1 , ..., x n ) and generated answer X = (x 1 , ..., xm ) using pre-trained KPQA. We provide each pair {question, generated answer} and {question, reference answer} to pre-trained KPQA and get the output of the softmax layer. We define these parts as KeyPhrase Weight (KPW) as shown in Figure  3 . We note that KPW (Q, X) = (w 1 , ..., w m ) is an importance weight of generated answer X for a given question Q. These weights reflect the importance of each token for evaluating the correctness. We then compute KPQA-metric by incorporat-ing the KPW into several existing metrics modifying the precision and recall to compute the weighted similarity. BLEU-1-KPQA: We derive BLEU-1-KPQA, which is an weighted precision of unigram (P KP QA U nigram ) as follows: P KP QA U nigram = ? m i=1 ? n j=1 KPW (Q, X) i ? I(i, j) ? m i=1 KPW (Q, X) i , (1) where I(i, j) is an indicator function assigned the value of 1 if token x i is the same as xj and 0 otherwise. 

 ROUGE-L-KPQA: We also derive ROUGE-L-KPQA, which is a modified version of ROUGE-L using KPW to compute weighted precision(P KP QA LCS ), recall(R KP QA LCS ) and F1(F 1 KP QA LCS ), as follows: P KP QA LCS = LCS KP QA (X, X) ? m i=1 KPW (Q, X) i , (2) R KP QA LCS = LCS KP QA (X, X) ? n i=1 KPW (Q,X) i , (3) F KP QA LCS = (1 + ? 2 )R KP QA LCS P KP QA LCS R KP QA LCS + ? 2 P KP QA LCS , ( 4 ) where LCS is the Longest Common Subsequence between a generated answer and a reference answer. The LCS KP QA (X, X) is defined as follows: LCS KP QA (X, X) = ? m i=1 I i ? KPW (Q, X) i , (5) where I i is an indicator function which is 1 if each word is in the LCS and 0 otherwise. ? is defined in  (Lin, 2004) . BERTScore-KPQA Similar to ROUGE-L-KPQA, we compute BERTScore-KPQA using KPW. We first compute contextual embedding x for generated answer X and x for reference X using the BERT model. Then, we compute weighted precision(P KP QA BERT ), recall(R KP QA BERT ) and F1(F 1 KP QA BERT ) with contextual embedding and KPW of each token as follows: P KP QA BERT = ? m i=1 KPW (Q, X) i ? max x j ?x x i T xj ? m i=1 KPW (Q, X) i (6) R KP QA BERT = ? n i=1 KPW (Q,X) i ? max xj ?x x i T xj ? n i=1 KPW (Q,X) i (7) F 1 KP QA BERT = 2 ? P KP QA BERT ? R KP QA BERT P KP QA BERT + R KP QA BERT ( 8 ) P KP QA LCS = LCS KP QA (X, X) ? m i=1 KPW (Q, X) i , (9) R KP QA LCS = LCS KP QA (X, X) ? n i=1 KPW (Q,X) i , (10) F KP QA LCS = (1 + ? 2 )R KP QA LCS P KP QA LCS R KP QA LCS + ? 2 P KP QA LCS , (11) where LCS is the Longest Common Subsequence between a generated answer and a reference answer. The LCS KP QA (X, X) is defined as follows: LCS KP QA (X, X) = ? m i=1 I i ? KPW (Q, X) i , (12) where I i is an indicator function which is 1 if each word is in the LCS and 0 otherwise. ? is defined in  (Lin, 2004) . Similar to ROUGE-L-KPQA, we also derive BLEU-1-KPQA and BERTScore-KPQA by intergating KPW and provide the formulas in Appendix. 4 Collecting Human Judgments GenQA Datasets: To evaluate GenQA metrics, it is necessary to measure the correlation between human judgments and automated text evaluation metrics for evaluating the model generated answers. Recently, Chen et al. (  2019 ) released human judgments of correctness for two GenQA datasets, Nar-rativeQA  (Ko?isk? et al., 2018)  and SemEval-2018 Task 11 (SemEval)  (Ostermann et al., 2018) . However, we find that the average lengths of the answer sentence are 4.7 and 2.5 for NarrativeQA and Se-mEval, respectively, as shown in Table  1 . These short answers are often short phrases and cannot be representative of GenQA, because the answers could be long and may deliver complex meaning. We argue that evaluating long and abstractive answers is more challenging and suitable for studying the metrics for general form of GenQA. To fill this gap, we collect the human judgments of correctness for model generated answers on two other GenQA datasets, MS-MARCO and AVSD, which have longer answers than NarrativeQA and Se-mEval as shown in Table  1 . For the MS-MARCO, we use the Natural Language Generation (NLG) subset, which has more abstractive and longer answers than the Q&A subset. GenQA Models: For each of the two datasets, we first generate answers for questions on validation sets using two trained GenQA models: UniLM  (Dong et al., 2019)  and MHPGM  (Bauer et al., 2018)  for MS-MARCO, MTN  (Le et al., 2019)  and AMF  (Alamri et al., 2018; Hori et al., 2017)  for AVSD. Details on these QA models are in Appendix. After training, we select 1k samples for each dataset in the validation set. Specifically, we first randomly pick the 500 questions in the validation set of each dataset and collect the corresponding model generated answers for each model so that we have two generated answers for each sample. Therefore, we collect a total of 1k samples, two different answers for 500 questions for each dataset. Also, we discard samples if one of two GenQA models exactly generates the ground-truth answer since human evaluation is useless during the sampling. 

 Collecting Human Judgments of Answer Correctness We hire workers from the Amazon Mechanical Turk (MTurk) to rate the correctness of the generated answers from the models we trained. We assign ten workers for each sample to get reliable data. We ask the workers to annotate correctness using a 5-point Likert scale  (Likert, 1932) , where 1 means completely wrong, and 5 means completely correct. We provide the full instruction in Appendix. Filtering Noisy Workers: Some workers did not follow the instructions, producing poor-quality judgments. To solve this problem, we filter noisy ratings using the z-score, as in (Jung and  Lease, 2011) . We first compute the z-score among the ten responses for each sample. Then, we consider the responses whose z-score is higher than 1 to be noise and remove up to five of them in the order of the z-score. The average number of annotators after filtering is shown in Table  2 . We use the average score of the annotators for each sample as a groundtruth evaluation score to assess the quality of the evaluation metric. Inter-Annotator Agreement: The final dataset is further validated with Krippendorff's alpha  (Krippendorff, 1970 (Krippendorff, , 2011 , a statistical measure of inter-rater agreement for multiple annotators. We observe that Krippendorff's ? is higher than 0.6 for both datasets and models after filtering, as shown in Table  2 . These coefficient numbers indicate a "substantial" agreement according to one of the general guidelines  (Landis and Koch, 1977)  for kappa-like measures. 

 Experiments 

 Implementation Details We choose three datasets SQuAD v1.1  (Rajpurkar et al., 2016) , HotpotQA  (Yang et al., 2018b)  and MS-MARCO Q&A subset to train KPQA. We combine the training set of the three datasets and use a 9:1 split to construct the training and development set of KPQA. For HotpotQA, we exclude yes/no type questions where the answers are not in the passage. For model parameters, we choose bert-baseuncased variants for the BERT model and use one fully-connected layer with softmax layer after it. We train 5 epochs and choose the model that shows the minimum evaluation loss. We provide more details in Appendix. 

 Results Evaluation Methods for Metrics: To compare the performance of various existing metrics and our metric, we use the Pearson coefficient and Spearman coefficient. We compute these correlation coefficients with human judgments of correctness. We test using MS-MARCO, AVSD, from which we collected human judgments, and NarrativeQA and SemEval from . Performance Comparison: We present the correlation scores for the baseline metrics and KPQAaugmented ones for multiple datasets in Table  3 . The correlations between human judgment and most of the existing metrics such as BLEU or ROUGE-L are very low, and this shows that those widely used metrics are not adequate to GenQA. Moreover, the performance of existing metrics is especially low for the MS-MARCO, which has longer and more abstractive answers than the other three datasets. We observe a significantly higher correlation score for our proposed KPQA-metric compared to existing metrics especially for MS-MARCO and AVSD where the answers are full-sentences rather than short phrases. For the NarrativeQA, where existing metrics also have higher correlations, the gap in performance between KPQA-metric and existing metrics is low. We explain this is because the answers in NarrativeQA are often a single word or short phrases that are already keyphrases. 

 Comparison with IDF: The next best metric after our proposed metric is the original BERTScore, which uses contextual embeddings and adopts IDF based importance weighting. Since IDF is dependent on the word-frequency among the documents, it can assign a lower weight to some important words to evaluate correctness if they frequently occur in the corpus as shown in to words in the answer sentence using the context of the question. This approach provides dynamic weights for each word that leads to a better correlation with human evaluation as shown in Table  3 . 

 Ablation Study Domain Effect: Our KPQA metric computes importance weights using a supervised model; thus our proposed method may suffer from a domain shift problem. Although our metric is evaluated on out-of-domain datasets except MS-MARCO, we further examine the effect of the domain difference by changing the trainset of KPQA. Since we train KPQA with the combination of SQuAD, HotpotQA and MS-MARCO Q&A, the original KPQA works as in-domain for MS-MARCO. To measure the negative domain effect, we exclude the MS-MARCO Q&A in the training set of KPQA and measure the performance of KPQA-metric on MS-MARCO. We annotate it "-KPQA /MARCO " and report the results in Table  4 . This drop shows the effect of the negative domain shift for our KPQA-metric. However, "-KPQA /MARCO " is still much higher than all of the previous metrics. Using the Question Context: Our KPQA uses the question as an additional context to predict the keyphrases in the sentence, as shown in Figure 3. To examine the power of utilizing the question information for the keyphrase predictor, we remove the question part from the dataset and train the keyphrase prediction model. With the newly trained model, we compute the importance weights for words in the target sentence and apply them to BLEU-1, ROUGE-L, and BERTScore. We call this metric as "-KP" and report the results in Table  4 . We observe that "-KPQA" metric is better than "-KP" metric for all of the three variants. These results show that training keyphrase predictor to find the short answer candidate in the sentence is effective for capturing the key information in the generated answer, but it is more effective when the question information is integrated. 

 Analysis Correlation Among Question Type: Since MS-MARCO provides the question type information (PERSON, NUMERIC, DESCRIPTION, LOCA-TION, ENTITY) for each {question, answer} pair, we evaluate the various metrics by the question type. We split the dataset into these five question types and measure the performance of various metrics with Pearson correlation coefficients. As shown in Figure  4 , our KPQA-metric variants outperform their original version in all of the question types. KPQA-metric is especially effective for the NU-MERIC question type, whose answer sentence often has shorter keyphrase such as a number. For ENTITY and PERSON question types, the gap between KPQA-integrated metric and original metric Question : How to cook sausage peppers onions ? Reference Answer : To cook sausage peppers onions first place the sausage in a large skillet over medium heat, and brown on all sides after that remove from skillet, and slice meelt butter in the skillet, stir in the yellow onion, red onion, and garlic, and cook 2 to 3 minutes and then mix in red bell pepper and green bell pepper season with basil, and oregano in last stir in white wine. Generated Answer : To cook sausage peppers onions , preheat the oven to 350 degrees fahrenheit . Place the onions in the oven and cook for 20 minutes is lower for BERTScore. We speculate that this is because the original BERTScore uses IDF-based importance weighting, unlike other metrics. Multiple Sentence Answers: Most of the answers in MS-MARCO and AVSD consist of single sentences, but the answers for GenQA can be multiple sentences like  (Fan et al., 2019) . To verify our KPQA-metric on multiple sentence answers, we collect additional 100 human judgments for the generated answer whose answers are multiple sentences in the MS-MARCO like the example in Figure  5 , and evaluate the various metrics on this dataset. As shown in Table  6 , our KPQA integrated metric shows still higher correlations than other metrics. We observe that the gap between KPQA integrated metrics and existing metrics is relatively lower than that of Table  3 . We speculate this is because many of the multiple sentence answers are DESCRIPTION type answers whose keyphrases are sometimes vague, similar to the results in Fig-  ure 4 . Error Analysis: We pick 100 error cases from MS-MARCO in the order of a large difference in ranks among 1k samples between human judgments and BERTScore-KPQA. The importance weights have no ground-truth data; thus we manually visualize the weights as shown in Table  5  and analyze the error cases. From the analysis, we observe some obvious reasons for the different judgments between humans and BERTScore-KPQA. We first classify error cases by the question types and observe that 51 cases belong to NUMERIC, and 31 cases belong to DESCRIPTION. We further analyze the NUMERIC question type and find that many parts of the errors Context ... , it can take 5-20 hours of walking to lose 1 pound ... , ... Question How long do i need to walk in order to loose a pound ? 

 Reference Walk for 5 to 20 hours to lose 1 pound . 

 IDF Walk for 5 to 20 hours to lose 1 pound . 

 KPW Walk for 5 to 20 hours to lose 1 pound . Human Judgment: 0.94, BERTScore: 0.72, BERTScore-KPQA: 0.93 UniLM You need to walk for 5 to 20 hours in order to loose a pound . 

 IDF You need to walk for 5 to 20 hours in order to loose a pound . 

 KPW You need to walk for 5 to 20 hours in order to loose a pound .   are due to higher weights on units such as "million" or "years." There exist a total of ten error cases for this type, and we believe that there is room for improvement with regard to these errors through post-processing. In the case of the DESCRIPTION question type, 17 out of 31 cases are due to inappropriate importance weights. We speculate this result is because the keyphrases for the answers to questions belonging to the DESCRIPTION type are sometimes vague; thus, the entire answer needs to be considered when it is evaluated. Rank-Pair: One practical usage of the text evaluation metric is ranking outputs of multiple models. Using the collected human judgments of correctness for the same 500 {question, reference answer} pairs for two models on MS-MARCO and AVSD, we can compare the output of each models through the human-annotated score. To see the alignment of ranking ability among the various metrics with that of human judges, we conduct a "win-lose match" experiment, counting the number of times that a metric ranks the output of two models as the same as human judges. To prepare test samples, we chose only those whose gap between human judgment scores on the two models is greater than 2. Finally, we obtain 93 and 193 samples for MS-MARCO and AVSD, respectively. Considering that the range of scores is 1-5, this approach ensures that each output of the models has a clear quality difference. Table  7  shows the percentage of rank-pair matches for each metric with human judgments of correctness on two datasets. Our KPQA-metric shows more matches than previous metrics in all of the datasets; thus, it is more useful for comparing the generated answers from different models. larly to other natural generation tasks such as neural machine translation. Hence, researchers often use n-gram based similarity metrics such as BLEU to evaluate the GenQA systems, following other natural language generation tasks. However, most of these n-gram metrics including BLEU were originally developed to evaluate machine translation and previous works  Nema and Khapra, 2018; Kryscinski et al., 2019)  have shown that these metrics have poor correlations with human judgments in other language generation tasks such as dialogue systems. As with other text generation systems, for GenQA, it is difficult to assess the performance through ngram metrics. Especially, n-gram similarity metrics can give a high score to a generated answer that is incorrect but shares many unnecessary words with the reference answer. Previous works  (Marton and Radul, 2006; Yang et al., 2018a;  have pointed out the difficulty of similar problems and studied automated metrics for evaluating QA systems. Inspired by these works, we focus on studying and developing evaluation metrics for GenQA datasets that have more abstractive and diverse answers. We analyze the problem of using existing n-gram similarity metrics across multiple GenQA datasets and propose alternative metrics for GenQA. 

 Conclusion In this paper, we create high-quality human judgments on two GenQA datasets, MS-MARCO and AVSD, and show that previous evaluation metrics are poorly correlated with human judgments in terms of the correctness of an answer. We propose KPQA-metric, which uses the pre-trained model that can predict the importance weights of words in answers to a given question to be integrated with existing metrics. Our approach has a dramatically higher correlation with human judgments than existing metrics, showing that our model-based importance weighting is critical to measure the correctness of a generated answer in GenQA. 
