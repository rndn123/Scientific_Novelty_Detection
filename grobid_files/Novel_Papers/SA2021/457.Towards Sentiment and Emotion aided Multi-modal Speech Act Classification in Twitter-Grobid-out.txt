title
Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter

abstract
Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA ('TA' means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants.

Introduction Identification of speech acts is one of the preliminary means of determining the communicative intent or pragmatics of a speaker (for example, statement, request, question etc.). This is true for dialogue system, speech transcription, social media such as Twitter, MySpace etc. Twitter is one of the leading micro-blogging services. By 2019, 330 million users were active monthly and 500 million tweets were sent per day 1 . Identification of tweet acts (TAs-speech acts in Twitter) is highly beneficial for Twitter as well as tweeters. For Twitter, it helps decipher a particular subject in terms 1 https://www.omnicoreagency.com/twitter-statistics/ of speech acts and discrepancy identification. It also helps in social media monitoring by analysing topic alteration or spamming. It assists the followers in monitoring and scanning the subject with the most advantageous speech acts based on their needs. This helps reduce their search space and encourages them to obtain useful information from out of millions of tweets. It gives the tweeter a greater sense of the content, mood and trend. A person's emotional state and sentiment greatly impacts its intended content  (Barrett et al., 1993) . Often sentiment and emotion are treated as two different problems  (Do et al., 2019) ,  (Soleymani et al., 2017) ,  (Albanie et al., 2018) ,  (Hossain and Muhammad, 2019) ,  (Majumder et al., 2019) . However, sentiment and emotion are are closely related. For example, emotions such as happy and joy are inherently related to a positive sentiment. But emotion is much more nuanced and fine-grained compared to sentiment  (Kumar et al., 2019) . Emotion along with sentiment provides better understanding of the state of mind of the tweeter. For example, a question or statement is associated with anticipation. An opinion is many times associated with anger or disgust. The close association between emotion and sentiment motivates considering tweeter's sentiment along with emotion while deciphering the tweet acts. For expressive TAs such as "expression", "request", "threat" etc., the tweeter's sentiment and emotion can aid in classifying true communicative intent and vice-versa. Additionally, multi-modal inputs, i.e., the combination of text and other nonverbal cues (emojis in tweets)  (Felbo et al., 2017)  help create reliable classification models aiding the identification of emotional state and sentiment of the tweeter which in turn help in determining correct TAs. In this paper, we leverage the relationships as delineated above to predict TAs of tweets in a multimodal framework. In this multi-task framework, TAC is treated as the primary task and Sentiment Analysis (SA) and Emotion Recognition (ER) as auxiliary (i.e., secondary) tasks. Contributions of this paper are as follows : i. We create a new dataset called EmoTA consisting of tweets with high-quality annotations of TAs, including emotionally aided and multi-modal cues; ii. We establish the need for considering the sentiment and emotional state of the tweeter while identifying TAs. iii. We propose a Dyadic Attention Mechanism (DAM) based multi-task adversarial learning framework for multi-modal TAC, SA and ER. In DAM, we incorporate intra-modal and inter-modal attention to integrate information across multiple modalities and learn generalized features across multiple tasks; iv. We illustrate performance gains by jointly optimizing TAC, SA and ER. Multi-modal and multi-task TAC performs significantly better than its uni-modal and single task TAC variants. 

 Related Works There exist plenty of works which address the task of TAC as a standalone problem. In  (Zhang et al., 2011) ,  (Vosoughi and Roy, 2016) , authors proposed Machine Learning based approaches for TAC namely Support Vector Machines (SVM), Logistic Regression etc. In  (Saha et al., 2020a) , authors proposed a first ever public dataset for the identification of speech acts in Twitter followed by a capsule based network built on top of BERT for TAC. In  (Vosoughi, 2015) , authors highlighted the importance of identification of tweet acts and established it to be one of the elementary steps for detection of rumours in Twitter. In  (Saha et al., 2020c) , authors proposed an attention based model built on top of the Transformer for predicting TAs. In  (Saha et al., 2020a) , authors proposed a capsule based network built on top of BERT for TAC. All these works utilized only the textual modality to identify TAs without any sentiment or emotional correlation of the tweeter. In  (Cerisara et al., 2018) , authors proposed a LSTM based study for jointly optimizing SA and TAC in a decentralized social media platform called Mastodon. However, they modelled their task as a multi-party conversation pretty different in essence to that of Twitter analysis. In  (Jeong et al., 2009) , authors presented a semi-supervised approach to identify speech acts in emails and different forums. These works, however, use datasets that comprise of face-to-face or telephone data that can not directly aid in advancing work on endless data in electronic mode such as micro-blogging networks, instant-messaging, etc. Apart from these, identification of speech acts has been studied extensively for dialogue conversations starting from early 2000's with  (Stolcke et al., 2000)  being one of the benchmark works where the authors presented varieties of approaches such as Hidden Markov Models, Neural Networks and Decision Trees to identify dialogue acts on a benchmark dialogue data known as the Switchboard (SWBD)  (Godfrey et al., 1992)  dataset. In  (Saha et al., 2021) , authors studied the role of emotion in identifying dialogue acts for a dyadic conversation by considering thee textual and the audio modality of the utterances in the conversation. In  (Saha et al., 2020b) , authors proposed studying the role of emotion in determining dialogue acts on a dyadic and multi-party conversational dataset in a multi-modal framework (incorporating text, audio and video). However, tweets are unstructured and noisy communications with spelling mistakes, random coinages with limitations in expression because of character constraint per tweet. This makes it very different from face-to-face or other conversations. 

 Dataset Here, we discuss the details of the newly created dataset, EmoTA. 

 Data Collection To begin with, we scanned the literature for the latest SA and ER dataset for Twitter in order to gather potentially emotionally rich tweets to explore its impact on TAC. Initially, we came across several SA and ER datasets for Twitter such as  (Oleri and Karagoz, 2016) , , SemEval-2018 , BTD  (Wang et al., 2012) , TEC  (Mohammad, 2012) , CBET (Shahraki and Zaiane, 2017), STS-Gold (Mohammad and Turney, 2013), STS  (Go et al., 2009) , SS-Twitter  (Thelwall et al., 2012)  etc. However, we chose to use SemEval-2018 dataset for further investigation of our task at hand. The reason behind this choice was that most of the ER datasets were annotated with only six Eckman's  (Ekman, 1999)  or eight  Plutchik's (Plutchik, 1980)  emotion categories. Whereas SemEval-2018 dataset contains tweets annotated with multi-label 11 emotion categories which aids the diversity of the problem statement. Intuitively, it was indeed possible to go the other way round and search for Twitter dataset annotated with TAs such as  (Zhang et al., 2011)      (Vosoughi and Roy, 2016) ,  (Saha et al., 2020a)  etc. However, the tweets in these datasets were devoid of nonverbal cues such as emojis which are quite excessively used in Twitter. 

 Data Annotation To the best of our knowledge, we were unaware of any sizable and open sourced Twitter dataset annotated for its TA and emotion labels. Hence, the SemEval-2018 dataset has been manually annotated for its TA categories. Unlike dialogic conversations, there isn't a standard TA tag-set available for annotating tweets. However, we made use of 7 TA categories of  (Saha et al., 2020a)  for annotating SemEval-2018 dataset as opposed to 5 and 6 TA categories of  (Zhang et al., 2011)  and  (Vosoughi and Roy, 2016) , respectively. The 7 TA tags are "Statement" (sta), "Expression" (exp), "Question" (que), "Request" (req), "Suggestion" (sug), "Threat" (tht) and "Others" (oth). For the current work, we selected a subset of SemEval-2018 dataset amounting to 6810 tweets to create EmoTA dataset. Three annotators who were graduate in English linguistics were accredited to annotate the tweets with the appropriate TA tags. They were asked to annotate these tweets individually by only viewing the tweet available without the information of the pre-annotated emotion tags. This was done so as to assure that the dataset does not get biased by specific TA-emotion pairs. The conflicting annotations were resolved through discussions and mutual agreements. The inter-annotator score over 80% was considered as reliable agreement. It was determined based on the count that for a given tweet more than two annotators agreed on a particular tag. For annotating the dataset with sentiment labels, we followed a semi-supervised approach instead of manual annotation which is cost intensive. We used the IBM Watson Sentiment Classifier 2 , an open-sourced API readily available for obtaining silver standard sentiment label of the tweets categorized into 3 tags namely "Positive", "Negative" and "Neutral". 

 Emotion-Tweet Act Dataset : EmoTA The EmoTA dataset 3 now comprises of 6810 tweets with the corresponding gold standard TA and multilabel emotion tags. Each of the tweet contains its Tweet ID and two modalities: text and emoji. Few sample tweets along with the corresponding TA, sentiment and emotion labels from the proposed dataset are shown in Table  1 . Distributions of TA,  emotion and sentiment labels across the dataset are shown in Figure  1a , 1b and 2, respectively. 

 Qualitative Aspects Below, we analyze using some samples from the dataset that require sentiment-emotion aided and multi-modal reasoning. Role of Sentiment and Emotion. In Figure  3b , we demonstrate using two examples from the dataset to establish our hypothesis that sentiment and emotional states of the tweeter can aid the identification of TAs. In the first instance, the tweeter questions about the impending doom supposedly because of a pessimistic expectation arising due to the negative sentiment. Similarly, in the second instance, because of a joyous emotion emerging due to positive sentiment, the tweeter shares an optimistic suggestion with the readers. The above examples highlight the need for incorporating these additional user behavior, i.e., sentiment and emotion while reasoning about TAs. Thus, stressing the requirement of addressing such synergy amongst TAC, SA and ER. Role of Multi-modality. In Figure  3a , we present two examples from the dataset to highlight the importance of including other nonverbal features such as emoji present in the tweet along with the text for several tweet analysis tasks. In the first example tweet, the text represents an overall negative sentiment with emotion such as anger and disgust. However, the presence of an emoji face with tears of joy gives it an emotion of joy along with the other emotions. Similarly, in the second example tweet, the text represents the emotional state of the tweeter as sad, whereas the ok, celebration and heart emojis depict the feeling of joy. These instances show that the presence of complementary information in the form of emojis aids the process of any twitter analysis task including TAC. 

 Proposed Methodology The proposed multi-tasking, multi-modal approach and implementation details are outlined in this section. 

 Feature Extraction The procedure for feature extraction across multiple modalities is discussed below. Textual Features. To extract textual features of a tweet U having n u number of words, the representation of each of the words, w 1 , ..., w u , where w i ? R du and w i 's are obtained from BERT  (Devlin et al., 2019)  which is a multi-layered attention aided bidirectional Transformer Encoder model based on the original Transformer model  (Vaswani et al., 2017)  where d u = 768. Emoji Features. To extract emoji features from a tweet, we use emoji, a python based library for eliciting the pictorial image of an emoji (primarily that of a face, object or symbols). A total of 1816 kind of emojis are available along with its different types. We then use emoji2vec  (Eisner et al., 2016) , which provides d v = 300 dimensional vector representation for each of the emojis present in the tweet. Let's say a tweet contains n v number of emoji. Thus, we obtain the final emoji representation V for a tweet as V ? R nv?dv . 

 Network Architecture The proposed network consists of four main components : (i) Modality Encoders (ME) produces respective modality encodings by taking as input the uni-modal features extracted above, (ii) Dyadic Attention Mechanism (DAM) that comprises dual attention mechanisms such as intra-modal and intermodal attentions, (iii) Adversarial Loss to make the feature spaces of task-specific and shared layers of each task mutually exclusive, (iv) Classification Layer that contains output channels for the three tasks at hand (TAC, SA and ER) to learn generalized representations across all the tasks.  

 Modality Encoders In this section we discuss how the architectural framework encodes different modalities. Text and Emoji Modalities. The features U and V obtained from each of the modalities corresponding to a tweet (discussed above) are then passed through two discrete Bi-directional LSTMs (Bi-LSTMs)  (Hochreiter and Schmidhuber, 1997)  to sequentially encode these representations and learn complementary semantic dependency based features into hidden states from these modalities. In case of textual modality (say), the final hidden state matrix of a tweet is obtained as H u ? R nu?2d l . d l represents the number of hidden units in each LSTM and n u is the sequence length. In the similar way, a representation of corresponding emoji modality encoding as H v ? R nv?2d l is obtained. The number of representations from modality encoders vary depending on the variant of the multitask learning framework used (e.g., fully shared (FS) or shared-private model (SP)). In a FS variant, two representations are obtained one for text and another for emoji cumulatively for optimizing all the three tasks. However, for a SP model, six encoding representations are obtained. Three for text and the remaining for emoji forming a pair of text-emoji representations for each of the three tasks. 

 Dyadic Attention Mechanism We use a similar concept as in  (Vaswani et al., 2017) , where the authors proposed to compute attention as mapping a query and a set of key-value pairs to an output. So, the representations obtained from the modality encoders above are passed through three fully-connected layers each termed as queries and keys of dimension d k = d f and values of dimension d v = d f . For a FS model, we have two triplets of (Q, K, V ) as : (Q u , K u , V u ) and (Q v , K v , V v ). Similarly for a SP model, we have six such triplets as : (Q u1 , K u1 , V u1 ), (Q v1 , K v1 , V v1 ), (Q u2 , K u2 , V u2 ), (Q v2 , K v2 , V v2 ), (Q u3 , K u3 , V u3 ), (Q v3 , K v3 , V v3 ) where pair of two triplets are from the textual and emoji modality encoders for each of the tasks 4 . These triplets are then used to compute attention values for different purposes in various combinations which include intra attention and inter-modal attention. Intra-modal Attention. We compute intramodal attention (IA) for all these individual modalities in order to learn the interdependence between the current words and the preceding part of the tweet. In a way, we aim to relate different positions of a single sequence to estimate a final representation of the same sequence for individual modalities  (Vaswani et al., 2017) . Thus, the IA scores for individual modalities are calculated as : IA j = sof tmax(Q j K T j )V j (1) where IA ? R nu?d f for IA u , IA ? R nv?d f for IA v for FS model and six such IA scores for SP model. Inter-modal Attention. The IA scores obtained above are then used to compute inter-modal attention (IRA). We re-iterate the same process (explained above) to now form triplets of (Q, K, V ) for these IA scores and then compute IRA scores amongst triplets of all IA scores by computing the matrix multiplication of combination of queries and keys of different IA modality scores using Equation  1 . In this manner, we obtain one IRA score as IRA uv ? R nu?d f for FS variant and three IRA scores for SP model as IRA uv1 , IRA uv2 and IRA uv3 . This is done to distinguish important contributions between various modalities to achieve optimal representation of a tweet. Attention Fusion. Next, we concatenate each of these computed IA and IRA vectors as : C = concat(IRA uv , IA u , IA v ), for FS (2) C 1 = concat(IRA uv1 , IA u1 , IA v1 ), for SP (3) C 2 = concat(IRA uv2 , IA u2 , IA v2 ), for SP (4) C 3 = concat(IRA uv3 , IA u3 , IA v3 ), for SP (5) Next, we obtain mean of these three different concatenated attention vectors for the SP variant or directly use the obtained C attention vector for the FS variant to obtain the final representation of a tweet. M = mean(C 1 , C 2 , C 3 ) (6) Shared Layer. Additionally, for the SP model, other than having task-specific layers, we allow a shared layer to learn task invariant features. Here, the shared layer is in the form of a fully-connected layer of dimension d f . The inputs to the shared layer are the hidden representations of three IRA vectors : IRA uv1 , IRA uv2 and IRA uv3 . Thus for a given tweet, the loss of the shared layer is minimized if the model correctly classifies the tasks of each of the tweets in the input. This helps learn domain invariant feature space for different tasks. Adversarial Loss. The goal of this adversarial loss function is to tune the weights of the shared layer so that it learns a representation that misleads the task discriminator. The adversarial loss l adv , aims to make the feature space of shared and taskspecific layers to be mutually exclusive  (Liu et al., 2017) . We follow the similar strategy as that of  (Liu et al., 2017) , where a task discriminator D (say) maps the shared feature to its original task. Thus, on a correct prediction when the loss at the shared layer decreases, the adversarial loss increases and vice-versa. Alternatively, the shared layer is tuned to work in an adversarial way, thereby prohibiting the discriminator to predict one of the three tasks. The adversarial loss is computed as : l adv = min F (max D ( N n=1 K k=1 d n k log[D(F (x n k ))])) (7) where d n k represents the true label amongst the type of the tasks, N , and x n k is the k th example for task n. The min-max optimization problem is addressed by the gradient reversal layer  (Ganin and Lempitsky, 2015) . 

 Classification Layer The final representation of the tweet obtained from the DAM module is shared across three channels pertaining to the three tasks, i.e., TAC, SA and ER (for FS model) and three DAM representations for three individual tasks are subjected to individual output layer (for SP model). The task-specific loss (l t ), shared loss (l s ) and adversarial loss (l adv ) are used as l f = l t + ?l s + ?l adv , for SP model (8) l f = l s + ?l adv , for FS model (9) where ? and ? are hyper-parameters. 

 Experimentation Details Hyper-parameters. 80% of the tweets of the EmoTA dataset were used for training and the remaining 20% were used for testing the models. The same training and testing data were used for all the experiments in order to ensure fair comparison of models. To encode different modalities, a Bi-LSTM layer with 100 memory cells was used. Dense layers of dimensions 100 were used for d f . The three channels contain 7, 3 and 11 output neurons, for TA, sentiment and emotion tags, respectively. Categorical crossentropy loss is used for TA and sentiment channels and Binary crossentropy loss function is used for emotion channel. A learning rate of 0.01 and Adam optimizer were used in the final experimental setting. All these values of the parameters were selected after a careful sensitivity analysis. Pre-processing.   <number> token. Ekphrasis  (Baziotis et al., 2017)  was used to extract hashtags by segmenting long string into its constituent words. All the characters of the tweet were lower-cased. Since the dataset is under-represented for most of the TA tags, we over-sample 80% of the tweets used for training as : the mediocrely represented tags (e.g., sug, que and oth) are over-sampled to be equally represented as the most represented tags (e.g., sta and exp). Similarly, the highly under-represented classes (e.g., req and tht) are over-sampled to be equally represented as the mediocrely represented tags in the EmoTA dataset. All the results reported below are on the 20% test data without any over-sampling. 

 Results and Analysis A series of experiments were conducted for evaluating the proposed approach. Experiments were conducted for single task and several combinations of multi-task framework with TAC being the pivotal task along with varying modalities. A thorough ablation study is performed to analyze the importance of each of the attention mechanisms of the proposed architectural framework along with several variations of multi-task learning (e.g., FS, SP etc.). Note that we aim to enhance the performance  of TAC with the help of other two auxiliary tasks. Following this, we report results and analysis with TAC strictly being the pivotal task in all the task combinations. Since, the dataset is unbalanced for all the task categories, we report results for different dimensions of TAC in the following set-up: ? Five-class Classification : This includes the top 5 highly occurring TA tags namely sta, exp, que, sug and oth. ? Seven-class Classification : This includes all the 7 categories of TAs used in the annotation process. Table  3  and 2 illustrate the results of single task TAC and varying combinations of multi-task proposed models for different set-up (as mentioned above). As evident, the addition of non-verbal cues in the form of emojis improves the uni-modal textual baseline consistently. This improvement implies that the proposed architecture utilizes the interaction among the input modalities very effectively. This highlights the importance of incorporating multi-modal features for different Twitter analysis tasks. We also report result for utilizing emoji as textual feature instead of treating it as a different modality in the single task TAC framework. Also, the five-class set-up gave better results than the seven-class set up. This is pretty obvious, as with 5-class set-up, the model needs to distinguish and identify lesser fine-grained features compared to the 7-class set-up. Additionally, the underrepresentation of two tags in the EmoTA dataset for the 7-class set-up also effects its performance. As seen in Table  2 , the multi-task framework with all the three tasks (i.e., TAC + SA + ER) consistently gave better results as compared to single task TAC. In the bi-task variant, TAC+SA, shows little improvement in different metrics as opposed to TAC+ER over and above the single task TAC. This gain is rather intuitive as sentiment alone is sometimes unable to convey complete information of the tweeter's state of mind. E.g., a negative sentiment can occur because of various emotions such as disgust, fear, sadness etc. Similarly, a positive sentiment can take place because of emotions such as happiness, surprise etc. Thus, with sentiment alone, sometimes this discreteness or fine differences in the state of mind cannot be completely determined and conveyed. To illustrate this, in Figure  5 , we provide a visualization of the learned weights of a tweet from the IA u layer (as this layer contains word-wise attention scores). For this particular tweet, its true TA label is tht. With the multi-task framework, the importance of warning bearing words are learnt well such as lost, revenge compared to the single-task TAC where attention is laid on expression bearing word such as put me. Additionally, we also report results for cases where sentiment and emotion were directly used as features in the single task TAC models to leverage from instead of deploying a multi-task based approach in Table  3 . As stated above, we treat SA and ER as auxiliary tasks aiding the primary task, i.e., TAC. However, we report the performance of SA and ER tasks on the proposed model for single as well as multi-task frameworks in Table  4  for further investigations. However, we do not make any explicit effort to enhance their performance. Comparison amongst Different Multi-task Architecture. In terms of varying ways of multitasking such as FS, SP along with adversarial loss (adv), it was observed that SP model gave better results compared to FS model. Additionally, incorporating adversarial loss further boosted the performance of different multi-task models. Intuitively, as TAC shares lesser amount of correlation with SA and ER compared to SA and ER themselves, FS model was not sufficient enough to learn diverse features across different tasks. This observation is in conformity with the existing literature. We also demonstrate the importance of different attentions used for the best performing multi-task model, i.e., SP+Adv. Furthermore, we also report results by replacing BERT model to extract textual representation with Glove embeddings  (Pennington et al., 2014) . Results indicate that each of these aspects contributed significantly to aid the performance of the proposed multi-tasking framework. All the reported results here are statistically significant  (Welch, 1947) . Comparison with the State of the Art Models. We also compare our proposed approach with the recent state of the art models for single task TAC as we are unaware of any other work which jointly optimized tweet act, emotion and sentiment in Twitter. In Table  6 , we report the results for the same by re-implementing those on the EmoTA dataset. As 5735 evident, the proposed model outperformed these SOTA approaches. Error Analysis. An in-depth analysis revealed several scenarios as to why the proposed model faltered which are as follows : i. Imbalanced Dataset : As visible in Figure  1a , except for "sta" and "exp" tags, all the classes are under-represented in the EmoTA dataset. Even though we apply oversampling to partially counter this issue but still the tags such as "req" and "tht" contain very little tweets for the model to learn fine differences amongst different categories. In accordance with this, we observe that five-class performs exceptionally better than the seven-class classification set-up; ii. Fine-grained tags : It was also observed that the tweets which were mis-classified were subset of each other. For instance, tweet such as "don't get discouraged! it's early on; it can get overwhelming. keep reading; use cue cards it'll get better!!" is wrongly predicted as "exp" rather than "sug" which in the superficial way is a subset of the former tag; iii. Miscellaneous : Tweets belonging to "oth" tag was also majorly mis-classified as there was no fixed pattern of tweets belonging to this category. To counter this, even more fine-grained categories of TAs needs to be identified and modelled. Sample utterances for the error analysis are shown in Table  5 . 

 Conclusion and Future Work In this paper, we studied the role of sentiment and emotion in speech act classification in Twitter. We curate a novel dataset EmoTA, that contains pre-annotated tweets with emotions collected from open-source dataset and annotated with TAs and sentiment categories. We propose a Dyadic Attention Mechanism based multi-modal (emojis and text), adversarial multi-task framework for joint optimization of TAs, sentiment and emotions. The DAM (dyadic attention mechanism) module employs intra-modal and inter-modal attention to fuse multiple modalities and learn generalized features across all the tasks. Results show that multi-modality and multi-tasking boosted the performance of TA identification compared to its uni-modal and single task TAC variants. In future, attempts will be made to predict TAs with more precision by incorporating fine-grained modality encodings and also identifying which other NLP tasks (e.g., named entity recognition) might assist TAC as a task. Figure 1: (a) Distribution of tweet act labels, (b) Distribution of emotion labels. 
