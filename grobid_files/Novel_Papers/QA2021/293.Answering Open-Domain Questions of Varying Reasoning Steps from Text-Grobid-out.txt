title
Answering Open-Domain Questions of Varying Reasoning Steps from Text

abstract
We develop a unified system to answer directly from text open-domain questions that may require a varying number of retrieval steps. We employ a single multi-task transformer model to perform all the necessary subtasks-retrieving supporting facts, reranking them, and predicting the answer from all retrieved documents-in an iterative fashion. We avoid crucial assumptions of previous work that do not transfer well to real-world settings, including exploiting knowledge of the fixed number of retrieval steps required to answer each question or using structured metadata like knowledge bases or web links that have limited availability. Instead, we design a system that can answer open-domain questions on any text collection without prior knowledge of reasoning complexity. To emulate this setting, we construct a new benchmark, called B QA, by combining existing one-and twostep datasets with a new collection of 530 questions that require three Wikipedia pages to answer, unifying Wikipedia corpora versions in the process. We show that our model demonstrates competitive performance on both existing benchmarks and this new benchmark. We make the new benchmark available at https: //beerqa.github.io/.

Introduction Using knowledge to solve problems is a hallmark of intelligence. Since human knowledge is often containned in large text collections, open-domain question answering (QA) is an important means for intelligent systems to make use of the knowledge in large text collections. With the help of large-scale datasets based on Wikipedia  (Rajpurkar et al., 2016 (Rajpurkar et al., , 2018  and other large corpora  (Trischler et al., 2016; Dunn et al., 2017; Talmor and Berant, 2018) , the research community has made substantial progress on tackling this problem in recent years, including * These authors contributed equally. in the direction of complex reasoning over multiple pieces of evidence, or multi-hop reasoning  (Yang et al., 2018; Welbl et al., 2018; Chen et al., 2020) . Despite this success, most previous systems are developed with, and evaluated on, datasets that contain exclusively single-hop questions (ones that require a single document or paragraph to answer) or two-hop ones. As a result, their design is often tailored exclusively to single-hop (e.g.,  Chen et al., 2017; Wang et al., 2018b)  or multi-hop questions (e.g.,  Nie et al., 2019; Min et al., 2019; Feldman and El-Yaniv, 2019; Zhao et al., 2020a; Xiong et al., 2021) . Even when the model is designed to work with both, it is often trained and evaluated on exclusively single-hop or multi-hop settings (e.g.,  Asai et al., 2020) . In practice, not only can we not expect open-domain QA systems to receive exclusively single-or multi-hop questions from users, but it is also non-trivial to judge reliably whether a question requires one or multiple pieces of evidence to answer a priori. For instance, "In which U.S. state was Facebook founded?" appears to be single-hop, but its answer cannot be found in the main text of a single English Wikipedia page. Besides the impractical assumption about reasoning hops, previous work often also assumes access to non-textual metadata such as knowledge bases, entity linking, and Wikipedia hyperlinks when retrieving supporting facts, especially in answering complex questions  (Nie et al., 2019; Feldman and El-Yaniv, 2019; Zhao et al., 2019; Asai et al., 2020; Dhingra et al., 2020; Zhao et al., 2020a) . While this information is helpful, it is not always available in text collections we might be interested in getting answers from, such as news or academic research articles, besides being labor-intensive and time-consuming to collect and maintain. It is therefore desirable to design a system that is capable of extracting knowledge from text without using such metadata, to maximally emphasize using knowledge available to us in the form of text. ? Q + retrieved paras ? NOANSWER ? Q + Ingerophrynus Gollum + The Lord of the Rings ? "150 million copies" Q. The Ingerophrynus Gollum is named after a character in a book that sold how many copies? The IRRR question answering pipeline answers a complex question in the HotpotQA dataset by iteratively retrieving, reading, and reranking paragraphs from Wikipedia. In this example, the question is answered in five steps: 1. the retriever model selects the words "Ingerophrynus gollum" from the question as an initial search query; 2. the question answering model attempts to answer the question by combining the question with each of the retrieved paragraphs and fails to find an answer; 3. the reranker picks the paragraph about the Ingerophrynus gollum toad to extend the reasoning path; 4. the retriever generates an updated query "Lord of the Rings" to retrieve new paragraphs; 5. the reader correctly predicts the answer "150 million copies" by combining the reasoning path (question + "Ingerophrynus gollum") with the newly retrieved paragraph about "The Lord of the Rings". 

 Retriever To address these limitations, we propose Iterative Retriever, Reader, and Reranker (IRRR), which features a single neural network model that performs all of the subtasks required to answer questions from a large collection of text (see Figure  1 ). IRRR is designed to leverage off-the-shelf information retrieval systems by generating natural language search queries, which allows it to easily adapt to arbitrary collections of text without requiring welltuned neural retrieval systems or extra metadata. This further allows users to understand and control IRRR, if necessary, to facilitate trust. Moreover, IRRR iteratively retrieves more context to answer the question, which allows it to easily accommodate questions of different number of reasoning steps. To evaluate the performance of open-domain QA systems in a more realistic setting, we construct a new benchmark called B QA 1 by combining the questions from the single-hop SQuAD Open  (Rajpurkar et al., 2016; Chen et al., 2017)  and the two-hop HotpotQA  (Yang et al., 2018)  with a new collection of 530 human-annotated questions that require information from at least three Wikipedia pages to answer. We map all questions to a unified version of the English Wikipedia to reduce stylistic differences that might provide statistical shortcuts to models. As a result, B QA provides a more realistic evaluation of open-ended question answering systems in their ability to answer questions without knowledge of the number of reasoning steps required ahead of time. We show that IRRR not only achieves competitive performance with stateof-the-art models on the original SQuAD Open and HotpotQA datasets, but also establishes a strong baseline for this new dataset. To recap, our contributions in this paper are: (  1 ) a new open-domain QA benchmark, B QA, that features questions requiring variable steps of reasoning to answer on a unified Wikipedia corpus. (2) A single unified neural network model that performs all essential subtasks in open-domain QA purely from text (retrieval, reranking, and reading comprehension), which not only achieves strong results on SQuAD and HotpotQA, but also establishes a strong baseline on this new benchmark. 2 

 Open-Domain Question Answering The task of open-domain question answering is concerned with finding the answer to a question from a large text collection D. Successful solutions to this task usually involve two crucial components: an information retrieval system that finds a small set of relevant documents D from D, and a reading comprehension system that extracts the answer from it.  3 Chen et al. (2017)  presented the first neuralnetwork-based approach to this problem, which was later extended by  Wang et al. (2018a)  with a reranking system to further reduce the amount of context the reading comprehension component has to consider to improve answer accuracy. More recently,  Yang et al. (2018)  showed that this single-step retrieve-and-read approach to opendomain question answering is inadequate for more complex questions that require multiple pieces of evidence to answer (e.g., "What is the population of Mark Twain's hometown?"). While later work approaches these by extending supporting fact retrieval beyond one step, most assumes that all questions are either exclusively single-hop or multihop during training and evaluation. We propose IRRR, a system that performs variable-hop retrieval for open-domain QA to address these issues, and present a new benchmark, B QA, to evaluate systems in a more realistic setting. 

 IRRR: Iterative Retriever, Reader, and Reranker In this section, we present a unified model to perform all of the subtasks necessary for open-domain question answering-Iterative Retriever, Reader, and Reranker (IRRR), which performs the subtasks involved in an iterative manner to accommodate questions with a varying number of steps. IRRR aims at building a reasoning path from the question , through all the necessary supporting documents or paragraphs ? D gold to the answer (where D gold is the set of gold supporting facts).  4  As shown in Figure  1 , IRRR operates in a loop of retrieval, reading, and reranking to expand the reasoning path with new documents from ? D. Specifically, given a question , we initialize the reasoning path with the question itself, i.e., 0 = [ ], and generate from it a search query with IRRR's retriever. Once a set of relevant documents D 1 ? D is retrieved, they might either help answer the question, or reveal clues about the next piece of evidence we need to answer . The reader model then attempts to read each of the documents in D 1 to answer the question combined with the current reasoning path . If more than one answer can be found from these candidate reasoning paths, we predict the answer with the highest answerability score, which we will detail in section 3.2. If no answer can be found, then IRRR's reranker scores each retrieved paragraph against the current reasoning path, and appends the top-ranked paragraph to the current reasoning path,  4  For simplicity, we assume that there is a single set of relevant supporting facts that helps answer each question. i.e., +1 = + [arg max ? 1 reranker( , )], before the updated reasoning path is presented to the retriever to generate new search queries. This iterative process is repeated until an answer is predicted from one of the reasoning paths, or until the reasoning path has reached a cap of documents. To reduce computational cost and improve model representations of reasoning paths from shared statistical learning, IRRR is implemented as a multitask model built on a pretrained Transformer model that performs all three subtasks. At a high level, it consists of a Transformer encoder  (Vaswani et al., 2017)  which takes the reasoning path (the question and all retrieved paragraphs so far) as input, and one set of task-specific parameters for each task of retrieval, reranking, and reading comprehension (see Figure  2 ). The retriever generates natural language search queries by selecting words from the reasoning path, the reader extracts answers from the reasoning path and abstains if its confidence is not high enough, and the reranker assigns a scalar score for each retrieved paragraph as a potential continuation of the current reasoning path. The input to our Transformer encoder is formatted similarly to that of the BERT model . For a reasoning path that consists of the question and retrieved paragraphs, the input is formatted as "  [CLS]   We will detail each of the task-specific components in the following subsections. 

 Retriever The goal of the retriever is to generate natural language queries to retrieve relevant documents from an off-the-shelf text-based retrieval engine.  5  This allows IRRR to perform open-domain QA in an explainable and controllable manner, where a user can easily understand the model's behavior and intervene if necessary. We extract search queries from the current reasoning path, i.e., the original question and all of the paragraphs that we have already retrieved, similar to G E Retriever's approach  (Qi et al., 2019) . This is based on the observation that there is usually a strong semantic overlap between the reasoning path and the next paragraph to retrieve, which helps reduce the search space of potential queries. We note, though, that IRRR differs from G E Retriever in two important ways: (1) we allow search queries to be any subsequence of the reasoning path instead of limiting it to substrings to allow for more flexible combinations of search phrases; (2) more importantly, we employ the same retriever model across reasoning steps to generate queries instead of training separate ones for each reasoning step, which is crucial for IRRR to generalize to arbitrary reasoning steps. To predict these search queries from the reasoning path, we apply a token-wise binary classifier on top of the shared Transformer encoder model, to decide whether each token is included in the final query. At training time, we derive supervision signal to train these classifiers with a binary cross entropy loss (which we detail in Section 3.4.1); at test time, we select a cutoff threshold for query words to be included from the reasoning path. In practice, we find that boosting the model to predict more query terms is beneficial to increase the recall of the target paragraphs in retrieval. 

 Reader The reader model attempts to find the answer given a reasoning path comprised of the question and retrieved paragraphs. To support unanswerable questions and the special non-extractive answers yes and no from HotpotQA, we train a classifier conditioned on the Transformer encoder representation of the [CLS] token to predict one of the 4 classes SPAN/YES/NO/NOANSWER. The classifier thus simultaneously assigns an answerability score to this reasoning path to assess the likelihood of the document having the answer to the original question on this reasoning path. Span answers are predicted from the context using a span start classifier and a span end classifier, following . We define answerability as the log likelihood ratio between the most likely positive answer and the NOANSWER prediction, and use it to pick the best answer from all the candidate reasoning paths to stop IRRR's iterative process, if found. We find that this likelihood ratio formulation is less affected by sequence length compared to prediction probability, thus making it easier to assign a global threshold across reasoning paths of different lengths to stop further retrieval. We include further details about answerability calculation in Appendix C. 

 Reranker When the reader fails to find an answer from the reasoning path, the reranker selects one of the retrieved paragraphs to expand it, so that the retriever can generate new search queries to retrieve new context to answer the question. To achieve this, we assign each potential extended reasoning path a score by linearly transforming the hidden representation of the [CLS] token, and picking the extension that has the highest score. At training time, we normalize the reranker scores across top retrieved paragraphs with softmax, and maximize the log likelihood of selecting gold supporting paragraphs from retrieved ones, which is a noise contrastive estimation (NCE;  Mnih and Kavukcuoglu, 2013; Jean et al., 2015)  of the reranker likelihood over all retrieved paragraphs. 

 Training IRRR 

 Dynamic Oracle for Query Generation Since existing open-domain QA datasets do not include human-annotated search queries, we need to derive supervision signal to train the retriever with a dynamic oracle. Similar to G E Retriever, we derive search queries from overlapping terms between the reasoning path and the target paragraph with the goal of maximizing retrieval performance. To reduce computational cost, we limit our attention to overlapping spans of text between the reasoning path and the target document when generating oracle queries. For instance, when "David" is part of the overlapping span "David Dunn", the entire span is either included or excluded from the oracle query to reduce the search space. Once overlapping spans are found, we approximate the importance of each with the following "importance" metric to avoid enumerating all 2 combinations to generate the oracle query Imp( ) = Rank( , { } =1, ? ) ? Rank( , { }), where are overlapping spans, and Rank( , ) is the rank of target document in the search result when spans are used as search queries (the smaller, the closer is to the top). Intuitively, the second term captures the importance of the search term when used alone, and the first captures its importance when combined with all other overlapping spans, which helps us capture query terms that are only effective when combined. After estimating importance of each overlapping span, we determine the final oracle query by first sorting all spans by descending importance, then including each in the final oracle query until the search rank of stops improving. The resulting time complexity for generating these oracle queries is thus ( ), i.e., linear in the number of overlapping spans between the reasoning path and the target paragraph. Figure  3  shows that the added flexibility of nonspan queries in IRRR significantly improves retrieval performance compared to that of G E Retriever, which is only able to extract contiguous spans from the reasoning path as queries. 

 Reducing Exposure Bias with Data Augmentation With to construct a new benchmark. Note that naively combining the datasets by merging the questions and the underlying corpora is problematic, as the corpora not only feature repeated and sometimes contradicting information, but also make them available in two distinct forms (full Wikipedia pages in one and just the introductory paragraphs in the other). This could result in models taking corpus style as a shortcut to determine question complexity, or even result in plausible false answers due to corpus inconsistency. To construct a high-quality unified benchmark, we begin by mapping the paragraphs each question is based on to a more recent version of Wikipedia.  6  We discarded examples where the Wikipedia pages have either been removed or significantly edited such that the answer can no longer be found from paragraphs that are similar enough to the original contexts the questions are based on.  7  As a result, we filtered out 22,328 examples from SQuAD Open, and 18,649 examples from HotpotQA's fullwiki setting. We add newly annotated challenge questions to the test set of the new benchmark, which require at least three steps of reasoning to answer. This allows us to test the generalization capabilities of QA models to this unseen scenario. The statistics of the final dataset, which we name B QA, can be found in Table  1 . For all benchmark datasets, we report standard answer exact match (EM) and unigram F 1 metrics. Training details. We use ELECTRA LARGE  (Clark et al., 2020)  as the pre-trained initialization for our Transformer encoder. We train the model on a combined dataset of SQuAD Open and HotpotQA questions where we optimize the joint loss of the retriever, reader, and reranker components simultaneously in an multi-task learning fashion. Training data for the retriever and reranker components is derived from the dynamic oracle on the training set of these datasets, where reasoning paths are expanded with oracle queries and by picking the gold paragraphs as they are retrieved for the reader component. We augment the training data with the technique in Section 3.4.2 and expand reasoning paths up to 3 reasoning steps on HotpotQA and 2 on SQuAD Open, and find that this results in a more robust model. After an initial model is finetuned on this expanded training set, we apply our iterative training technique to further reduce exposure bias of the model by generating more data with the trained model and the dynamic oracle. 

 Results In this section, we present the performance of IRRR when evaluated against previous systems on standard benchmarks, and demonstrate its efficacy on our new, unified benchmark, especially with the help of iterative training. 

 Performance on Standard Benchmarks We first compare IRRR against previous systems on SQuAD Open and the fullwiki setting of HotpotQA. On each dataset, we compare the performance of IRRR against best previously published systems, as well as unpublished ones on public leaderboards. For a fairer comparison to previous work, we make use of their respective Wikipedia corpora, and limit the retriever to retrieve 150 paragraphs of text from Wikipedia at each step of reasoning. We also compare IRRR against the Graph Recurrent Retriever (GRR;  Asai et al., 2020)  on our newly collected 3-hop question challenge test set, using the author's released code and models trained on HotpotQA. In these experiments, we report IRRR performance both from training on the dataset it is evaluated on, and from combining the training data we derived from both SQuAD Open and HotpotQA. As can be seen in Tables  2 and 3 , IRRR achieves competitive performance with previous work, and further outperforms previously published work on SQuAD Open by a large margin when trained on combined data. It also outperforms systems that were submitted after IRRR was initially submitted to the HotpotQA leaderboard. On the 3+ hop challenge set, we similarly notice a large performance margin between IRRR and GRR, although neither is trained with questions requiring three or more hops, demonstrating that IRRR generalizes well to System SQuAD Open EM F 1 DrQA  (Chen et al., 2017)  27.1 -DensePR  (Karpukhin et al., 2020)  38.1 -BERTserini  (Yang et al., 2019)  38.6 46.1 MUPPET  (Feldman and El-Yaniv, 2019) 39.3  46.2 RE 3  (Hu et al., 2019)  41.9 50.2 Knowledge-aided  43.6 53.4 Multi-passage BERT  53.0 60.9 GRR  (Asai et al., 2020)  56.5 63.8 FiD  (Izacard and Grave, 2020)  56.7 -SPARTA  (Zhao et al., 2020b)  59 Table  3 : End-to-end question answering performance on HotpotQA and the new 3+ hop challenge questions, evaluated on the official HotpotQA Wikipedia paragraphs. ? denotes anonymous/preprint unavailable at the time of writing of this paper. ? indicates results we obtained using the publicly available code and pretrained models. questions that require more retrieval steps than the ones seen during training. We note that the systems that outperform IRRR on these datasets typically make use of trainable neural retrieval components, which IRRR can potentially benefit from adopting as well. Specifically, SPARTA  (Zhao et al., 2020b ) introduces a neural sparse retrieval system that potentially works well with IRRR's oracle query generation procedure to further improve retrieval performance, thanks to its use of natural language queries. HopRetriever  introduces a novel representation of documents for retrieval that is particularly suitable for discovering documents connected by the same entity to answer multi-hop questions, which IRRR could benefit from as well. We leave exploration of these directions to future work. To better understand the behavior of IRRR on these benchmarks, we analyze the number of paragraphs retrieved by the model when varying the number of paragraphs retrieved at each reasoning step among {50, 100, 150}. As can be seen in Figure  5 , IRRR stops its iterative process as soon as all necessary paragraphs to answer the question have been retrieved, effectively reducing the total number of paragraphs retrieved and read by the model compared to always retrieving a fixed number of paragraphs for each question. Further, we note that the optimal cap for the number of reasoning steps is larger than the number of gold paragraphs necessary to answer the question on each benchmark, which we find is due to IRRR's ability to recover from retrieving and selecting non-gold paragraphs (see the example in Figure  6 ). Finally, we note that increasing the number of paragraphs retrieved at each reasoning step remains an effective, if computationally expensive, strategy, to improve the end-to-end performance of IRRR. However, the tradeoff between retrieval budget and model performance is more effective than that of previous work (e.g., GRR), and we note that the queries generated by IRRR are explainable to humans and can help humans easily control its behavior. 

 Performance on the Unified Benchmark To demonstrate the performance of IRRR in a more realistic setting of open-domain QA, we evaluate it on the new, unified benchmark. As is shown in Table  4 , IRRR's performance remains competitive on all questions from different origins in the unified benchmark, despite the difference in reasoning complexity when answering these questions. The model also generalizes to the 3-hop questions despite having never been trained on them. We note that the large performance gap between the development and test settings for SQuAD Open questions is due to the fact that test set questions (the original SQuAD dev set) are annotated with multiple human answers, while the dev set ones (originally from the SQuAD training set) are not. To better understand the contribution of the various components and techniques we proposed for IRRR, we performed ablation studies on the model iterating up to 3 reasoning steps with 50 paragraphs for each step, and present the results in Table  5 . First of all, we find it is important to allow IRRR to dynamically stop retrieving paragraphs to answer the question. Compared to its fixed-step retrieval counterpart, dynamically stopping IRRR improves F 1 on both SQuAD and HotpotQA questions by 27.0 and 2.1 points respectively (we include further analyses for dynamic stopping in Appendix D). We also find combining SQuAD and HotpotQA datasets beneficial for both datasets in an opendomain setting, and that ELECTRA is an effective alternative to BERT for this task. 

 Related Work The availability of large-scale question answering (QA) datasets has greatly contributed to the research progress on open-domain QA. SQuAD  (Rajpurkar Question  The Ingerophrynus gollum is named after a character in a book that sold how many copies? Step 1 (Non-Gold) Ingerophrynus is a genus of true toads with 12 species. ... In 2007 a new species, "Ingerophrynus gollum", was added to this genus. This species is named after the character Gollum created by J. R. R. Tolkien." Query Ingerophrynus gollum book sold copies J. R. R. Tolkien Step 2 (Gold) Ingerophrynus gollum (Gollum's toad) is a species of true toad. ... It is called "gollum" with reference of the eponymous character of The Lord of the Rings by J. R. R. Tolkien. 

 Query Ingerophrynus gollum character book sold copies J. R. R. Tolkien true Lord of the Rings Step 3 (Gold) The Lord of the Rings is an epic high fantasy novel written by English author and scholar J. R. R. Tolkien. ... is one of the best-selling novels ever written, with 150 million copies sold. Answer/GT 150 million copies Figure  6 : An example of IRRR answering a question from HotpotQA by generating natural language queries to retrieve paragraphs, then rerank them to compose reasoning paths and read them to predict the answer. Here, IRRR recovers from an initial retrieval/reranking mistake by retrieving more paragraphs, before arriving at the gold supporting facts and the correct answer.  et al., 2016, 2018)  is among the first question answering datasets adopted for this purpose by  Chen et al. (2017)  to build QA systems over Wikipedia articles. Similarly, TriviaQA  (Joshi et al., 2017)  and Natural Questions  (Kwiatkowski et al., 2019)  feature Wikipedia-based questions that are written by trivia enthusiasts and extracted from Google search queries, respectively. More recently,  Petroni et al. (2021)  presented, KILT, a new benchmark based on Wikipedia where many knowledge-based tasks are evaluated in a unified version of Wikipedia, including open-domain question answering, entity linking, dialogue, etc. Unlike B QA, however, single-hop and multi-hop QA are held completely separate during evaluation in KILT, which makes the evaluation of open-domain QA less realistic. Aside from Wikipedia, researchers have also used news articles  (Trischler et al., 2016)  and search results from the web  (Dunn et al., 2017; Talmor and Berant, 2018)  as the corpus for open-domain QA. Inspired by the TREC QA challenge,  8 Chen et al. (2017)  were the first to combine information retrieval systems with accurate neural network-based reading comprehension models for open-domain QA. Recent work has improved open-domain QA performance by enhancing various components in this retrieve-and-read approach. While much research focused on improving the reading comprehension model  (Seo et al., 2017; Clark and Gardner, 2018) , especially with pretrained langauge models like BERT , researchers have also demonstrated that neural network-based information retrieval systems achieve competitive, if not better, performance compared to traditional IR engines  Khattab et al., 2020; Guu et al., 2020; Xiong et al., 2021) . Aside from the reading comprehension and retrieval components, researchers have also found value from reranking search results  (Wang et al., 2018a)  or answer candidates  (Wang et al., 2018b; Hu et al., 2019) . While most work focuses on questions that require only a local context of supporting facts to answer,  Yang et al. (2018)  presented HotpotQA, which tests whether open-domain QA systems can generalize to more complex questions that require evidence from multiple documents to answer. Researchers have explored various techniques to extend retrieveand-read systems to this problem, including making use of hyperlinks between Wikipedia articles  (Nie et al., 2019; Feldman and El-Yaniv, 2019; Zhao et al., 2019; Asai et al., 2020; Dhingra et al., 2020; Zhao et al., 2019)  and iterative retrieval  (Talmor and Berant, 2018; Das et al., 2019; Qi et al., 2019) . While most previous work on iterative retrieval makes use of neural retrieval systems that directly accept real vectors as input, our work is similar to that of  Qi et al. (2019)  in using natural language search queries. A crucial distinction between our work and previous work on multi-hop open-domain QA, however, is that we don't train models to exclusively answer single-hop or multi-hop questions, but demonstrate that one single set of parameters performs well on both tasks. 

 Conclusion In this paper, we presented Iterative Retriever, Reader, and Reranker (IRRR), a system that uses a single model to perform subtasks to answer open-domain questions of arbitrary reasoning steps. IRRR achieves competitive results on standard opendomain QA benchmarks, and establishes a strong baseline on B QA, the new unified benchmark we present, which features questions with mixed levels of complexity. 
