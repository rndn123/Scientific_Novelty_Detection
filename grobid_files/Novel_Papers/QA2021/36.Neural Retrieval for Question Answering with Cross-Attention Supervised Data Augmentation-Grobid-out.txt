title
Neural Retrieval for Question Answering with Cross-Attention Supervised Data Augmentation

abstract
Early fusion models with cross-attention have shown better-than-human performance on some question answer benchmarks, while it is a poor fit for retrieval since it prevents pre-computation of the answer representations. We present a supervised data mining method using an accurate early fusion model to improve the training of an efficient late fusion retrieval model. We first train an accurate classification model with cross-attention between questions and answers. The cross-attention model is then used to annotate additional passages in order to generate weighted training examples for a neural retrieval model. The resulting retrieval model with additional data significantly outperforms retrieval models directly trained with gold annotations on Precision at N (P@N) and Mean Reciprocal Rank (MRR).

Introduction Open domain question answering (QA) involves finding answers to questions from an open corpus  (Surdeanu et al., 2008; Yang et al., 2015; Chen et al., 2017; Ahmad et al., 2019) . The task has led to a growing interest in scalable end-to-end retrieval systems for question answering. When QA is formulated as a reading comprehension task, cross-attention models like BERT  (Devlin et al., 2019)  have achieved better-than-human performance on benchmarks such as the Stanford Question Answering Dataset (SQuAD)  (Rajpurkar et al., 2016) . Cross-attention models are especially well suited for problems involving comparisons between paired textual inputs, as they provide early fusion of fine-grained information within the pair. This encourages careful comparison and integration of details across and within the two texts. However, early fusion across questions and answers is a poor fit for retrieval, since it prevents precomputation of the answer representations. Rather, neural retrieval models independently compute embeddings for questions and answers, typically using dual encoders for fast scalable search  (Henderson et al., 2017; Gillick et al., 2018; Yang et al., 2019b; Karpukhin et al., 2020) . Using dual encoders results in late fusion within a shared embedding space. For machine reading, early fusion using crossattention introduces an inductive bias to compare fine grained text spans within questions and answers. This inductive bias is missing from the single dot-product scoring operation of dual encoder retrieval models. Thus, late fusion is expected to require more training data to learn the necessary representations for fine grained comparisons. To support learning improved representations for retrieval, we explore a supervised data augmentation approach leveraging a complex classification model with cross-attention between questionanswer pairs. Given gold question passage pairs, we first train a cross-attention classification model as the supervisor. Then any collection of questions can be used to mine potential question passage pairs under the supervision of the cross-attention model. The retrieval model training benefits from additional training pairs annotated with the graded predictions from the cross-attention model augmenting the existing gold data. Experiments on MultiReQA-SQuAD and MultiReQA-NQ establish significant improvements on Precision at N (P@N) and Mean Reciprocal Rank (MRR). The supervised mining approach is closely connected to the recently studied hard negative mining for neural retrieval models  (Xiong et al., 2020; Lu et al., 2020) . The key differences is that the proposed approach finds the positive training examples, while the negative mining approaches find the negative examples for training. The two approaches are complementary and can be combined. 

 Neural Passage Retrieval for Open Domain Question Answering Open domain question answering systems usually follow a two-step approach: first retrieve question relevant passages, and then scan the returned text to identify the answer span using a reading comprehension model  (Jurafsky and Martin, 2018; Kratzwald and Feuerriegel, 2018; Yang et al., 2019a) . Prior work has focused on the answer span annotation task and has even achieved super human performance on some datasets. However, the evaluations implicitly assume the trivial availability of passages for each question that are likely to contain the correct answer. While the retrieval task can be approached using traditional keyword based retrieval methods such as BM25, there is a growing interest in developing more sophisticated neural retrieval methods  Guu et al., 2020; Karpukhin et al., 2020) . 3 Retrieval Question-Answering (ReQA)  Ahmad et al. (2019)  introduced Retrieval Question-Answering (ReQA), a task that has been rapidly adopted by the community  (Guo et al., 2020; Zhao and Lee, 2020; Roy et al., 2020) . Given a question, the task is to retrieve the answer sentence from a corpus of candidates. ReQA provides direct evaluation of retrieval, independent of span annotation. Compare to Open Domain QA, ReQA focuses on evaluating the retrieval component and, by construction, avoids the need for span annotation. We explore the proposed approach on MultiReQA-NQ and MultiReQA-SQuAD  (Guo et al., 2020) . 1 MultiReQA  (Guo et al., 2020)    

 Methodology In this section we describe the proposed approach using a neural retrieval model augmented with su-  pervised data mining. Figure  2  illustrates our approach using a cross-attention classifier to supervise the data augmentation process for training a retrieval model. After training the cross-attention model, we retrieve additional potential answers to questions using an off-the-shelf retrieval system 2 . The predicted scores from our classifier with cross-attention are then used to weight and filter the retrieved candidates with positive examples serving as additional training data for the dual encoder based retrieval model. 

 BERT Classification Model Cross-attention models like BERT are often used for re-ranking after retrieval and can significantly improve performance as they allow for fine-grained interactions between paired inputs  (Nogueira et al., 2019; Han et al., 2020) . Here we formalize a binary classification task for predicting question answer relatedness. We use the question-answer pairs from the training set as our positive examples. Negatives are sampled for each question using the following strategies with a 1:1:1 ratio: (1) A sentence from the top 10 nearest neighbors returned by a term based BM25  (Robertson and Zaragoza, 2009 ) over a sentence pool containing all supporting documents in a corpus. (2) A sentence from the top 10 nearest neighbors using the Universal Sentence Encoder -QA (USE-QA)  (Yang et al., 2019b) . (3) A sentence randomly sampled from its supporting documents, excluding the question's gold answer. The sampled non-answer sentences are paired with their questions as negative examples. A BERT model is fine-tuned following the default setup from the  Devlin et al. (2019) . The BERT dual-encoder model can be fine-tuned using the in batch sampled softmax loss  (Gillick et al., 2018) : J = (x,y)?Batch e ?(x,y) ?Y e ?(x,?) (1) Where x is the question, y is the correct answer, Y is all answers in the same batch that are used as sampled negatives, and ?(x, y) is the dot product of question and answer representations. Note that the dot product is scaled by X100 during training, which is a critical component when applying l 2 normalization to the embeddings.  

 Weighted In-batch Softmax for Dual-Encoder Retrieval Model The neural retrieval model is trained using the batch negative sampling loss  (Gillick et al., 2018)  in equation 2. We modify the standard formulation to include a weight, w(x, y), for each pair. J = (x,y)?Batch w(x, y) e ?(x,y) ?Y e ?(x,?) (2) We set w(x, y) to 1 if (x, y) is a ground truth positive pair and p(x, y) 2 , otherwise, whereby p(x, y) is the probability from the cross-attention model. 

 Evaluation In this section we evaluate the proposed approach using the MultiReQA evaluation splits for NQ and SQuAD. Models are assessed using Precision at N (P@N) and Mean Reciprocal Rank (MRR). Following the ReQA setup  (Ahmad et al., 2019) , we report P@N for N=  [1, 5, 10] . P@N evaluates whether the true answer sentence appears in the top-N ranked candidates. MRR is calculated as MRR = 1 N N i=1 1 rank i , where N is the total number of questions, and rank i is the rank of the first correct answer for the ith question. 

 Configurations Our cross-attention QA models are fine-tuned from the public English BERT for 10 epochs, using a batch size of 256 and a weighted Adam optimizer with learning rate 3e-5. We experiment with both BERT Base and BERT Large . All hyper-parameters are set using a dev set split out from the training data (10%). When mining for silver data, we only keep candidate pairs with positive cross-attention QA model scores (? 0.5). The BERT Base model is used to initialize the dual encoder retrieval model. During training we use a batch size of 64, and a weighted Adam optimizer with learning rate 1e-4. The maximum input length is set to 96 for questions and 384 for answers. Models are trained for 200 epochs. The embeddings are l 2 normalized. Hyper-parameters are manually tuned on a held out development set. 

 Performance for the Classification Task The classification data created using the method from section 4.1 contains a total of 531k and 469k training examples for NQ and SQuAD, respectively. Test sets extracted from the SQuAD and NQ test splits contain 15k and 41k examples.  3  Table  2  provides the performance of the crossattention models, compared to a majority baseline which always predict false and a BERT dual encoder retrieval model without any mined examples that uses cosine similarity for prediction. Crossattention based models outperform the baselines by a wide margin, 4 with BERT Large achieving the highest performance on all metrics. This is consistent with our hypothesis that early fusion models outperform late fusion based retrieval models. Both models achieve better performance on SQuAD than NQ. The SQuAD task has higher token overlap, as described in section 3, making the task somewhat easier. We use the BERT Large model to supervise the data augmentation in the next section. 

 Mined Examples We mined the SQuAD and NQ training data to construct additional QA pairs. After collecting and scoring addition pairs using the method described in section 4.3, we obtained 53% (56,148) and 12%  (10, 198)   Much less data is mined for SQuAD then NQ. We believe it is because of the way SQuAD was created, whereby workers write the questions based on the content of a particular article. The resulting questions are much more specific and biased toward a particular question types, e.g. what questions  Ahmad et al. (2019) . Additionally, the candidate pool for SQuAD is only half that of NQ, resulting in questions having fewer opportunities to be matched to good additional answers. 

 Results on the Retrieval QA Table  3  gives P@N and MRR@100 for retrieval models on MultiReQA-SQuAD and MultiReQA-NQ. The first two rows show the result from two simple baselines: BM25  (Robertson and Zaragoza, 2009) , USE-QA, and USE-QA finetune reported by  Guo et al. (2020) . BM25 remains a strong baseline, especially with 62.8% P@1 and 70.5% MRR for SQuAD. BM25's performance on NQ is much lower, as there is much less token overlap between NQ questions and answers. USE-QA matches the performance of BM25 on NQ but performs worse on SQuAD. 5 BERT dual encoder performs well compared to other baselines, especially on NQ with a +6.6 point improvement compared to the USE-QA finetune model.  6  Its P@1 on SQuAD performs better than USE-QA and BM25, but -3.1 points MRR worse than USQ-QA finetune . On average, BERT dual encoder is the best among those baselines. Performance improves by a large margin using augmented training data from our cross-attention QA model, obtaining a +8.6 and +7.0 improvement on NQ P@1 and MRR. Compare to NQ, the improvement on SQuAD is rather marginal. The augmented BERT dual encoder retrieval model only achieves slightly improved performance on SQuAD, with +1 points for both P@1 and MRR. As discussed in section 5.3, we mine much less data from SQuAD compare to NQ, with only 10% more data than the original training set. As demonstrated by the strong BM25 performance and shown in  (Guo et al., 2020) , the SQuAD QA pairs have high token overlap between question and answers, Models NQ SQuAD P@1 P@5 P@10 MRR P@1 P@5 P@10 MRR BM25  minimizing the advantage of the neural methods in capturing more complex semantic relationships. Effectiveness of Weighted Softmax. We further experimented the Retrieval QA tasks using the model with the non-modified softmax using the augmented data. All other configurations are keep the same. The MRR of the model using nonmodified softmax is 60.1 on MultiReQA-NQ and 71.9 on MultiReQA-SQuAD, which are much worse than the model using weighted softmax. This result indicates the weighted softmax is important for the proposed approach. 

 Conclusion In this paper, we propose a novel approach for making use of an early fusion classification model to improve late fusion retrieval models. The early fusion model is used for data mining to augment the training set for the late fusion model. The proposed approach mines 53% (56,148) and 12%  (10, 198)  more examples for MultiRQA-NQ and MultiRQA-SQuAD, respectively. Compared to the models directly trained with gold annotations, the resulting retrieval models improve +8.6% and +1.0% P@1 on NQ and SQuAD respectively. The current pipeline assumes there exists annotated indomain question answer pairs to train the crossattention model. With a strong general purpose cross-attention model, our method could be modified to train in-domain retrieval models without gold data. We leave this to the future work. Figure 1 : 1 Figure1: Use of a cross-attention model for the supervised mining of additional QA pairs. Our accurate cross-attention model supervises the mining process by identifying new previously unannotated positive pairs. Mined QA pairs augment the original training data for the dual encoder based neural passage retrieval model. 
