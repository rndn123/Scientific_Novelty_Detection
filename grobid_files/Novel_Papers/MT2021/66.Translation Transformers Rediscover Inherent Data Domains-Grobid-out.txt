title
Translation Transformers Rediscover Inherent Data Domains

abstract
Many works proposed methods to improve the performance of Neural Machine Translation (NMT) models in a domain/multi-domain adaptation scenario. However, an understanding of how NMT baselines represent text domain information internally is still lacking. Here we analyze the sentence representations learned by NMT Transformers and show that these explicitly include the information on text domains, even after only seeing the input sentences without domains labels. Furthermore, we show that this internal information is enough to cluster sentences by their underlying domains without supervision. We show that NMT models produce clusters better aligned to the actual domains compared to pre-trained language models (LMs). Notably, when computed on document-level, NMT cluster-to-domain correspondence nears 100%. We use these findings together with an approach to NMT domain adaptation using automatically extracted domains. Whereas previous work relied on external LMs for text clustering, we propose re-using the NMT model as a source of unsupervised clusters. We perform an extensive experimental study comparing two approaches across two data scenarios, three language pairs, and both sentencelevel and document-level clustering, showing equal or significantly superior performance compared to LMs.

Introduction Neural machine translation (NMT,  Bahdanau et al., 2015; Vaswani et al., 2017b)  heavily depends on training data and the text domains covered in it. Full-scale NMT Transfomer models  (Vaswani et al., 2017b)  are usually trained on a mix of corpora from several domains  (Barrault et al., 2020) . However, the field lacks an understanding of how these NMT models represent the training data domains in their inner vector spaces. * Equal contribution This paper consists of two main parts. First, we analyze domain representations learned by the NMT Transformer. We consider sentence-level as well as document-level representations via mean pooling of token contextual embeddings. Our analysis shows that NMT models explicitly learn to include the domain information in their representational spaces across layers. Furthermore, we show that text representations preserve enough domainspecific information to reveal the underlying domains with Principal Component Analysis and kmeans clustering without supervision. In the case of document-level clustering, the result of k-means matches the original corpora almost perfectly. In the case of sentence-level clustering, we observe some deviation between automatic clusters and the original corpora that the sentences belong to, showing corpus heterogeneity on the sentence level.  Aharoni and Goldberg (2020)  previously revealed that a similar property exists in pre-trained language models (LMs). We compare LMs with NMT Transformers in how well we can extract unsupervised domain clusters from them and show the superiority of NMT models. In the second part of the paper, we show how to effectively utilize our analysis to improve an existing approach to NMT domain adaptation which uses automatically extracted domains  (Tars and Fishel, 2018; Currey et al., 2020) . This method targets the case when training domain labels are not precise (e.g.  Currey et al., 2020)  or missing overall, as in case of heterogeneous corpora (e.g. Paracrawl,  Espl? et al., 2019) . This framework has so far been used with external models for clustering, which automatically makes us rely on clusters not necessarily aligned with the objectives of translation or target data domains. We propose exploiting clusters extracted from the NMT baseline (already trained as a part of the framework) to improve translation quality without relying on external language models. We test our Figure  1 : PCA plots of sentence representations extracted from all layers of the 60th checkpoint of the trained baseline NMT model. Representations are computed with English sentences. The dots, denoting sentences, are colored according to the domain the corresponding sentences come from. The model learns to distinguish between domains in its hidden space, despite not being explicitly provided with any information about domains. L0 corresponds to fixed encoder embeddings, L1-L6 are encoder layers' representations, L7 shows fixed decoder embeddings and L8-L13 -the decoder layers' representations. The figure shows that representations from the same domain cluster together. proposal empirically, covering three language pairs and two data settings: a mix of corpora with known domain labels and a heterogeneous corpus without such labels. We show that fine-tuning the NMT models to the automatically discovered clusters on average matches or surpasses tuning to the original corpus labels (when available) and deep LM-based clusters. Our contributions are thus two-fold: 1 ? we analyze the NMT encoder's representations, showing their ability to automatically discover inherent text domains and cluster unlabelled corpora, testing both sentencelevel and document-level representations (Section 3); ? we utilize findings from our analysis to improve an existing Automatic Domains for NMT approach (Section 4) and perform an extensive experimental study, showing the superiority of our method (Section 5); 2 Related Work  Aharoni and Goldberg (2020)  found that BERT  (Devlin et al., 2019)  produces meaningful unsupervised domain clusters and used this finding for NMT data selection. In this work we analyse (sentence-level and document-level) hidden representations produced by a baseline NMT model and find that it learns superior unsupervised clusters by itself. In NMT, domain-specific information on the word level was recently analyzed by Jiang et al.  1  We release our code at https://github.com/ TartuNLP/inherent-domains-wmt21 (2020) in the context of domain mixing in a joint modular multi-domain NMT system. They found that representations contain domain-specific information related to the multiple domains in different proportions on the word level. We analyze representation on the sentence and document level, revealing that domain-specific information in representations converges to the one specific domain with a broader context.  Currey et al. (2020)  used contextual embeddings and mean-pooled representation clustering for domain adaptation. We compare our approach to  Currey et al. (2020) , however in their case the representations were extracted from multilingual BERT (mBERT). We cluster based on the NMT encoder's representations directly and also experiment with document-level representations in addition to sentence-level ones. Before  Currey et al. (2020) , the automatic domains framework has been used in NMT only with external models for clustering as well.  Tars and Fishel (2018)  used fixed embeddings from Fast-Text  (Bojanowski et al., 2017)  for clustering meanpooled sentence representations and then either tuning NMT systems to these clusters or supplying the cluster identity to the NMT system as additional input for multi-domain translation. 

 Analysis In this section, we perform an analysis of inherent domain representations in translation transformers. We reveal how well the domain-specific information in text representations is preserved in NMT models. We focus on "out-of-the-box" NMT systems without any changes and explore the extent to which we can use their internal representations to match the original text domains using Principal Component Analysis (PCA) and k-means clustering. We also measure the effect of using broader document-level representations. Additionally, we compare NMT representations to the ones extracted from a pre-trained language model, for which  Aharoni and Goldberg (2020)  revealed a high degree of domain-specific information. 

 Models and Data In our analysis, we start by following  Currey et al. (2020)  and similarly to them use a multilingual LM (XLM-R,  Conneau et al., 2020)  to obtain clusters. XLM-R is a multilingual masked language modeling transformer covering 100 languages. We then train Transformer-base  (Vaswani et al., 2017a)  NMT models, which have ?97M parameters each. We train the models on parallel data covering four corpora/text domains: parliament speeches (Europarl,  Koehn, 2005) , medical  (EMEA, Tiedemann, 2012) , subtitles (OpenSubtitles,  Lison and Tiedemann, 2016)  and legal (JRC-Acquis,  Steinberger et al., 2006) . We sub-sampled the larger corpora in order to balance the size of training data across domains. The NMT models were trained for 60 epochs. A detailed description of the setup, models, and data is provided in Appendix B. We focus on sentence-level and documentlevel representations, and two language pairs: English?Estonian (EN-ET) and German?English (DE-EN). 

 Dimensionality Reduction We start by unsupervised dimensionality reduction using PCA to visualize domain placement. We take the development set data, extract token embeddings from each model's layer, and average them to obtain sentence representations. Then we apply cosine-based PCA and t-SNE dimensionality reduction to the representations to visualize the data in a 2D space, and post factum color each data point (sentence) according to its corresponding domain. We show the resulting visualizations in Figure  1  (best viewed in color) for ET-EN (and in Figure  4  for t-SNE in the Appendix A, which mirrors the PCA result). Figure  1  shows that NMT partitions the domains quite well at all encoder hidden layers and deep decoder layers. Encoder layer 0 corresponds to the fixed embeddings, and the latent space is not well partitioned there yet; however, as we go deeper into the network, the separation increases. Layer 7 is the decoder's embedding layer, and there the same logic applies. While the encoder learns to partition the hidden space based on domains from scratch, the decoder has access to the encoder hidden states via encoder-decoder attention, which might simplify its task. In summary, Figure  1  is our initial evidence that the NMT encoder places the domains separably. 

 Clustering Our primary method, however, is unsupervised kmeans clustering. We consider four data clustering setups: sentence-level XLM-R clusters, sentencelevel NMT clusters, document-level XLM-R clusters, and document-level NMT clusters. The first one is the baseline clustering approach investigated by  Aharoni and Goldberg (2020)  while the remaining three are our original contributions. 

 Per-layer Clustering Purity Metric In our analysis, we estimate how well the NMT model preserves domain-specific information in its internal text representations. To do that, we measure the goodness-of-fit between unsupervised clusters and oracle domains. Specifically, we follow  Aharoni and Goldberg (2020)  and use the clustering purity metric. To compute clustering purity, we align domains and clusters by the highest overlap in numbers of sentences. The number of overlapping data points for each cluster-domain pair gives us the number of 'correctly predicted' examples. Then, the sum of all 'correctly predicted' examples divided by the total number of examples will be the clustering purity score. 

 Embedding and Clustering We first take the concatenation of a small subset of sentences (3k) from each of the four domains and try to partition them into four clusters based on the representations from each layer of XLM-R and NMT Transformer. We only use source sentences since we do not have targets at runtime in NMT. Specifically, we follow the steps below for each layer of each of the two models: 1. For each sentence in the dataset, we extract contextualized token embeddings from a layer of the model. 2. We use the average of contextualized token embeddings as sentence representations. 3. We apply k-means clustering to sentence representations to assign a cluster label to each sentence. 4. We compute clustering purity for predicted labels and oracle domains. We perform ten random restarts of k-means clustering, selecting the iteration with the smallest withincluster variance. Results Figure  2  shows per-layer clustering purity computed for sentence representations for XLM-R and two NMT Baseline checkpoints (after the first epoch and after the 60th epoch of training). Figure  2  shows that NMT surpasses the language model in its ability to rediscover domains. About 3.5x higher performance at the encoder layers shows that the encoder is the part that learned to be very aware of the input domains (in an unsupervised way). Figure  2  also shows that the checkpoint saved after the 1st training epoch rediscovers clusters slightly better then 60th checkpoint. However, this does not suggest that an NMT model should be trained for one epoch since the translation quality is suboptimal early on. Instead, we assume that the model quickly learns domain-specific information (perhaps due to the common lexical statistics) and then slightly "moves away" towards a higher level of abstraction as training progresses. This abstraction is necessary to successfully learn a task as complex as NMT.  

 Large-scale Clustering Next, we repeat the same steps for the entire training dataset and include a second language pair. Specifically, we pick one of the best performing layers We also extend our analysis to the documentlevel scenario. Specifically, we average over sentence representations to get document embeddings and cluster-based on them. Then, we assign the predicted label for each document to each sentence in that document.  2  Results We present large-scale clustering confusion matrices in Figure  3  and clustering purity in Table  1 . These show that sentence-level NMT is generally better than sentence-level XLM-R at rediscovering domains. However, they both show quite modest results for both language pairs. At the same time, document-level clusters are much better at rediscovering original domains. The reason for that might be that sentence-level clustering largely relies on the more shallow information in the text. For example, we observed that both sentence-level NMT and XLM-R produced a cluster responsible for extremely short sentences (the average sentence length is about four tokens for these clusters). On the other hand, document-level representations factor out these shallow stylistic features by averaging over sentence representations. Therefore, the models are inclined to cluster by topics. An alternative explanation is that domainspecific lexical statistics, which not all sentences might preserve, get more robust as we average sentence embeddings to get a document embedding. Even though sentence-level clustering maintains a general idea about oracle domains, they split sentences into clusters quite freely. For example, JRC-Acquis consistently gets mixed with Europarl, which both belong to legal domains. We can see it from NMT SENT for both language pairs. For documents, the rediscovery trend is common and pronounced for both language pairs, and separation is generally consistent between train and test. However, for EN-ET XLM-R DOC we can observe that EMEA and JRC-Acquis got split between two clusters in the training set. Considering that we perform ten random k-means restarts and choose the best iteration, this suggests that XLM-R may become inconsistent (as a source of sentence representations on the document level) in some cases.  

 Practical Application Our analysis in Section 3 revealed that NMT models represent domains in their embedding space separately, similarly to what pre-trained language models do  (Aharoni and Goldberg, 2020) . We demonstrated that simple clustering on NMT representations allows recovering original data domains to a large degree. This section proposes to utilize this finding to improve an existing framework of automatic domain generation for NMT. In this framework, related work first clusters the training data using representations from an external encoder, and then the base-line NMT model is adapted (fine-tuned) on each cluster separately. We propose to re-use the NMT baseline itself as the encoder in this framework. Representations extracted from translation Transformers are specific to the task of translation. We hypothesise that it might result in clusters most suitable for downstream translation tasks like finetuning to specific domains/clusters. Moreover, an advantage of our scenario is that we cluster the same data (with our NMT model) that we use for NMT model training. It is a frequent multi-domain NMT setup, where multiple target domains are available in training. In the pre-trained language model setup, the data will be more outof-domain, despite the model's generality. 

 Existing Framework (Background) In this subsection we describe an existing framework which uses automatic domains (clusters) to perform NMT domain adaptation  (Tars and Fishel, 2018) . Recent work  (Currey et al., 2020)  employs large pretrained language models as part of the framework. It consists of several steps. In step 1.1, we begin with a single heterogeneous dataset ("Original Dataset") and train a baseline NMT model on it. At the same time (step 1.2), we pass this dataset through the external pre-trained XLM-R model to extract hidden sentence/document representations for the whole dataset. In step 2, we use the extracted sentence/document representations to train a k-means clustering model. In step 3, we use this k-means model to separate the original dataset into subdatasets corresponding to the clusters. Lastly, we use the cluster-specific datasets to fine-tune the baseline NMT model from step 1.1 on each dataset separately, resulting in a set of specialized models. We use the k-means model at runtime to determine which NMT model to use to translate a new sentence/document. If we only use sentence clusters, the approach is equivalent to the one proposed by  Currey et al. (2020) . Refer to Figure  6  from Appendix A for the illustration of the steps described above. 

 Improved Framework (Ours) In this subsection we describe our modification to the existing automatic domains pipeline presented in Section 4.1. We propose reusing an NMT baseline to produce sentence representations for the clustering step instead of using an external encoder. Specifically, in step 1, we train a baseline NMT model just like in the existing framework. However, we found we can omit using the XLM-R model (step 1.2). Instead, to extract sentence/document representations for step 2, we reuse the trained NMT baseline. The rest of the pipeline remains the same. Figure  7 (Appendix  A ) illustrates the updated framework. Moreover, to produce clusters in both frameworks, we additionally study text representations on the level of documents. 

 Experiments In this section we perform an extensive experimental study comparing performance of the existing automatic domains framework (Section 4.1) with our proposed version (Section 4.2). We experiment with both sentence-level and document-level representations as a basis for k-means algorithm on three language pairs and two data scenarios. We first train baseline Transformer NMT models on concatenated data from all domains (same baseline as in Section 3) and then cluster the training, development, and test data using either this same baseline or XLM-R. Next, we fine-tune 3 our baseline models to the different obtained data partitions (clusters) and compare the translation quality of resulting fine-tuned (adapted) models. 

 Setup We explore two data scenarios. First, we perform experiments on a mixture of distinct corpora. For these experiments, we reuse the data and concat baseline NMT model (Transformer-base) described in Section 3  (EN-ET and DE-EN) . In this setting, we can compare the performance of models fine-tuned to automatically discovered domains to that of oracle models (fine-tuned using known domains/datasets). We also randomly partition the data (into equal parts) and fine-tune the baseline models to them to get our lower bound estimates. Second, we explore a scenario with a single corpus, which is highly heterogenous, and thus may contain multiple domains which are unknown. In this setting, we use the ParaCrawl  (Espl? et al., 2019)  parallel corpus 4 , which consists of diverse documents crawled from the web. We use three language pairs: English?Estonian (EN-ET), German?English (DE-EN), and English?Czech (EN-CS). We use ?3M sentence pairs for all languages for training, and ?3,000 sentence-pairs for development and testing. The exact experimental setup with data sizes, training and preprocessing details can be found in in Appendix B. For our concat baselines we follow the setup from Section 3.1 (described in more detail in Appendix B). In the mixture of corpora experiments, baseline fine-tuning is performed for 50 epochs, and in the single heterogenous corpus experiments for 25 epochs (fine-tuning hyperparameters can be found in Appendix B). For comparison, we also continue training the baseline models for longer as suggested by Gururangan et al. (  2020 ) (concatcont). We continue training for the same number of epochs fine-tuning is done for in the corresponding experiment. For each of the models, we evaluate the checkpoint which shows the highest BLEU score on the particular model's development set, and translate the test sets with beam size set to 5. We use the BLEU score  (Papineni et al., 2002) , specifically, the sacreBLEU implementation  (Post, 2018)  to assess the models' translation performance. To test for statistical significance, we use paired bootstrap resampling  (Koehn, 2004) . 

 Labelled Domain Mix Experiments In this section we consider a scenario which can be practically interesting in cases where the data consists of several distinct domains with the labels unavailable or corrupted as in  Currey et al. (2020) . Moreover, it serves as an oracle experiment showing how well automatic domains perform compared to the golden labels. This way we have a better idea what to expect when applying them to unlabeled data as in Section 5.3. Table  2  shows the results for DE?EN. We see that, for all corpora except Europarl, at least one model of the two that are based on document-level clustering always manages to surpass the oracle performance obtained by fine-tuning to known domains, and on Europarl the document-level models perform comparably to oracle. In most cases, document-level models show significantly better translation quality than XLM-R sentence-level models, which have been used in previous work, while NMT sentence-level models closely match the performance of XLM-R sentence ones. When scores are averaged over all four domains, document clustering obtained from the NMT encoder is the overall winner. Table  3  shows results for the EN?ET language pair. While fine-tuning on oracle domains yields an average improvement of 0.8 BLEU points over the baseline, fine-tuning on unsupervised document clusters obtained from the NMT encoder allows us to match that performance. However, for the EMEA test set XLM-R sentence clusters turn out to be the most successful approach, showing significantly higher BLEU scores than all other automatic partitions and outperforming the oracle by 1.2 BLEU points, while document-level NMT clustering also manages to surpass the oracle performance, albeit slightly. For OpenSubtitles and JRC-Acquis, oracle shows the highest overall scores, with document-level NMT clustering a close second, outperforming XLM-R sentence clustering by a noticeable margin. For OpenSubtitles, however, none of the automatic domain approaches manage to improve the baseline performance (and neither does continued training of the baseline), and even the oracle partition does not manage to do so by a statistically significant degree. For Europarl, all automatic domain approaches yield comparable BLEU scores, with none being significantly better or worse than XLM-R sentence clusters. Document-level XLM-R automatic domains have a low average score due to underperforming on the EMEA test set. We see from Figure  3   test set is mostly translated by the model fine-tuned on cluster 1, whose training set predominantly consists of Europarl data. Cluster 0, which sees the most EMEA examples during fine-tuning, is not used to translate the test set at all, as we see from Figure  3 . 

 Heterogeneous Corpus Experiments In this subsection we present results for our method applied to the Paracrawl dataset, which constitutes a heterogeneous corpus of data crawled from the web with no training-time domain information known.  

 EN-ET 

 EN-CS & DE-EN As separating the data into 8 clusters yields the highest BLEU score among all fine-tuning scenarios for EN?ET, we choose this number of clusters for experiments on other language pairs. Table  5  shows the BLEU scores for EN?ET, EN?CS, and DE?EN models finetuned to automatic domains. For EN-CS, only the NMT sentence-level clustering manages to outperform the baseline, noticeably surpassing all other automatic domain extraction methods as well. For DE-EN, none of the approaches outperform the baseline model by a considerable margin. Sentence-level clustering based on XLM-R performs comparably to the baseline. Document-level NMT clustering shows a slightly lower score, but the difference is not statistically significant. At the same time, document XLM-R and sentence NMT perform worse than sentence XLM-R.  

 Additional Exploration While automatic domains demonstrate reasonable performance for EN-ET and EN-CS language pairs, DE-EN does not seem to benefit from either XLM-R or NMT-based clustering. In this section we perform additional experiments with DE-EN data to see whether there are conditions under which automatic domains could be beneficial in this case. Data Size and Number of Clusters First, we increase the training data size and vary the number of clusters. Specifically, we use 10M parallel sentence pairs for training instead of 3M, and partition the dataset into 4 and 12 clusters instead of 8. The resulting BLEU scores for DE-EN are shown in Table  6 . We do not observe any significant improvement over the concat-cont baseline for any of the methods. With the data separated into 12 clusters, sentence-level NMT clustering significantly outperforms sentence-level XLM-R, but still does not beat continued training of the baseline. 

 Model Size It is also possible that NMT needs different model capacity for handling different language pairs, so we experiment with decreasing the model size. We use the same number of layers, but decrease the width of the model (4 attention heads, embeddings of size 160, dimension of the feed-forward layer 320) so that the total number of parameters decreases five-fold. We compute NMT clusters based on the new, smaller baseline model. Our motivation for this is to understand whether automatic domains are not useful for DE-EN ParaCrawl at all, or could aid a weaker baseline. The results are shown in Table  7 . The smaller baseline does benefit from adaptation to automatic domains (clusters). While NMT clusters are generated by a model which is 5 times as small, XLM-R and NMT show equivalent performance. 

 Discussion Our analysis is implicit inductive evidence for the high degrees of domain-specific information in sentence and document NMT representations. However, it is still open to what kind of information is preserved (topical/stylistic/lexical). For example, our approach could result in clusters by domain/dataset due to standard lexical statistics and not sentence semantics. However, on the practical side, we show that adapting NMT to these types of clusters is just as good or better as to other possible types of clusters since it benefits the baseline performance. Moreover, previous work that uses pre-trained language models to obtain the clusters is likely to suffer from the same issue. Moreover, while XLM-R is a general-purpose encoder, NMT models are only that helpful for domains we train them on. However, the data constitutes all domains of interest by definition for a multi-domain NMT (the task we tackle). Thus, NMT models are a perfect fit that simplifies and outperforms an existing approach.  

 Conclusion In this work, we made a two-fold contribution. The first is to the field of NMT interpretation and analysis. We have shown that a baseline Transformer NMT encoder preserves enough domain-specific information distinguish between oracle domains in a mixed corpus without supervision. We showed an evolution of this property across the Transformer layer using PCA and k-means clustering on the level of sentences and documents. Comparison to XLM-R based clusters demonstrated that both sentence-level and document-level NMT clusters show higher cluster purity (similarity to original text domains). Next, we utilized our analysis insights to improve an existing practical cluster-based multidomain NMT approach  (Tars and Fishel, 2018; Currey et al., 2020) . In a setting with preset domains (i.e., available corpus/domain labels), tuning to NMT clusters on average matches or surpasses XLM-R clusters. Additionally, NMT cluster-based tuning mostly matches the translation quality when tuning to original corpus labels, with some exceptions that we also analyze and explain. Finally, in the case of a heterogeneous corpus (ParaCrawl), the performance of fine-tuned NMT models depends on the number of clusters, language pairs, and other parameters. We see significant improvement for EN-ET and EN-CS translation when comparing XLM-R and NMT-based clusters (on both sentence and document levels). For DE-EN, the domain tuning results depend on the NMT model's capacity for learning each language pair's translation.   A.2 Language Model XLM-R Base Our model of choice from the family of BERT-like models is the Base version of the XLM-R  (Conneau et al., 2020) . It is single multilingual model covering about 100 languages, which is very useful when dealing with machine translation systems, where for different language pairs we may not have a separate monolingual BERT for each source language. We choose XLM-R as opposed to the multilingual BERT  (Devlin et al., 2019)  since it is a more recent and better performing  (Hu et al., 2020)  model. We choose the Base version because it is most compatible to our NMT baseline in terms of capacity. 
