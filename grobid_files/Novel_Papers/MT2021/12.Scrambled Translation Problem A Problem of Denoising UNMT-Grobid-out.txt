title
Scrambled Translation Problem: A Problem of Denoising UNMT

abstract
In this paper, we identify an interesting kind of error in the output of Unsupervised Neural Machine Translation (UNMT) systems like Undreamt 1 . We refer to this error type as Scrambled Translation problem. We observe that UNMT models which use word shuffle noise (as in case of Undreamt) can generate correct words, but fail to stitch them together to form phrases. As a result, words of the translated sentence look scrambled, resulting in decreased BLEU. We hypothesise that the reason behind scrambled translation problem is 'shuffling noise' which is introduced in every input sentence as a denoising strategy. To test our hypothesis, we experiment by retraining UNMT models with a simple retraining strategy. We stop the training of the Denoising UNMT model after a pre-decided number of iterations and resume the training for the remaining iterations-which number is also pre-decided-using original sentence as input without adding any noise. Our proposed solution achieves significant performance improvement UNMT models that train conventionally. We demonstrate these performance gains on four language pairs, viz., English-French, English-German, English-Spanish, Hindi-Punjabi. Our qualitative and quantitative analysis shows that the retraining strategy helps achieve better alignment as observed by attention heatmap and better phrasal translation, leading to statistically significant improvement in BLEU scores.

Introduction Training a machine translation system using only the monolingual corpora of the two languages was successfully demonstrated by  (Artetxe et al., 2018c; Lample et al., 2018) . They train the machine translation system using denoising auto-encoder (DAE) and backtranslation (BT) iteratively. Recently, pre-training of large language models  (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020)  using monolingual corpus is used to initialize the weights of the encoder-decoder models. These encoder-decoder models are later fine-tuned using backtranslated sentences for the task of Unsupervised Neural Machine Translation (UNMT). While we appreciate language model (LM) pre-training to better initialise the models, it is important to understand the shortcomings of earlier approaches. In this paper, we explore in this direction. We observe that the translation quality of undreamt models  (Artetxe et al., 2018c)  suffers partially due to wrong positioning of the target words in the translated sentence. For many instances, though the reference sentence and its corresponding generated sentence are formed with almost the same set of words, the sequence of words is different resulting in the sentence being ungrammatical and/or loss of meaning. This results in a difference in syntax and semantic rules. We define such generated sentences as scrambled sentences and the problem as scramble translation problem. Scrambled sentences can be either disfluent or fluent-but-inadequate. Here, if the LM decoder is not learnt well, we observe disfluent translations. If the LM decoder is learnt well, we observe fluent-but-inadequate translations. An example of fluent-but-inadequate translation will be 'leaving better kids for our planet' instead of 'leaving better planet for our kids'. Due to this phenomenon, during BLEU computation n-gram matching lessens, for n > 1. However, this error is absent in translation generated from recent state-of-the-art systems  (Conneau and Lample, 2019; Song et al., 2019; Liu et al., 2020) . We hypothesise, DAE introduces uncertainty to the previous UNMT  (Lample et al., 2018; Artetxe et al., 2018c Artetxe et al., , 2019 Wu et al., 2019)  models, specifically to the encoders. It has been observed that encoders are sensitive to the exact ordering of the input sequence  (Michel and Neubig, 2018; Murthy V et al., 2019; Ahmad et al., 2019) . By performing random word-shuffle in all the source sentences, encoder may lose important information about the sentence composition. The DAE fails to learn informative representation which affects the decoder resulting in wrong translations generated. If our hypothesis is true, retraining these previous UNMT system models with noise-free sentences as input should resolve the problem for previous systems  (Artetxe et al., 2018c; Lample et al., 2018) . Moreover, using this retraining strategy will not benefit recent approaches  (Conneau and Lample, 2019; Song et al., 2019)  as they do not shuffle words of input sentence while training with back-translated data. In this paper, we prove our hypothesis by showing that a simple retraining strategy mitigates the 'scrambled translation problem'. We observe consistent improvements in BLEU score and word-alignment over the denoising UNMT approach by  Artetxe et al. (2018c)  for four language pairs. We do not wish to beat the state-of-the-art UNMT systems with pre-training, instead, we demonstrate a limitation of previous denoising UNMT  (Artetxe et al., 2018c; Lample et al., 2018)  systems and prove why it happens. 

 Related Work Neural machine translation (NMT)  (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015)  typically needs lot of parallel data to be trained on. However, parallel data is expensive and rare for many language-pairs. To solve this problem, unsupervised approaches to train machine translation  (Artetxe et al., 2018c; Lample et al., 2018; Yang et al., 2018)  was proposed in the literature which uses only monolingual data to train a translation system.  Artetxe et al. (2018b)  and  Lample et al. (2018)  introduced denoising-based U-NMT which utilizes cross-lingual embeddings and trains a RNN-based encoder-decoder model  (Bahdanau et al., 2015) . Architecture proposed by  Artetxe et al. (2018c)  contains a shared encoder and two language-specific decoders while architecture proposed by  Lample et al. (2018)  contains a shared encoder and a shared decoder. In the approach by  Lample et al. (2018) , the training starts with word-by-word translation followed by denoising and backtranslation. Here, noise in the input sentences in the form of shuffling of words and deletion of random words from sentences was performed. Conneau and Lample (2019) (XLM) proposed a two-stage approach for training a UNMT system. The pre-training phase involves training of the model on the combined monolingual corpora of the two languages using Masked Language Modelling (MLM) objective  (Devlin et al., 2019) . The pre-trained model is later fine-tuned using denoising auto-encoding objective and backtranslated sentences.  Song et al. (2019)  proposed a sequence to sequence pre-training strategy. Unlike XLM, the pre-training is performed via MAsked Sequence to Sequence (MASS) objective. Here, random ngrams in the input is masked and the decoder is trained to generate the missing ngrams in the pre-training phase. The pre-trained model is later fine-tuned using backtranslated sentences.  demonstrated that LSTM encoders of the NMT system are sensitive to the word-ordering of the source language. They considered the scenario of zero-shot translation from language l 3 to l 2 . They train a NMT system for l 1 ? l 2 languages and use l 1 -l 3 languages bilingual embeddings. This enables the trained model to perform zero-shot translation from l 3 ? l 2 . However, if the word-order of the languages l 1 and l 3 are different, the translation quality from l 1 -l 3 is hampered.  Michel and Neubig (2018)  have also made a similar observation albeit in the monolingual setting. They observe that accuracy of the machine translation system gets adversely affected due to noise in the input sentences. They discuss various sources of noise with one of them being word emission/insertion/repetition or grammatical errors. The lack of robustness to such errors could be attributed to the sequential processing of LSTM or Transformer encoders. As the encoder processes the input as a sequence and generates encoder representation at each time-step, such errors would lead to bad encoder representations resulting in bad translations generated. Similar observations have also been made by  Ahmad et al. (2019)  for cross-lingual transfer of dependency parsing. They observe that self-attention encoder with relative position representations is more robust to word-order divergence and enable better cross-lingual transfer for dependency parsing task compared to RNN encoders. Here, shuffling is performed by swapping neighboring words l/2 times, where l is the number of words in the sentence. 

 Baseline Approach For completeness, we also experimented with XLM UNMT  (Conneau and Lample, 2019 ) with initialise the model with MLM objective followed by finetuning it with DAE and BT iteratively. In this approach, they do not add noise with the input sentence while training with backtranslated data. 

 Proposed Retraining Strategy Our proposed strategy to train a denoising-based UNMT system consists of two phases. In the first phase, we proceed with training using denoised sentences similar to the baseline system  (Artetxe et al., 2018c)  for M number of iterations. Adding random shuffling in the input side, however, could introduce uncertainty to the model leading to inconsistent encoder representations. To overcome this, in the second phase, we retrain the model with simple AE and on-the-fly BT using sentences with the correct ordering of words for (N-M) iterations as shown in Fig.  2 . Here, N is the total number of iterations and M < N . More concretely, this training approach consists of 4 more sub-processes other than the 4 subprocesses of the baseline system. These are: (v) AE src : Auto-encoding of source sentences in which we train shared-encoder, source-decoder, and attention. (vi) AE trg : Auto-encoding of target sentences in which we train shared-encoder, target-decoder, and attention. (vii) BT src : Training shared-encoder, target-decoder, and attention with back-translated source sentences as input and actual target sentences as output. (viii) BT trg : Training shared-encoder, source-decoder, and attention with back-translated target sentences as input and actual source sentences as output. The second phase ensures that the encoder learns to generate context representation with information about the correct ordering of words. For XLM  (Conneau and Lample, 2019) , we add these 4 subprocesses only with fine-tuning step. We do not change anything in LM pretraining step. 

 Experimental Setup We test our hypothesis with undreamt as a previous approach and XLM as a SOTA approach. We applied our retraining strategy on both the approaches and observed the result. For undreamt, we have used monolingual data of six languages, i.e. English (en), French (fr), German (de), Spanish (es), Hindi (hi), and Punjabi (pa). Among these languages, Hindi and Punjabi are of SOV word-order where the other four languages are of SVO word order. In our experiments, we choose language-pairs such that the word-order of source language matches with that of target language. We have used the NewsCrawl corpora for en, fr, de of WMT14, and for es of WMT13. For hi-pa, we use Wikipedia dumps of the august 2019 snapshot for training. The en-fr and en-de models are tested using WMT14 test-data and en-es models using WMT13 test-data, and hi-pa models using ILCI test data  (Jha, 2010) . We have preprocessed the corpus for normalization, tokenization and lowercasing using the scripts available in Moses  (Koehn et al., 2007)  and Indic NLP Library  (Kunchukuttan, 2020) , for BPE segmentation using subword-NMT  (Sennrich et al., 2016)  with number of merge operations set to 50k. We use the monolingual corpora to independently train the embeddings for each language using skip-gram model of word2vec  (Mikolov et al., 2013) . To map embeddings of two languages to a shared space, we use Vecmap 2 by  Artetxe et al. (2018a) . We use undreamt 3 tool to train the UNMT system proposed by  Artetxe et al. (2018c) . We train the baseline model untill convergence and noted the number of steps N required to reach convergence. We now train our proposed system for N/2 steps and re-train the model after removing denoising noise for the remaining N/2 steps. They converge between 500k to 600k steps depending on the language pairs. Further details of dataset and network parameters are available in Appendix. We also report results on XLM 4 approach  (Conneau and Lample, 2019) . XLM employs two-stage training of UNMT model. The pre-training stage trains encoder and decoder with masked language modeling objective. The retraining stage employs denoising along with iterative back-translation. However, XLM uses a different denoising (word shuffle) mechanism compared to  Artetxe et al. (2018c) . We replace the denoising mechanism by  Conneau and Lample (2019)  with the denoising mechanism used by  Artetxe et al. (2018c) . We use the pre-trained models for English-French, English-German, and English-Romanian provided by  Conneau and Lample (2019) . We retrain the XLM model until convergence using the denoising approach which makes the baseline system. We later retrain the pre-trained XLM model using our proposed approach where we remove the denoising component after N/2 steps. We report both BLEU scores and n-gram BLEU scores using multi-bleu.perl of Moses. We have tested statistical significance of BLEU improvements  (Koehn, 2004) . To analyse the systems, we have produced heatmaps of attention generated by the models.  Table  1 : The Translation performance using the Baseline approach and our Approach. Trained for a total of N iterations for all approaches. Undreamt and XLM results are results from our replication using the code provided by the authors. ? indicates statistically significant improvements using paired bootstrap re-sampling  (Koehn, 2004)  for a p-value less than 0.05 . 

 Language (a) English ? French (b) French ? English Figure  3 : Change in translation accuracy using undreamt-baseline vs. our approach with increasing number of iterations for English-French (BLEU scores reported). 

 Results and Analysis Table  1  reports BLEU score of the trained models using the undreamt  (Artetxe et al., 2018c)  and XLM  (Conneau and Lample, 2019)  and retraining them with our approach. Undreamt and XLM results are results from our replication using the code provided by the authors. In Table  1a  we observe that the proposed re-training strategy of AE used in conjunction with BT results in statistically significant improvements (p-value < 0.05) across all language pairs when compared to the undreamt baseline approach  (Artetxe et al., 2018c) . We report results on XLM  (Conneau and Lample, 2019)  with our retraining approach in Table  1b . XLM is one of the state-of-the-art (SOTA) UNMT approaches for these language pairs. The approach by XLM  (Conneau and Lample, 2019)  does not add noise to the input backtranslated sentence during training. Therefore, our retraining strategy does not benefit here.  (a) English ? Spanish (b) Spanish ? English Figure  5 : Change in translation accuracy using undreamt-baseline vs. our approach with increasing number of iterations for English-Spanish (BLEU scores reported). Table  2 : Improvements in n-BLEU (represented in %) on using our approach over baseline for en-fr, en-de, en-es, hi-pa test sets. Figure  6 : Change in translation accuracy using undreamt-baseline vs. our approach with increasing number of iterations for Hindi-Punjabi (BLEU scores reported). Language ? BLEU-1 ? BLEU-2 ? BLEU-3 ? BLEU- German der us-senat genehmigte letztes jahr ein 90 millionen dollar teures pilotprojekt , das 10.000 autos umfasst h?tte . English reference the u . s . senate approved a $ 90 -million pilot project last year that would have involved about 10,000 cars . Artetxe et al. 2018 the u . s . district of the last $ 90 million a year , it would have 10,000 cars . Our approach the u . s . district last year approved 90 million initiative that would have included 10,000 cars .   Spanish el anuncio del probable descubrimiento del bos?n de higgs gener? una gran conmoci?n el verano pasado , y con raz?n . English reference the announcement of the probable discovery of the higgs boson created quite a stir last summer , and with good reason . 

 Artetxe et al. 2018 the likely announcement of the discovery of the higgs boson triggered a major shock last summer , and with reason . Our approach the announcement of the likely discovery of the higgs boson generated a major shock last summer , and with reason . We also observe robustness of the pre-trained language models to the scrambled translation problem. In India, China and other countries, other people work from fifteen to one. Our approach en inde , en chine et de nombreux autres pays , les gens travaillent quinze ? douze heures un jour . (Google translation) In India, China and many other countries, people work fifteen to twelve hours a day . increasing number of iterations on test-data. We observe that our proposed approach leads to increase in BLEU score in the re-training phase as the denoising strategy is removed. The baseline system suffers from drop in BLEU score due to denoising strategy introducing ambiguity into the model. 

 Quantitative analysis We hypothesize that the baseline UNMT model using DAE is able to generate correct word translation but fails to stitch them together to generate phrases. To validate the hypothesis, we calculate the percentage improvement on using our approach over the baseline system in terms of individual n-gram (n=1,2,3,4) specific BLEU scores for each language-pair and a particular value of n. The results presented in Table  2  indicate that our method achieves higher improvements in n-gram BLEU for higher n?grams (n > 1) compared to the improvement in n-gram BLEU for lower values of n, indicating better phrasal matching. This could be attributed to the proposed approach not suffering from the scrambled translation problem introduced by the DAE. 

 Qualitative analysis We observe several instances where our proposed approach results in better translations compared to the baseline. On manual analysis of translation outputs generated by the baseline system, we have found out some instances of scrambled translation problem. Due to uncertainty introduced by shuffling of words before training, the baseline model chooses to generate sentences that are more acceptable by a language model. Fig  7  shows such an example in our test data. Here, two German phrases 'ein 90 millionen' ('a 90 million') and 'letztes jahr' ('last year') are mixed up and translated as 'last $ 90 million a year' in English. However, our approach handled the issue correctly. Fig  8  shows an example of a situation where the baseline model prefers to generate a word in multiple probable positions. Here, the source Punjabi sentence consists of a phrase 'jAM phira' ('or') meaning 'yA phira'('or') in Hindi. In the translation produced by the baseline model, the correct phrase is generated along with the word 'phira' wrongly occurring again forming another phrase 'phira se' ('again'). Note that, both the phrases are commonly used in Hindi. In Fig  9 , the model trained on baseline system produced the word 'likely', which is a synonym of 'probably', in the wrong position. In Fig  10 , the model trained on baseline system produced the word 'autres'('other') in the multiple positions. Attention Analysis: Attention distributions generated by our proposed systems have lesser confusion when compared with the attention distribution generated by baseline systems, as shown in Heatmaps of Fig.  11 . Production of word-aligned attention distribution was easy for the attention models, which we retrained on sentences without noise. 

 Conclusion and Future work In this paper, we addressed 'scrambled translation problem', a shortcoming of previous denoisingbased UNMT approaches like UndreaMT approach  (Artetxe et al., 2018c; Lample et al., 2018) . We demonstrated that adding shuffling noise to all input sentences is the reason behind it. Our simple retraining strategy, i.e. retraining the trained models by removing the denoising component from auto-encoder objective (AE), results in significant improvements in BLEU scores for four language pairs. We observe larger improvements in n-gram specific BLEU scores for higher value of n indicating better phrasal translations. We also observe robustness of the pre-trained language models to the scrambled translation problem. We would also like to explore applicability of our approach in other ordering-sensitive DAE-based tasks. Figure 1 : 1 Figure 1: Our baseline training procedure: Undreamt. DAE src : Denoising of source sentences; DAE trg : Denoising of target sentences; BT S src : Training with shuffled back-translated source sentences; BT S trg : Training with shuffled back-translated target sentences. 
