title
ISTIC's Triangular Machine Translation System for WMT' 2021

abstract
This paper describes the ISTIC's submission to the Triangular Machine Translation Task of Russian-to-Chinese machine translation for WMT' 2021. In order to fully utilize the provided corpora and promote the translation performance from Russian to Chinese, the pivot method is used in our system which pipelines the Russian-to-English translator and the English-to-Chinese translator to form a Russian-to-Chinese translator. Our system is based on the Transformer architecture and several effective strategies are adopted to improve the quality of translation, including corpus filtering, data pre-processing, system combination, model averaging, model ensemble and reranking.

Introduction The Institute of Scientific and Technical Information of China (ISTIC) participated in the Triangular Machine Translation Task of Russian-to-Chinese in the Sixth Conference on Machine Translation 1 (WMT' 2021). This paper demonstrates the overall framework of the ISTIC's submission and its technical details. In this evaluation, we adopted the neural machine translation architecture of Google Transformer  (Vaswani et al., 2017)  as a part of our system. We use the three parallel corpora released by the evaluation organizer and adopted a two-stage method for data pre-processing. Several filtering methods of the corpus are explored to reduce the data noise and improve the data quality. As for model construction, we use the pivot method to get a Russian-to-Chinese translator by bridging the trained Russian-to-English translator and Englishto-Chinese translator. Model averaging  (Claeskens and Hjort, 2008) , model ensemble  (Lutellier et al., 2020)  and reranking  strategies are adopted to generate the final output translation. We removed spaces between words and restored the target language translation results to the prescribed file format in data post-processing. In our experiment, the performance of the system under different settings was compared and further analyzed the experimental results. The structure of this paper is as follows: the second part introduces the technical architecture of our machine translation system; the third part describes the data pre-processing, parameter settings, experimental results, and related analysis; the fourth part gives the conclusion and future work. 

 System Overview The overall framework of the ISTIC's triangular machine translation system is shown in Figure  1 . 

 Single Transformer System Our baseline single system used in participated evaluation tasks is the Transformer based encoderdecoder architecture. Transformer is completely based on a self-attention mechanism. It can achieve algorithm parallelism, speed up model training, further alleviate long-distance dependence and improve translation quality  (Zhang and Zong, 2020) . The encoder and the decoder are formed by stacking N identical layer blocks, where N is set to 6. 

 Context-based Combination System As shown in Figure  2 , based on the Transformer model, our team adopts a context-based  (Voita et al., 2018)  system combination method, which is an encoder-decoder structure composed of n identical network layers, where n is set to 6. Two different methods of system combination are designed according to the fusion in different positions, which are Encoder Combination method and Decoder Combination method. Both of them adopt multiencoder  to encode the source sen- tences and the context information from machine translation results of the source sentence. In the Encoder Combination method, the hidden layer information of context (multi-system translation) is transformed into new representation through attention network, and merges the hidden layer information of source sentence through gating mechanism at encoder end; In Decoder Combination method, the hidden layer information of multi-system translation and the hidden layer information of source sentence is calculated at the decoder to obtain the fusion vector. The attention calculation method is the same as the original transformer model, to obtain a higher quality fusion translation. The Encoder Combination model (see Figure  3 ) uses multiple system translations, and then converts the system translations into new representations through the attention network, integrating the hidden layer information of homologous language sentences for attention fusion through the gating mechanism in the Encoder. In the Encoder Combination mode and the Self-Attention of the multi-system translation Encoder, Q, K, and V are all from the upper layer output of the multi-system translation Encoder; in the Self-Attention of the source language Encoder, Q, K, and V are all from the upper layer output of the source language En- (1) H = M utiHead(H Tr , H s ) (2) The Decoder Combination model (see Figure  4 ) combines the hidden layer information of multiple encoders with attention in the decoder. The Decoder can process multiple encoders separately, and then fuse them using the gating mechanism inside the Decoder to obtain the combined vector. In the Decoder Combination mode and the Self-Attention of the target language Decoder, Q, K, and V are all from the output of the previous layer of the target language Decoder; in the Translation Attention of the target language Decoder, Q comes from the (3) 

 Direct Method In the direct method (see Figure  5 ), we use the preprocessed Russian/Chinese parallel corpus to train a direct Russian-to-Chinese translator by means of the single Transformer System or the context-based Combination System, depending on which kind of system performs best. 

 Pivot Method In the pivot method (see Figure  5 )  (Park and Zhao, 2019) , firstly, we use the pre-processed Russian/English parallel corpus to train a Russianto-English translator; secondly, we use the preprocessed English/Chinese parallel corpus to train an English-to-Chinese translator; finally, we pipeline them to form a pivot Russian-to-Chinese translator. All translators can be trained by means of the single Transformer System or the contextbased Combination System. By comparing the experimental results, the system with optimal performance is accepted for Russian-to-Chinese translation. 3 Experiments 

 Data Pre-processing The evaluation organizers provide three parallel corpora: the Chinese/Russian corpus is crawled from the web and aligned at the segment level, and combined with different public resources; the Chinese/English corpus combines several public resources; the Russian/English corpus gathers multiple public resources. A two-stage method  (Wei et al., 2020)  is used for data pre-processing, consist of a general pre-processing stage and a specific pre-processing stage. The general pre-processing stage includes conversion from traditional Chinese to simplified Chinese by the hanziconv 2 package , conversion between full angle and half-angle, special character filtering, same content filtering, sentence length filtering, and sentence length ratio filtering. Among them, sentence length of the Chinese language is calculated in the unit of "character" and sentence length of non-Chinese language is calculated in the unit of "token". Sentence length filtering removes sentence pairs which source sentence length or target sentence length exceeds the range of  [5, 200] . Sentence length ratio filtering excludes the sentence pairs whose ratio of source sentence length and target sentence length exceeds the range of [0.2, 20]. In the specific pre-processing stage, the word segmentation of English and Chinese sentences is implemented using the lexical tool Urheen 3 and the word segmentation of Russian sentences is implemented using the lexical tool Natasha 4 . The scales of sentence pairs of all corpora before and after data pre-processing are shown in Table  1 . After data preprocessing, we split the corpora into training set, development set and test set. The scales of the data partition are shown in Table  2 . 

 System Settings The open-source project fairseq 5  is chosen for this evaluation system. The main parameters are set as follows. Each model uses 1-3 GPUs for training, and the batch size is 2048. The embedding size and hidden size are set to 1024, the dimension of the feed-forward layer is 4096. We use six self-attention layers for both encoder and decoder, and the multi-head self-attention mechanism has 16 heads. The dropout mechanism  (Provilkov et al., 2020 ) was adopted, and dropout probabilities are set to 0.3. BPE  (Sennrich et al., 2016)  is used in all experiments, where the merge operations is set to 32000. The maximum number of tokens is set to 4096. The loss function is set to "label_smoothed_cross_entropy". The parameter "adam_betas" is set to (0.9, 0.997). For the baseline system, the initial learning rate is 0.0007, the warm-up steps are set to 4000, and the maximum epoch number is set to 15. For the Encoder Combination system and Decoder Combination system 6 , the initial learning rate is 0.0001, the warm-up steps are set to 4000, and the maximum epoch number is set to 10. 

 Experimental results In the training of Russian-to-English translator, English-to-Chinese translator and Russian-to-Chinese translator, the single Transformer systems Our primary submission uses the pivot method, which use English translation as the bridge. The Russian sentences are translated into English intermediate results by the well-trained Russian-to-English translator and then the English intermediate results are translated into Chinese output by the well-trained English-to-Chinese translator. Our contrast submission uses the direct method, which uses the well-trained Russian-to-Chinese translator to generate the target output. As a result, our primary submission achieves a BLEU score of 19.2 and ranked the fourth among all participating teams. Our contrast submission achieves a BLEU score of 18.1 (shown in Table  4 ). 

 Conclusions This paper introduces the main technologies and methods of ISTIC's submission in WMT 2021. To sum up, our model is constructed on the Transformer architecture of self-attention mechanism and context-based system combination method. In the aspect of data pre-processing, we explore several corpus filtering methods. In the process of translation output, the strategies of model ensemble and reranking are adopted. Experimental results show that these methods can effectively improve the quality of translation. It is worth mentioning that the pivot language translation bridge method outperforms the direct translation method. Figure Figure 1: Overall framework 

 Figure 2 : 2 Figure 2: Context-based combination system 

 Figure 4 : 4 Figure 4: Decoder combination model 

 Figure 5 : 5 Figure 5: Direct and pivot method 

 Table 1 : 1 Data pre-processing results Direction Before Pre-processing After Pre-processing Russian-English 69217438 42939395 English-Chinese 28579587 22233706 Russian-Chinese 33422682 21892537 Direction Train Set Dev Set Test Set Russian-English 42935974 2000 1421 English-Chinese 22231506 1100 1100 Russian-Chinese 21891537 965 1000 

 Table 2 : 2 Data partition results 

 Table 3 : 3 BLEU results on self-built test set System Russian-English English-Chinese Russian-Chinese Transformer 20.89 17.83 16.07 Transformer+ Encoder 21.53 18.87 16.66 Combination Transformer+ Decoder 21.76 18.91 16.79 Combination Method BLEU Primary: Pivot Method 19.2 Contrast: Direct Method 18.1 

 Table 4 : 4 BLEU results on released test set are trained for 15 epochs. The context-based combination systems with Encoder Combination model or Decoder Combination model are trained for 10 epochs. The best epoch model and the last epoch model are ensembled to generate better results. The BLEU (Papineni et al., 2002)  scoring results on the self-built test set are shown in Table3.The context-based combination systems with Decoder Combination model are used as our final submission since they outperform other systems. 

			 https://github.com/berniey/hanziconv 3 https://www.nlpr.ia.ac.cn/cip/ software.html 

			 https://github.com/natasha/natasha 5 https://github.com/pytorch/fairseq/ tree/v0.6.2 6 https://github.com/libeineu/ Context-Aware
