title
Data and Parameter Scaling Laws for Neural Machine Translation

abstract
We observe that the development crossentropy loss of supervised neural machine translation models scales like a power law with the amount of training data and the number of non-embedding parameters in the model. We discuss some practical implications of these results, such as predicting BLEU achieved by large scale models and predicting the ROI of labeling data in low-resource language pairs.

Introduction As training neural networks becomes an organizational and multi-million dollar venture , it is imperative to quantifiably predict the benefits of scaling up neural networks. In this paradigm, machine learning is an engineering effort, in which money can buy resources (data, compute) and the main concern is to predict returnon-investment (ROI) while avoiding bottlenecks. Recent work has observed that the cross entropy loss of neural language models and other autoregressive generative models scales like a power law in the amount of training data, compute, and number of model parameters over several orders of magnitude  (Hestness et al., 2019; . Similar intuitions exist in the realm of supervised MT: doubling the amount of parallel training data leads to roughly a fixed improvement in BLEU in both phrase-based statistical MT  (Irvine and Callison-Burch, 2013; Turchi et al., 2008)  and neural MT  (Koehn and Knowles, 2017; Sennrich and Zhang, 2019) . In Section 2, we show that these MT intuitions can be quantified and explained via cross-entropy power law scaling; using a handful of experiments on small subsets of MT datasets, we precisely predict the performance of large systems trained on orders of magnitude more data. In Section 3, we demonstrate how these trends might be utilized to make ROI predictions when annotating more data for low-resource language pairs. 

 Machine Translation Scaling Laws To investigate the predictability of MT system performance as parameters/data increase, we train many Transformers of various sizes (Table  2 ) on randomly selected subsets of data ( 1 2 , 1 4 , 1 8 , ...) for several standard MT datasets. The smallest data subsets contain ?0.1% of the total data available. We use three language pairs in our experiments:   German-English (de-en), Russian-English (ru-en), and Chinese-English (zh-en). The data for each language pair is a concatenation of WMT 2017 data (which includes news commentary, parliamentary proceedings, and web-crawled data) and Open-Subtitles2018  (Lison and Tiedemann, 2016; Tiedemann, 2016) . Datasets are tokenized using the Moses 1 tokenizer, after which a 30k BPE vocabulary is constructed using the full dataset. For evaluation, we use newstest2016 concatenated with the last 2500 lines of OpenSubtitles2018. Transformers are trained with early stopping and a learning rate of 0.0002 2 with a plateau-reduce schedule for a maximum of 350k updates. Other training details can be found in the code supplement.  3  The resulting losses are plotted in Figure  1 , with model sizes ranging from 393k-56M parameters and data sizes from 40k-50M lines of text. 2.1 Cross-entropy vs. Data/Parameters  provide an ansatz that predicts cross-entropy loss given the amount of training data and the size of the neural model: Table  3 : Difference between scaling exponents when using the full dataset (? N ,? D ) vs. estimating the scaling exponents using only models trained on smaller subsets of the data (? N , ? D ). We see that even when using 3-6% of the data, the best-fit scaling exponents of Equation 1 stay very similar. L(N, D) = N C N ? N ? D + D C D ? D ( where L is the per-token development crossentropy loss (in nats), N is the number of nonembedding parameters, D is the amount of training data (in bytes), and ? N , ? D , N C , and D C are constants determined by the particulars of the data distribution and training setup.  4  Figure  1  shows that this equation is highly predictive of our results.  5  The predictions are also fairly stable; Table  3  shows that the best-fit parameters of this equation stay similar even when restricting ourselves to using only 3-6% of the data. In Appendix A we perform a retrospective analysis of the results from Zhang and Duh (2020) to give some insight into how different hyper-parameter settings may influence scaling coefficients. As either N or D approaches infinity, L(N, D) simplifies to a "pure power law" in the other variable, which looks like a straight line on a log-log graph. For example, if we assume all models are large enough that data becomes the main performance bottleneck, then: L(D) = D C D ? D (2) We will use this assumption later when dealing with very low-resource language pairs. 

 BLEU vs. Cross-Entropy Loss Predicting cross-entropy loss by itself does not tell us much about the quality of the translation system; we would really like to predict the achieved BLEU score, which is more interpretable to humans as a measure of adequacy, fidelity, and fluency  (Papineni et al., 2002) . Figure  2  shows that the relationship between BLEU and cross-entropy can vary between different language pairs and BPE settings. However, when these factors are fixed, BLEU seems to exponentially increase as crossentropy decreases: BLEU(L) ? Ce ?kL (3) This relationship is fairly predictable for high BLEU values, but becomes noisier as BLEU drops below 15. Notably, changing the BPE encoding does not seem to affect k, but does change the multiplying constant C.  6  Why should this relationship be exponential? We might gain some insight by re-writing Equation 3 in terms of the per-token perplexity (P): BLEU(P ) ? C 1 P k (4) where (1/P ) can intuitively be interpreted as the expected unigram precision of an autoregressively sampled translation with the same length as the reference sentence  (Manning and Schutze, 1999) . This is only intuition, however: in practice, we do not sample translations but decode using beam search, and BLEU combines multiple modified ngram precisions besides unigram precision. 7 6 We evaluate BLEU using multi-bleu.perl from the Moses toolkit. De-bpe-ing, de-tokenizing, and using Sacrebleu  (Post, 2018)  adds a small amount of noise but does not qualitatively change our results. See Appendix Figure  5 .  7  The relationship between precision and perplexity for higher values of n is not clear. In general, expected bigram precision = (1/P ) 2 .  

 Preventing Breakdown At Smaller Dataset Sizes Some extremely low-resource MT datasets (which we examine in Section 3) can have less than 5 MB of data (?40k sentence pairs). Figure  3  shows that when we extend our previous experiments to datasets smaller than this size, using 0.05% -0.0125% of the data, the data scaling power law seems to break down, casting doubt on our ability to extrapolate extremely low-resource results to medium and high-resource data regimes. However, the results are not simply noisy but predictably plateau to an apparent ceiling of 7.8 nats. For reference, a unigram language model trained on only the English part of the training data (with a 30k BPE vocab) achieves a per-token crossentropy of ?7 nats. This leads us to suspect that models in this data regime are learning to rely on simple unigram statistics that do not change much as we decrease the data size. Using a much smaller BPE vocabulary of 2k tokens rectifies this plateau and returns to power law scaling, even with datasets <5 MB. We believe this is because the smaller vocabulary makes it difficult to exploit unigram statistics for rare words. While this is not conclusive evidence, we recommend that cross-entropies near or above unigram LM performance should not be relied upon to extrapolate performance. Dataset subsets which contain less than half of the BPE vocabulary should similarly be avoided.  8  Figure  4 : USD-to-BLEU projections for low-resource language pairs, with a training setup similar to Section 2.  10  We assume each byte of data costs about 0.01 USD to acquire. 9 Negative dollars represent using less data than is currently available, whereas positive dollars represents our projections if we were to spend that much USD on acquiring more data. 

 Predicting ROI of Annotating Low-Resource Language Pairs If we assume that data is the main performance bottleneck (as it is in many low-resource language pairs), we can plug Equation 2 into Equation 3 to directly model the relationship between BLEU and data size: BLEU(D) = C exp K D ? D (5) where K = k(D C ) ? D . This can be further combined with the hourly cost of fluent human translators to give us an approximate USD-to-BLEU tradeoff when annotating more data for low-resource language pairs. Figure  4  shows some example projections for Tagalog-English (tl-en) and Swahili-English (swen), with each dataset containing less than 50k sentence pairs  (Zavorin et al., 2020) . Under some assumptions about the costs of human translation 9 , we predict that spending ?$60k USD to acquire more tl-en/sw-en data (which would roughly double the size of the either dataset) would lead to an improvement of around 10-15 BLEU.  9  We assume translation costs around 0.10 USD per word, each word is composed of 5 characters on average, and each character requires around a byte of space.  10  We train a 12 layer model using a 2k BPE dataset subsets (100%, 90%, ..., 50%) with five different data shuffling seeds. We also increase the checkpoint frequency for earlier stopping. 

 Limitations There is a reasonable amount of noise in the crossentropy/BLEU relationship at this scale (shown in Appendix Figure  8 ) which limits the precision and reliability of these predictions. In practice, we expect small amounts of data can be acquired in batches and predictions can be re-evaluated before deciding to continue. However, these predictions give a general sense of the cost of progress in lowresource machine translation. When engineering a real-world system, the simple option of acquiring more data and predictably improving performance should always be carefully weighed against more complicated and less predictable options. That being said, predictably achieving a high BLEU score on a test dataset is not equivalent to "solving translation" for that language pair. Underspecification  (D'Amour et al., 2020)  still poses a challenge for effectively evaluating machine translation systems in real-world scenarios, especially in low-resource language pairs where evaluation data is usually from a narrow domain. More robust evaluation methods are needed, and it is not clear whether the output of these methods will be as predictable as cross-entropy loss or BLEU. And finally, while our work demonstrates empirical power law scaling of NMT systems, it does not attempt to provide any causal explanation for these results. We also do not investigate the specific training factors that lead to a particular scaling exponent, but we expect this to be a fruitful research direction for future exploration. 11 

 Conclusion We have shown that supervised neural machine translation performance with Transformers scales like a power law in non-embedding parameters and training data, aligning with similar observations in unsupervised auto-regressive modeling. We've also seen that as development cross-entropy decreases, BLEU exponentially increases. These two relationships can be combined to predict an effective USD-to-BLEU trade-off when annotating more data, even in low-resource regimes. Xuan Zhang and Kevin Duh. 2020. Reproducible and efficient benchmarks for hyperparameter optimization of neural machine translation systems. Transactions of the Association for Computational Linguistics, 8:393-408. Figure  5 : Same results as Figure  2  (Left), but translations are de-bpe'd, de-tokenized, and BLEU is computed using Sacrebleu  (Post, 2018) . This introduces some noise but does not qualitatively change the exponential relationship between cross-entropy and BLEU. Figure  6 : The number of unique words seen during training drops precipitously around 5 MB of data when using a BPE of size 30k, but remains constant when using a BPE of size 2k. A Parameter Scaling in Japanese-English Translation In this section, we provide a brief retrostpective analysis of the results of Zhang and Duh (2020), in which many MT systems were trained to evaluate the efficacy of hyper-parameter optimization techniques. Specifically, we examine their results on the Japanese-English WMT 2019 Robustness task  (Li et al., 2019) . Figure  9  shows power-law scaling of the development cross-entropy loss with the number of non-embedding parameters.  12  We see that changing the BPE encoding vocabulary size and the number of layers can affect the constant multiplier N C , but does not seem to affect the exponent ? N . Furthermore, multiple attention   head settings  (8, 16)  were trained for each model size but they do not seem to impact scaling trends. We exclude some outliers with unexpectedly large losses at for larger model sizes. This only occurs for specific learning rates, so we believe those models failed to converge due to improper learning rate tuning. Figure 1 : 1 Figure 1: The development cross-entropy vs. the amount of data used to train each model from Section 2. Each point is colored by the number of non-embedding parameters in the model. The best fit of Equation 1 via leastsquares regression for each language pair is shown as the dotted line. 
