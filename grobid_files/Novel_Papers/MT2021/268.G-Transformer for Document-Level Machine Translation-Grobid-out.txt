title
G-Transformer for Document-level Machine Translation

abstract
Document-level MT models are still far from satisfactory. Existing work extend translation unit from single sentence to multiple sentences. However, study shows that when we further enlarge the translation unit to a whole document, supervised training of Transformer can fail. In this paper, we find such failure is not caused by overfitting, but by sticking around local minima during training. Our analysis shows that the increased complexity of target-to-source attention is a reason for the failure. As a solution, we propose G-Transformer, introducing locality assumption as an inductive bias into Transformer, reducing the hypothesis space of the attention from target to source. Experiments show that G-Transformer converges faster and more stably than Transformer, achieving new state-of-the-art BLEU scores for both nonpretraining and pre-training settings on three benchmark datasets.

Introduction Document-level machine translation (MT) has received increasing research attention  (Gong et al., 2011; Hardmeier et al., 2013; Garcia et al., 2015; Miculicich et al., 2018a; Maruf et al., 2019; . It is a more practically useful task compared to sentence-level MT because typical inputs in MT applications are text documents rather than individual sentences. A salient difference between document-level MT and sentence-level MT is that for the former, much larger inter-sentential context should be considered when translating each sentence, which include discourse structures such as anaphora, lexical cohesion, etc. Studies show that human translators consider such contexts when conducting document translation  (Hardmeier, 2014; L?ubli et al., 2018) . Despite that neural models achieve competitive performances on sentence- * * Corresponding author. level MT, the performance of document-level MT is still far from satisfactory. Existing methods can be mainly classified into two categories. The first category translates a document sentence by sentence using a sequence-tosequence neural model  (Zhang et al., 2018; Miculicich et al., 2018b; Maruf et al., 2019; Zheng et al., 2020) . Document-level context is integrated into sentence-translation by introducing additional context encoder. The structure of such a model is shown in Figure  1(a) . These methods suffer from two limitations. First, the context needs to be encoded separately for translating each sentence, which adds to the runtime complexity. Second, more importantly, information exchange cannot be made between the current sentence and its document context in the same encoding module. The second category extends the translation unit from a single sentence to multiple sentences  (Tiedemann and Scherrer, 2017; Agrawal et al., 2018;  and the whole document  (Junczys-Dowmunt, 2019; . Recently, it has been shown that when the translation unit increases from one sentence to four sentences, the performance improves  Scherrer et al., 2019) . However, when the whole document is encoded as a single unit for sequence to sequence translation, direct supervised training has been shown to fail . As a solution, either large-scale pre-training  or data augmentation (Junczys-Dowmunt, 2019) has been used as a solution, leading to improved performance. These methods are shown in Figure  1 (b). One limitation of such methods is that they require much more training time due to the necessity of data augmentation. Intuitively, encoding the whole input document as a single unit allows the best integration of context information when translating the current sentence. However, little work has been done investigating the underlying reason why it is difficult to train such a document-level NMT model. One remote clue is that as the input sequence grows larger, the input becomes more sparse  (Pouget-Abadie et al., 2014; Koehn and Knowles, 2017) . To gain more understanding, we make dedicated experiments on the influence of input length, data scale and model size for Transformer (Section 3), finding that a Transformer model can fail to converge when training with long sequences, small datasets, or big model size. We further find that for the failed cases, the model gets stuck at local minima during training. In such situation, the attention weights from the decoder to the encoder are flat, with large entropy values. This can be because that larger input sequences increase the challenge for focusing on a local span to translate when generating each target word. In other words, the hypothesis space for target-to-source attention is increased. Given the above observations, we investigate a novel extension of Transformer, by restricting selfattention and target-to-source attention to a local context using a guidance mechanism. As shown in Figure  1 (c), while we still encode the input document as a single unit, group tags 1 2 3 are assigned to sentences to differentiate their positions. Target-to-source attention is guided by matching the tag of target sentence to the tags of source sentences when translating each sentence, so that the hypothesis space of attention is reduced. Intuitively, the group tags serve as a constraint on attention, which is useful for differentiating the current sentence and its context sentences. Our model, named G-Transformer, can be thus viewed as a combination of the method in Figure  1 (a) and Figure  1 (b), which fully separate and fully integrates a sentence being translated with its document level context, respectively. We evaluate our model on three commonly used document-level MT datasets for English-German translation, covering domains of TED talks, News, and Europarl from small to large. Experiments show that G-Transformer converges faster and more stably than Transformer on different settings, obtaining the state-of-the-art results under both non-pretraining and pre-training settings. To our knowledge, we are the first to realize a truly document-by-document translation model. We release our code and model at https://github.com/baoguangsheng/g-transformer. 

 Experimental Settings We evaluate Transformer and G-Transformer on the widely adopted benchmark datasets  (Maruf et al., 2019) , including three domains for English-German (En-De) translation. TED. The corpus is transcriptions of TED talks from IWSLT 2017. Each talk is used as a document, aligned at the sentence level. tst2016-2017 is used for testing, and the rest for development. News. This corpus uses News Commentary v11 for training, which is document-delimited and sentence-aligned. newstest2015 is used for development, and newstest2016 for testing. Europarl. The corpus is extracted from Europarl v7, where sentences are segmented and aligned using additional information. The train, dev and test sets are randomly split from the corpus. The detailed statistics of these corpora are shown in Table  1 . We pre-process the documents by splitting them into instances with up-to 512 tokens, taking a sentence as one instance if its length exceeds 512 tokens. We tokenize and truecase the sentences with MOSES  (Koehn et al., 2007)  tools, applying BPE  (Sennrich et al., 2016)  with 30000 merging operations. We consider three standard model configurations. Base Model. Following the standard Transformer base model  (Vaswani et al., 2017)   dimension hidden vectors. Big Model. We follow the standard Transformer big model  (Vaswani et al., 2017) , using 6 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. Large Model. We use the same settings of BART large model , which involves 12 layers, 16 heads, 1024 dimension outputs, and 4096 dimension hidden vectors. We use s-BLEU and d-BLEU ) as the metrics. The detailed descriptions are in Appendix A. 

 Transformer and Long Inputs We empirically study Transformer (see Appendix B) on the datasets. We run each experiment five times using different random seeds, reporting the average score for comparison. 

 Failure Reproduction Input Length. We use the Base model and fixed dataset for this comparison. We split both the training and testing documents from Europarl dataset into instances with input length of 64, 128, 256, 512, and 1024 tokens, respectively. For fair comparison, we remove the training documents with a length of less than 768 tokens, which may favour small input length. The results are shown in former. Data Scale. We use the Base model and a fixed input length of 512 tokens. For each setting, we randomly sample a training dataset of the expected size from the full dataset of Europarl. The results are shown in Figure  2b . The performance increases sharply when the data scale increases from 20K to 40K. When data scale is equal or less than 20K, the BLEU scores are under 3, which is unreasonably low, indicating that with a fixed model size and input length, the smaller dataset can also cause the failure of the training process. For data scale more than 40K, the BLEU scores show a wide dynamic range, suggesting that the training process is unstable. Model Size. We test Transformer with different model sizes, using the full dataset of Europarl and a fixed input length of 512 tokens. Transformer-Base can be trained successfully, giving a reasonable BLEU score. However, the training of the Big and Large models failed, resulting in very low BLEU scores under 3. It demonstrates that the increased model size can also cause the failure with a fixed input length and data scale. The results confirm the intuition that the performance will drop with longer inputs, smaller datasets, or bigger models. However, the BLEU scores show a strong discontinuity with the change of input length, data scale, or model size, falling into two discrete clusters. One is successfully trained cases with d-BLEU scores above 10, and the other is failed cases with d-BLEU scores under 3. Figure  5 : For the successful model, the attention distribution shrinks to narrow range (low entropy) and then expands to wider range (high entropy). 

 Failure Analysis Training Convergence. Looking into the failed models, we find that they have a similar pattern on loss curves. As an example of the model trained on 20K instances shown in Figure  3a , although the training loss continually decreases during training process, the validation loss sticks at the level of 7, reaching a minimum value at around 9K training steps. In comparison, the successfully trained models share another pattern. Taking the model trained on 40K instances as an example, the loss curves demonstrate two stages, which is shown in Figure  3b . In the first stage, the validation loss similar to the failed cases has a converging trend to the level of 7. In the second stage, after 13K training steps, the validation loss falls suddenly, indicating that the model may escape successfully from local minima. From the two stages of the learning curve, we conclude that the real problem, contradicting our first intuition, is not about overfitting, but about local minima. Attention Distribution. We further look into the attention distribution of the failed models, observing that the attentions from target to source are widely spread over all tokens. As Figure  4a  shows, the distribution entropy is high for about 8.14 bits on validation. In contrast, as shown in Figure  4b , the successfully trained model has a much lower attention entropy of about 6.0 bits on validation.  Figure  5  shows the self-attention distributions of the successfully trained models. The attention entropy of both the encoder and the decoder drops fast at the beginning, leading to a shrinkage of the attention range. But then the attention entropy gradually increases, indicating an expansion of the attention range. Such back-and-forth oscillation of the attention range may also result in unstable training and slow down the training process. 

 Conclusion The above experiments show that training failure on Transformer can be caused by local minima. Additionally, the oscillation of attention range may make it worse. During training process, the attention module needs to identify relevant tokens from whole sequence to attend to. Assuming that the sequence length is N , the complexity of the attention distribution increases when N grows from sentence-level to document-level. We propose to use locality properties  (Rizzi, 2013; Hardmeier, 2014; Jawahar et al., 2019)  of both the language itself and the translation task as a constraint in Transformer, regulating the hypothesis space of the self-attention and target-to-source attention, using a simple group tag method. 
