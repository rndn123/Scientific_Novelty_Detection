title
Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders

abstract
Encoder pre-training is promising in end-toend Speech Translation (ST), given the fact that speech-to-translation data is scarce. But ST encoders are not simple instances of Automatic Speech Recognition (ASR) or Machine Translation (MT) encoders. For example, we find that ASR encoders lack the global context representation, which is necessary for translation, whereas MT encoders are not designed to deal with long but locally attentive acoustic sequences. In this work, we propose a Stacked Acoustic-and-Textual Encoding (SATE) method for speech translation. Our encoder begins with processing the acoustic sequence as usual, but later behaves more like an MT encoder for a global representation of the input sequence. In this way, it is straightforward to incorporate the pre-trained models into the system. Also, we develop an adaptor module to alleviate the representation inconsistency between the pre-trained ASR encoder and MT encoder, and develop a multi-teacher knowledge distillation method to preserve the pre-training knowledge. Experimental results on the LibriSpeech En-Fr and MuST-C En-De ST tasks show that our method achieves state-of-the-art BLEU scores of 18.3 and 25.2. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable or even better BLEU performance than the cascaded ST counterpart when large-scale ASR and MT data is available 1 .

Introduction End-to-end Speech Translation (E2E ST) has become popular recently for its ability to free designers from cascading different systems and shorten 

 Setting Model the pipeline of translation  (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017) . Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems  (Berard et al., 2018) . For example, one can initialize the ST encoder using a large-scale ASR model  (Bansal et al., 2019) . But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table  1 ). In this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT encoder. More specifically, there are two problems. ? Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder  (Berard et al., 2018) , it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation  (Yang et al., 2018) . ? Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder. We address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-and-Textual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic encoding output to an MT encoder with two principles: informative and adaptive. In this way, pre-trained ASR and MT encoders can work for what we would originally design them, and the incorporation of pre-trained models into ST is more straightforward. In addition, we develop a multi-teacher knowledge distillation method to robustly train the ST encoder and preserve the pretrained knowledge during fine-tuning . We test our method in a Transformer-based endto-end ST system. Experimental results on the Lib-riSpeech En-Fr and MuST-C En-De speech translation benchmarks show that it achieves the stateof-the-art performance of 18.3 and 25.2 BLEU points. Under a more challenging setup, where the large-scale ASR and MT data is available, SATE achieves comparable or even better performance than the cascaded ST counterpart. We believe that we are the first to present an end-to-end system that can beat the strong cascaded system in unrestricted speech translation tasks. 

 Related Work Speech translation aims at learning models that can predict, given some speech in the source language, the translation into the target language. The earliest of these models were cascaded: they treated ST as a pipeline of running an ASR system and an MT system sequentially  (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004) . This allows the use of off-the-shelf models, and was (and is) popular in practical ST systems. However, these systems were sensitive to the errors introduced by different component systems and the high latency of the long pipeline. As another stream in the ST area, end-to-end methods have been promising recently  (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018) . The rise of end-to-end ST can be traced back to the success of deep neural models  (Duong et al., 2016) . But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation  (Pino et al., 2019 (Pino et al., , 2020 . This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals  (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021) . Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model  (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020) . More sophisticated methods include better training and fine-tuning  (Wang et al., 2020a,b) , the shrink mechanism , the adversarial regularizer  (Alinejad and Sarkar, 2020) , and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly. 3 Why is ST Encoding Difficult? Following previous work in end-to-end models  (Berard et al., 2016; Weiss et al., 2017) , we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence. In such a scenario, all end-to-end ST, ASR and MT systems can be viewed as instances of the same architecture. Then, components of these systems can be pre-trained and re-used across them. An underlying assumption here is that the ST encoder is doing something quite similar to what the MT (or ASR) encoder is doing. However,  Sperber et al. (2018)  find that the ASR model benefits from a small attention window, which is inconsistent with the MT model  (Yang et al., 2018) . To verify this, we compare the behavior of ST, ASR and MT encoders. We choose Transformer as the base architecture  (Vaswani et al., 2017)  and run experiments on the MuST-C En-De corpus. We report the results on the MuST-C En-De tst-COMMON test data. For stronger systems, we use Connectionist Temporal Classification (CTC)  (Graves et al., 2006)  as the auxiliary loss on the encoders when we train the ASR and ST systems  (Watanabe et al., 2017; Karita et al., 2019; Bahar et al., 2019) . The CTC loss forces the encoders to learn alignments between speech and transcription. It is necessary for the state-of-the-art performance  (Watanabe et al., 2018) . Here we define the localness of a word as the sum of the attention weights to the surrounding words (or features) within a fixed small window 2 . The window size is 10% of the sequence length. Figure  1 (a) shows the localness of the attention weights for different layers of the encoders. We see that the ST and ASR encoders prefer local attention which indicates a kind of short-distance dependencies in processing acoustics feature sequences. Whereas the MT encoder generates a more global distribution of attention weights for word sequences, especially when we stack more layers. This result arises a new question: Is local attention sufficient for speech translation? Then, we design another experiment to examine if the high localness in attention weights of the ASR and ST encoders is due to the bias imposed by CTC. In Figure  1 (b), we use the CTC loss in the intermediate layer and show the average localness of the layers above or below CTC. The CTC loss demonstrates strong preference for locally attentive models. The upper-level layers act more like an MT encoder, that is, the layers with no CTC loss generates more global distributions. Taking this further, Figure  1 (c) demonstrates a slightly higher BLEU score when we free more upper-level layers from the guide of CTC. Meanwhile, the word error rate (WER) increases because only lower parts of the model are learned in a standard manner of ASR. Now we have some hints: the ST encoder is not a simple substitution of the ASR encoder or the MT encoder. Rather, they are complementary to each other, that is, we need the ASR encoder to deal with the acoustic input, and the MT encoder to generate the representation vector that can work better with the decoder. 

 The Method In speech translation, we want the encoder to represent the input speech to some sort of decoderfriendly representations. We also want the encoder to be "natural" for pre-training. In the following, we describe, Stacked Acoustics-and-Textual Encoding (SATE), a new ST encoding method to meet these requirements, and improvements of it.  

 Stacked Acoustic-and-Textual Encoding Unlike previous work, the SATE method does not rely on a single encoder to receive the signal from both the CTC loss and the feedback of the decoder. Instead, it is composed of two encoders: the first does exactly the same thing as the ASR encoder (call it acoustic encoder), and the other generates a higher-level globally-attentive representation on top of the acoustic encoder (call it textual encoder). See Figure  2  for the architecture of SATE. The acoustic encoder is trained by CTC in addition to the supervision signal from the translation loss. Let (x, y s , y t ) be an ST training sample, where x is the input feature sequence of the speech, y s is the transcription of x, and y t is the translation in the target language. We define the output of the acoustic encoder as: h s = E s (x) (1) where E s (?) is the encoding function. Then, we add a Softmax layer on h s to predict the CTC label path ? = (? 1 , ? ? ? , ? T ), where T is the length of the input sequence. The probability of path P(?|h s ) is the product of the probability P(? t |h s t ) at every time t based on conditionally independent assumption: P(?|h s ) ? T t P(? t |h s t ) (2) CTC works by summing over the probability of all possible alignment paths ?(y s ) between x and y s , as follows: P CTC (y s |h s ) = ?(y s ) P(?|h s ) (3) Then, the CTC loss is defined as: L CTC = ? log P CTC (y s |h s ; ? CTC ) (4) where ? CTC is the model parameters of the acoustic encoder and the CTC output layer. The acoustic encoder is followed by an adaptor. It receives h s and P (?|h s ), and produces a new representation required by the textual encoder. Let A(?, ?) be the adaptor module. Its output is defined as: ?s = A(h s , P(?|h s )) (5) We leave the design of the adaptor to Section 4.2. Furthermore, we stack the textual encoder on the adaptor. The output h t is defined as: h t = E t ( ?s ) (6) where E t (?) is the textual encoder. h t is fed into the decoder for computing the translation probability P Trans (y t |h t ), as in standard MT systems. We define the translation loss as: L Trans = ? log P Trans (y t |h t ; ? ST ) (7) where ? ST is all model parameters except for the CTC output layer. Finally, we interpolate L CTC and L Trans (with coefficient ?) for the loss of the entire model: L = ? ? L CTC + (1 ? ?) ? L Trans (8) Since the textual encoder works for the decoder only, it is trained as an MT encoder. In this way, the acoustic and textual encoders can do what we would originally expect them to do: the acoustic encoder deals with the acoustic input (i.e., ASR encoding), and the textual encoder generates a representation for translation (i.e., MT encoding). Also, SATE is friendly to pre-training. One can simply use an ASR encoder as the acoustic encoder, and use an MT encoder as the textual encoder. Note that SATE is in general a cascaded model, in response to the pioneering work in ST  (Ney, 1999) . It can be seen as cascading the ASR and MT systems in an end-to-end fashion. 

 The Adaptor Now we turn to the design of the adaptor. Note that the pre-trained MT encoder assumes that the input is a word embedding sequence. Simply stacking the MT encoder and the ASR encoder obviously does not work well. For this reason, the adaptor fits the output of the ASR encoder (i.e., the acoustic encoder) to what an MT encoder would like to see. We follow two principles in designing the adaptor: adaptive and informative. We need an adaptive representation to make the input of the textual encoder similar to that of the MT encoder. To this end, we generate the soft contextual representation that shares the same latent space with the embedding layer of the MT encoder. As shown in Eq. (  2 ), the CTC output P(? t |h s t ) indicates the alignment probability over the vocabulary at time t. Instead of replacing the representation by the embedding of the most-likely token , we employ a soft token which is the expectation of the embedding over the distribution from CTC. Let W e be the embedding matrix of the textual encoder, we define the soft representation h s soft as: h s soft = P(?|h s ) ? W e (9) Also, an informative representation should contain information in the original input  (Peters et al., 2018) . The output acoustic representation of the ASR encoder generally involves paralinguistic information, such as emotion, accent, and emphasis. They are not expressed in the form of text explicitly but might be helpful for translation. For example, the generation of the declarative or exclamatory sentences depends on the emotions of the speakers. We introduce a single-layer neural network to learn to map the acoustic representation to the latent space of the textual encoder, which preserves the acoustic information: h s map = ReLU(W map ? h s + b map ) (10) where W map and b map are the trainable parameters. The final output of the adaptor is defined to be: A(h s , P (?|h s )) = ? ? h s map + (1 ? ?) ? h s soft (11) where ? is the weight of h s map and set to 0.5 by default. Figure  3  shows the architecture of the adaptor. Note that, in the adaptor, we do not change the sequence length for textual encoding because such a way is simple for implementation and shows satisfactory results in our experiments. Although there is a length inconsistency issue, the sequence representation of the speech should be similar with the Mapping Layer  correspond transcription. Shrinking the sequence simply results in information incompleteness. We will investigate this issue in the future. 

 Multi-teacher Knowledge Distillation Another improvement here is that we develop a multi-teacher knowledge distillation (MTKD) method to preserve the pre-trained knowledge during fine-tuning  (Hinton et al., 2015) . The ST model mimics the teacher distribution by minimizing the cross-entropy loss between the teacher and student  (Liu et al., 2019) . For a training sample (x, y s , y t ), we define two loss functions: We can rewrite Eq. (  8 ) to obtain a new loss: L KD CTC = ? T m=1 |V | k=1 Q(? m = v k |x; ? ASR ) ? log P(? m = v k |x; ? CTC ) (12) L KD Trans = ? |y t | n=1 |V | k=1 Q(y t n = v k |y s ; ? MT ) ? log P(y t n = v k |x; ? ST ) (13 L = ? ? ? ? L CTC + (1 ? ?) ? L KD CTC +(1 ? ?) ? ? ? L Trans + (1 ? ?) ? L KD Trans (14) where both ? and ? are the hyper-parameters that balance the preference between the teacher distribution and the ground truth. 

 Experiments 

 Datasets and Preprocessing We consider restricted and unrestricted settings on speech translation tasks. We run experiments on the LibriSpeech English-French (En-Fr)  and MuST-C English-German (En-De)  (Gangi et al., 2019)  corpora, which correspond to the low-resource and highresource datasets respectively. Available ASR and MT data is only from the ST data under the restricted setting. For comparison in practical scenarios, the unrestricted setting allows the additional data for ASR and MT models. LibriSpeech En-Fr Followed previous work, we use the clean speech translation training set of 100 hours, including 45K utterances and doubled translations of Google Translate. We select the model on the dev set (1,071 utterances) and report results on the test set (2,048 utterances). MuST-C En-De MuST-C is a multilingual speech translation corpus extracted from the TED talks. We run the experiments on the English-German speech translation dataset of 400 hours speech with 230K utterances. We select the model on the dev set (1,408 utterances) and report results on the tst-COMMON set (2,641 utterances). Unrestricted Setting We use the additional ASR and MT data for pre-training. The 960 hours Lib-riSpeech ASR corpus is used for the English ASR model. We extract 10M sentences pairs from the WMT14 English-French and 18M sentence pairs from the Opensubtitle2018 3 English-German translation datasets. Preprocessing Followed the preprocessing recipes of ESPnet  (Inaguma et al., 2020) , we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses  (Koehn et al., 2007) . We learn Byte-Pair Encoding  (Sennrich et al., 2016)  subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets. 3 http://opus.nlpl.eu/OpenSubtitles-v2018.php 

 Model Settings All experiments are implemented based on the ES-Pnet toolkit 4 . We use the Adam optimizer with ? 1 = 0.9, ? 2 = 0.997 and adopt the default learning schedule in ESPnet. We apply dropout with a rate of 0.1 and label smoothing ls = 0.1 for regularization. For reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2 ? 2 and downsample the sequence by a factor of 4  (Weiss et al., 2017) . The encoder consists of 12 layers for both the ASR and vanilla ST models, and 6 layers for the MT model. The encoder of SATE includes an acoustic encoder of 12 layers and a textual encoder of 6 layers. The decoder consists of 6 layers for all models. The weight of CTC objective ? for multitask learning is set to 0.3 for all ASR and ST models. The coefficients ? and ? are set to 0.5 in Eq. (  14 ) for the MTKD method. Under the restricted setting, we employ the Transformer architecture, where each layer comprises 256 hidden units, 4 attention heads, and 2048 feed-forward size. For the unrestricted setting, we use the superior architecture Conformer  (Gulati et al., 2020)  on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8. The ASR 5 and MT models pre-train with the additional data and fine-tune the model parameters with the task-specific data. During inference, we average the model parameters on the best 5 checkpoints based on the performance of the development set. We use beam search with a beam size of 4 for all models. Different from previous work, we report the case-sensitive SacreBLEU 6  (Post, 2018)  for future standardization comparison across papers. model significantly. This demonstrates the superiority of stacked acoustic and textual encoding for the speech translation task. Incorporating the pretrained ASR and MT models into SATE releases the encoding burden of the model and achieves a remarkable improvement. The MTKD method provides a strong supervised signal and forces the model to preserve the pre-trained knowledge. Furthermore, we utilize the SpecAugment  (Park et al., 2019)  which is applied in the input speech features for better generalization and robustness 7 . It yields a remarkable improvement of 1.9 BLEU points over the cascaded baseline and achieves a new state-ofthe-art performance. 

 Results 

 Results on MuST-C En-De Under the unrestricted setting, the large-scale ASR and MT data is available, whereas the ST data is scarce. This leads to the cascaded method outperforms the vanilla E2E method with a huge margin of 4.5 BLEU points. The pre-training only slightly closes the gap due to the modeling deficiency and representation inconsistency. SATE incorporates the pre-trained models fully, which achieves a significant improvement of 3.7 BLEU points. With the MTKD and SpecAugment methods, we achieve a comparable performance of 28.1 BLEU points. To our knowledge, we are the first to develop an end-to-end ST system that achieves comparable performance with the cascaded counterpart when large-scale ASR and MT data is available. it is of small magnitude with clean speech data. This results in that the performance of the vanilla E2E baseline is even better than the cascaded counterpart under the restricted setting. Furthermore, pre-training helps the model achieve an improvement of 0.8 BLEU points over the cascaded baseline. More interestingly, SATE without pre-training outperforms the above methods significantly, even achieves a slight improvement than the MT model. A possible reason is that the diverse acoustic representation is fed to the textual encoder, which improves the robustness of the model. This demonstrates the superiority of our method. 

 Results on LibriSpeech En-Fr Combining our proposed methods yields a substantial improvement of 2.0 BLEU points over the cascaded baseline. It is a new state-of-the-art result of 18.3 BLEU points. Also, we outperform the cascaded counterpart by 0.2 BLEU points on the unrestricted task. 

 Analysis 

 Model Performance vs. Speedup In Table  4 , we summarize the performance and inference speedup based on the real time factor (RTF). The vanilla E2E ST model yields an inference speedup of 1.91? than the cascaded counterpart and demonstrates the low latency of the end-to-end methods. We increase the encoder layers for comparison with SATE under the similar model parameters. However, there is a remarkable gap of 0.5 or 0. Our method not only improves the performance of 1.9 BLEU points but also reaches up to 1.69? speedup than the cascaded baseline. This encourages the application of the end-to-end ST model in practical scenarios. 

 Effects of Pre-trained Modules The effects of the pre-trained modules are shown in Table  5 . The model performance drops significantly without the pre-trained ASR encoder, especially on the MuST-C corpus that contains noisy speech. The model parameters of pre-trained MT model are updated for adapting the output representation of the random initialized acoustic encoder. This results in the catastrophic forgetting problem  (Goodfellow et al., 2015) . The effect of the pretrained MT model is more remarkable on the Lib-riSpeech corpus due to the modeling burden on the translation. The benefit of the pre-trained MT decoder is larger than the MT encoder. This is contrary to the previous conclusions that the MT encoder helps the performance significantly  ASR encoder provides a rich representation and acts as part of the MT encoder, this leads to lower performance degradation when the textual encoder trains from scratch. Each pre-trained module has a great effect on the final performance. With the complete integration of the pre-trained modules, the model parameters are updated slightly, which preserves the pre-trained knowledge. 

 Effects of The Adaptor We show the effects of the adaptor in Table  6 . The straight connection which omits the representation inconsistency issue results in the lower benefit of pre-training. Although the soft representation aims at generating the adaptive representation, there is no obvious improvement on the MuST-C corpus. A possible reason is that the noisy speech inputs produce the misalignment probabilities, which disturbs the textual encoding. The mapping method achieves a slight improvement by transforming the acoustic representation to the textual representation. Fusing the soft and mapping representation enriches the information and avoids the representation inconsistency issue, which achieves the best performances. 

 Impact on Localness We show the encoder localness of the vanilla E2E ST model and SATE model with pre-training in Figure  4 . As mentioned above, the vanilla ST model inherits the preference of ASR, which focuses on short-distance dependencies. SATE initializes with the pre-trained ASR and MT encoders, which stacks acoustic and textual encoding. The complementary behaviors of the pre-trained models benefit the translation, that is, the lower layers act like an ASR encoder while the upper layers capture global representation like an MT encoder. 

 Conclusion In this paper, we investigate the difficulty of speech translation and shed light on the reasons why pretraining has been challenging in ST. This inspires us to propose a Stacked Acoustic-and-Textual Encoding method, which is straightforward to incorporate the pre-trained models into ST. We also introduce an adaptor module and a multi-teacher knowledge distillation method for bridging the gap between pre-training and fine-tuning. Results on the LibriSpeech and MuST-C corpora demonstrate the superiority of our method. Furthermore, we achieve comparable or even better performance than the cascaded counterpart when large-scale ASR and MT data is available. Figure 1 : 1 Figure 1: (a) Localness in each layer of the ST, ASR, and MT encoders, (b) the impact of CTC position on localness, and (c) the impact of CTC position on performance of ST and ASR models. 
