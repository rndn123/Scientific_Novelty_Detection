title
RoBLEURT Submission for the WMT2021 Metrics Task

abstract
In this paper, we present our submission to Shared Metrics Task: RoBLEURT (Robustly Optimizing the training of BLEURT). After investigating the recent advances of trainable metrics, we conclude several aspects of vital importance to obtain a well-performed metric model by: 1) jointly leveraging the advantages of source-included model and referenceonly model, 2) continuously pre-training the model with massive synthetic data pairs, and 3) fine-tuning the model with data denoising strategy. Experimental results show that our model reaching state-of-the-art correlations with the WMT2020 human annotations upon 8 out of 10 to-English language pairs.

Introduction Automatically evaluating the adequacy of machine translation (MT) candidates is crucial for judging the quality of MT systems. N-gram-based metrics, such as BLEU  (Papineni et al., 2002) , TER  (Snover et al., 2006)  and chrF++  (Popovic, 2015 (Popovic, , 2017 , have dominated in the topic of MT metric. Despite the success, recent studies  (Smith et al., 2016; Mathur et al., 2020a ) also pointed out that, N-grambased metrics often fail to robustly match paraphrases and capture distant dependencies. As MT systems become stronger in recent decades, these metrics show lower correlations with human judgements, leading the derived results unreliable. One arising direction for metric task is using trainable model to evaluate the semantic consistency between candidates and golden references via predicting scores. BERTScore  (Zhang et al., 2020) , BLEURT  (Sellam et al., 2020)  and COMET  (Rei et al., 2020)  have shown higher correlations with human judgements than N-gram-based automatic metrics. Benefiting from the powerful pre-trained language models (LMs), e.g., BERT  (Devlin et al., 2019) , those fine-tuned metric models first derive the representation of each input, then introduce an extra linear regression module to give predicted score which describes to what degree the MT system output adequately expresses the semantic of source/reference contents. Furthermore, related work  (Takahashi et al., 2020; Rei et al., 2020)  reports that, metrics which additionally introduces source sentences into inputs can further boost the performance of metric model. To push such "model as a metric" approach further, we present RoBLEURT -Robustly optimizing the training of BLEURT  (Sellam et al., 2020) , to achieve a better consistency between model predictions and human assessments. Specifically, for low-resource scenarios, using only hypotheses and references can give more accurate results, alleviating the sparsity of source-side language; for the high-resource language pairs, we format the model input as the combination of source, hypothesis and reference sentences, making model attending to both source input and target reference when evaluating the consistency of semantics. Then, we collect massive pseudo data from real MT engines tagged by pseudo scores with strong baselines for supervised model pre-training. As to the fine-tuning phase, we rescore the noisy WMT metric data of previous years with strong metric baselines, which are then utilized to fine-tune our model. Experimental results show that, following the setting of WMT2021 metric task, our RoBLEURT model outperforms the reported results of state-of-the-art metrics on multilingual-to-English language pairs. 

 RoBLEURT 

 Combining Multilingual and Monolingual Language Model Same as previous years, translation tasks cover both low-resource and high-resource scenarios. To give higher reliable outputs, we believe our metric model can benefit from separately pre-trained and fine-tuned over each kind of scenarios: ? For low-resource multilingual-to-English language pairs, we can hardly obtain massive parallel data with high quality, nor access wellperformed automatic translation systems to produce syntectic data for pre-training. We mainly consider model outputs and gold references as our model inputs. Thus we mainly consider the monolingual English language model (called RoBLEURT-NOSRC) in this scenario. ? As to high-resource language pairs, they do not suffer from limitations above, thus can benefit from the information of source input, model output and target reference. A multilingual version of pre-trained LM (called RoBLEURT-SRC) can be used for this scenario. The main architecture of our model is TRANS-FORMER  (Vaswani et al., 2017) , which has been widely used in recent researches. As related studies point out that RoBERTa  (Liu et al., 2019)  outperforms conventional BERT  (Devlin et al., 2019) , we employ the well-trained model checkpoint from RoBERTa family. Besides, the conventional BLEURT model is trained based on uncased-BERT, which tokenizes the input sentences with the lower case format whereas RoBERTa uses casesensitive tokenizer, which may be helpful to distinguish more information. Moreover, model with larger scale is generally related with better performance and higher capacity of available knowledges. Recently, several approaches which further finetune RoBERTa model can give better performance over multiple natural language inference tasks. To make sure our model can also benefit from this, we finally use RoBERTa-large-mnli 1 and RoBERTa-large-xnli 2  (Conneau et al., 2020)  for low-resouce and high-resource language pairs, respectively. 

 Model Combination We are also interested in exploring whether we can boost the performance of combine RoBLEURT-NOSRC and RoBLEURT-SRC. Combining the out-puts from models trained with different settings is widely used in MT tasks  (Barrault et al., 2020) . In this paper, We simply use weighted combination of all available well-trained models. 

 Input Formatting Our model consists of a well-trained RoBERTa model to obtain segment-level representations. Here we also try with two solutions: the model input includes source sentence (RoBLEURT-SRC) or not (RoBLEURT-NOSRC). For the former, the model input is formatted as: <s> hyp' </s> </s> ref </s>. (1) As the latter, due to the number of input sentences is larger than RoBERTa predefined training format, we redesigned the input format as: <s> src </s> </s> hyp' </s> </s> ref </s>. (2) 

 Prediction Module To obtain a scalar value as predicted score, we directly derive the representation at the first position of input X ? R 1?d as the representation of input tuple, where d is the size of hidden states. It is then fed to projection layer, after which we yield a scalar for describing how adequately the hypothesis express the semantics: s = WX + b, (3) where W ? R 1?d , b ? R 1 are both trainable pa- rameters. During training, the learning objective is to reduce the mean squared error (MSE) between model prediction s and annotated score score: L = (s ? score) 2 . (4) 

 Continuous Pre-training with Synthetic Data Continuous Pre-training the model on synthetic data is proven helpful to improve the performance  (Sellam et al., 2020)  ? Available well-performed checkpoints. We collect the officially released COMET 4 and BLEURT checkpoints 5 . After collecting the predictions with all checkpoints above, we identify the noisy data items by computing the variance of rankings within whole dataset. Finally, we rescore those noisy items with those models, tagging pseudo labels for fine-tuning. Besides, to guarantee the scores are unbiased, we re-normalize them within the dataset of each year by Z-score following  Sellam et al. (2020) . 

 Experiments 

 Settings of Continuous Pre-training Synthetic Data Collection To continue pretraining the model, we simply collect parallel data from the previous WMT conferences, taking the training data from MT track cs/de/ja/ru/zh-en language pairs to obtain high-resource pseudo data. Finally, for each language pair, we collect 2.0 million quadruples for metric model pre-training. For lowresource scenarios, we reuse the datasets above, where the only difference is removing the source sentences. As to development set, we directly collect the direct assessment (DA) dataset from the WMT2020 Metrics task track. We evaluate the model performance following DARR assessments  (Ma et al., 2019; Rei et al., 2020) , and choose the best checkpoint for fine-tuning. Hyper-parameters During the continuous pretraining, we determine the maximum learning rate as 5 ? 10 ?6 , training steps as 0.5M and warm-up steps as 50K. The learning rate first linearly warms up from 0 to maximum learning rate, then decays to   (Sellam et al., 2020)  12.6 45.6 25.8 9.3 13.7 25.8 32.7 5.7 20.7 23.0 COMET  (Rei et al., 2020)  12.9 48.5 27.4 15.6 17.1 28.1 29.8 9.9 15.8 24.1 SOTA Results  (Mathur et al., 2020b)   Table  1 : DARR Kendall correlation (%) over WMT2020 data for each language pair (xx-en). Results of baseline systems are conducted from official report  (Mathur et al., 2020b) . Best viewed in bold. 0 till the end of training. To avoid over-fitting, we apply the dropout ratio as 0.1. We conduct the pretraining experiments with 8 Nvidia V100 GPUs, where each batch size for each GPU device contains 4 quadruplets. To avoid memory issues during pre-training, we simply reduce the number of total tokens, leaving 128 and 192 for RoBLEURT-NOSRC and RoBLEURT-SRC, respectively. 

 Settings of Fine-tuning Data Collection We fine-tune our model with the WMT2015-2019 dataset as training set, where the WMT2018-2019 subsets are processed with our data denoising strategy as discussed in ? 2.3. To directly confirm the effectiveness of our approach, we simply use WMT2020 dataset as dev set to compare reported results in WMT2020 metric task. To select the model for participating the WMT2021 metric task, we divide the WMT2020 dataset into 4 folds, where the data items are firstly gathered with the identical source and reference sentence. For each fold, we select the corresponding fold of the WMT2020 subset as the dev set, and use the combination of the WMT2015-2019 dataset and the other unused WMT2020 subsets as the training set. Hyper-parameters During fine-tuning, we set the training steps and warm-up steps as 20K and 2K, respectively. The other hyper-parameters are identical to those of pre-training phase. For each fine-tuning experiment, we determine the batch size as 16, and whole training process requires one single Nvidia V100 GPU. 

 Main Results We first testify the effectiveness of our approach by comparing with the results from the WMT2020 Metrics Task submissions. To be fairness, all of the model based metric baselines are trained on the WMT2015-2019 dataset. As shown in Table  1 , comparing to baselines, our RoBLEURT achieves the best performance on cs/de/ja/ru/zh/iu/pl/ta-to-en settings, and achieves competitive results on km-to-en and ps-to-en. 

 Ablation Studies 

 Model Pedestal and Size We first investigate the impact of model pedestal for metric task. As shown in Table  3 , using RoBERTalarge instead of RoBERTa-base model as the base of RoBLEURT-SRC model gives a better performance. Furthermore, using the fine-tuned checkpoint RoBERTa-large-xnli can further improves the performance. This indicates our view, that powerful pre-trained LM, as well as the carefully reoptimized variants, can boost the performance of fine-tuned metric model. 

 Pre-training To identify the improvement after introducing extra pre-training steps for metric model, we conduct the results in Table  4  for comparison. As seen, the performance drops significantly without pre-training phase. This caters to the previous findings  (Sellam et al., 2020) , where pre-training with pseudo data helps the supervised learning of metric model. 

 Data Denoising Strategy As reported in  (Sellam et al., 2020) , the WMT2018-2019 DA subsets are bothered with noisy labels. We also investigate the impact of those data, whether introducing them into model training, or even clean them via rescoring with stronger metric. We thus arrange such ablation study during finetuning, and results are conducted in   of full training set (237K vs. 247K), the performance of RoBLEURT model trained without these noisy items does not diminish significantly. After rescoring with available checkpoints, these data segments further improves model performance. 

 Model Combination We first identify whether introducing source side information to metric model helps training. As seen in Table  2 , accepting source (row RoBLEURT-SRC) than not (row RoBLEURT-NOSRC) as extra input significantly improves the correlation scores. However, for low-resource scenarios, experimental results show that source-side information does not help much for model training. This indicates that source information does not provide help for model training over low-resource scenarios, as the inadequacy of pre-training data may harms model training if source side is introduced. To derive better performance, one general idea is to combine several well-trained models during inference. We also explore whether combining both RoBLEURT-SRC and RoBLEURT-NOSRC models can give better performance. As shown in Table  2 , directly averaging scores from multiple models lead to a significant performance drop. On the contrary, our model, which takes models over both RoBLEURT-NOSRC and RoBLEURT-SRC settings can effectively leverage the predictions, achieving significant performance gain across all language pairs. 

 Conclusion In this paper, we describe our submission metric -RoBLEURT, from the perspective of combining multilingual and monolingual language model, continuous pre-training with the massive synthetic data pairs, and fine-tuning with data denoising strategy. Experimental results confirms the effectiveness of our pipeline, demonstrating state-of-the-art correlations with the WMT2020 human annotations upon 8 out of 10 to-English language pairs. Table 5 . 5 Although the noisy portion contributes a great share Model cs de ja ru zh iu km pl ps ta RoBLEURT-NOSRC 13.5 46.9 27.4 10.8 14.8 28.2 30.6 8.3 14.7 25.0 RoBLEURT-SRC 14.1 47.9 28.7 11.7 14.9 27.5 29.9 6.4 16.0 24.0 RoBLEURT 15.2 49.3 29.1 17.3 17.7 29.0 31.4 13.2 20.1 25.4 
