title
Small Model and In-Domain Data are All You Need

abstract
I participated in the WMT shared news translation task and focus on one high resource language pair: English and Chinese (two directions, Chinese to English and English to Chinese). The submitted systems (ZengHuiMT) focus on data cleaning, data selection, back translation and model ensemble. The techniques I used for data filtering and selection include filtering by rules, language model and word alignment. I used a base translation model trained on initial corpus to obtain the target versions of the WMT21 test sets, then I used language models to find out the monolingual data that is most similar to the target version of test set, such monolingual data was then used to do back translation. On the test set, my best submitted systems achieve 35.9 and 32.2 BLEU for English to Chinese and Chinese to English directions respectively, which are quite high for a small model. 
 Monolingual Data Filtering Using Language Model In terms of monolingual data, I collected more than 20 million Chinese sentences and more than 15 million English sentences from various websites.

Introduction I participated in the WMT shared news translation task and focus on the English and Chinese language pair. This language pair is challenging due to the plentiful in-domain bitext training data and abundant monolingual data. High resource means fierce competition, many high-tech companies and universities chose this language pair also. My neural machine translation system is developed using base transformer  (Vaswani et al., 2017)  architecture and the toolkit I used is THUMT  (Zhang et al., 2020) . Rules and word aligning model are used to clean parallel data. Language model is used to clean monolingual 1 http://mteval.cipsc.org.cn:81/agreement/description data. I use a base transformer  (Vaswani et al., 2017)  architecture since I have only one GPU. The following techniques are used on model training: a. Increase the number of encoder layers to 12 to further improve the encoder's representation capability; b. Back translation  are applied to fully utilize the monolingual corpus. c. Shared vocabulary is used for better performance. d. Four different models using diversified data are trained for ensemble decoding. 

 Data Filtering and Selection The parallel data is mainly from CCMT Corpus 1 , and the monolingual data is collected from the internet. I did not use any other datasets since I think they are not highly related to this news translation task. To evaluate my model's performance, I merged the test set from WMT2017 to WMT2020 to build a big development set. The following rules are used for a simple cleaning: ?Remove duplicated sentences. ?Remove the sentences containing special characters. ?Remove the sentences containing html addresses or tags. Afterwards, language models are used to filter the monolingual data. For English sentences, lmscorer 2 is used to calculate a score for each sentence, which is the mean of tokens' probabilities. The pre-trained model used for English is GPT-2  (Radford et al., 2019) .  3  For Chinese sentences, a pre-trained Chinese GPT-2  (Radford et al., 2019)  4 model is used to calculate a score for each sentence. Then, the English and Chinese sentences are filtered by their scores. GPT-2  (Radford et al., 2019 ) is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. GPT-2  (Radford et al., 2019)  is trained with a simple objective: predict the next word, given all of the previous words within some text. The threshold I used is determined based on my personal evaluation on the text. After calculating the scores for all the sentences, I sampled the sentences by their scores and perform a language quality check. I started from the extremely low scores and the extremely high scores, and then gradually move the scale from the two ends to the middle until I find that the language quality is up to my standard. There are about 16 million Chinese sentences and 10 million English sentences left after filtering using language model. 

 Parallel Data Filtering Using Rules For CCMT parallel Corpus and synthetic parallel corpus from back translation, I used the following rules to filter data. a. Remove duplicated sentence pairs. b. Remove the lines having identical source and target sentences. c. Remove the sentence pairs containing special characters. d. Remove the sentence pairs containing html addresses or tags. e. Remove the sentence pairs with empty source or target side. 

 Parallel Data Filtering Using Word Alignment In order to get word alignment results, fast_align  (Dyer et al., 2013)  is used on the CCMT Corpus filtered by rules, then extract-lex 5 is used to generate bilingual phrase tables. The phrase tables are then pruned according to probabilities. Afterwards, I use the pruned phrase table to measure the confidence of the sentence pairs being mutual translations. The confidence score is calculated like this: check each token of the target sentence to find if it has a counterpart in the source side, then perform this operation in the reverse direction, the final confidence score is calculated by summing up the two percentages from two respective directions and then getting the average. Then the confidence score is used to remove bad sentence pairs. The sentence pairs with confidence scores below 0.6 are discarded. In this way, I finally got a high quality parallel CCMT Corpus. 

 System Description This section illustrate how I train the model step by step. 

 Data pre-processing For data preprocessing, I use the tokenizer developed on my own to process both Chinese and English. Chinese text (including punctuations and numbers) is split to single character level. I keep the upper and lower case letters of English as they are, since I believe they are also important features for the model. Numbers in English text are also split into single digits. I use byte pair encoding (BPE)  to create a shared vocabulary, so that the vocabulary size is reduced to 45467. I also wrote a post-processor to restore the Chinese and English text to normal form. 

 Normal Model Training To As shown in Table  1 , using the same filtered CCMT Corpus, the BLEU scores of models with deeper encoder (12-layer-encoder, 6-layerdecoder) are slightly higher than that of the base version. Back translation  is a useful data augmentation technique to boost model performance with target side monolingual data. The technique starts from training a target to source translation model using initial bilingual corpus, which is later used to translate the monolingual data in the target language back to source language. Then the synthetic backtranslated corpus is concatenated with the original bilingual corpus to train the source to target translation model. After the source to target model is enhanced, the same method can be applied again to train the back-translation system in the reversed direction. I repeat this process using half of the filtered monolingual data for several iterations until the BLEU is not increasing. 

 Training on In-domain Data BERT  (Devlin et al., 2019)  is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. Before feeding word sequences into BERT  (Devlin et al., 2019) , 15% of the words in each sequence are replaced with a [MASK] token. The model then attempts to predict the original value of the masked words, based on the context provided by the other, nonmasked, words in the sequence. After the WMT2021 test set was released, I first translated the Chinese and English test sentences to target versions using the above models, then I generated feature representations for the target versions of the test sentences using pre-trained English BERT  (Devlin et al., 2019)  6 and Chinese BERT  (Devlin et al., 2019 ) 7 models. The example representations are shown in Figure  1  and Figure  2 . Then, the similarity scores are used to find out monolingual sentences that are most similar to the WMT2021 test set. For each test set sentence, hundreds of monolingual sentences are extracted. In order to determine a threshold score, I randomly sampled 100 test set sentences and their extracted counterparts. Then I checked their similarities and scores using my personal linguistic competences in these two languages. The determined threshold score was then used to automatically extract indomain data. Finally, I extracted around 550 thousand Chinese sentences and 420 thousand English sentences as in-domain monolingual data. These sentences are then divided into four equal portions. On the basis of the best models using back translation and the first half of monolingual data, I use four portions of in-domain English data and four portions of in-domain Chinese data to do back translation until the BLEU stops increasing. Therefore, I get four in-domain English to Chinese and four in-domain Chinese to English translation models. These models are then ensembled to build two most powerful models for each direction. 

 Results The BLEU scores on the aforesaid big development set (I merged the test set from WMT2017 to WMT2020 to build a big development set) for each corpus plus model combination are shown in Table  1 . On the WMT 2021 test set, my best submitted systems achieve 35.9 and 32.2 BLEU for English to Chinese and Chinese to English directions respectively, which are even higher than most of the systems from famous high-tech companies. 

 Conclusion This paper describes Hui Zeng's translation systems (ZengHuiMT) for the WMT2021 news translation shared task. The potential of small model plus in-domain data is explored. I am pleased to argue that, with high quality in-domain data, small model could achieve BLEU scores comparable to that of huge models. Figure 1 : 1 Figure1: The BERT representation of "I like this competition very much", the tensor shape is[1, 9 , 768]     
