title
Enlivening Redundant Heads in Multi-head Self-attention for Machine Translation

abstract
Multi-head self-attention recently attracts enormous interest owing to its specialized functions, significant parallelizable computation, and flexible extensibility. However, very recent empirical studies show that some selfattention heads make little contribution and can be pruned as redundant heads. This work takes a novel perspective of identifying and then vitalizing redundant heads. We propose a redundant head enlivening (RHE) method to precisely identify redundant heads, and then vitalize their potential by learning syntactic relations and prior knowledge in text without sacrificing the roles of important heads. Two novel syntax-enhanced attention (SEA) mechanisms: a dependency mask bias and a relative local-phrasal position bias, are introduced to revise self-attention distributions for syntactic enhancement in machine translation. The importance of individual heads is dynamically evaluated during the redundant heads identification, on which we apply SEA to vitalize redundant heads while maintaining the strength of important heads. Experimental results on WMT14 and WMT16 English?German and English?Czech language machine translation validate the RHE effectiveness.

Introduction Recently, self-attention network (SAN)  (Lin et al., 2017)  has been applied to various natural language processing tasks. Instead of drawing distanceaware dependencies like recurrent neural network  (Hochreiter and Schmidhuber, 1997)  and convolutional neural network  (Kim, 2014) , SAN captures short-and long-range relations between elements. SAN involves all signals with a weighted averaging operation, which may incorporate too many unrelated elements to concentrates on specific relations. Recent work has modified SAN to enhance specific relation learning. For example, in  (Shen  Figure  1 : Rationale of the multi-head SAN. The left and middle parts are two existing SAN methods, the right one illustrates our proposed method. The colored circles represent different functions of individual heads.  et al., 2018) , a directional self-attention network (DiSAN) uses one to multiple positional masks to model the asymmetric attention between two elements and capture context-aware relations for all tokens.  modeled the local information by revising the attention distribution with a learnable Gaussian bias to focus on neighboring relations.  (Shaw et al., 2018)  extended SAN to efficiently consider distinct representations of the relative linear position relations between sequence elements. However, the above approaches consider the multi-head SAN as a whole but ignore unbalanced contribution distributions between heads. Furthermore, multi-head SAN combines different attentions from multiple subspaces to construct Transformer  (Vaswani et al., 2017)  and achieves the state-of-the-art results in recent neural machine translation (NMT) tasks  (Hassan et al., 2018) . The very recent work  (Voita et al., 2019)  shows that the encoder-side individual heads in Transformer make different contributions, multi-heads can be classified into important heads and redundant heads and pruning redundant heads does not seriously affect performance. They also assume that important heads play various roles which influence the generated translations to different extents, including syntactic function (focusing on dependent relations), positional function (focusing on neighboring words), and rare words-based function. To date, our understanding of the roles of dis-tinct multi-heads is very limited, with no systematic analysis available of the roles of different heads. In this paper, we precisely identify redundant heads at the encoder-side of Transformer and demonstrate the potential of syntactically reactivating the redundant heads to improve the multi-head SAN performance. Fig.  1  illustrates the different rationales of existing work against ours in multi-head SAN. The left part represents those approaches that directly enhance overall heads as a whole w.r.t. their designed functions but do not differentiate their roles. Such approaches may downplay the functions of important heads and the diversity of the multi-head mechanism. The middle part represents the methods that analyze contributions and functions of multi-head SAN and then prune the determined redundant heads but rely on those important heads only. As shown in the right part, this paper proposes a dynamic and unified strategy to identify redundant heads and then enliven them to fulfill their potential. By enlivening the redundant heads, our approach enhances the performance of redundant heads without sacrificing the essential functions of important heads. In addition, our method further increases the scale of important heads. Specifically, we take NMT as an example to illustrate our method of identifying and reactivating the redundant heads in multi-head SAN. We firstly propose two novel Syntax-Enhanced Attention (SEA) mechanisms for machine translation: 1) the Dependency-Enhanced Attention to use a dependent matrix as mask to model the intensive attention between dependent elements and filter elements without direct dependent relations; and 2) Local-phrase-Enhanced Attention to incorporate a distinct and learnable relative local-phrasal position matrix as bias, which is transformed from a constituency tree under the rules of local-phrase. These syntax-enhanced attention mechanisms simulate the specific functions of important heads but differ from the existing self-attention improvement approaches. Compared to the dependency tree, there is distinct syntactic layer information for each word in the constituency tree, which is extracted to calculate the relative phrasal position to reflect syntactic relations between elements. To this end, we define a novel phrase type local-phrase to only extract syntactically related words as phrase by leveraging the constituency tree, regardless of sequence distance. Further, we propose a dynamic and lightweight Redundant Heads Enlivening (RHE) strat-egy for multi-head SAN to reactivate and enhance the roles of redundant heads. Lastly, a dynamic function gate is designed, which is transformed from the average of maximum attention weights to compare with syntactic attention weights and identify redundant heads which do not capture meaningful syntactic relations in the sequence. We test the above design on three widelyused translation tasks WMT14 and WMT16 English?German and WMT16 English?Czech. Extensive analyses reveal that enlivening redundant heads in multi-head SAN beats improving overall heads, and the proposed syntax-enhanced attention mechanisms with dependency and local phrases further effectively improve the translation performance. 

 Related Work One popular extension to the SAN is to revise attention distribution by static and dynamic biases. Different dimensions of biases have been considered, including directional relation  (Shen et al., 2018)  and localness  (Sperber et al., 2018; Zhang et al., 2018a; .  (Shen et al., 2018)  improves SAN with directional masks and multidimensional features by explicitly revising attention distribution. In this paper, we focus on the explicit syntactic biases by proposing dependencyenhanced attention and local-phrase-enhanced attention. Several papers show that explicitly modeling dependency  (Bastings et al., 2017; Nadejde et al., 2017)  or phrase  (Wang et al., 2017; Huang et al., 2018; Zhang et al., 2018b Zhang et al., , 2020  is useful for tasks such as NMT. Related to our work,  (Strubell et al., 2018)  and  (Hao et al., 2019)  also modify parts of self-attention heads with syntactic information. However, they randomly assign heads instead of analysing the importance and function of each head in advance.  (Sperber et al., 2018)  restricts SAN with the neighboring elements and performs better for longer sequences in acoustic modeling and natural language inference tasks.  leverages Gaussian bias predicted by the query vector to dynamically model the localness for SAN. Other work analyzes the attention weights of different NMT models  (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018) .  (Voita et al., 2019)  considers how different heads correspond to specific relations and proves that redundant heads can be pruned without greatly decreasing translation performance. However, they disregard the full potential of redundant heads as in our SEA.  (Li et al., 2018)  realizes the diversity of multiple attention heads and introduces a disagreement regularization to explicitly encourage the diversity. Nevertheless, they do not realize that only partial individual heads are redundant, which is a prerequisite for optimizing multi-head diversity. In summary, while some of the related work recognizes the approach of revising attention distribution with bias, our work represents the first to propose a complement and precise strategy to analyze individual heads, identify redundant heads and then enliven them with syntactic bias. 3 Background 3.0.1 Multi-head Self-attention Multi-head SAN  (Vaswani et al., 2017; Shaw et al., 2018; Shen et al., 2018;  projects the input sequence to multiple subspaces (h attention heads), applies the scaled dot-product attention to the hidden states in each head, and then concatenates the output. For each self-attention head head i (1 ? i ? h) in the multi-head SAN for NMT, given an input sequence x = {x 1 , ..., x n }, each hidden state in the l-layer is constructed by attending to the states in the (l ? 1)-th layer. Specifically, the hidden states of (l ? 1)-th layer H l?1 ? R n?d h are firstly transformed into the queries Q ? R n?d h , the keys K ? R n?d h , and the values V ? R n?d h with three separate weight matrices, where d h represents the dimensionality of each head. The hidden state H i of the l-th layer is calculated as: H l i = n j=1 Att(Q i , K j )(V j W V ) (1) where Att(?) is a scaled dot-product attention model, defined as: Att(Q i , K j ) = sof tmax (x i W Q )(x j W K ) T ? d k (2) where ? d k is the scaling factor with d being the dimensionality of layer states. 

 Multi-head Analysis In  (Voita et al., 2019) , a "confidence" scalar h conf is calculated as the average of maximum attention weights of all n source tokens in one head: h conf = 1 n n i=1 M ax (Att(Q i , K j )) (3) M ax (Att(Q i , K j )) represents the maximum attention weight to x i among all source tokens x j in the sequence. Further, a fixed gate value f gate (0 < f gate ? 1) is given that judges a head as important if h conf > f gate for all training examples and epochs. In addition, three head functions are identified according to the frequency of maximum attention weight assigned to a specific position: syntactic function, positional function, and rare words function. 4 The RHE Design DEA is a syntactic extension of standard selfattention. DEA focuses on the internal dependency between elements. We place a dependency mask bias d to the logit similarity in Eq. (2): Att(Q i , K j ) = sof tmax (x i W Q )(x j W K ) T ? d k + D i,j 1 (4) Softmax Softmax V V 

 Multi-Head Hidden States Output ... Given a dependency mask D ? {0, ?} n?n , we set the bias d to a constant vector D i,j 1 in Eq. (  4 ), where 1 is an all-one vector. Note that, due to the exponential operation in the softmax function, adding the alignment score with a bias d ? {0, ?} n?n approximates to multiplying the attention distribution by a weight ? [1, 0). 

 Redundant Heads Important Heads 

 Redundant Heads Identification To encode the dependency information into this mask, we define the value of D i,j according to head-dependent relations Dep (x i , x j ) between elements x i and x j : D i,j = 0, x i , x j in Dep (x i , x j ) or i = j ?, x i , x j not in Dep (x i , x j ) (5) In fact, Eq. (  5 ) shows that we ignore the relations between independent word pairs (x i , x j ) by set D i,j = ?; meanwhile, the attention weights are more concentrated on dependent word pairs. By assuming each dependent relation to be equally important, we do not assign different biases for different dependency word pairs by set D i,j = 0. This enhances the ability of self-attention to capture dependent relations. 

 LPEA: Local-phrase-Enhanced Attention LPEA includes a distinct and learnable syntactic bias to revise the attention weights. A local-phrase bias p represents relative phrasal position information between x i and x j (x j ? local_phrase(x i )). Meanwhile, it masks the attention between words not in local_phrase(x i ). Similar to DEA, we modify Eq. (  2 ) as: Att(Q i , K j ) = sof tmax (x i W Q )(x j W K ) T ? d k + P i,j (6) We further introduce the concept of local-phrase obtained from the constituency tree in terms of two rules, different from general phrases which mostly consist of neighboring words. A local-phrase contains syntactically related words regardless of sequence distance, hence local-phrase carries the distinct and hierarchical syntactic relations between elements. ? Rule 1: Given a constituency tree with m layers, the word x i and its ancestor node sequence ast = (ast layer(x i )?1 , ..., ast 0 ), we assume that its local_phrase(x i ) contains words which belong to the lowest multi-descendant ancestor ast layer(x i )?m (0 ? m ? layer(x i )). ? Rule 2: If word x i ? local_phrase(x j ) (j < i) according to Rule 1, we assume that word x j ? local_phrase(x i ). To obtain the local-phrase bias p, we firstly extract a relative phrasal position matrix RP from the constituency tree. As Fig.  3  shows, first, given a matrix of RP ? R n?n , where each element represents the relative syntactic distance between words x i and x j . Then, for words x i and x j not in the same local-phrase (e.g. "Sharon" and "talk"), we set the relative position as ? (3 th row, 6 th column). Finally, for words which in a local-phrase, such as "held" and "talk", we calculate the relative phrasal position distance according to their relative phrase layer (Layer 3 ? Layer 4 = ?1) and set the RP 2,4 = 1. Accordingly, we obtain the matrix RP. As the RP matrix cannot be directly encoded in attention distribution, inspired by  (Shaw et al., 2018) , We use a group of vectors to represent the relative phrasal position between words in RP. Considering that the precise relative phrasal position information beyond a certain distance is not useful, the maximum relative phrasal position is clipped to a maximum absolute value of k. Therefore, we consider 2k + 1 unique edge labels for relative phrasal position vectors and transform the integral matrix RP into the corresponding vector matrix M ? R n?n?d h , where: M ij = w clip(j?i,k) clip(x, k) = max(?k, min(k, x)) (7) Then, we learn the relative phrasal position representations w = (w ?k , ..., w k ), where w i ? R d h . After obtaining the matrix M, we apply a feedforward network to transform the relative localphrasal position vector M ij to a relative localphrasal position hidden state. It is further mapped to a negative scalar P ij of local-phrase bias matrix p by a linear projection U P ? R d h ?1 , namely: P = ?| tanh(W P M + b P )U P | (8) W P ? R d h ?d h and b P ? R d h are model parameters. Fig.  3  shows the process of extracting relative local-phrase bias p from the constituency tree. -  4.2 Incorporating SEA into Multi-head Self-attention ? dh 0 0 1 1 1 2 0 0 1 1 1 2 0 0 -1 -1 ? ? 0 0 -1 -1 ? ? 0 -1 -1 1 ? ? 0 -1 -2 ? -2 ? Bush 

 Redundant Head Identification We enhance the syntactic function of self-attention heads by dynamically identifying the redundant heads that lack the ability of capturing both shortand long-term syntactic relations to enhance these heads by incorporating SEA. We firstly apply the dependency mask Dep_mask to the attention weight matrix to obtain the corresponding syntactic attention weights which reflect short-and long-term syntactic relations. Then, we sum the syntactic attention weights for each x i among all syntax-related source tokens x j in the sequence. Finally, we calculate the average of syntactic attention weight scalar Syn attn as follows: Syn attn = 1 n n i=1 n j=1 Dep_mask (Att(Q i , K j )) (9) We propose a function gating criteria: when the average of syntactic attention weights is higher than the average of maximum attention weights, the head is regarded as important and contains syntactic functions. Different from the work in  (Voita et al., 2019)  which simply uses a fixed gate value to measure the importance of individual head for all training examples and epochs, our method dynamically identifies individual heads for each sentence during the training process. We compare syntactic attention weights Syn attn with dynamic and learnable syntactic gate Syn gate transformed from head confidence h conf in Eq. (  3 ) by sigmoid activation functions, i.e., Syn gate = sigmoid(h conf ) to determine the head function. If Syn attn is lower than Syn gate , we treat the corresponding head as redundant. h label = 1, Syn attn > Syn gate 0, other (10) h label represents whether a head is important (h label = 1) or redundant (h label = 0). Another aspect of additional reason for comparing with the head confidence is that some 

 Enlivening Redundant Heads After differing redundant heads from those important ones in the multi-head self-attention, we further enliven the redundant heads with a syntactic bias per Eq. (  4 ) or Eq. (  6 ) without interfering with the important head functions.  (Voita et al., 2019)  shows that redundant heads are mostly distributed in the lower encoder layers, meanwhile  (Hao et al., 2019;  shows that the bottom layer in the encoder, which directly takes word embedding as input, benefits more from modeling local relations. We evaluate the performance of applying our method on the low-and high-level encoder layers in the next section, and obtain the best performance when applying on the first encoder layer. 

 Experiments 

 Settings We carry out experiments on the English?German (En?De) and English?Czech (En?Cs) language translation. For En?De, the classic WMT14 data consists of 4.5M sentence pairs (newstest2013 and newstest2014 as development set and test set), and the WMT16 News Commentary v11 data consists of 0.22M sentence pairs (newstest2015 and newstest2016 as development and test sets). For En?Cs, the WMT16 News Commentary v11 data data consists of approximately 0.18M sentence pairs (newstest2015 and newstest2016 as development set and test set). We evaluate our approach in terms of different languages and data sizes. We use the Berkeley Neural Parser (?) to generate constituency trees for English, and an open-source tool spaCy 1 to parse dependency trees for English. Besides, we make statistical significance test with the method in  (Collins et al., 2005) . The byte-pair encoding (BPE) toolkit 2  (Sennrich et al., 2016)  is used with 32K merge operations. The 4-gram NIST BLEU score  (Papineni et al., 2002)  is used as the evaluation metric. We implement the proposed RHE and all the baselines on top of Transformer model  (Vaswani et al., 2017)  by using open-source toolkit OpenNMT  (Klein et al., 2017) . Please refer to the Appendix for more details of dataset and parameter setting . 

 RHE for NMT Results Table  1  shows the ablation study results of the Transformer enabled by the two proposed SEA mechanisms DEA and LPEA and the RHE approach. First, the Rows of "+DEA" and "+LPE" represent the models with all heads of the first encoder layer, including original important heads, are replaced by the syntax-enhanced attention networks DEA and LPEA respectively. Second, the RHE approach (containing the Rows of "+DEA+RHE" and "+LPEA+RHE" ) significantly lifts both DEA and LPEA mechanisms across all small and large language pairs. This tests the effectiveness of identifying and modifying redundant heads without interfering important head functions. RHE lifts the LPEA, which together i.e. LPEA+RHE substantially outperforms Transformer by +1.0 BLEU points on En?De (WMT16), +0.96 BLEU points on En?De (WMT14), and +0.81 BLEU points on En?Cs (WMT16). These results demonstrate the efficacy and applicability of both SEA and RHE designs. The upper part of Table  1  shows the results of Transformer enabled by two SAN enhancement strategies: the relative position encoding method (Rel_Pos)  (Shaw et al., 2018)  which considers the relative position between sequence elements, and the modeling localness (Localness)  method which enhances the ability of capturing local context for self-attention with a learnable Gaussian bias. While both Rel_Pos and Localness make improvement over Transformer owing to their strategies of enhancing SAN, our DEA, DEA+RHE, LPEA and LPEA+RHEenabled Transformers substantially and consistently beat the standard Transformer and both Rel_Pos and Localness-enhanced Transformers. For example, our DEA+RHE on Transformer outperforms Rel_Pos by over 0.49 BLEU points on En?De (WMT16), 0.29 BLEU points on En?De (WMT14), and 0.36 BLUE points on En?Cs (WMT16). This is owing to the SEA and RHE design of assigning a distinct syntactic bias for each word and modeling both short-and long-term syntactic relations. 

 RHE Mechanism Analysis Here, we analyze the RHE generalizability, the impact of different factors, and the visualization of multi-head attention matrices. Owing to space limitation, we only report the testing results on the En?De (WMT16) set, and explore the influence caused by syntax parsing quality and applied encoder layers in Appendix. 

 The RHE Applicability Table  2  shows that RHE lifts Rel_Pos and Localness by +0.28 and +0.20 BLEU point respectively. This proves (1) RHE is general and can enhance other multi-head SAN; and (2) the necessity of preserving important heads while improving multi-head self-attention mechanisms. By pruning redundant heads, the experiment also shows that RHE can precisely identify redundant heads and the RHE-enabled Transformer only drops 0.1 BLEU point after pruning the identified redundant heads, meanwhile the training speed improves slightly. This shows the importance of precisely identifying redundant heads, and only by then pruning redundant heads would trivially affect the learning performance as shown in  (Voita et al., 2019) .   

 Systems 

 Selection of Multi-head Function Gate Two strategies can be used to select the multi-head function gate: one is a fixed gate by a constant number throughout the whole training process; the other is a dynamic gate transformed from the average of maximum attention weight c of an individual head, which provides a flexible criteria to determine the head function. Fig.  4  shows the comparison between multiple fixed gate values and the dynamic gate. We adjust the value of the fixed gate in a range (0.1, 0.5)  3  . The results show that the dynamic gate strategy significantly outperforms all fixed gate values. The performance becomes unstable when the fixed gate value increases. Self-attention heads develop their ability to capture syntactic relations during the training epochs; accordingly, the average syntactic attention weights Syn attn increase gradually. Low fixed gate value reduces the recall of RHE because Syn attn goes high in later epochs; high fixed gate value reduces the accuracy of RHE as all important heads and redundant heads receive small Syn attn in the initial epochs. Hence, the high fixed gate might mistakenly treat a high portion of heads as redundant. 

 Effect of Maximum Relative Local-Phrasal Position Compared to the dependency tree, the constituency tree characterizes the distinct relative phrasal position for each word, which enriches the syntactic relations between elements. We thus evaluate the effect of varying the clipping distance k of the maximum absolute relative local-phrasal position. The results in Table  3  show that the performance increases with the increase of k from 0 to 6, while   

 Visualization of LPEA+RHE-enlivened Attention To evaluate the effect of LPEA+RHE-enlivened redundant heads against Rel_Pos and Localness, we further visualize the attention matrices of an individual head in the first encoder layer. The source sentence is Relations between Obama and Netanyhu have been strained for years EOS . The improvement between redundant head and LPEA+RHE-enlivened head is shown in Fig.  5 (a ) and (b). In Fig.  5  (a), the distribution of original redundant head attention concentrates more on the end of the sentence (16 th column) but less on the specific meaningful words. In Fig.  5 (b) , SEA masks those words that do not belong to the localphrase in each row and improves the attention in local-phrase: 1) 'have been ... for years' in rows 8 and 9, which is a long-distance and discontinuous phrase; 2) SEA strengthens the attention between 'Relations' and 'Obama', 'Netanyhu' in the 1 st row, which has the nmod dependency. ..and' in the 6 th row. However, the attention may focus on the word itself sometimes, such as the high attention weights of 'Relations' ('R el ations' in the subword form) in the 1 st column and 'strained' ('st ra in ed' in the subword form) in the 11 th column. In contrast, LPEA+RHE enlivens the redundant head by modeling the latent syntactic localness beyond the constraints of sequence distance. Fig.  5  (e) shows the attention matrix of an important head, which focuses on neighboring words. This result is consistent with the previous findings in  (Voita et al., 2019) . 

 Conclusions While multi-head self-attention networks show a significant potential in improving learning tasks such as NMT, an open challenging topic is to quantify the redundancy and importance of each head and further improve the weak heads. This paper makes one step forward by not only precisely analyzing and identifying redundant heads but introducing a dynamic redundant heads enlivening (RHE) mechanism to identify and enliven each redundant head toward full potential without affecting the function of other important heads as in alternatively enhancing all heads. The proposed dependency-enhanced attention and local-phraseenhanced attention effectively capture the different syntactic relations between elements. We'll work on strategies to integrate DEA and LPEA in future. guages, with about 0.30 BLEU points improvement. We think that the improvement of parsing and translation is owing to that the neural-based parser leverages Transformer as encoder to represent the sentence. Although exploring the best performance of parsing tools is not the focus of this work, we believe that, with higher quality of parsing tool, our SEA mechanisms have more potential to represent the syntactic bias for self-attention network.  

 Metric Fig. 2 2 Fig.2shows the architecture of our proposed redundant heads enlivening (RHE) approach to identify redundant heads and then enliven them by revising self-attention distributions with a syntactic bias. RHE takes full advantage of the multi-head SAN by capturing both dependent and distinct phrasal relations. First, two Syntax-Enhanced Attention (SEA) mechanisms: Dependency Enhanced Attention (DEA) and Local-phrase Enhanced Attention (LPEA), are proposed. DEA disables the attention between elements without dependencies by leveraging the dependency mask, and LPEA precisely regulates the self-attention distribution by a distinct and learnable local-phrase bias. The bias represents relative local-phrasal position transformed from a constituency tree. LPEA precisely captures both short-and long-term syntactic relations. Second, the Redundant Head Identification module dynamically determines the importance and function of each head during the training process per the average sum of syntactic attention weights. Lastly, the self-attention of redundant heads is replaced by SEA to enliven their full potential and roles.4.1 SEA: Syntax-Enhanced Attention 4.1.1 DEA: Dependency-Enhanced Attention 

 Figure 3 : 3 Figure 3: The process of extracting relative phrasal position bias. 

 Figure 4 : 4 Figure 4: Comparison between fixed syntactic function gate values and dynamic syntactic function gate (the red line), and the black dashed line represents the baseline. 

 Figure 5 : 5 Figure 5: Visualization of attention matrices of the same input sentence and the same encoder layer.The darker color of a cell represents higher attention weight of the source token. 

 Fig. 5 (c) and (d) shows the results of Rel_Pos and Localness, both explicitly models the locality for self-attention networks. Both of their attention weights mainly distribute along the diagonal and some short-range elements. Rel_Pos captures the phrase 'have been' in rows 8 and 9 but ignores long-range phrase elements 'for years' since the influence of relative position representation decays as the sequence distance increases. In Fig. 5 (d), the attention weight distribution of Localness is more flexible because they assign a distinct Gaussian bias to each position, which pays more attention to the local syntactic context. It captures the phrase 'between. 

 2 Relative Phrasal Position Matrix Constituency Tree Layer 0 S Layer 1 NP VP Layer 2 NNP VBD NP PP Extraction Layer 3 Bush held DT NN IN NP Layer 4 a talk with NNP Layer 5 Sharon M2,2 ... Mn,2 M2,2 M2,1 ... Mn,1 ... ... M2,1 Mn,2 Mn,1 M2,2 ... ... M2,1 Mn,2 Mn,1 P1,2 P2,2 ... ... ... . . . ... . . . ... ... . . . ... P1,n P2,n Pn,n ... ... ... ... . . . P2,1 ... Pn,2 Pn,1 P1,1 dh -abs Linear Projection 1 dh dh M1,1 ... M2,1 Mn,1 M1,2 M2,2 ... Mn,2 M2,2 ... ... M2,1 Mn,2 Mn,2 Mn,1 ... ... ... . . . ... ... . . . ... ... Mn,n ... Mn,2 Mn,1 Mn,1 M1,1 M1,2 M2,2 ... ... ... ... . . . M2,1 ... M1,1 M1,2 M1,n M2,2 M2,n ... ... ... ... . . . M2,1 ... Mapping Clipping Relative Phrasal Position Bias Matrix Vector Matrix 

 Table 1 : 1 Test results of SEA and RHE against baseline SAN-enhanced Transformer for NMT on WMT16 and WMT14 En?De, and WMT16 En?Cs. "# Para" denotes the trainable parameter size of each model (M = million). Symbols "? /?" refer to the improvement significance level over the self-attention baseline (p < 0.05/0.01) tested by bootstrap resampling. Architecture En?De (WMT16) En?De (WMT14) En?Cs (WMT16) #Para BLEU #Para BLEU #Para BLEU Transformer 71.82M 25.28 88.00M 27.31 70.02M 15.46 + Rel_Pos 71.85M 25.49 88.10M 27.53 70.05M 15.60 + Localness 71.84M 25.53 88.80M 27.61 70.04M 15.65 + DEA 71.82M 25.75 88.10M 27.71 ? 70.02M 15.84 ? + DEA + RHE 71.82M 25.98 ? 88.10M 27.82 ? 70.02M 15.96 ? + LPEA 71.82M 25.90 ? 88.10M 27.96 ? 70.02M 15.97 ? + LPEA + RHE 71.82M 26.28 ? 88.10M 28.27 ? 70.02M 16.27 ? 

 Table 3 : 3 Results w.r.t. the clipping relative local-phrase layer distance k. 

 Table A3 : A3 Performance of two classical constituency tree parser tools on the Penn Treebank WSJ test set (F1 score) and its corresponding effect on the LPEA+RHE NMT model (BLEU score). Task PCFG Neural F1 WSJ Parsing 91.20 93.55 En-De (WMT16) 25.98 26.28 BLEU En-De (WMT14) 28.02 28.27 En-Cs (WMT16) 15.92 16.27 

			 https://spacy.io 2 https://github.com/rsennrich/subword-nmt 

			 Once the average of syntactic attention weights satisfies Synattn > 0.5, it is higher than the average non-syntactic attention weights, hence we assume that the head is functional.
