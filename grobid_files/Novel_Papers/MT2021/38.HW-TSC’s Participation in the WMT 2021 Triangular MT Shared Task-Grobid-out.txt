title
HW-TSC's Participation in the WMT 2021 Triangular MT Shared Task

abstract
This paper presents the submission of Huawei Translation Service Center (HW-TSC) to WMT 2021 Triangular MT Shared Task. We participate in the Russian-to-Chinese task under the constrained condition. We use Transformer architecture and obtain the best performance via a variant with larger parameter sizes. We perform detailed data pre-processing and filtering on the provided large-scale bilingual data. Several strategies are used to train our models, such as Multilingual Translation, Back Translation, Forward Translation, Data Denoising, Average Checkpoint, Ensemble, Fine-tuning, etc. Our system obtains 32.5 BLEU on the dev set and 27.7 BLEU on the test set, the highest score among all submissions.

Introduction This paper introduces our submission to the WMT21 Triangular task. We adopt Transformer  (Vaswani et al., 2017)  architecture and strictly obey the constrained condition in terms of data usage. On one hand, we perform multiple data filtering strategies to enhance data quality; on the other hand, we leverage multilingual model  (Johnson et al., 2017) , pivot language, forward  and back translation  (Edunov et al., 2018) , and data denoising  (Wang et al., 2018)  strategies to further enhance training effects. In addition, we also adopt fine-tuning  (Sun et al., 2019)  and ensemble  (Garmash and Monz, 2016) , two widely used strategies, to further enhance system performance. We compare and contrast different strategies based on our experiment results and give our analysis accordingly. The overall training process is illustrated in Figure  1 . Section 2 mainly focuses on our training techniques, including model architecture, data processing and training strategies. Section 3 describes our experiment settings and training process. Section 4 presents the experiment results while section 5 analyze how our multilingual, data denoise and data augmentation strategies influence system performances. 

 Method 

 Model Architecture Our system uses Transformer  (Vaswani et al., 2017)  model architecture, which adopts full self-attention mechanism to realize algorithm parallelism, accelerate model training speed, and improve translation quality. In this shared task, Transformer-Deep  is used, which features 35-layer encoder, 6-layer decoder, 768 dimensions of word vector, 3072-hidden-state, 16-head self-attention, and pre-norm. 

 Data Processing an Augmentation We strictly comply with the constrained condition and use only the officially provided data. 

 Data Filtering We perform the following steps to cleanse all data: ? Filter out repeated sentences  (Khayrallah and Koehn, 2018; . ? Convert XML escape characters. ? Normalize punctuations using Moses  (Koehn et al., 2007) . ? Delete html tags, non-UTF-8 characters, unicode characters and invisible characters. ? Filter out sentences with mismatched parentheses and quotation marks; sentences of which punctuation percentage exceeds 0.3; sentences with the character-to-word ratio greater than 12 or less than 1.5; sentences of which the source-to-target token ratio higher  than 3 or lowers than 0.3; sentences with more than 120 tokens. ? Apply langid  (Joulin et al., 2016b,a)  to filter sentences in other languages. ? Use fast-align  (Dyer et al., 2013)  to filter sentence pairs with poor alignment, about 10% of the data is filtered. We perform the additional steps to process Chinese data: ? Convert traditional Chinese characters to simplified ones. ? Convert fullwidth forms to halfwidth forms. Data sizes before and after cleansing are listed in Table  1 . 

 Data Augmentation Back-translation  (Edunov et al., 2018)  is an effective way to boost translation quality by using monolingual data to generate synthetic training parallel data. As described in , similar to back translation, the monolingual corpus in source language can also be used to generate forward translation text with a trained MT model, and the generated forward and backward translation data can both be merged with the authentic bilingual data. This strategy can increase the data size to a large extent. Since there is no officially provided monolingual data, we use the target side of en2zh data and the source side of zh2ru data filtered out in section 2.2.1 for back translation. We adopt the top-k sampling method. Then, we use the source side of ru2en data for forward translation, which is done based on beam search. Through sampling, we ensure that the sizes of data generated by forward and back translation are relatively equal. In this paper, we refer to the combination of forward and sampling back translation as FTST. 

 Filter Using LaBSE Apart from the commonly used data cleansing methods, we also explore other techniques based on neural networks. LaBSE  (Feng et al., 2020)  is a multilingual BERT embedding model that can measure semantic similarities across languages. In our experiment, we notice that traditional data cleansing methods described in section 2.2.1 are unable to produce high-quality data, so we further filter the data using pre-training model LaBSE. For all parallel data, we calculated the similarity scores and For forward model, the training is divided into three steps: 1) Use all official provided data in three directions (ru2zh, en2zh, and ru2en) for training; 2) Use all clean data selected by LaBSE for incremental training; 3) Finally, use ru2zh clean data selected by LaBSE for incremental training. For backward model, we only perform two steps: 1) Use all data (en2ru, zh2ru) for training; 2) Use zh2ru clean data selected by LaBSE for incremental training. 

 Fine-tuning and Ensemble To achieve better results, fine-tuning with smallsize in-domain data is necessary  (Sun et al., 2019 ). An effective strategy for fine-tuning is to leverage the dev set available in this task. The fine-tuning strategies employed in our experiment include: 1) Add noise to the target side of the dev set to generate synthetic training data  (Meng et al., 2020) ; 2) Use multiple models to generate synthetic data through beam search decoding, and then add synthetic data to the dev test for fine-tuning. Model ensemble is also a widely used technique in previous WMT workshops  (Garmash and Monz, 2016) , which can boost the performance by combining the predictions of several models at each decoding step. We selected the best four models from the six we trained for ensemble. 

 Settings 

 Experiment Settings We use the open-source fairseq  (Ott et al., 2019)  for training, and use sacreBLEU  (Post, 2018)  to measure system performances instead of the BLEU script mentioned in the task. The main parameters are as follows: Each model is trained using 8 GPUs. The size of each batch is set as 2048, parameter update frequency as 32, learning rate as 5e-4  (Vaswani et al., 2017)  and label smoothing as 0.1  (Szegedy et al., 2016) . The number of warmup steps is 4000, and the dropout is 0.1. We employ joint sentencepiece model  (Kudo and Richardson, 2018; Kudo, 2018)  for word segmentation, with the size of the vocabulary set to 32k. Jieba tokenizer is used for Chinese word segmentation while Moses tokenizer for English and Russian word segmentation. The three languages share a vocabulary of 45K words. In the inference phase, we use the opensource marian  (Junczys-Dowmunt et al., 2018)    

 Training Process We combine multi-stage denoising training with data augmentation methods. Figure  1  illustrates our training process: 1) We cleanse the training data using methods mentioned in 2.2.1 and train three forward models and one backward model. 2) We further denoise data using LaBSE (as mentioned in 2.2.3) and conduct denoising training until the model converge on the dev set. 3) We perform data augmentation as described in 2.2.2. We collect a total of 45M Russian monolingual data and split them into three sets, each with 15M sentences. We use three different forward models to generate three sets of training data. Hoping to add diversity to incremental training, we use the data synthesized by one model to train the other two models. For example, we use the synthetic data generated by forward model A to incremental train forward model B, C and so on. We also collect a total of 15M Chinese monolingual data and back translate the data using the backward model. We repeat back translation for three times and obtain three sets of back translation data. We incrementally train six models using the above synthetic data. 4) We average the last 5 checkpoints of each model and select the best four from the six models we trained for final ensemble. 

 Experiment Result Our overall training strategy is to train a baseline model, conduct incremental training with techniques such as multilingual model, denoise training, data augmentation, and fine-tuning. Our submitted results come from ensembled models.  achieves an increase of 5.9 BLEU. Our baseline model is trained with data processed with methods mentioned in section 2.2.1. The BLEU score of the baseline model on the dev set is 26.6. Comparing with the baseline model, our multilingual strategy leads to a huge improvement of 2.7 BLEU. Our simplified denoising training strategy contributes to an increase of 0.7 BLEU. It should be noted that data augmentation techniques (FTST method and LaBSE denoising on ru2zh data) also result in a significant increase of 1.9 BLEU. Finally, an increase of 0.6 BLEU is gained via ensemble. Our submitted system gain 32.5 BLEU on the dev set, which demonstrate the effectiveness of our multiple strategies. According to the organizer's feedback, our submitted model gains 27.7 BLEU on the WMT21 test set. 

 Analysis 

 Multilingual Model and Model Performance Our experiment results demonstrate that multilingual model has positive effects on system performance. We have experimented on different multilingual models and compare their results.   

 Denoising Training and System Performance Our experiment also demonstrates the contribution of denoising training to system performance. Table  4  compares the results of baseline and denoising training model, from which we can see an increase of 1.4 BLEU. We further compare the results measured at the three stages of denoising training. We use the enhanced target and source model to conduct simplified denoising training. Our experiment shows that full-data denoising training leads to an increase of 0.7 BLEU while ru2zh data denoising further leads to an increase of 0.5 BLEU. The experimental results show that the denoise strategy is effective and can lead to at least 1 BLEU improvement even after multilingual model enhancement. 

 Data Augmentation and System Performance Data augmentation strategy also leads to huge BLEU improvements. We try multiple data augmentation strategies, including back translation (BT), forward translation (FT), FTST (2.2.2). Sampling BT means sampling from the model conditional distribution and beam BT means using beam search, when generating synthetic data. Table  5  shows the effects of different data enhancement methods. Our results show that sampling back translation can lead to better results (about 0.3 BLEU in our experiment). We also conduct two forward translation experiments: FT is translating Russian to Chinese directly, and Pivot FT is using English as the pivot language, which achieve only an undesirable result. We then using the FTST method and gain the best result with a BLEU score of 30.5. The experimental results show that the combination of sampling BT and FT data (FTST) can produce the best data augmentation effect. 

 Conclusion This paper presents HW-TSC's submission to WMT21 Triangular Machine Translation Task. In general, we use Transformer architecture and explore multiple data filtering and selection methods. In terms of training and data processing strategies, multilingual model, denoising training, data augmentation, and FTST we used can effectively improve system performance. Our final result achieves an increase of 5.9 BLEU when comparing baseline model on the dev set and gain a BLEU score of 27.7 on the test which is the highest among all submissions. Figure 1 : 1 Figure 1: This figure shows the training process for the WMT 2021 Triangular MT Shared Task, which consists of three stages. In stage 1, three forward models and one backward model are trained. In stage 2, denoise corpus is used to train models incrementally. In stage 3, the synthetic data by FTST and denoise corpus are used to train models incrementally. Finally, model ensemble is used to boost the performance. 
