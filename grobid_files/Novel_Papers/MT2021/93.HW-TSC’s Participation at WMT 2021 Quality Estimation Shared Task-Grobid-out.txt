title
HW-TSC's Participation at WMT 2021 Quality Estimation Shared Task

abstract
This paper presents our work in WMT 2021 Quality Estimation (QE) Shared Task. We participated in all of the three sub-tasks, including Sentence-Level Direct Assessment (DA) task, Word and Sentence-Level Post-editing Effort task and Critical Error Detection task, in all language pairs. Our systems employ the framework of Predictor-Estimator, concretely with a pre-trained XLM-Roberta as Predictor and task-specific classifier or regressor as Estimator. For all tasks, we improve our systems by incorporating post-edit sentence or additional high-quality translation sentence in the way of multitask learning or encoding it with predictors directly. Moreover, in zero-shot setting, our data augmentation strategy based on Monte-Carlo Dropout brings up significant improvement on DA sub-task. Notably, our submissions achieve remarkable results over all tasks. * Indicates equal contribution. 1 http://www.statmt.org/wmt21/quality-estimationtask.html

Introduction Quality Estimation (QE) focuses on estimating the quality of machine translation (MT) system output when no ground truth reference is available  (Specia et al., 2018) . QE covers wide range of tasks including word-level, sentence-level and document-level. It has wide range of applications in MT quality check and post-editing effort estimation. In WMT2021 Quality Estimation shared task 1 , there are three sub tasks -Sentence-Level Direct Assessment task, Word and Sentence-Level Postediting Effort task and Critical Error Detection task. Each sub task involves several language pairs. Our team participated in all the above three tasks over all language pairs. We summarized our main contributions as follow: ? We employ Predictor-Estimator architecture  (Kim et al., 2017b; Kim and Lee, 2016)  which is a two-stage model consisting of a word prediction model trained from large-scale parallel corpora, and a estimation model trained from quality-annotated QE data. Different from the original Predictor-Estimator model in  (Kim et al., 2017a) , we use pre-trained XLM-Roberta large as predictor instead of RNN-based model to achieve better QE features, and use task-specific classifier or regressor as quality estimator. ? We extend PE assisted QE (PEAQE)  (Kepler et al., 2019; Wang et al., 2020)  by integrating real PE or addtional high-quality translation in the way of multitask learning or directly encoding it with predictor. ? We explore data augmentation method based on Monte Carlo (MC) dropout  (Gal and Ghahramani, 2016)  to enhance the performance of zero-shot language pairs in Direct Assessment(DA) task. Our methods achieve impressive performance on both word and sentence level tasks. Specifically, we peak the top-1 on sentence-level DA over English-German and English-Japanese pairs. For word and sentence-level post-editing effort task, our submissions of the majority language pairs obtain the best Pearson's correlation or Matthews correlation coefficient. We also win the first place in critical error detection task in English-Chinese and English-Japanese. We will describe the tasks, datasets, and our methods for DA task, post-editing task, and critical error detection task in section 2, section 3, and section 4 respectively. Section 5 presents details of our experimental setup and results, with a brief discussion and conclusion in the end. 2 Sentence-Level Direct Assessment Task 

 Task Description The sentence-level Direct Assessment task focuses on estimating sentence-level translation quality scores which are annotated with Direct Assessment (DA) scores by professional translators. The original DA scores are in scale of 0-100. The scores are then standardised using the z-score by rater. The goal is to estimate a z-standardised DA score for each translation sentence. Sentence-level DA task is evaluated by Pearson's correlation between the predicted score and the gold human annotated z-standardised DA score. The system is assessed from two aspects: single language pair and multilingual track which takes all languages into account, including zero-shot pairs, calculating the averaged Pearson correlation overall. 

 Dataset For each language, 7000, 1000 and 1000 sentence pairs are provided officially as training, development and test20 set before releasing another 1000 for the real blind test21, including highresource English-German (En-De) and English-Chinese (En-Zh), medium-resource Romanian-English (Ro-En) and Estonian-english (Et-En), lowresource Sinhalese-English (Si-En) and Nepalese-English (Ne-En), as well as Russian-English (Ru-En). Besides, 4 language pairs -English-Czech (En-Cz), English-Japanese (En-Ja), Pashto-English (Ps-En) and Khmer-English (Km-En), are only offered blind test (1000), without training data. 

 Implemented Systems The systems for DA employ Predictor-Estimator architecture. Following previous sota works  (Fomicheva et al., 2020; Moura et al., 2020; Rei et al., 2020) , we use a pre-trained XLM-Roberta (XLM-R)  (Conneau et al., 2019)  model as a predictor due to its impressive performance on crosslingual downstream tasks. Practically, we concatenate source(SRC) and target(MT) sentences in the format [CLS] SRC  [EOS]  [SEP] MT [EOS] following XLM-R usage, and take the embedding of pooled output of [CLS] token as features of a sentence pair. For Estimator, we simply stack two-layer FFN, taking the [CLS] feature generated above as the input to predict sentence-level DA scores. 

 PE Assisted Sentence-Level DA Prediction Inspired by the Pseudo-PE techniques  (Kepler et al., 2019; Wang et al., 2020) , we take full use of postediting sentences provided in Post-editing Effort task through multitask learning. The model jointly learns to score (SRC, MT) pair in a regression task, and distinguish between translations and postedited sentences -which is the better translation in a classification task. In inference stage, the model only conducts regression task to predict DA score, as post-editing sentences are not available for blind test set. The regression task applies loss function as: L reg = (?(E s,t ) ? Y human ) 2 (1) where E s,t is the embedding of sentence pair (source, mt), ? is the regressor taking them as input, through a two-layer FFN to compute DA score, and Y human is the Z-normalized DA score annotated by human. The classification task forces the model to capture more expressive cross-lingual sentence representation which is paramount for DA score. In implementation, we get the model to learn which is the pair with better translation between embedding of concatenated source and target E s,t and embedding of concatenated source and PE E s,p . We splice two vectors in random order and apply two stacked FFN layers to compute classification result, in which 0 means the former pair is the better (i.e. the former contains PE), 1 means the former is the worse and 2 means translation and post-edit are exactly the same. Equation (2) gives the loss function for the classification task, where M is the number of classes (M = 3), Y is the binary indicator (0, 1, 2) if class label c is the correct classification for observations, P is the model predicted probabilities that the observation is of classes. L cls = ? M ?1 c=0 Y c log(P c ) (2) 

 Data Augmentation for Zero-shot Languages Instead of directly applying the multilingual DA model trained on other 7 language pairs to zeroshot languages, we exploit a data augmentation strategy based on MC dropout to improve the performance. Specifically, we compute the expectation and variance for the set of estimated DA scores of zero-shot languages obtained by performing N (N=30) stochastic forward passes through the welltrained but dropout-perturbed QE model. In order to control the uncertainty introduced by the disturbance, we only retain dropout in estimator and last two layers in XLM-R. We take variance as an indicator to detect observations with less uncertainty and use expectation as DA score label. Then, we mix the generated zero-shot DA data with randomly selected non-zero-shot training set to fine-tune the model. Experiments show that our data augmentation is effective to improve the performance, achieving better Pearson correlation. 3 Word and Sentence-Level Post-editing Effort Task Sentence-Level QE predicts the Human Translation Error Rate (HTER). HTER is the ratio between the number of edits (insertions / deletions / replacements) needed and the reference translation length. The evaluation metrics of the sentence-level task is Pearson's correlation metric. 

 Dataset The dataset in these task provides the same source and translation as DA task, with an extra postedit sentence for each observation and task-specific token-level and sentence-level labels. Besides, we generate addition-translation sentence (AMT) for each source sentence by using well-trained machine translation systems. The motivation here is to add an additional criterion which is in the same language as the provided translation sentence. We suppose that to detect the difference between two sentence in the same language is a simpler task for model. There are some important label properties to highlight: ? The number of BAD tags and OK tags is imbalanced, especially for GAP tags. ? AMT's BLEU score is significantly lower than MT taking post-edits as reference. Its average HTER is higher than MT. It indicates that the generated AMT is less closer to post-edits than MT. 

 Method The systems for QE shared task2 also employ Predictor-Estimator architecture  (Kim et al., 2017b) . Predictor. Similar to Task1, we use pre-trained XLM-Roberta (XLM-R) model as predictor after fine-tuning it with mask language modeling task  (Devlin et al., 2018)  using the provided source and PE sentences. In order to improve the performance, refers to approach in  (Wang et al., 2020) , we concatenate SRC, MT, AMT sentences together in the format of [BOS] SRC [EOS] [SEP] MT [EOS] [SEP] AMT [EOS]. We notate the predictor as f ; SRC, MT and AMT text as X and Y and Z, corresponding features as H x , H y , H z respectively: H x , H y , H z = f (X, Y, Z), (3) Estimator. We utilise 4 independent 2-layers FFN including binary three classification tasks to predict SRC word tags, MT/AMT word tags, MT/AMT gap tags respectively, and a regression task to predict HTER score of MT/AMT. All predictions are obtained by performing specific transformations ?. We define the predicted logits of SRC word, MT word, MT gap, AMT word, AMT gap as V xw , V yw , Vyg , V zw , Vzg ; and HTER predicted score of MT and AMT as V yh , V zh . The estimator can be described as: V xw = ? xw (H x ), V yw = ? w (H y ), V zw = ? w (H z ), Vyg = ? g (f cat (H y , V yw )), Vzg = ? g (f cat (H z , V zw )), V yh = ? h (f gap (f cat (H y , V yw , Vyg ))), V zh = ? h (f gap (f cat (H z , V zw , Vzg ))), (4) where f cat is the concatenate method in the last dimension, f gap is the global average pooling in the second dimension ignoring padding tokens in a batch just like  (Lin et al., 2013)  3.2. Loss. We prepend and append two special <pad> labels to the original word label sequence, append a special <pad> label to the original gap label sequence during training, but loss of the padded labels is not computed. For all classification tasks, to deal with the problem of imbalance between OK and BAD number, we use weighted cross entropy as the loss function, and the weight is calculated as w i = N C i , where w i is the inverse of the proportion of the instance with class C i . For sentencelevel HTER score loss, we use mean squared error (MSE) as the loss function. We define the tags of SRC word, MT word, MT gap, AMT word, AMT gap as V xw , V yw , V yg , V zw , V zg ; and HTER score of MT and AMT as V yh , V zh . The model is trained under the multi-task learning framework by summing up the loss of all subtasks with specific weights: loss = ? ?{xw,yw,yg,zw,zg} ? logP (V? |X, Y, Z)+ ? ?{hy,hz} ? (V? ? V? ) 2 , (5) where xw, yw, yg, zw, zg represents for classification tasks, hy, hz represents for regression tasks, ? is the weight of loss for a specific task. The multi-task framework can improve the overall performance. 4 Critical Error Detection 

 Task Description This is a new QE task focusing on predicting sentence-level binary scores indicating whether or not a translation contains (at least one) critical error. The key point is to identify whether the translation will lead to misleading or more serious consequences, e.g. the translation involves critical mistranslation, hallucination or critical content deletion. Only binary prediction (whether or not any critical error contained) is required. The evaluation metrics of this task is also the MCC. 

 Dataset The dataset contains 4 languages which are English-German, English-Chinese, English- least one catastrophic error in the translation. It is noticed that the number of NOT and ERR tag is imbalanced. 

 Methods Similar as the above two tasks, our baseline system takes pre-trained XLM-R as predictor, stacked FFN layers as binary classifier. We also experimented with replacing XLM-R by mBART  (Liu et al., 2020)  and replacing FFN layers with TextCNN, Bi-LSTM and other types of network. Based on the intuition that the semantic difference between two monolingual sentences are easier to distinguish than that of two cross-lingual sentences, we propose to incorporate a "good" MT of the source sentence into (src. mt) pair during training, so that the auxiliary information provided by the "good" MT can help the model to directly compare mt with MT+src, instead of only depending on cross-lingual src. With consideration of expensive overhead of manual translation, we assume that au- Voting-Based Ensemble. Finally, we ensemble several models and take their majority voting as prediction results. 

 Experimental Results 5.1 Task1: Sentence-level Direct Assessment Experimental Settings Our system is implemented with hugging face transformers package. The pre-trained xlm-roberta-large model which has approximately 550M parameters is taken as pre-dictor. We train the predictor and the estimator together on the multilingual QE DA dataset using Adam  (Kingma and Ba, 2015)  as optimizer with constant learning rate of 1e ?6 and training batch size of 16. The model is trained on a Nvidia Tesla V100 GPU. 

 Results Table  1  shows the results on test20 set. Our baseline is the system described in section 2.3. +multitask method is introduced in section 2.3.1. To achieve more competitive scores while also maintain a relatively small number of parameters, we ensemble our result with MC dropout approach, that is to run N (N=50) pass forwards with dropout and take the expectation of the N predictions as final answers. Results Table  3  shows the results on dev and test21 set. Our baseline is the QE system without AMT data. +AMT method is the QE system with AMT data. In the experiments, we generate 3 different kinds of AMT data with the machine translation system trained for the WMT2021 Machine Translation of News Shared Task, Baidu Fanyi 4 and Google Translate 5 . For each kind of AMT, we run N (N=10) pass forward with dropout=0.1 using the a unified model trained with all AMT together. The expectations of 3N predictions of score and token labels is taken as the final answers. 

 Task3: Critical Error Detection Table  4  shows the results of our system on development and blind test set. Experiments show that the best results obtained when applying XLMR-Large and FFN layer on development set. The involvement of AMT also brings significant improvement over all language pairs. For ensemble settings, we ensemble multiple models with different pre-trained models and classification layers using voting-based method as introduced in section 4.3. In order to solve the problem of label imbalance, we also investigate different label weights when computing cross-entropy loss. Due to the large gap between the number of NOT and ERR labels in the dataset, the weights(NOT:ERR) are clipped as 1:6, 1:4, 1:5, 1:15 for enzh, ende, encs, enja. Meanwhile, to better fit the data in the test set and avoid over-fitting, we utilise dropout with rate of 0.1 and weight decay of 1e ?5 . 

 Conclusion We present our work on WMT 2021 QE shared task in this paper. For all the three tasks to estimate sentence-level DA, token and sentence-level post-edit effort and sentence-level critical error, we employ predictor-estimator framework as our baseline. To further boost performance, we investigate the usage of additional high-quality translations. For task1, we mainly focus on introducing postedits with multi-task learning. Also, the effect of data augmentation method based on MC dropout is studied here to improve the result of zero-shot pairs. For task 2 and 3, we generate high-quality translations for each observation using multiple welltrained machine translation systems. By directly concatenating AMT with the original source and target sentence then encoding it with pre-trained predictor, we achieved remarkable results over all language pairs and tasks. In future, we will continue to invest time and effort on studying the effect of involving additional translations into QE tasks, for example, how the additional translation quality will affect QE performance, what the better ways are to incorporate additional translations in. Table 2 : 2 Pearson correlation between prediction of our system and human DA judgement on test21 set. Language Baseline +Multitask +Ensemble En-De 0.490 0.552 0.547 En-Zh 0.494 0.502 0.519 Ro-En 0.886 0.897 0.902 Et-En 0.798 0.805 0.814 Ne-En 0.776 0.789 0.801 Si-En 0.648 0.677 0.675 Ru-En 0.761 0.787 0.787 Average 0.693 0.716 0.721 Table 1: Pearson correlation between prediction of our system and human DA judgement of non-zero-shot lan- guage pairs on test20 set. Language Baseline +AugData +All En-De 0.481 / 0.584 En-Zh 0.523 / 0.583 Ro-En 0.878 / 0.901 Et-En 0.775 / 0.808 Ne-En 0.810 / 0.858 Si-En 0.564 / 0.581 Ru-En 0.753 / 0.787 En-Cz 0.546 0.557 0.573 En-Ja 0.297 0.349 0.364 Ps-En 0.592 0.622 0.622 Km-En 0.661 0.653 0.659 Multilingual 0.621 / 0.665 Czech, English-Japenese. 7000 training, 1000 validation, and 1000 blind test sentence pairs are available for each language. Ground truth label has two classes, NOT means no catastrophic error, and ERR means at 
