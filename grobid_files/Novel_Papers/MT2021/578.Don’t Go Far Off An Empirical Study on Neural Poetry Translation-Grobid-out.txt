title
Don't Go Far Off : An Empirical Study on Neural Poetry Translation

abstract
Despite constant improvements in machine translation quality, automatic poetry translation remains a challenging problem due to the lack of open-sourced parallel poetic corpora, and to the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. We present an empirical investigation for poetry translation along several dimensions: 1) size and style of training data (poetic vs. non-poetic), including a zeroshot setup; 2) bilingual vs. multilingual learning; and 3) language-family-specific models vs. mixed-language-family models. To accomplish this, we contribute a parallel dataset of poetry translations for several language pairs. Our results show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size, both in terms of automatic metrics (BLEU, BERTScore, COMET) and human evaluation metrics such as faithfulness (meaning and poetic style). Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data. 1

Introduction American poet Robert Frost once defined poetry as "that which gets lost out of both prose and verse in translation"  (Frost, 1961) . Indeed, the task is so complex that translators often have to "create a poem in the target language which is readable and enjoyable as an independent, literary text"  (Jones, 2011) . But even though poetry is destined to lose its accuracy, integrity, and beauty even in human translation, the process conceives new opportunities to stress-test the ability of machine translation models to deal with figurative language. While most computational work has focused on poetry generation  (Hopkins and Kiela, 2017 ; Uthus  1  The italics part of the title is the translation of a poem by Pablo Neruda with the same name. 2 Example taken from  (Ghazvininejad et al., 2018) . 

 Original Il ny avait que sable et boue O? s?tait ouverte la tombe. Le long des murs de la prison On ne voyait aucune tombe 

 Human For where a grave had opened wide, There was no grave at all: Only a stretch of mud and sand By the hideous prison-wall, 

 GCK But there was only sand and mud. To where the grave was laid. Along the walls of prison wall. 

 We saw no masquerade 

 Google There was only sand and mud Where the grave had opened. Along the prison walls No tomb could be seen. Table  1 : A French poem accompanied by human translation,  (Ghazvininejad et al., 2018 ) (GCK) system translation, and Google Translate. 2 et al., 2021; Van de Cruys, 2020;  Ghazvininejad et al., 2016; H?m?l?inen and Alnajjar, 2019; Deng et al., 2019; Yang et al., 2018) , research on poetry translation is in its infancy  (Ghazvininejad et al., 2018; Genzel et al., 2010) . For example,  Ghazvininejad et al. (2018)  employs a constrained decoding technique to maintain rhyme in French to English poetry translation. However, while keeping the poetic style and fluency, the translation might diverge in terms of meaning w.r.t. the input. Table  1  shows how the system generates a semantically inconsistent word "masquerade" to rhyme with "laid", whereas the original poem talks about "tomb". Meanwhile, state-of-the-art machine translation systems trained on large non-poetic data might preserve meaning and fluency, but not the poetic style (e.g., Google Translate's output in Table  1 ). Two main challenges exist for automatic poetry translation: the lack of open-sourced multilingual parallel poetic corpora and the intrinsic complexities involved in preserving the semantics, style and figurative nature of poetry. To address the first, we collect a multilingual parallel corpus consisting of more than 190,000 lines of poetry spanning over six languages. We try to tackle the second challenge by leveraging multilingual pre-training (e.g., mBART ) and multilingual finetuning  (Tang et al., 2020; Aharoni et al., 2019)  that have recently led to advances in neural machine translation for low-resource languages. Moreover, it has been shown that adaptive pre-training and/or fine-tuning on in-domain data always lead to improved performance on the end task  (Gururangan et al., 2020) . Since poetry translation falls into the lowresource (no or little parallel data) and in-domain translation scenarios, we present an empirical investigation on whether advances in these areas bring us a step closer to poetry translation systems that don't go far off in terms of faithfulness (i.e., keeping the meaning and poetic style of the input). We make the following contributions: ? We release several parallel poetic corpora enabling translation from Russian, Spanish, Italian, Dutch, German, and Portuguese to English. We also release test sets for poetry translation from Romanian, Ukranian and Swedish to evaluate the zero-shot performance of our models. ? We show that multilingual fine-tuning on poetic text significantly outperforms multilingual fine-tuning on non-poetic text that is 35X larger in size (177K vs 6M), both in terms of automatic and human evaluation metrics such as faithfulness. However, for the bilingual case the pattern is not so evident. Moreover, multilingual fine-tuning on poetic data outperforms bilingual fine-tuning on poetic data. The latter two results showcase the importance of multilingual fine-tuning for poetry translation. ? We also show that multilingual fine-tuning on languages belonging to the same language family sometimes leads to improvement over fine-tuning on all languages. Beyond advancing poetic translation, our findings will be helpful for other figurative language or literary text translation tasks. Our code and data and can be found in https://github.com/ tuhinjubcse/PoetryTranslationEMNLP2021 while our pre-trained models can be found at https://huggingface.co/TuhinColumbia. We hope that the data, models and the code released will encourage further research in this area. 

 Datasets 

 Poetic Training Data Given the lack of available multilingual poetic corpora, we collect several medium-scale parallel datasets. We identify websites that provide English translations for Spanish (Es), Russian (Ru), Portuguese (Pt), German (De), Italian (It) and Dutch (Nl) poetry. Table  2  shows the number of parallel sentences for each language pair as well as the websites from which they have been collected. Given that most of the websites were specifically designed for poetry translation, where translations are typically written by experts (professional translators), we believe our data to be of high quality. We make a simplifying assumption and focus on line-by-line translation. Thus, during scraping from these websites, we discard translations that are different in  the number of lines from the original poems. We collect approximately 190K (with 177K in training) parallel poetic lines spanning 6 different languages (see Table  3  for examples). This data is further split into train and validation. 

 Non-Poetic Training Data We also benchmark the quality of poetry translations obtained by models trained on non-poetic data. For this we rely on OPUS100 corpus (Tiedemann, 2012) as well as the ML50 corpus  (Tang et al., 2020)  designed to demonstrate the impact of multilingual fine-tuning. Each of the language pairs in OPUS100 have 1 million parallel sentences in their training set, several orders of magnitude larger than our poetic parallel data. For example, Portuguese-English non-poetic data is 65 times larger than the poetic data, while the Russian-English non-poetic data is 18 times larger than the poetic data. The size of the smallest non-poetic parallel corpus is about 6 times larger than all our poetic parallel data combined. For ML50  (Tang et al., 2020) , benchmark data is collected across 50 languages from publicly available datasets such as WMT, IWSLT, WAT, TED. The size of parallel sentences in ML50 corresponding to the languages under study are: De (45.8M), Es (14.5M), Ru (13.9M), Nl (0.23M), It (0.2M), and Pt (0.04M). 

 Test Data We create a high quality blind test set for every language independent of data mentioned in 3 Methods mBART  is a multilingual sequence-to-sequence (seq2seq) denoising autoencoder, which is trained by applying the BART objective  (Lewis et al., 2019)  to large-scale monolingual corpora across many languages. The input texts are noised by masking phrases and permuting sentences, and a single Transformer model is learned to recover the texts. Unlike other pre-training approaches for machine translation, mBART pre-trains a complete autoregressive seq2seq model. It is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs for supervised machine translation without any task-specific or language-specific modifications or initialization schemes. For supervised sentence-level MT, mBART initialization leads to significant gains (up to 12 BLEU points) across low/medium-resource pairs (< 10M bi-text pairs). This makes mBART an ideal candidate for our task of poetry translation given the scale of our parallel corpora. However, while mBART was trained on a variety of languages, the multilingual nature of the pretraining is not used during fine-tuning. To solve this,  Tang et al. (2020)  propose multilingual finetuning of pre-trained models, and demonstrate large improvements compared to bilingual fine-tuning. They explore 3 configurations to create different versions of multilingual translation models: Manyto-one (N ? 1), one-to-Many (1 ? N), and Manyto-Many (N?N) via a pivot language. The Manyto-one model encodes N languages and decodes to English. Given that we are translating poems in various languages to English, we further fine-tune the Many-to-one model for our task. 

 Implementation Details For bilingual fine-tuning on poetic data, we use the mbart-large-50 checkpoint from  (Wolf et al., 2020) , and fine-tune it for up to 8 epochs, saving the best checkpoint based on eval-BLEU scores. For bilingual fine-tuning on non-poetic data, we fine-tune the model for 3 epochs. For multilingual fine-tuning, we use the mbart-large-50-manyto-one-mmt. We perform multilingual fine-tuning for 3 epochs for both poetic/non-poetic data. We use the same hyperparameters as the standard huggingface implementation. We use (2-4) nvidia A100 GPUs for fine-tuning pretrained checkpoints. For fine-tuning mBART on non-poetic data, we set the gradient_accumulation_steps to 10 and batch size to 8 while for poetic fine-tuning we vary batch size between 24 and 32, and set gra-dient_accumulation_steps to 1. To perform multilingual fine-tuning, we concatenate bitexts of different language pairs (i, j) into a collection B i,j = (x i , y j ) for each direction (i, j). Following mBART , we augment each bitext (x i , y j ) by adding a source and a target language token at the beginning of x and y, respectively, to form a target language token augmented pair (x 0 , y 0 ). We then initialize transformer based seq-to-seq model by the pretained mBART, and provide the multilingual bitexts B = ? i,j B i,j to fine-tune the pretrained model. 

 Experimental Setting We experiment with several systems to evaluate performance across several dimensions: poetic vs non-poetic data; multilingual fine-tuning vs. bilingual fine-tuning; language-family-specific models vs. mixed-language-family models. ? Non-Poetic Bi (OPUS): fine-tuned mBART50 on Non-Poetic data from OPUS100 (Section 2.2) for respective languages bilingually. ? Non-Poetic Multi (ML50): mBART-large-50-many-to-one model implemented in the huggingface package. This is a multilingually fine-tuned model on 50 languages from the ML50 data that is 4 times larger than OPUS and created using all of the data that is publicly available (e.g., WMT, IWSLT, WAT, TED). ? Non-Poetic Multi (OPUS): multilingually fine-tuned mBART-large-50-many-to-one model on Non-Poetic data for 6 languages from OPUS100 (Section 2.2) (6M parallel sentences). ? Poetic: fine-tuned mBART50 bilingually (e.g., Ru-En, Es-En, It-En) on poetic data described in Section 2.1. ? Poetic All: multilingually fine-tuned mBARTlarge-50-many-to-one on all poetic data combined. ? Poetic LangFamily: multilingually finetuned mBART-large-50-many-to-one on poetic data for all languages belonging to the same language family. For instance, Pt, Es, It belong to the Romance language family, while De and Nl are both Germanic languages. 

 Automatic Evaluation Setup For the automatic evaluation, we compare the performance of all the above mentioned models in terms of three metrics: BLEU, BERTScore and COMET. BLEU  (Papineni et al., 2002)  is one of the most widely used automatic evaluation metrics for Machine Translation. We use the SacreBLEU  (Post, 2018)  python library to compute BLEU scores between the system output and the human written gold reference. BERTScore  (Zhang et al., 2019)  has been used recently for evaluating text generation systems using contextualized embeddings, and it is said to somewhat ameliorate the problems with BLEU. BERTScore also has better correlation with human judgements  (Zhang et al., 2019) . It computes a similarity score using contextual embeddings for each token in the system output with each token in the reference. We report F1-Score of BERTScore. We use the latest implementation to date which replaces BERT with deberta-large-mnli, which is a DeBERTa model  (He et al., 2020)  fine-tuned on MNLI  (Williams et al., 2017) . Recently  Kocmi et al. (2021)  criticized the use of BLEU through a systematic study of 4380 machine translation systems and recommend use of a pre-trained metric COMET  (Rei et al., 2020) . COMET leverages recent breakthroughs in crosslingual pre-trained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. We rely on the recommended model wmt-large-da-estimator-1719, which is trained to minimize the mean squared error between the predicted scores and the DA  (Graham et al., 2013)  quality assessments. Notice that these scores are normalized per annotator and hence not bounded between 0 and 1, allowing negative scores to occur; higher score means better translation. 

 Human-based Evaluation Setup Even though arguably useful for evaluating meaning preservation, automatic metrics are not as suitable to measure other aspects of poetic translation such as the use of figurative language and style. We conduct human evaluation by recruiting three bilingual speakers as volunteers for each language. NMT systems are susceptible to producing highly pathological translations that are completely unrelated to the source input often termed as hallucinations  (Raunak et al., 2021 )(e.g., the word Lungs in Table  10 ). To account for these effects, we use faithfulness as a measure that combines both meaning preservation and poetic style. We evaluate the best translations from multilingual models trained on poetic and non-poetic data. Human judges were asked to evaluate on a binary scale whether: i) the model introduces hallucinations or translates the input into something arbitrary, i.e. (Are they keeping the meaning of the input text?) and at the same time ii) the syntactic structure is poetic and the translations are rich in poetic figures of speech (e.g., metaphors, similes, personification). In this evaluation we compare the multilingually fine-tuned models on Non-Poetic data (Non-Poetic Multi (OPUS) and Non-Poetic Multi (ML50)) vs. multilingually fined-tuned models on Poetic data (Poetic All and Poetic LangFamily). We chose the best model in each category based on the BERTScore in the automatic evaluation. We chose a subset of the test set for human evaluation: 1044 sentences spanning across 80 poems in 6 languages (204 lines in Russian, 173 lines in Italian, 140 lines in Portuguese, 220 lines in Spanish, 148 lines in German, and 159 lines in Dutch with corresponding human translations). Human judges were also provided with gold translations to make the judgement easier. Agreement rates were measured using Krippendorff's ? and a moderate agreement of 0.61 was achieved. 

 Results Our results based on automatic metrics are summarized in Table  4  and the human evaluation in Table  5 . The first insight is that multilingual fine-tuning on Poetic data (Poetic All and Poetic LangFamily) outperforms mutilingual fine-tuning on Non-Poetic data (Non-Poetic Multi (ML50, Opus)) for all languages both in terms of automatic metrics (BLEU and BERTScore) and human evaluation based on faithfulness (Table  5 ). Between Poetic-All and Non-Poetic Multi we see at least 2.5 point improvement in BLEU scores as well as 1 point improvement in BertScore in translation of every language pair. For the recently developed metric COMET, we see that the best models are the multilingually fine-tuned poetic models, which is consistent with the results obtained using the other two metrics. However, when comparing the bilingually finetuned models (Poetic vs. Non-Poetic Bi(Opus)) the pattern is not as clear based on automatic metrics. We see comparable performance, but not a clear winner across languages and metrics. However, as with the multilingual case, the size of Poetic data is much smaller than the Non-Poetic data (20X to 50X smaller depending on the language). We also mixed poetic and non-poetic data in equal proportion and fine-tuned mBART by framing it as a domain adaption problem, however it did not lead to significant improvements and degenerated in a few languages. We also tried intermediate finetuning  (Phang et al., 2018) , where we first fine-tune a pre-trained mBART model on our Non-Poetic data and then fine-tune the best model checkpoint on our Poetic data. The results for this experiment also did not lead to any significant difference in performance. The third insight is that language-family-specific multilingual fine-tuning (Poetic LangFamily) helps in some of the languages when compared to mul-  tilingual fine-tuning on all languages (Poetic All). We also ran a preliminary experiment where we tested if multilingual fine-tuning with a dissimilar language hurts the performance compared to finetuning with a language from the same language family (e.g., De and It vs. De and Nl). Our initial experiments show that fine-tuning on languages from the same language family helps compared to languages from different language family. Last but not least, we notice that the multilingual fine-tuned model on poetic data (Poetic All) is consistently better than the bilingual fine-tuned model on poetic data (Poetic) across all languages. While we show that multilingual fine-tuning is an effective way to improve performance on low resource poetic data, we believe techniques like iterative backtranslation  (Hoang et al., 2018)     

 Zero-Shot Performance on Unseen Languages We test the generalization capabilities of our model fine-tuned on poetic data using poetry written in languages not seen during fine-tuning. We compare the zero-shot performance of our model fine-tuned multilingually on poetic data (excluding the unseen languages) to the Non-Poetic Multi (OPUS) and Non-Poetic Multi (ML50) model. We chose Ukrainian, Romanian and Swedish poetry given the fact that our model is fine-tuned on poetry belonging to languages from the Slavic, Romance, and Germanic families. Table  6  shows that our multilingually fine-tuned poetic model outperforms the other two multilingual models fine-tuned on Non-Poetic data, even though the languages were not contained in the fine-tuning data. This suggests that performance improvements of poetic fine-tuning are not only due to language-specific training data, but rather to multilinguality, presence of language family related data, as well as poetic style. These corroborate recent findings by  Ko et al. (2021)  who adapt high-resource NMT models to translate lowresource related languages without parallel data. They exploit the fact that some low-resource languages are linguistically related or similar to highresource languages, and often share many lexical or syntactic features. 

 Shortcomings of Style Transfer Techniques as a Post-Editing tool We evaluate whether style transfer techniques could help attenuate the shortcomings of translation models trained on non-poetic data. We use the romantic poetry style transfer model provided by  Krishna et al. (2020)  to paraphrase our non-poetic translations. This is the only available poetic style transfer model to our knowledge.To control for faithfulness, we generate 20 outputs for each input (i.e., non-poetic translations) using nucleus sampling (p = 0.6), we then select the sentence that has the highest similarity score with input using the SIM model by  Wieting et al. (2019) .   (-11.82 ) 60.07 (-10.83) NL 6.96  (-19.14) 60.95 (-11.95)  Table  7 : BLEU and BERTScore after style transfer applied to the Multi(OPUS) configuration. Value in parenthesis reports decrease from the score obtained just by using Multi(OPUS). The style transfer experiments decrease performance across all languages on both BLEU and BERTScore metrics as evaluated on the Multi(OPUS) model (see results in Table  7 ). Qualitatively, this may happen due to errors cascading from incorrect translations by the nonpoetic model, introduction of archaic language where it is not appropriate, and change in meaning. An example output is provided in Table  8 . 

 Gold What fun it is, with feet in sharp steel shod, M How fun it is to wear iron-clad shoes, M+ST Their iron shoes are saucy fun,  

 Analysis It is well-known that occasionally NMT systems have a tendency to generate translations that are grammatically correct but unrelated to the source sentence particularly for low-resource settings (e.g., hallucinate words that are not mentioned in the source language)  (Arthur et al., 2016; Koehn and Knowles, 2017) . Pre-trained multilingual language models and techniques like multilingual training or fine-tuning can indeed be effective for dealing with low-resource data such as poetry as seen in Figures  1, 2, 3 , showing examples of poetic translations by Poetic All and Multi(OPUS) configurations. However, it is surprising that even a model trained on 6M parallel lines from OPUS(100) performs worse than models trained on in-domain data that is 35X smaller. Table  9  shows how model fine-tuned multilingually on non-poetic data suffer from loss of metaphoric expression in poetry, while a model fine-tuned multilingually on Poetic data is able to capture it. Table  10  shows how every model except our best poetic model fine-tuned multilingually suffer from hallucinations. The Non-Poetic model, 

 Original: ? ? ? ? ?, ? ? ? ? ? ? ?, ? ? ? ? ? ? ? ?, ? ? ? ? ?, ? ? ? ?, ? ? ?, ? ? ? ? ?. 

 Gold: I love the lavish withering of nature, The gold and scarlet raiment of the woods, The crisp wind rustling o'er their threshold, The sky engulfed by tides of rippled gloom, The sun's scarce rays, approaching frosts, And gray-haired winter threatening from afar. 

 Poetic All: I love the luxuriant decay of nature, The forests dressed in crimson and gold, In their haylofts the wind's noise and fresh breath, And the heavens are covered with wavy mist, And the rare rays of sun, and the first frosts, And threats of the distant gray-haired winter. 

 Multi(OPUS): I love the lush nature of decay, And forests clothed in purple and gold, In their shadows is the sound of the wind, and the breath of fresh air, And the heavens are covered with clouds, And the rarest ray of sunshine, and the first frosts, And distant threats of the gray winter.  while fluent to the reader, is not faithful to the original translation. 

 Related work Domain adaptation in neural machine translation Chu and Wang (2018) categorize domain adaptation for NMT in two groups: data centric and model centric. Data centric techniques mostly focus on data augmentation for limited parallel corpora of low-resource languages. For example,  Currey et al. (2017)  propose copying the target data to the source side to incorporate monolingual training data for low-resource languages. Back-translation has been used for synthetic parallel corpora generation  (Sennrich et al., 2016) . To improve performance on specific domains,  Chu et al. (2017)  Original: Tonda, gelida dei suoi oceani, trasparente come una cellula sotto il microscopio eppure orizzontale con monti posati saldamente sopra i prati con la lingua dei fiumi e il mare steso. Solo a volte sospetto la vertigine: ruotiamo pi? veloci. Dormendo grido "cado" e l? sento lo spazio, il nero, le stelle sulla nuca lo spavento che vomita se stesso in mille sfere. 

 Gold: Round, frozen in its oceans, transparent like a cell under the microscope or horizontal with mountains planted firmly above fields with the tongue of rivers and the stretched out sea. Every now and then I have an inkling of vertigo: we're turning faster. Asleep, I cry out "I'm falling" and then I feel space, blackness, the stars at the nape of my neck, fear which vomits forth a thousand spheres. 

 Poetic All: Round, frozen of its oceans, transparent like a cell under the microscope yet horizontally with mountains steadily resting on the meadows with the tongue of rivers and the rising sea. Only at times do I fear the vertigo; we turn more swiftly. As we sleep we cry out and there I feel space, blackness, stars on the nape The fright that vomit itself into a thousand spheres. 

 Multi(OPUS): Round, icy of its oceans, transparent as a cell under the microscope but horizontally with mountains set high above the meadows with the tongue of the rivers and the low sea. I only sometimes suspect vertigo: We spin faster. Sleeping I scream "cado" And there I feel space, black, stars on my neck The scare that vomits itself into a thousand balls. A conventional model-centric approach is fine-tuning on in-domain parallel corpora or on mixed in-domain and out-of-domain corpora  (Chu and Wang, 2018) . In our work, we deal with a model-centric approach where we leverage a multilingual pre-trained model (mBART) and then finetune it multilingually on in-domain corpus. Recently,  Hu et al. (2019)  introduced a domain adaptation technique using lexicon induction, where large amounts of monolingual data are leveraged to find translations of in-domain unseen words. However, word-level lexicon induction might not be the most useful augmentation technique in our case, since poetic text deals with multi-word unseen phenomena such as metaphors. Poetic and literary translation Jones and Irvine (2013) discuss the difficulties of faithful machine translation of literary text in terms of the competing objectives of staying faithful to the original text but, on the other hand, trying to convey the experience of reading a literary piece to the reader. translation of Agatha Christie's novel from English to Dutch using Goolge's NMT system and found that most frequent issues were incorrect translation, coherence, style, and register. Even though a lot of work has been done in the direction of automatic literary translation, automatic poetry translation is still in its infancy.  Genzel et al. (2010)  produce poetry translations with meter and rhyme using phrase-based statistical MT approaches.  Ghazvininejad et al. (2018)  present a neural poetry translation system that focuses on form rather than meaning. They also only focus on poetry translation from French to English, and their code or data is not publicly available. In our work, the focus is on faithfulness and the ability to preserve figurative language in translation across multiple languages. 

 Conclusion and Future Work We release poetic parallel corpora for 6 language pairs. Our work shows the clear benefit of domain adaptation for poetry translation. It further shows that improvements can be achieved by leveraging multilingual fine-tuning, and that the improvements transfer to unseen languages. Future directions include addition of new languages and larger corpora, adapting low-resource machine translation techniques for poetry translation, translating to languages that are morphologically richer than English, as well as working on better evaluation metrics to detect hallucinations. While computational methods for poetry translation may never outperform the human standard, we hope our contributions spark interest in the machine translation community to take up this rather challenging task. Additionally, by open-sourcing our work we hope to provide a helpful resource for professional translators. 
