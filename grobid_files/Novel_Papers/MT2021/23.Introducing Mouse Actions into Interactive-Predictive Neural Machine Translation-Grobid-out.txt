title
Introducing Mouse Actions into Interactive-Predictive Neural Machine Translation

abstract
The quality of the translations generated by Machine Translation (MT) systems has highly improved through the years, but we are still far away to obtain fully automatic high-quality translations. To generate them, translators use Computer-Assisted Translation (CAT) tools, among which we find the Interactive-Predictive Machine Translation (IPMT) systems. This paper uses bandit feedback as the principal and only information needed to generate new predictions that correct the previous translations. Furthermore, the application of bandit feedback reduces the number of words that the translator needs to type in an IPMT session. In conclusion, this technique saves valuable time, and effort for translators. Moreover, its performance improves improves with the future advances in MT, so we recommend its application in the actuals IPMT systems.

Introduction In recent years there had been a large number of advances in the Machine Translation (MT) field that has led to a significant improvement in the quality of the translations. Currently, even with all the new advances, the MT systems are still not able to generate perfect ready to use translations  (Toral, 2020) . Indeed, MT systems usually require human post-editing in order to achieve perfect translations. The Computer-Assisted Translation (CAT) tools aim to generate high-quality translations using the knowledge and experience of professional translators while reducing the effort that they need to do. There is a large variety of CAT tools approaches, among which we focus on the Interactive-Predictive Machine Translation (IPMT) systems. Some of the recent projects in this field are TransType  (Langlais et al., 2000; Esteban et al., 2004; Cubel et al., 2003) , Matecat  (Federico et al., 2014) , CasMacat  (Alabau et al., 2014 (Alabau et al., , 2013 Sanchis-Trilles et al., 2014)  and MMPE  (Herbig et al., 2020) . They aim to create a workbench with an array of innovative features that were not available in other tools when they started. IPMT is one of the main paradigms that include these projects, where an expert translator provides feedback to the system, typically using the keyboard and mouse, to generate new predictions that correct previous errors. There are two main IPMT approaches, both use usually the keyboard and mouse as the main feedback interface, but the validation process changes between prefix  (Foster et al., 1997)  and segments  (Peris et al., 2017; Domingo et al., 2017) . In this project, we use the validation by prefix approach. Figure  1  illustrates a conventional IPMT session. Initially, the user is provided with a source sentence x to be translated. At iteration 0, the IPMT system generates the first 

 SOURCE (x): Una versi?n traducida de un texto. 

 REFERENCE (y): A translated version of a text. ITER-0 (p) (? h ) ( ) A written version of a story. ITER-1 (p) (s t ) (k) (? h ) A written version of a story. translated version of a text. 

 ITER-2 ( p) (s t ) (k) (? h ) A translated version of a text. ( ) (#) ( ) FINAL (p ? y) A translated version of a text. Figure  1 : Example of a conventional IPMT session to translate a sentence from Spanish to English. Non-validated hypotheses are displayed in italics, and accepted prefixes are printed in normal font. hypothesis ?h . At the next iteration, the user moves the cursor to the first error of the sentence, validanting the prefix p, and corrects the next word typing k. With this new information, the IPMT system searches the suffix ?h with the highest probability for the validated prefix p. This process continues until the whole sentence is validated and the user introduces the special token '#'. IPMT aims to reduce the effort that the experts have to made in their translation sessions while preserving high-quality translations. Indeed, in Figure  1 , the user has translated correctly the source sentence performing only three actions. Normally, in a regular post-editing system, the translator would have needed to perform five actions: two mouse movements, two word strokes, and the sentence validation. In this paper, we reduce the effort done by the user taking into account bandit feedback. The system only needs the error position to correct the sentence, information that can be provided by the user easily with an interface like a mouse. For this reason, and to simplify, we are going to suppose that the feedback is provided with the mouse, although any other interface capable to provide a sentence position or make a click could be useful. 

 Related Work The reduction of the effort needed in the translation process is a problem that has been thoroughly studied, resulting in a large variety of approaches. Some projects have investigated which information and display are more useful to the users, like showing the word alignment information  (Brown et al., 1993) , setting a maximum length for the predictions displayed  (Alabau et al., 2012)  or just using touch-based actions  (Wang et al., 2020) . Other approaches reduce the effort that the user has to do more directly: using confidence measures to reduce the number of words to check  (Gonz?lez-Rubio et al., 2010) , autocompleting the predictions typed by the user  (Barrachina et al., 2009) , or adding new input information to the system reduces the human effort of generating a new prediction  (Sanchis-Trilles et al., 2008a) . There are also projects like  Lam et al. (2018 Lam et al. ( , 2019  that investigated how to reduce the human effort in an IPMT system using Reinforcement Learning. This technique lets them use new kinds of feedback to the system that they use as a reward to adjust the parameters of the model and obtain better translations. In the paper, we take the approach introduced by Sanchis-  Trilles et al. (2008a,b) , demonstrating that with only the error position, the Interactive-Predictive Statistical Machine Translation (IPSMT) systems are capable of correct their translations. We apply and implement this technique on an Interactive-Predictive Neural Machine Translation (IPNMT) system, obtaining a higher reduction in the human effort. 

 Interactive-Predictive Neural MT In this section, we see briefly the IPNMT framework. First of all, we have to see the general framework of the Neural Machine Translation (NMT) models that we use to understand how the translations are created and how we later add human feedback to the equation. This framework was introduced by  Casta?o and Casacuberta (1997)  and has demonstrated its power in the last years  (Cho et al., 2014; Klein et al., 2017) . Given a sentence x J 1 = x 1 , ..., x J from the source language X, to find the sentence ? ? 1 = ?1 , ..., ? from the target language Y , that has the highest probability of being the translation of x J 1 , the fundamental equation of the statistical approach to NMT would be: ? ? 1 = arg max I,y I 1 Pr(y I 1 | x J 1 ) ? arg max I,y I 1 I i=1 p(y i | y i?1 1 , x J 1 ; ?) (1) where Pr(y i |y i?1 1 , x J 1 ) and p(y i |y i?1 1 , x J 1 ), are the probability distribution and the probability that assigns the neural model to the next word given the source sentence and the previous words so far. ? are the parameters of the neural model which are obtained from trying to minimize the minus log-likelihood on a set of parallel corpus  (Shen et al., 2016) . The IPNMT framework adds the feedback generated by the human to Equation (1) to help with the translation process. When the expert translator finds an error in position p, moves the cursor and types the correct word, producing the feedback f p 1 = f 1 , ..., f p where f p is the word that the user has typed to correct the error. We add the feedback with the last generated hypothesis to Equation (1): ? ? 1 = arg max I,y I 1 Pr(y I 1 | x J 1 , ? ? 1 , f p 1 ) = arg max I,y I 1 I i=1 Pr(y i | y i?1 1 , x J 1 , ? ? 1 , f p 1 ) (2) subject to 1 ? i < p f i = y i = ?i f p = y p = ?p where ? ? 1 = ?1 , . .., ? is the previous hypothesis, f p 1 is the feedback provided, and p is the length of the feedback. With the constraints 1 ? i < p f i = y i = ?i and f p = y p = ?p , we assure that the feedback that the expert has provided appears in the hypothesis generated by the system. As the user corrects and validates the translation from left to right, this equation can be seen as obtaining the most probable suffix for the prefix provided. 

 Enriching User-Machine Interaction Until now, the only interface that we have explored to IPMT is the combination of keyboard and mouse. The IPMT system provides a translation, and the user corrects it by placing the cursor before the first error and typing the correct word. In this paper, we retake the work introduced by  Sanchis-Trilles et al. (2008a) . We use the mouse as an interface for the user-machine interaction to provide the IPMT system the information about the position of the first error. First of all, we have to consider the two different classes of actions that can be performed with the mouse, non-explicit Mouse Actions (MAs) and interaction-explicit MAs. 

 Non-Explicit MA In conventional IPMT systems, before the user types any word, he has to move the cursor to the position where he wants to make the correction. With the cursor movement, the user is already providing valuable information to the system that we can use. He validates all the previous words and tags the next as incorrect. Just with this information, the system can generate a new hypothesis, in which the prefix remains unaltered, and the suffix changes for the following hypothesis with the higher probability that starts by a different word. This action does not suppose an extra cost for the translator, it is automatically performed when the mouse already needs to be moved to perform a correction. This process does not assure that the new suffix is correct but in the worst scenario, the user behaves as in a conventional IPMT system. In Equation (  2 ) we calculate the best hypothesis using the feedback that the user provides to the system f p 1 = f 1 , ..., f p where f p is the word that the user types to correct the error. In this new situation, the user does not provide the correct word in position p, but we know that it has to be different from the used in the previous hypothesis y p . This situation can be expressed as follows: ? ? 1 = arg max I,y I 1 Pr(y I 1 | x J 1 , ? ? 1 , f p 1 ) = arg max I,y I 1 I i=1 Pr(y i | y i?1 1 , x J 1 , ? ? 1 , f p 1 ) (3) subject to Pr(y p , y 1 ? i < p f i = y i = ?i I p+1 | x J 1 , y p?1 1 ) (4) where y p is the word that the system is trying to correct. To assure that the new word at position p from the suffix is different from the one used in the previous hypothesis y p we add the constraint y p = ?p to Equation (4) that is responsible for the generation of new suffixes. y p ? ? p+1 is the suffix with the highest probability given the source sentence and the prefix that the user has validated. 

 Interaction-Explicit MA The non-explicit MAs does not suppose an extra cost for the translator. In a conventional IPMT system, the user needs to move the cursor to the correct position in order to change a word. Once the user has moved the cursor to the correct position and the system has performed a non-explicit MA, if the translation still has an error in the same position the user can perform an interaction-explicit MA. This kind of MA needs that the user explicitly executes the action of asking for a new suffix, for this reason, the interaction-explicit MAs suppose a little extra cost that can save the user the effort of typing the correct word. In the end, is the user who has to decide which kind of action performs depending on his experience. In this project, we have used the mouse as an interface to provide to the system the position of the error, and the action of performing an interaction-explicit MA. Note that the interface used could be different, e.g. using a touch screen, or typing some special key such as F1 or Tab. However, it is explained with the mouse because we found it more intuitive and understandable. 

 SOURCE (x): Escriba aqu 4 la traducci?n. 

 REFERENCE (y): Write the translation here. ITER-0 ( p) (? h ) ( ) Write there the translation. ITER-1 ( p) (s t ) (? h ) Write there the translation. here the translation. ITER-2 ( p) (s t ) (? h ) Write here the translation. the translation here. ITER-3 ( p) (s t ) (k) (? h ) Write the translation here. ( ) (#) ( ) FINAL (p ? y) Write the translation here. Figure  2 : Example of an IPMT session with non-explicit and interaction-explicit MAs. At iteration 0, the user moves the cursor before 'there', and the system provides a new suffix. At iteration 1, before manually correcting the word, the user performs an interactive-explicit MA. At iteration 3, the user validates the translation. Non-validated hypotheses are displayed in italics, and accepted prefixes are in normal font. The MAs are indicated by the symbol ' '. Each time we perform an MA for the same position p, we obtain a new word that we do not want to get in the new suffix. The following equation solves this problem by keeping track of the k previous hypotheses, where k is the number of MAs performed in the same position: ? ? 1 = arg max I,y I 1 Pr(y I 1 | x J 1 , ? ? 1 , f p 1 , k) = arg max I,y I 1 I i=1 Pr(y i | y i?1 1 , x J 1 , ? ? 1 , f p 1 , k) (5) subject to 1 ? i < p f i = y i = ?i y p : ? y (k) p ? ? p+1 y (k) p ? ? p+1 = arg max I ,y p ,y I p+1 y p / ?{?p,y (1) p ,...,y (k?1) p } Pr(y p , y I p+1 | x J 1 , y p?1 1 ) (6) where y (k) p is the word that occupies the position p of the new hypothesis when the user performs the k th MA. y (l) p l < k are the words that have been generated before the user performs the k th MA, and ? is the first hypothesis generated before performing any MA in position p. We can see an example of a conventional IPMT session where the user performs a nonexplicit MA and an interactive-explicit MA in Figure  2 . At iteration 0 the system provides to the user the translation, and the cursor stays at the start of the sentence. At iteration 1 the user moves the cursor to the first error, validating the prefix (p) and performing a non-explicit MA. The system automatically generates a new suffix ?h that the user has to check in the next iterations. At iteration 2, the translation is still incorrect and the user decides to perform an interactive-explicit MA to correct it. The system generates a new suffix that can not start with the words 'there' or 'here'. Finally, at iteration 3, the user does not see any error and validates all the sentence. 

 Experimental Setup 

 System Evaluation In this article, we report our results using different metrics to measure the human effort performed in an IPMT session, differentiating between the keystrokes and the mouse actions performed. We report the effort done by the user in Word Stroke Ratio (WSR), Mouse Action Ratio (MAR), character MAR (cMAR), and useful MAR (uMAR) that gives us a reference of the mouse actions performed and the quality of them. WSR, introduced by  Tom?s and Casacuberta (2006) , is computed as the number of words that the user needs to type to generate the reference translation, normalized by the total number of words in the sentence. In this context, a word stroke is interpreted as a single action. Moreover, it is assumed to have a constant cost. MAR, cMAR and uMAR were introduced by Sanchis-Trilles et al. (  2008b ) when they first considered the mouse actions as significant information to IPMT systems. MAR is computed as the number of MAs that the user needs to perform in order to generate the reference translation, normalized by the total number of words in the sentence. The cMAR is calculated normalizing by the total number of characters. Non-explicit and Interaction-explicit MAs have the same cost. Lastly, uMAR indicates the amount of MAs that are useful to achieve the translation that the user has in mind i.e. the MAs that actually ending changing correctly the first word of the suffix. Formally, uMAR is defined as follows: uMAR = MAC ? nWSC MAC (7) where Mouse Action Count (MAC) is the total number of MAs performed, Word Stroke Count (WSC) is the number of words typed and n is the maximum amount of MA allowed before the user types in a word. Note that in order to perform a word-stroke the user previously must have performed n MAs, so in Equation (  7 ), we are removing from the total count of MAs those that were not useful and did not help to find the correct word. 

 Corpora We conduct our experiments on the domain Europarl  (Koehn, 2005) . The Europarl corpus is built from the Proceedings of the European Parliament, which exists in all official languages of the European Union, and is publicly available on the internet. We use the pair of languages Deutch-English (De-En), Spanish-English (Es-En) and French-English (Fr-En) in both directions in all our experiments. Their characteristics are described in Table  1 . All the corpora have been cleaned, lower-cased and tokenized using the scripts included in the toolkit Moses, developed by  Koehn et al. (2007) . Once we have them tokenized, we have applied the subword subdivision BPE, described in  Sennrich et al. (2016) , with a maximum of 32000 merges.   

 User Simulation Our experiments have not used real humans to translate the source sentences interactively because it would have been costly and slow. Instead, we have simulated the expected behaviour of professional translators. When the simulated user receives a new prediction from the IPMT system, they search for the first error of the translation, comparing the words and position from the hypothesis and the reference. Then, when the user has found an error, they perform a non-explicit MA if the mouse is not in the correct position or an interaction-explicit MA. The simulated user performs a maximum of n MAs for the same position, where n is a value set at the start of the experiment. If the error is not corrected once the user performs all the possible actions, they type the correct word looking at the reference. We repeat this process until the simulated user translates all the sentence correctly. 

 Model Architecture We built our NMT models using NMT-Keras ( ?lvaro  Peris and Casacuberta, 2018) . We have tested the experiments using a Recurrent Neural Network (RNN) and a Transformer. All the systems used Adam  (Kingma and Ba, 2017)  as the learning algorithm, with a learning rate of 0.0002. We clipped the L 2 norm of the gradient to 5. The batch size was set to 30 and the beam size to 6. The RNN-based NMT system used was an encoder-decoder architecture with an attention model  (Chorowski et al., 2015)  and LSTM cells  (Hochreiter and Schmidhuber, 1997) . The dimensions of the encoder, decoder, attention model and word embeddings were set to 512. We used a single hidden layer of the encoder and the decoder. The Transformer  (Vaswani et al., 2017)  model used a word embedding and dimension size of 512. The hidden and output dimensions of the feed-forward layers were set to 2048 and 512. Each multi-head attention layer had 8 heads, and we stacked 6 layers of encoder and decoder. Table  2  shows the translation performance in terms of BLEU of RNN-based and Transformer neural models.  

 Experimental Results The results of both models are displayed in Tables  3 and 4 . There, we compare the results obtained from a conventional IPMT system, with the addition to the system of the non-explicit MAs, and the interaction-explicit MAs with a maximum of 4 explicit actions per position. By just adding the non-explicit MAs to the system, on average, the user reduces his effort by 27.45%. The models are good enough that the correct word is the second most probably from the error position. And if we take account of the interactive-explicit MAs, the reduction is 55.9%. Note how with the non-explicit MAs the MAR values remains almost identical because the non-explicit MAs does not suppose an extra cost. The differences in values are special cases where the system predicted a correct sentence different to the obtained by typing the correct word. We have realized an ANOVA (ANalysis Of VAriance) with a confidence of the 95% comparing for each pair of languages the results obtained from the RNN and the Transformer to see if the models are statistically the same or not. The results are displayed in Tables  3 and 4 , where we tagged with an asterisk the results that we have statistical significance that they are different. Figure  3  shows the uMAR results versus the WSR obtainer for each maximum value of MAs up to five with the RNN and Transformer models. Each time that we increase the maximum number of MAs the number of errors fixed without typing the correct word is lower. If we look at the uMAR values obtained at each iteration we can understand how the reduction has worked. The uMAR values do not have a high variance, the value remains more or less the same for both models while increasing the maximum number of MAs, 35. Each time that we have increased the maximum number of MAs the 35% of the errors that were not corrected with the previous maximum are corrected now. Knowing how the uMAR value evolves, helps the human translator to choose between performing an interaction-explicit MA or typing directly the correct word. 

 Comparison Results In the last years, this same approach was explored on Interactive-Predictive Statistical Machine Translation (IPSMT) systems and was tested in the Europarl corpora  (Sanchis-Trilles et al., 2008b) . In this section, we compare the results obtained in their project with the Statistical Machine Translation (SMT) models versus ours results with NMT models. We compare their results only with the Transformer because both models have obtained very similar results. In Figure  4 , we can see the comparison results obtained in the Europarl corpus with the SMT and NMT models. Taking into account the results obtained with a maximum of 5 MAs, the SMT models get a WSR relative improvement around 24%, while the NMT models obtained a relative improvement around 57%. From the uMAR results, we can see that in the SMT models the percentage of uMAR goes from 6% to 12%, causing a lower WSR relative improvement. Meanwhile, the NMT model maintains the percentage of uMAR around 35%. Looking at these two results we can see how the NMT models are more likely to fix an error correctly than the SMT models. Although the human interaction was simulated the same for both projects, the uMAR score that gives us the percentage of useful MAR is very different, so we can conclude that the NMT models produce better corrections with the information that we are providing. 6 Conclusions and Future Work 

 Conclusions In this paper, we have implemented the use of bandit feedback to generate new predictions preserving the validated prefix. We have tested RNN and Transformer models with the Europarl corpus, and both models obtained very similar results. Both models have improved the baseline, proving that this kind of input information is useful and can reduce drastically the effort needed to correct a translation. Moreover, as the non-explicit MAs do not suppose an extra cost for the translator there are no cons to implement this approach on actual IPMT systems. Additionally, we have compared our results with a previous work that used this same approach on SMT models, and the WSR relative improvement obtained in our experiments is greater. Proving that the NMT models obtain better results with this kind of interaction and feedback provided than the SMT models. 

 Future Work In all the experiments that we have performed the user has been simulated following some basic rules. As future work, we need to test the use of mouse actions with an application where we can study the results of real humans that need to adapt to this new kind of input.   Figure 3 : 3 Figure 3: WSR when considering up to five maximum MAs versus uMAR with RNN and Transfromer in the Europarl corpus. 

 funds from the Comunitat Valenciana under project EU-FEDER (IDIFEDER/2018/025), Generalitat Valenciana under project ALMAMATER (Prome-teoII/2014/030), and Ministerio de Ciencia under project MIRANDA-DocTIUM (RTI2018-095645-B-C22). 

 Figure 4 : 4 Figure 4: Comparison results with the Europarl Corpus considering up to five maximum MAs. The left column shows WSR versus MAR and in the right column shows WSR versus uMAR. Our results (up) and Sanchis-Trilles et al. (2008b) results (down) 

 Table 1 : 1 Characteristics of the Europarl corpus. K and M stands for thousands and millions. Proceedings of the 18th Biennial Machine Translation SummitVirtual USA, August 16 -20, 2021, Volume 1:MT Research Track    

 Table 2 : 2 Translation quality for the Europarl task in terms of BLEU for RNN and Transformer. BLEU (?) RNN Transformer De-En 27.8 28.8 En-De 21.8 19.2 Es-En 32.1 32.1 En-Es 31.7 31.4 Fr-En 30.9 31.1 En-Fr 33.0 32.3 

 Table 3 : 3 Experimental results with RNN in the Europarl corpus when considering non-explicit and interaction-explicit MAs. Systems significantly differents from the Transformers systems are indicated with a *. baseline non-explicit interaction-explicit MAR WSR MAR WSR WSR rel. MAR WSR WSR rel. (?) (?) (?) (?) (?) (?) (?) (?) De-En 44.2 42.2 46.0 31.0* 26.5 145.8 19.2* 54.6 En-De 46.9 45.0 49.0 34.0* 24.3 162.0 22.7* 49.6 Es-En 41.0 38.7 42.6 27.6 28.6 131.2 16.9 56.4 En-Es 41.2 39.3 43.1 28.8 26.9 136.2 17.9 54.5 Fr-En 42.0 39.6 43.6 28.7* 27.6 135.9 17.6* 55.5 En-Fr 38.4 36.5 40.0 26.2 28.2 123.1 15.5 57.5 baseline non-explicit interaction-explicit MAR WSR MAR WSR WSR rel. MAR WSR WSR rel. (?) (?) (?) (?) (?) (?) (?) (?) De-En 42.5 40.5 44.3 29.1* 28.2 136.7 17.5* 56.7 En-De 49.7 47.8 51.8 36.2* 24.3 173.1 24.5* 48.8 Es-En 40.5 38.2 42.2 27.0 29.3 127.9 16.3 57.4 En-Es 41.4 39.6 43.3 28.7 27.6 135.9 17.8 55.1 Fr-En 41.2 38.9 42.9 27.3* 29.9 129.6 16.4* 58.0 En-Fr 38.1 36.2 39.7 25.7 29.0 121.2 15.3 57.7 

 Table 4 : 4 Experimental results with Transformer in the Europarl corpus when considering nonexplicit and interaction-explicit MAs. Systems significantly differents from the RNN systems are indicated with a *.
