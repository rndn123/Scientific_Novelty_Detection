title
TenTrans Large-Scale Multilingual Machine Translation System for WMT21

abstract
This paper describes TenTrans large-scale multilingual machine translation system for WMT 2021. We participate in the Small Track 2 in five South East Asian languages, thirty directions: Javanese, Indonesian, Malay, Tagalog, Tamil, English. We mainly utilized forward/back-translation, in-domain data selection, knowledge distillation, and gradual fine-tuning from the pre-trained model FLORES-101. We find that forward/backtranslation significantly improves the translation results, data selection and gradual fine-tuning are particularly effective during adapting domain, while knowledge distillation brings slight performance improvement. Also, model averaging is used to further improve the translation performance based on these systems. Our final system achieves an average BLEU score of 28.89 across thirty directions on the test set.

Introduction We participate in the WMT 2021 large-scale multilingual machine translation task small track 2 in 6 languages: English, Indonesian, Javanese,  Malay, Tamil, Tagalog (briefly, En, Id, Jv, Ms, Ta, Tl) . Any two of these languages translated into each other produces a total of 30 directions, including English?Indonesian, English?Javanese, English?Malay, English?Tamil, English? Tagalog, Indonesian?Javanese, Indonesian?Malay, Indonesian?Tamil, Indonesian?Tagalog, Javanese ?Malay, Javanese?Tamil, Javanese? Tagalog, Malay?Tamil, Malay?Tagalog and Tamil?Tagalog. To meet the requirements for data restrictions, our systems are all built with constrained data sets. For all systems, we adopt a universal encoder-decoder architecture that shares parameters across all languages  (Johnson et al., 2017) . Our systems are based on several techniques and approaches. We experiment with base and deeper Transformer  (Vaswani et al., 2017)  architectures to get reliable baselines, fine-tune the pre-training model FLORES-101  (Goyal et al., 2021)  to further improve the baseline system. Moreover, we generate pseudo bilingual sentences from the large-scale monolingual data, apply sequence level knowledge distillation  (Kim and Rush, 2016)  on partial language pairs, and try a more effectively fine-tuning strategy to domain adaptation . Particularly in the language pairs with inferior translations, we specifically improve their performance. All of these technologies have improved our systems, particularly data selection and gradual finetuning. We carefully rethought this strategy and found the main gain may come from in-domain knowledge adaptation. This paper was structured as follows: Section 2 describes the data set. Then, we present a detailed overview of our systems in Section 3. The experiment settings and main results are shown in Section 4. Finally, we conclude our work in Section 5. 

 Data Prepration We use FLORES-101 SentencePiece (SPM) 1 tokenizer model with 256K tokens to tokenize bitext and monolingual sentences 2 . Since it is important to clean data strictly  (Wang et al., 2018) , we follow m2m-100 data preprocessing procedures 3 to filter bitext data. The rules are as follows: ? Remove sentences with more than 50% punctuation.   ? Deduplicate training data. ? Remove all instances of evaluation data from the training data. ? Filter sentences that are longer than 250 tokens or length ratio upper than 3. For monolingual data, we still employ those rules except the length ratio filter. See Table  1  for the statistics of bitext data sets and Table  2  for monolingual data sets. 

 System Overview 

 Base Systems Our systems are based on the Transformer architecture  (Vaswani et al., 2017)  as implemented in TenTrans 4 , a unified end-to-end multilingual and multi-task training platform. We first train a model following the Transformer base setup to jointly training all language pairs as our base system. Then, inspired by , we experiment with raising network capacity by increasing encoder/decoder layers and feed-forward networks. We found that using a deeper encode layer (24) and a larger feed-forward network size (4096) can provide reasonable performance improvements while maintaining manageable network size and not increasing inference time. Because of the recent popularity of using largescale pre-training models to fine-tune specific languages and tasks  (Fan et al., 2020; Liu et al., 2020) , we use the pre-trained model FLORES-101 released by the organizer to fine-tune on the bitext data. This system has further improved our translation performance in all thirty translation directions. Note that to fine-tune FLORES-101 we train our models using FAIRSEQ  (Ott et al., 2019) . 

 Forward-Translation and Back-Translation Back-translation is an effective and common way to boost translation quality by using monolingual data to produce pseudo training parallel data. As opposed to back-translation, forward translation use source-side monolingual data to translate into the target language, and can be quite effective in some cases  (Bogoychev and Sennrich, 2019) .  has shown that when monolingual data from source and target languages are used together to produce pseudo data, the translation quality is best, and the experimental performance will be improved with the increase of data. In this work, considering the excellent performance of forward-translation and back-translation, we use both methods together. For translation directions with more than 5 million bitext data, such as En?Id, En?Ms, En?Tl, we separately train an individual model for each direction and use it for the pseudo-corpus generation. For other translation directions with less than 5 million bitext data, we use the baseline system of all language pairs jointly training for translating pseudo sentences. Due to a large amount of English monolingual data, English monolingual sentence was randomly divided into 13.36M, 25M, 25M, 25M, and 25M for En?Id, En?Jv, En?Ms, En?Ta, and En?Tl translation respectively. All monolingual data of Id, Jv, Ms, Ta, and Tl are used in translation to all other directions. 

 In-domain Data Selection The training data is provided by the publicly available Opus repository, which contains data of various quality from a variety of domains, while the hidden test set is the same domain as the provided dev and devtest datasets. After fine-tuning on a mixture of authentic bitext and pseudo-data, we select domain-specific data from the bitext and continue to fine-tune to further improve translation quality. Due to the scarcity of in-domain data, we utilize pre-trained language model multilingual BERT  (Devlin et al., 2019)  to train a domain classifier for extracting in-domain sentences from authentic bilingual sentences. To train the domain classifier, we consider all available dev data as positive data, and randomly sample bilingual data as negative samples. At the same domain test set, the domain classifier recognition accuracy is achieved at 93.97%. We select sentences predicted to be positive with a probability greater than threshold 0.7 to form an in-domain corpus. 

 Knowledge Distillation Knowledge distillation  (Hinton et al., 2015)  is a way to train a smaller network of students to perform better by learning from a larger teacher model. On this basis, sequence-level knowledge distillation trains the student model on the new data generated by the teacher model to further improve the performance of the student  (Kim and Rush, 2016) . A multilingual translation model that trains too many languages at the same time may degrade performance , especially involving 30 translation directions in this work. It makes it harder for the model to accommodate all language pairs. Based on this, we fine-tune the FLORES-101 model on five language pairs with En?Ta, Id?Ta, Jv?Ta, Ms?Ta, Tl?Ta to produce an Any-to-Ta specific translation model  (Tan et al., 2019) . These five language pairs are chosen because they do not perform very well and have more room for improvement. We used this model as the teacher model to translate the training data of the five language pairs. The new data was then combined with data of other language pairs to train the student model. 

 Gradual Fine-tuning Fine-tuning can improve the machine translation model by adapting the initial model trained on abundant but less domain-specific examples to the data in the target domain. This domain adaptation is usually accomplished with a phase of finetuning. While  Xu et al. (2021)  prove that gradual fine-tuning over a multi-stage process can yield substantial further gains. Intuitively, the model is iteratively trained to convergence on data whose distribution progressively approaches that of the in-domain data, similar to the curriculum learning strategy  (Bengio et al., 2009; Kocmi and Bojar, 2017) . In this work, we use gradual fine-tuning combined with in-domain data selection. After training the domain classifier, authentic bilingual sentences with positive predictions and probabilities greater than the thresholds of 0.7, 0.8, 0.9, and 0.99 are selected to form in-domain corpora with different similarity degrees. Data statistics with different thresholds are shown in the Table  3 . The higher the threshold, the more the selected data fits into the domain of the dev set and test set. We started with a gradual fine-tuning on the domain-specific data selected at the 0.7 thresholds, followed by the 0.8 thresholds, and so on. 

 System Average To further improve performance, we selected 12 language pairs that are significantly better than the baseline system. We consider them BLEU-sensitive and performance-friendly language pairs, which include En?Ta, Id?Ta, Jv?En, Jv?Ta, Jv?Tl, Ms?Ta, Ta?En, Ta?Id, Ta?Jv, Ta?Ms, Ta?Tl and Tl?Ta. After the gradual fine-tuning, we recover all the authentic bilingual sentences of these 12 language pairs, while the training sentences of other language pairs are still the training data when the threshold is 0.99. We continue to fine-tune the multilingual translation model. We find that the results still improve on these 12 language pairs and the performance of other language pairs is almost unchanged. 

 Model Averaging Model averaging is typically used between 5 or 10 adjacent checkpoints on the same system. It is almost impossible to average different systems because neurons or parameters at the same location in different systems may be responsible for completely different knowledge or responsibilities. Our systems kept the random seeds consistent, and the training data did not differ too much, so we tried a variety of model averaging methods to see whether the performance was improved. We finally chose average multiple checkpoints in a single system, and then averaged on different systems. In this way, the translation result can be further improved. 

 Experiments 

 Experiment Settings Except for the FLORES-101 fine-tuning experiments training on 48 NVIDIA P40 GPUs, the rest of our experiments are carried out with 16 NVIDIA P40 GPUs. Our model apply Adam  (Kingma and Ba, 2015)  as optimizer with ? 1 = 0.9, ? 2 = 0.98, and = 10 ?9 . We set the label smoothing to 0.2 and the dropout rate to 0.3. The initial learning rate is set to 5e-4 varied under a warm-up strategy with 4000 steps. For training, the batch size is 4096 tokens per GPU. For fine-tuning FLORES-101, we apply a temperature sampling strategy with sampling temperature T = 1.5  (Arivazhagan et al., 2019) . During inference, we decode with beam search and set beam size to 4 for all language pairs. The translation results we reported is detokenized and then the quality is evaluated using the 4-gram case-sensitive BLEU  (Papineni et al., 2002)  with the SacreBLEU tool  (Post, 2018) . 5 

 Main Results Results for all of our systems are shown in Table  4 . For convenience, we only report the average BLEU for 30 language pairs. The detailed BLEU scores for each language pair of systems implemented by TenTrans tool are shown in Table  5 , and the relevant systems for fine-tuning FLORES-101 are shown in Table  6 . As shown in Table  4 , we found that the baseline system with fine-tuning FLORES-101 performed better than the baseline system with no pre-training model  (24.23 vs. 22.25) . Forward-translation and back-translation (F&B) greatly improved the translation performance in both TenTrans (25.05 vs. 22.25) and  frameworks. The results of individual models for forward-translation and back-translation are shown in Table  7 . Deep Transformer with 24 encoder layers further improves translation results, but still not as high as fine-tuning FLORES-101 systems. Given the excellent performance of the pre-trained model, our subsequent series of approaches are based on fine-tuning FLORES-101. In-domain data selection is restricted to indomain data size (threshold 0.7), but we also obtain a solid improvement of 0.74 BLEU on average. Gradual fine-tuning (Gradual FT) is also effective, which enables the model to potentially better fit      8 . We guess that it may be because the translation quality of the teacher model is not excellent enough, which leads to the improvement of the student model is not satisfactory. We then recovered bilingual sentences for 12 BLEU-sensitive language pairs. As shown in Table  6 , the performance of these 12 language pairs improved significantly, while the results of the other language pairs barely changed, so our average BLEU improved further. For model averaging, we tried different combinations and finally found that averaging the three best checkpoints in "+ Gradual FT" and "+ Recover 12" will produce the best performance (28.94). 

 Submitted Results As shown in Table  9 , we ultimately chose the bestperforming model on devtest to submit to Dynabench 6 and achieve 28.89 in the hidden test set. 

 Conclusion This paper introduced our TenTrans submissions on WMT21 large-scale multilingual machine translation small task 2. Our main exploration is using more diversified architectures and fine-tuning strategy, utilizing forward-translation and back translation and approaches including in-domain data selection, knowledge distillation, and gradual finetuning. We experimented with these methods and continuously improve our system performance. On the whole, all of our systems performed competitively and ranked 3rd on the leaderboard. Table 1 : 1 Number of sentences in bitext data sets. En Id Jv Ms Ta Tl No filter 126.44M 5.46M 0.41M 1.87M 2.06M 0.41M Filtered 113.36M 5.26M 0.38N 1.85M 2.03M 0.39M 
