title
Towards Modeling the Style of Translators in Neural Machine Translation

abstract
One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state-of-the-art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality. * Y. Wang carried out this work during an internship with Amazon AI.

Introduction Translators often translate the original content with provided guidelines for styles. 1 However, guidelines are supposed to be high level and not comprehensive. Personal stylistic choices are thus welcome as creative part of the translator's job, as long as their translation style consistency is ensured to the task. By contrast, although neural machine translation (NMT) models  (Cho et al., 2014; Sutskever et al., 2014)  are trained from these human translations (e.g. Europarl, TED Talks), the models do not explicitly learn to capture the rich variety of translators' styles from the data. This limits their capability to creatively translate new data with different and consistent styles as translators do. We believe that modeling the style of translators is an important yet overlooked aspect in NMT. Our contribution, to the best of our knowledge, is to fill this gap for the first time. In particular, our work investigates ways to integrate translator information into NMT, with an emphasis on mimicking the translator's style. Our study uses the TED talk dataset, with four language pairs with translator annotations. We present and compare a set of different methods of using a discrete translator token to model and control translator-related stylistic variations in translation. Note that using a discrete token is a common approach to model and control not only specific traits in translation such as verbosity, politeness and speaker-related variances  (Sennrich et al., 2016a; Michel and Neubig, 2018) ) but also other aspects in NMT such as language ids  (Johnson et al., 2017; Fan et al., 2020) . However, our study is the first to use such a discrete token to model the style of translators. It also provides several insights regarding translation style modeling as follows. First, we show that the state-of-the-art Transformer model implicitly learns the style of translators only to a limited extent. Moreover, methods that add translator information to the decoder surprisingly result in NMT that fully ignores the additional knowledge. This is regardless of whether the token is added to the bottom (i.e. the embedding layer) or to the top (i.e. the softmax layer) of the decoder. Meanwhile, methods that add the information to the encoder seem to model the translator's style effectively. Second, we show that our best style-augmented NMT method is able to control the generation of translation in a way that mimics the translator's style, e.g. lexical and grammatical preferences, verbosity. While output produced by the styleaugmented NMT can vary significantly with the translator-token values, with BLEU score variations up to +4.5, a human evaluation confirms that observed differences are all about style and not translation quality. Finally, we show that the translator information has more impact on NMT than the speaker information, which was investigated by  Michel and Neubig (2018) . 

 Related Work Style itself is a broad concept  (Kang and Hovy, 2019) . It includes both simple high-level stylistic aspects of language such as verbosity  (Marchisio et al., 2019; Agrawal and Carpuat, 2019; Lakew et al., 2019) , formality  (Niu et al., 2017; Xu et al., 2019) , politeness  and complex aspects such as demography  (Vanmassenhove et al., 2018; Moryossef et al., 2019; Hovy et al., 2020)  and personal traits  (Mirkin and Meunier, 2015; Rabinovich et al., 2017; Michel and Neubig, 2018) . Our study focuses on capturing the personal style of translators. The closest work to our study is thus the work of  Michel and Neubig (2018) , where they study instead the effects of using the speaker information in NMT. In our results, we show that the translator information has indeed more impact to NMT than the speaker information. Finally, another distantly related research line tries to improve the diversity in the top rank translations of an input  (Li et al., 2016; Shen et al., 2019; Agrawal and Carpuat, 2020) . In fact, adding the translator information to NMT also provides means to generate translations with significantly different stylistic variations. 

 NMT with Translator Information NMT reads an input sequence x = x 1 , ..., x n in the source language with an encoder and then produces an output sequence y = y 1 , ..., y m in the target language. The generation process is performed in a token-by-token manner and its probability can be factored as m j=1 P (y j | y <j , x), where y <j denotes the previous sub-sequence before j-th token. The prediction for each token over the vocabulary V is based on a softmax function as follows: P (y j |y <j , x) = softmax(W V o j + b V ). (1) Here, o j ? R d is an output vector with size d (e.g. 512 or 1024), encoding both the context from the encoder and the state of the decoder at time j. Meanwhile, W V ? R |V|?d and b V ? R |V| are a trainable projection matrix and bias vector. We adjust NMT in different ways as below to let it mimic and control the translator's style. Source Token. In our first approach, we insert the translator token T as the beginning of each input sentence. The translator token is thus assigned with an embedding vector like any other source token. Hence, the embedding sequence E enc for the MT encoder becomes: E enc = [e(T ), e(x 1 ), ..., e(x n )], (2) where e(?) is an embedding lookup function. Token Embedding. We also consider adding the embedded translator token e(T ) to every token embedding in the encoder and/or decoder as follows: E enc = [e(T ) + e(x 1 ), ..., e(T ) + e(x n )], (3) E dec = [e(T ) + e(y 1 ), ..., e(T ) + e(y m )]. ( 4 ) Our motivation is to reinforce the influence of the translator token in MT. Output Bias. Following  Michel and Neubig (2018) , we add the translator token information to the output bias at the final layer of the decoder (FULL-BIAS variant). Specifically, the method directly modulates the word probability over vocabulary V as follows: P (y j |y <j , x, T ) = softmax(W V o j + b V + b T ). (5) Here, b T ? R |V| is the translator-specific bias vector, which can be thought of as a translator-token embedding with dimension |V| rather than d. We also explore another variant, named FACT-BIAS, as in  Michel and Neubig (2018) . This variant instead learns the translator bias through the factorization: b T = Ws T , (6) with parameters W ? R |V|?k and s T ? R k?1 where k << |V|. Note that while the above methods digest the translator token at an earlier stage, this one consumes translator signals in a late fusion manner. 

 Experiments 

 Dataset and Models We run experiments with the WIT 3 public dataset of TED talks  (Cettolo et al., 2012) , with four language pairs: English-German (en-de), English-French (en-fr), English-Italian (en-it) and English-Spanish (en-es). The dataset contains both speaker and translator information for each talk and translation, thus allowing to measure the effects of translators and speakers.  We construct training, validation and test sets for each translation direction as follows. We first extract all talks that are translated by the 10 most popular translators (see Figure  1 ) and split them into parallel sentences. From the data of each translator, we then sample 500 sentences for testing, and, from the remaining data, 90% for training and 10% for validation. All training, testing, and validation sentence pairs are put together and annotated with training and speaker labels. Table  1  shows the data statistics for four language pairs. For preprocessing, we employ Moses  (Koehn et al., 2007 ) tool 2 for tokenization and apply subword-nmt 3  (Sennrich et al., 2016b)  to learn subword representations. We choose Transformer  (Vaswani et al., 2017)  as the baseline and employ Fairseq  for our implementations. Our Transformer model is comprised of 6 layers of encoder-decoder network, where each layer contains 16 heads with a self-attention hidden state of size 1024 and a feedforward hidden state of size 4096. We employ Adam optimizer  (Kingma and Ba, 2015)  to update model parameters. We warm up the model by linearly increasing the learning rate from 1 ? 10 ?7 to 5 ? 10 ?4 for 4000 updates and then decay it with an inverse square root of the rest training steps by a rate of 1 ? 10 ?4 . We apply a Dropout of 0.3 for en-de and 0.1 for both en-fr and en-it. For all MT systems, we load weights from pretrained models to set up a better model initialization. Specifically, we employ models pretrained on WMT data for en-de and en-fr  (Ott et al., 2018) , and pretrain models for en-it and en-es using our large in-house out-of-domain data, as there are no previous pretrained models for these pairs. We finetune models on TED talk data for 10 epochs 4 and select the best model based on the validation loss. During inference, we employ beam search with a beam size of 4 and add a length penalty of 0.4. We use the BLEU score  (Papineni et al., 2002 ) to evaluate translation accuracy. 

 Results 

 Adding Translator Token We first compare methods to integrate the translator token into the Transformer. Notice that we report performance of the model in two settings: (i) when fed with the oracle translator label (as at training time) and (ii): when fed with randomly assigned labels. Intuitively, if a model really leverages the translator information, we expect to see a performance drop in the randomized setting. Results are shown in Table  2 . Our findings are as follows. First, it is surprisingly ineffective to add the translator token into the decoder, whether to the input (DEC-EMB) or to the softmax (FULL-BIAS, FACT-BIAS). In most cases, our randomization experiment shows that the model simply ignores the information. Second, methods adding the token to the encoder (SRC-TOK, ENC-EMB) are significantly more effective. Translation accuracy is also consistently better (at most by 0.4 BLEU) than with the Transformer baseline, indicating the translator token is useful. For those models, randomizing translator labels results in visible drops in BLEU score (up to 1.0 BLEU), indicating that the translator information has an important effect to the model.  

 Style Imitation Following the common practice in evaluating the style imitation (e.g. see  (Michel and Neubig, 2018; Hovy et al., 2020 )), we train a classifier to predict the translator style of the output of various models. We employ a Logistic Regression classifier based on both uni-gram and bi-gram word features. The classifier, trained on NMT training data, is applied on the outputs of NMT models. Figure  2  shows the results of this experiment. As can be seen, the standard Transformer learns the style of translators only to a limited extent. The style of translation outputs are less consistent with the original translator's style, i.e. accuracy is between 20% and 35%). Meanwhile, the classification accuracy is significantly higher (up to +12% relative) under SRC-TOK and ENC-EMB. This confirms that explicitly incorporating translator information at the sentence level allows for transferring some of her/his personal traits into the translations. Meanwhile, we notice higher accuracy achieved with the reference translations (e.g. 42% in EN-ES), suggesting there is room for improvement. 

 Stylistic Variations We analyzed stylistic variations using different translator token labels. In particular, we evaluate model outputs on en-fr after translating the entire test set with the same translator token labels. As in Table  3 , translator-informed NMT can produce quite different outputs, resulting in BLEU score variations up to +4.5, (i.e. between T 7 and T 3, T 8, T 10). We also observe differences in BLEU (albeit smaller) when testing with the WMT 2014 test set. In particular, BLEU score variations are up to +0.84 between T 7 and T 5. We also compute the symmetric-BLEU distances between any two of the translators using their predictions for both TED and WMT test set and visualize their heatmaps in Figure  3 . We observe that a similar BLEU distance between various translators in both test sets. Besides, T7 has a farther distance with others but its gap is closer on WMT than TED. These findings verify the consistency of translator styles in data from different domains. Then, we asked 3 professional translators to grade the quality of translation produced with the labels T 7 and T 3 on the TED talks. The evaluation is on a 1-6 scale (higher is better) on a random sample of 100 sentences. This resulted in average scores of 4.867 and 4.860 for T3 and T7, respectively. A similar human evaluation with T 7 and T 5 labels was also run on a random sample of 100 sentences of the WMT 2014 test set. It provided the same conclusion: average scores are very similar: 4.99 and 5.0 for T5 and T7 respectively. Both evaluations confirm that there is no difference in translation quality when using different token labels, i.e. the low BLEU score of T7 is only an effect due to stylistic differences.  Grammar Src: I had just tweeted, "Pray for Egypt". T3: J'avais tweet? : "Priez pour l'Egypte". T7: Je venais de tweeter, "Priez pour l'Egypte." Table  4 : Examples of stylistic differences: T3 and T7 have different preferences of grammars and words in translation. Their translations are also different in the verbosity (Using T7 results in consistently less verbose output than as of using T3), which is indeed also what translations by T3 and T7 differ in the training data. 

 Translator vs. Speaker Effects Finally, we compared the effect of the translator token with that of the speaker token, which was proposed in  Michel and Neubig (2018)  to perform extreme personalization. Results on all four directions (see Table  5 ) show that the translator token has more impact.  6  Given that speaker and author style has received much more attention in the liter-6 One probable reason is that the speaker signal is more sparse than the translator signal, i.e. each speaker is represented by one TED talk, while translators by multiple talks.  ature, we hope that this final result will spark more interests on the style of translators. 

 Conclusion We designed various ways of incorporating translator information into NMT, in order to model and control the generation of translation with different translator styles. We show that resulting styleaugmented NMT produces significantly different stylistic variations, mimicking professional translators. Human evaluation confirms that the generated variations are all of same translation quality. Figure 3 : 3 Figure 3: Heatmap visualization for symmetric-BLEU distances between translators. 
