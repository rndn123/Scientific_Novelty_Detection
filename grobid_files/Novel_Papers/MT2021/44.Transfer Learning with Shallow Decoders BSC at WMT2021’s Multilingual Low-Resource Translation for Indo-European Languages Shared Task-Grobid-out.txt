title
Transfer Learning with Shallow Decoders: BSC at WMT2021's Multilingual Low-Resource Translation for Indo-European Languages Shared Task

abstract
This paper describes the participation of the BSC team in the WMT2021's Multilingual Low-Resource Translation for Indo-European Languages Shared Task. The system aims to solve the Subtask 2: Wikipedia cultural heritage articles, which involves translation in four Romance languages: Catalan, Italian, Occitan and Romanian. The submitted system is a multilingual semisupervised machine translation model. It is based on a pre-trained language model, namely XLM-RoBERTa, that is later finetuned with parallel data obtained mostly from OPUS. Unlike other works, we only use XLM to initialize the encoder and randomly initialize a shallow decoder. The reported results are robust and perform well for all tested languages.

Introduction We present the work carried out by the BSC Team in the context of WMT2021's first edition of the Multilingual Low-Resource Translation Shared Task. The tasks addresses the issue of multilinguality in machine translation (MT) for lowresource languages, focusing on two language families: North Germanic and Romance. We take part in the Subtask 2, which involves translation in four Romance languages: Catalan, Italian, Occitan and Romanian. 

 Background Machine translation for low-resource languages is characterised by the lack of sufficient parallel data of a given language pair, either because the combination is infrequent or because the languages involved are themselves low-resource. Several works have attempted to overcome this pitfall, using different techniques. A common solution is to employ back-translation  (Sennrich et al., 2016) , while other research focuses on using other languages as pivots to compensate for the lack of data  (Firat et al., 2016; Zoph et al., 2016) .  Artetxe et al. (2018) ;  Lample et al. (2018)  make use of monolingual data only. Our approach is based on multilinguality. Previous works such as Verg?s Boncompte and Ruiz Costa-Juss? (2020);  Tubay and Costa-Jussa (2018)  have shown that the use of multilingual MT is beneficial, as it generalizes better by sharing parameters among all the languages involved, especially if the languages belong to the same linguistic family. At the same time, training of multilingual MT models from scratch usually requires large parallel corpora and may not be feasible in a low-resource and zeroresource translation scenarios. Pre-training of large language models from scratch on monolingual data and then fine-tuning them for the specific downstream tasks, has proved to be an extremely successful approach for many natural language processing problems. Crosslingual language models such as XLM and XLM-RoBERTa  (Conneau and Lample, 2019; Conneau et al., 2020) , that combine unsupervised (monolingual data) and supervised (parallel data) training objectives, perform especially well both on cross-lingual NLU tasks and in machine translation. The idea of combining the power of pre-trained cross-lingual language models with a multilingual machine translation setting naturally follows from there. This idea was explored in  where a denoising seq2seq auto-encoder (mBART) based on BART  was pre-trained on extensive monolingual corpora in many languages. A similar approach is implemented in  (Lin et al., 2020)  where alignment information is used to pre-train a multilingual MT transformer on existing public parallel datasets. Both approaches require either a computationally intensive pre-training on monolingual data or access to extensive large-scale parallel data. Initializing an encoder and decoder of a bilingual MT seq2seq transformer with a pre-trained crosslingual language model was famously proposed in  (Conneau and Lample, 2019) . A natural next step is to initialize a multilingual MT seq2seq transformer with a shared encoder and a shared decoder by the XLM-like language encoder which was first performed in  (Ma et al., 2020) . We reuse this idea by initializing the encoder with a pre-trained XLM-Roberta  (Conneau et al., 2020) , as in  (Ma et al., 2020) . However, unlike  (Ma et al., 2020) , we only initialize the encoder, with the motivation of being able to instantiate a shallower decoder, following previous works that for a given compute budget suggest that it is more efficient to use deeper encoders and shallower decoders  (Kasai et al., 2020) . The encoder-only initialization was already implemented in Fairseq  (Ott et al., 2019) . 1 3 Experimental Framework 

 Fine-Tuning Data To train our MT system, we use all parallel data available in OPUS 2 for the targeted language pairs, namely ca-it, ca-oc, ca-ro. We further include a small dataset ca-oc, the Catalan -Occitan Gencat Crawling, specifically obtained for the occasion, by leveraging parallel data from a crawling of the Catalan Government Internet domains and subdomains. We use the Cor-pusCleaner 3 pipeline to process the WARC files obtained from the crawling. This allows us to maintain the metadata and retrieve the original url per each document. We then extract the content of the same URLs in both languages and align them at document level using vecalign 4 . The final dataset of 503 sentences was obtained by manually reviewing 1,237 automatically aligned sentences. Although smaller than expected, one motivation to crawl this brand new dataset is to contribute to the development of MT resources for Occitan, which is a severely under-resourced language. We are publicly releasing this new dataset with an open license.  The resulting statistics of the corpora used to train our system can be seen in Table  1 . As expected, the number of aligned sentences is much larger for Italian and Romanian as target languages, since Occitan is such a low-resource language. Nonetheless, we must bear in mind that Catalan and Occitan belong to the same sub-branch in the family tree of the Romance languages, as shown in Figure  1 . Thus, their considerable typological closeness makes up for the reduced amount of aligned sentences available for this language pair. 

 Preprocessing We start by preprocessing our data with a filtering and a tokenization step. To ensure that there is no train-test overlap, we filter all of our training data by removing all sentences from the validation and test sets present in our train set. To build our system, we use SentencePiece BPE tokenization with the original shared vocabulary of 250,000 tokens of XLM-R model  (Conneau et al., 2020; Kudo and Richardson, 2018) , and we only keep sentences with a maximum size of 512 tokens. The final number of parallel sentences used for training is shown in Table  1 . 

 System Description We base our system on XLM-RoBERTa  (Conneau et al., 2020)  and then fine-tune it with the collected parallel data. As described earlier, the seq2seq multilingual transformer with shared encoders, shared decoders and shared embedding tables is initialized by XLM-R BASE pretrained language model on the encoder side, whereas a shallow decoder of 3 layers is initialized randomly. Sharing of embedding tables for all directions (ca-it, it-ca, ca-ro, ro-ca, ca-oc, oc-ca) was initially implemented due  to memory constraints but eventually turned out to work well. The token indicating the required target language is prepended to the target sentences, thus the model is aware to what language it has to translate to, as in  (Wu et al., 2016) . It is important to note that the data used to train XLM-RoBERTa does not contain any Occitan text, as can be seen in Table  2 . Thus the only knowledge that the multilingual transformer has about the language directions including Occitan comes from the XLM-R language model being pre-trained on text in related languages, such as Catalan. We use default Fairseq parameters for finetuning, first of all, the Adam optimizer (Kingma and Ba, 2017) with ? 1 = 0.9 and ? 2 = 0.98. The polynomial decay learning rate schedule starts from 5e?04, warmed up to over 1000 updates and gradually decays to 0 over around 60k updates. The model was fine-tuned for 2 days on 4 NVIDIA V100 GPUs. The final learning rate was around 3e?04 with a batch size of 3, 072 sentences. During inference we use the beam search generation algorithm with a beam size of 5. Since the languages between which we are translating are typologically close, we do not assign any length penalty, and we use the best checkpoint for generating. 

 Results Here we report the official evaluation.  6  We submitted our results a bit later than the deadline due to some bottlenecks in our in-house computational resources. Table  3  reports the results obtained by our system on the evaluation test set, together with the two official baselines provided by the organisers (M2M-100 and mt5-devFinetuned). Out of 7 competing systems and 2 baselines, our system was ranked 5th in Average, 3rd in the Catalan-to-Occitan direction, 4th in the Catalanto-Romanian direction and 6th in the Catalan-to-Italian direction. 

 Human Evaluation The organizers of the workshop have recently released the results of a human evaluation for the   -dev-ft. 30.38 40.14 17.33 29.28  Table  3 : Official BLEU scores for the evaluation of the final test set language pairs ca-it and ca-oc. 7 A sentence level evaluation has been performed taking into account the document as context. Each sentence is evaluated in a Likert-like scale  [1, 5]  answering the question of direct assessments. A second human evaluation is performed where 60 selected terms (mostly named entities, dates and locations) are annotated as being either well translated, not translated or mistranslated, by majority voting among the annotators. The results can be found in Tables  4 and 5 , respectively. 

 Discussion 

 Little sisters over big cousins As seen in Table  3  the average results of our system are above both baselines, although it is the results for Catalan-Occitan that give the greater leverage, because in the other two scenarios M2M has a higher BLEU. Actually, the score for Catalan-Occitan is substantially higher than the score obtained for the other two pairs, although the finetuning data used in this model is, at least, ten times smaller than the data used in the other two models. These results are replicated in most of the other competing systems 8 . The reason for this apparent anomaly is clearly due to the linguistic similarity between the Catalan and Occitan, which in medieval times were practically one and the same language. This result confirms the intuition that when two languages are similar enough, less data is needed. That said, we also hypothesize a positive impact of the curated dataset (Catalan -Occitan Gencat Crawling) added to the rest of parallel data obtained in the OPUS repository, but there is no definitive proof of it. Furthermore, we can also hypothesize that the presence of Spanish in the multilingual corpus, being a high-resource language and also 7 http://www.statmt.org/wmt21/ multilingualHeritage-translation-manual. html 8 http://statmt.org/wmt21/ multilingualHeritage-translation-results. html linguistically close to Catalan and Occitan (more so than to the other two Romance languages involved in the task), has a beneficial impact on the results. Indeed, low-resource languages can greatly benefit from their similarity to other languages present in the multilingual training. In such scenarios, less data can lead to satisfactory results, and with a smaller carbon print, since the models use less computational power for training. 

 Human Evaluation results The human evaluation on two of the test sets shown in in table 4 validates the relative position of our system in the global ranking. Interestingly, human scores correlate well with BLEU for Catalan-Italian, and less well for Catalan-Occitan. In the latter case, human scores tend to be lower than the corresponding BLEU. The reason for this may again have to do with linguistic similarity between Catalan and Occitan: "Catalanish" Occitan may be deemed acceptable by subword-based BLEU, but not by human evaluators. The performance of our system as evaluated for term translation, shown in in table 5 is consistent with the other evaluations regarding the position of the system in the overall ranking. 

 Vocabulary One of the shortcomings of our approach is the big vocabulary size (250k tokens), inherited from XLM. This big vocabulary size was required by XLM to cover a very diverse set of languages. However, this makes it sometimes challenging to fit the embedding tables in memory, which is especially inefficient taking into account that a large proportion of tokens are not used (since we focus on a tiny subset of languages). Thus, either pruning the vocabulary, or using pre-trained models specifically trained for the Romance languages family (with a reduced vocabulary size) would be better alternatives. 

 Shallow decoders and transfer learning While recent works have suggested that allocating more computation to (deeper) encoders  (Kasai et al., 2020)  at the expense of allocating less computation to (shallower) decoders is more efficient, this approach is not yet standard in the machine translation literature, especially when applying transfer learning. This method has the advantage of not reusing pre-trained weights for the decoder, although a middle ground is perhaps worth exploring. 

 ca-it ca-oc Model z-score raw z-score raw Human 0.8?0.4 4.8?0.6 0.8?0.7 4.0?1.0 Ours -0.1?0.8 3.7?1.1 0.3?0.9 3.4?1.2 M2M-100 0.4?0.7 4.2?1.0 -0.7?0.8 2.0?1.0 mT5-dev-ft. -1.2?0.9 2.3?1.2 -1.0?0.7 1.7?0.9 Namely, use just some of the pre-trained weights to initialize the decoder layers. For example reuse the first N layers of XLM in the decoder, even if there is no 1-to-1 mapping between layers because there are less in the fine-tuned model. 

 Conclusions We have showed that our approach is a simple, yet effective method for multilingual machine translation between linguistically similar languages. The encoder-only initialization allows for having a shallow decoder, which is computationally wise. As future work, we plan to further explore transfer learning techniques in the context of shallow decoders as well as applying different vocabulary pruning techniques. 

 Code availability We release  9  with an open license the scripts used for this work for the sake of reproducibility. Figure 1 : 1 Figure 1: Family tree of Romance languages showing only the languages targeted by the Shared Task. 

 Table 1 : 1 Number of aligned sentences per corpus. The last row shows the final number of aligned sentences after the cleaning and tokenization steps. crawling_ca-oc 

 Table 2 : 2 Number of million tokens per language present in the training corpus of XLM-R. 

 Table 4 : 4 Official human evaluation scores at sentence level ca-it ca-oc Model well mis no well mis no Human 53 0 3 56 40 0 2 42 Ours 27 7 5 39 33 4 0 37 M2M-100 33 2 6 41 26 9 0 35 mT5-dev-ft. 20 17 10 47 25 11 4 40 

 Table 5 : 5 Official human evaluation scores for 60 selected terms 

			 http://statmt.org/wmt21/ multilingualHeritage-translation-results. html
