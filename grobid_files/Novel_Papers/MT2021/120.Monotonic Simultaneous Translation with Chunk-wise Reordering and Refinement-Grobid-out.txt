title
Monotonic Simultaneous Translation with Chunk-wise Reordering and Refinement

abstract
Recent work in simultaneous machine translation is often trained with conventional full sentence translation corpora, leading to either excessive latency or necessity to anticipate asyet-unarrived words, when dealing with a language pair whose word orders significantly differ. This is unlike human simultaneous interpreters who produce largely monotonic translations at the expense of the grammaticality of a sentence being translated. In this paper, we thus propose an algorithm to reorder and refine the target side of a full sentence translation corpus, so that the words/phrases between the source and target sentences are aligned largely monotonically, using word alignment and non-autoregressive neural machine translation. We then train a widely used wait-k simultaneous translation model on this reorderedand-refined corpus. The proposed approach improves BLEU scores and resulting translations exhibit enhanced monotonicity with source sentences.

Introduction Simultaneous interpretation is widely used in various scenarios such as cross-lingual communication between international speakers, international summits, and streaming translation of a live video. Simultaneous interpretation has a latency advantage over conventional full-sentence translation, i.e. offline translation, as it requires only partial sequence to start translating. However, as the source and target languages differ in word orders, there is a difficulty in simultaneous interpretation that does not exist in offline translation which translates only after the whole source sentence is received. For example, when dealing with language pairs that significantly differ in word order (e.g., between SOV language and SVO language), an interpreter Figure  1 : An example illustration of monotonic reordering and refinement for simultaneous translation may not receive sufficient information with partial sequence to start generating a translation that respects the natural order of the target language. One of the approaches to address this problem is to perform anticipation 1 . Note that the nature of anticipation relies on interpreters' assumptions and the anticipation may provide incorrect translations. Alternatively, human interpreters strategically resort to producing monotonic translations that follow the word order of the source sequence  (Cai et al., 2020) . To illustrate the differences between the two strategies, the example in Figure  1  may be referred to. Of the two targets, offline target y respects the target language order and an online target ? roughly follows the source word ordering. Successful anticipation in Figure  1 's case would be to predict the initial words in y (I was a "frog in a well") before receiving the full x. This would pose difficulty even to professional translators as all the relevant information is in the latter part of the x (? ? ? ?I /"? ? ? ? a well / ? ? ?in / ? ? ? ? a f rog " ? ? ? ? ? ? ?was.).  Bart?omiejczyk (2008)  reports the success rate of human interpreters' anticipation attempts to be as low as 38.1% even though they make predictions based on pre-acquired domain knowledge. On the other hand, a monotonic approach would be to gen-erate an ? style translation -the grammaticality in the resulting sequence is sacrificed to translate only the received information. A similar case applies to Simultaneous Machine Translation (SimulMT) models, which start translating before a whole sentence is given. Several studies  (Ma et al., 2019; Arivazhagan et al., 2019;  often utilize offline full-sentence translation corpora to train SimulMT models. Offline full-sentence parallel corpora are expected to follow the natural order of languages and mostly contain source to offline-target pairs. Naturally, when SimulMT models are trained on these corpora, the models inevitably learn to perform anticipation. Recent SimulMT studies are focused on reducing anticipation  (Zhang et al., 2020a)  or performing better anticipation  (Zhang et al., 2020b ). On the contrary, studies on enabling monotonic translation in SimulMT are scarcely available. Recently,  Chen et al. (2020)  suggest utilizing pseudoreferences for monotonic translation. In this paper, we propose a paraphrasing method to generate a monotonic parallel corpus to allow a monotonic interpretation strategy in SimulMT. Our method consists of two stages. The method first chunks source and target sequences into segments and monotonically reorders the target segments based on source-target word alignment information (Section 3.1). Then, the reordered targets are refined to enhance fluency and syntactic correctness (Section 3.2). To show the effectiveness of our method, we train wait-k models  (Ma et al., 2019)  on the resulting monotonic parallel corpus of reordering-and-refinement. Results show improvements in BLEU scores over baselines and models producing monotonic translations. Our main contributions are as follows: ? We propose a method to reorder and refine the target side in an offline corpus to build a monotonically aligned parallel corpus for SimulMT. ? We investigate the monotonicity in different language pairs, and show monotonicity can be improved after the reordering-and-refinement process. ? We train widely used wait-k models on generated monotonic parallel corpora in multiple language pairs. The results show improvements over baselines in both translation quality and monotonicity.  

 Monotonicity Analysis In this section, we analyze the degree of word order differences in multiple language pairs, i.e., the monotonicity in different language pairs. To measure the monotonicity, two rank correlation statistics are utilized: Kendall's ? and Spearman's ?. The analyzed language pairs are: English-{Korean, Japanese, Chinese, German, French}. According to  Polinsky (2012) , English is a headinitial language and Korean and Japanese are rigid head-final languages; Korean and Japanese are likely to exhibit extreme word order differences with English. German and Chinese are considered a mixture of head-final and head-initial languages; they are likely to have word differences with English, but not as severe as Korean or Japanese. French is also head-initial, so English and French pair is likely to have similar word order. Figure  2  show monotonicity measurements between English and five different languages which vary in monotonicity: English-German and English-French pairs show high monotonicity, while English-Japanese and English-Korean pairs show low monotonicity. Lower monotonicity in language pairs presents higher difficulties for SimulMT tasks. For example, wait-k algorithm only sees k + t source tokens to generate a target token at step t which could lead to unwanted anticipation. To avoid such anticipation, as we mentioned in Section 1, human interpreters often provide a monotonic translation. In the same sense, we conjecture that promoting monotonicity in training corpora is beneficial for translation quality in SimulMT. 

 Monotonic Reordering and Refinement In this section, we describe our proposed paraphrasing method of chunk-wise reordering and refinement to generate monotonic corpus for SimulMT. Given source x = {x 1 , x 2 , ? ? ? , x |x| } and offline full sentence target y = {y 1 , y 2 , ? ? ? , y |y| }, an alignment a is defined as a set of position pairs of x and y. a = {(s, t) : s ? {1, ? ? ? , |x|}, t ? {1, ? ? ? , |y|}} First, in chunk-wise reordering phase, we generate source chunk set C X C X = {(x 1:p 1 ), (x p 1 +1:p 2 ), ? ? ? , (x p k?1 +1:p k )}, where 0 < p 1 < p 2 < ? ? ? < p k = |x| and re- ordered target chunk set C Y C Y = {(y 1:q 1 ), (y q 1 +1:q 2 ), ? ? ? , (y q k?1 +1:q k )}, where 0 < q 1 < q 2 < ? ? ? < q k = |y|, and y i ? y is reordered target token from offline target y. The elements of a reordered target chunk C Y i are corresponding target tokens of a source chunk C X i based on given alignment information a. Also, we preserve the original target order within each C Y i . For example, offline and reordered target in Figure  1  correspond to y and y respectively, and both sequences are only different in token orders. The number of source chunks in one sentence is the same as the number of reordered target chunks (|C X | = |C Y |), while the number of tokens in |C X i | and |C Y i | could vary. We experiment two chunking methods; fixed-size chunking and alignment-aware adaptive size chunking. Given chunked sets C X and C Y , we refine reordered target tokens to generate more natural and fluent sentence with a Non-Autoregressive Translation (NAT) model. In the refinement algorithm, final paraphrased sentence ? is generated from reordered sequence y . Furthermore, we incorporate an Autoregressive Translation (AT) model into our refinement process. The more detailed steps for each phase will be explained in the following subsections. 

 Chunk-wise Reordering 

 Fixed-size Chunk Reordering In the fixed-size chunk reordering method, we simply chunk a sequence of tokens into fixed size segments. The source chunk set C X in this chunking method is as follows: C X = {(x 1:K ), (x K+1:2K ), ? ? ? , (x [|x|/K]:|x| )}, where K ? [1, |x|] is chunk size. If k = 1, C X is identical with x. We conduct subword operation such as sentencepiece or BPE after chunking process in order to avoid subword separation. 

 Alignment-Aware Chunk Reordering In the alignment-aware chunking method, we segment a sentence adaptively by leveraging alignment information a, as described in Algorithm 1. The left grid in Figure  3  presents the subword alignment between source and target sentence. We run aligner on subword over word because the alignment performance is consistently better  Zenkel et al. (2020)  when using GIZA++  (Och and Ney, 2003)  , which we use in our experiments. Based on this alignment information, we initialize a list of chunks C. As observable, there are some tokens which have no alignment information. To avoid omission, we assign the same alignment as the previous token; if a token is at the head, it follows the next token's alignment. To ensure subwords can be properly detokenized after reordering, we merge mid-splitted subwords. The middle grid in Figure  3  presents the result of these initialization steps. After initialization, we generate consistent chunks by merging all the inconsistent ones, following the definition of consistency in  Zens et al. (2002) . In a consistent chunk, tokens are only aligned to each other, not to tokens in other chunks. If any chunk in C has size smaller than a minimum size threshold ?, we merge a chunk pair that are adjacent in both source and target side and have the shortest target distance between them. If the distances are the same between multiple candidate pairs, we choose the pair of chunks that makes the smallest size after merging. We additionally merge the chunks adjacent to the merged one if they are arranged monotonically. Merging is repeated until all chunks meet the size requirements. An example of final result is the right grid in Figure  3 . Phrase extraction method used in statistical machine translation  Koehn (2004)  also makes phrase level alignments from word alignments using heuristics like ours, but it tends to choose shorter phrases since the number of co-occurrences decrease drastically as the phrase size grows, which makes it difficult to generate larger chunks to prevent hurting grammatical correctness while reordering phase. 

 Refinement Reordered target results from previous phase inevitably entail irregularities mainly for two rea- One could be broken connectivity of collocations in segmentation process. The other would be disfluently missing or containing words of endings and preposition as the position of chunk has been changed, thus requiring an addition of new words or clearing unnecessary words. In this part, we focus on refining aforementioned anormalities in order to enhance fluency, while preserving the monotonicity at the same time. 

 Refinement with NAT We iteratively decode partial source C X with pretrained translation model, given partial reordered target C Y as a guidance in order to generate corresponding online target ? . More specific process is explained in Algorithm 2. As the model refines given [ ?i?1 ; C Y i ], previous refined output ?i?1 could be altered as the model re-generates the entire sequence from scratch. Similarly in re-Algorithm 2: Chunk-wise Refinement Input: Source and target chunks C X , C Y Output: Paraphrased target ? 1 i = 1 2 ?0 = [] 3 while i ? |C X | do 4 X i = C X 1:i and Y i = [ ?i?1 ; C Y i ] 5 ?i = arg max Y log p R (Y |X i , Y i ) 6 i = i + 1 7 end Return: ?|C X | translation  (Arivazhagan et al., 2020; Han et al., 2020b) , we set an option of fixed or alterable prefix to force the model whether to generate same target prefix of ?i?1 or to allow the model to modify the prefix. As we limit the visibility of source information and iteratively generate target tokens with increasing source chunks, we expect the refinement model to generate monotonically aligned and paraphrased targets ? with enhanced fluency. We use NAT architecture as the core refinement model R. In NAT inference, the model's decoder is first given source features and fed an empty target sequence. Then the NAT decoder develops the empty sequence into a translation of the source sequence. This development is often iterative. Note that at every iteration step, the target sequence is refined -closer to the source sequence in meaning and become more fluent. This motivates us to utilize NAT architecture in our refinement process for monotonic-yet-disfluent sequences. In our approach, the NAT model starts refinement iteration with initialized tokens of previous output and reordered target chunk Y i , instead of an empty sequence. This target initialization act as a weak supervision to generate monotonically aligned target, which allow model to focus only on the fluency the reordered targets. 

 Incorporation of AT Despite the aptness of NAT structure to our refinement phase, NAT model entails a performance degradation compared to AT model in the expense of speedup. Also, there exists repetition problem in NAT  (Lee et al., 2018; Gu and Kong, 2021)  which is generated in the process of multiple chunkwise iterative refinement. In order to complement the aforementioned weaknesses of NAT decoding, we incorporate AT into our refinement process with NAT model.The final probability is computed jointly with the probability of AT and NAT model: p R (Y |X i , Y i ) ? p AT (Y |X i ) ? ? p N AT (Y |X i , Y i ) (1?) , (1) where ? ? [0, 1] is hyper-parameter deciding the ratio between AT and NAT probability.  

 Experiments 

 Dataset In this section, we describe the utilized datasets. Detailed statistics are presented in Table  1 . Utilized EnKo trainset and devset are created using in-house translation corpora while test scores are reported on IWSLT17  (Cettolo et al., 2017)  EnKo testset. The DeEn trainset of WMT15 translation task  (Bojar et al., 2015)  is utilized. newstest2013 is utilized as devset and newstest2015 is used as testset. The EnJa trainset and validset are respectively the combination trainsets and validsets of KFTT  (Neubig, 2011) , JESC  (Pryzant et al., 2018) , TED  (Cettolo et al., 2012) . The trainset and validset are used as preprocessed and provided by the MTNT authors 2  (Michel and Neubig, 2018) . Only the TED portion of testsets is used. For EnZh training UN Corpus v1.0  (Ziemski et al., 2016)  is used. Trainset, devset, and testset follow the original splits. Monotonicity of EnFr in Figure  2  is measured on the WMT14  (Bojar et al., 2014)  trainset. Additional details regarding utilized tokenization and vocabulary training are listed in Appendix A. 

 Metric All the BLEU scores are cased-BLEU measured using sacreBLEU  (Post, 2018) . Test scores are measured using models that report best BLEU on their respective devsets. All references and translations of each Korean, Japanese, and Chinese languages are tokenized prior to BLEU evaluation. Tokenizers utilized are mecab-ko 3 , KyTea 4 , and jieba for Korean, Japanese and Chinese respectively. We report detokenized BLEU on DeEn results. To measure monotoniticy, we use Kendal's ? rank correlation coefficient. 

 Implementation Details The default setting for NMT and SimulMT models follow the base configuration of transformer  (Vaswani et al., 2017)   

 Corpus Generation and Training We demonstrate the effectiveness of our reorderingand-refinement method by training wait-k models on the resulting datasets. The wait-k models are trained on the combination of the monotonically aligned training pairs and offline trainset. AlignAw + NAT + AT denotes monotonically aligned corpora using alignment-aware reordering and refinement using joint probability of NAT and AT models. And Offline refers to the offline full-sentence corpus. 

 Reordering Fixed: For fixed-size reordering, we experiment with chunk sizes K ? {4, 6, 8, 10, 12}. In waitk training, k and K are matched. All fixed-size reordered-and-refined corpora have the same size as corresponding offline corpus. AlignAw: For each corpus, we generate four variations of alignment-aware reordering with source and target minimum chunk size of 2, 3. Alignmentaware reordering is not applicable on the alreadymonotonic cases and the sentence pairs which are locally non-monotonic inside a chunk and globally monotonic among chunks within a single pair -typically, the reordering method is applicable to 20% to 50% of offline corpus. We gather unique pairs from the created four variations to generate the final reordered pairs. The statistics of reordered set for each translation direction is in  

 Refinement NAT: NAT models are utilized to refine the reordered pairs. Both the fixed prefix and alterable prefix refinement is performed and combined. BertScore  (Zhang et al., 2020c)  is measured and used to discard refinement results that show below average scores. The size of the resulting set is the same as the corresponding offline corpus. NAT + AT: NAT and AT models can both be utilized to jointly compute token probability in refinement (Section 3.2.2). The AT models utilized are the baseline wait-k models trained on offline corpora. We experiment with ? ? {0.25, 0.5}. The examples of reordered-and-refined sequences can be found in Appendix E. 

 Results and Analysis 

 Experimental Results on EnKo Table  2  shows BLEU scores and Kendal's ? s of wait-k models trained using original offline corpus and variations of reordered-and-refined corpus. We observe that the models trained on monotonically reordered-and-refined corpora show higher BLEU scores and monotonicity. Reordering: Of the variations, corpora including AlignAw chunking process generally show better BLEU scores over Fixed + NAT when k ? 6. This could be the benefit of the semantically plausible way to split sentences provided by AlignAw chunking. On the other hand, models trained with Fixed + NAT corpora show higher BLEU when k ? 8. Refinement: Experiments on utilizing AT probabilities show degraded BLEU scores in k ? 6, 8, 10. On the contrary, the models trained on AlignAw + NAT + AT corpora show enhanced monotonicity. The ? value may be adjusted to make trade-off between promoting monotonicity in translation or enhancing translation quality in terms of BLEU. Repetition Reduction with AT: Following  (Welleck et al., 2020) , we report n-gram repetition rate, seq-rep-n, on each generated corpus in Table  3 . We observe from seq-rep-n in all of the tested n values, that employing AT models in refinement help alleviating the repetition problem of posed by NAT models. 

 Language Pairs Comparison Figure  4  shows the difference in monotonicity between different language pairs: EnKo, EnJa, EnZh, and DeEn. It is observable in Figure  4  that the overall monotonicity in EnKo and EnJa pairs is enhanced after paraphrasing, while monotonicity scores of DeEn remain almost the same, only showing slight improvement. The extent of monotonicity enhancement in EnZh is between that of EnKo/Ja and DeEn. In all language pairs, the enhancements are generally lower in long or very short sequences. In the case of long sequence pairs, a pair may contain multiple sequences and be already aligned at the sequence level, thus resulting in marginal monotonicity enhancement. In the case of shorter length sequences, the whole sentence may be merged into a single chunk, less benefiting from our process. After the reordered sets are refined, monotonicity marginally decreases. This is expected as forcibly aligned tokens are refined to augment the fluency in the resulting sentence. To present the effectiveness of generated monotonic corpus in different language pairs, we train wait-k models on EnKo, EnJa, EnZh, and DeEn, and report BLEU and Kendal's ? of the models in Figure  5 . The horizontal dotted line presents the BLEU of unidirectional offline model. The important observation we can find is that the monotonicity increment of wait-k model in Figure  5  is proportional to that of generated monotonic corpus in Figure  4 , suggesting that promoting monotonicity in training corpus is beneficial for SimulMT models to generate monotonic output, especially in language pairs with differing word orders. Within two paraphrasing methods, Fixed + NAT and AlignAw + NAT , we can see that the monotonicity of Fixed + NAT is always in between that of Offline and AlignAw + NAT in Figure  5 , and the gap increases as the k value get higher. While our methods are effective in EnKo, the performance of suggested method is similar or lower than that of baseline in EnJa. We presume that the ineffectiveness in EnJa is due to its short average sentence length with highest emission rate, as shown in Table  1 . A short sentence often cannot preserve semantic properties while being split into chunks and reordered. For example, Fixedlength reordering always chunks all sentences ignores such feature and only increases disfluency in the chunked result. Also, even though AlignAw enforces the consistency requirement on the chunks, adjustment of such requirement like changing minimum chunk size may be required considering the high emission rate. Based on the highest performance at k = 8, there is about 8% BLEU improvement over Offline in EnKo whereas there is about 3% improvement in EnZh and about 2% improvement in DeEn. It is roughly proportional to the monotonicity improvements shown in Figure  4 . 

 Evaluation on Online References We test wait-k models on our in-house EnKo online and offline testsets of 150 lines. We choose EnKo because the impact of reordering-and-refinement is the greatest in that pair. The source sentences of both testsets are identical. The online references are constructed by a professional interpreter under a simulated simultaneous interpretation scenario and the offline references are constructed by the same interpreter assuming a typical translation scenario. In construction of online references interpreter was encouraged to perform monotonic interpretation rather than anticipation. BLEU scores are computed with both online and offline references for each trained model. Figure  6  plot the subtraction of BLEU scores on offline references from BLEU scores on online references. It is noticeable that the wait-k models trained on offline corpus have negative value while all the models trained on generated corpus present positive values, which implies the effectiveness of our approach. Overall, the substantial differences at k = 6 may suggest that the chunk size utilized by human interpreter has comparable value. 

 Related Work Due to word order differences between languages, SimulMT training often face situations where anticipation is required. Note that word order difference is observed to be problematic even for human interpreters  (Al-Rubai'i, 2004; Tohyama and Matsubara, 2006) .  Chen et al. (2020)  suggest using pseudo-references which involve utilizing wait-k inference output to limit "future anticipation" in training.  Zhang et al. (2020b)  utilize NMT teachers to implicitly embed future information in their SimulMT students for better anticipation performance.  Zhang et al. (2020a)  study adaptive policy to tackle this problem -authors suggest an adaptive SimulMT policy that dictate READ/WRITE actions based on whether "meaningful units" are fully formed with consumed input tokens. Related work in the broader SimulMT and para-phrasing domain is presented in Appendix F. 

 Conclusion Most of SimulMT models are trained on offline translation corpora, which could lead to limitation in translation quality and achievable latency, especially in non-monotonic language pairs. To address this problem, we propose a reordering-andrefinement algorithm to generate monotonically aligned online target with NAT model. We then train widely used wait-k SimulMT models on this newly generated corpus. Resulting models show BLEU score improvement and significant enhancement on monotonicity in multiple language pairs. 
