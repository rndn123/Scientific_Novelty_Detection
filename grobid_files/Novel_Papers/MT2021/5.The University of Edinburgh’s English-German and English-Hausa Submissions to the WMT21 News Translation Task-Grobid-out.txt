title
The University of Edinburgh's English-German and English-Hausa Submissions to the WMT21 News Translation Task

abstract
This paper presents the University of Edinburgh's constrained submissions of English-German and English-Hausa systems to the WMT 2021 shared task on news translation. We build En-De systems in three stages: corpus filtering, back-translation, and fine-tuning. For En-Ha we use an iterative back-translation approach on top of pre-trained En-De models and investigate vocabulary embedding mapping.

Introduction We describe the University of Edinburgh's participation in English?German (En?De) and English?Hausa (En?Ha) at the WMT 2021 news translation task. We apply distinct sets of techniques to the two language pairs separately, as the two pairs are very different in terms of language proximity and the availability of resources. We follow the constrained condition where we only use the provided data available to all participants. For En?De we first employ rule-based and dual conditional cross-entropy filtering to clean the datasets. Then we add to training back-translations generated in a few ways: tagged, greedy, beam search and sampling. We fine-tune our models on past years' test sets, and finally tune a few configurations: length normalization, test sentence splitting, and German post-processing. For En?Ha we adopt iterative back-translation, where at each iteration we initialize the model parameters from an En-De model in the corresponding direction (En?De for En?Ha and De?En for Ha?En). These En-De models are trained in the same way as those submitted to the En-De task, except that their vocabulary includes subwords from the Hausa language. Besides, we experiment with vocabulary mapping at the embedding level. Some configurations are kept consistent across language pairs and systems. Sentences are tok-enized using SentencePiece  (Kudo and Richardson, 2018)  with a 32K shared vocabulary, except that we added a few extra tokens for tagged backtranslation. All models are trained following Marian's Transformer-Big task preset  (Vaswani et al., 2017; Junczys-Dowmunt et al., 2018)  unless otherwise specified: 6 encoder and decoder layers, 16 heads, 1024 hidden embedding size, tied embeddings  (Press and Wolf, 2017), etc. 1  Section 2 and Section 3 describe the detailed model building process for En?De and En?Ha respectively. While awaiting human evaluation results, we summarize our automatic metric scores on the WMT 2021 test sets computed by the task organizers in Table  1 .  

 English?German 

 Data and cleaning English-German is considered to be a highresource language pair, with over 90 million parallel and hundreds of millions monolingual sentences provided in the shared task. Following our last year's submission  (Germann, 2020) , we divide the data into three categories, and we use all the parallel data, as well as monolingual news from 2018 to 2020: ? High-quality parallel: News Commentary, Europarl and Rapid. ? Crawled parallel: ParaCrawl, WikiMatrix, CommonCrawl, and WikiTitles. ? Monolingual news: News Crawl The majority of parallel data are mined and aligned sentences from the web  (Ba?n et al., 2020; Schwenk et al., 2021) , so our first step is corpus filtering to remove noisy sentences which could harm neural machine translation  (Khayrallah and Koehn, 2018) . We run rule-based filtering using FastText language identification  (Joulin et al., 2016) , and various handcrafted features such as sentence length, character ratio and length ratio. Similar rules are applied on the monolingual data, omitting the features designed for parallel data. More details can be found in our cleaning script which is made public.  2  We then train seed Transformer-Base models on the filtered high-quality data, as well as the crawled data separately, to (self-)score translation crossentropy of the crawled parallel sentences. This enables us to rank and filter out sentences by their dual conditional cross-entropy (Junczys-Dowmunt, 2018). The method prefers the sentences in a pair to have low and similar translation cross-entropy given each other. After empirical trials, we find it is always better to score using models trained on the high-quality data, and we choose to keep the best 75% of the crawled data. The filtering efforts are reported in Table  2 . Next, we train Transformer-Big models on the combination of filtered highquality and crawled data. These models serve as baselines and are used for back-translation later.  

 Amount of crawled 

 Back-translation Since its introduction, back-translation  (Sennrich et al., 2016)  has been widely used to boost NMT. We use ensembles of our best seed and baseline models trained on the filtered data, to generate back-translations from the monolingual news data from 2018 to 2020, hoping that the domains are similar to that of the test. For En?De we mix back-translations generated using greedy search, beam search, and sampling; for De?En, we adopt tagged back-translation  (Caswell et al., 2019) . After merging the original and back-translated data, for each direction we train 4 standard Transformer-Big models, as well as a model with 8 encoder layers and 4 decoder layers. Specifically for De?En, we have an extra pre-layer normalized variant. As we observed last year, validation BLEU does not improve after we add back-translated data to training. As a result, after the models converge, we continue training them on filtered parallel data only. The models' validation BLEU scores 3 on WMT19 test are displayed in Table  3 .  

 Configuration 

 Fine-tuning and submission We grid search on length normalization during decoding, and find 1.2 to be ideal for En?De and 0.8 for De?En. Particularly for En?De, we have two more steps to make German text read more natural: 1) continued training on 25% title-cased parallel data to improve headline translation and 2) post-processing on German quotes to make them consistent. Previous submissions show that fine-tuning on past years' test data helps model performance  (Schamper et al., 2018; . In the early years of WMT news translation tasks, the test sentence pairs can originate in either source or target language, and are translated and merged into one set. However, the current evaluation is on translating sentences originally in the source language only. Therefore, we experiment with fine-tuning on the combined sets, as well as on sentence pairs originated from the source language. We fine-tune all our models on WMT 2008-2019 test sets and validate on WMT 2020 test set. While the training data contain mainly one sentence per line, the test set can have multiple sentences in the same segment. As a result, we split each test instance into single sentences, translate, and rejoin them. We experiment with fine-tuning and sentence splitting on the 8-encoder-4-decoder variant for both languages. For each translation direction, we apply the best configuration to each model and ensemble them by averaging their predictions post-softmax. Overall, we have a 5-model ensemble for En?De, and a 6-model ensemble De?En. 

 English?Hausa 

 Data The main sources of English-Hausa parallel data are OPUS  (Tiedemann, 2012)  and ParaCrawl. We also include data from WikiTitles 4 and the Khamenei 5 corpora, which are however much smaller. In total, we gather 759,061 parallel sentences. For back-translation, we use 9.5 million monolingual Hausa sentences from Common Crawl, Extended Common Crawl, and News Crawl provided by the task organizers. We randomly select 50 million English monolingual sentences from the News Crawl collections from 2018, 2019, and 2020. For training, we use a mix of back-translated monolingual data and parallel data. Since the dataset sizes differ substantially, we over-sample the parallel data to achieve a balanced mix: 10? for English?Hausa, and 50? for Hausa?English. Similar to our En-De models, we used tagged backtranslation to distinguish synthetic and authentic sentences in the data. 

 Iterative back-translation and fine-tuning In our experiments, we combine a transfer learning approach  (Zoph et al., 2016; Kocmi and Bojar, 2018)  with 3 iterations of back-translation  (Hoang et al., 2018; Edunov et al., 2018) . In each iteration, we initialize the En?Ha model with a pre-trained En?De Transformer-Big model (and vice versa for the other direction). Then, we fine-tune the model on the English-Hausa data created by the model from the previous back-translation iteration (the initial model for the first iteration is fine-tuned on parallel data only). We notice that the model generates a large number of empty translations. We suppress this issue by taking the second-best candidate translation from the n-best list if the first one is empty. Another problem is heavy overfitting in the models. In many translations, the sentences begin with the prefix "Never miss an important update!", followed by the actual translation. Unfortunately, we only noticed this issue after the submission. 

 Vocabulary embedding mapping An additional approach we investigate is mapping the Hausa vocabulary to the German embeddings of the En?De model, when initializing the En?Ha model. We train the models with a 32K Senten-cePiece vocabulary obtained from datasets in all three languages. Using the frequency-based metric introduced by  (Wang et al., 2020)  we assign each SentencePiece token to an English, German, Hausa or joint vocabulary. This results in 9192 German tokens, 6485 Hausa tokens and a joint vocabulary of approximately 11k. Having established a separate Hausa and German vocabulary it is then possible to map between the embeddings of the two. In order to map the vocabularies, we independently train BWEs (bilingual word embeddings) using an implementation of Bivec  (Luong et al., 2015)  combined with FastText  (Bojanowski et al., 2017) . This implementation uses a joint learning objective as described by  Liu et al. (2020)  utilising alignments combined with sub-word information. In lieu of a parallel De-Ha dataset an En?De NMT model is used to translate the English side of the En-Ha dataset. We constrain SentencePiece encoding using the previously extracted vocabularies for example the Huasa data is encoded using only the Hausa tokens and the joint tokens. Once both sides are encoded FastAlign is used to extract automatic alignments and the BWEs are trained. We first map the Hausa tokens to their nearest neighbour using the Cross-Domain Similarity Local Scaling  (Lample et al., 2018)  distance metric in the order of Hausa tokens' frequency, and only permit a German token to be mapped to exactly one Hausa token. For tokens that do not have a one-to-one mapping, we adapt Gu et al. (  2018 )'s approach, whereby the embedding of a Hausa token is initialized to the weighted sum of all German embeddings. The weights are given by a probability distribution derived from the distance of the Hausa token to each German token in the bilingual embedding space. It is worth noting that we only map between the tokens in the Hausa and German vocabularies not any of the joint tokens. Finally, we initialize the embedding table using the new embeddings and remove all tokens identified as German. After initialization, we fine-tune the model using the parallel and back-translated data as described previously. Our experiments show that although initializing the embedding table using a mapping-based approach results in faster model convergence, it does not improve the final BLEU score compared to just fine-tuning from the En-De models. This was observed for both the parallel data and the combined parallel and back-translated data. The outputs of the mapping approach to the baseline for the Ha-En system are qualitatively very similar and indicates that while the embedding mapping increases convergence there is no knowledge transfer from the German embeddings. 

 Conclusion We describe our English-German and English-Hausa submissions to the news translation task at WMT 2021. For the En?De task, fine-tuning and splitting test instances significantly boosts BLEU while back-translation alone does not help. In the En?Ha task, we experiment with interesting low resource NMT techniques, but unfortunately, our submission contains translations from overfitted models. Table 3 : 3 Average BLEU scores of BT experiments on WMT19 test used as dev. De?En En?De Baseline 42.2 43.4 + BT 41.8 43.0 + cont. training 42.5 43.6 
