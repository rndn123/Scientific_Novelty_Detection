title
Cross Attention Augmented Transducer Networks for Simultaneous Translation

abstract
This paper proposes a novel architecture, Cross Attention Augmented Transducer (CAAT), for simultaneous translation. The framework aims to jointly optimize the policy and translation models. To effectively consider all possible READ-WRITE simultaneous translation action paths, we adapt the online automatic speech recognition (ASR) model, RNN-T, but remove the strong monotonic constraint, which is critical for the translation task to consider reordering. To make CAAT work, we introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. We implement CAAT with Transformer while the general CAAT architecture can also be implemented with other attention-based encoder-decoder frameworks. Experiments on both speech-to-text (S2T) and text-to-text (T2T) simultaneous translation tasks show that CAAT achieves significantly better latency-quality trade-offs compared to the state-of-the-art simultaneous translation approaches. 1

Introduction Simultaneous translation, which starts to translate input sentences before they are finished, is of importance to many real-life applications such as teleconference systems and time-sensitive spoken document analysis and conversion. While a substantial progress has been made on offline machine translation  (Wu et al., 2016; Vaswani et al., 2017; Hassan et al., 2018) , more research on simultaneous translation is yet highly desirable. Central to the task is performing high-quality low-latency translation, which involves the key challenges of developing optimal policies for the READ-WRITE action paths  1  The code is available at https://github.com/ danliu2/caat. as well as generating high-quality target sequences based only on partial source sequences. This paper aims to optimize the policy and translation model jointly, by expanding target sequences with blank symbols for READ actions. The loss function can be defined as negative loglikelihood (NLL) of marginal distribution through all expanded paths. A similar problem in automatic speech recognition (ASR) has been tackled with RNN-T (Recurrent Neural Network Transducer)  (Graves, 2012 ) by an efficient forward-backward algorithm. However, RNN-T is trained based on the monotonic alignment between source and target sequences, which is not suitable for simultaneous translation, as it cannot properly consider reordering. On the other hand, the forward-backward algorithm is not available for attention-based encoderdecoder  (Bahdanau et al., 2015)  architectures, including Transformer  (Vaswani et al., 2017) , due to the deep coupling between source contexts and target history contexts. To solve this problem, we separate the cross attention mechanism from target history representation in attention-based encoder-decoder, which can also be viewed as RNN-T with the joiner being augmented by cross attention mechanism, resulting in Cross Attention Augmented Transducer (CAAT). However, cross attention mechanism removes the alignment constraint in RNN-T which originally encourages an appropriate latency. To ensure latency under control, jointly minimizing a latency loss is required. Both the NLL loss and latency loss can be efficiently optimized by a forward-backward algorithm. The main contributions of this paper are threefold: (1) We propose a novel architecture, Cross Attention Augmented Transducer, which jointly optimizes the policy and translation model by considering all possible READ-WRITE simultaneous translation action paths. (2) We introduce a novel latency loss whose expectation can be optimized by a forward-backward algorithm. Training with this latency loss ensures the latency of CAAT simultaneous translation model to be under control. (3) The proposed model achieves significantly better latency-quality trade-offs compared to the state-ofthe-art simultaneous translation approaches. 

 Related Work Recent work on simultaneous translation falls into two categories. The first category uses a fixed policy for the READ/WRITE actions.  Cho and Esipova (2016)  propose simultaneous translation with the wait-if-* policy for an offline model.  Ma et al. (2019)  propose a wait-k policy for both the training and inference period. The second category includes models with a flexible policy learned and/or adaptive to current context.  Gu et al. (2017)  introduce an agent trained by reinforcement learning from the interaction with a pre-trained offline neural machine translation model.  Zheng et al. (2019a)  train the agent by supervise learning with label sequences generated via the rank of golden target words given partial input. A special subcategory of flexible policy jointly optimize policy and translation by monotonic attention customized to translation model, e.g., Monotonic Infinite Lookback (MILk) attention  (Arivazhagan et al., 2019)  on RNN encoder-decoder  (Bahdanau et al., 2015)  and Monotonic Multihead Attention (MMA)  (Ma et al., 2020c)  on Transformer  (Vaswani et al., 2017) . End-to-end speech-to-text (S2T) simultaneous translation has been investigated in  (Ma et al., 2020b,d; Ren et al., 2020) , among which  Ma et al. (2020b)  adapt latency metrics from T2T simultaneous translation to S2T simultaneous translation, and experiment with both the fixed and flexible policy.  Ma et al. (2020d)  study the effect of speech block processing on S2T simultaneous translation.  Ren et al. (2020)  experiment with the wait-k policy based on a source language CTC segmenter. In our work, we optimize the marginal distribution of all expanded paths motivated by RNN-T  (Graves, 2012) . Unlike RNN-T, the CAAT model removes the monotonic constraint, which is critical for considering reordering in machine translation tasks. The optimization of our latency loss is motivated by Sequence Discriminative Training in ASR  (Povey, 2005) . 

 Preliminaries 

 Notations and formulation Let x and y denote the source sequence and target sequence, and f and g the encoder and decoder function, respectively. For simultaneous translation, let a j denotes the length of source sequence processed when deciding the target y j . The policy of simultaneous translation is denoted as an action sequence p ? {R, W } |x|+|y| where R denotes the READ action and W the WRITE action. If the READ action is replaced with a blank symbol ?, the policy can also be represented by the expanded target sequence ? ? (V ? {?}) |x|+|y| , where V is the vocabulary of the target language. Note that removing all ? in ? results in the original target sequence y. The mapping from y to sets of all possible expansion ? is denoted as H(x, y). 

 Recurrent Neural Network Transducer RNN-T  (Graves, 2012)  draws condition probability Pr(y|x) by marginalizing all possible alignment paths as : Pr (y|x) = ?H(x,y) Pr(?|x) = ?H(x,y) |x|+|y| k=1 Pr(? k |i k , j k ) (1) where i k and j k denote the source and target position of the k-th element in ?, respectively, and ? = (? 1 , ?2 , ..., ?|x|+|y| ) ? H(x, y) ? {V ? ?} |x|+|y| corresponds to a possible expansion path which yields y after removing the blank symbol ?. As shown in Figure  1 (b), to calculate P (? k |h i k , y <j k ), RNN-T divides decoder into predictor and joiner, where the predictor, denoted f pred , produces target history representation (Eq. (  2 )), and the joiner products output probability Pr(y|i, j) by joint representations from predictor and encoder (Eq. (  3 )). h pred j = f pred (y <j ) (2) Pr(y|i, j) = sof tmax(W h h i + W p h pred j ) (3) Though named as RNN Transducer, other sequence processing architectures work well as the encoder or predictor, e.g., Transformer  Yeh et al., 2019) . Online decoding is natural for RNN-T if the encoder works with streaming input, which makes RNN-T widely adopted in both the online and offline ASR tasks. One drawback of RNN-T is that it is based on a monotonic alignment between the input and output sequence, making it unsuitable for sequenceto-sequence tasks with reordering, e.g., machine translation. 4 The Proposed Method The goal of simultaneous translation is to achieve high translation quality and low latency. A natural loss function hence measures the NLL loss of marginal conditional distribution and expectation of latency metric through all possible expanded paths: L(x, y) = L N LL (x, y) + L latency (x, y) = ? log ? Pr(?|x) + E ?l(?) = ? log ? Pr(?|x) + ? Pr(?|y, x)l(?) (4) where Pr(?|y, x) = Pr(?|x) ? ?H(x,y) Pr(? |x) , ? ? H(x, y) is one of the expanded paths of the target sequence y, and l(?) is the latency loss for path ?. As the total number of expanded paths is exponential with regard to |x| + |y|, computing the marginal probability ?H(x,y) Pr(?|x) is non-trivial. RNN-T solves this with a forwardbackward algorithm  (Graves, 2012) , which inherently requires paths in the graph to be mergeable. That is, the representations of the same location in different paths should be identical. Conventional attention-based encoder-decoder architectures as shown in Figure  1 (a), however, do not satisfy this requirement. Take Figure  2  as an example, the decoder hidden states for the red path ?1 and the blue path ?2 are described below (we denotes s n i as the representation of the i-th decoder step in the expanded path ?n ) : s 1 2 = g(s 1 1 , h ?2 ) = g(g(s 0 , h ?2 ), h ?2 ) (5a) s 2 2 = g(s 2 1 , h ?2 ) = g(g(s 0 , h ?1 ), h ?2 ) (5b) The decoder states at output step 2 with different history paths, s 1 2 and s 2 2 , are not identical. This is due to the coupling of source and previous target representation by the attention mechanism in the decoder. The same problem exists in Transformer, from the coupling of self-attention and encoderdecoder cross attention in each block. To solve this, we separate the cross attention mechanism from the target history representation, which is similar to the joiner and predictor in RNN-T. The novel architecture, as shown in Figure  1(c) , can be viewed as an extended version of RNN-T with the joiner augmented by cross attention mechanism, and is named as Cross Attention Augmented Transducer (CAAT). Different from RNN-T, the joiner in CAAT is a complex architecture with attention mechanisms as in Eq. (  6 ): s i,j = s(attn(h pred j , h enc ?i ), h pred j ) (6) Note that s i,j is independent of previous nodes s i ,j in path ?, and the same location from different paths in Figure  2  produces the same state representation. By analyzing the diffusion of the output probability through the lattice in Figure  2 , we can find that Pr(y|x) is equal to the sum of probabilities over any top-right to bottom-left diagonal nodes. Defining the forward variable ?(i, j) as the probability of outputting y [1:j] during x [1:i] , and the backward variable ?(i, j) as the probability of outputting y [j+1:|y|] during x [i:|x|] , we can draw the marginal likelihood Pr(y|x) as : Pr(y|x) = (i,j):i+j=m ?(i, j) ? ?(i, j) L N LL (x, y) = ? log Pr(y|x) (7) where 1 ? m ? |x| + |y|. The detailed derivation of NLL loss of CAAT can be found in Appendix A.1. The proposed CAAT can be implemented with a variety of attention-based encoder-decoder frameworks. In this paper, we implemented CAAT with Transformer, by dividing Transformer's decoder into the predictor and joiner module. As shown in Figure  3 , the predictor and joiner share the same number of transformer blocks as the conventional transformer decoder, but there are no crossattention blocks in the predictor module and no self-attention blocks in the joiner. 

 Multi-Step Decision The CAAT architecture gains the ability of handling source-target reordering at the cost of an expensive joiner. The complexity of joiner is O(|x| ? |y|) during training. For RNN-T, the joiner is efficient because only softmax operates at O(|x| ? |y|). But for CAAT, joiner takes up half of the parameters of decoder, which means the complexity of CAAT is about |x| 4 times higher than the conventional encoder-decoder framework during training. RNN-T needs to ensure the output timing of y j is the corresponding source frame a j = align(x, y j ). However, based on attention mechanism, CAAT only needs to ensure output timing to be after the corresponding position (a j ? align(x, y j )). Therefore, it is no longer necessary to make decision each encoder frame; the decision step size d > 1 is appropriate for CAAT, which reduces the complexity of the joiner from O (|x| ? |y|) to O |x|?|y| d . Besides, the decision step size is also an effective way to adjust latency-quality trade-off. 

 Latency Loss CAAT relaxes the restriction of output timing by attention mechanism, which means all source step i ? align(x, y j ) should be appropriate for output y j , including the offline path (?j : a j = |x|). To avoid the CAAT model bypassing online policy by choosing the offline path, the latency loss L latency (x, y) as defined in Eq. (  4 ) is required.  (Povey, 2005) , we optimize the latency loss with the forward-backward algorithm. To calculate the expectation of latency loss through all paths ?, mergeable is also a requirement to the latency loss definition, which means the latency loss through path ? may be defined as l(?) = |x|+|y| k=1 l(? k ) and l(? k ) is independent of l(? k =k ). However, both Average Lagging  (Ma et al., 2019)  and Differentiable Average Lagging  (Arivazhagan et al., 2019)  do not meet this requirement. We hence introduce a novel latency function as follows: 

 Motivated by Sequence Criterion Training in ASR l(i, j) = 1 |y| max i ? j ? |x| |y| , 0 (8) l(? k ) = 0 if ?k = ? l(i k , j k ) else (9) l(?) = |?| k=1 l(? k ) (10) where i k = k k =1 I(? k = ?) and j k = k k =1 I(? k = ?) denote the number of READ and WRITE actions before ?k , respectively. The maximization operation is used to avoid encouraging over-aggressive decision paths. This latency definition is not rigorous enough to be an evaluation metric for the under-estimation after source ended, as analyzed in  (Arivazhagan et al., 2019) , but it can still be used as a loss function. By defining the forward latency variable ? lat (i, j) as the expectation of latency of outputting y [1:j] during x [1,2,? ,i] , and the backward latency variable beta lat (i, j) as the expectation of latency of outputting y [j+1:|y|] during decision steps x [i,? ,|x|] , the latency loss can be drawn as: c(i, j) = ? lat (i, j) + ? lat (i, j) L latency (x, y) = E ?H(x,y) l(?) = (i,j):i+j=m ?(i, j)?(i, j)c(i, j) Pr(y|x) where 1 ? m ? |x| + |y|. The detailed derivation of latency loss of CAAT can be found in Appendix A.2. 

 Offline Auxiliary Loss We add the negative log-likelihood loss of the offline translation path as an auxiliary loss to CAAT model training for two reasons. First, we hope the CAAT model falls back to offline translation in the worst case; second, the CAAT translation is carried out in accordance with offline translation when a source sentence finishes. The final loss function for CAAT training is defined as follows: L(x, y) = L N LL (x, y) + ? latency L latency (x, y) + ? of f line L of f line (x, y) = ? log ? Pr(?|x) + ? latency ? Pr(?|y, x)l(?) ? ? of f line log P of f line (y|x) (11) where ? latency and ? of f line are the scaling factors corresponding to L latency and L of f line , respectively. And we set ? latency = ? of f line = 1.0 if not specified. 

 Streaming Encoder Unidirectional Transformer encoder  (Arivazhagan et al., 2019; Ma et al., 2020c)  is not effective for speech data processing, because of the close relatedness to the right context for speech feature x i . Block processing  (Dong et al., 2019;  is introduced for online ASR, but it lacks direct observation to infinite left context. We process the streaming encoder for speech data by block processing with the right context and infinite left context. First, input representations h is divided into overlapped blocks with block shift step m and block size m + r. Each block consists of two parts, the main context m n = h m * n+1 , ? ? ? , h m * (n+1) and the right con- text r n = h (n+1) * m+1 , ? ? ? , h (n+1) * m+r . The query, key, and value of block b n in self-attention can be described as follows: Q = W q [m n , r n ] (12) K = W k [m 1 , ? ? ? , m n , r n ] (13) V = W v [m 1 , ? ? ? , m n , r n ] (14) By reorganizing the input sequence and designed self-attention mask, training is effective by reusing conventional transformer encoder layers. And unidirectional transformer can be regarded as a special case of our method with {m = 1, r = 0}. Note that the look-ahead window size in our method is fixed, which enables us to increase transformer layers without increasing latency. We set the main context size and right context size to 8 and 4, respectively, for our experiments on speech-to-text simultaneous translation, and conventional unidirectional transformer encoder {m = 1, r = 0} for experiments on text-to-text simultaneous translation. 

 Inference of CAAT Simultaneous Translation The online inference for CAAT is adapted from beam search for RNN-T  (Graves, 2012) , and the changes are as follows 2 : (1) We only merge paths between decision steps, as the cost of the joiner of CAAT is significantly more expensive than that of RNN-T. (  2  We use SentencePiece  (Kudo and Richardson, 2018)  to generate a unigram vocabulary of size 20,000 for the source and target language jointly. Our experiments on speech-to-text simultaneous translation are based on Transformer  (Vaswani et al., 2017) . Since the variance of the length of speech frames is more significant than that of text length, we use both cosine positional embedding  (Vaswani et al., 2017)  and relative positional attention  (Shaw et al., 2018)  for speech encoder, and only cosine positional embedding for the decoder. Detailed hyper-parameters of our models can be found in Appendix C.1. 

 Training and Inference Training speech translation models is often regarded to be more difficult than training text machine translation or ASR models. We use two methods to improve the performance and stability of model training. The first is to pre-train encoder with ASR task  (Ma et al., 2020b) , and the second is to leverage sequence-level knowledge distillation with text machine translation model  (Ren et al., 2020) . Training CAAT models require significantly larger GPU memory than that used in conventional Transformer due to the spatial complexity O( |x||y| d ) of the joiner module; we solve this by splitting hidden states into small pieces before sending them into the joiner and recombining them during backpropagation. Our implementation is based on the Fairseq library  (Ott et al., 2019) ; the NLL and latency loss for CAAT are implemented based on warp-rnnt 4 . Evaluation We evaluate our models with SimulEval  (Ma et al., 2020a) . Translation quality is measured by detokenized case-sensitive BLEU  (Papineni et al., 2002) ; latency is measured with the adapted version of word-level Average Lagging (AL)  (Ma et al., 2020a) . 

 Results We compare CAAT to the current state-of-the-art model in speech-to-text simultaneous translation  (Ma et al., 2020b) , which uses wait-k with a fixed pre-decision step size of 320ms. All our simultaneous speech translation models, both wait-k and CAAT are trained with encoder pretrained on ASR task and sequence-level knowledge distillation with text translation model. Two inference methods are used for wait-k, conventional beam search only on target tail (when source finishes) and speculative beam search (SBS)  (Zheng et al., 2019b) , both with a beam size of 5; the forecast steps in SBS is set to be 2. For CAAT we set the intra-decision beam size b 1 = 5 and inter-decision beam size b 2 = 1 as described in Sec. 4.3. The latency-quality curves of CAAT are produced by varying decision step size d ? {8, 16, 32, 48, 64, 80, +?}, and wait-k by varying k ? {1, 2, 4, 6, 8, 10, 12, +?}. The AL-BLEU curves on the MuST-C EN?DE and EN?ES test sets are shown in Figure  4 .  5  From the figure we can observe that: (1) In general CAAT significantly outperforms wait-k (with and without SBS) in both the EN?DE and EN?ES task. Especially in the low-latency region (AL < 1000ms)  (Ansari et al., 2020) , CAAT outperforms wait-k with SBS by more than 3 BLEU points. (2) The Offline models of CAAT and wait-k obtain similar BLEU, suggesting that the adapted architecture of CAAT performs comparably with conventional Transformer in an offline scenario. (3) With the same wait step k, SBS can produce lower latency. This is due to the word-level latency metrics we used requires an additional token to ensure complete word submitted, which can be offset by the forward exploration in SBS. 

 Ablation Study 

 Effectiveness of Streaming Encoder The performance of our offline models with full-sentence encoder compared to the state-of-the-art offline speech translation systems  Inaguma et al., 2020)  are demonstrated in Table  1 . We also show the ablation analyses on sequencelevel knowledge distillation with text translation model (KD) and pretrain encoder with ASR task (Pretrain). 

 Model EN?DE EN?ES  22.7 27.2  (Inaguma et al., 2020)  22.9 We further compare offline translation models with streaming encoders to those with the conventional full-sentence encoder. As shown in Table  1 , the performance of the translation model with a unidirectional encoder drops 2-3 BLEU points compared to that with a full-sentence encoder, and the gap is gradually narrowed by the increase of main block size m and introduction of right context. Considering the effect on latency, we choose {m = 8, r = 4}. Effectiveness of ? latency and ? of f line The effectiveness of ? of f line is demonstrated in Table  3 . Furthermore, as shown in Figure  5 , though ? latency may affect the trade-off between translation quality and latency, varying ? latency is not as effective as varying the decision step size d, and we found the model training will be unstable when ? latency ? 2.0.  

 Effectiveness of Beam Search The effectiveness of the intra-decision beam size b 1 and interdecision beam size b 2 on simulation translation performance is shown in Table  4 . We can find that beam search in one decision step brings an improvement of about 0.7 BLEU over the greedy search. And if we allow multiple hypothesizes between decision steps we may get another 0.5 BLEU improvement at the cost of latency (AL increases from 1114.9 to 2433.5). However, this may be useful in the scenarios where revision is allowed (Arivazhagan et al., 2020), e.g., simultaneous translation for subtitle. b Case Study We perform case study to demonstrate the advantages of CAAT model over wait-k with SBS, we compare wait-k k = 2 with CAAT d = 32 for they have similar AL latency. As shown in Figure  7 , wait-k generates meaningless translation by 'predict' in the place of pauses and changes in speech rate, while CAAT does not suffer from this problem. As a result, CAAT outperforms waitk with SBS. 

 Text-to-Text Simultaneous Translation We further performed experiments on the text-totext simultaneous translation task. Experiments are carried out on the WMT15 German-English (DE?EN) dataset with newstest2013 as the validation set and newstest2015 as the test set. We strictly follow the same settings of  (Arivazhagan et al., 2019)    properly designed latency loss incorporated to ensure latency to be under control. Experiments demonstrate that CAAT achieves better latencyquality trade-offs compared to the state-of-the-art approaches in speech-to-text and text-to-text simultaneous translation tasks. We provide detailed analyses to demonstrate how CAAT works and improves the performance. 

 A Derivation of CAAT Losses A.1 Derivation of CAAT NLL loss Given the encoder representation h n , where 1 ? n ? |x|, the predictor vector h pred j , where 0 ? j ? J and J = |y|. and decision step size d ? 1. The maximum decision step is I = |x| d , and the output logits at decision step i, target position j should be s(i, j) = g h <i * d , h pred j (15) s(i,j) is a vector of |V |+1 dimension corresponding to V and blank symbol ?. s(k, i, j) denotes the k-th dimension of s(i, j). The conditional output distribution can be yielded as : Pr(k|i, j) = e s(k,i,j) k e s(k ,i,j) (16) To simplify notation, define y(i, j) := Pr(y j+1 |i, j) ?(i, j) := Pr(?|i, j) (17) Define the forward variable ?(i, j) as the probability of outputting y [1:j] during decision steps [1, 2, ? ? ? , i]. The forward variables for all 1 ? i ? I and 0 ? j ? |y| can be calculated recursively using ?(i, j) = ?(i ? 1, j) ? ?(i ? 1, j) + ?(i, j ? 1) ? y(i, j ? 1) (18) with initial condition ?(1, 0) = 1. The total output sequence probability is equal to the forward variable at the terminal node: Pr(y|x) = ?(I, J) ? ?(I, J) (19) Define the backward variable ?(i, j) as the probability of outputting y [j+1:J] during decision steps [i, ? ? ? , I]. Then: ?(i, j) = ?(i + 1, j) ? ?(i, j) + ?(i, j + 1) ? y(i, j) (20) with initial condition ?(I, J) = ?(I, J). Pr(y|x) is equal to the sum of ?(i, j)?(i, j) over any topright to bottom-left diagonal through the nodes. That is, ?m : 1 ? m ? I + J Pr(y|x) = (i,j):i+j=m ?(i, j) ? ?(i, j) (21) From Eqs. 18, 20 and 21, we can draw the derivation of loss function L = ? log Pr(y|x) as ?L ? Pr(k|i, j) = ?(i, j) Pr(y|x) ? ? ? ? ? ?(i, j + 1) if k = y j+1 ?(i + 1, j) if k = ? 0 otherwise (22) A.2 Derivation of CAAT Latency Loss To calculate the marginal expectation in Eq. 23, we define forward latency variable ? lat (n, j) as the expectation latency of outputting y [1:j] during decision steps [1, 2, ? ? ? , i], and backward latency variable beta lat (i, j) as the expectation latency of outputting y [j+1:J] during decision steps [i, i + 1, ? ? ? , i]. Here we denote l(n, j) as the latency function for output y j at source position n. L latency (x, y) = E ?H(x,y) l(?) = ? Pr(?|y, x)l(?) (23) The forward latency variables can be calculated recursively using ? lat (i, j) = p 1 (i, j) ? f 1 (i, j) + p 0 (i, j) ? f 0 (i, j) (24) with initial condition ? lat (1, 0) = 0. Where p 1 (i, j) = ?(i, j ? 1) ? y(i, j) ?(i, j) p 0 (i, j) = ?(i ? 1, j) ? ?(i ? 1, j) ?(i, j) f 1 (i, j) = ? lat (i, j ? 1) + l(i, j ? 1) f 0 (i, j) = ? lat (i ? 1, j) (25) For backward latency variables ? lat (i, j) = q 1 (i, j) ? b 1 (i, j) + q 0 (i, j) ? b 0 (i, j) (26) with initial condition ? lat (I, J) = 0. Where q 1 (i, j) = ?(i, j + 1) ? y(i, j) ?(i, j) q 0 (i, j) = ?(i + 1, j) ? ?(i, j) ?(i, j) b 1 (i, j) = ? lat (i, j + 1) + l(i, j) b 0 (i, j) = ? lat (i + 1, j) (27) To simplify notation, define the latency expectation of all paths go through grid (n, j) as c(i, j) = ? lat (i, j) + ? lat (i, j) c(i, j, 0) = ? lat (i, j) + ? lat (i + 1, j) c(i, j, 1) = ? lat (i, j) + l(i, j) + ? lat (i, j + 1) (28) The expectation latency for all paths ? ? H(x, y) is equal to the expectation through diagonal nodes. That is, ?m : 1 ? m ? N + J: ? = c(I, J) = (i,j):i+j=m ?(i, j)?(i, j)c(i, j) Pr(y|x) (29) And the latency loss L latency (x, y) = ?. From Eqs. 24, 26, 28 and 29, it follows that: r(i, j) = ? ? ? ? ? ?(i, j + 1) (c(i, j, 1) ? ?) if k = y j+1 ?(i + 1, j) (c(i, j, 0) ? ?) if k = ? 0 otherwise ?L latency ? Pr(k|n, j) = ?(i, j) Pr(y|x) r(i, j) (30) 

 B Beam Search Algorithm for CAAT The pseudo code of beam search algorithm for CAAT is described in Algorithm 1. C Hyper-parameters     Figure 1 : 1 Figure 1: The difference between Attention-based Encoder-Decoder, RNN-T and CAAT. 

 Figure 2 : 2 Figure 2: Expanded paths in simultaneous translation 

 Figure 3 : 3 Figure 3: Architecture of CAAT Transformer 

 ) We extract common prefix of existing hypotheses as determined target output at each decision time step. (3) Different beam sizes are introduced for intra-decision (b 1 ) and inter-decision (b 2 ) pruning, to ensure timely determination of outputs. b 1 and b 2 are set to be 5 and 1, respectively, if not otherwise specified. We use the MuST-C v1.0 3 (Di Gangi et al., 2019) English?German (EN?DE) and English?Spanish (EN?ES) speech translation datasets in our experiments. We use the dev set for validation and report performance on the tst-COMMON set. The 80dimensional log-Mel filter bank features are extracted for speech feature with a 25ms window size and a 10ms window shift; SpecAugment (Park et al., 2019) were performed on the training data. 

 Figure 4 : 4 Figure 4: Translation quality vs. Average Lagging on the EN?DE and EN?ES speech translation test set. The y-axis is BLEU and x-axis Average Laggging (AL). 

 Figure 5 : 5 Figure 5: AL-BLEU curves drawn by varying the decision step size d and latency loss scale ? latency on the MuST-C EN?DE test set. varying d means setting d = {8, 16, 32, 48, 64, 80} with ? latency = 1.0; varying ? latency means setting ? latency = {2, 1.5, 1, 0.5, 0} with decision step size d = 32. 

 Figure 6 : 6 Figure 6: Translation quality vs. Average Lagging on the WMT15 DE?EN text translation test set.The yaxis is BLEU and x-axis Average Laggging (AL). 

 Figure 7 : 7 Figure 7: An example from the EN?DE test set, which demonstrate that CAAT outperforms wait-k with close latency. The five components from top to bottom: speech, source transcription (aligned to speech audio), reference translation, hypothesis from CAAT, and hypothesis from wait-k with SBS. 

 Figure 8 : 8 Figure 8: Translation quality against latency (AL, AP and DAL) on the MuST-C EN?DE and EN?ES speech translation test sets. 

 Figure 9 : 9 Figure 9: Translation quality against latency (AL,AP and DAL) on the WMT15 DE?EN text translation test set. 

 Table 2 : 2 Effect of the number of joiner layers on quality and latency on the MuST-C EN?DE test set. # Joiner layers d AL BLEU 0 1 2715.7 9.74 0 32 2154.4 9.42 1 32 1156.2 20.94 4 32 1141.2 21.78 6 32 1114.9 21.81 Effectiveness of Joiner Layers The perfor- mance of CAAT models with different numbers of joiner layers are shown in Table 2. Note that in the table, the first two rows (# joiner layers=0) cor- responds to the conventional Transducer without cross attention, in which encoder representations are downsampled d times using average-pooling and then directly fused with predictor outputs by addition. We can find that the introduction of the cross attention mechanism significantly improves the performance of simultaneous translation, and the BLEU scores are close when the number of joiner layers is greater than 4. ? of f line AL BLEU 0 1111.6 19.84 0.5 1106.5 20.83 1 1114.9 21.81 1.5 1144.0 21.77 2.0 1176.5 21.87 

 Table 3 : 3 Effect of the ? of f line on quality and latency on the MuST-C EN?DE test set. 

 Table 4 : 4 Performance of CAAT models with different b 1 and b 2 on the EN?DE test set, all with the decision step d = 32. 1 b 2 AL BLEU 1 1 1116.0 21.18 3 1 1109.0 21.75 5 1 1114.9 21.81 10 1 1126.2 21.86 5 2 1929.7 22.1 5 3 2433.5 22.3 

 Table 5 : 5 Complete results on the MuST-C EN?DE speech translation test set. BLEU AL AP DAL BLEU AL AP DAL k wait-k k wait-k 1 18.1 1147.0 0.75 1571.1 1 20.0 932.1 0.74 1499.3 2 18.8 1394.9 0.78 1760.6 2 21.5 1203.9 0.77 1713.1 4 20.7 1911.6 0.84 2220.6 4 23.1 1727.5 0.82 2170.3 6 21.8 2377.4 0.88 2652.0 6 25.2 2232.7 0.86 2633.2 8 22.2 2792.5 0.91 3034.5 8 26.0 2676.5 0.89 3033.0 10 22.2 3155.7 0.929 3383.0 10 26.4 3074.3 0.92 3396.6 12 22.5 3477.3 0.94 3674.0 12 26.6 3428.9 0.94 3721.1 +? 23.0 5431.6 1.00 5431.6 +? 27.8 5997.1 1.00 5997.1 k wait-k with SBS k wait-k with SBS 1 18.7 866.6 0.70 1310.9 1 20.4 614.6 0.69 1230.8 2 19.6 1125.8 0.74 1515.3 2 21.5 903.0 0.73 1453.0 4 21.0 1649.2 0.81 1985.2 4 23.0 1437.7 0.79 1914.1 6 22.0 2134.0 0.86 2426.7 6 25.6 1969.6 0.84 2397.4 8 22.3 2571.4 0.89 2835.3 8 26.0 2437.5 0.88 2826.7 10 22.7 2967.9 0.92 3205.5 10 26.6 2865.0 0.91 3214.4 12 22.7 3310.8 0.94 3524.1 12 26.9 3241.9 0.93 3557.9 +? 23.0 5431.6 1.00 5431.6 +? 27.8 5997.1 1.00 5997.1 d CAAT d CAAT 8 20.5 508.1 0.64 1100.4 8 24.0 355.9 0.64 1146.3 16 21.4 813.8 0.68 1335.3 16 25.8 623.2 0.67 1359.4 32 21.8 1114.9 0.74 1758.1 32 26.3 955.9 0.72 1785.0 48 22.2 1443.4 0.78 2193.6 48 26.4 1275.9 0.77 2231.4 64 22.4 1800.6 0.82 2633.5 64 26.6 1647.7 0.81 2680.7 80 22.6 2137.8 0.86 3025.4 80 27.1 1977.3 0.84 3083.7 +? 23.2 5431.6 1.00 5431.6 +? 27.5 55997.1 1.00 5997.1 

 Table 6 : 6 Complete results on the MuST-C EN?ES speech translation test set. BLEU AL AP DAL k wait-k 2 19.7 1.57 0.59 3.12 4 23.6 3.38 0.65 4.69 6 26.4 5.29 0.72 6.42 8 28.2 7.24 0.77 8.19 10 28.5 9.02 0.81 9.95 12 29.5 10.77 0.85 11.64 16 29.8 13.95 0.90 14.71 +? 30.6 27.90 1.00 27.90 k wait-k with SBS 2 21.1 2.16 0.60 3.16 4 24.5 3.81 0.66 4.67 6 26.9 5.58 0.72 6.41 8 28.9 7.39 0.77 8.19 10 28.8 9.14 0.81 9.94 12 29.9 10.84 0.85 11.64 16 30.0 13.99 0.90 14.72 +? 30.6 27.90 1.00 27.90 ? avg MMA-IL 0.8 21.1 3.26 0.63 4.65 0.6 23.9 3.74 0.65 5.24 0.4 24.7 4.47 0.68 6.87 0.2 27.5 8.72 0.81 13.0 0.1 26.6 21.37 0.97 24.48 0. 29.5 27.49 1.0 27.49 d CAAT 1 26.8 2.67 0.60 4.96 2 27.8 3.27 0.63 5.79 4 28.1 4.23 0.67 7.20 8 29.2 6.07 0.73 9.85 12 29.3 8.00 0.79 12.58 16 29.6 10.02 0.83 15.23 20 29.9 12.16 0.87 17.64 +? 30.2 27.90 1.00 27.90 

 Table 7 : 7 Complete results on the WMT15 DE?EN test translation test set. 

			 Details of inference algorithm can be found in Appendix C.3 https://ict.fbk.eu/must-c/ 

			 https://github.com/1ytic/warp-rnnt 5 Full-size graphs for all latency metrics (AL, AP, and DAL) along with the corresponding numeric scores are available in Appendix D. 

			 https://github.com/pytorch/fairseq/ tree/master/examples/simultaneous_ translation 7 Limited by GPU memory, we failed to train CAAT with d < 4, so we just set d = {1, 2} in inference on model trained with d = 4.8 Full-size graphs for all latency metrics along with the corresponding numeric scores are available in Appendix D. 

			 https://github.com/pytorch/fairseq/ tree/master/examples/simultaneous_ translation/models/transformer_ monotonic_attention.py
