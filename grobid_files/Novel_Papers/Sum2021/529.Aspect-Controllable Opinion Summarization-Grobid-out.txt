title
Aspect-Controllable Opinion Summarization

abstract
Recent work on opinion summarization produces general summaries based on a set of input reviews and the popularity of opinions expressed in them. In this paper, we propose an approach that allows the generation of customized summaries based on aspect queries (e.g., describing the location and room of a hotel). Using a review corpus, we create a synthetic training dataset of (review, summary) pairs enriched with aspect controllers which are induced by a multi-instance learning model that predicts the aspects of a document at different levels of granularity. We fine-tune a pretrained model using our synthetic dataset and generate aspect-specific summaries by modifying the aspect controllers. Experiments on two benchmarks show that our model outperforms the previous state of the art and generates personalized summaries by controlling the number of aspects discussed in them. 
 HUMAN summaries General Staff was service focused and very welcoming. Common areas of the hotel smelled fresh because of how clean everything was. The rooms were comfortable and came with a fridge and a microwave. Food, both hot and cold, was very well presented and fresh. The hotel was located within walking distance to the French quarter and felt very safe at night. Building It's older, looking at the hotel and lobby, but has lots of charm & character. Cleanliness The hotel's lounge, bathrooms, hallways, and even the bedding were all clean and even smelled fresh. Food The breakfast is very good and plentiful and was more than just continental, offering eggs, sausage and grits in addition to the usual waffles, cereal, and fruit. Location The location is very good, walking distance to all major sights in French quarter. Rooms The room is comfortable and equipped with just about everything anyone could need ... a refrigerator, microwave, desk, sofa, iron and ironing board, and hairdryer. The room was also spacious and the hotel was very quiet. Service Hotel staff were unbelievably friendly and helpful; they often went above and beyond to be accommodating. 
 ACESUM summaries General The hotel is in a great location, close to the French quarter and the market. The room was clean and comfortable. Breakfast was good, and the staff was very helpful. There is a small restaurant in the lobby. Building The lobby is a bit small. The lobby area is a little bit dated, but the rooms are very comfortable. Cleanliness The room was clean and comfortable. The bathroom was very clean with a nice shower. Food The breakfast was very good, with a variety of choices. The breakfast buffet was good. Location The location is great, right in the heart of Bourbon street, and within walking distance of the French quarter. Rooms The room was very spacious and the bathroom was very nice. The room had a TV, a microwave, and a separate shower. There was a small fridge in the room, which was nice. Service The staff was very friendly and helpful.

Introduction Consumers oftentimes resort to review websites to inform their decision making (e.g., whether to buy a product or use a service). The proliferation of online reviews has accelerated research on opinion mining  (Pang and Lee, 2008) , where the ultimate goal is to glean information from reviews so that users can make decisions more efficiently. Opinion mining has assumed several guises in the literature such as sentiment analysis  (Pang et al., 2002) , aspect extraction  (Hu and Liu, 2004; He et al., 2017) , combinations thereof  (Mukherjee and Liu, 2012; Pontiki et al., 2016) , and notably opinion summarization  (Hu and Liu, 2006; Wang and Ling, 2016) , whose aim is to create a textual summary of opinions found in multiple reviews. Text summarization models, both extractive  (Narayan et al., 2018; Zheng and Lapata, 2019; Cachola et al., 2020)  and abstractive  (See et al., 2017; Gehrmann et al., 2018; Liu and Lapata, 2019) , operate under the assumption that salient content is 

 General The room was clean and comfortable. The staff was very friendly and helpful. It was a great location, just a short walk to the beach. There wasn't much to do in the area, but the food was good. 

 Location The location was great, right on the Boardwalk, and close to the Venice beach. 

 Rooms The room was very clean and the bathroom was very nice. The bathroom had a large separate shower. There was a TV in the room. 

 Location and Rooms The location is great, right on Boardwalk, and the beach is very nice. The room was very clean and the bathroom was very nice and the shower was great. Cleanliness, Location, Room, and Service The staff was very friendly and helpful. The room was very clean, and the bathroom was very nice. It was a great location, right on the beach. Table  1 : General and aspect-specific summaries generated by our model for a hotel from the SPACE dataset. Aspects and aspect-specific sentences are color-coded. relevant  (Erkan and Radev, 2004)  and should be presented in the summary. Opinion summarization is no exception, focusing on creating summaries based on opinions that are popular or redundant across reviews  (Angelidis and Lapata, 2018b; Chu and Liu, 2019; Amplayo and Lapata, 2020; Bra?inskas et al., 2020; . However, the notion of salience in reviews largely depends on user interest. For example, one might only care about the connectivity of a television product, an aspect which might be unpopular amongst reviews. As a result, models that create general opinion summaries may not satisfy the needs of all users, limiting their ability to make decisions.  mitigate this problem with an extractive approach that produces both general and aspect-specific opinion summaries. They achieve this essentially by clustering opinions through a discrete latent variable model  (van den Oord et al., 2017)  and extracting sentences based on popular aspects or a particular aspect. By virtue of being extractive, their summaries can be incoherent, and verbose containing unnecessary redundancy. And although their model creates summaries for individual aspects, it is not clear how to control the number of aspects in the output (e.g., to obtain summaries that mention multiple rather than a single aspect of an entity). In this paper, we propose an abstractive opinion summarization model that generates aspectcontrollable summaries. Using a corpus of reviews on entities (e.g., hotels, television sets), we construct a synthetic training dataset consisting of reviews, a pseudo-summary, and three types of aspect controllers which reflect different levels of granularity: aspect-related keywords, review sentences, and document-level aspect codes. We induce aspect controllers automatically based on a multiple instance learning model  (Keeler and Rumelhart, 1991)  and very little human involvement. Using the aspect-enriched dataset, we then fine-tune a pretrained model  (Raffel et al., 2020 ) on summary generation. By modifying the controllers, we can flexibly generate general and aspect-specific summaries, discussing one or more aspects. Figure  1  shows summaries generated by our model. We perform experiments on SPACE , a single domain dataset consisting of hotel reviews, and OPOSUM  (Angelidis and Lapata, 2018b ), a dataset with product reviews from multiple domains (e.g., "laptop bags", "boots"). Automatic and human evaluation show that our model outperforms previous approaches on both tasks of general and aspect-specific summarization. We also demonstrate that it can effectively generate multi-aspect summaries based on user preferences. We make our code and data publicly available. 1 

 Related Work Earlier work on opinion summarization has focused on general summarization using extractive  (Hu and Liu, 2006; Kim et al., 2011; Angelidis and Lapata, 2018b)  or abstractive methods  (Ganesan et al., 2010; Carenini et al., 2013; Fabbrizio et al., 2014) . Due to the absence of opinion summaries in review websites and the difficulty of annotating them on a large scale, more recent methods consider an unsupervised learning setting where there are only reviews available without corresponding summaries  (Chu and Liu, 2019; Bra?inskas et al., 2020) . They make use of autoencoders (Kingma and Welling, 1 https://github.com/rktamplayo/AceSum 2014) and variants thereof to learn a review decoder through reconstruction, and use it to generate summaries conditioned on averaged representations of the inputs. A more successful approach to opinion summarization is through the creation of synthetic datasets, where (review, summary) pairs are constructed from a review corpus to enable supervised training. These methods usually start by randomly selecting a review which they treat as a pseudo-summary and subsequently pair it with a set of input reviews based on different strategies. These include random sampling  (Bra?inskas et al., 2020) , generating noisy versions of the pseudo-summary (Amplayo and Lapata, 2020), ranking reviews based on similarity and relevance  (Elsahar et al., 2021) , and making use of content plans to create more naturalistic pairs . Our work is closest to  who propose an extractive summarization model that uses a vector-quantized variational autoencoder  (van den Oord et al., 2017)  to learn aspectspecific review representations. Their model effectively groups opinion sentences into clusters and extracts those capturing aspect-relevant information. We employ multi-instance learning to identify aspect-bearing elements in reviews with varying degrees of granularity (e.g., words, sentences, documents) which we argue affords greater flexibility and better control of the output summaries. In doing so, we also introduce an effective method to create synthetic datasets for aspect-guided opinion summarization. Our work also relates to approaches which attempt to control summarization output based on length  (Kikuchi et al., 2016) , content  (Fan et al., 2018) , style  (Cao and Wang, 2021) , or textual queries  (Dang, 2006) . Although we focus solely on aspect, our method is general and could be used to adjust additional properties of a summary such as sentiment (e.g., positive vs. negative) or style (e.g., formal vs. colloquial). 

 Problem Formulation Let C denote a corpus of reviews about entities (e.g., products, hotels). Let R e = {r 1 , r 2 , ..., r N } denote a set of reviews for entity e and A e = {a 1 , a 2 , ..., a M } a set of aspects that are relevant for the entity (e.g., cleanliness, location). Each review r i is a sequence of tokens {w 1 , w 2 , ...}, while each aspect a j is represented by a small set of seed words {v 1 , v 2 , ...} (e.g., spotless, dirty, stain). These seed words can be acquired automatically  (Angelidis and Lapata, 2018b)  or provided by users (see Appendix for those used in our experiments). Our approach creates two types of summaries: (a) a general summary that contains salient opinions about all aspects of an entity, and (b) an aspect-specific summary that focuses on opinions about particular aspects of interest specified by a query Q = {q 1 , q 2 , ..., q M }; here, q j is an indicator function which designates whether the aspect should be mentioned in the summary. We emphasize that the query can represent more than one aspect to reflect real-world usage. To facilitate supervised training, we create a synthetic training dataset D = (X, z, y), which is a set of triples composed of input reviews X, a pseudo-summary y, and aspect controllers z (Section 3.2). Our aspect controllers are induced with a unified model based on multi-instance learning (Section 3.1) and correspond to different levels of granularity: (1) document-level aspect codes, (2) aspect-related review sentences, and (3) aspect keywords. At training time, we fine-tune a pretrained sequence-to-sequence Transformer model  (Raffel et al., 2020)  using controllers z as input and a pseudo-summary as output. During inference, we modulate summary generation by modifying the controllers, e.g., we produce a general summary using all aspect codes, or an aspect-specific one based on a subset thereof (Section 3.3). 

 Controller Induction Model A key feature of our approach is the set of aspect controllers which allow our summarization model to be controllable. We induce these controllers using a multiple instance learning (MIL) model, illustrated in Figure  1 . MIL is a machine learning framework where labels are associated with groups of instances (i.e., bags), while instance labels are unobserved  (Keeler and Rumelhart, 1991) . The goal is then to infer labels for bags  (Dietterich et al., 1997; Maron and Ratan, 1998)  or jointly for instances and bags  (Zhou et al., 2009; Wei et al., 2014; Kotzias et al., 2015; Xu and Lapata, 2019; Angelidis and Lapata, 2018a) . Our MIL model is an example of the latter variant. In our setting, documents are bags of sentences and sentences are bags of tokens. We further assume that only documents have aspect labels. Given review r with tokens {w k }, we obtain token encodings e = {e k } from a pretrained language model (PLM; ) which uses the popular Transformer architecture  (Vaswani et al., 2017) . We use a non-linear transformation to obtain tokenlevel aspect predictions z T : e = PLM({w k }) (1) z T = tanh(W e + b) (2) where z T ? R N ?M , and N and M are the number of tokens and aspects, respectively. A positive value denotes that the token is related to the aspect of interest (and otherwise unrelated). 

 Multiple Instance Pooling To obtain sentencelevel aspect predictions z S , we aggregate tokenlevel predictions z T using a new pooling method particularly effective for our multi-instance learning setting. We first obtain multiple predictions z h for each attention head h: z h = k (z T * a h [k]) (3) a h = softmax(key h ? qry h ) (4) where * is element-wise multiplication, ? is dot product, k is the token index, qry h is a headspecific query vector, and key h is defined below: key h = tanh(W h e + b h ) (5) We hypothesize that different attention heads represent different aspects of the semantic space, and are thus helpful at predicting multiple aspects. We obtain a sentence-level prediction by max pooling the predictions of individual heads: z S = max-pool({z h }) (6) We use max pooling since we want to isolate the most pertinent aspects for a given sentence; standard pooling methods such as mean and attention pooling  (Angelidis and Lapata, 2018a; Xu and Lapata, 2019)  assume that all instances of a bag contribute to its label. In Figure  1  (right) we illustrate our pooling mechanism and empirically show in experiments (see Section 5.1) it is superior to alternatives. We so far discussed how multiple instance pooling is applied at the token-level to obtain sentencelevel predictions z S . Analogously, multiple instance pooling is applied to sentences to obtain document-level predictions z D (see Figure  1 ). 

 Training and Inference Training the multiple instance model just described requires a dataset consisting of (review, aspect label) pairs. Unfortunately, we do not have access to annotations denoting which aspects are discussed in each review. Recall, however, that aspects are represented by seed words {v 1 , v 2 , ...}, which we exploit to induce silver-standard labels. Specifically, for each review in the dataset, we obtain binary labels ?D where ?D [a] = 1 if at least one seed word for aspect a is found in the review (and ?1 otherwise). We train the model using a soft margin loss, summing over all aspects a ? A: L ctrl = a log(1 + exp(?z D [a] * ?D [a])) (7) The parameters of the pretrained language model (see Equation (2)) are frozen, i.e., they are not finetuned during training which makes our controller induction model lightweight and efficient. 

 Summary y At first they took us to an unready room which was disappointing but after a short wait they took us to a really big room with a great harbor scene as an apology to the mess. The rooms are pretty new or renovated recently. Bathroom is clean and wide. The beds are comfortable and big. 

 Review x1 Check in was quick and our bags were brought to the room in a timely manner. The rooms and hallways left a little more to be desired. The rooms didnt look nearly as good as they did in other less known cities. No safe or frig in the rooms. The staff was great. 

 Review x2 Only option for a hot meal for breakfast was scrambled eggs and bacon; The toaster was broken as well, with burned out elements. Other food in the lounge was good (fruit, coffee). Recommendation: eat elsewhere; even room service would probably have been better. Figure  2 : Pseudo-summary y and input reviews X; the aspect code for summary y is room. Review sentences with the same aspect are underlined and same aspectkeywords are magnified. 

 Synthetic Dataset Creation The MIL model allows us to learn three kinds of aspect controllers which are subsequently used to create a synthetic dataset for training our summarizer. These are aspect codes, essentially document-level aspect predictions z D , which control the overall aspect of the summary, aspect keywords ensure content support by explicitly highlighting which tokens from the input should appear in the summary, and aspect-relevant sentences which provide textual context for summary generation (while nonaspect-related sentences are ignored). We first sample review r i as a pseudo-summary from review set R e of entity e. We treat r i as a pseudo-summary provided it resembles a real summary. We assume that opinion summaries discuss specific aspects regarding entity e. We use our controller induction model to verify this, i.e., document-level aspect predictions z D for r i should be positive for at least one aspect. Provided r i fulfills this constraint, we use it as summary y and R e ? {r i } as review set X. A simplified example is shown in Figure  2 , the pseudo summary is highlighted in gray and the input reviews in cyan. The summary focuses on the room aspect of a hotel and this is its aspect code (shown in blue). Let (X, y) denote review set X for summary y (we only show two reviews in Figure  2  but there are usually hundreds). We obtain (positive) documentlevel aspect predictions z (y) D for summary y and sentence-level aspect predictions z (x) S for all re-views x ? X. We then rank review sentences in X based on their similarity to the summary's overall aspect. Specifically, we compare predictions z (x) S with z (y) D using the soft margin loss function from Equation (7). We also compare token-level predictions z (x) T with z (y) D using the same function to induce aspect keywords. In Figure  2  sentences which discuss the same aspect as the summary are underlined, and same-aspect keywords are magnified. For illustration purposes we only show one aspect code in Figure  2 , but these can be several, and different review sentences and keywords would be selected for different aspects. 

 Opinion Summarization Model We use a pretrained sequence-to-sequence Transformer model  (Raffel et al., 2020)  to generate opinion summaries. We transform the aspect controllers z into the following format: Instead of the full set of input reviews X, the encoder takes z as input and produces multi-layer encodings Z. The decoder then outputs a token distribution p(y t ) for each time step t, conditioned on both Z and y 1:t?1 through attention: [CODE] [ASPECT 2 ] [ASPECT 3 ] [KEY] keyword 1 keyword 2 ... [SNT] first sentence [SNT] second sentence ... Z = Encoder(z) (8) p(y t ) = Decoder(y 1:t?1 , Z) (9) We fine-tune the model using a maximum likelihood loss to optimize the probability distribution p(y) based on gold summary ?: L gen = ? t ?t log p(y t ) (10) During inference, we can generate different kinds of opinion summaries by modifying the aspect controllers. When creating a general summary, we use all aspect codes as input. Analogously, when generating a single aspect summary, we use one aspect code. The aspect codes guide the selection of keywords and sentences from the input reviews (see Figure  2 ) which are given as input to our Transformer model to generate the summary (see Equation (  8 )). Implementation For our pretrained Transformer models, we used weights and settings available in the HuggingFace library  (Wolf et al., 2020) . Specifically, we used distilroberta-base  Sanh et al., 2019)  as our language model and t5-small  (Raffel et al., 2020)  as our sequence-to-sequence model. We trained the controller induction model with a learning rate of 1e?4 for 100K steps, using h = 12 heads. For OPO-SUM+, we trained separate controller induction models for different domains. For the aspect controllers, we selected 10-best keywords, and review sentences were truncated up to 500 tokens to fit in the pretrained model. For summarization, we used a learning rate of 1e ? 6 and 500K training steps. We used Adam with weight decay  (Loshchilov and Hutter, 2019)  to optimize both models. We added a linear learning rate warm-up for the first 10K steps. We generate summaries with beam search of size 2 and refrain from repeating ngrams of size 3  (Paulus et al., 2018) . 

 Results We compared our Aspect Controlled Summarization (ACESUM) model with several extractive and abstractive approaches. Traditional extractive systems include selecting as a summary the review closest to the CENTROID  of the input reviews and LEXRANK  (Erkan and Radev, 2004) , a PageRank-like algorithm that selects the most salient sentences from the input. For both methods we used BERT encodings  (Devlin et al., 2019)  to represent sentences and documents. Other extractive systems include QT 2 , a neural clustering method that uses Vector-Quantized Variational Autoencoders (van den  Oord et al., 2017)  to represent opinions in quantized space, and ACESUMEXT, an extractive version of our model that uses sentences ranked by our controller induction model as input (truncated up to 500 tokens) to LexRank. Abstractive systems include MEANSUM  (Chu and Liu, 2019) , an autoencoder that generates summaries by reconstructing the mean of review encodings, COPYCAT  (Bra?inskas et al., 2020) , a hierarchical variational autoencoder which learns a latent code of the summary, and two variants of T5  (Raffel et al., 2020)  trained with different synthetic dataset creation methods. For T5-RANDOM, summaries are randomly sampled  (Bra?inskas et al., 2020) , whereas for T5-SIMILAR reviews are sampled based on their similarity to a candidate summary (Amplayo and Lapata, 2020). Finally, we compared against two upper bounds: an extractive ORACLE which selects as a summary the review with the best ROUGE score against the input, and a HUMAN upper bound, calculated as inter-annotator ROUGE. Examples generated by our model are in  

 Automatic Evaluation We evaluated the quality of general and aspectspecific opinion summaries using F 1 ROUGE  (Lin and Hovy, 2003) . Unigram and bigram overlap (ROUGE-1/2) are proxies for assessing informativeness while the longest common subsequence (ROUGE-L) measures fluency. Aspect-Specific Opinion Summarization Most comparison systems (all except QT) cannot naturally generate aspect-specific summaries. We use a simple sentence-filtering method to remove nonaspect-related sentences from the input during inference. Specifically, we use BERT encodings  (Devlin et al., 2019)  to represent tokens in review sentences {r (bert) i 

 General Opinion Summarization } and aspect seeds {a (bert) j }. We then rank the review sentences based on the maximum similarity between seed and sentence tokens, calculated as max i,j (sim(r (bert) i , a (bert) j )), where sim(a, b) is the cosine similarity function. This method cannot be ported to the CENTROID and ORACLE baselines, and thus we do not compare with them. Our results are summarized in    that SPACE and OPOSUM+ focus exclusively on single aspect summaries. We assess our model's ability to generate summaries covering multiple aspects in the following section. Overall, ACESUM performs best across datasets and metrics, which shows that our controllers can effectively customize summaries based on aspect queries. Interestingly, amongst extractive models, ACESUMEXT performs best. This suggests that, a simple centralitybased extractive approach such as LexRank  (Erkan and Radev, 2004)  can produce good enough summaries as long as an effective sentence filtering method is applied beforehand (in our case this is based on the controller induction model). T5 models perform substantially worse on this task, indicating that synthetic datasets based on either random or similarity-based sampling techniques are not suited to aspect-specific opinion summarization. 

 Ablation Studies We present various ablation studies on the controller induction model and the summarization model itself. In Table  5 , we compare our multiple instance pooling (MIP) mechanism with three standard pooling methods: mean, max, and attention-based pooling. We evaluate models using document and sentence F 1 which measures the quality of document-and sentencelevel aspect predictions. We extrapolate aspect labels for documents and sentences from the development set which contains aspect-specific summaries. We assume the aspect for which a summary is written is the document label and that all sentences within the summary are also representative of the same aspect. Results show that attention and mean pooling are not suitable for multi-instance learning, underperforming especially on document-level F 1 . This suggests that token-level predictions are not used effectively to predict higher level aspects. Our results confirm that using multiple experts (i.e., attention heads) yields better aspect predictions. In Table  6 , we evaluate the contribution of different aspect controllers to summarization output. Selecting sentences randomly rather than based on aspect hurts performance, in particular when generating aspect-specific summaries. We also find that aspect codes substantially increase model performance in OPOSUM+. We conjecture that this is due to OPOSUM+ having multiple domains and, consequently, more aspects compared to SPACE. 

 Human Evaluation We conducted several human elicitation studies to further analyze the summaries produced by competing systems using the Amazon Mechanical Turk crowdsourcing platform. Best-Worst Scaling The first study assessed the quality of general opinion summaries using Best-Worst Scaling (BWS;  Louviere et al., 2015) . Participants were shown a human-written summary, in relation to which they were asked to select the best and worst among system summaries, taking into account the following criteria: Informativeness (how consistent are the opinions with the reference?), Coherence (is the summary easy to read and wellorganized?), Conciseness (does the summary provide useful information in a concise manner?), and Fluency (is the summary grammatical?). We compared general summaries produced by the two best performing extractive (LEXRANK, QT) and abstractive (T5-SIMILAR, ACESUM) systems according to ROUGE. We elicited three judgements for all entities in the SPACE and OPOSUM+ test sets. Table  7  summarizes our results. BWS values range from ?100 (unanimously worst) to 100 (unanimously best). ACESUM is deemed best for all criteria on both datasets. Crowdworkers also rated QT high on informativeness, which indicates that aspect modeling is helpful, but low on other criteria (e.g., coherence and conciseness) due to its extractive nature. 

 Aspect Controllability We also conducted a user study to assess the quality of aspect-specific summaries. We showed participants the aspect in question as well as aspect summaries from T5-SIMILAR, QT, ACESUM, and HUMAN. Crowdworkers were asked to decide whether the summaries discussed the given aspect exclusively, partially, or not at all. We elicited three judgments for all test entities. As can be seen in Table  8 , SPACE summaries produced by ACESUM exclusively discuss a single aspect 50.9% of the time. T5-SIMILAR mostly produces general summaries (74.8% of them partially discuss the given aspect) which is not surprising, given that it has no specialpurpose mechanism for modeling aspect. QT summaries are more topical for the opposite reason. In general, automatic systems perform worse on OPO-SUM+ whose larger number of domains renders this dataset more challenging. Finally, we observe a big gap between model and HUMAN performance. We further verified whether ACESUM can produce summaries covering two aspects. Although it can generate summaries with more aspects (see Table  1 ), we hypothesize that user queries pertain- ing to two aspects would be most frequent. Besides, if performance with two aspects is inferior, there is little chance it will improve with more aspects. For each test example we elicited three judgments and randomly selected two aspect pairs from the set of all possible aspect combinations. We compared ACESUM against QT (for which we used seed words representing both target aspects). Participants were shown the two aspects and the summaries generated by QT and ACESUM. They were asked to decide whether the summaries discussed (a) both target aspects exclusively (b) one of the aspects (c) other aspects in addition to the target ones, and (d) none of the two aspects. The results in Table  9  show that ACESUM is able to produce two-aspect summaries effectively 61.3% of the time on SPACE and 47.0% of the time on OPOSUM+. QT on the other hand mostly creates single-aspect summaries. Summary Veridicality Our third study examined the veridicality of the generated summaries, i.e., whether the opinions mentioned in them are indeed discussed in the input reviews. Participants were shown reviews and corresponding system summaries and were asked to verify, for each sentence of the summary, whether it was fully supported by the reviews, partially supported, or not at all supported. We performed this experiment on OPOSUM+ only since the number of reviews is small and participants could read them all in a timely fashion. We collected three judgments for all system summaries, both general and aspectspecific ones. Participants assessed the summaries produced by T5-SIMILAR and ACESUM. We also included GOLD-standard summaries as an upper bound but no output from an extractive system as it by default produces veridical summaries which contain facts mentioned in the reviews. Table  10  reports the percentage of fully (Full-Supp), partially (PartSupp), and un-supported (No-Supp) sentences. Not unsurprisingly, GOLD summaries display the highest percentage of fully supported sentences for both general and aspectspecific summaries. ACESUM and T5-SIMILAR present similar proportions of supported sentences when it comes to general summaries, with ACE-SUM having a slight advantage. The proportion of supported sentences is higher in aspect summaries for T5-SIMILAR. Note that this model struggles to actually generate aspect-specific summaries (see Table  8 ); instead, it generates any-aspect summaries which maybe veridical but off-topic. 

 Conclusions In this work, we presented an abstractive approach to aspect-controlled opinion summarization. Key to our model is the induction of aspect controllers which facilitate the creation of a synthetic training dataset and guide summary generation towards the designated aspects. Extensive experiments on two benchmarks show that our model achieves state of the art across the board, for both general and aspect-specific opinion summarization. In the future, we would like to focus on controlling additional facets of opinion summaries such as sentiment or length. It would also be interesting to learn aspects from data rather than specifying them apriori as well as dealing with unseen aspects (e.g., in a scenario where reviews discuss new features of a product).  filter brush attachments attachment turbo ease of use easy push corners awkward impossible suction power suction powerful power hair quiet Table  12 : OPOSUM+ seed words for various domains and their aspects. 

 A Appendix A.1 List of Seed Words Tables  11 and 12  shows the seed words we used in our experiments. These were generated semiautomatically: we first obtained aspect-specific words through the automatic method introduced in Angelidis and Lapata (2018b) and  and then asked human annotators to filter out the noise (i.e., words that were assigned incorrect aspects). 

 A.2 Results using Automatic Seed Words Table  13  shows comparisons between ACESUM and QT using automatically generated seed words for aspect-specific summarization (as used in . Our model performs better than QT on both datasets, while both models bene-   29.43 8.45 22.37 23.99 4.36 16.61 ACESUM 31.80 9.53 25.03 27.55 6.44 20.16  Table  13 : ROUGE scores of QT and ACESUM for aspect-specific summarization. fit from better quality seed words with noticeable increase in ROUGE scores. 

 A.3 Extensions to OPOSUM Dataset In this section, we present our additions to the OPO-SUM dataset  (Angelidis and Lapata, 2018b) . Firstly, we increased the size of the review corpus. The original dataset includes only 359K reviews, which is the result of down-sampling the Amazon Product Dataset introduced in  McAuley et al. (2015) . We instead gathered all reviews tagged with at least one of the OPOSUM domains ("Laptop Bags", "Bluetooth Headsets", "Boots", "Keyboards", "Televisions", and "Vacuums") from the newest version of the Amazon Product Dataset compiled by  Ni et al. (2019) . Since "Laptop Bags" and "Bluetooth Headsets" were significantly smaller than the other four domains, we additionally included all reviews tagged with "Bags" and "Headsets". We were able to increase the dataset to 4.13M reviews, i.e., by a factor of 12. Secondly, we created a large collection of human-written abstractive summaries for aspectspecific summarization evaluation. For each test product (e.g., television set) and for each aspect (e.g., image quality), we asked three annotators to write an opinion summary about the given aspect. The annotators were shown 10 input reviews, in which opinions about the target aspect were highlighted to aid them in their task. We only used the three most common aspects for each domain, since opinions about less common aspects do not appear frequently in reviews. We gathered 540 aspectspecific summaries in total. 

 A.4 Example Summaries Finally, we show general and aspect-specific summaries produced by QT, T5-SIMILAR, ACESUM, and HUMAN on SPACE (Table  14 ) and OPOSUM+ (Table  15 ). We also show two-aspect summaries produced by  QT and ACESUM in Table 16.   where[CODE],[KEY], and [SNT] are indicators denoting that the next tokens correspond to aspect codes, keywords, and review sentences. 

 Table 2 : 2 Statistics for SPACE and OPOSUM++ (underlined summaries are extractive). Dataset SPACE OPOSUM+ review corpus size 1.14M 4.13M #domains 1 6 #aspects 6 18 #test examples 50 60 #reviews/example 100 10 #summaries/example 3 3 #general summaries 150 180 #aspect summaries 900 540 4 Experimental Setup Datasets We performed experiments on two opinion summarization datasets covering differ- ent review domains. SPACE (Angelidis et al., 2021) is a large corpus of "hotel" reviews from TripAdvisor; it contains human-written abstractive opinion summaries for evaluation only. Each in- stance in the evaluation set consists of 100 reviews and seven summaries: one general summary and six aspect-specific ones representing the aspects set with extractive general summaries. We extended this dataset by (a) adding aspect-specific summaries which are human-written and abstractive follow- ing the methodology from Angelidis et al. (2021), and (b) increasing the size of the corpus. We call this extended dataset OPOSUM+. Both datasets include five human-annotated seed words for each aspect (see Appendix for details). Data statistics are shown in Table 2. Using our synthetic dataset creation method, we were able to generate 512K and 341K training instances for SPACE and OPO- SUM+, respectively. building, cleanliness, food, location, rooms, and service. OPOSUM (Angelidis and Lapata, 2018b ) is a large corpus of product reviews from six different domains: "laptop bags", "bluetooth headsets", "boots", "keyboards", "televisions", and "vacuums". It also includes an evaluation 

 Table 1 and the Appendix. SPACE OPOSUM+ Model R1 R2 RL R1 R2 RL CENTROID 31.29 4.91 16.43 33.44 11.00 20.54 LEXRANK 31.41 5.05 18.12 35.42 10.22 20.92 QT 38.66 10.22 21.90 37.72 14.65 21.69 ACESUMEXT 35.50 7.82 20.09 38.48  *  15.17  *  22.82  * MEANSUM 34.95 7.49 19.92 26.25 4.62 16.49 COPYCAT 36.66 8.87 20.90 27.98 5.79 17.07 T5-RANDOM 37.65 10.62 22.82 29.88 5.64 17.19 T5-SIMILAR 38.84 10.82 22.74 30.42 6.07 17.17 ACESUM 40.37  *  11.51  *  23.23 32.98 10.72 20.27 ORACLE 40.23 13.96 23.46 41.88 21.52 29.30 HUMAN 49.80 18.80 29.19 55.42 37.26 44.85 Table 3: Automatic evaluation for general summariza- tion. Extractive/Abstractive/Upper-bound models are shown in first/second/third block. Best systems are boldfaced; an asterisk (*) means there is a significant difference between best and 2nd best systems (based on paired bootstrap resampling; p < 0.05). 

 Table 3 3 re- 

 Table 4 4 . Note 

 Table 6 : 6 Variants of ACESUM with different aspect controllers. Results are shown using ROUGE-L for general and aspect-specific opinion summaries. SPACE OPOSUM+ Model Doc F1 Sent F1 Doc F1 Sent F1 MIP (ours) 77.35 40.85 83.28 50.48 Max 63.35 35.12 66.52 44.00 Attention 31.77 29.30 34.00 35.80 Mean 27.38 27.87 30.38 34.35 Table 5: Performance of controller induction models (document-and sentence-level); comparison of multi- ple instance pooling (MIP) against max, mean, and at- tention pooling. SPACE OPOSUM+ Model General Aspect General Aspect ACESUM 23.23 25.03 19.64 20.16 No aspect code 22.29 24.99 17.22 17.54 No keywords 21.88 24.82 18.97 19.97 Random sentences 22.42 19.16 18.96 13.44 

 Table 7 : 7 Best-Worst Scaling evaluation. Best values are bold-faced. An asterisk (*) means that the system is significantly better than the second best system (one-way ANOVA with posthoc Tukey HSD tests, p < 0.05). Inf: informative, Coh: coherent, Con: concise, Flu: fluent. SPACE Inf Coh Con Flu LEXRANK ?48.3 ?38.4 ?36.9 ?43.3 T5-SIMILAR 5.8 11.2 17.2 0.6 QT 20.4 1.3 1.2 2.6 ACESUM 22.1 26.0  *  18.5 38.8  * OPOSUM+ Inf Coh Con Flu LEXRANK ?27.3 ?21.1 ?18.2 ?23.8 T5-SIMILAR ?31.1 10.0 4.7 ?1.9 QT 20.3 ?25.3 ?21.6 ?9.6 ACESUM 38.1  *  36.3  *  35.2  *  35.3  * 

 Table 8 : 8 Proportion of summaries that discuss the target aspect exclusively, partially, or not at all. SPACE Exclusive Partial None T5-SIMILAR 10.6 74.8 14.6 QT 43.8 39.0 17.1 ACESUM 50.9 42.6 6.5 HUMAN 64.9 31.6 3.5 OPOSUM+ Exclusive Partial None T5-SIMILAR 9.4 48.2 42.5 QT 22.2 41.9 35.9 ACESUM 42.2 45.4 12.4 HUMAN 63.0 31.5 5.6 SPACE All One Other None QT 10.0 35.3 34.7 20.0 ACESUM 61.3 19.3 18.0 1.3 OPOSUM+ All One Other None QT 18.8 27.5 33.6 20.1 ACESUM 47.0 16.8 26.8 9.4 Table 9: Proportion of target aspects discussed in sys- tem summaries (All: both aspects are mentioned; One: only one is mentioned; Other: other aspects are also mentioned; None: no aspects are mentioned). 

 Table 10 : 10 Summary veridicality evaluation. Proportion of summaries that are fully supported, partially supported, or not supported at all. OPOSUM+ General Model FullSupp PartSupp NoSupp T5-SIMILAR 53.3 36.9 9.8 ACESUM 59.9 32.2 8.0 HUMAN 88.4 7.0 4.6 OPOSUM+ Aspect Model FullSupp PartSupp NoSupp T5-SIMILAR 57.3 29.4 13.3 ACESUM 54.2 32.3 13.5 HUMAN 67.8 20.7 11.6 

 Table 11 : 11 SPACE seed words for the "Hotels" domain. Aspect "Laptop Bags" looks looks color stylish looked pretty quality quality material poor broke durable size fit fits size big space Aspect "Bluetooth Headsets" comfort ear fit comfortable fits buds ease of use easy button simple setup control sound quality sound quality hear noise volume Aspect "Boots" comfort comfortable foot hurt ankle comfy looks cute look looked fringe style size size half big little bigger Aspect "Keyboards" build quality working months build stopped quality feel/comfort feel comfortable feels mushy shallow layout key keys delete backspace size Aspect "Televisions" connectivity hdmi computer port usb internet image quality picture color colors bright clear sound quality sound speakers loud tinny bass Aspect "Vacuums" accessories 

 .34 21.77 23.16 4.13 16.81  ACESUM 30.78 8.39 23.82 27.11 6.05 19.67   using human seed words QT SPACE OPOSUM+ Model R1 R2 RL R1 R2 RL using automatic seed words QT 28.95 8 

			 We report results for QT using our seed words which are human-annotated. We also present results in the Appendix with their seed words which were automatically induced.
