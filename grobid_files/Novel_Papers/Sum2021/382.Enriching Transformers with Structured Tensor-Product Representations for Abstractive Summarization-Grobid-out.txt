title
Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization

abstract
Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-TRANSFORMER  (Schlag et al., 2019) , an architecture that enriches the original Transformer  (Vaswani et al., 2017)  with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-TRANSFORMER outperforms the Transformer and the original TP-TRANSFORMER significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and improved syntactic interpretability in the TPR layer outputs. 1

Introduction Abstractive summarization is the task of generating a shorter version of a source text without necessarily reusing the sentences from the original source, Original Text (Truncated): Authorities said the incident took place on Sao Joao beach in Caparica, south-west of Lisbon. The National Maritime Authority said a middle-aged man and a young girl died after they were unable to avoid the plane.  [....]  Other reports said the victims had been sunbathing when the plane made its emergency landing.  [?]  Video footage from the scene carried by local broadcasters showed a small recreational plane parked on the sand, apparently intact and surrounded by beachgoers and emergency workers.  [?]  Reference Summary: A man and a child have been killed after a light aircraft made an emergency landing on a beach in Portugal. while preserving the meaning of its salient contents. It is a complex task that requires: semantic understanding of the source text and reasoning over its lexical units, making inferences about their relation to extract salient facts which are scattered across the long document, as well as generating a concise and coherent sequence of new sentences that covers the salient facts. While humans are remarkably good at this type of reasoning and abstraction, developing models that are capable of extraction, comprehension, abstraction, and reformulation of salient contents has been an open research question. One prominent aspect of abstractive summarization is that models struggle with combining multiple salient aspects in the source text into a coherent and grammatical set of sentences that preserve the original information in the source document. As shown in Fig.  1 , these pieces of salient information ("death", "emergency landing", "beach") are often connected by complex syntactic, causal, and temporal relations and are loosely grouped under the main topic of the source document. The transformer models  (Vaswani et al., 2017)  encode syntactic and semantic information of the input text into a single representation space with the self-attention, and decode the salient aspects into a short summary with the cross-attention. However, despite the large number of training examples, current state-of-theart transformer based approaches still struggle with systematic generalization of the composition of multiple salient pieces of information. In this paper, we investigate new types of computational primitives for transformers based on Tensor Product Representations (TPRs)  (Smolensky, 1990)  which are explicitly-compositional vector embeddings of symbolic structures. A Tensor Product Representation encodes a constituent in a symbolic structure as a composite of a role, which encodes the structural information (e.g., the dependency relation with another word), and a filler, which encodes the content of the constituent (e.g., the meaning of a word). Analogously, the TP-TRANSFORMER constructs a pair of representations for every token at every layer: a filler vector returned by attention and a novel role vector. As visualized in Fig.  2 , the model then binds the role and filler vectors to produce the output of every token as a TPR. We adapt the TP-TRANSFORMER  (Schlag et al., 2019) , which was proposed for solving mathematics problems, for the task of abstractive summarization. Unlike the original TP-TRANSFORMER, which directly projects the input representation into a continuous role vector space, our model generates the role vectors by attending to a learned dictionary of role embeddings  (Palangi et al., 2018) . We observe that most learned role attention distributions are approximately one-hot, thus restricting the role vectors to a highly discrete space. This structural inductive bias encourages the TP-TRANSFORMER to encode the syntactic information in the discrete roles while isolating the semantics in the continuous fillers. To test the ability of our TP-TRANSFORMER with discrete roles against the standard Transformer and the TP-TRANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-TRANSFORMER significantly outperforms the standard Transformer and the TP-TRANSFORMER with continuous roles on the XSum  (Narayan et al., 2018) , Wikihow  (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018)  datasets and achieves competitive performance on the CNN/Daily Mail  (Hermann et al., 2015; Nallapati et al., 2016)  dataset, measured by automatic metrics including ROUGE  (Lin, 2004)  and METEOR  (Denkowski and Lavie, 2014) . Our human evaluations on XSum and Wikihow datasets also correlate with the automatic metrics, demonstrating that summaries generated by our TP-TRANSFORMER are indeed better than the Trans-former's generations. Furthermore, to investigate the structural representation that naturally emerges during training and the advantage of having compositional TPR hidden states, we design a suite of decoder probing tasks to explore the information encoded in the role, filler, and TPR space. We adopt the encoder probing task design presented in  Tenney et al. (2019b)  and create four decoder probing tasks: Part-of-speech tagging (POS), Dependency Labeling (DEP), Semantic Role Labeling (SRL), and Named Entity Labeling (NEL). Our findings collectively show that the decoder's role vectors encode a wealth of syntactic structures, aiding the decoder in deducing the syntactic features (e.g., being a proper noun, being the object of the root predicate) of the next token to be generated. The decoder's filler vectors on the other hand encode more semantic information (e.g., being a person's name). Furthermore, we observe that having the compositional TPR results in a more interpretable final representation than the original Transformer has at every layer, regarding the syntactic features of the next word to be generated. Our results support our hypothesis that by disentangling semantics and syntax, such structured intermediate representations enable the model to better control both the content to be conveyed and the syntactic structure needed to express it, ultimately improving the factuality and grammaticality of the generated summaries. Our overall contributions are as follows: (1) we present a novel adaptation of the original Transformer architecture that incorporates a dictionary of role embeddings at every layer and generates Tensor Product Representation by binding the role vectors with attention outputs (filler vectors); (2) show that our TP-TRANSFORMER outperforms the Transformer as well as the original TP-TRANSFORMER  (Schlag et al., 2019)  on several abstractive summarization datasets; and (3) demonstrate the emergent structures in representations by revealing the disentangled syntactic and semantic information encoded in the role and filler spaces. 

 The TP-TRANSFORMER We build our TP-TRANSFORMER based on the Transformer architecture used in  Raffel et al. (2020) . A TP-TRANSFORMER encoder applied to a sequence of tokens i = 1, ..., I can be seen as a 2-dimensional lattice of cells (i, l) where i is the position of the input token and l = 1, ..., L are the layer indices. All cells in the encoder have the same architecture and the cells at the same layer share the same weights. We introduce the basic components of a TP-TRANSFORMER cell in Sec. 2.2 and its encoder and decoder cells in Sec. 2.3. 

 Tensor-Product Representation Basics Tensor-Product Representations (TPR;  (Smolensky, 1990 )) are explicitly-compositional vector embeddings of symbolic structures, where each constituent of the structure is represented as the product of a role vector, which encodes its structural information, and a filler vector, which contains the content. The TPR of a whole structure is the sum of the representation of its constituents. To represent any 3-digit number using TPRs, we need three role vectors: {r(p1): Ones place, r(p2): Tens place, r(p3): Hundreds place} and ten filler vectors f for ten digits. For example, the TPR of the number 985 is r(p1) ? f (5) + r(p2) ? f (8) + r(p3) ? f (9), where ? is the tensor product. When representing a number, the role vectors operate similarly as the positional embeddings in a Transformer  (Vaswani et al., 2017) . However, when representing natural languages, the role vectors need to encode a variety of structural information (e.g., predicate-argument, tense, etc) and thus it is infeasible to hand-design an entire suite of role vectors as we did for numbers. To overcome this challenge, for every token, we dynamically compute its role vector from a dictionary of a finite number of role embeddings learned with the entire model and treat the self-attention outputs as the fillers. We introduce the full computation procedure in Sec. 2.2.2. 

 The TP-TRANSFORMER Cell Similar to the basic Transformer cell, at every layer, a TP-TRANSFORMER Encoder cell starts with a layer normalization and the multi-head selfattention followed by a residual layer. Then, the cell treats the output vectors as fillers and binds them to role vectors to construct a Tensor Product Representation, which is then passed through the feed-forward network to yield the final states. 

 Multi-Head Attention The TP-TRANSFORMER cell adopts multi-head attention  (Vaswani et al., 2017)  to enable information passing between tokens. At any layer, denote the input vectors as X?R kx?dm and the attention target vectors as Y ?R ky?dm , where k x , k y are the length of the sequences and d m is the dimension of the input vectors. In the case of self attention, we have Y =X; while for the encoder-decoder cross attention, Y is the encoder's output vectors. We first apply layer normalization  (Ba et al., 2016)  to get X and then linearly project it to the query, key, and value vectors for each attention head h = 1, ..., H. Q h = XW h q + b h q K h = Y W h k + b h k V h = Y W h v + b h v (1) where W q , W k , W v ? R dm?d k . The attention output matrix V for each head h is computed as: V = softmax( QK T ? d k )V (2) where d k is the dimension of the key vectors K. The multi-head attention output O is the concatenation of the attention outputs from all heads followed by another linear projection W o ? R dm?dm . We end the Multi-head Attention with a residual connection with the layer input vectors X: MHAttn(X, Y ) = X + [ V1 , ..., VH ]W o (3) where Vh is the attention output for the h-th head. 

 Computing TPRs Role Embeddings. Following  Palangi et al. (2018) , but departing from  Schlag et al. (2019) , every layer of our TP-TRANSFORMER is equipped with a dictionary r ? R Nr?dr of N r distinct role embeddings with a dimension of d r . Each role embedding r n , n=1,. . . ,N r , is randomly initialized in the entire network. The role embeddings are normalized before computing role vectors: rn = r n r n 2 for n = 1, ..., N r (4) At each layer, the model computes a weighted combination of these role embeddings r to form a unique role vector for every token. Multi-Head TPR Binding. Our filler vectors correspond to the multi-head attention output F = MHAttn(X) (Eqn. 3). The filler F of each token has a corresponding role vector R. We first compute the R h ? R dr at every head h = 1, ..., H as a weighted average of the normalized role embeddings r. We then concatenate the R h ? R kx?dr of H heads to get the multi-head role vectors R ? R kx?(dr?H) for all k x tokens. We define this process formally as: R h = softmax(F W h r )r R = [R 1 , ..., R H ] (5) where W r ? R dm?Nr is the linear projection that computes the attention scores over the role embeddings for every token.  2  We use a Hadamard product 3 to approximate the full Tensor product in binding the role vectors R with filler vectors F , as it was shown in  Schlag et al. (2019)  that using the Hadamard products allows learning an optimial lower-rank approximation of the full TPRs. The binding operation is followed by an addition with the unbound fillers (F ) to return the residual TPR vectors. TPR(F ) = R F + F (6) 

 Residual Feed-forward Layer The feed-forward layer of a cell consists of a linear projection followed by a ReLU activation and a second linear projection. The feed-forward output is then added to the input vectors: FF(X) = X +ReLU(XW g +b g )W f +b f (7) Here, W g ?R dm?d f , b g ? R d f , W f ? R d f ?dm , b f ? R dm , and x is the function argument. 

 TP-TRANSFORMER Encoder & Decoder Given the components of our basic TP-TRANSFORMER cell in the previous section, we now describe how we construct the TP-TRANSFORMER encoder and decoder. First, the self-attention and the encoder-decoder cross-attention for every token can be computed as: Self(X) = TPR(MHAttn(X, X)) Cross(Y, H) = TPR(MHAttn(Y, H)) ( 8 ) where H is the output of the encoder's final layer. Y represent the previous layer's output vectors of either the partially (so-far) decoded sequence at test time or the masked reference summary at training time. The encoder and decoder's operations at every layer can be summarized as: Encode(X) = FF(Self(X)) Decode(H, Y ) = FF(Cross(Self(Y ), H)) (9) After L layers of encoding and decoding, the final distribution of the i-th output token is given by: ?i = softmax(E T y i,L ) (10) where Y L = Decode(H, Y L?1 ) are the decoder's output states at the last layer and E is the tied input/output word embeddings. 3 Summarization Experiments 

 Abstractive Summarization Datasets We train our models on four English abstractive summarization datasets varying the level of abstractiveness (explained below) and the length of summaries, as well as input domain. XSum  (Narayan et al., 2018)  consists of 227k BBC articles from 2010 to 2017 concerning various subjects along with professionally written singlesentence summaries. Its summaries cover a wide variety of syntactic structures (relative clause, etc) and relations  (causal, temporal, etc) . Wikihow  (Koupaee and Wang, 2018)    

 Arxiv (Abbreviated) We study the phase behavior of a nematic liquid crystal confined between a flat substrate with strong anchoring and a patterned substrate whose structure and local anchoring strength we vary. [. . . ] In addition the effective energy method allows one to determine the energy barriers between two states in a bistable nematic device . 

 CNN/DM Mentally ill inmates in Miami are housed on the "forgotten floor". Judge Steven Leifman says most are there as a result of "avoidable felonies". While CNN tours facility, patient shouts: "I am the son of the president". Dataset Abstractiveness. We show a summary from each of these four datasets in Table  1 . According to the comparison made by  Zhang et al. (2020)  using the coverage and density measures  (Grusky et al., 2018) , the XSum and Wikihow datasets are more abstractive than the others since their summaries rarely contain large chunks of words overlapping with the source documents. CNN/Daily Mail is the least abstractive of the four. Furthermore, in most cases, a sentence in a CNN/Daily Mail summary only refers to a single sentence from the source document as suggested in  Lebanoff et al. (2019) , while a sentence in an XSum or Wikihow summary usually aggregates information from multiple source sentences. 

 Experimental Setup The Transformer and the two TP-TRANSFORMERS all have 6 layers, 8 heads per layer, dimension per head d k =64, model dimension d m =512, and feedforward dimension d f =2048 for the encoder and decoder. Our TP-TRANSFORMER with discrete roles has N r =50 role embeddings of dimension d r =64 at every layer. For each dataset above, we train the all three models from scratch using an Adafactor Optimizer  (Shazeer and Stern, 2018)  with square root learning rate decay and dropout rate of 0.1. We evaluate the models using automatic metrics including ROUGE F1 score and METEOR. 

 Results We report automatic metric scores from our evaluated models in 2019) as TPT-c, and our own TP-TRANSFORMER with a discrete set of role embeddings as TPT-d. On the XSum, Arxiv, and Wikihow datasets, our TP-TRANSFORMER (TPT-d) outperforms the original Transformer on all metrics. On the CNN/Daily Mail dataset, both models obtain similar performance across all metrics. On every dataset, the TPT-c model which excels on the mathematics dataset, is the worst among the three models being compared. This suggests that continuous role vectors are not suited to the summarization tasks. As we explain in Sec. 3.1, CNN/Daily Mail is the least abstractive one among the four datasets. In contrast, summaries from the XSum and Wikihow datasets contain very few n-grams (n>2) that can be copied from the source documents and thus push the model's ability to compose a coherent summary restating the salient aspects from the source. Furthermore, as illustrated in Table  1 , the XSum summary contains a long sentence that combines multiple pieces of information scattered through the long source document. These facts are usually connected by syntactic, temporal 4 , or causal 5 relations and thus the model must be able to connect and reason across these salient facts and then convert them into a coherent sentence that faithfully reflects the original facts and their relations. We argue that the compositional TPR can better enable these abilities required for XSum, where we indeed find that our TP-TRANSFORMER achieves the largest advantage over the Transformer among its improvements on all datasets. 

 Human Evaluation We conduct human evaluation to compare the summaries generated by the Transformer and our TP-TRANSFORMER. We randomly sample 120 examples from the test sets of XSum and Wikihow datasets with the beam-searched model summaries. We refer to appendix for the complete setup. As shown in Table  3 , on the XSum dataset, summaries generated by the TP-TRANSFORMER are significantly better in grammar. This corroborates our claim that having the TPR can improve the model's ability to follow the correct syntax in composing the summary. On the Wikihow dataset, the Transformer receives more votes in regarding the saliency. However, our TP-TRANSFORMER maintains an advantage in grammar and achieves significantly better overall preferences. Unfaithful XSum Examples It is well-known that the XSum dataset contains a portion of unfaithful reference summaries that mention facts not included in the source article  (Durmus et al., 2020; Maynez et al., 2020) . Therefore, we are interested to find out whether our TP-TRANSFORMER is better than the baseline only at expressing the faithful content or it can also generate some external, "unfaithful" facts that the baseline can't cover. To answer this question, we randomly sample 100 examples from the XSum dev set and manually examine the source document, reference summary, and the two generated summaries. Among these 100 examples, we identify 71 examples whose reference summary includes "unfaithful" facts that are not mentioned in the source. In 21 out of 71 examples, the Transformer baseline manages to generate some "unfaithful" facts that match those in the reference while our TP-TRANSFORMER achieves this in 17 examples. Such "unfaithful" facts that were recovered by the models include the full name of a person when only the last name is mentioned in the source, the political party or the job title of a person, each of which can be attributed to at least one example seen by models during the training. Therefore, we believe that both models learn to draw external information from its memory of the seen examples, while our TP-TRANSFORMER doesn't do better than the baseline Transformer at referring to external facts to obtain higher ROUGE scores. Probing is a method to test whether some particular information is present in the model's encodings. To achieve this, an auxiliary classifier is trained to predict specified linguistic features from the model's internal representations. We probe different components (roles, filler, TPRs) in our TP-TRANSFORMERs as well as the attention+residual outputs (equivalent to the filler) of the Transformer to assess the naturally emergent structures encoded in the role vectors and the effectiveness of the TPR in the decoding process. By conducting the probing experiments, we aim to (1) provide some insights and evidence of the different information encoded by the role and filler vectors; and (2) explain the ROUGE advantage of our TP-TRANSFORMER by showing that its output representation can better encode the linguistic structural information concerning multiple probing tasks. 

 Decoder Probing Tasks When studying an encoder, previous works probe its i-th intermediate representation at a certain layer for information about the i-th input token For a decoder, however, we probe its i-th representation for clues about the i-th token it generates given the ? 1 previously generated tokens as the input. Intuitively, we are probing for the decoder's internal decision about the syntactic roles and semantic content of this token before it was ultimately selected. Based on encoder probing tasks used by  Tenney et al. (2019b) , we select and adapt four tasks to probe our decoders. Part-of-speech tagging (POS) is the syntactic task of assigning tags such as noun (singular/mass noun: NN, proper noun: NNP, etc), verb (past tense: VBD, past participle: VBN, etc), adjective (comparative: JJR, etc), etc. to each token i. We let s 1 = [i, i + 1) be a single token, and seek to predict its POS tag. Dependency labeling (DEP) seeks to predict the functional relationships of one token relative to another: e.g. is it a modifier-head relationship, a subject-verb relationship, etc. We take s 1 = [i, i + 1) to be a single token and s 2 = [j, j + 1) to be its syntactic head, and seek to predict the dependency relation between tokens i and j. Semantic role labeling (SRL) is the task of imposing predicate-argument structure onto a sentence. We let s 1 = [i 1 , j 1 ) represent a known predicate (e.g., "push") and s 2 = [i 2 , j 2 ) represent a known argument ("Peter") of that predicate, and seek to predict the role that the argument s 2 fills-e.g. ARG0 (agent, the pusher) vs. ARG1 (patient, the pushee). Named entity labeling (NEL) is the task of predicting the category of an entity. The categories include PERSON, LOCATION, ORGANIZATION, etc. We let s 1 = [i, j) represent a known entity span and seek to predict its type. 

 Experimental Setup As there is no existing dataset for probing decoders, we create our own training and evaluation data by running off-the-shelf models on the summarization datasets. Specifically, to probe a decoder trained on the XSum dataset on the POS task, we run an POS tagger on the reference summaries from the XSum training set and the model-generated summaries for the XSum dev set to create the ground-truth labels for the training set and model-specific dev set. We restore the model trained on a summarization dataset and freeze its parameters. Following  Tenney et al. (2019b) , we train a span convolution layer followed by a 2-layer MLP on top of the target representation that project it onto the output label space. 

 Results Table  4  presents the results of probing the decoder of a TP-TRANSFORMER trained on the XSum  (Narayan et al., 2018)  dataset. Note that the Transformer doesn't have role vectors. It directly outputs the vector after the multi-head attention and the residual layer. Therefore, its fillers and final representations are equivalent. The decoder role vectors can encode grammatical information while the filler vectors represent the semantics. We first focus on the results of POS tagging probing task. Overall, we see a trend of increasing scores as the representations get closer to the final step of computing the distribution over the vocabulary. This implies that, as the computation progresses through the layers, the generated representations are gradually deciding the POS tag of the next word to generate. Next, we observe that the role vectors (the 1st number in the TPT-d column) of TP-TRANSFORMER encode a considerable amount of information about the POS tag of the next word generated. Additionally, because the job of deducing the POS tag of the next word is partially shared by the role vectors, the filler vectors' performance degrades compared to the Transformer. This pattern demonstrates that the TP-TRANSFORMER's decoder is representing the next word to be generated as a composite of structural information encoded in the role vectors and semantic contents encoded in the filler vectors. Comparing the fillers (the 2nd number in TPT-d column) with the TPR (the 3rd number in the TPTd column) of TP-TRANSFORMER, we see that the TPRs, which bind the roles and fillers, outperform the roles and fillers alone at every layer. This indicates that the TPR effectively aggregates the linguistic knowledge encoded in the roles and fillers into a shared space, where the POS tag of the next word can be decoded more easily than in the role space or filler space alone. Last, the final representations of TP-TRANSFORMER achieve higher F1 scores than their counterparts in the Transformer in the last three layers. This demonstrates the benefits of having the TPR in interpreting the POS tag of the word to be generated. When we consider the Dependency labeling (DEP) and Semantic role labeling (SRL) tasks, we observe that our TP-TRANSFORMER's final representations consistently beat the Transformer across all layers, with only one exception in the DEP task at the layer 2. We also observe that the TP-TRANSFORMER's advantage becomes larger in the last three layers except for the final layer in SRL task. However, unlike in the POS task, the TPR only achieve similar F1 scores to the fillers. Finally, in the Named entity labeling (NEL) task which is considered to require more semantic information rather than syntax, the role vectors' performance is poorer than their performance in the three syntactic tasks. For example, the TP-TRANSFORMER's final representations at layer 6 obtain similar F1 scores in the POS and NEL tasks (74.5 VS 73.8), but its role vectors only achieve a 42.2 F1 score in the NEL tasks compared to the 56.0 in the POS. However, even though the role vectors encode little information about the named entity type of the next token to be generated, the TPR still strongly outperforms the Transformer's filler-only representation at every layer. We argue that although the syntactic information encoded in the role vectors is not enough to predict the correct named entity, it is still a beneficial complement to the knowledge encoded in the distributed filler vectors in certain situations. For example, whether the subject "Chanel" refers to a PERSON or an OR-GANIZATION could depend on its syntactic role and its relation to other words in the sentence (e.g., whether it is the subject or object of "wears") . Compositional representations improves interpretability of the representations. Overall, by probing the different intermediate representations of the TP-TRANSFORMER and the Transformer, we show that having the compositional TPR results in more interpretable final representations at every layer regarding the syntactic features of the next word to be generated. Considering automatic evaluations generated summaries in Sec. 3.3, we argue that this compositionality in learned representation and its syntactic interpretability enable the decoder to take better control of the syntactic structure of the generation when assembling multiple distant facts, and thus lead to summaries of better quality. 

 Discrete Role Vectors During the training of our TP-TRANSFORMER models on the summarization datasets, we observe that most learned role attention distributions are approximately one-hot, as more than 90% of the role attention distributions (as computed in Eqn. 5) have a maximum score larger than 0.98. Because each role vector is the concatenation of H vectors, each selected from N r role embeddings, the completely one-hot role attentions will yield (N r ) H possible role vectors. Therefore, the learned, approximately one-hot role vectors span (N r ) H discrete subspaces, each of which only covers the close proximity of a concatenation of H role embeddings. This finding indicates that as we represent the role vectors as multi-head attention over a learnable dictionary of role embeddings, the structural inductive bias: (1) pushes the role vector space to be even more discrete, and (2) induces the syntactic structures encoded in these discrete role vectors. We also believe there is a connection between the above two effects, as the structural, syntactic information favors a lower-dimensional or even discrete space while the distributed, semantic information favors a higher-dimensional space. 

 Related Work Explicit TPR Structures in Neural Networks While earlier TPR work based on  (Smolensky, 1990)  focused on computability rather than learnability questions, recently TPRs have been incorporated into several recurrent deep learning models in order to solve various NLP tasks including Part-of-Speech tagging, constituency parsing, image captioning  (Huang et al., 2018 (Huang et al., , 2019 , question answering  (Palangi et al., 2018; Schlag and Schmidhuber, 2018) , and natural-to-formal language generation (program synthesis)  (Chen et al., 2020) . Most recently, TPRs have been introduced into Transformer architectures, starting with  Schlag et al. (2019)  which introduced the TP-TRANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. Structured Representations for Abstractive Summarization Compared to the extractive methods, abstractive summarization models usually fail to show extractive properties, and have tendency to copy text from the source  (See et al., 2017; Paulus et al., 2018; Pasunuru and Bansal, 2018; Celikyilmaz et al., 2018) . More recent approaches that use standard transformers deal with this issue by introducing hierarchical structures to encode local and global information separately focusing on only the semantic content  Lapata, 2018, 2019) . To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while  Song et al. (2018)  introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism,  Liao et al. (2018)  uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 

 Conclusion In this work, we enrich the Transformer model with the structured Tensor Product Representation for abstractive summarization tasks. We represent every token as a pair of role and filler vectors. We show that our TP-TRANSFORMER with discrete roles outperforms Transformer and TP-TRANSFORMER with continuous roles on several abstractive summarization datasets, in both metrics scores and human evaluation. We further demonstrate the syntactic structures encoded in the role vectors and show the improved syntactic interpretability in our model's hidden states. 
