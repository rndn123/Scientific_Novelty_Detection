title
GOLD GOLD GOLD GOLD GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation

abstract
Practical dialogue systems require robust methods of detecting out-of-scope (OOS) utterances to avoid conversational breakdowns and related failure modes. Directly training a model with labeled OOS examples yields reasonable performance, but obtaining such data is a resource-intensive process. To tackle this limited-data problem, previous methods focus on better modeling the distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal technique that augments existing data to train better OOS detectors operating in low-data regimes. GOLD generates pseudo-labeled candidates using samples from an auxiliary dataset and keeps only the most beneficial candidates for training through a novel filtering mechanism. In experiments across three target benchmarks, the top GOLD model outperforms all existing methods on all key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median baseline performance. We also analyze the unique properties of OOS data to identify key factors for optimally applying our proposed method. 1

Introduction Detecting out-of-scope scenarios is an essential skill of dialogue systems deployed into the real world. While an ideal system would behave appropriately in all conversational settings, such perfection is not possible given that training data is finite, while user inputs are not  (Geiger et al., 2019) . Out-of-distribution issues occur when the model encounters situations not covered during training, including novel user intents, domain shifts or custom entities  (Kamath et al., 2020; Cavalin et al., 2020) . Unique to conversations, dialogue breakdowns represent cases where the user cannot continue the interaction with the system, perhaps due to ambiguous requests or prior misunderstandings  (Martinovsky and Traum, 2003; Higashinaka et al., 2016) . Such breakdowns might fall within the distribution of plausible utterances, yet still fail to make sense due to the given context. OOS detection aims to recognize both out-of-distribution problems and dialogue breakdowns. Prior methods tackling OOS detection in text have shown great promise, but typically assume access to a sufficient amount of labeled OOS data during training  (Larson et al., 2019) , which is unrealistic in open-world settings  (Fei and Liu, 2016) . Alternative methods have also been explored which train a supporting model using in-scope data rather than directly training a core model to detect OOS instances  (Gangal et al., 2020) . As a result, they suffer from a mismatch where the objective during training does not line up with the eventual inference task, likely leading to suboptimal performance. More recently, data augmentation techniques have been applied to in-scope (INS) data to improve out-of-domain robustness  (Ng et al., 2020a; Zheng et al., 2020) . However, we hypothesize that since INS data comes from a different distribution as OOS data, augmentation on the former will not perform as well as augmentation on the latter. In this paper, we propose a method of Generating Out-of-scope Labels with Data augmentation (GOLD) to improve OOS detection in dialogue. To create new pseudo-labeled examples, we start with a small seed set of known OOS examples. Next, we find utterances that are similar to the known OOS examples within an auxiliary dataset. We then generate candidate labels by replacing text from the known OOS examples with the similar utterances uncovered in the previous step. Lastly, we run an election to filter down the candidates to only those which are most likely to be out-of-scope. Our method is complementary to other indirect prediction techniques and in fact takes advantage of progress by other methods. We demonstrate the effectiveness of GOLD across three task-oriented dialogue datasets, where our method achieves state-of-theart performance across all key metrics. We conduct extensive ablations and additional experiments to probe the robustness of our best performing model. Finally, we provide analysis and insights on augmenting OOS data for other dialogue systems. 

 Related Work 

 Direct Prediction A straightforward method of detecting out-ofscope scenarios is to train directly on OOS examples  (Fumera et al., 2003) . These situations are encountered more broadly by the insertion of any out-of-distribution response or more specifically when a particular utterance does not make sense in the current context. Out-of-Distribution Recognition An utterance may be out-of-scope because it was not included in the distribution the dialogue model was trained on. Distribution shifts may occur due to unknown user intents, different domains or incoherent speech. We differ from such methods since they either operate on images  (Kim and Kim, 2018; Hendrycks et al., 2019; Mohseni et al., 2020)  or assume access to an impractically large number of OOS examples in relation to INS examples  (Tan et al., 2019; Kamath et al., 2020; Larson et al., 2019)  Dialogue Breakdown In comparison to out-ofdistribution cases, dialogue breakdowns are unique to conversations because they depend on context  (Higashinaka et al., 2016) . In other words, the utterances fall within the distribution of reasonable responses but are out-of-scope due to the state of the particular dialogue. Such breakdowns occur when the conversation can no longer proceed smoothly due to an ambiguous statement from the user or some misunderstanding made by the agent  (Ng et al., 2020b) . GOLD also focuses on dialogue, but additionally operates under the setting of limited access to OOS data during training  (Hendriksen et al., 2019) . 

 Indirect Prediction An alternative set of methods for OOS detection assume access to a supporting model trained solely on in-scope data. There are roughly three ways in which a core detector model can take advantage of the pre-trained supporting model. 

 Probability Threshold The first class of methods utilize the output probability of the supporting model to determine whether an input is outof-scope. More specifically, if the supporting model's maximum output probability falls below some threshold ? , then it is deemed uncertain and the core detector model labels the input as OOS  (Hendrycks and Gimpel, 2017) . The confidence score of the supporting model can also be manipulated in a number of ways to help further separate the INS and OOS examples  (Liang et al., 2018; . Other variations include setting thresholds on reconstruction loss  (Ryu et al., 2017)  or on likelihood ratios  (Ren et al., 2019) . Outlier Distance Another class of methods define out-of-scope examples as outliers whose distance is far away from known in-scope examples  (Gu et al., 2019; Mandelbaum and Weinshall, 2017) . Variants can tweak the embedding function or distance function used for determining the degree of separation.  (Cavalin et al., 2020; Oh et al., 2018; Yilmaz and Toraman, 2020) . For example, Local Outlier Factor (LOF) defines an outlier as a point whose density is lower than that of its nearest neighbors  (Breunig et al., 2000; Lin and Xu, 2019) . 

 Bayesian Ensembles The final class of methods utilize the variance of supporting models to make decisions. When the variance of the predictions is high, then the input is supposedly difficult to recognize and thus out-of-distribution. Such ensembles can be formed explicitly through a collection of models  (Vyas et al., 2018; Shu et al., 2017; Lakshminarayanan et al., 2017)  or implicitly through multiple applications of dropout  (Gal and Ghahramani, 2016) . 

 Data Augmentation Our method also pertains to the use of data augmentation to improve model performance under low resource settings. Augmentation in NLP Data augmentation for NLP has been studied extensively in the past  (Jia and Liang, 2016; Silfverberg et al., 2017; F?rstenau and Lapata, 2009) . Common methods include those that alter the surface form text  (Wei and Zou, 2019)  or perturb a latent embedding space  (Wang and Yang, 2015; Fadaee et al., 2017; Liu et al., 2020) , as well as those that perform paraphrasing  (Zhang et al., 2019) . Alternatively, masked language models generate new examples by proposing context-aware replacements for the masked token  (Kobayashi, 2018; Wu et al., 2019) . Data Augmentation for Dialogue Methods for augmenting data to train dialogue systems are most closely related to our work. Previous research has used data augmentation to improve natural language understanding (NLU) and intent detection in dialogue  (Niu and Bansal, 2019; Hou et al., 2018) . Other methods augment the in-scope sample representations to support out-of-scope robustness  (Ryu et al., 2018; Ng et al., 2020a; Lee and Shalyminov, 2019) . Recently, generative adversarial networks (GANs) have been used to create out-ofdomain examples that mimic known in-scope examples  (Zheng et al., 2020; Marek et al., 2021) . In contrast, we operate directly on OOS samples and consciously generate data far away from anything seen during pre-training, a decision which our later analysis reveals to be quite important. 

 Background and Baselines In this section we formally describe the task of outof-scope detection and the different approaches to handling this issue. 

 Problem Formulation Let D direct = {(x 1 , y 1 ), ..., (x n , y n )} be a target dataset containing a mixture of in-scope and out-of-scope dialogues. The input context x i = {(S 1 , U 1 ), ..., (S t , U t )} is a series of system and user utterances within t turns of a conversation. The desired output y i ? [0, 1] is a binary label representing whether that context is out-of-scope. We define OOS to encompass both out-of-distribution utterances, such as out-of-domain intents or gibberish speech, as well as in-distribution utterances spoken in an ambiguous manner. A model given access to such a dataset is an OOS detector P ? (y i |x i ) performing direct prediction. In contrast, the problem we tackle in this paper is indirect prediction, where only a limited or nonexistent number of OOS examples are available during training. Instead, the training data is sampled from in-scope dialogues D indirect ? P IN S , and the labels y j ? Y represent a set of known user intents. This data may be used to train an intent classifier which then acts as a supporting model to the core OOS detector during inference. Critically, the supporting model P ? (y j |x i ) has never encountered out-of-scope utterances during training. 

 Baselines Prior methods for approaching indirect prediction generally fall into three categories: probability threshold, outlier distance and Bayesian ensemble. In all cases, the supporting model trained on the intent classification task uses a pretrained BERT model as its base  (Devlin et al., 2019) . Starting with Probability Threshold baselines, (1) MaxProb declares an example as OOS if the maximum value of the supporting model's output probability distribution falls below some threshold ?  (Hendrycks and Gimpel, 2017) . (  2 ) ODIN enhances this by adding temperature scaling and small perturbations to the input which help to increase the gap between INS and OOS instances  (Liang et al., 2018) . (3) Entropy considers an example to be OOS if the supporting model is uncertain, as determined by the entropy level rising above a threshold ? (Lewis and Gale, 1994). Outlier model pre-trained on intent classification and measures separation by Euclidean distance. Based on the success in  (Podolskiy et al., 2021) , the (5) Mahalanobis method embeds examples with a vanilla RoBERTa model and uses the Mahalanobis distance . Finally, inspired by BADGE for active learning  (Ash et al., 2020) , the (6) Gradient method sets the embedding of each example as the gradient vector of the input tokens as computed by back-propagation. Bayesian Ensembles predict labels by the amount of variation formed by the estimates of an ensemble. More specifically, (7) Dropout implicitly creates a new model whenever it randomly drops a percentage of its nodes (Gal and Ghahramani, 2016). During inference, each input is passed through the supporting model k times to estimate the user intent. If the ensemble fails to reach a majority vote on the intent classification task, then the example is assigned as out-of-scope. 

 GOLD To avoid a mismatch between training and inference, we are motivated to explore the direct prediction paradigm in a way the does not violate the OOS data restriction inherent to indirect prediction methods. Concretely, GOLD performs data augmentation on a small sample of labeled OOS examples to generate pseudo-OOS data. This weaklylabeled data is then combined with INS data for training a core OOS detector. We limit the number of OOS samples to be only 1% of the size of inscope training examples. Note that indirect methods also typically have access to a modest number of OOS samples for tuning hyper-parameters, such as thresholds, so this adjustment is not an exclusive advantage of our method. In addition to a small seed set of OOS examples, we assume access to an external pool of utterances, which serve as the source of data augmentations, similar to  Hendrycks et al. (2019) . We refer to this auxiliary data as the source dataset S, as opposed to the target dataset T used for evaluating our method. GOLD now proceeds in three basic steps. (See Algorithm 1 for full details.) 

 Match Extraction Our first step is to find utterances in the source data that closely match the examples in the OOS seed data. We encode all source and seed data into a shared embedding space to allow for comparison. When the seed example is a multi-turn dialogue, we embed only the final user utterance. Then for each seed utterance, we extract d similar utterances from source S as measured by cosine distance, 2 where d is the desired number of matches. For example, as seen in Figure  1 , the seed text "Do you know if it will rain on Friday?" extracts "Will it rain that day?" as a match. We discuss different types of embedding mechanisms in section 5.3. 

 Candidate Generation Since dialogue contexts often contain multiple utterances, we want our augmented examples to also span multiple turns. Accordingly, our next step involves generating candidates by carefully crafting new conversations using the existing dialogue contexts in the seed data. Each new candidate is formed by swapping a random user utterance in the seed data with a match utterance from the source data. Notably, agent utterances in the seed data are left untouched during this process. 

 Target Election Candidates are merely pseudo-labeled as OOS, so relying on such data as a training signal might be quite noisy. Accordingly, we apply a filtering mechanism to ensure that only the candidates most likely to be out-of-scope are "elected" to become target OOS data. Elections are held by running all the candidates through an ensemble of baseline detectors. Specifically, we choose the top detectors from each of the major indirect prediction categories which results in three voters. If the majority of voters agree that an example is out-of-scope, then we include that candidate in our target pool. As if majority(votes) then: SM Calendar Flow FLOW is also a taskoriented dataset with turn-level annotations  (Andreas et al., 2020) . Originally built for semantic parsing, FLOW is structured as a novel dataflow object that takes the form of a computational graph. 14: A i ? A i ? For our purposes, we take advantage of the 'Fence' related labels found in the dataset, which represent situations where a user is straying too far away from discussions within the scope of the system, and thus need to be "fenced-in". We focus on utterances associated with a clear intent, once again dropping turns representing greetings and other pleasantries, which results in 71,551 examples spanning 44 total intents. The test set is hidden behind a leaderboard, so we divide the development set in half, resulting in an approximate 90/5/5 split for train, dev and test, respectively. Real Out-of-Domain Sentences From Taskoriented Dialog ROSTD is a dataset explicitly designed for out-of-distribution recognition  (Gangal et al., 2020) . The authors constructed sentences to be OOS examples with respect to a separate dataset collected by  Schuster et al. (2019) . The dialogues found in the original dataset then represent the INS examples. ROSTD contains 47,913 total utterances spanning 13 intent classes and comes with a pre-defined 70/10/20 split which we leave unaltered. The dataset is less conversational since each example consists of a single turn command, while its labels are higher precision since each OOS instance is human-curated. 

 Evaluation Metrics Following prior work on out-of-distribution detection  (Hendrycks and Gimpel, 2017; Ren et al., 2019) , we evaluate our method on three primary metrics. (1) Area under the receiver operating characteristic curve (AUROC) measures the probability that a random OOS example will have a higher probability of being out-of-scope than a randomly selected INS example  (Davis and Goadrich, 2006) . This metric averages across all thresholds and is therefore threshold independent. (  2    (Hendrycks et al., 2019) . Thus, unlike the first two metrics, a lower FPR@N is better. We report FPR at values of N={0.90, 0.95}. 

 Experiments on Model Variants In addition to testing against baseline methods, we also run experiments to study the impact of varying the auxiliary dataset and the extraction options. 

 Source Datasets We consider a range of datasets as sources of augmentation, starting with known out-of-scope queries (OSQ) from the Clinc150 dataset  (Larson et al., 2019) . Because our work falls under the dialogue setting, we also consider Taskmaster-2 (TM) as a source of task-oriented utterances  (Byrne et al., 2019)  and PersonaChat (PC) for examples of informal chit-chat  (Zhang et al., 2018) . Upon examining the validation data, we note that many examples of OOS are driven by users attempting to ask questions that the agent is not able to handle. Thus, we also include a dataset composed of questions extracted from Quora (QQP) (Iyer et al., ). Finally, we consider mixing all four datasets together into a single collection (MIX). 

 Extraction Techniques To optimize the procedure of extracting matches from the source data, we try four different mechanisms for embedding utterances. (1) We feed each OOS instance into a SentenceRoBERTa model pretrained for paraphrase retrieval to find similar utterances within the source data (Reimers and Gurevych, 2019). (  2 ) As a second option, we encode source data using a static  BERT Transformer model (Devlin et al., 2019) . Then for each OOS example encoded in the same manner, we extract the nearest source utterances. (3) We embed OOS and source data as a bag-of-words where each token is a 300-dim GloVe embedding  (Pennington et al., 2014) . (  4 ) As a final variation, we embed all utterances with TF-IDF embeddings of 7000 dimensions. The spectrum of extraction techniques aim to progress from methods that capture strong semantic connections to the OOS seed data towards options with weaker relation to original seed data. 

 Key Results We now present the results of our main experiments. As evidenced by Figure  3 , MIX performed as the best data source across all datasets, so we use it to report our main metrics within Table  2 . Also, given the strong performance of GloVe extraction technique across all datasets, we select this version for comparison purposes in the following analyses. Performance is even more impressive in lowering the false positive rate with improvements of 24.2% and 29.8% at recalls of 0.95 and 0.90, respectively. Among the different baselines, we observe the Outlier Distance methods generally outperforming the others, with the Mahalanobis method doing the best. Among GOLD variations, there are mixed results as GloVe and TF-IDF both produce high overall accuracy. Notably, the Paraphrase method meant to extract matches most similar to the seed data performed the worst. for FPR@0.95 over the nearest baseline. We again notice that the Paraphrase variation does not perform quite as well among GOLD methods. 

 STAR Results 

 Left columns of 

 FLOW Results 

 Central columns of 

 ROSTD Results As seen in Tables  2 and 3 , GOLD outperforms not only all baselines, but also prior work on ROSTD across all metrics. The GloVe method cements its standing at the top with gains of 1.7% in AUROC, 13.8% in AUPR and 97.9% in FPR@0.95 against the top baselines. Given the consistently poor performance of Paraphrase yet again, we conclude that unlike traditional INS data augmentation, augmenting OOS data should not aim to find the most similar examples to seed data. We hypothesize that producing pseudo-labeled OOS data that are too similar to given known-OOS data causes the model to overfit since it is simply optimizing towards the same examples over and over again. 

 Discussion and Analysis In this section, we conduct follow-up experiments to analyze the impact of our method's components and identify best practices when applying data augmentation for OOS detection. 

 Ablations How much does augmentation help? Given the extra labels from the seed set, it is natural to ask whether the augmented data add any value. Furthermore, if the augmented data are useful, then we might want to know what an ideal number of additional datapoints would be. Figure  4  displays the AUROC of a model trained on varying the number of augmented datapoints, where "0" represents including only known OOS examples. We see a trend that accuracy improves for all target datasets as we add more pseudo-labeled examples, showing that augmentation helps. Improvement reaches a max around 24 matches per seed example, which suggests that the benefit of adding more datapoints has a limit. Accordingly, we use 24 matches for all results listed in Table  2 .   2  reveals the result of using random selection as an extraction technique. While Random is not always the worst, its poor performance across all metrics strongly suggests that augmented data should have at least some connection to the original seed set. 

 Does the extraction Is filtering even necessary? Since the source data distribution is obviously distinct from the target data distribution, perhaps it is possible to bypass elections and simply accept all candidates as OOS, similar to Outlier Exposure from  Hendrycks et al. (2019) . As seen in row 4 of Table  4 , we observe that skipping elections leads to a drop in the AUROC of all the models on all datasets. The effect is most pronounced for STAR, where some of the QQP dialogues overlap with in-scope STAR domains. 

 STAR Data FLOW Data ROSTD Data Methods AUROC AUPR FPR@.95 FPR@.90 AUROC AUPR FPR@.95 FPR@.90 AUROC AUPR FPR@.95 FPR@.90    

 Applicability How well would a direct classifier perform? Indirect prediction is often necessary in real-life because while in-scope data may be trivial to obtain, out-of-scope data is typically lacking. Accordingly, we artificially limited the amount of data available to mimic this setting. If such a limitation were to be lifted such that a sufficient amount of known OOS data were available, we could train a model to directly classify such examples. The first row in Table  2  shows the results of using all the available OOS data to perform direct prediction and represents an upper-bound on accuracy. This also shows there is still substantial room for improvement. When does GOLD help the most? GOLD depends on a small seed set to perform data augmentation, so if this data is unavailable or extremely sparse, then the method will likely suffer. To test this limit, we train a model with half the size of the seed data and double the number of matches (d = 24 ? 48) to counterbalance the effect. Despite having an equal amount of pseudo-labeled OOS examples, the model with a tiny seed set (row 5 in Table  4 ) severely underperforms the original model (row 1). Separately, we note that dialogue breakdowns are more likely in conversations that contain multiple turns of context, like in STAR, as opposed dialogues consisting of single lines, as in ROSTD. Given the more prominent gains by our method in STAR, we conclude that GOLD achieves its gains partially from being able to recognize dialogue breakdowns. What attributes make a source dataset useful? In studying Figure  3 , we find that the most consistent single source dataset is QQP, which we use as the default for Table  4 . Reading through some examples in QQP, the pattern we found was that many of the samples contained reasonable, but unanswerable questions that were beyond the skillset of the agent. One method for curating a useful source dataset then is to look for a corpus containing questions your dialogue model likely cannot answer. Furthermore, PersonaChat (PC) performed particularly well with STAR, a task-oriented dataset. We believe that since goal-oriented chatbots aim to solve specific tasks rather than engage in chit-chat, open-domain chat datasets serve as a good source of OOS examples. The themes above suggest that good source datasets are simply those sufficiently different from the target data. We wondered if there was such as a thing as going to 'far', and conversely if there was any harm in being quite 'close'. Concretely, we expected a dataset containing medical questions would represent a substantially different dialogues compared to our target data  (Ben Abacha and Demner-Fushman, 2019) . Table  4  presents results when training with source data from a medical question-answering dataset (MQA) or from unlabeled samples (US) from the same target dataset. The results show a significant drop in performance, indicating that augmentations far away from the decision boundary might not add much value. Rather, pseudo-labels near the border of INS and OOS instances are the most helpful. (Further analysis in Appendix C) How does one create good OOS examples? As a final experiment, we replace only the last utterance with a match when generating candidates, rather than swapping any user utterance. We speculate this creates less diverse pseudo-examples, and therefore decreases the coverage of the OOS space. Indeed, row 6 in Table  4  reveals that worse candidates are generated when only the final utterance is allowed to be replaced. In conjunction with the insight from Section 6.3 that generated examples should be sufficiently different from given OOS examples, we believe that the key to producing good pseudo-OOS examples is to maximize the diversity of fake examples. OOS detection is less about finding out-of-scope cases, but rather an exercise in determining when something is not in-scope. This subtle distinction implies that the appropriate inductive biases should aim to move away from INS distribution, rather than close to OOS distribution. 

 Conclusion This paper presents GOLD, a method for improving OOS detection when limited training examples are available by leveraging data augmentation. Rather than relying on a separate model to support the detection task, our proposed method directly trains a model to detect out-of-scope instances. Compared to other data augmentation methods, GOLD takes advantage of auxiliary data to expand the coverage of out-of-scope distribution examples rather than trying to extrapolate from in-scope examples. Moreover, our analysis reveals key techniques for further diversifying the training data to support robustness and prevent overfitting. We demonstrate the effectiveness of our technique across three dialogue datasets, where our top models outperform all baselines by a large margin. Future work could explore detecting more granular levels of errors, as well as more sophisticated methods of filtering candidates  (Welleck et al., 2020) .  Figure 1 : 1 Figure 1: GOLD performs data augmentation by extracting utterances from a source dataset and merging those sentences with known OOS samples from the target dataset to generate pseudo-labeled OOS examples. 
