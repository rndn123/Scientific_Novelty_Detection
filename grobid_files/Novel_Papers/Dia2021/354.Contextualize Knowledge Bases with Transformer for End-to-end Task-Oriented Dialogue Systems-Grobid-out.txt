title
Contextualize Knowledge Bases with Transformer for End-to-end Task-Oriented Dialogue Systems

abstract
Incorporating knowledge bases (KB) into endto-end task-oriented dialogue systems is challenging, since it requires to properly represent the entity of KB, which is associated with its KB context and dialogue context. The existing works represent the entity with only perceiving a part of its KB context, which can lead to the less effective representation due to the information loss, and adversely favor KB reasoning and response generation. To tackle this issue, we explore to fully contextualize the entity representation by dynamically perceiving all the relevant entities and dialogue history. To achieve this, we propose a COntextaware Memory Enhanced Transformer framework (COMET), which treats the KB as a sequence and leverages a novel Memory Mask to enforce the entity to only focus on its relevant entities and dialogue history, while avoiding the distraction from the irrelevant entities. Through extensive experiments, we show that our COMET framework can achieve superior performance over the state of the arts.

Introduction Task-oriented dialogue systems aim to achieve specific goals such as hotel booking and restaurant reservation. The traditional pipelines  (Young et al., 2013; Wen et al., 2017)  consist of natural language understanding, dialogue management, and natural language generation modules. However, designing these modules often requires additional annotations such as dialogue states. To simplify this procedure, the end-to-end dialogue systems  are proposed to incorporate the KB (normally relational databases) into the learning framework, where the KB and dialogue history can be directly modeled for response generation, without the explicit dialogue state or dialogue action. System I'll send the route with no traffic on your screen , drive carefully ! Table  1 : An example in SMD dataset . The top is the entities in KB and the bottom is a twoturn dialogue between the user and system. An example of the end-to-end dialogue systems is shown in Tab. 1. When generating the second response about the "traffic info": (1) the targeted entity "no traffic" is associated with its same-row entities (KB context) like "Tome's house", "friend's house" and "6 miles". These entities can help with enriching the information of its representation and modeling the structure of KB. (2) Also, the entity is related to the dialogue history (dialogue context), which provides clues about the goal-related row (like "Tom's house" and "580 Van Ness Ave" in the first response). These clues can be leveraged to further enhance the corresponding representations and activate the targeted row, which benefits the retrieval of "no traffic". Therefore, how to fully contextualize the entity with its KB and dialogue contexts, is the key point of end-to-end dialogue systems  (Madotto et al., 2018; Qin et al., 2020) , where the full-context enhanced entity representation can make the reasoning over KB and the response generation much easier. However, the existing works can only contextualize the entity with perceiving parts of its KB context and ignoring the dialogue context: (1)  (Madotto et al., 2018; Qin et al., 2020)  rep-  This representation can only partially eliminate this issue at the row level. However, at the entity level, the entity can only perceive the information of itself, which is isolated with other KB and dialogue contexts. (3)  (Yang et al., 2020)  converts KB to a graph (cf. Fig.  1 (c)). However, they fails to answer what is the optimal graph structure for KB. That indicates their graph structure may need manual design 1 . Also, the dialogue context is not encoded into the entity representation, which can also lead to the suboptimal entity representation. To sum up, these existing methods can not fully contextualize the entity, which leads to vulnerable KB reasoning and response generation. In this work, we propose COntext-aware Memory Enhanced Transformer (COMET), which provides a unified solution to fully contextualize the entity with the awareness of both the KB and dialogue contexts (shown in Fig.  1(d) ). The key idea of COMET is that: a Memory-Masked En-coder is used to encode the entity sequence of KB, along with the information of dialogue history. The designed Memory Mask is utilized to ensure the entity can only interact with its same-row entities and the information in dialogue history, whereas the distractions from other rows are prohibited. More specifically, (1) for the KB context, we represent the entities in the same row as a sequence. Then, a Transformer Encoder  (Vaswani et al., 2017)  is leveraged to encode them, where the same-row entities can interact with each other. Furthermore, to retain the structure of KB and avoid the distractions from the entities in different rows, we design a Memory Mask (shown in Fig.  3 ) and incorporate it into the encoder, which only allows the interactions between the same-row entities. (2) For the dialogue context, we create a Summary Representation (Sum. Rep) to summarize the dialogue history, which is input into the encoder to interact with the entity representations (gray block in Fig.  2 ). We also utilize the Memory Mask to make the Sum. Rep overlook all of the entities for better entity representations, which will serve as the context-aware memory for further response generation. By doing so, we essentially extend the entity of KB to (N + 1)-tuple representation, where N is the number of entities in one row and "1" is for the Sum. Rep of the dialogue history. By leveraging the KB and dialogue contexts, our method can effectively model the information existing in KB and activate the goal-related entities, which benefits the entity retrieval and response generation. Please note that the function of fully contextualizing entity is unified by the designed Memory Mask scheme, which is the key of our work. We conduct extensive experiments on two public benchmarks, i.e., SMD  Madotto et al., 2018)  and Multi-WOZ 2.1  (Budzianowski et al., 2018; Yang et al., 2020) . The experimental results demonstrate significant performance gains over the state of the arts. It validates that contextualizing KB with Transformer benefits entity retrieval and response generation. In summary, our contributions are as follows: ? To the best of our knowledge, we are the first to fully contextualize the entity representation with both the KB and dialogue contexts, for end-to-end task-oriented dialogue systems.  

 Methodology In this section, we first introduce the general workflow for this task. Then, we elaborate on each part of COMET, i.e., the Dialogue History Encoder, Context-aware Memory Generation, and Response Generation Decoder (as depicted in Fig.  2 ). Finally, the objective function will be introduced. 

 General Workflow Given a dialogue history with k turns, which is denoted as H = {u 1 , s 1 , u 2 , s 2 , ..., u k } (u i and s i denote the i-th turn utterances between the user and the system), the goal of dialogue systems is to generate the k-th system response s k with an external KB B = {[b 11 , ..., b 1c ], ..., [b r1 , ..., b rc ]}, which has r rows and c columns. Formally, the procedure mentioned above is defined as: p(s k |H, B) = n i=1 p(s k,t |s k,1 , ..., s k,t?1 , H, B), where we first derive the dialogue history representation (Section 2.2) and generate the Context-aware Memory, a.k.a., contextualized entity representation (Section 2.3), where these two parts will be used to generate the response s k (Section 2.4). 

 Dialogue History Encoder We first transform H into the word-by-word form with a special token [SUM]: ? = {x 1 , x 2 , ..., x n }, x 1 = [SUM], which is used to globally aggregate information from H. Then, the sequence ? is encoded by a standard Transformer Encoder and generate the dialogue history representation H enc N , where H enc N,1 is denoted as the Summary Representation (Sum. Rep) of the dialogue history. 2 It will be used to make the memory aware of the dialogue context. 

 Context-aware Memory Generation In this subsection, we describe how to "fully contextualize KB". That is, the Memory Mask is leveraged to ensure the entities of KB with the awareness of all of its related entities and dialogue history, which is the key contribution of our method. 

 Memory Generation Different from existing works which fail to contextualize all the useful context information for the entity representation, we treat KB as a sequence, along with Sum. Rep. Then, a Transformer Encoder with the Memory Mask is utilized to model it, which can dynamically generate the entity representation with the awareness of its all favorable contexts, i.e., the same-row entities and dialogue history, while blocking the distraction from the irrelevant entities. The procedure of memory generation is as follows. Firstly, the entities in the KB B is flatten as a memory sequence, i.e., M = [b 11 , ..., b 1c , ..., b r1 , ..., b rc ] = [m 1 , m 2 , ..., m |M| ], where the memory entity m i means an entity of KB in the k-th row. By doing so, the Memory-Masked Transformer Encoder can interact the same-row entities with each other while retaining the structure information of KB.  3  Then, M will be transformed into the entity embeddings , i.e., E = [e m 1 , ..., e m |M| ] , where e m i corresponds to m i in M and it is the sum of the word embedding u i and the type embedding t i , i.e., e m i = u i + t i . Note that, the entity types are the corresponding column names, e.g., "poi_type" in Table  1 . For the entities which have more than one token, we simply treat them as one word, e.g., "Stanford Exp" ? "Stanford_Exp". Next, the entity embeddings are concatenated with the Sum. Rep from the Dialogue History Encoder, i.e. E 0 = [H enc N,1 ; E]. The purpose of intro- ducing H enc N,1 is that it passes the information from the dialogue history and further enhances the entity representation with the dialogue context. Finally, E 0 and the Memory Mask M mem are used as the input of the Transformer Encoder (tf _enc(?)) to generate the context-aware memory (a.k.a, contextualized entity representation): E l = tf _enc(E l?1 , M mem ), l ? [1, K], where K is the total number of Transformer Encoder layers. E K ? R (|M|+1)?dm is the generated memory, which is queried when generating the response for entity retrieval. 

 Memory Mask Construction To highlight, we design a special Memory Mask scheme to take ALL the contexts grounded by the entity into account, where the Memory Mask ensures that the entity can only attend to its context part, which is the key contribution of this work. This is in contrast to the standard Transformer Encoder, where each entity can attend to all of the other entities. The rationale of our design is that by doing so, we can avoid the noisy distraction of the non-context part. 3 When the memory sequence is long, some existing methods like the linear attention  (Kitaev et al., 2020)  can be used to tackle the issue of O(N 2 ) complexity of Self Attention. Formally, M mem ? R (|M|+1)?(  |M|+1 ) is defined as: M mem i,j = ? ? ? ? ? 1, if M i?1 , M j?1 ? b k , 1, if i or j = 1, ? ?, else. A detailed illustration of the Memory Mask construction is shown in Fig.  3 . With this designed Memory Mask, a masked attention mechanism is leveraged to make the entity only attend the entities within the same row and the Sum. Rep.  

 C1 

 Response Generation Decoder Given the dialogue history representation H enc N and generated memory E K , the decoder will use them to generate the response for a specific query. In COMET, we use a modified Transformer Decoder, which has two cross attention modules to model the information in H enc N and E K , respectively. Then, a gate mechanism is leveraged to adaptively fuse H enc N and E K for the decoder, where the response generation is tightly anchored by them. Following  Qin et al., 2020; Yang et al., 2020) , we first generate a sketch response that replaces the exact slot values with sketch tags.  4  Then, the decoder links the entities in the memory to their corresponding slots. 

 Sketch Response Generation For the k-th turn generating sketch response Y = [y 1 , ...y t?1 ], it is converted to the word representation H dec 0 = [w d 1 , ..., w d t?1 ]. w d i = v i + p i , where v i and p i means the word embedding and absolute position embedding of i-th token in Y. Afterward, N -stacked decoder layers are applied to decode the next token with the inputs of H dec 0 , E K and H enc N . The process in one decoder layer can be expressed as: H d?d l = M HA(H dec l?1 , H dec l?1 , H dec l?1 , M dec ), H d?e l = M HA(H d?d l , H enc N , H enc N ), H d?m l = M HA(H d?d l , E K , E K ), g = sigmoid(F C(H d?m l )), H agg l = g H d?e l + (1 ? g) H d?m l , H dec l = F F N (H agg l ), l ? [1, N ], where the input {Q, K, V, M } of the Multi-Head Attention M HA(Q, K, V, M ) means the query, key, value, and optional attention mask. F F N (?) means the Feed-Forward Networks. M dec is the decoder mask, so as to make the decoded word can only attend to the previous words. F C(?) is a fully-connected layer to generate the gating signals, which maps a d m -dimension feature to a scalar. N is the number of the total decoder layers. After obtaining the final H dec N , the posterior distribution for the t-th token, p v t ? R |V | (|V | denotes the vocabulary size), is calculated by: p v t = sof tmax(H dec N,t?1 W v + b v ). 

 Entity Linking After the sketch response generation, we replace the sketch tags with the entities in the contextaware memory. We denote the representation from the decoder at the t-th time step, i.e., the t-th token, as H dec N,t , and represent the time steps that need to replace sketch tags with entities as T . The probability distribution over all possible linked entities can then be calculated by p s t = sof tmax(H dec N,t E T K ), ?t ? T where E K means the final generated memory. 

 Objective Function For the training process of COMET, we use the the cross-entropy loss to supervise the response generation and entity linking 5 . Moreover, we propose an additional regularization term to further regularize p s t . The regularization is based on the prior knowledge that for a given response, only a small subset of entities should be linked. Formally, we construct the following entity linking probability matrix P s = [p s t 1 , p s t 2 , ..., p s t |T | ] and minimize its L 2,1 -norm  (Nie et al., 2010) : L r = |M| i=1 t?T (p s t,i ) 2 , where p s t,i denotes the i-th dimension of p s t . This regularization term can encourage the network to select a small subset of entities to generate the response. The same idea has been investigated in  (Nie et al., 2010)  for multi-class feature selection. Finally, COMET is trained by jointly minimizing the combination of the above three losses. 

 Experiments 

 Datasets Two public multi-turn task-oriented dialogue datasets are used to evaluate our model, i.e., SMD 6  and Multi-WOZ 2.1 7  (Budzianowski et al., 2018) . Note that, for Multi-WOZ 2.1, to accommodate end-to-end settings, we use the revised version released by  (Yang et al., 2020) , which equips the corresponding KB to every dialogue. We follow the same partition as  (Madotto et al., 2018)  on SMD and  (Yang et al., 2020)  on Multi-WOZ 2.1. 

 Experimental Settings The dimension of embeddings and hidden vectors are all set to 512. The number of layers (N ) in Dialogue History Encoder and Response Generation Decoder is set to 6. The number of layers for Context-aware Memory Generation (K) is set to 3. The number of heads in each part of COMET is set to 8. A greedy strategy is used without beam-search during decoding. The Adam optimizer (Kingma and Ba, 2014) is used to train our model from scratch with a learning rate of 1e ?4 . More details about the hyper-parameter settings can be found in Appendix A.2. 

 Baselines We compare COMET with the following methods: ? Mem2Seq (Triplet)  (Madotto et al., 2018) : Mem2Seq incorporates the multi-hop attention mechanism in memory networks into the pointer networks. KB-retriever improves the entity-consistency by first selecting the target row and then picking the relevant column in this row. ? GLMP (Triplet) : GLMP uses a global memory encoder and a local memory decoder to incorporate the external knowledge into the learning framework. ? DF-Net (Triplet)  (Qin et al., 2020) : DF-Net applies a dynamic fusion mechanism to transfer knowledge in different domains. ? GraphDialog (Graph)  (Yang et al., 2020) : GraphDialog exploits the graph structural information in KB and in the dependency parsing tree of the dialogue. 

 Results Following the existing works  (Qin et al., 2020; Yang et al., 2020) , we use the BLEU and Entity F1 metrics to evaluate model performance. The results are shown in Tab. 2. It is observed that: COMET achieves the best performance over both datasets, which indicates that our COMET framework can better leverage the information in the dialogue history and external KB, to generate more fluent responses with more accurate linked entities. Specifically, for the BLEU score, it outperforms the previous methods by 2.9% on the SMD dataset and 2.1% on the Multi-WOZ 2.1 dataset, at least. Also, COMET achieves the highest Entity F1 score on both datasets. That is, the improvements of 0.9% and 7.3% are attained on the SMD and Multi-WOZ 2.1 datasets, respectively. In each domain of the two datasets, improvement or competitive performance can be clearly observed. The results indicate the superior of our COMET framework. To highlight,  KB-Transformer (E. et al., 2019 ) also leverages Transformer, but our COMET outperforms it by a large margin. On the SMD dataset, the BLEU score of COMET is higher than that of KB-Transformer by 3.4%. The improvement introduced by COMET on Entity F1 score is as significant as 26.5%. This shows naively introducing Transformer to the end-to-end dialogue system will not necessarily lead to higher performance. A careful design of the whole dialogue system, such as our proposed one, plays a vital role. 

 Ablation Study In this subsection, we first investigate the effects of the different components, i.e., the Memory Mask, Sum. Rep, gate mechanism, and L 2,1 -norm regularization (Tab. 3). Then, we design careful experiments to further demonstrate the effect of the Memory Mask, which is the key contribution of this work: (1) we replace the context-aware memory of COMET with the existing three representations of KB, (i.e., triplet, row-entity, and graph) to show the superior of the fully contextualized entity (Tab. 4). (  2 ) We also replace our Memory Mask with the full attention layer by layer, which further shows the importance of our Memory Mask (Tab. 5). Our ablation studies are based on the SMD dataset. The effects of the key components in the COMET framework are reported in Tab. 3. As observed, removing any key component of the COMET, both the BLEU and Entity F1 metrics degrade to some extend. More specifically: (1) If the Memory Mask is removed, the Entity F1 score drops to 49.6. This significant discrepancy demonstrates the importance of restricting self-attention as our designed Memory Mask did. (2) For the variant without the Sum. Rep, the Entity F1 score drops to 61.4. That indicates the effectiveness of contextualizing the KB with the dialogue history, which can further boost the performance. (3) We also remove the gate and only use the information from the dialogue history (H enc N ) or memory (E K ). We can see that the former case can only achieve 61.1 while the latter case achieves 61.4 of the Entity F1 score. It is obvious that using the gate mechanism to fuse both information sources is helpful for the entity linking. (4) When removing the L 2,1norm, the performance also drops to 62.3, which means regularizing the entity-linking distribution can further benefit the performance. We also replace our context-aware memory with other ways of representing KB, while other parts of our framework keep unchanged 8 . The result is reported in Tab. 4. It is observed that, After replac-  8  The implementation details are in Appendix A.3. ing our context-aware memory with the existing three representations of KB, the performance drops a lot in all the metrics, where the BLEU score drops 2.4% and the Entity F1 score drops 3.8% at least. Besides, the result of the variant which only considers the KB context part (i.e., w/o Sum. Rep), is also reported, so as to further fairly compare with the aforementioned KB representations. The result shows that only considering the KB context, our method can still outperform other KB representations by 1.6% of Entity F1 at least. That further indicates the fully contextualizing entity with its relevant entity and the dialogue history, can better represent the KB for dialogue systems. We also conduct the experiment which replaces the Memory Mask with the full attention, layer by layer. That is, the first (n-k) layers use the proposed Memory Mask (M) and the last k layers use the full attention (F). As shown in Tab. 5, the more full attention is added, the more performance of COMET drops in all of the metrics since the full attention introduces too much distraction from other rows. The result further indicates that the Memory Mask is indeed a better choice which takes the inductive bias of KB into account. Note that we also explore other Memory Mask schemes, but these schemes can not further boost the performance, where the results are omitted due to the page limitation. For further improvement, more advanced techniques like Pre-trained Model  (Devlin et al., 2018; Radford et al., 2019 ) may be needed to deeply understand the dialogue and KB context, which we leave for future work. 

 Case Study To demonstrate the superiority of our method, several examples on the SMD test set, which are generated by our COMET and the existing state of the arts GLMP  and DF-Net  (Qin et al., 2020) , are given in Tab. 6. As reported, compared with GLMP and DF-Net, COMET can generate   and DF-Net  (Qin et al., 2020)  from the SMD dataset. Goal means the row that the user is queried. and ? mean the right or wrong entity linked. more fluent, informative, and accurate responses. Specifically, in the first example, GLMP and DF-NET are lack of the necessary information "11am" or provide the wrong entity "5pm". But COMET can obtain all the correct entities, which is more informative. In the second example, our method can generated the response with the right "distance" information but GLMP and DF-Net can not. In the third example, GLMP and DF-Net can not even generate a fluent response, let alone the correct temperature information. But COMET can still perform well. The fourth example is more interesting: the user queries the information about "starbucks" which does not exist in the current KB. GLMP and DF-Net both fail to faithfully respond, whereas COMET can better reason KB to generate the right response and even provide an alternative option. 

 Related Work Task-oriented dialogue systems can be mainly categorized into two parts: modularized (Williams and  Young, 2007; Wen et al., 2017)  and end-toend . For the end-to-end task-oriented dialogue systems,  first explores the end-to-end method for the task-oriented dialogue systems. However, it can only link to the entities in the dialogue context and no KB is incorporated. To effectively incorporate the external KB,  proposes a keyvalue retrieval mechanism to sustain the grounded multi-domain discourse.  (Madotto et al., 2018)  augments the dialogue systems with end-to-end memory networks  (Sukhbaatar et al., 2015) . ) models a dialogue state as a fixed-size distributed representation and uses this representation to query KB.  (Lei et al., 2018)  designs belief spans to track dialogue believes, allowing task-oriented dialogue systems to be modeled in a sequence-tosequence way.  (Gangi Reddy et al., 2019)  proposes a multi-level memory to better leverage the external KB.  proposes a global-to-local memory pointer network to reduce the noise caused by KB.  (Lin et al., 2019)  proposes Heterogeneous Memory Networks to handle the heterogeneous information from different sources.  (Qin et al., 2020)  proposes a dynamic fusion mechanism to transfer the knowledge among different domains.  (Yang et al., 2020)  exploits the graph structural informa-tion in KB and the dialogue. Other works also explore how to combine the Pre-trained Model  (Devlin et al., 2018; Radford et al., 2019)  with the endto-end task-oriented dialogue systems.  (Madotto et al., 2020a)  directly embeds the KB into the parameters of GPT-2  (Radford et al., 2019)  via finetuning.  (Madotto et al., 2020b)  proposes a dialogue model that is built with a fixed pre-trained conversational model and multiple trainable light-weight adapters. We also notice that some existing works also combine Transformer with the memory component, e.g.,  (Ma et al., 2021) . However, our method is distinguishable from them, since the existing works like  (Ma et al., 2021)  simply inject the memory component into Transformer. In contrast, inspired by the dynamic generation mechanism  (Gou et al., 2020) , the memory in COMET (i.e., the entity representation) is dynamically generated by fully contextualizing the KB and dialogue context via the Memory-masked Transformer. 

 Conclusion In this work, we propose a novel COntext-aware Memory Enhanced Transformer (COMET) for the end-to-end task-oriented dialogue systems. By the designed Memory Mask scheme, COMET can fully contextualize the entity with all its KB and dialogue contexts, and generate the (N + 1)-tuple representations of the entities. The generated entity representations can further augment the framework and lead to better capabilities of response generation and entity linking. The extensive experiments demonstrate the effectiveness of our method. 
