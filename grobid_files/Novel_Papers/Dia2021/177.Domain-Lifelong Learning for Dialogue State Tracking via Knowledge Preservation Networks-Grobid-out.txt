title
Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks

abstract
Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domainlifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.

Introduction Task-oriented dialogue systems aim at helping users to accomplish various tasks, such as reserving restaurants, booking flights, and checking weather  (Young et al., 2013; Lei et al., 2018; . Dialogue state tracking (DST) is an essential component of task-oriented dialogue systems, which estimates user goals for downstream modules  (Bohus and Rudnicky, 2006; , dialogue history, a DST model should be able to output an accurate dialogue state. In general, the dialogue state is represented as a set of slot-value pairs, such as ((restaurant-area, north), (restaurantprice, expensive)). Previous DST models are usually trained offline, which requires a fixed dataset prepared in advance. These offline solutions are often impractical in realworld applications, as online dialogue systems usually involve continually emerging new data and domains, especially when new services are introduced. In addition, it is infeasible to retrain DST models from scratch every time new domain data arrives due to computational costs, storage budgets, and data privacy  (McMahan et al., 2017) . To tackle this realistic issue, we explore Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST). As shown in Figure  1 , the DLL-DST task aims to continually train a DST model on new data to learn incessantly emerging new domains. At each step, new data generally contains one or multiple new domains, and the updated model should be able to make accurate predictions for all the domains observed so far. A plain approach to domain-lifelong learning is to simply fine-tune a pre-trained model on new data. However, this approach suffers from the problem of catastrophic forgetting  (McCloskey and Cohen, 1989; French, 1999) . To be more specific, fine-User 1: I want an upscale restaurant in the northern part. 

 State: restaurant-price=expensive, restaurant-area=north User 2: Hello, I want an expensive restaurant in the north. User 3: Is there a fine dining restaurant in the north? tuning the model on new data usually results in a significant performance drop on old data. To address this problem, there are two mainstream lifelong learning methods: (1) regularization-based methods, which try to identify and preserve the parameters important to old data  (Kirkpatrick et al., 2017; Aljundi et al., 2018) ; (2) replay-based methods, which reserve some representative old samples and combine them with new data to re-train the model  (Rebuffi et al., 2017; Hou et al., 2019) . Recently, replay-based methods have shown promising results in alleviating catastrophic forgetting of class-lifelong learning tasks in NLP scenarios  (Han et al., 2020; Cao et al., 2020) . However, when deploying previous replay-based methods on the DLL-DST task, we find two main problems: expression diversity and combinatorial explosion. Expression diversity: In the DST task, dialogue texts usually contain a variety of expressions for each dialogue state, as shown in Figure  2 . The expression diversity makes it difficult for previous replay-based methods to select the most representative old samples. The unrepresentative samples, such as the first utterance in Figure  2 , do not contain typical expressions for any slot. Retraining models with these unrepresentative samples is not conducive to retaining the performance on old domains. Combinatorial explosion: Ideally, we should reserve at least one sample for each dialogue state in old domains. However, the classes of dialogue states explode rapidly as the number of slot-value pairs increases. For example, the Mul-tiWOZ 2.1 dataset  (Eric et al., 2019)  has an average of 2732 classes of dialogue states per domain. In the DLL-DST task, it is infeasible for replaybased methods to reserve samples for each class of dialogue states in old domains due to limitations of memory capacity and training time. Since the reserved samples involve only a few types of dialogue states, previous methods may gradually forget previous knowledge, leading to catastrophic forgetting. To address the above two problems, we propose Knowledge Preservation Networks (KPN), which contain two main components: (1) to handle expression diversity, we propose multi-prototype enhanced retrospection, which computes multiple slot prototypes for each domain and selects the most representative old samples based on these slot prototypes; (2) to cope with the combinatorial explosion problem, we propose multi-strategy knowledge distillation, which enables the model at the current step to preserve the knowledge of the model trained in the last step from multiple aspects, instead of just reserving some old samples. Experimental results demonstrate that KPN outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively. The contributions of this paper are listed as follows: ? To the best of our knowledge, we are the first to formally introduce domain-lifelong learning into dialogue state tracking and we construct two benchmarks through two widely used DST datasets, MultiWOZ 2.1 and SGD. ? We propose Knowledge Preservation Networks, which handle expression diversity and combinatorial explosion in the DLL-DST task via multi-prototype enhanced retrospection and multi-strategy knowledge distillation. ? Experimental results show that our method outperforms previous lifelong learning methods and achieves state-of-the-art performance. We will release the source code and the benchmarks for further research (https://gi thub.com/liuqingbin/Knowledge-Preservation-Networks). 

 Task Formulation The DST task is usually formulated as a slot-filling task  (Bohus and Rudnicky, 2006; . At each dialogue turn, the DST model takes the user utterance and the dialogue history as input and predicts values for each slot. As shown in Figure  2 , the DST model is expected to fill the slot "restaurant-price" with the value "expensive". The DLL-DST task continually trains DST models on emerging data to learn new domains. New data arrives in a stream form ({D 1 , D 2 , ..., D N }). At each step, the new data (D i ) can contain one or multiple new domains. In addition, inspired by other lifelong learning work  (Lopez-Paz and Ranzato, 2017; Zenke et al., 2017)   across the same multiple domains as data of a special domain, since these cross-domain dialogues usually contain specific expressions that distinguish them from other dialogues, such as domain transformation and slot reference  (Ouyang et al., 2020; Hu et al., 2020)  ). The updated model should still perform well on all previous domains. Therefore, in the testing stage of the ith step, we evaluate the updated model on the test data of all observed domains (i.e., i k=1 D test k ). The evaluation protocol indicates that it will be more and more difficult to achieve high performance for DST models as more and more domains arrive. 

 Method In this work, we propose Knowledge Preservation Networks (KPN) to handle the DLL-DST task. KPN consists of two core components, i.e., multi-prototype enhanced retrospection and multistrategy knowledge distillation, for dealing with the two main challenges, i.e., expression diversity and combinatorial explosion. The framework of KPN is shown in Figure  3 . 

 Background Our method, KPN, is a lifelong learning framework, which is model-agnostic. The DST model is only a basic component, not our research focus. DST models, such as TRADE  (Wu et al., 2019) , SAS  (Hu et al., 2020) , and SOM-DST  (Kim et al., 2020) , can all be used as this basic component. We adopt the previous best model, SOM-DST, in this work. In each dialogue turn, SOM-DST simplifies the dialogue history to the last system response and the last dialogue state, and then combines them with the current user utterance into an input sequence for the BERT encoder  (Devlin et al., 2019) . BERT is a multi-layer Transformer  (Vaswani et al., 2017) , pre-trained on large-scale unlabeled corpora. To fit the input form of BERT, the tokens [CLS] and  [SEP]  are placed in the input sequence. In addition, a special token [SLOT] is placed at the beginning of each slot in the last dialogue state. The BERT encoder obtains the contextual representation for the input sequence. The encoded hidden state of [SLOT] is used as the feature vector of each slot. For each slot, SOM-DST first classifies it into four categories, including "dontcare", "carryover", "update", and "delete". "dontcare" means that the user does not care about this slot. "carryover" indicates that the slot inherits the value of the same slot from the last dialogue state. "update" means that the model needs to generate a value for the slot. "delete" means that this slot does not contain any value. A softmax classifier is added to the feature vector of each slot to predict its category. The cross-entropy loss is used to train the classifier: L c = ? 1 |N | x?N s?C y s log(p s ) (1) where y s is the one-hot label for the slot s and p s is the predicted probability. N is the training samples and C is the slots of all observed domains. For each slot belonging to the "update" category, SOM-DST generates a value for this slot via the GRU decoder  (Cho et al., 2014) . The decoder is equipped with the ability to copy words from the input sequence  (Kim et al., 2020) . The cross-entropy loss is used to train the generation probability: L g = ? 1 |N | x?N s?U i?d y v i log(p v (v i |v <i )) (2) where p v (v i |v <i ) is the predicted probability of the i-th word of the value v. y v i is the one-hot label. d is the length of the value. U is the slots that are predicted to be the "update" category. 

 Multi-Prototype Enhanced Retrospection In this paper, we focus on the domain-lifelong learning scenario for DST. Given a model trained on old data, we aim to continually learn a unified DST model for all observed domains so far based on a new combined dataset N = D train i P. D train i is the training data of the new domains at step i. P is a bounded memory that stores representative old samples, denoted as P = {P 1 , P 2 , ..., P m }. P i is the set of stored samples for the i-th domain. m is the number of old domains. Since the DLL-DST task suffers from expression diversity, we propose multi-prototype enhanced retrospection to reserve the most representative samples of old domains. In this way, important information about the data distribution of all previous domains enters the subsequent training process. This approach is inspired by prototype learning  (Snell et al., 2017; Yang et al., 2018) , which uses prototypes as representative points. Specifically, after learning on the new domains, we store |P i | = B/m samples for each new domain. m is the number of all observed domains and B is the total number of samples that can be reserved. We encode all samples of the i-th domain into the hidden representation and compute a prototype ? s for each slot s in this domain: ? s = 1 |N | x?N f s (x) (3) where N is the training samples of the i-th domain. f s (x) is the hidden state of [SLOT] in front of the slot s, which is the slot representation. Then, we compute the distance between the slot representation of each training sample and the corresponding slot prototype. Based on the average distance of all slots, we produce a sorted list of new training samples. Intuitively, the closer the samples to these slot prototypes, the more representative the samples will be for these slots. Based on the sorted list of samples, the top B/m samples are selected as exemplars to be stored in the memory. Since the storage size of memory is constant, when new domains arrive, the memory needs to remove some reserved exemplars of old domains to allocate space for the exemplars of new domains. Suppose the number of new domains is t. The memory needs to remove B/(m?t)?B/m stored samples of each old domain. For each old domain, we remove the samples that are far from these prototypes according to the sorted list. 

 Multi-Strategy Knowledge Distillation As mentioned above, just reserving some old samples makes previous lifelong learning methods still suffer from combinatorial explosion. Since the reserved samples involve only a few types of dialogue states, these methods may gradually forget the previous knowledge. To handle this problem, we propose multi-strategy knowledge distillation, which preserves the knowledge of the model trained in the last step through multiple distillation strategies. In this way, the current model can perform well on the old domains. Knowledge distillation is an effective way to transfer knowledge from one network to another  (Hinton et al., 2015) . 

 Encoder Feature Distillation For each slot, we denote its feature vector extracted by the BERT encoder of the current model and the BERT encoder of the last model as f s (x) and f s, * (x), respectively. To preserve the feature distribution in the original encoder, we adopt an encoder feature distillation loss: L ef = 1 |N | x?N s?C 1 ? f s (x), f s, * (x) (4) where f s (x), f s, * (x) measures the cosine similarity between the two features. This loss is computed for all samples from the new domains and the reserved exemplars. If the features of the current encoder don't greatly deviate from those of the last encoder, the current model can effectively preserve the knowledge of the model trained in the last step. 

 Classifier Prediction Distillation In addition, we also adopt a classifier prediction distillation, which preserves the previous knowledge by encouraging the predictions of the current classifier to match the predictions of the last classifier. For each slot, the classification logits (i.  L cp = ? 1 |N | x?N s?C ? i=1 ? * i log(? i ) ? * i = e o * i /T ? j=1 e o * j /T , ? i = e o i /T ? j=1 e o j /T (5) where T is the temperature scalar. T is usually greater than 1 to increase the weights of small probability values. The classifier prediction distillation loss is also computed for the training samples of the new domains and the reserved exemplars of the old domains. 

 Decoder Feature Distillation To retain the previous knowledge of the last decoder, we adopt a decoder feature distillation loss to learn the feature distribution of the last decoder. L df = 1 |N | x?N s?U i?d 1 ? g i (x), g * i (x) (6) where g i (x) and g * i (x) are the i-th hidden state decoded by the current decoder and the last decoder for the slot s. 

 Generation Prediction Distillation We adopt another prediction distillation loss L gp to mimic the generation probability predicted by the last decoder. Because the sigmoid function in the decoder  (Kim et al., 2020)  makes it impossible to adopt the above prediction distillation loss, we use the KL-divergence as the generation prediction distillation loss as follows: L gp = 1 |N | x?N s?U i?d q i log( q i p i ) (7) where p i and q i are the i-th probability predicted by the current and last decoder for the slot s. 

 Training During each step of the domain-lifelong learning process, we combine the above losses to train the DST model: L = L c +L g +?(L ef +L df )+?(L cp +L gp ) (8) where ? and ? are two adjustment coefficients. The coefficients are used to balance the performance of the old domains and the new domains. If ? and ? are very small, the model will pay more attention to the new domains, thus hurting the performance of the old domains. At each step, we combine the training set (D train i P) to train the model with the loss L, and then select the most representative samples to update the memory. Therefore, our method can continually learn new domains while avoiding catastrophically forgetting old domains. 

 Experiments 

 DLL-DST Benchmarks To the best of our knowledge, we are the first to formally introduce the DLL-DST task. Therefore, we construct two benchmarks based on the following method: for a given DST dataset, we arrange its domains in a fixed random order. Each domain has its own data and ontology (i.e., slot-value pairs). In a domain incremental manner, the lifelong learning methods continually train a DST model on one or multiple new domains. Following other tasks  (Li and Hoiem, 2017; Cao et al., 2020) , we adopt one new domain at each step. As described in Section 2, inspired by other tasks, we treat dialogues across the same multiple domains as data of a special new domain, since they usually contain many specific expressions. Based on two widely used datasets, MultiWOZ 2.1  (Eric et al., 2019)  and SGD  (Rastogi et al., 2019) , we propose two instantiations of the above construction method. MultiWOZ benchmark: We use the data splitting of the official Multi-WOZ 2.1 dataset. Since the domains in MultiWOZ 2.1 has a long-tail frequency distribution, we use the data of the top 10 most frequent domains (including the combined domains). SGD benchmark: Same as the MultiWOZ benchmark, we use the data of the top 15 most frequent domains. Table  1  shows the statistics of the two benchmarks. 

 Experimental Settings For the DST task, joint goal accuracy (JGA) is used as the evaluation metric  (Zhong et al., 2018) . For the DLL-DST task, every time the model finishes training on new domains, we report JGA on the test data of all observed domains. For example, after the i-th step, the result is denoted as JGA i . In addition, after the last step, we report Aver-  1 2 3 4 5 6 7 8 9    age JGA which is the average score of all steps ( 1 k k i=1 JGA i ), and Whole JGA which is the JGA score on the whole testing data of all domains. Our method uses the HuggingFace's Transformers library 1 to implement the BERT-based DST model. The learning rate is set to 5e ? 5. The batch size is 4. The hyper-parameters ? and ? are 0.2 and 0.1, respectively. T = 2 in our experiments. The capacity of memory is 50. All hyper-parameters are obtained by a grid search on the validation set. 

 Baselines In this work, we propose a model-agnostic lifelong learning method to handle the DLL-DST task. Therefore, we adopt other model-agnostic lifelong learning methods that achieve state-of-the-art performance on other tasks as our baselines: EWC  (Kirkpatrick et al., 2017) , which slows down the update of important parameters by adding a L 2 regularization of parameter changes. LwF  (Li and Hoiem, 2017) , which matches the prediction of the current network with that of the original network by knowledge distillation. EMR , which alleviates forgetting by randomly storing some old samples. AdapterCL  (Madotto et al., 2020) , which adds the model parameters to learn new data. EMAR  (Han et al., 2020) , which selects representative samples based on only one prototype and consolidates the model through the prototype. FineTune, which simply fine-tunes the pre-trained model on new data. UpperBound, which uses training samples from all observed domains to train the model. We regard it as the upper bound of the benchmark. and whole JGA (%) on the whole testing data ("Whole" column) after the last step. 

 Main Results Figure  4  shows the JGA scores over the observed domains during the whole lifelong learning process. We also list the results after the last step in Table  2 . From the results, we can observe that: (1) Our proposed method KPN significantly outperforms other baselines and achieves state-of-theart performance in both the MultiWOZ and SGD benchmarks. For example, compared to EMAR, our method achieves 4.25% and 8.27% improvements of the whole joint goal accuracy on the Multi-WOZ benchmark and the SGD benchmark, respectively. It verifies the effectiveness of our method on the DLL-DST task. (2) At each step of the domain-lifelong learning process, there is a performance gap between EMAR and our method KPN. The reason is that EMAR ignores the problems of expression diversity and combinatorial explosion in the DLL-DST task. Therefore, EMAR fails to reserve the most representative samples and tends to gradually forget the previous knowledge of the original model, eventually resulting in catastrophic forgetting. The architecture-based method, AdapterCL, greatly increases the computation time due to the need to select the parameters to be executed. Besides, because AdapterCL only trains domain-specific parameters, it has weak representation capabilities for each domain and achieves low performance. (3) FineTune always achieves the worst results on both benchmarks, which confirms that catastrophic forgetting is indeed a major difficulty in the DLL-DST task. In addition, there is still a gap between our method and the upper bound. It indicates that, although we have proposed an effective approach for the DLL-DST task, there remain numerous challenges to be addressed. 

 Ablation Experiment In this work, we propose a model-agnostic domainlifelong learning method, KPN, which consists of two core components: multi-prototype enhanced retrospection and multi-strategy knowledge distillation. In this section, we show ablation studies of the two components. 

 Effectiveness of Multi-Prototype Enhanced Retrospection We conduct experiments to verify the effectiveness of the proposed multi-prototype enhanced retrospection. The results are shown in Table  3 . From the results, we can see that: (1) For "-MPR", we remove multi-prototype enhanced retrospection and randomly select samples. Our method KPN outperforms this variant by 1.51% and 3.6% in terms of the whole JGA. The results show that the multi-prototype enhanced retrospection is effective in selecting the most representative samples from diverse dialogues. (2) In addition, we compare our method with previous data selection methods. For "+ iCaRL"  (Rebuffi et al., 2017) , the model computes only one prototype for each domain based on the hidden state of the [CLS] token and selects samples based on this prototype. For "+ K-Means"  (Han et al., 2020) , this model selects diverse samples by choosing the central samples of clusters in the [CLS] hidden vector space. KPN significantly outperforms "+ iCaRL" and "+ K-Means". "+ iCaRL" is even worse than the random selection "-MPR" because the [CLS] prototype is often not representative for any slot. By contrast, our method adopts multiple prototypes based on the slot representation, which effectively selects the most representative samples. 

 Effectiveness of Multi-Strategy Knowledge Distillation To gain more insights into the multi-strategy knowledge distillation, we test many variants of KPN. The results are shown in Table  4 . We can see that: (1) Removing any knowledge distillation strategy, encoder feature distillation (EFD), classifier prediction distillation (CPD), decoder feature distillation (DFD), or generation prediction distillation (GPD), brings performance degradation. If we remove all knowledge distillation strategies (MSKD), the performance further declines. It shows that these knowledge distillation strategies can retain performance on old domains by effectively preserving the knowledge of the original model from multiple perspectives. (2) When we remove both multi-prototype enhanced retrospection and multi-strategy knowledge distillation (i.e., the model EMR), the performance drops significantly. It indicates that simultaneously exploiting the two components is very effective. Table  5 : Effect of the number of reserved samples. We compare our method KPN with EMAR on the Multi-WOZ benchmark. 

 Discussion: Memory Capacity As shown in Table  5 , we test the models that reserve different numbers of samples. Both EMAR and our method KPN achieve performance improvements as the number of reserved samples increases. In each case, our method significantly outperforms EMAR. Our method using only 30 samples achieves comparable performance to EMAR using 50 samples. It demonstrates the effectiveness of our proposed method. The proposed multiprototype enhanced retrospection effectively selects the most representative samples. The proposed multi-strategy knowledge distillation alleviates the impact of combinatorial explosion. 5 Related Work   (Bohus and Rudnicky, 2006; Henderson et al., 2014a) . These methods mainly focus on modeling the relation between slots and dialogue history, such as NBT  (Mrk?i? et al., 2017) , GLAD  (Zhong et al., 2018) , SST , and CHAN  (Shan et al., 2020) . Generative DST methods treat dialogue state tracking as a generation task  (Rastogi et al., 2017; Xu and Hu, 2018; Wu et al., 2019) . By generating values from the dialogue history and the vocabulary, generative DST methods handle unknown values that are not predefined in the ontology. Therefore, generative DST methods dominate this research, such as SpanPtr  (Xu and Hu, 2018) , COMER  (Ren et al., 2019) , BERT-DST  (Chao and Lane, 2019) , TRADE  (Wu et al., 2019) , SAS  (Hu et al., 2020) , and SOM-DST  (Kim et al., 2020) . Despite the great progress in single-domain or multi-domain DST tasks, previous DST methods usually assume the training data is fixed, containing predefined domains. They can not learn newly emerging domains online, which makes it impractical in real-world applications. Our method handles the domain-lifelong learning problem, where data of new domains continually arrives, whether it is new single-domain or new multi-domain data. 

 Lifelong Learning Lifelong learning, also called continual learning, is a long-standing research topic in machine learning, which enables models to perform online learning on new data  (Cauwenberghs and Poggio, 2000; Kuzborskij et al., 2013) . Architecture-based methods dynamically extend the model architecture to learn new data  (Fernando et al., 2017; Shen et al., 2019) . However, the model size grows rapidly with the increase of new data, which limits the application of architecture-based methods. Existing lifelong learning methods can be divided into two main categories: regularization-based methods  (Zenke et al., 2017; Aljundi et al., 2018)  and replay-based methods  (Rebuffi et al., 2017; Hou et al., 2019) . Regularization-based methods design reasonable metrics to identify the parameters important to old data and slow down the update of them  (Kirkpatrick et al., 2017; Li and Hoiem, 2017) . Replay-based methods retain the previous knowledge by storing a small amount of old data  Han et al., 2020) . In addition, generative replay-based methods generate old data to alleviate catastrophic forgetting  (Shin et al., 2017; Kemker and Kanan, 2018; Ostapenko et al., 2019; Zhai et al., 2019) . Although lifelong learning has been widely investigated in NLP and CV scenarios  (Kou et al., 2020; Kundu et al., 2020) , its exploration in DST is relatively rare. In other dialogue tasks, Lee (2017) fine-tunes a dialogue model trained on open-domain dialogues to learn task-oriented dialogues. However, their setting is only a one-step incremental process.  Shen et al. (2019)  continually train a slot-filling model on new data from the same domain.  Madotto et al. (2020)  introduce continual learning into multiple dialogue tasks. However, they ignore cross-domain dialogues that exist widely in the real world. In addition, they only adopt a plain architecture-based method, which does not address the main challenges of the dialogue tasks. In contrast to previous work, we formally introduce domain-lifelong learning into DST, which is practical in real-world applications. In addition, we propose Knowledge Preservation Networks to handle the main challenges of the DLL-DST task. 

 Conclusion In this paper, we introduce domain-lifelong learning into dialogue state tracking and propose Knowledge Preservation Networks to overcome catastrophic forgetting. To handle expression diversity, we propose multi-prototype enhanced retrospection to reserve the most representative samples. Moreover, to alleviate the adverse effects of combinatorial explosion, we propose multi-strategy knowledge distillation to learn the previous knowledge of the original model. Experimental results on the MultiWOZ and SGD benchmarks demonstrate the effectiveness of our model. Figure 1 : 1 Figure 1: An example of domain-lifelong learning for DST. Italicized words denote domains. The DST model requires lifelong learning of new domains. 
