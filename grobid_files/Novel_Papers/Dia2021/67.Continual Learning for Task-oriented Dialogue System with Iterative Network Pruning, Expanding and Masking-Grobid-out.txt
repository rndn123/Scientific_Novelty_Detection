title
Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking

abstract
This ability to learn consecutive tasks without forgetting how to perform previously trained problems is essential for developing an online dialogue system. This paper proposes an effective continual learning for the task-oriented dialogue system with iterative network pruning, expanding and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Specifically, TPEM (i) leverages network pruning to keep the knowledge for old tasks, (ii) adopts network expanding to create free weights for new tasks, and (iii) introduces task-specific network masking to alleviate the negative impact of fixed weights of old tasks on new tasks. We conduct extensive experiments on seven different tasks from three benchmark datasets and show empirically that TPEM leads to significantly improved results over the strong competitors. For reproducibility, we submit the code and data at: https://github.com/siat-nlp/TPEM.

Introduction Building a human-like task-oriented dialogue system is a long-term goal of AI. Great endeavors have been made in designing end-to-end taskoriented dialogue systems (TDSs) with sequenceto-sequence (Seq2Seq) models  (Eric and Manning, 2017; Madotto et al., 2018; Gangi Reddy et al., 2019; Qin et al., 2020; Mi et al., 2019; Wang et al., 2020; Qin et al., 2021) , which have taken the state-of-the-art of TDSs to a new level. Generally, Seq2Seq models leverage an encoder to create a vector representation of dialogue history and KB information, and then pass this representation into a decoder so as to output a response word by word. For example, GLMP  (Wu et al., 2019)  is a representative end-to-end TDS, which incorporates KB information into Seq2Seq model by using a global memory pointer to filter irrelevant KB knowledge and a local memory pointer to instantiate entity slots. Despite the remarkable progress of previous works, the current dominant paradigm for TDS is to learn a Seq2Seq model on a given dataset specifically for a particular purpose, which is referred to as isolated learning. Such learning paradigm is theoretically of limited success in accumulating the knowledge it has learned before. When a stream of domains or functionalities are joined to be trained sequentially, isolated learning faces catastrophic forgetting  (McCloskey and Cohen, 1989; Yuan et al., 2020 . In contrast, humans retain and accumulate knowledge throughout their lives so that they become more efficient and versatile facing new tasks in future learning  (Thrun, 1998) . If one desires to create a human-like dialogue system, imitating such a lifelong learning skill is quite necessary. This paper is motivated by the fact that a cognitive AI has continual learning ability by nature to develop a task-oriented dialogue agent that can accumulate knowledge learned in the past and use it seamlessly in new domains or functionalities. Continual learning  (Parisi et al., 2019; Yuan et al., 2020  is hardly a new idea for machine learning, but remains as a non-trivial step for building empirically successful AI systems. It is essentially the case for creating a high-quality TDS. On the one hand, a dialogue system is expected to reuse previously acquired knowledge, but focusing too much on stability may hinder a TDS from quickly adapting to a new task. On the other hand, when a TDS pays too much attention to plasticity, it may quickly forget previously-acquired abilities . In this paper, we propose a continual learning method for task-oriented dialogue system with iterative network pruning, expanding and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on the future tasks. Concretely, TPEM adopts the global-to-local memory pointer networks (GLMP)  (Wu et al., 2019)  as the base model due to its powerful performance in literature and easiness for implementation. We leverage iterative pruning to keep old tasks weights and thereby avoid forgetting. Meanwhile, a network expanding strategy is devised to gradually create free weights for new tasks. Finally, we introduce a task-specific binary matrix to mask some old task weights that may hinder the learning of new tasks. It is noteworthy that TPEM is model-agnostic since the pruning, expanding and binary masking mechanisms merely work on weight parameters (weight matrices) of GLMP. We conduct extensive experiments on seven different domains from three benchmark TDS datasets. Experimental results demonstrate that our TPEM method significantly outperforms strong baselines for task-oriented dialogue generation in continual learning scenario. 

 Our Methodology 

 Task Definition Given the dialogue history X and KB tuples B, TDS aims to generate the next system response Y word by word. Suppose a lifelong TDS model that can handle domains 1 to k has been built, denoted as M 1:k . The goal of TDS in continual learning scenario is to train a model M 1:k+1 that can generate responses of the k + 1-th domain without forgetting how to generate responses of previous k domains. We use the terms "domain" and "task" interchangeably, because each of our tasks is from a different dialogue domain. 

 Overview In this paper, we adopt the global-to-local memory pointer networks (GLMP)  (Wu et al., 2019)  as base model, which has shown powerful performance in TDS. We propose a continual learning method for TDS with iterative pruning, expanding, and masking. In particular, we leverage pruning to keep the knowledge for old tasks. Then, we adopt network expanding to create free weights for new tasks. Finally, a task-specific binary mask is adopted to mask part of old task weights, which may hinder the learning of new tasks. The proposed model is model-agnostic since the pruning, expanding and binary masking mechanisms merely work on weight parameters (weight matrices) of the encoder-decoder framework. Next, we will introduce each component of our TPEM framework in detail. 

 Preliminary: The GLMP Model GLMP contains three primary components: external knowledge, a global memory encoder, and a local memory decoder. Next, we will briefly introduce the three components of GLMP. The readers can refer to  (Wu et al., 2019)  for the implementation details. External Knowledge To integrate external knowledge into the Seq2Seq model, GLMP adopts the end-to-end memory networks to encode the word-level information for both dialogue history (dialogue memory) and structural knowledge base (KB memory). Bag-of-word representations are utilized as the memory embeddings for two memory modules. Each object word is copied directly when a memory position is pointed to. 

 Global Memory Encoder We convert each input token of dialogue history into a fixed-size vector via an embedding layer. The embedding vectors go through a bi-directional recurrent unit (BiGRU)  (Chung et al., 2014)  to learn contextualized dialogue representations. The original memory representations and the corresponding implicit representations will be summed up, so that these contextualized representations can be written into the dialogue memory. Meanwhile, the last hidden state of dialogue representations is used to generate two outputs (i.e., global memory pointer and memory readout) by reading out from the external knowledge. Note that an auxiliary multi-label classification task is added to train the global memory pointer as a multi-label classification task. Local Memory Decoder Taking the global memory pointer, encoded dialogue history and KB knowledge as input, a sketch GRU is applied to generate a sketch response Y s that includes the sketch tags rather than slot values. If a sketch tag is generated, the global memory pointer is then passed to the external knowledge and the retrieved object word will be picked up by the local memory pointer; otherwise, the output word is generated by the sketch GRU directly. To effectively transfer knowledge for subsequent tasks and reduce the space consumption, the global memory encoder and external knowledge in GLMP are shared among all tasks, while a separate local memory decoder is learned by each task. 

 Continual Learning for TDS We employ an iterative network pruning, expanding and masking framework for TDS in continual learning scenario, inspired by . Network Pruning To avoid "catastrophic forgetting" of GLMP, a feasible way is to retain the acquired old-task weights and enlarge the network by adding weights for learning new tasks. However, as the number of tasks grows, the complexity of model architecture increases rapidly, making the deep model difficult to train. To avoid constructing a huge network, we compress the model for the current task by releasing a certain fraction of neglectable weights of old tasks  (Frankle and Carbin, 2019; Geng et al., 2021) . Suppose that for task k, a compact model M 1:k that is able to deal with tasks 1 to has been created and available. We then free up a certain fraction of neglectable weights (denoted as W F k ) that have the lowest absolute weight values by setting them to zero. The released weights associated with task k are extra weights which can be utilized repeatedly for learning newly coming tasks. However, pruning a network suddenly changes the network connectivity and thereby leads to performance deterioration. To regain its original performance after pruning, we re-train the preserved weights for a small number of epochs. After a period of pruning and re-training, we obtain a sparse network with minimal performance loss on the performance of task k. This network pruning and re-training procedures are performed iteratively for learning multiple subsequent tasks. When inferring task k, the released weights are masked in a binary on/off fashion such that the network state keeps consistent with the one learned during training. Network Expanding The amount of preserved weights for old tasks becomes larger with the growth of new tasks, and there will be fewer free weights for learning new tasks, resulting in slowing down the learning process and making the found solution non-optimal. An intuitive solution is to expand the model while learning new tasks so as to increase new capacity of the GLMP model for subsequent tasks  (Hung et al., 2019b,a) . To effectively perform network expansion while keeping the compactness of network architecture, we should consider two key factors: (1) the proportion of free weights for new tasks (denoted as F k ) and (2) the number of training batches (denoted as N k ). Intuitively, it is difficult to optimize the parameters that are newly added and randomly initialized with a small number of training data. To this end, we define the following strategy to expand the hidden size H k for the k-th task from H k?1 : H k = H k?1 + ? * (P k?1 ? F k ) * log(1 + N k /?) (1) where ? and ? are two hyperparameters. P k?1 is the pruning ratio of task k ? 1. In this way, we are prone to expand more weights for the tasks that have less free weights but more training data. 

 Network Masking The preserved weights W P k of old tasks are fixed so as to retain the performance of learned tasks and avoid forgetting. However, not all preserved weights are beneficial to learn new tasks, especially when there is a large gap between old and new tasks. To resolve this issue, we apply a learnable binary mask M k for each task k to filter some old weights that may hinder the learning of new tasks. We additionally maintain a matrix Mk of real-valued mask weights, which has the same size as the weight matrix W. The binary mask matrix M k , which participates in forward computing, is obtained by passing each element of Mk through a binary thresholding function: M k ij = 1, if Mk ij > ? 0, ortherwise (2) where ? is a pre-defined threshold. The real-valued mask Mk will be updated in the backward pass via gradient descent. After obtaining the binary mask M k for a given task, we discard Mk and only store M k . The weights selected are then represented as M k W P k , which get along with free weights W F k to learn new tasks. Here, denotes element-wise product. Note that old weights W P k are "picked" only and keep unchanged during training. Thus, old tasks can be recalled without forgetting. Since a binary mask requires only one extra bit per parameter, TPEM only introduces an approximate overhead of 1/32 of the backbone network size per parameter, given that a typical network parameter is often represented by a 32-bit float value.   72/67.15 11.35/48.48 11.88/54.25 7.29/31.79 6.21/32.59 8.42/30.78 16.71/51.35 11.23/45.20  Table  1 : BLEU/Entity F1 results evaluated on the final model after all 7 tasks are visited. We use Avg. to represent the average performance of all tasks for each method. 

 Experimental Setup Datasets Since there is no authoritative dataset for TDS in continual learning scenario, we evaluate TPEM on 7 tasks from three benchmark TDS datasets: (1) In-Car Assistant  (Eric and Manning, 2017 ) that contains 2425/302/304 dialogues for training/validation/testing, belonging to calendar scheduling, weather query, and POI navigation domains, (2) Multi-WOZ 2.1  (Budzianowski et al., 2018)  that contains 1,839/117/141 dialogues for training/validation/testing, belonging to restaurant, attraction, and hotel domains, and (3) Cam-Rest  (Wen et al., 2016)  that contains 406/135/135 dialogues from the restaurant reservation domain for training/validation/testing. Implementation Details Following  (Wu et al., 2019) , the word embeddings are randomly initialized from normal distribution N (0, 0.1) with size of 128. We set the size of encoder and decoder as 128. We conduct one-shot pruning with ratio P = 0.5. The hyperparameters ? and ? are set to 32 and 50, respectively. We use Adam optimizer to train the model, with an initial learning rate of 1e ?3 . The batch size is set to 32 and the number of memory hop k is set to 3. We set the maximum re-training epochs to 5. That is, we adopt the same re-training epochs for different tasks. We run our model three times and report the average results. Baseline Methods First, we compare TPEM with three widely used TDSs: Ptr-Unk  (Eric and Manning, 2017) , Mem2Seq  (Madotto et al., 2018) , and GLMP  (Wu et al., 2019) . In addition, we also compare TPEM with UCL  (Ahn et al., 2019)  which is a popular continual learning method. Furthermore, we report results obtained by the base model when its parameters are optionally re-initialized after a task has been visited (denoted as Re-init). We also report the results of Re-init with network expansion (denoted as Re-init-expand). Different from GLMP that keeps learning a TDS by utilizing parameters learned from past tasks as initialization for the new task, both Re-init and Re-initexpand save a separate model for each task in inference without considering the continual learning scenario. 

 Experimental Results 

 Main Results We evaluate TPEM and baselines with BLEU  (Papineni et al., 2002)  and entity F1  (Madotto et al., 2018) . We conduct experiments by following the common continual learning setting, where experimental data from 7 domains arrives sequentially. The results of each task are reported after all 7 tasks have been learned. That is, each model keeps learning a new task by using the weights learned from past tasks as initialization. The evaluation results are reported in Table  1 . The typical TDSs (i.e., Ptr-Unk, Mem2Seq, GLMP) perform much worse than the continual learning methods (UCL and TPEM). This is consistent with our claim that conventional TDSs suffer from catastrophic forgetting. TPEM achieves significantly better results than baseline methods (including Reinit and Re-init-expand) on both new and old tasks. The improvement mainly comes from the iterative network pruning, expanding and masking.   of TPEM drops more sharply when discarding network pruning than discarding the other two components. This is within our expectation since the expansion and masking strategies rely on network pruning, to some extent. Not surprisingly, combining all the components achieves the best results. Furthermore, by comparing the results of Re-init and Re-init-expand, we can observe that only using network expanding cannot improve the performance of Re-init. Case Study We provide visible analysis on the middle states of all the models. Figure  1  shows how the results of each task change as new tasks are being learned subsequently. Taking the third task as an example, we observe that the performance of conventional TDSs and UCL starts to decay sharply after learning new tasks, probably because the knowledge learned from these new tasks interferes with what was learned previously. However, TPEM stable results over the whole learning process, without suffering from knowledge forgetting. Effect of Task Ordering To explore the effect of task ordering for our TPEM model, we randomly sample 5 different task orderings in this experiment. The average results of TPEM over 7 domains with 5 different orderings are shown in Figure  2 . We can observe that although our method has various behaviors with different task orderings, TPEM is in general insensitive to orders because the results show similar trends, especially for the last 2 tasks. 

 Conclusion In this paper, we propose a continual learning method for task-oriented dialogue systems with iterative network pruning, expanding and masking. Our dialogue system preserves performance on previously encountered tasks while accelerating learning progress on subsequent tasks. Extensive experiments on 7 different tasks show that our TPEM method performs significantly better than compared methods. In the future, we plan to automatically choose the pruning ratio and the number of re-training epochs in the network pruning process for each task adaptively. Figure 1 : 1 Figure 1: The change of BLEU/Entity F1 scores for each task during the whole learning process (i.e., after learning new tasks). 
