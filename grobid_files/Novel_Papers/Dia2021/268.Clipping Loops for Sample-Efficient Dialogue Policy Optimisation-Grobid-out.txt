title
Clipping Loops for Sample-Efficient Dialogue Policy Optimisation

abstract
Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguishes useless actions from others and reduces the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in the training phase.

Introduction Based on dialogue policies, task-oriented dialogue systems decide when and how to give or request information from users. Learning dialogue policies is often formulated as a reinforcement learning (RL) problem since we usually receive feedback from users for the whole dialogue but not the correct answer for a single response  (Young et al., 2013; Levin et al., 1997) . With high-capacity of function approximation, deep reinforcement learning has been widely applied to dialogue policy optimisation  (Su et al., 2016; Li et al., 2016; . Typically, when applying deep reinforcement learning for dialogue policy management, more than thousands of dialogues are required to reach convergence . However, requiring thousands of human dialogues during training is quite impractical for most academic or real-life scenarios. Users might lose patience and exhibit different behaviour during training. Therefore, in most prior work, the agents are trained via simulated users instead of real ones . Model-based reinforcement learning (MBRL) is commonly applied to make dialogue policy optimisation sample-efficient. MBRL approaches for dialogue management build a user model to predict users' behaviour  (Wu et al., 2020b,a; Peng et al., 2018; Su et al., 2018; Wu et al., 2019; Zhang et al., 2019) . Using the user model, DDQ  (Peng et al., 2018)  generates pseudo-data. The accuracy of the user model strongly affects the quality of generated pseudo-data. If the behaviour of pseudo-data is far from real users' behaviour, dialogue policies learnt from these data might not be optimal  (Su et al., 2018) . Manipulating when to use how much data in experience buffers becomes critical in these approaches. Trainable-action-mask (TAM)  (Wu et al., 2020b ) blocks useless actions by learning action-masks from data to explore the action space more efficiently. Instead of predicting the users' behaviour directly, TAM predicts only the termination and similarity of future dialogue states to ease the training difficulties. However, the wrong predictions of the user model block the wrong actions, which makes the policy performance unstable. Moreover, the wrong output of policy does not learn from the predictions of the user model since it is blocked. Wrong values in policy networks make the performance unstable. In this work, we propose loop-clipping policy optimisation (LCPO), which clips off useless actions in trajectories, computes advantages of actions in/out of the loop separately and optimises policy based on proximal policy optimisation (PPO)  (Schulman et al., 2017) . First, LCPO is a model-free and parameter-free algorithm. There is no additional effort of tuning hyperparameters of the user model. Also, it takes almost no extra running time during testing. Second, instead of brutally blocking actions like TAM does, LCPO directly reduces the probabilities of useless actions which makes optimisation smoother and easier. In our experiment on the Cambridge Restaurant Dialogue System, LCPO uses only 260 dialogues in the training phase to reach an 80% success rate, while the PPO baseline requires 2160. In the humanin-the-loop experiment, LCPO that trained with only 100 dialogue receives 3.7/5 scores and high remarks of conciseness and fluency. Overall, our main contributions are two-fold: ? We propose LCPO, a parameter-free, sample efficient algorithm to optimise dialogue policies. This algorithm is easy to implement and has barely any overhead. ? We demonstrate that training dialogue systems with real users is feasible within 100 dialogues on Cambridge Restaurant Dialogue System. 

 Preliminaries This section goes through the notations in this paper. We start with formulating dialogue management as an RL problem in section 2.1. In section 2.2, we explain how to optimise the policy through proximal policy optimisation (PPO). In section 2.4, we explain what is episodic memory. 

 Reinforcement learning for dialogue systems When applying reinforcement learning for dialogue management  (Levin et al., 1997; Young et al., 2013; Williams, 2008 ), a state s, or a belief state, is the belief distribution over users' requests. An action a is the summarised action taken by a system. A reward r and a termination t are given by simulated users or real users. An episode E is a dialogue. The goal of reinforcement learning is to learn a policy ?(a i |s i ) that maximises the cumulative reward R = L i=0 ? i r i , where L is the length of the dialogue. 

 Proximal Policy Optimisation (PPO) Policy gradient is a fundamental optimisation algorithm with the loss function: L P G (?) = ?i [log ? ? (a i |s i ) ?i ], (1) where ?i is the estimated advantage at timestep i. In order to ensure new policy is not changing far from the old one, trust region policy optimisation (TRPO) is set to surrogate the KL-divergence between the old and the current policies. In a similar but much simpler way, proximal policy optimisation (PPO)  (Schulman et al., 2017)  clips probability ratios r i to mitigate the excessive updates in TRPO. r i = ? ? (a i |s i ) ? ? old (a i |s i ) . (2) L P P O (?) = ?[min(r i (?) ?i , r clip i ?i )], (3) where r clip i = clip(r i (?), 1 ? , 1 + ) (4) Advantage ?i and state-value Vi are estimated by generalised advantage estimation (GAE) as follow: ?i = ? i + ? ?i+1 , (5) Vi = ?i + V i , (6) where ? decays future state-value, which represents our confidence in state-value estimation. ? decays the future TD-error, which represents a trade-off between bias and variance of advantage estimation. V i is the predicted state-value of s i , and ? i is the TD-error: ? i = r i + ?V i+1 ? V i . (7) 

 Trainable-action-mask (TAM) Trainable-action-mask (TAM)  (Wu et al., 2020b ) is a model-based baseline that blocks useless actions directly. TAM learns a user model during dialogue interaction. The user model predicts the termination, reward, and the similarity between the current and the next dialogue state, and the action mask is constructed based on these features. Though TAM is simple and effective, it is not stable enough. The first reason is a common pitfall of model-based approaches: the user model is hard to train and usually leads to inaccurate predictions that harm the dialogue policy. Second, the policy and state-value approximator (i.e. the policy network and value network in PPO) do not learn from the predictions of the user model. The wrong values estimated by these networks can not be updated efficiently since these actions are blocked. 

 Episodic memory In most policy gradient algorithms, the history of interactions is recorded in a memory buffer M , which contains several episodes E. An episode E consists of N transitions {T 0 , T 1 , ..T N }. A transition T i ? {s i , a i , s i+1 , r i , t i }, where s i is the current state. a i is the action taken on s i , which leads to the next state s i+1 with a reward r i . If the episode terminates after taking action a i , t i is T rue or otherwise F alse. In this paper, we propose loop-clipping policy optimisation (LCPO) to improve sample efficiency. As illustrated in Figure  1 , LCPO consists of three components: loop clipping, advantage clipping, and policy optimisation. We adopt proximal policy optimisation (PPO)  (Schulman et al., 2017)  for the policy optimisation part in this work. Firstly, we give definitions to loops in section 3.1, and illustrate how to get clean trajectories via loop clipping in section 3.2. In section 3.3, we demonstrate how to estimate and clip advantages and statevalues of loops for policy optimisation. Note that in the following subsections, we utilise two domain knowledge in dialogue systems. ? Prior 1: Information gain is non-negative since by asking more questions, we know better about user needs. ? Prior 2: The last action of a failed dialogue and actions that loop over the same state are unwanted. 

 Definition of loop In this paper, loop means transitions that consist of useless or unwanted actions. As illustrated in Figure  2 , we define two kinds of the loop: N -hop loop and termination loop corresponding to our prior 2. Definition 1. A N -hop loop L N i is a sequence of N transitions {T i , T i+1 , ..T i+N ?1 } where s i = s i+N . Since in a loop that the starting state s i becomes the same as final state s N , {a i , a i+1 , .., a i+N ?1 } is a useless action sequence on state s i . In dialogue systems, N -hop loop might result from repetitively asking the same questions or giving the same information. Compared with the definition of useless actions in TAM  (Wu et al., 2020b ) which only considers the similarity of the next state (i.e 1-hop loop), N -hop loop is a more general definition and is able to detect more useless actions. Definition 2. A termination loop L T i is a transition T i state s i where t i = T rue and r i ? 0. In dialogue systems, a i is a useless action on state s i since the dialogue is terminated and failed. For example, termination loops might result from saying goodbye before completing tasks or making users out of patience. Note that the definition of loops utilises domain knowledge and might not be suitable for other applications. 

 Loop clipping As illustrated in Figure  3 , the original trajectory might contain several identical states. We search for the identical states pair-wisely and detect loops by definitions in section 3.1.  1  The detected loops are clipped off from the original trajectory. After clipping, the trajectory becomes succinct so that reward signals can be assigned to useful actions effectively (Figure  3b ). In dialogue systems, the information for each state in a loop is the same since there is no information gain after taking useless actions. Therefore, a N -hop loop can be viewed as multiple one-hop loops as illustrated in Figure  3c . 

 Loop advantage estimation (LAE) After loop clipping, the original trajectory is split into a clean trajectory and several loops. Then We estimate the advantages of the clean trajectory and loops separately. For the clean trajectory, standard 1 When the loop structure is complex. The solution of N-hop loop clipping is not unique. generalised advantage estimation (GAE)  (Schulman et al., 2015)  is applied as shown in Eq. 5, 6. If we only update the policy based on clean trajectories, the clipped useless actions will not be treated as training data. Therefore, these useless actions will not be penalised, resulting in unwanted lengthy dialogue. We first illustrate how to estimate state-values and advantage for loops, noted as loop advantage estimation (LAE) to distinguish from GAE. Second, we propose an advantage clipping trick, which makes the policy optimisation much more sample-efficient. State-value estimation According to prior 1, in dialogue systems, information gain V i+1 ? V i ? 0. In a loop L N i with length N , since the Vi ? Vi+1 ? .. ? Vi+N (8) and the same states share the same value i.e. Vi = Vi+N , (9) all the state-values in L N i are the same: Vi = Vi+1 = .. = Vi+N . ( 10 ) Advantage estimation The loop advantage for action a i is: ?LAE i = ? i + ? ?GAE , (11) where ? i = r i + ?V i+1 ? V i . Note that ?GAE is the next advantage ?i+N after the loop L n i . No matter how long the loop is, the loop advantage is computed from the transition after loop. When state-values converge, V i+1 V i in loop L i by eq. 10. We can see that V i+1 V i =? ? i r i + (? ? 1)V i . = R i =? ?LAE i R i + ? ?GAE =? ?LAE i ?LAE i+1 , (12) where R i = r i + (? ? 1)V i . It is straightforward that when values converge, the advantage of loop is the advantage of best actions A GAE i with a oneturn penalty for all useless actions on state s i (since the agent wastes one more turn on the same state). When ?GAE converges to zero, ?LAE converges to R i . Advantage clipping However, we found that the advantage estimation is still not very accurate in the early stage of training process. The advantages of looping actions sometimes are higher than others and these actions are not penalised. To properly penalise the looping actions, we clip the advantages in both LAE and GAE. The clipping threshold R i = r + (? ? 1)V i since ?LAE converges to this value. ?ClipGAE i = max(R i , ?GAE i ), (13) ?ClipLAE i = min(R i , ?LAE i ), (14) where R i = r i + (? ? 1)V i , so that ?ClipGAE i ? R i ? ?ClipLAE i . ( 15 ) This trick distinguishes bad responses from good ones explicitly and makes policy converge faster. The instruction of LCPO implementation is in Algorithm 1. 

 Experiments Experiments are conducted on the Cambridge restaurant dialogue system using the PyDial toolkit . We evaluate the agents on both a simulated user and real users. From section 4.1 to 4.5, we illustrate the experiments with a simulated user. For human-in-the-loop experiment, see section 4.6. 

 Settings User simulator We use a goal-driven simulated user on the semantic level  (Schatzmann et al., 2007; Schatzmann and Young, 2009) . The maximum dialogue length is set to 25 turns and ? = 0.99. The reward is defined as 20 for a successful dialogue   ferent slot-values. In practice the optimal threshold depends on the noise level of state observation. Evaluation In the experiment with the simulated user, we evaluate each agent with 500 dialogues after every 100 training dialogues. The mean and standard deviation of performance is computed over 10 runs with different neural networks initialisation. The mean ? standard deviation is depicted as the shaded area. The x-axes of figures are in log-scale to emphasise both the early stage and the final performance of the training process. 

 Baseline Comparisons In figure  4 , we compare the performance of PPO  (Schulman et al., 2017) , TAM  (Wu et al., 2020b) , and LCPO. The left part of the figure shows the learning curves of the success rate. We can see that LCPO is considerably stable and sampleefficient. Worth to note that LCPO has the best final performance. TAM learns slower, and PPO requires a large number of training dialogues. The right part of the figure shows the average turns taken by the agent. The lower, the better. We can see that LCPO takes more turns in the beginning but becomes more concise than the baselines later. In table 1, we can see the detail of performance at 200 and 2000 training dialogues respectively. In low resource scenario, where the dialogue policy is trained by 200 dialogues, LCPO outperforms other baselines with small variance. Yet the average number of loops in each dialogue is higher. That is because LCPO takes more turn than other agents. Other agents often give poor responses so that the users leave the dialogue out of patience with fewer turns. Regarding final performance at 2000 dialogues, all of the agents perform similarly. We can note that LCPO takes the least number of turns since its algorithm prevents from doing useless actions. LCPO requires only 260 dialogues to reach 80% success rate while PPO takes 2160. In addition, LCPO is light-packed and does not consume a lot of additional training time like TAM.  

 Ablation study: termination loop In the left part of figure  5 , the red and brown lines are LCPO with and without clipping termination loop L T respectively. We can see that without clipping L T , the learning curves become less stable and inhibit the cold-start problem at the beginning of training. In a failed dialogue, some actions are good and should not be penalised for the failure of conversation. Therefore, we should clip off the last transition in failed dialogue, so that the rest transitions in the clean trajectories (not in loops) are not penalised for the failure. For example, if we clip off the last the action "bye" in a failed dialogue, only 'bye' is strongly penalised while other normal interactions are not. 

 Ablation study: advantage clipping We propose 4 agents for comparisons: 1) clip both GAE and LAE, 2) clip LAE, 3) clip GAE, 4) no advantage clipping. The success rates after training with 100, 200, 2000 dialogues are reported in Table  2 . In the low-resource scenario (less than 200 dialogues), clipping both GAE and LAE outperforms other methods considerably. And LCPO with no advantage clipping is the worst. Without clipping, inaccurate advantage estimation in the early stage of the training process cannot reduce the probabilities of useless actions efficiently. Regarding the final performance after training agents with 2000 dialogues, all of the methods perform similarly. Yet, if we only clip the GAE, the final performance is slightly worse than others. That is because not all the actions in clean trajectories are useful. The 'clean' trajectories still contain several useless actions though not detected. Assigning larger advantages to all actions in clean trajectories makes performance unstable. 

 Robustness to hyperparameters In the right part of figure  5 , policy update interval is set to 50 and 100 for PPO and LCPO. The red and brown lines are LCPO and the green and blue lines are PPO with different update intervals. We can see that the performance of PPO is strongly affected by the update interval. In contrast, LCPO still shows high stability and sample efficiency. Its robustness to hyperparameters makes tuning LCPO effortless. 

 Human-in-the-loop Evaluation General Settings The dialogue system uses a rule-based belief tracker, and an NLG model  (Wen et al., 2015) . In each dialogue, one of the agents is randomly picked to talk with a user. The users have to interact with the agent according to a given instruction on the user goal sampled from the corpus. The users can decide to leave the dialogue session if they are out of patience. Figure 1 : 1 Figure 1: Illustration of LCPO. (a) Loop clipping. Clip off loops in trajectories to make them succinct. (b) Advantage clipping. Set a threshold such that the advantages of loops are always less than other useful actions. (c) Policy optimisation. We adopt proximal policy optimisation (PPO) in this paper. 

 Figure 2 : 2 Figure 2: Illustration of two kinds of loops. In this paper, loop means transitions consist of useless or unwanted actions. (a) N-hop loop L N , where N = 5 in this example. (b) Termination loop L T . 

 Figure 3 : 3 Figure 3: Loop clipping. The shaded circles are states and the arrows are actions taken between states. (a) The original trajectory. (b) The clean trajectory after loop clipping. In this example, a 1-hop loop and a 2-hop loop are detected. (c) In dialogue systems, the states in a loop are the same since there is no information gain. 

 Figure 4 : 4 Figure 4: Learning curves of different algorithms. Left: Success rate. Right: Average number of turns. The results are evaluated by 10 runs. The lines are averages and the shades represent standard deviations. 

 Figure 5 : 5 Figure 5: Left: Ablation study of termination loop. Right: Robustness to hyper-parameters. 

 Algorithm 1: LCPO Algorithm 1 Collect N transitions into Memory M 2 for Episode E in M do // Loop clipping 3 for T i in E do 4 if i<ptr then 5 continue 6 for T i in E do 7 if s i == s j then 8 ptr = j 9 if i<ptr then 10 T i ? L // Advantage estimation in reversed order 11 for Transition T i in reversed(E) do 12 if T i ? L then 13 Estimate ?, V via clipped LAE (Eq. 14, 10) 14 else 15 Estimate ?, V via clipped GAE (Eq. 13, 6) minus the number of turns in the dialogue. 15% semantic error rate (SER) is included in the user simulator to accommodate for automatic speech recognition (ASR) error. Policy optimisation Proximal policy optimisa- tion (PPO) is applied. The state and action di- mension of policy and value networks are 268 and 16. Dimensions of two hidden layers are 130 and 50. The agent collects a N = 100 transitions to update the policy ? with K = 10 epochs and mini- batch size B = 16. After an update, the memory is flushed and becomes empty again. Optimiser is ADAM (Kingma and Ba, 2014) with a learning rate 0.001. Entropy coefficient is 0.01 and standardised advantages is applied. During testing, actions are sampled from the output distributions of the policy network. Loops Detection In theory, the starting state and the ending state are identical in a loop. Yet, due to numerical uncertainty, we use cosine similarity with threshold ? = 0.99 to justify whether two states are the same. Under this strict setting, two states are considered different if they have any dif- 

 Table 1 : 1 Baseline comparison. The highest performance in each column is highlighted. The success rate, number of turns, and number of loops are reported for 200 and 2000 training dialogues. The number of training dialogue required to reach 80% success rate and the training time usage are also listed in the table. #Dialogue means the average required number of dialogues to reach 80% success rate. Time means the average training time for 2000 training dialogues. Low resource experiment@200D Final performance@2000D Algorithm Suc. Turns #Loops #Dialog Suc. Turns #Loops Time PPO 37.8 ? 17.2% 7.4 ? 2.3 3.5 2160 95.16 ? 1.1% 7.8 ? 0.4 2.3 58 min TAM 36.2 ? 6.8% 8.6 ? 0.9 4.1 1140 93.58 ? 1.5% 7.4 ? 0.5 2.5 125 min LCPO 76.0 ? 2.9% 11.5 ? 0.8 4.7 260 95.7 ? 1.1% 6.3 ? 0.3 0.9 68 min 

 Table 2 : 2 Comparison of different advantage clipping methods. Success rates are reported after training agents with 100, 200, 2000 dialogues. The highest success rate in each column is highlighted. Success Rate Methods @100 @200 @2000 Clip GAE+LAE 55.4 76.0 93.7 Clip LAE 50.7 73.5 93.9 Clip GAE 51.1 72.6 91.0 No adv clip 45.7 61.8 93.5 

			 Optimise the policy ? via PPO (Eq. 3), with K epochs and mini-batch size B
